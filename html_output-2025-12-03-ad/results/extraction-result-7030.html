<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7030 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7030</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7030</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-272690023</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.10294v2.pdf" target="_blank">MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation</a></p>
                <p><strong>Paper Abstract:</strong> For the knowledge graph to text(KG-to-text) generation task, recent works have attempted to incorporate graph structure information into pre-trained language models(PLMs) to capture the structure information of knowledge graphs. However, these improvements only capture single-granularity structure information, either between entities in the original graph or between words within or across entities. Considering only entity-level structures neglects the semantic information between words, while focusing solely on word-level structures ignores the relationships between complete entities. Therefore, this paper proposes the Multi-Granularity Graph Structure Attention (MGSA), which integrates both granularities of information. The encoder of the model architecture features an entity-level structure encoding module, a word-level structure encoding module, and an aggregation module that synthesizes information from both structure. This allows the model to more comprehensively understand the information contained in the original knowledge graph, thereby improving the quality of the generated text. We evaluated the MGSA model on two popular KG-to-text generation benchmark datasets and achieved superior performance compared to models based on single-granularity structures.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7030.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7030.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entity-level linearization (<H>,<R>,<T>)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entity-level triple concatenation with special tokens <H>, <R>, <T></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential tokenization that concatenates RDF-style triples into a single token sequence, marking head, relation, and tail with special tokens to represent triple boundaries and roles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triple concatenation with <H>/<R>/<T></td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>All KG triples are concatenated into one token sequence; each triple's head, relation, and tail are delimited with special tokens <H>, <R>, and <T>. Triples are clustered by head-entity before concatenation to keep related triples adjacent.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (token-based) augmented with structural matrices</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>edge-list concatenation (concatenate triples), with head-entity clustering (group triples by head) before serialization</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG v2.0; EventNarrative</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>knowledge-graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MGSA (BART-base backbone, reported total ~167M params)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder architecture built on BART; encoder augmented with entity-level and word-level graph-structure attention modules and an aggregation module; MGSA total ~167M parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU4, METEOR, ROUGEL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>WebNLG: BLEU4=66.45, METEOR=46.93, ROUGEL=76.47; EventNarrative: BLEU4=35.22, METEOR=27.61, ROUGEL=64.46</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>When used with MGSA's graph-structure attention matrices, the triple-concatenation input allowed fine-tuning of a BART base model to improve automatic metrics (BLEU/METEOR/ROUGE) versus baselines; clustering by head-entity improved coherence and helped performance on small-to-moderate graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Linearization alone loses explicit graph connectivity (hence requires additional structure encodings); token sequence length constraints (max input length 256) limit the size of graphs that can be represented; for large graphs generated text showed missing content, hallucinations, or redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Unlike plain linearization used by baseline PLM fine-tuning, this approach is combined with explicit structure matrices (R_E, R_N) and clustering; reported to outperform approaches that only inject multi-level embeddings (e.g., KGPT) or single-granularity structure encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7030.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7030.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word-level linearization ([N])</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word-level tokenization with per-word nodes marked by [N]</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Entities and relations are split into word tokens; each word is treated as a node in a word-level graph and marked with a token ([N]) in the linearized input sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Word-level token sequence with [N] markers</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each entity/relation string is split into words and concatenated in triple order; each word is flagged with a special marker [N] so token positions correspond to word-level nodes in a constructed word graph used to compute word-level structure attention.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (token-based) augmented with word-level graph matrix</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>split nodes into words and serialize by concatenating word tokens per triple; words are treated as nodes in a word-level graph for shortest-path based encoding</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG v2.0; EventNarrative</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>knowledge-graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MGSA (BART-base backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BART-based encoder-decoder where word-level inputs are paired with a word-level relative position matrix; the word-level encoder creates attention that augments PLM token attention.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU4, METEOR, ROUGEL (including ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Full MGSA (both granularities) on WebNLG: BLEU4=66.45. Ablation: removing the word-level module reduced BLEU4 to 65.58 on WebNLG (approx −0.87 BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Adding word-level linearization plus a word-level relative position matrix yields modest gains when combined with entity-level attention; contributes to improved BLEU/METEOR/ROUGE versus entity-only in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Smaller contribution than entity-level encoding (ablation shows entity-level has larger effect); increases token sequence length and attention computation cost; potential for diminishing returns on large/complex graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Similar in spirit to token-level structure encodings used by Graformer and UniD2T, but here explicitly combined with entity-level matrices and clustering; reported to complement entity-level attention and outperform single-granularity models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7030.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7030.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entity-level relative position matrix (R_E)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative position matrix over bipartite entity–relation nodes (R_E)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discrete matrix encoding connection types between pooled nodes in a bipartite graph (entities vs relations) used to bias encoder self-attention toward graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Entity-level relative position matrix (R_E)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Constructed from a bipartite graph mapping entities and relations to nodes; entries take small integer codes depending on connection type: 1 for neighboring entities, 2 for (entity,edge) pairs, 3 for (edge,entity) pairs; mapped into vectors via γ(·) and added to attention logits.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>matrix-based structural encoding (used to augment sequential token attention)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>pool token-level representations into m entity/relation nodes (mean pooling), build bipartite adjacency and R_E based on node connection types, incorporate γ(R_E) into scaled dot-product attention</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG v2.0; EventNarrative</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>knowledge-graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MGSA (entity-level module)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Entity-level module pools token vectors into entity/relation nodes and computes graph-aware self-attention using adjacency matrix A and relative position matrix R_E added to attention logits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU4, METEOR, ROUGEL; ablation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation indicates R_E materially improves generation: models using only relative-position encoding outperform those using only adjacency by ~0.52 BLEU on WebNLG; removing entity-level encodings produces larger drops than removing word-level encodings (examples in Table IV).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Incorporating R_E into encoder attention improved final generation metrics and led to better capture of entity relationships compared to using raw linear attention or adjacency-only encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Design uses discrete connection-type codes which may be coarse; adjacency and relative-position both required to fully capture entity structure; still depends on pooling (mean), which may lose fine-grained token distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Reported to be more effective than adjacency-only graph encodings in this model; outperforms approaches that only inject embeddings into the linearized sequence (e.g., KGPT) when combined with word-level matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7030.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7030.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word-level relative position matrix (R_N)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word-level relative position matrix based on shortest-path distances (R_N)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A relative-distance matrix at the word-node level where entries encode shortest-path distances between word nodes (and a special encoding for words in the same entity), used to bias token attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Word-level relative position matrix (R_N)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct a word-level graph by splitting each entity/relation into word nodes; compute shortest path distances δ(n_i,n_j); R_N entries are δ values, signed/directional when paths differ, encode words from the same entity via encode(p)=sgn(p)*δ_max + p, and unreachable pairs as ∞; γ(R_N) is added to scaled attention.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>matrix-based structural encoding (token-level)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>build word-level graph; compute shortest-path distances between words; encode same-entity words with special offset and unreachable with ∞; inject γ(R_N) into attention computation</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG v2.0; EventNarrative</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>knowledge-graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MGSA (word-level module)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Word-level module produces Q/K/V from token-level inputs and augments attention with γ(R_N)V_N term to bias attention by word-graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU4, METEOR, ROUGEL; ablation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation shows adding the word-level R_N increases performance modestly (full model BLEU4=66.45 vs entity-only model lower; removing the word-level module yields BLEU4 65.58 on WebNLG).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Improves model's ability to capture intra-entity and cross-entity lexical relationships, producing small but consistent gains when combined with entity-level encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Computational cost for computing shortest-path distances and handling long distances/infinite distances; smaller absolute impact than entity-level matrix; may increase token attention complexity and memory use.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Similar aim to token-level relative-position approaches in the literature (e.g., Graformer), but MGSA's R_N is combined with an entity-level matrix and aggregation module, which the paper reports yields better end-to-end generation than single-granularity alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7030.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7030.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clustering by head-entity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clustering triples by head entity before linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing canonicalization that groups triples by their head entities so that related triples appear adjacent in the serialized token sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Head-entity clustering canonicalization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Before linearizing triples into a token sequence, triples are grouped (clustered) by their head entities so that information about the same entity appears contiguously in the linearized input.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>preprocessing canonicalization (hierarchical/sequential)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>group triples by head entity and then concatenate grouped triples (within each group follow triple ordering from dataset), producing a clustered serialized sequence</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG v2.0; EventNarrative</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>knowledge-graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MGSA (preprocessing step for linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>No extra parameters; data-processing step designed to align linearization with human discourse patterns (topic locality) to improve generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU4, METEOR, ROUGEL (reported as part of full-model experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Implicit in MGSA results (full-model scores above baselines); authors state clustering helps preserve completeness and coherence in generated text, especially for small-to-moderate sized KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Helps model produce more coherent, entity-focused text and likely reduces learning difficulty by aligning input order with human language habits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not a guaranteed global canonical ordering across datasets; effectiveness depends on dataset properties (quality and typical triple grouping); does not alone recover graph connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Authors argue this clustering better matches human discourse than random triple orderings used in datasets; yields better coherence when combined with MGSA structure encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7030.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7030.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGPT (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KGPT: Knowledge-grounded pre-training for data-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that linearizes structures and injects multi-level structural embeddings into the token sequence for PLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>KGPT: Knowledge-grounded pre-training for datato-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized sequence + multi-level structural embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>KG is linearized into a sequence and multi-level structural embeddings are attached to tokens to provide structural cues to the PLM without separate graph-aware attention modules.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (token-based) with auxiliary embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>linearization (triples) and addition of special structural embeddings at token positions</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (reported comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>knowledge-graph-to-text generation / data-to-text pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KGPT (reported ~177M params in paper Table II)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-training approach that augments token sequences with multi-level structural embeddings to better ground PLMs for data-to-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU4, METEOR, ROUGEL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>WebNLG (reported in this paper): BLEU4=64.11, METEOR=46.30, ROUGEL=74.57</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Adding multi-level embeddings improves PLM performance compared to vanilla PLM but, according to MGSA authors, is less effective than MGSA's explicit multi-granularity graph-structure attention.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Embedding injection is a lighter-weight signal and may not fully capture graph relational structure compared to explicit graph-aware attention; according to MGSA, MGSA's approach yields higher BLEU/ROUGE.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>MGSA outperforms KGPT by reported margins (e.g., +2.34% BLEU on WebNLG), suggesting explicit graph-structure attention (multi-granularity) can be more effective than embedding injection alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7030.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7030.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graformer (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graformer (word-level relative-position approach referenced in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior model that designs a relative graph position matrix (token-level) to capture word-level structure information for text generation from graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Token-level relative graph position matrix</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Designs relative positions among tokens to embed graph structure into Transformer attention (captures word-level adjacency/relative distances).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>matrix-based token-level structural encoding</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>derive relative positions for tokens from the graph and stitch into attention (Transformer-relative positional scheme)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (reported comparison in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>knowledge-graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based model augmented with a relative graph position matrix to incorporate word-level graph structure into attention.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU4, METEOR (as compared in Table II)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>WebNLG (reported in this paper): BLEU4=61.15, METEOR=43.38</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Captures token-level structural relations to improve generation over vanilla Transformer in prior work; in MGSA comparisons, MGSA outperforms Graformer on WebNLG.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reported to produce lower scores than MGSA on WebNLG; MGSA authors speculate part of the gap is due to their choice of BART versus generic Transformer backbone used by Graformer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>MGSA outperforms Graformer on WebNLG (BLEU4 difference reported in paper), likely because MGSA combines both word- and entity-level structure rather than only word-level.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7030.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7030.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniD2T (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unifying Structured Data as Graph for Data-to-Text Pre-Training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model aimed at unifying various structured inputs as graphs and using a structure-enhancement module to capture word-level structure information for data-to-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unifying Structured Data as Graph for Data-to-Text Pre-Training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structure-enhanced word-level encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes structured inputs (treated as graphs) into PLMs via a structure-enhancement module focusing on word-level relations in inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential token-based + structure-enhancement module</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>treats structured data as graphs and applies module to enhance token embeddings with structure-aware signals</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (reported comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>data-to-text generation / KG-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UniD2T</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pre-training approach for data-to-text that unifies multiple structured formats as graphs and uses a module to enrich token-level representations with structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU4, METEOR, ROUGEL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>WebNLG (reported in this paper): BLEU4=60.41, METEOR=44.35</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Designed to be a general pre-training approach across structured data types; according to MGSA, UniD2T performed worse on WebNLG than MGSA's KG-specialized multi-granularity approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Less effective than MGSA on WebNLG in reported numbers; being general-purpose may reduce task-specific gains on KG-only tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>MGSA (task-specific, KG-only focus) outperformed UniD2T on WebNLG (MGSA BLEU4 66.45 vs UniD2T BLEU4 60.41).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>KGPT: Knowledge-grounded pre-training for datato-text generation <em>(Rating: 2)</em></li>
                <li>Unifying Structured Data as Graph for Data-to-Text Pre-Training <em>(Rating: 2)</em></li>
                <li>GAP: A graph-aware language model framework for knowledge graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Modeling graph structure via relative position for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>JointGT: Graph-text joint representation learning for text generation from knowledge graphs <em>(Rating: 1)</em></li>
                <li>GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7030",
    "paper_id": "paper-272690023",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Entity-level linearization (&lt;H&gt;,&lt;R&gt;,&lt;T&gt;)",
            "name_full": "Entity-level triple concatenation with special tokens &lt;H&gt;, &lt;R&gt;, &lt;T&gt;",
            "brief_description": "A sequential tokenization that concatenates RDF-style triples into a single token sequence, marking head, relation, and tail with special tokens to represent triple boundaries and roles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Triple concatenation with &lt;H&gt;/&lt;R&gt;/&lt;T&gt;",
            "representation_description": "All KG triples are concatenated into one token sequence; each triple's head, relation, and tail are delimited with special tokens &lt;H&gt;, &lt;R&gt;, and &lt;T&gt;. Triples are clustered by head-entity before concatenation to keep related triples adjacent.",
            "representation_type": "sequential (token-based) augmented with structural matrices",
            "encoding_method": "edge-list concatenation (concatenate triples), with head-entity clustering (group triples by head) before serialization",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "WebNLG v2.0; EventNarrative",
            "task_name": "knowledge-graph-to-text generation",
            "model_name": "MGSA (BART-base backbone, reported total ~167M params)",
            "model_description": "Encoder-decoder architecture built on BART; encoder augmented with entity-level and word-level graph-structure attention modules and an aggregation module; MGSA total ~167M parameters.",
            "performance_metric": "BLEU4, METEOR, ROUGEL",
            "performance_value": "WebNLG: BLEU4=66.45, METEOR=46.93, ROUGEL=76.47; EventNarrative: BLEU4=35.22, METEOR=27.61, ROUGEL=64.46",
            "impact_on_training": "When used with MGSA's graph-structure attention matrices, the triple-concatenation input allowed fine-tuning of a BART base model to improve automatic metrics (BLEU/METEOR/ROUGE) versus baselines; clustering by head-entity improved coherence and helped performance on small-to-moderate graphs.",
            "limitations": "Linearization alone loses explicit graph connectivity (hence requires additional structure encodings); token sequence length constraints (max input length 256) limit the size of graphs that can be represented; for large graphs generated text showed missing content, hallucinations, or redundancy.",
            "comparison_with_other": "Unlike plain linearization used by baseline PLM fine-tuning, this approach is combined with explicit structure matrices (R_E, R_N) and clustering; reported to outperform approaches that only inject multi-level embeddings (e.g., KGPT) or single-granularity structure encoders.",
            "uuid": "e7030.0",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Word-level linearization ([N])",
            "name_full": "Word-level tokenization with per-word nodes marked by [N]",
            "brief_description": "Entities and relations are split into word tokens; each word is treated as a node in a word-level graph and marked with a token ([N]) in the linearized input sequence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Word-level token sequence with [N] markers",
            "representation_description": "Each entity/relation string is split into words and concatenated in triple order; each word is flagged with a special marker [N] so token positions correspond to word-level nodes in a constructed word graph used to compute word-level structure attention.",
            "representation_type": "sequential (token-based) augmented with word-level graph matrix",
            "encoding_method": "split nodes into words and serialize by concatenating word tokens per triple; words are treated as nodes in a word-level graph for shortest-path based encoding",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "WebNLG v2.0; EventNarrative",
            "task_name": "knowledge-graph-to-text generation",
            "model_name": "MGSA (BART-base backbone)",
            "model_description": "BART-based encoder-decoder where word-level inputs are paired with a word-level relative position matrix; the word-level encoder creates attention that augments PLM token attention.",
            "performance_metric": "BLEU4, METEOR, ROUGEL (including ablation)",
            "performance_value": "Full MGSA (both granularities) on WebNLG: BLEU4=66.45. Ablation: removing the word-level module reduced BLEU4 to 65.58 on WebNLG (approx −0.87 BLEU).",
            "impact_on_training": "Adding word-level linearization plus a word-level relative position matrix yields modest gains when combined with entity-level attention; contributes to improved BLEU/METEOR/ROUGE versus entity-only in this work.",
            "limitations": "Smaller contribution than entity-level encoding (ablation shows entity-level has larger effect); increases token sequence length and attention computation cost; potential for diminishing returns on large/complex graphs.",
            "comparison_with_other": "Similar in spirit to token-level structure encodings used by Graformer and UniD2T, but here explicitly combined with entity-level matrices and clustering; reported to complement entity-level attention and outperform single-granularity models.",
            "uuid": "e7030.1",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Entity-level relative position matrix (R_E)",
            "name_full": "Relative position matrix over bipartite entity–relation nodes (R_E)",
            "brief_description": "A discrete matrix encoding connection types between pooled nodes in a bipartite graph (entities vs relations) used to bias encoder self-attention toward graph structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Entity-level relative position matrix (R_E)",
            "representation_description": "Constructed from a bipartite graph mapping entities and relations to nodes; entries take small integer codes depending on connection type: 1 for neighboring entities, 2 for (entity,edge) pairs, 3 for (edge,entity) pairs; mapped into vectors via γ(·) and added to attention logits.",
            "representation_type": "matrix-based structural encoding (used to augment sequential token attention)",
            "encoding_method": "pool token-level representations into m entity/relation nodes (mean pooling), build bipartite adjacency and R_E based on node connection types, incorporate γ(R_E) into scaled dot-product attention",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "WebNLG v2.0; EventNarrative",
            "task_name": "knowledge-graph-to-text generation",
            "model_name": "MGSA (entity-level module)",
            "model_description": "Entity-level module pools token vectors into entity/relation nodes and computes graph-aware self-attention using adjacency matrix A and relative position matrix R_E added to attention logits.",
            "performance_metric": "BLEU4, METEOR, ROUGEL; ablation analysis",
            "performance_value": "Ablation indicates R_E materially improves generation: models using only relative-position encoding outperform those using only adjacency by ~0.52 BLEU on WebNLG; removing entity-level encodings produces larger drops than removing word-level encodings (examples in Table IV).",
            "impact_on_training": "Incorporating R_E into encoder attention improved final generation metrics and led to better capture of entity relationships compared to using raw linear attention or adjacency-only encodings.",
            "limitations": "Design uses discrete connection-type codes which may be coarse; adjacency and relative-position both required to fully capture entity structure; still depends on pooling (mean), which may lose fine-grained token distinctions.",
            "comparison_with_other": "Reported to be more effective than adjacency-only graph encodings in this model; outperforms approaches that only inject embeddings into the linearized sequence (e.g., KGPT) when combined with word-level matrices.",
            "uuid": "e7030.2",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Word-level relative position matrix (R_N)",
            "name_full": "Word-level relative position matrix based on shortest-path distances (R_N)",
            "brief_description": "A relative-distance matrix at the word-node level where entries encode shortest-path distances between word nodes (and a special encoding for words in the same entity), used to bias token attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Word-level relative position matrix (R_N)",
            "representation_description": "Construct a word-level graph by splitting each entity/relation into word nodes; compute shortest path distances δ(n_i,n_j); R_N entries are δ values, signed/directional when paths differ, encode words from the same entity via encode(p)=sgn(p)*δ_max + p, and unreachable pairs as ∞; γ(R_N) is added to scaled attention.",
            "representation_type": "matrix-based structural encoding (token-level)",
            "encoding_method": "build word-level graph; compute shortest-path distances between words; encode same-entity words with special offset and unreachable with ∞; inject γ(R_N) into attention computation",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "WebNLG v2.0; EventNarrative",
            "task_name": "knowledge-graph-to-text generation",
            "model_name": "MGSA (word-level module)",
            "model_description": "Word-level module produces Q/K/V from token-level inputs and augments attention with γ(R_N)V_N term to bias attention by word-graph structure.",
            "performance_metric": "BLEU4, METEOR, ROUGEL; ablation analysis",
            "performance_value": "Ablation shows adding the word-level R_N increases performance modestly (full model BLEU4=66.45 vs entity-only model lower; removing the word-level module yields BLEU4 65.58 on WebNLG).",
            "impact_on_training": "Improves model's ability to capture intra-entity and cross-entity lexical relationships, producing small but consistent gains when combined with entity-level encoding.",
            "limitations": "Computational cost for computing shortest-path distances and handling long distances/infinite distances; smaller absolute impact than entity-level matrix; may increase token attention complexity and memory use.",
            "comparison_with_other": "Similar aim to token-level relative-position approaches in the literature (e.g., Graformer), but MGSA's R_N is combined with an entity-level matrix and aggregation module, which the paper reports yields better end-to-end generation than single-granularity alternatives.",
            "uuid": "e7030.3",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Clustering by head-entity",
            "name_full": "Clustering triples by head entity before linearization",
            "brief_description": "A preprocessing canonicalization that groups triples by their head entities so that related triples appear adjacent in the serialized token sequence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Head-entity clustering canonicalization",
            "representation_description": "Before linearizing triples into a token sequence, triples are grouped (clustered) by their head entities so that information about the same entity appears contiguously in the linearized input.",
            "representation_type": "preprocessing canonicalization (hierarchical/sequential)",
            "encoding_method": "group triples by head entity and then concatenate grouped triples (within each group follow triple ordering from dataset), producing a clustered serialized sequence",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "WebNLG v2.0; EventNarrative",
            "task_name": "knowledge-graph-to-text generation",
            "model_name": "MGSA (preprocessing step for linearization)",
            "model_description": "No extra parameters; data-processing step designed to align linearization with human discourse patterns (topic locality) to improve generation quality.",
            "performance_metric": "BLEU4, METEOR, ROUGEL (reported as part of full-model experiments)",
            "performance_value": "Implicit in MGSA results (full-model scores above baselines); authors state clustering helps preserve completeness and coherence in generated text, especially for small-to-moderate sized KGs.",
            "impact_on_training": "Helps model produce more coherent, entity-focused text and likely reduces learning difficulty by aligning input order with human language habits.",
            "limitations": "Not a guaranteed global canonical ordering across datasets; effectiveness depends on dataset properties (quality and typical triple grouping); does not alone recover graph connectivity.",
            "comparison_with_other": "Authors argue this clustering better matches human discourse than random triple orderings used in datasets; yields better coherence when combined with MGSA structure encodings.",
            "uuid": "e7030.4",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "KGPT (mention)",
            "name_full": "KGPT: Knowledge-grounded pre-training for data-to-text generation",
            "brief_description": "A prior approach that linearizes structures and injects multi-level structural embeddings into the token sequence for PLM fine-tuning.",
            "citation_title": "KGPT: Knowledge-grounded pre-training for datato-text generation",
            "mention_or_use": "mention",
            "representation_name": "Linearized sequence + multi-level structural embeddings",
            "representation_description": "KG is linearized into a sequence and multi-level structural embeddings are attached to tokens to provide structural cues to the PLM without separate graph-aware attention modules.",
            "representation_type": "sequential (token-based) with auxiliary embeddings",
            "encoding_method": "linearization (triples) and addition of special structural embeddings at token positions",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (reported comparison)",
            "task_name": "knowledge-graph-to-text generation / data-to-text pretraining",
            "model_name": "KGPT (reported ~177M params in paper Table II)",
            "model_description": "Pre-training approach that augments token sequences with multi-level structural embeddings to better ground PLMs for data-to-text tasks.",
            "performance_metric": "BLEU4, METEOR, ROUGEL",
            "performance_value": "WebNLG (reported in this paper): BLEU4=64.11, METEOR=46.30, ROUGEL=74.57",
            "impact_on_training": "Adding multi-level embeddings improves PLM performance compared to vanilla PLM but, according to MGSA authors, is less effective than MGSA's explicit multi-granularity graph-structure attention.",
            "limitations": "Embedding injection is a lighter-weight signal and may not fully capture graph relational structure compared to explicit graph-aware attention; according to MGSA, MGSA's approach yields higher BLEU/ROUGE.",
            "comparison_with_other": "MGSA outperforms KGPT by reported margins (e.g., +2.34% BLEU on WebNLG), suggesting explicit graph-structure attention (multi-granularity) can be more effective than embedding injection alone.",
            "uuid": "e7030.5",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Graformer (mention)",
            "name_full": "Graformer (word-level relative-position approach referenced in paper)",
            "brief_description": "A prior model that designs a relative graph position matrix (token-level) to capture word-level structure information for text generation from graphs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Token-level relative graph position matrix",
            "representation_description": "Designs relative positions among tokens to embed graph structure into Transformer attention (captures word-level adjacency/relative distances).",
            "representation_type": "matrix-based token-level structural encoding",
            "encoding_method": "derive relative positions for tokens from the graph and stitch into attention (Transformer-relative positional scheme)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (reported comparison in this paper)",
            "task_name": "knowledge-graph-to-text generation",
            "model_name": "Graformer",
            "model_description": "Transformer-based model augmented with a relative graph position matrix to incorporate word-level graph structure into attention.",
            "performance_metric": "BLEU4, METEOR (as compared in Table II)",
            "performance_value": "WebNLG (reported in this paper): BLEU4=61.15, METEOR=43.38",
            "impact_on_training": "Captures token-level structural relations to improve generation over vanilla Transformer in prior work; in MGSA comparisons, MGSA outperforms Graformer on WebNLG.",
            "limitations": "Reported to produce lower scores than MGSA on WebNLG; MGSA authors speculate part of the gap is due to their choice of BART versus generic Transformer backbone used by Graformer.",
            "comparison_with_other": "MGSA outperforms Graformer on WebNLG (BLEU4 difference reported in paper), likely because MGSA combines both word- and entity-level structure rather than only word-level.",
            "uuid": "e7030.6",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "UniD2T (mention)",
            "name_full": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
            "brief_description": "A model aimed at unifying various structured inputs as graphs and using a structure-enhancement module to capture word-level structure information for data-to-text tasks.",
            "citation_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
            "mention_or_use": "mention",
            "representation_name": "Structure-enhanced word-level encoding",
            "representation_description": "Encodes structured inputs (treated as graphs) into PLMs via a structure-enhancement module focusing on word-level relations in inputs.",
            "representation_type": "sequential token-based + structure-enhancement module",
            "encoding_method": "treats structured data as graphs and applies module to enhance token embeddings with structure-aware signals",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (reported comparison)",
            "task_name": "data-to-text generation / KG-to-text generation",
            "model_name": "UniD2T",
            "model_description": "A pre-training approach for data-to-text that unifies multiple structured formats as graphs and uses a module to enrich token-level representations with structure.",
            "performance_metric": "BLEU4, METEOR, ROUGEL",
            "performance_value": "WebNLG (reported in this paper): BLEU4=60.41, METEOR=44.35",
            "impact_on_training": "Designed to be a general pre-training approach across structured data types; according to MGSA, UniD2T performed worse on WebNLG than MGSA's KG-specialized multi-granularity approach.",
            "limitations": "Less effective than MGSA on WebNLG in reported numbers; being general-purpose may reduce task-specific gains on KG-only tasks.",
            "comparison_with_other": "MGSA (task-specific, KG-only focus) outperformed UniD2T on WebNLG (MGSA BLEU4 66.45 vs UniD2T BLEU4 60.41).",
            "uuid": "e7030.7",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "KGPT: Knowledge-grounded pre-training for datato-text generation",
            "rating": 2,
            "sanitized_title": "kgpt_knowledgegrounded_pretraining_for_datatotext_generation"
        },
        {
            "paper_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
            "rating": 2,
            "sanitized_title": "unifying_structured_data_as_graph_for_datatotext_pretraining"
        },
        {
            "paper_title": "GAP: A graph-aware language model framework for knowledge graph-to-text generation",
            "rating": 2,
            "sanitized_title": "gap_a_graphaware_language_model_framework_for_knowledge_graphtotext_generation"
        },
        {
            "paper_title": "Modeling graph structure via relative position for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "modeling_graph_structure_via_relative_position_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "JointGT: Graph-text joint representation learning for text generation from knowledge graphs",
            "rating": 1,
            "sanitized_title": "jointgt_graphtext_joint_representation_learning_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism",
            "rating": 1,
            "sanitized_title": "grasame_injecting_tokenlevel_structural_information_to_pretrained_language_models_via_graphguided_selfattention_mechanism"
        }
    ],
    "cost": 0.017682,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation</p>
<p>Shanshan Wang 
Beijing Jiaotong University
BeijingChina</p>
<p>Chun Zhang chzhang1@bjtu.edu.cn 
Beijing Jiaotong University
BeijingChina</p>
<p>Ning Zhang nzhang1@bjtu.edu.cn 
Beijing Jiaotong University
BeijingChina</p>
<p>MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation
B104ABC81CDC757B32622429DAEBC75AKG-to-textMulti-granularityStructure attentionPLMs
The Knowledge Graph-to-Text Generation task aims to convert structured knowledge graphs into coherent and human-readable natural language text, a crucial step in making complex data accessible to non-expert users.Recent efforts in this field have focused on enhancing pre-trained language models (PLMs) by incorporating graph structure information to capture the intricate structure details of knowledge graphs.However, most of these approaches tend to capture only singlegranularity structure information, concentrating either on the relationships between entities within the original graph or on the relationships between words within the same entity or across different entities.This narrow focus results in a significant limitation: models that concentrate solely on entity-level structure fail to capture the nuanced semantic relationships between words, while those that focus only on word-level structure overlook the broader relationships between original entire entities.To overcome these limitations, this paper introduces the multigranularity graph structure attention (MGSA), which is based on PLMs.The encoder of the model architecture features an entity-level structure encoding module, a word-level structure encoding module, and an aggregation module that synthesizes information from both structure.This multi-granularity structure encoding approach allows the model to simultaneously capture both entity-level and word-level structure information, providing a more comprehensive understanding of the knowledge graph's structure information, thereby significantly improving the quality of the generated text.We conducted extensive evaluations of the MGSA model using two widely recognized KG-to-Text Generation benchmark datasets, WebNLG and EventNarrative, where it consistently outperformed models that rely solely on single-granularity structure information, demonstrating the effectiveness of our approach.</p>
<p>I. INTRODUCTION</p>
<p>Knowledge Graphs (KGs) [1]- [3] are structured data storage formats used to represent knowledge and information, with strong capabilities in data integration and information retrieval.Their graph-structured representation significantly enhances the reasoning capabilities between pieces of knowledge.However, the graph-structured nature of KGs differs greatly from natural language text, making it more difficult for humans to directly comprehend the information they contain.Therefore, the task of KG-to-text generation aims to transform structured KGs into human-readable natural language text, serving as a bridge between KGs and natural language.As illustrated in Fig. 1, a given KG and its corresponding natural language description are presented.The task of KG-to-text has a wide range of applications, such as question-answering systems [4], [5], dialogue generation or dialogue agents [6], and event narration [7], among others.</p>
<p>KG-to-text generation typically requires encoding the KG so that the model can understand the information it contains, thereby generating text that accurately describes the KG.Unlike AMR-to-Text Generation [8], which involves a more restrictive space where the graph follows predefined dense connection templates, the sparsity of KGs makes it difficult for typical text generation models to align the relationships between the KG and the target generated text.Given that PLMs have already learned rich linguistic knowledge, contextual information, and semantic relationships from large-scale pretraining corpora, recent works on KG-to-text generation [9]- [11] have achieved state-of-the-art (SOTA) results by leveraging PLMs.These models linearize the KG into a token sequence as input to the model, transforming the KG-to-text task into a sequence-to-sequence (seq2seq) task by fine-tuning PLMs or adding additional pre-training tasks.</p>
<p>Although fine-tuning PLMs has yielded encouraging results, several issues remain.First, linearizing the KG as input leads to the loss of graph structure information.Some works [10], [12]- [14] have attempted to add additional embedding information to the linearized sequence or introduce graphaware modules into the model's encoder to capture the graph structure.However, these methods either focus solely on entity-level or word-level structure, without simultaneously considering both granularities of structure information.Sec-arXiv:2409.10294v2[cs.CL] 23 Sep 2024 ond, when linearizing the KG, the order of the triples is randomly arranged based on the original dataset, without considering human language conventions.Our proposed model not only integrates both granularities of structure information but also clusters and arranges the linearized KG according to human language habits.</p>
<p>Our proposed MGSA model consists of three key modules: (1) Entity-level structure encoding module: A relative position matrix and an adjacency matrix are designed to capture entitylevel structure attention information.(2) Word-level structure encoding module: A word-level relative position matrix is designed to capture word-level structure information both within and between entities.(3) Aggregation module: The aggregation module integrates the outputs of the two granularity encoding modules, and its output is used as part of the input to the decoder for generating the target text.</p>
<p>We evaluated the MGSA model on two popular KG-to-text generation datasets, WebNLG [15] and EventNarrative [7], and it demonstrated better overall performance than the baseline models.Our main contributions are as follows:</p>
<p>• We propose a novel KG-to-text generation model based on multi-granularity graph structure attention.By integrating graph structure information at multiple granularities, the model fully understands the information contained in the KG, thereby generating higher-quality text.</p>
<p>• Based on our model, we further evaluated the effectiveness of the two granularity-level modules, providing additional evidence of the effectiveness of integrating multi-granularity structure information for encoding KG structure.</p>
<p>• Through case analysis, we explored the factors influencing the model's performance.In addition to intrinsic factors within the model, we discovered that external factors, such as the quality of the dataset, also impact the model's effectiveness.• Experiments conducted on two datasets achieved more competitive results compared to the baseline models.</p>
<p>II. RELATED WORK</p>
<p>A. KG-to-Text Generation KG-to-text generation is a subset of Data-to-Text generation [16] and a branch of Natural Language Generation (NLG) [17].Unlike traditional NLG tasks (seq2seq), where the input is a linear sequence, a KG is structured as a graph rather than a linear sequence.Therefore, models designed for conventional NLG tasks cannot be directly applied to KG-to-text generation.To address this challenge, recent work has primarily focused on two directions: one direction utilizes Graph Neural Networks (GNNs) [18]- [22], which encode node information in the graph through neighborhood aggregation and then decode the encoded results to generate the corresponding textual description.Alternatively, some approaches directly modify the encoder part of Transformer models [23], combining them with Graph Attention Networks (GATs) [24], enabling the model to directly compute self-attention information for all nodes in the graph.The other direction leverages the success of PLMs in NLG tasks, such as BART [25], T5 [26], and GPT [27].These methods fine-tune PLMs by linearizing the structured KG into a token sequence, using the linearized KG nodes as input to generate sentences, thus transforming the graph-to-text task into a seq2seq task.However, the drawback of this approach is that the original graph structure information is lost during the linearization process.</p>
<p>Although explicitly encoding graph structure information with GNNs has been shown to be effective, methods based on PLMs have demonstrated superior results compared to GNNs.Therefore, we also chose to explore the PLM-based approach for the KG-to-text generation task.</p>
<p>B. Structure Information for PLMs</p>
<p>Recent work has focused on injecting the structure information of KGs into PLMs.Studies such as [28] and [29] incorporate additional embeddings into the linearized sequence, primarily to capture token-level structure information in the KG.In [12] and [11], a relative distance matrix is designed based on the relative distances between words or entities, and this matrix is integrated into the attention mechanism of the encoder to capture word-level or entity-level structure information from the KG.Meanwhile, [10], [13], [30] design a graph structure-aware module within the encoder to capture entity-level structure information from the KG.Additionally, [31] and [32] combine GNNs with PLMs to align the entitylevel graph structure of KGs with the token-level semantic information in the linear sequence.</p>
<p>Overall, the aforementioned work either incorporates wordlevel structure information, entity-level structure information, or directly integrates entity-level structure information with the semantic information of linearized sequences.However, these approaches do not simultaneously consider the structure information between entities and the structure information between words within the same entity or across different entities in the KG.Inspired by [22] and [33], which explore multi-granularity structure information in KGs using GNNs, we also consider both entity-level and word-level structure information when incorporating graph structure into PLMs.Experimental results demonstrate the effectiveness of our model.</p>
<p>III. METHODOLOGY</p>
<p>A. Problem Definition and Model Architecture KG-to-text generation aims to convert a set of triples into natural language text.For a given KG G = {(h i , r ij , t j ) | h i , t j ∈ V, r ij ∈ R}, where V is the set of all nodes in the graph and R is the set of all relation labels in the graph, the triples are linearized into a token sequence G linear = (x 1 , x 2 , . . ., x n ) where x i represents the i-th token in the linear sequence and n represents the number of tokens in the sequence.The linearized sequence is input into the model to generate the corresponding text sequence T = (t 1 , t 2 , . . ., t k ), where t i represents the i-th token generated by the model and k represents the length of the text.The overall architecture of the model follows an encoderdecoder structure based on BART.As illustrated in Fig. 2, the encoder section is designed to fully capture the information embedded in the graph by utilizing two granularity-specific encoding modules: a word-level module and an entity-level module, along with an aggregation module.The entity-level module captures entity-level structure information using a relative position matrix between nodes in the graph, while the word-level module captures semantic information between words within the same node or across different nodes using a relative position matrix between words.The aggregation module is responsible for fusing the encoding vectors from both granularities.In the decoder section, the model adopts a standard Transformer [34] decoder architecture, where the output from the encoder serves as part of the hidden input to the decoder for generating the target text.</p>
<p>B. Entity-level Structure Encoding Module</p>
<p>This module encompasses the linearization of entity-level structure in the KG, the design of a relative position matrix, and the incorporation of this relative position matrix into the self-attention weight calculations to effectively capture the structure information.</p>
<p>1) Entity-level Linearization: For a given KG in the form of a set of triples, the linearization process follows the method in existing work [13].All triples are concatenated together, with special tokens <H>, <R>, and <T> used to denote the head, relation, and tail of each triple, respectively.</p>
<p>2) Linear Attention: The linearized sequence of the KG is then fed into the module, where it first undergoes self-attention computation.This process captures the global information of the linear sequence by calculating the global attention across all tokens in the sequence.Let the linearized sequence be denoted as X E , and the computation process of the linear self-attention is as follows:
X lin = σ QK ⊤ √ d V,(1)
where 3) Entity-level Graph Structure Attention: While linear attention primarily captures the global information of linear sequences, it is insufficient to obtain the structure information of entities and relations in the KG.To further capture the structure information between entities and relations, the vectors X lin ∈ R n×d obtained from linear attention are first transformed into entity and relation vectors X p ∈ R m×d through a pooling layer.
Q = X E W Q , K = X E W K , V = X E W V , and W Q , W K , W V are learnableX p = pooling(X lin )(2)
Here, n and m represent the number of tokens in the linear sequence and the number of entities and relations in the KG, respectively.The pooling operation employs mean pooling.</p>
<p>Transforming the KG into a bipartite graph, as illustrated in Fig. 3a.The nodes on the left side represent all entities from the original KG, while the nodes on the right side represent the relations.The relative position matrix R E is generated based on the connection types between nodes in the bipartite graph:
R E ij =      1, if i and j are neighboring entities, 2, if (i, j) is an (entity, edge) pair, 3, if (i, j) is an (edge, entity) pair,(3)
The connection types are defined based on the neighboring entities or relations in the original KG.By assigning different weights to different connection types, the relative position matrix R E is derived.Using this relative position matrix R E , the graph structure attention between entities is calculated:
X g = σ Q p K p⊤ √ d + A + γ R E V p ,(4)
Here Q p , K p , V p are constructed from X p by multiplying it with their corresponding learnable parameter W Q p , W K p , Fig. 3. Entity-level linearization and word-level linearization.</p>
<p>W V p .A ∈ R m×m is the adjacency matrix corresponding to the bipartite graph, and γ(•) maps the relative distances between entities or relations to a vector space.Finally, the entitylevel structure attention representation is combined with the linear attention representation through a residual connection, producing the output of the entity-level structure encoding module.</p>
<p>X E = gather(X g ) + X lin (5) Here, the function gather(•) remaps the m-dimensional node representations back to the n-dimensional token representation space.</p>
<p>C. Word-level Structure Encoding Module 1) Word-level Linearization: An entity or relation is usually composed of several words.In word-level linearization, these entities or relations are split into individual words, with each word marked by a special token [N].The words are then concatenated in the order of the original set of triples.</p>
<p>2) Word-level Graph Structure Attention: In the entitylevel bipartite graph, each node consists of several words.Based on this, the graph is transformed into a word-level graph, as illustrated in Fig. 3b, where each node from the original bipartite graph is split into multiple nodes, with each node containing only one word.When calculating word-level structure attention, the word-level graph is used to generate the word-level relative position matrix R N based on the shortest path distances between words:
R N ij =          ∞, if δ(n i , n j ) = ∞ and δ(n j , n i ) = ∞, encode(p), if (n i , n j ) ∈ SAME p , δ(n i , n j ), if δ(n i , n j ) ≤ δ(n j , n i ), −δ(n j , n i ), if δ(n i , n j ) &gt; δ(n j , n i ),(6)
Here, δ(n i , n j ) represents the shortest path distance from word node n i to word node n j .If n i to n j are unreachable from each other, their shortest path distance is set to ∞.The term SAME p indicates words that belong to the same entity.The function encode(•) maps the distance between words from the same entity to a value outside the range of the δ(•) function.The encoding function is defined as encode(p) := sgn(p) • δ max + p, where δ max is the diameter of the graph, representing the maximum shortest path length between two words.Let the linearized sequence be denoted as X N , the word-level graph structure attention is calculated as follows:
X W = σ Q N K N ⊤ √ d + γ R N V N ,(7)
Here, Q N , K N , V N are constructed from X N by multiplying it with their weight matrices.</p>
<p>D. Aggregation Module</p>
<p>After encoding through the two granularity-specific module, we obtain the entity-level and word-level graph structure encoding vectors, which are then concatenated:
c = X E ∥ λX W (8)
Here, λ is a hyperparameter that controls the contribution of the word-level structure encoding vector.For the vector c, the overall attention is calculated to fuse the graph structure information at both granularities as follows:
X c = σ Q c K c⊤ √ d V c ,(9)
Here, Q c , K c , V c are constructed from c by multiplying it with their weight matrices.The representation is then enhanced through a residual connection, followed by a two-layer feedforward network (FF) to adjust the dimensionality of the vector representation.Finally, the output representation O, which fuses the information from both granularities, is obtained through another residual connection and layer normalization (LN) as follows:
O = LN (F F (X c + c) + X c )(10)</p>
<p>E. Loss Function</p>
<p>The model's decoder adopts the standard Transformer decoder structure.For a given KG G, the loss for text generation is calculated using the negative log-likelihood (NLL) as follows:
L = − k i=1 log p (t i | t 1 , . . . , t i−1 ; G)(11)
Here, p represents the generation probability of each token.</p>
<p>IV. EXPERIMENTS A. Experiment Setup 1) Dataset: The experiments were conducted on two datasets designed for the KG-to-text generation task: WebNLG v2.0 and EventNarrative.WebNLG consists of crowdsourced RDF data manually created by human annotators.Each example in the dataset contains up to seven triples and one or more reference texts.For comparison with previous work, version 2.0 was used in our experimental analysis.EventNarrative extracts events from EventKG [35] and enriches each event with additional data from Wikidata [3], including related attributes and objects.Detailed textual descriptions related to these events are then obtained from Wikipedia.This process ensures a close correspondence between the KG and the text, resulting in a large-scale, high-quality parallel dataset.Due to computational resource limitations, ablation studies and further analysis experiments were only conducted on the WebNLG dataset, while the optimal model obtained from WebNLG was validated and tested on the EventNarrative dataset.Table I presents the official train/validation/test splits for both datasets.2) Data Processing: Before linearizing the KG, we clustered the set of triples based on their head entities.This ensures that information related to the same entity is not only adjacent in the graph structure but also in the linear sequence, thus preserving the completeness and coherence of the generated text in describing the entity.This approach aligns with human language habits-when we speak, we tend to focus on one topic at a time and move on to the next only after finishing the current one, rather than interweaving multiple topics in a disorganized manner.</p>
<p>3) Parameter Settings: In the experiments, we used the pre-trained bart-base checkpoint from Hugging Face to initialize the parameters of the entity-level module and wordlevel module.When training on the WebNLG dataset, we set the number of epochs to 40, the batch size to 16, the learning rate to 2e-5, the number of warm-up steps to 1600, the beam width to 5, and chose Adam as the optimizer with ϵ set to 1e-8.The maximum input length for the linear sequence was set to 256, and the maximum generation length for the text sequence was set to 128.The hyperparameter λ, which controls the word-level graph structure vectors, was set to 0.5.During evaluation, we followed existing KG-to-text work and used BLEU4 [36], METEOR [37], and ROUGEL [38] scores as evaluation metrics to analyze the model's performance.</p>
<p>4) Baseline Selection: When analyzing the experimental results, we selected KG-to-text generation models that linearize KGs on the same dataset for a fair comparison, including KGPT [28], JointGT [13], Graformer [12], GAP [10], and UniD2T [14].The KGPT model injects multi-level structure information into the linearized sequence using three special embeddings.The JointGT and GAP models design a structure-aware semantic aggregation module and a graphaware attention module, respectively, to capture entity-level structure information.For the JointGT model, we chose the BART-based pre-trained model consistent with this experiment.The Graformer model designs a relative graph position matrix based on the Transformer model to capture word-level structure information in the KG.The UniD2T model, targeting various structured data-to-text generation tasks, captures word-level structure information in the input graph through a designed structure-enhancement module.</p>
<p>B. Main Results</p>
<p>Table II and III present the experimental results of the proposed model on the WebNLG and EventNarrative datasets, along with the corresponding experimental data from baseline models.To ensure objectivity and fairness in comparison, all data are derived from the experimental results reported in the original papers.SOTA-NPT [39]   For the WebNLG dataset, our model outperforms the SOTA-NPT model, which has no pre-training, by 5.45%, 4.93%, and 5.47% in BLEU4, METEOR, and ROUGEL metrics, respectively.Additionally, Compared to the BART-base and T5-base models, it achieves approximately 2% and 1.5% improvements in BLEU4 and ROUGEL metrics, respectively, demonstrating that the incorporation of graph structure attention matrices enhances the performance of the KG-totext generation task.Compared to the KGPT model, which adds multi-level structure embeddings to the linearized sequence, our model achieves 2.34% and 1.90% improvements in BLEU4 and ROUGEL metrics, respectively.This indicates that our approach, which integrates multi-granularity graph structure attention, is more effective than adding multi-level embedding information.In the BLEU4 metric, our model surpasses models with single-granularity structure-aware modules such as JointGT, Graformer, GAP, and UniD2T by 0.53%, 5.30%, 0.25%, and 6.04%, respectively.This suggests that our multi-granularity graph structure attention matrices can more comprehensively capture both entity-level structure information and word-level semantic information, leading to a deeper understanding of the KG and the generation of higher-quality text.The significant improvement over Graformer may be attributed to our choice of the BART model, which enhances the KG-to-text generation task more effectively than the Transformer model.As for UniD2T, we speculate that our model's focus solely on graph-structured data-to-text generation tasks, without involving other structured data, contributes to its superior performance.</p>
<p>For the EventNarrative dataset, our model achieves the highest scores in BLEU4 and METEOR metrics, outperforming the second-best GAP model by 0.14% and 0.11%, respectively, and the second-highest score in the ROUGEL metric.Compared to the BART-base model, our model shows improvements of 3.84%, 0.93%, and 1.81% across the three evaluation metrics, further confirming that the integration of multi-granularity graph structure attention can achieve excellent performance even with more complex KGs.</p>
<p>Overall, built on top of BART, our model integrates multigranularity graph structure attention, effectively captures the structure information of the graph.This enables the model to extract information from the given KG at multiple granularities, resulting in the generation of higher-quality text.</p>
<p>C. Ablation Study</p>
<p>Due to the limitations of experimental resources, we conducted ablation studies only on the WebNLG dataset.Through experiments with single-granularity structure attention, we analyzed the impact of the entity-level structure encoding module and the word-level structure encoding module on the model's performance.1) Entity-level: In analyzing the impact of entity-level structure encoding on model performance, we conducted experiments on models where the entity-level structure encoding module was removed, the relative position encoding matrix was removed, the adjacency matrix was removed, and both the relative position encoding and adjacency matrices were removed.The experimental results are shown in Table IV.</p>
<p>From the results, it can be observed that both types of encoding contribute to improving the model's performance.Furthermore, the relative position matrix has a greater impact on the model's effectiveness than the adjacency matrix, with the model using only the relative position matrix outperforming the one using only the adjacency matrix by 0.52%.</p>
<p>2) Word-level: As shown in Table V, the difference between removing the word-level structure encoding module and removing only the word-level relative position matrix is not significant.However, when the relative position matrix</p>
<p>D. Impact of λ values</p>
<p>Additionally, we explored the impact of the λ value in the aggregation module on the model's performance.The Fig. 4 shows the trend of experimental results with λ values set to 0, 0.25, 0.50, 0.75, and 1.When λ = 0, it is equivalent to removing the word-level module.From λ = 0 to λ = 0.5, the experimental results show an upward trend, reaching the maximum at λ = 0.5.From λ = 0.5 to λ = 1, the experimental results gradually decline.Although the optimal model performance may not be achieved at λ = 0.5, due to resource limitations, we selected λ = 0.5 as the optimal value in our experiments.Overall, the experimental results indicate that the value of λ also affects the model's performance.</p>
<p>E. Case Study</p>
<p>To demonstrate the effectiveness of the proposed method, we analyzed the generated texts on the WebNLG and Event-Narrative datasets in Table VI.Two examples were provided for each dataset: one with a small-scale KGs (containing 1-4 triples) and one with a larger-scale KGs (containing more than 4 triples).As shown in the examples below, for smallscale KGs, the generated texts are almost identical to the reference texts in content and successfully convey the semantic knowledge contained in the set of triples.This explains why The 9th 2019 World Para Athletics Championships, the ninth edition of the para athletics World Championships, was held in Dubai, United Arab Emirates from 9 to 15 November 2019.Dubai was selected as the host city by the International Paralympic Committee (icc) on 9 August 2016.The 2019 championships was the first time that Dubai hosted the World Championships.The athletics federation of africa announced dubai as the venue for the event, and Dubai became the first city to bid for the championships in the same year.The event was scheduled to be held at the olympic the earlier experimental results yielded higher scores on the WebNLG dataset, which contains more small-scale KGs.However, for larger-scale KGs, the generated texts start to exhibit issues such as missing content, hallucinations, and redundant expressions.These issues are less apparent on the WebNLG dataset.For instance, when the number of triples reaches 7, there is only one case of missing content and hallucination, with the rest of the generated content almost semantically aligned with the reference text.However, the performance on the EventNarrative dataset is significantly worse; although only one triple's content is missing, the generated text almost fails to faithfully represent the given KG, and the reference text itself also fails to fully align with the given set of triples.From this analysis, we can conclude that the model's performance is influenced not only by the scale of the KG but also by the quality of the data.</p>
<p>V. CONCLUSION</p>
<p>This paper proposes a KG-to-text generation model based on multi-granularity graph structure attention, which simultaneously considers both entity-level and word-level structure information in the knowledge graph.The model primarily consists of three modules: an entity-level structure encoding module, a word-level structure encoding module, and an aggregation module that integrates structure encoding information from both granularities.Experimental evaluations conducted on two KG-to-text benchmark datasets demonstrate that the proposed model consistently outperforms baseline models (which utilize single-granularity structure).An analysis of the factors influencing the model further explains its effectiveness in generating text from knowledge graph.</p>
<p>Fig. 1 .
1
Fig. 1.The knowledge graph and its corresponding description text, with italic bold indicating relationship labels and colored highlights marking entities.</p>
<p>Fig. 2 .
2
Fig. 2. Overall model architecture.The input consists of two levels of granularity in the linearization process.The encoder module incorporates structure attention at both granularities, while the decoder module follows the standard Transformer decoder structure.</p>
<p>weight parameters.d is the dimensionality of the vectors.σ(•) denotes the softmax function.</p>
<p>Fig. 4 .
4
Fig. 4. Impact of λ values.</p>
<p>TABLE I DATASET
I
SPLITS FOR WEBNLG AND EVENTNARRATIVE.
DatasetTrainValidTestWebNLG34,3524,3164,224EventNarrative179,5431,00022,441</p>
<p>TABLE II PERFORMANCE
II
COMPARISON ON WEBNLG.
Model#PPre+BLEU4METEORROUGELSOTA-NPT-No61.0042.0071.00BART-base140MYes64.5546.5175.13T5-base220MYes64.4246.5874.77KGPT177MYes64.1146.3074.57JointGT160MYes65.9247.1576.10Graformer--61.1543.38-GAP153MNo66.2046.7776.36UniD2T-Yes60.4144.35-MGSA (ours)167MNo66.4546.9376.47</p>
<p>denotes the state-of-the-art (SOTA) model without any pre-training.BART-base and T5base are the unmodified base models.#P indicates the number of model parameters, and Pre+ denotes whether the model underwent additional pre-training.Bolded values represent the highest scores under the current evaluation metrics, while underlined values indicate the second-highest scores.</p>
<p>TABLE III PERFORMANCE
III
COMPARISON ON EVENTNARRATIVE.
ModelBLEU4METEORROUGELBART-base31.3826.6862.65T5-base12.8022.7752.06JointGT31.1926.5864.91GAP35.0827.5064.28MGSA (ours)35.2227.6164.46</p>
<p>TABLE IV EXPERIMENTAL
IVE/MAR EBLEU4METEORROUGELw/o--62.3144.5373.11w/✓×64.8645.3775.10w/×✓65.3446.4175.85w/××64.5345.0874.77w/ (MGSA)✓✓66.4546.9376.47
RESULTS WITH(W/) OR WITHOUT(W/O) ENTITY-LEVEL ENCODING MODULE(E/M).</p>
<p>is added, the model's performance improves by nearly 1% compared to the model without the relative position matrix.Additionally, by comparing the data in Table V with that in TableIV, we find that removing the entity-level module has a greater impact on the model's performance than removing the word-level module, indicating that the entity-level module contributes more to the model's overall improvement.
TABLE VEXPERIMENTAL RESULTS WITH OR WITHOUT WORD-LEVEL ENCODINGMODULE(W/M).W/MR WBLEU4METEORROUGELw/o-65.5846.3875.27w/×65.7746.5975.63w/ (MGSA)✓66.4546.9376.47</p>
<p>TABLE VI ANALYSIS
VI
OF GENERATION PERFORMANCE ON WEBNLG AND EVENTNARRATIVE DATASETS WITH DIFFERENT KNOWLEDGE GRAPH SCALES.The 2019 World Para Athletics Championships was a para athletics track and field event organised by the World Para Athletics subcommittee of the International Paralympic Committee.It was held in Dubai, United Arab Emirates from 7 to 15 November 2019.In July 2017 during 2017 World Para Athletics Championships there were reports and speculation that London could once again hold the games in 2019 due to the success of the 2017 event and the 2012 Summer Paralympics.As of day 9, Friday 15 November 2019.
DatasetKnowledge Graph TriplesReference TextGenerated TextWebNLG(Acharya Institute of Technology, affiliation, Visves-The Acharya Institute of Tech-The Acharya Institute of Technology(4 and 7 triples)varaya Technological University)nology 's campus is located inis located in Soldevanahalli, Acharya(Acharya Institute of Technology, country, India)Soldevanahalli, Acharya Dr. Sar-Dr. Sarvapalli Radhakrishnan Road,(Acharya Institute of Technology, campus, In Sol-vapalli Radhakrishnan Road, Hes-Hessarghatta Main Road, Bangalore -devanahalli, Acharya Dr. Sarvapalli Radhakrish-sarghatta Main Road, Bangalore560090, India. It is affiliated with thenan Road, Hessarghatta Main Road, Bangalore --560090, India. It is affiliatedVisvesvaraya Technological University560090.)with the Visvesvaraya Technolog-in Belgaum.(Visvesvaraya Technological University, city, Bel-ical University in Belgaum.gaum)(School of Business and Social Sciences at theThe School of Business and So-The School of Business and Social Sci-Aarhus University, city, Aarhus)cial Sciences at the Aarhus Univer-ences at the Aarhus University in Aarhus,(School of Business and Social Sciences at thesity is located in Aarhus, DenmarkDenmark was established in 1928. It is af-Aarhus University, affiliation, European Universityand it was established in 1928. Itsfiliated with the European University As-Association)dean is Thomas Pallesen and it hassociation and its dean is Thomas Palle-(School of Business and Social Sciences at the16,000 students. Its Latin name issen. There are 16000 students and 16000Aarhus University, dean, Thomas Pallesen)"Universitas Aarhusiensis". It is af-Latin speaking people.(School of Business and Social Sciences at thefiliated to the European UniversityAarhus University, number of students, 16000)Association.(School of Business and Social Sciences at theAarhus University, country, Denmark)(School of Business and Social Sciences atthe Aarhus University, latin name, UniversitasAarhusiensis)(School of Business and Social Sciences at theAarhus University, established, 1928)EventNarrative(2016 Tianjin Health Industry Park, sport, tennis)The 2016 Tianjin Health Indus-The 2016 Tianjin Health Industry Park(3 and 8 triples)(2016 Tianjin Health Industry Park, location, Tianjin)try Park was a professional tenniswas a professional tennis tournament(2016 Tianjin Health Industry Park, point in time,tournament played on outdoor hardplayed on outdoor hard courts. It took2016)courts. It took place in Tianjin,place in Tianjin, China, on 2016 -2016.China, on 2016-2016.(2019 World Para Athletics Championships, location,Dubai)(2019 World Para Athletics Championships, orga-nizer, International Paralympic Committee)(2019 World Para Athletics Championships, sport,para athletics)(2019 World Para Athletics Championships, editionnumber, 9)(2019 World Para Athletics Championships, country,United Arab Emirates)(2019 World Para Athletics Championships, point intime, 2019)(2019 World Para Athletics Championships, endtime, 15 November 2019)(2019 World Para Athletics Championships, follows,2017 World Para Athletics Championships)</p>
<p>Dbpedia: A nucleus for a web of open data. Sören Auer, 2007SpringerBerlin, Heidelberg; Berlin Heidelberginternational semantic web conference</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Proceedings of the 2008 ACM SIGMOD international conference on Management of data. the 2008 ACM SIGMOD international conference on Management of data2008</p>
<p>Wikidata: a free collaborative knowledgebase. Denny Vrandečić, Markus Krötzsch, Communications of the ACM. 572014</p>
<p>Retrieve-rewrite-answer: A kg-to-text enhanced llms framework for knowledge graph question answering. Yike Wu, arXiv:2309.112062023arXiv preprint</p>
<p>Chatkbqa: A generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models. Haoran Luo, arXiv:2310.089752023arXiv preprint</p>
<p>Knowledge Graph for NLG in the context of conversational agents. Hussam Ghanem, Massinissa Atmani, Christophe Cruz, arXiv:2307.015482023arXiv preprint</p>
<p>EventNarrative: A large-scale event-centric dataset for knowledge graph-to-text generation. Anthony Colas, arXiv:2111.002762021arXiv preprint</p>
<p>Amr-to-text generation with cache transition systems. Lisa Jin, Daniel Gildea, arXiv:1912.016822019arXiv preprint</p>
<p>Investigating pretrained language models for graph-to-text generation. Leonardo Fr Ribeiro, arXiv:2007.084262020arXiv preprint</p>
<p>GAP: A graph-aware language model framework for knowledge graph-to-text generation. Anthony Colas, Mehrdad Alvandipour, Daisy Zhe Wang, arXiv:2204.066742022arXiv preprint</p>
<p>Structure-aware Knowledge Graph-to-text Generation with Planning Selection and Similarity Distinction. Feng Zhao, Hongzhi Zou, Cheng Yan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Modeling graph structure via relative position for text generation from knowledge graphs. Martin Schmitt, arXiv:2006.092422020arXiv preprint</p>
<p>Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. Pei Ke, arXiv:2106.105022021arXiv preprint</p>
<p>Unifying Structured Data as Graph for Data-to-Text Pre-Training. Shujie Li, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Creating training corpora for nlg microplanning. Claire Gardent, 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics2017. 2017</p>
<p>A Systematic Review of Data-to-Text NLG. Chinonso Osuji, Thiago Cynthia, Brian Castro Ferreira, Davis, arXiv:2402.084962024arXiv preprint</p>
<p>Pre-trained language models for text generation: A survey. Junyi Li, ACM Computing Surveys. 562024</p>
<p>Semi-supervised classification with graph convolutional networks. Thomas N Kipf, Max Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Zhijiang Guo, Transactions of the Association for Computational Linguistics. 72019</p>
<p>Modeling global and local node contexts for text generation from knowledge graphs. Leonardo Fr Ribeiro, Transactions of the Association for Computational Linguistics. 82020</p>
<p>A comprehensive survey of graph neural networks for knowledge graphs. Zi Ye, IEEE Access. 102022</p>
<p>Boosting KG-to-Text Generation via Multi-granularity Graph Representations. Tianyu Yang, Yuxiang Zhang, Tao Jiang, 2022 International Joint Conference on Neural Networks (IJCNN). IEEE2022</p>
<p>Text generation from knowledge graphs with graph transformers. Rik Koncel-Kedziorski, arXiv:1904.023422019arXiv preprint</p>
<p>Graph attention networks. Petar Veličković, arXiv:1710.109032017arXiv preprint</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, arXiv:1910.134612019arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Journal of machine learning research. 211402020</p>
<p>Language models are unsupervised multitask learners. Alec Radford, OpenAI blog. 192019</p>
<p>KGPT: Knowledge-grounded pre-training for datato-text generation. Wenhu Chen, arXiv:2010.023072020arXiv preprint</p>
<p>Graph-to-Text Generation Combining Directed and Undirected Structural Information in Knowledge Graphs. Hongda Gong, Shimin Shan, Hongkui Wei, 2023 5th International Conference on Natural Language Processing (ICNLP). IEEE2023</p>
<p>Improving plms for graph-to-text generation by relational orientation attention. Tao Wang, Neural Processing Letters. 552023</p>
<p>Exploring the Synergy of Dual-path Encoder and Alignment Module for Better Graph-to-Text Generation. Tianxin Zhao, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)2024</p>
<p>GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism. Shuzhou Yuan, Michael Färber, arXiv:2404.069112024arXiv preprint</p>
<p>Enriched entity representation of knowledge graph for text generation. Kaile Shi, Complex &amp; Intelligent Systems. 92023</p>
<p>Attention is all you need. A Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Eventkg: A multilingual event-centric temporal knowledge graph. Simon Gottschalk, Elena Demidova, The Semantic Web: 15th International Conference, ESWC 2018. Heraklion, Crete, GreeceSpringer International PublishingJune 3-7, 2018. 201815</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Meteor universal: Language specific translation evaluation for any target language. Michael Denkowski, Alon Lavie, Proceedings of the ninth workshop on statistical machine translation. the ninth workshop on statistical machine translation2014</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, 2004Text summarization branches out</p>
<p>Handling rare items in datato-text generation. Anastasia Shimorina, Claire Gardent, Proceedings of the 11th international conference on natural language generation. the 11th international conference on natural language generation2018</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-530 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-530</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-530</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-6536f36648d39f0f9f6105562f76704fcc0b19e8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6536f36648d39f0f9f6105562f76704fcc0b19e8" target="_blank">A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> A persistent spatial semantic representation method is proposed, and it is shown how it enables building an agent that performs hierarchical reasoning to effectively execute long-term tasks.</p>
                <p><strong>Paper Abstract:</strong> Natural language provides an accessible and expressive interface to specify long-term tasks for robotic agents. However, non-experts are likely to specify such tasks with high-level instructions, which abstract over specific robot actions through several layers of abstraction. We propose that key to bridging this gap between language and robot actions over long execution horizons are persistent representations. We propose a persistent spatial semantic representation method, and show how it enables building an agent that performs hierarchical reasoning to effectively execute long-term tasks. We evaluate our approach on the ALFRED benchmark and achieve state-of-the-art results, despite completely avoiding the commonly used step-by-step instructions.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e530.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e530.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (HLSM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (fine-tuned CLS embedding used in HLSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-trained bidirectional Transformer (BERT) is fine-tuned and used to encode the high-level natural language instruction into a fixed task embedding (CLS token) phi^L that conditions subgoal prediction and mask refinement, providing procedural priors about required subgoal sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained Transformer encoder (BERT) whose CLS token embedding is fine-tuned end-to-end in HLSM to produce a task embedding (phi^L) used by the high-level controller and by the mask refiner; no architectural changes beyond fine-tuning are described.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED (language-conditioned embodied instruction following)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>High-level natural language instructions specifying household tasks in simulated indoor environments (AI2Thor/ALFRED). The agent receives first-person RGB, pose and inventory, and must output navigation and interaction actions (MoveAhead, RotateLeft/Right, Pickup/Put/Toggle/Open/Close/Slice with masks) to satisfy goal conditions by the episode end.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following; multi-step planning; navigation + object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (procedural: subgoal sequencing; object-relational: argument class grounding), with influence on spatial grounding via conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora (BERT) and fine-tuning on ALFRED language-to-subgoal supervised data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning (supervised) to produce phi^L; embedding is used as conditioning input to downstream modules (REFINER, subgoal predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit vector embedding (CLS) that encodes procedural expectations (typical subgoal sequences) and semantics of task instructions; used to condition spatial grounding modules</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task Success Rate (SR) and Goal Condition rate (GC) of HLSM when BERT is present vs. ablated</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Full HLSM (with BERT) validation: Seen SR 29.6% / GC 38.8%, Unseen SR 18.3% / GC 31.2%; Ablation without language encoder (i.e., removing BERT): validation Seen SR 0.9% / GC 8.6%, Unseen SR 0.2% / GC 7.5% (huge degradation). Test unseen HLSM SR 20.27% (full model).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When present, the language embedding enables correct sequencing of subgoals (procedural knowledge) and conditions spatial grounding networks to select appropriate object instances (via phi^L conditioning of REFINER/LingUNet). It supports choosing appropriate argument classes and disambiguating actions that require specific object types.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Language embedding alone cannot recover from perceptual failures or poor spatial maps; when segmentation/generalization is poor, language-conditioned predictions can still be mis-grounded. Also, language-only predictions (no state) sometimes propose subgoals that cannot be executed due to absent objects or unobserved state.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Ablation w/o language encoder: SR drops to 0.9%/0.2% (val seen/unseen) vs. full HLSM 29.6%/18.3%. Compared against HiTUT G-only (flat transformer baseline) HLSM outperforms (test unseen HLSM 20.27% vs HiTUT G-only 10.23%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing the language encoder essentially eliminates task success (validation SR ~1% seen, ~0.2% unseen). This shows procedural knowledge encoded in BERT embeddings is critical for mapping high-level instructions to subgoal sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pre-trained language models (BERT) fine-tuned as a task embedding effectively encode procedural expectations (subgoal sequences) that are necessary for high-level planning; removing the language encoder collapses performance, showing that procedural knowledge is primarily provided by the language model rather than being recoverable from state alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e530.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e530.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π^H (High-level controller)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HLSM High-level Controller (π^H)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autoregressive high-level policy that consumes the language embedding (phi^L), a pooled encoding of the persistent semantic 3D map, and prior subgoal history to predict the next subgoal tuple (type, argument class, 3D argument mask).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>HLSM π^H (uses BERT + Transformer components)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A learned module combining a fine-tuned BERT embedding for language, a max-pooled encoding of the semantic voxel map & inventory, and a Transformer autoregressive encoder over past subgoals; outputs distributions over subgoal type and argument class, and conditions a mask refiner for spatial grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED (hierarchical subgoal planning within HLSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given the current persistent world representation and instruction, π^H predicts the next abstract subgoal (e.g., Pickup(CD), Open(SAFE)) including an argument mask indicating where the object instance is or should be.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>hierarchical planning; procedural sequencing; object-grounded instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + spatial + object-relational (procedural: subgoal sequence prediction; spatial/object-relational: outputs argument class and conditions spatial mask generation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised fine-tuning on ALFRED subgoal dataset (language paired with subgoal sequences) and state representations</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised learning to predict next subgoal given phi^L, encoded state, and subgoal history; sampling at inference to allow retries/backtracking</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit subgoal tuples (type, arg^C, arg^M) where arg^M is produced by a downstream refiner; procedural knowledge is encoded in the learned mapping from language+state to distributions over subgoal types and argument classes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task SR/GC when π^H is used in the HLSM pipeline and in encoder ablations (w/o subgoal history, w/o state repr encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Full HLSM validation SR 29.6%/18.3% (seen/unseen). Ablation w/o subgoal history encoder: negligible change (29.4% seen / 16.6% unseen). Ablation w/o state representation encoder when predicting type & arg class: mixed results (30.0% seen / 18.9% unseen).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>π^H successfully decomposes high-level instructions into plausible subgoal sequences (e.g., pick up object, open receptacle, put object) and chooses argument classes consistent with common-sense procedural knowledge encoded from language supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Errors occur when perceptual/state inputs are noisy (mis-segmentation leads to wrong arg^M) or when subgoal predictions assume presence of objects not observed; π^H sometimes predicts wrong argument classes (e.g., CD vs EGG) when perception is ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to HiTUT (flat transformer) HLSM (with spatial map) achieves higher SR in high-level-only instruction setting (test unseen HLSM 20.27% vs HiTUT G-only 10.23%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing subgoal history encoding had little effect, indicating index-position encoding suffices in many cases; removing state representation when predicting type/arg class gives mixed effects, implying procedural prediction can sometimes be done from language alone while grounding (arg mask) requires state.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The high-level language-conditioned controller learns procedural knowledge (subgoal sequencing) from language supervision and uses the persistent spatial map primarily to ground 'where' subgoals should apply; predicting 'what' can sometimes rely mostly on language, but grounding requires spatial memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e530.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e530.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic 3D Voxel Map (V^S)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Persistent Semantic 3D Voxel Map (V_t^S) with Observability & Affordances</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense persistent 3D voxel representation that stores per-voxel class probability distributions and an observability map, plus derived top-down affordance features used by both high- and low-level controllers to reason about object locations and navigable space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>HLSM (semantic voxel memory component)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An explicit spatial memory: V_t^S ∈ [0,1]^{X×Y×Z×C} encodes class distributions per voxel, V_t^O indicates observed voxels, inventory vector v_t^S tracks held object classes, and AFFORD(ĥs_t) produces top-down affordance channels (pickable, receptacle, togglable, openable, ground, obstacle, observed). The map is constructed by projecting per-frame segmentation+depth and accumulated over time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED (spatial memory for long-horizon planning and grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Constructed online from RGB (segmentation+depth), the voxel map provides a persistent spatial-semantic memory enabling navigation planning, selection of exploration frontiers, object instance localization outside of view, and affordance-based interaction pose prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial mapping; long-horizon memory for navigation & manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (explicit spatial locations and class distributions; supports relational reasoning like object in receptacle/state)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>constructed online from predicted segmentation and depth (U-Net) projected via pinhole model and accumulated (no external knowledge base)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>constructed via observation model F(ĥs_{t-1}, o_t, g_k) using per-frame segmentation and depth predictions, projection into 3D voxels, and accumulation (element-wise max/update), plus derived affordances</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit dense spatial tensor (voxel grid) with per-voxel semantic distributions and observability flags; compact top-down affordance channels and max-pooled summaries used by language-conditioned planners</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>downstream task SR/GC; ablations with sensory oracles (gt depth/gt seg) measure importance of accurate spatial memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using ground-truth segmentation alone (+ gt seg) improves validation SR by +6.6% seen / +16.4% unseen; using both gt depth and gt seg yields +11.1% seen / +21.9% unseen (validation), indicating the spatial map's perceptual quality strongly affects performance. Full HLSM validation: Seen SR 29.6% / Unseen SR 18.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables selecting subgoals using objects outside current view, choosing exploration targets, and producing affordance-informed interaction poses; persistent memory supports multi-step tasks requiring remembered object locations (e.g., put object into safe found earlier).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Perception/generalization errors (noisy segmentation) corrupt the map and lead to wrong obstacles/blocked paths or incorrect instance grounding (picking wrong object). Memory scale/memory resolution limit scalability; pose/localization uncertainty not modeled and can degrade close-range mask refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Ablations with sensory oracles: +gt seg and +gt depth significantly improve SR (see Performance Result); compared to non-map approaches (HiTUT) HLSM with map outperforms in high-level-instruction-only setting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Perception oracles (+gt seg, +gt depth) substantially increase performance, showing the spatial map is a critical component and its quality is a primary bottleneck for generalization. Removing state encoding in π^H had mixed effects indicating map noise in unseen scenes can hurt high-level planning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit persistent spatial-semantic memory is effective to ground language-predicted procedures into spatial targets, to support selection of exploration and interaction poses, and to enable long-horizon multi-step tasks; however, its utility is limited by perceptual generalization and memory scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e530.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e530.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REFINER / LingUNet grounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REFINER (LingUNet-based mask refiner conditioned on language and map)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-conditioned image-to-image encoder-decoder (LingUNet) that refines a class-filtered voxel candidate map into a 3D argument-instance mask arg^M, using the language embedding phi^L and egocentric/top-down transforms to produce instance-level spatial grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mapping instructions to actions in 3D environments with visual goal prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LingUNet-based REFINER (conditioned on phi^L)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A LingUNet-style conditional convolutional encoder-decoder that takes egocentric affordance+history+class-filtered voxel input and the language embedding phi^L to produce a refined 3D voxel mask for the predicted argument instance; uses AlloTransform/ EgoTransform to convert between reference frames.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED (object instance grounding / mask prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given predicted argument class and the persistent semantic map (or an empty vector if unobserved) plus language embedding, REFINER produces a 3D soft mask identifying the likely instance of the object to act on; projected to first-person to form interaction masks for manipulation actions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object grounding; spatial grounding; manipulation targeting</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (maps class to instance location, conditioned on language semantics and required action type/affordance)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>trained supervised on ALFRED subgoal mask labels; conditions on language embedding (phi^L) and accumulated voxel map + affordances</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised learning (LingUNet) to map conditioned top-down/egocentric features and phi^L to 3D argument masks; used online during execution to propose where to act</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit 3D soft mask over voxels (arg^M) representing belief about the object instance location; produced by a learned model conditioned on language and spatial memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success of downstream interaction actions and overall task SR; failure modes shown qualitatively (wrong object picked when mask incorrect)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When REFINER + map are used within HLSM full system achieves validation SR 29.6%/18.3% (seen/unseen). Perception errors that corrupt inputs to REFINER produce notable failures (e.g., wrong instance masks leading to wrong pickups); no isolated mask-accuracy numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>REFINER effectively leverages language conditioning to disambiguate which instance of a class is relevant for the task (e.g., which CD to pick), and to produce masks even for objects outside current view if they are in the map.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When map or segmentation is noisy, REFINER can produce incorrect masks (pick wrong object) or fail to identify unobserved instances (all-zero mask), leading to subgoal failure; failure compounded by localization/pose noise which the system assumes away.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Using perfect segmentation/depth improves downstream performance substantially (+gt seg, +gt depth ablations), indicating REFINER's outputs are sensitive to perceptual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>If arg^M is all zero (object unobserved), the high-level controller treats object as unobserved and low-level exploration is triggered; no direct ablation of REFINER alone reported, but system-level perception oracles show its dependence on good segmentation/depth.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Conditioning spatial grounding networks on language embeddings (phi^L) enables mapping abstract instructions to precise spatial instance masks; such grounding requires accurate persistent spatial memory and perceptual inputs, and language conditioning provides crucial disambiguation for object-relational grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e530.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e530.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language-only procedural prediction (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-only subgoal prediction (π^H without state representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation condition where π^H predicts subgoal types and argument classes without using the spatial state encoding, showing that procedural sequencing (what to do) is often recoverable from language alone even when spatial grounding ('where') is missing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π^H (language-only input)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-level controller variant trained/predicted without the pooled state representation input (only phi^L and subgoal index/history), used to test how much procedural knowledge resides in language alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED (subgoal type/class prediction without spatial input)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict the next subgoal type and argument class using only the instruction embedding and subgoal index/history, i.e., without the persistent map or inventory information.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>procedural inference; instruction parsing</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural (subgoal sequencing) primarily</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>language model fine-tuning on ALFRED subgoal sequences (supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised prediction from language embedding and subgoal position encodings (ablation during model training/evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit procedural mapping in model weights from instruction embedding to distributions over subgoal types/arg classes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task SR/GC when state encoding removed for subgoal type & arg class prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Validation ablation 'w/o state repr enc.': Seen SR 30.0% (slightly higher than full), Unseen SR 18.9% (slightly higher than full). But this condition still requires spatial map for arg^M generation and low-level execution; overall shows 'what' can often be inferred from language.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully predicts plausible subgoal sequences from language alone for many tasks (i.e., procedural knowledge embedded in language supervision), enabling selection of types and classes consistent with common-sense task decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Without state, predictions may ignore environment-specific constraints (missing objects, needing exploration), and grounding (arg^M) and execution depend on the spatial map; in unseen environments language-only predictions can be harmed when protean perceptual errors cause grounding mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to full HLSM, this ablation yields mixed results (sometimes slightly better on validation due to noisy maps in unseen scenes), but overall confirms that language supplies strong procedural priors while spatial memory supplies grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing state encoder for predicting type & arg class produced mixed effects (no large drop; sometimes small improvements due to noisy state encodings in unseen scenes). This indicates a strong procedural signal in language that can substitute for state when state is noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Procedural knowledge (sequence of subgoals and argument classes) is heavily encoded in language and can be elicited by a language model alone; however, accurate embodied execution requires spatial grounding (maps and mask prediction) to answer 'where' and 'how' in a particular environment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. <em>(Rating: 2)</em></li>
                <li>Hierarchical task learning from language instructions with unified transformers and self-monitoring. <em>(Rating: 2)</em></li>
                <li>Language-guided semantic mapping and mobile manipulation in partially observable environments. <em>(Rating: 2)</em></li>
                <li>A modular vision language navigation and manipulation framework for long horizon compositional tasks in indoor environment. <em>(Rating: 1)</em></li>
                <li>Modular framework for visuomotor language grounding. <em>(Rating: 1)</em></li>
                <li>Agent with the big picture: Perceiving surroundings for interactive instruction following. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-530",
    "paper_id": "paper-6536f36648d39f0f9f6105562f76704fcc0b19e8",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "BERT (HLSM)",
            "name_full": "BERT (fine-tuned CLS embedding used in HLSM)",
            "brief_description": "Pre-trained bidirectional Transformer (BERT) is fine-tuned and used to encode the high-level natural language instruction into a fixed task embedding (CLS token) phi^L that conditions subgoal prediction and mask refinement, providing procedural priors about required subgoal sequences.",
            "citation_title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
            "mention_or_use": "use",
            "model_name": "BERT",
            "model_size": null,
            "model_description": "Pre-trained Transformer encoder (BERT) whose CLS token embedding is fine-tuned end-to-end in HLSM to produce a task embedding (phi^L) used by the high-level controller and by the mask refiner; no architectural changes beyond fine-tuning are described.",
            "task_name": "ALFRED (language-conditioned embodied instruction following)",
            "task_description": "High-level natural language instructions specifying household tasks in simulated indoor environments (AI2Thor/ALFRED). The agent receives first-person RGB, pose and inventory, and must output navigation and interaction actions (MoveAhead, RotateLeft/Right, Pickup/Put/Toggle/Open/Close/Slice with masks) to satisfy goal conditions by the episode end.",
            "task_type": "instruction following; multi-step planning; navigation + object manipulation",
            "knowledge_type": "procedural + object-relational (procedural: subgoal sequencing; object-relational: argument class grounding), with influence on spatial grounding via conditioning",
            "knowledge_source": "pre-training on large text corpora (BERT) and fine-tuning on ALFRED language-to-subgoal supervised data",
            "has_direct_sensory_input": false,
            "elicitation_method": "fine-tuning (supervised) to produce phi^L; embedding is used as conditioning input to downstream modules (REFINER, subgoal predictor)",
            "knowledge_representation": "implicit vector embedding (CLS) that encodes procedural expectations (typical subgoal sequences) and semantics of task instructions; used to condition spatial grounding modules",
            "performance_metric": "task Success Rate (SR) and Goal Condition rate (GC) of HLSM when BERT is present vs. ablated",
            "performance_result": "Full HLSM (with BERT) validation: Seen SR 29.6% / GC 38.8%, Unseen SR 18.3% / GC 31.2%; Ablation without language encoder (i.e., removing BERT): validation Seen SR 0.9% / GC 8.6%, Unseen SR 0.2% / GC 7.5% (huge degradation). Test unseen HLSM SR 20.27% (full model).",
            "success_patterns": "When present, the language embedding enables correct sequencing of subgoals (procedural knowledge) and conditions spatial grounding networks to select appropriate object instances (via phi^L conditioning of REFINER/LingUNet). It supports choosing appropriate argument classes and disambiguating actions that require specific object types.",
            "failure_patterns": "Language embedding alone cannot recover from perceptual failures or poor spatial maps; when segmentation/generalization is poor, language-conditioned predictions can still be mis-grounded. Also, language-only predictions (no state) sometimes propose subgoals that cannot be executed due to absent objects or unobserved state.",
            "baseline_comparison": "Ablation w/o language encoder: SR drops to 0.9%/0.2% (val seen/unseen) vs. full HLSM 29.6%/18.3%. Compared against HiTUT G-only (flat transformer baseline) HLSM outperforms (test unseen HLSM 20.27% vs HiTUT G-only 10.23%).",
            "ablation_results": "Removing the language encoder essentially eliminates task success (validation SR ~1% seen, ~0.2% unseen). This shows procedural knowledge encoded in BERT embeddings is critical for mapping high-level instructions to subgoal sequences.",
            "key_findings": "Pre-trained language models (BERT) fine-tuned as a task embedding effectively encode procedural expectations (subgoal sequences) that are necessary for high-level planning; removing the language encoder collapses performance, showing that procedural knowledge is primarily provided by the language model rather than being recoverable from state alone.",
            "uuid": "e530.0",
            "source_info": {
                "paper_title": "A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "π^H (High-level controller)",
            "name_full": "HLSM High-level Controller (π^H)",
            "brief_description": "Autoregressive high-level policy that consumes the language embedding (phi^L), a pooled encoding of the persistent semantic 3D map, and prior subgoal history to predict the next subgoal tuple (type, argument class, 3D argument mask).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "HLSM π^H (uses BERT + Transformer components)",
            "model_size": null,
            "model_description": "A learned module combining a fine-tuned BERT embedding for language, a max-pooled encoding of the semantic voxel map & inventory, and a Transformer autoregressive encoder over past subgoals; outputs distributions over subgoal type and argument class, and conditions a mask refiner for spatial grounding.",
            "task_name": "ALFRED (hierarchical subgoal planning within HLSM)",
            "task_description": "Given the current persistent world representation and instruction, π^H predicts the next abstract subgoal (e.g., Pickup(CD), Open(SAFE)) including an argument mask indicating where the object instance is or should be.",
            "task_type": "hierarchical planning; procedural sequencing; object-grounded instruction following",
            "knowledge_type": "procedural + spatial + object-relational (procedural: subgoal sequence prediction; spatial/object-relational: outputs argument class and conditions spatial mask generation)",
            "knowledge_source": "supervised fine-tuning on ALFRED subgoal dataset (language paired with subgoal sequences) and state representations",
            "has_direct_sensory_input": false,
            "elicitation_method": "supervised learning to predict next subgoal given phi^L, encoded state, and subgoal history; sampling at inference to allow retries/backtracking",
            "knowledge_representation": "explicit subgoal tuples (type, arg^C, arg^M) where arg^M is produced by a downstream refiner; procedural knowledge is encoded in the learned mapping from language+state to distributions over subgoal types and argument classes",
            "performance_metric": "task SR/GC when π^H is used in the HLSM pipeline and in encoder ablations (w/o subgoal history, w/o state repr encoder)",
            "performance_result": "Full HLSM validation SR 29.6%/18.3% (seen/unseen). Ablation w/o subgoal history encoder: negligible change (29.4% seen / 16.6% unseen). Ablation w/o state representation encoder when predicting type & arg class: mixed results (30.0% seen / 18.9% unseen).",
            "success_patterns": "π^H successfully decomposes high-level instructions into plausible subgoal sequences (e.g., pick up object, open receptacle, put object) and chooses argument classes consistent with common-sense procedural knowledge encoded from language supervision.",
            "failure_patterns": "Errors occur when perceptual/state inputs are noisy (mis-segmentation leads to wrong arg^M) or when subgoal predictions assume presence of objects not observed; π^H sometimes predicts wrong argument classes (e.g., CD vs EGG) when perception is ambiguous.",
            "baseline_comparison": "Compared to HiTUT (flat transformer) HLSM (with spatial map) achieves higher SR in high-level-only instruction setting (test unseen HLSM 20.27% vs HiTUT G-only 10.23%).",
            "ablation_results": "Removing subgoal history encoding had little effect, indicating index-position encoding suffices in many cases; removing state representation when predicting type/arg class gives mixed effects, implying procedural prediction can sometimes be done from language alone while grounding (arg mask) requires state.",
            "key_findings": "The high-level language-conditioned controller learns procedural knowledge (subgoal sequencing) from language supervision and uses the persistent spatial map primarily to ground 'where' subgoals should apply; predicting 'what' can sometimes rely mostly on language, but grounding requires spatial memory.",
            "uuid": "e530.1",
            "source_info": {
                "paper_title": "A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Semantic 3D Voxel Map (V^S)",
            "name_full": "Persistent Semantic 3D Voxel Map (V_t^S) with Observability & Affordances",
            "brief_description": "A dense persistent 3D voxel representation that stores per-voxel class probability distributions and an observability map, plus derived top-down affordance features used by both high- and low-level controllers to reason about object locations and navigable space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "HLSM (semantic voxel memory component)",
            "model_size": null,
            "model_description": "An explicit spatial memory: V_t^S ∈ [0,1]^{X×Y×Z×C} encodes class distributions per voxel, V_t^O indicates observed voxels, inventory vector v_t^S tracks held object classes, and AFFORD(ĥs_t) produces top-down affordance channels (pickable, receptacle, togglable, openable, ground, obstacle, observed). The map is constructed by projecting per-frame segmentation+depth and accumulated over time.",
            "task_name": "ALFRED (spatial memory for long-horizon planning and grounding)",
            "task_description": "Constructed online from RGB (segmentation+depth), the voxel map provides a persistent spatial-semantic memory enabling navigation planning, selection of exploration frontiers, object instance localization outside of view, and affordance-based interaction pose prediction.",
            "task_type": "spatial mapping; long-horizon memory for navigation & manipulation",
            "knowledge_type": "spatial + object-relational (explicit spatial locations and class distributions; supports relational reasoning like object in receptacle/state)",
            "knowledge_source": "constructed online from predicted segmentation and depth (U-Net) projected via pinhole model and accumulated (no external knowledge base)",
            "has_direct_sensory_input": true,
            "elicitation_method": "constructed via observation model F(ĥs_{t-1}, o_t, g_k) using per-frame segmentation and depth predictions, projection into 3D voxels, and accumulation (element-wise max/update), plus derived affordances",
            "knowledge_representation": "explicit dense spatial tensor (voxel grid) with per-voxel semantic distributions and observability flags; compact top-down affordance channels and max-pooled summaries used by language-conditioned planners",
            "performance_metric": "downstream task SR/GC; ablations with sensory oracles (gt depth/gt seg) measure importance of accurate spatial memory",
            "performance_result": "Using ground-truth segmentation alone (+ gt seg) improves validation SR by +6.6% seen / +16.4% unseen; using both gt depth and gt seg yields +11.1% seen / +21.9% unseen (validation), indicating the spatial map's perceptual quality strongly affects performance. Full HLSM validation: Seen SR 29.6% / Unseen SR 18.3%.",
            "success_patterns": "Enables selecting subgoals using objects outside current view, choosing exploration targets, and producing affordance-informed interaction poses; persistent memory supports multi-step tasks requiring remembered object locations (e.g., put object into safe found earlier).",
            "failure_patterns": "Perception/generalization errors (noisy segmentation) corrupt the map and lead to wrong obstacles/blocked paths or incorrect instance grounding (picking wrong object). Memory scale/memory resolution limit scalability; pose/localization uncertainty not modeled and can degrade close-range mask refinement.",
            "baseline_comparison": "Ablations with sensory oracles: +gt seg and +gt depth significantly improve SR (see Performance Result); compared to non-map approaches (HiTUT) HLSM with map outperforms in high-level-instruction-only setting.",
            "ablation_results": "Perception oracles (+gt seg, +gt depth) substantially increase performance, showing the spatial map is a critical component and its quality is a primary bottleneck for generalization. Removing state encoding in π^H had mixed effects indicating map noise in unseen scenes can hurt high-level planning.",
            "key_findings": "Explicit persistent spatial-semantic memory is effective to ground language-predicted procedures into spatial targets, to support selection of exploration and interaction poses, and to enable long-horizon multi-step tasks; however, its utility is limited by perceptual generalization and memory scale.",
            "uuid": "e530.2",
            "source_info": {
                "paper_title": "A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "REFINER / LingUNet grounding",
            "name_full": "REFINER (LingUNet-based mask refiner conditioned on language and map)",
            "brief_description": "A language-conditioned image-to-image encoder-decoder (LingUNet) that refines a class-filtered voxel candidate map into a 3D argument-instance mask arg^M, using the language embedding phi^L and egocentric/top-down transforms to produce instance-level spatial grounding.",
            "citation_title": "Mapping instructions to actions in 3D environments with visual goal prediction.",
            "mention_or_use": "use",
            "model_name": "LingUNet-based REFINER (conditioned on phi^L)",
            "model_size": null,
            "model_description": "A LingUNet-style conditional convolutional encoder-decoder that takes egocentric affordance+history+class-filtered voxel input and the language embedding phi^L to produce a refined 3D voxel mask for the predicted argument instance; uses AlloTransform/ EgoTransform to convert between reference frames.",
            "task_name": "ALFRED (object instance grounding / mask prediction)",
            "task_description": "Given predicted argument class and the persistent semantic map (or an empty vector if unobserved) plus language embedding, REFINER produces a 3D soft mask identifying the likely instance of the object to act on; projected to first-person to form interaction masks for manipulation actions.",
            "task_type": "object grounding; spatial grounding; manipulation targeting",
            "knowledge_type": "spatial + object-relational (maps class to instance location, conditioned on language semantics and required action type/affordance)",
            "knowledge_source": "trained supervised on ALFRED subgoal mask labels; conditions on language embedding (phi^L) and accumulated voxel map + affordances",
            "has_direct_sensory_input": false,
            "elicitation_method": "supervised learning (LingUNet) to map conditioned top-down/egocentric features and phi^L to 3D argument masks; used online during execution to propose where to act",
            "knowledge_representation": "explicit 3D soft mask over voxels (arg^M) representing belief about the object instance location; produced by a learned model conditioned on language and spatial memory",
            "performance_metric": "success of downstream interaction actions and overall task SR; failure modes shown qualitatively (wrong object picked when mask incorrect)",
            "performance_result": "When REFINER + map are used within HLSM full system achieves validation SR 29.6%/18.3% (seen/unseen). Perception errors that corrupt inputs to REFINER produce notable failures (e.g., wrong instance masks leading to wrong pickups); no isolated mask-accuracy numbers provided.",
            "success_patterns": "REFINER effectively leverages language conditioning to disambiguate which instance of a class is relevant for the task (e.g., which CD to pick), and to produce masks even for objects outside current view if they are in the map.",
            "failure_patterns": "When map or segmentation is noisy, REFINER can produce incorrect masks (pick wrong object) or fail to identify unobserved instances (all-zero mask), leading to subgoal failure; failure compounded by localization/pose noise which the system assumes away.",
            "baseline_comparison": "Using perfect segmentation/depth improves downstream performance substantially (+gt seg, +gt depth ablations), indicating REFINER's outputs are sensitive to perceptual inputs.",
            "ablation_results": "If arg^M is all zero (object unobserved), the high-level controller treats object as unobserved and low-level exploration is triggered; no direct ablation of REFINER alone reported, but system-level perception oracles show its dependence on good segmentation/depth.",
            "key_findings": "Conditioning spatial grounding networks on language embeddings (phi^L) enables mapping abstract instructions to precise spatial instance masks; such grounding requires accurate persistent spatial memory and perceptual inputs, and language conditioning provides crucial disambiguation for object-relational grounding.",
            "uuid": "e530.3",
            "source_info": {
                "paper_title": "A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Language-only procedural prediction (ablation)",
            "name_full": "Language-only subgoal prediction (π^H without state representation)",
            "brief_description": "An ablation condition where π^H predicts subgoal types and argument classes without using the spatial state encoding, showing that procedural sequencing (what to do) is often recoverable from language alone even when spatial grounding ('where') is missing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "π^H (language-only input)",
            "model_size": null,
            "model_description": "High-level controller variant trained/predicted without the pooled state representation input (only phi^L and subgoal index/history), used to test how much procedural knowledge resides in language alone.",
            "task_name": "ALFRED (subgoal type/class prediction without spatial input)",
            "task_description": "Predict the next subgoal type and argument class using only the instruction embedding and subgoal index/history, i.e., without the persistent map or inventory information.",
            "task_type": "procedural inference; instruction parsing",
            "knowledge_type": "procedural (subgoal sequencing) primarily",
            "knowledge_source": "language model fine-tuning on ALFRED subgoal sequences (supervised)",
            "has_direct_sensory_input": false,
            "elicitation_method": "supervised prediction from language embedding and subgoal position encodings (ablation during model training/evaluation)",
            "knowledge_representation": "implicit procedural mapping in model weights from instruction embedding to distributions over subgoal types/arg classes",
            "performance_metric": "task SR/GC when state encoding removed for subgoal type & arg class prediction",
            "performance_result": "Validation ablation 'w/o state repr enc.': Seen SR 30.0% (slightly higher than full), Unseen SR 18.9% (slightly higher than full). But this condition still requires spatial map for arg^M generation and low-level execution; overall shows 'what' can often be inferred from language.",
            "success_patterns": "Successfully predicts plausible subgoal sequences from language alone for many tasks (i.e., procedural knowledge embedded in language supervision), enabling selection of types and classes consistent with common-sense task decomposition.",
            "failure_patterns": "Without state, predictions may ignore environment-specific constraints (missing objects, needing exploration), and grounding (arg^M) and execution depend on the spatial map; in unseen environments language-only predictions can be harmed when protean perceptual errors cause grounding mismatch.",
            "baseline_comparison": "Compared to full HLSM, this ablation yields mixed results (sometimes slightly better on validation due to noisy maps in unseen scenes), but overall confirms that language supplies strong procedural priors while spatial memory supplies grounding.",
            "ablation_results": "Removing state encoder for predicting type & arg class produced mixed effects (no large drop; sometimes small improvements due to noisy state encodings in unseen scenes). This indicates a strong procedural signal in language that can substitute for state when state is noisy.",
            "key_findings": "Procedural knowledge (sequence of subgoals and argument classes) is heavily encoded in language and can be elicited by a language model alone; however, accurate embodied execution requires spatial grounding (maps and mask prediction) to answer 'where' and 'how' in a particular environment.",
            "uuid": "e530.4",
            "source_info": {
                "paper_title": "A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution",
                "publication_date_yy_mm": "2021-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Alfred: A benchmark for interpreting grounded instructions for everyday tasks.",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical task learning from language instructions with unified transformers and self-monitoring.",
            "rating": 2
        },
        {
            "paper_title": "Language-guided semantic mapping and mobile manipulation in partially observable environments.",
            "rating": 2
        },
        {
            "paper_title": "A modular vision language navigation and manipulation framework for long horizon compositional tasks in indoor environment.",
            "rating": 1
        },
        {
            "paper_title": "Modular framework for visuomotor language grounding.",
            "rating": 1
        },
        {
            "paper_title": "Agent with the big picture: Perceiving surroundings for interactive instruction following.",
            "rating": 1
        }
    ],
    "cost": 0.019261749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution</h1>
<p>Valts Blukis ${ }^{1,2}$, Chris Paxton ${ }^{1}$, Dieter Fox ${ }^{1,3}$, Animesh Garg ${ }^{1,4}$, Yoav Artzi ${ }^{2}$<br>${ }^{1}$ NVIDIA ${ }^{2}$ Cornell University ${ }^{3}$ University of Washington<br>${ }^{4}$ University of Toronto, Vector Institute</p>
<h4>Abstract</h4>
<p>Natural language provides an accessible and expressive interface to specify long-term tasks for robotic agents. However, non-experts are likely to specify such tasks with high-level instructions, which abstract over specific robot actions through several layers of abstraction. We propose that key to bridging this gap between language and robot actions over long execution horizons are persistent representations. We propose a persistent spatial semantic representation method, and show how it enables building an agent that performs hierarchical reasoning to effectively execute long-term tasks. We evaluate our approach on the ALFRED benchmark and achieve state-of-the-art results, despite completely avoiding the commonly used step-by-step instructions. https://h1sm-alfred. github.io/</p>
<p>Keywords: vision and language, spatial representations</p>
<h2>1 Introduction</h2>
<p>Mobile manipulation in a home environment requires addressing multiple challenges, including exploration and making long-term inference about actions to perform. In addition to reasoning, robots require an accessible, yet sufficiently expressive interface to specify their tasks. Natural Language provides an intuitive mechanism for task specification, and coupled with advances in automated language understanding, is increasingly applied to embodied agents [e.g., 1-11].
In this paper, we study the problem of learning to map high-level natural language instructions to low-level mobile manipulation actions in an interactive 3D environment [12]. Existing work largely studies language tightly aligned to the robot actions, either using single-sentence instructions [e.g., 1, 2, 5, 9] or sequences of instructions [13-18]. In contrast, we focus on high-level instructions, which provide more efficient human-robot communication, but require long-horizon reasoning across layers of abstraction to generate actions not explicitly specified in the instruction.
Robust reasoning about manipulation goals from unrestricted high-level natural language instructions has a variety of open challenges. Consider the instruction secure two discs in a bedroom safe (Figure 1). The robot must first locate the safe in the bedroom. It then needs to distribute the actions entailed by secure to two objects (two discs), each requiring a distinct sequence of actions, but targeting the same safe. It is also required to map the verb secure to its action space. In parallel, the robot must address mobile manipulation challenges, and often can only identify required actions as it observes and manipulates the world (e.g., if the safe needs to be opened).
We propose to construct and continually update a spatial semantic representation of the world from robot observations (Figure 2). Similar to widely used map representations [19-22], we retain the spatial properties of the environment, allowing the robot to navigate and reason about relations between objects, as required to accomplish its task. We propose the Hierarchical Language-conditioned Spatial Model (HLSM), a hierarchical approach that uses our spatial representation as a long-term memory to solve long-horizon tasks. HLSM consists of a high-level controller that generates subgoals, and a low-level controller that generates sequences of actions to accomplish them. In our example (Figure 1), the sequence of subgoals is $\langle$ pick up a CD, open the safe, put the CD in the safe, $\ldots\rangle$, each requiring a sequence of actions. The spatial representation allows selecting subgoals that use previously observed objects outside of the agent's view, or to decide about needed exploration.
We evaluate our approach on the ALFRED [12] benchmark and achieve state-of-the-art results without using the low-level instructions used by previous work [16-18, 23], neither during training</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the task and our hierarchical formulation. The agent receives a high-level task in natural language. It needs to map RGB images to navigation and manipulation actions to complete the task.</p>
<p>nor at test-time. This paper makes three key contributions: (a) a modular representation learning approach for the problem of mapping high-level natural language task descriptions to actions in a 3D environment; (b) a method for utilizing a spatial semantic representation within a hierarchical model for solving mobile manipulation tasks; and (c) state-of-the-art performance on the ALFRED benchmark, even outperforming all approaches that use detailed sequential instructions.</p>
<h2>2 Related Work</h2>
<p>Natural language has been extensively studied in robotics research, including with focus on instruction [1, 24], reference resolution [25], question generation [26–28], and dialogue [4, 29, 30]. Most work in this area has considered either synthetic instructions of relatively simple goals [7, 31–33], or natural language instructions where all intermediate steps are explained in detail [5, 12–14, 34–38]. In contrast, we focus on high-level instructions, which are more likely in home environments [39].</p>
<p>Representation of world state, action history, and language semantics plays a central role in robot systems and their algorithm design. Symbolic representations have been extensively studied for instruction following agents [1–4, 19, 20, 39–45]. While they simplify the symbol grounding problem and enable robustness, the ontologies on which they rely on are laborious to scale to new, unstructured environments and language. Representation learning presents an alternative by learning to map observations and language directly to actions [5, 8, 9, 11, 13, 34]. World state and language semantics are represented with vectors [13] or by memorizing past observations [8, 17]. Modelling improvements have enabled these approaches to achieve good performance on complex navigation tasks [7, 9, 11, 13, 14, 37], a success that has not yet translated to mobile manipulation [12, 46, 47].</p>
<p>We propose integrating a semantic voxel map state representation within a hierarchical representation learning system. Similar semantic 2D maps have been successfully used in navigation [7, 8, 48, 49] and more recently even in mobile manipulation instruction-following tasks [23]. We extend these maps to 3D and show state-of-the-art results on a challenging mobile manipulation benchmark. Our map design is related to sparse metric, topological and semantic maps [10, 19–21, 50] that have enabled grounding symbolic instruction representations. Our map does not impose a topological structure or require reasoning about object instances, instead modelling a distribution over semantic classes for every voxel.</p>
<h2>3 Problem Definition</h2>
<p>Let $\mathcal{A}$ be the set of agent actions, and $\mathcal{S}$ the set of world states. Given a natural language instruction $L$ and an initial state $s_{0} \in \mathcal{S}$, the agent's goal is to generate an execution $\Xi=$ $\langle s_{0}, a_{0}, s_{1}, a_{1}, \ldots, s_{T}, a_{T}\rangle$, where $a_{t} \in \mathcal{A}$ is an action taken by the agent at time $t$, $s_{t} \in \mathcal{S}$ is the state before taking $a_{t}$, and $s_{t+1}=\mathcal{T}\left(s_{t}, a_{t}\right)$ under environment dynamics $\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$. The state $s_{t}$ is defined by the environment layout and the poses and states of all objects and the agent. The agent does not have access to the state $s_{t}$, but only to an observation $o_{t}$. An observation $o_{t}=\left(I_{t}, P_{t}, v_{t}^{\mathcal{S}}, L\right)$ includes a first-person RGB camera image $I_{t}$, the agent's pose $P_{t}$, a one-hot encoding of the object class the agent is holding $v_{t}^{\mathcal{S}}$, and the instruction $L$. The task is considered successful if all goal-conditions corresponding to the task $L$ are true at the final state $s_{T}$. Partial success is measured as the fraction of goal-conditions that have been achieved.</p>
<p>The ALFRED dataset includes sets of seen and unseen environments. The set of actions $\mathcal{A}=\mathcal{A}<em _int="{int" _text="\text">{\text {nav }} \cup \mathcal{A}</em>}}$ includes parameter-free navigation actions $\mathcal{A<em _int="{int" _text="\text">{\text {nav }}=$ {MoveAhead, RotateLeft, RotateRight} and interaction actions $\mathcal{A}</em>=$}</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Model architecture consisting of an observation model, high-level controller ( $\pi^{H}$ ), and low-level controller ( $\pi^{L}$ ). The observation model updates the semantic voxel map state representation from RGB observations. $\pi^{H}$ predicts the next subgoal given the instruction and the map. $\pi^{L}$ outputs a sequence of actions to achieve the subgoal. The semantic voxel map is visualized in the middle with agent position illustrated as a black pillar, ans the current sugoal argument mask in yellow. Other colors are different segmentation classes. Saturated voxels are observed in the current timestep.
{Pickup, Put, ToggleOn, ToggleOff, Open, Close, Slice} parameterized by a binary mask that identifies the object of the interaction in the agent's current first-person view. We compute $P_{t}$ and $v_{t}^{S}$ using dead-reckoning from RGB observations and actions.</p>
<h1>4 Hierarchical Model with a Persistent Spatial Semantic Representation</h1>
<p>We model the agent behavior with a policy $\pi$ that maps an instruction $L$ and the observation $o_{t}$ at time $t$ to an action $a_{t}$. The policy $\pi$ is made of an observation model $F$ and two controllers: a high-level controller $\pi^{H}$ and a low-level controller $\pi^{L}$. The observation model builds a spatial state representation $\hat{s}<em t="t">{t}$ that captures the cumulative agent knowledge of the world at time $t . \hat{s}</em>$ for near-term reasoning, such as object search, navigation, collision avoidance, and manipulation. Figure 2 illustrates the policy.
The high-level controller $\pi^{H}$ computes a probability over subgoals. A subgoal $g$ is a tuple $\left(\right.$ type, $\left.\arg ^{C}, \arg ^{M}\right)$, where type $\in \mathcal{A}}$ is used by both $\pi^{H}$ for high-level long-horizon task planning, and $\pi^{L<em _int="{int" _text="\text">{\text {int }}$ is an interaction type (e.g., Open, Pickup), $\arg ^{C}$ is the semantic class of the interaction argument (e.g., SAFE, CD), and $\arg ^{M}$ is a 3D mask identifying the location of the argument instance. In ALFRED, each interaction action in the set $\mathcal{A}</em>}}$ corresponds to a subgoal type. When predicting the $k$-th subgoal at time $t, \pi^{H}$ considers the instruction $L$, the current state representation $\hat{s<em i="i">{t}$, and the sequence of past subgoals $\left\langle g</em>},\right\rangle_{i<k}$. During inference, we sample from $\pi^{H}$. Unlike $\arg \max$, sampling allows the agent to re-try the same or different subgoal incase of a potentially random failure (e.g., if a MUG was not found, pick up a CuP).
The low-level controller $\pi^{L}$ is given the subgoal $g_{k}$ as its goal specification at time $t$. At every timestep $j>t, \pi^{L}$ maps the state representation $\hat{s<em k="k">{j}$ and subgoal $g</em>$ to indicate successful or failed subgoal completion.
The execution flow is as follows. At time $t=0$ the initial observation $o_{0}$ is received. At each timestep, we update the state representation $\hat{s}}$ to an action $a_{j}$, until it outputs one of the stop actions: $a_{\text {PASS }}$ or $a_{\text {FAIL }<em k="k">{t}$ using the observation model. If there is no currently active subgoal, we sample a new subgoal $g</em>$ is exceeded. Algorithm 1 in Appendix A. 4 describes this process.}$ from $\pi^{H}$, and then sample an action $a_{t}$ from $\pi^{L}$. If $a_{t}$ is $a_{\text {PASS }}$, we increment subgoal counter $k$. If it is $a_{\text {FAIL }}$, we discard the current subgoal $k$. We repeat sampling subgoals and actions until an executable action $a_{t}$ is sampled. We execute $a_{t}$, increment the timestep $t$, and receive the next observation $o_{t}$. The episode ends when the subgoal $g_{\text {STOP }}$ is sampled or the horizon $T_{\max </p>
<h3>4.1 State Representation</h3>
<p>The state representation $\hat{s}<em t="t">{t}$ at time $t$ captures the agent's current understanding of the state of the world, including the locations of objects observed and the agent's relation to them. The state representation is a tuple $\left(V</em>$ is a 3D voxel map that for every position indicates which of the $c \in[1, C]$ object classes are present in the voxel. The}^{S}, V_{t}^{O}, v_{t}^{S}, P_{t}\right)$. The semantic map $V_{t}^{S} \in[0,1]^{X \times Y \times Z \times C</p>
<p>observability map $V_{t}^{O}\in{0,1}^{X \times Y \times Z}$ is a 3D voxel map that indicates whether the corresponding position has been observed. The inventory vector $v_{t}^{S} \in{0,1}^{C}$ indicates which of the $C$ object classes the agent is currently holding. The agent pose $P_{t}=\left(x, y, \omega_{p}, \omega_{y}\right)$ is specified by the 2 D position $(x, y)$, pitch angle $\omega_{p}$, and yaw angle $\omega_{y}$.
We also compute 2D state affordance features $\operatorname{AFFORD}\left(\hat{s}<em t="t">{t}\right) \in[0,1]^{7 \times X \times Y}$ in a top-down view that represent each position with one or more of seven affordance classes {pickable, receptacle, togglable, openable, ground, obstacle, observed}. Each $\left[\operatorname{AFFORD}\left(\hat{s}</em>\right)\right]<em t="t">{(r, x, y)}=1.0$ if at least one of the voxels at position $(x, y)$ has affordance class $r$, otherwise it is zero. $\operatorname{AFFORD}\left(\hat{s}</em>$}\right)$ is suited for object class agnostic reasoning, for example predicting a pose to pick up an object. ${ }^{1</p>
<h1>4.2 Observation Model</h1>
<p>The observation model $F\left(\hat{s}<em t="t">{t-1}, o</em>$. The computation of $F$ consists of three steps: perception, projection, accumulation.}, g_{k}\right)$ updates the state representation with new observations. It considers the current subgoal $g_{k}$ to actively acquire information relevant to $g_{k</p>
<p>Perception Step We predict semantic segmentation $I_{t}^{S}$ and depth map $I_{t}^{D}$ from the RGB observation $I_{t}$. We use neural networks pre-trained in the ALFRED environment. The semantic segmentation $\left[I_{t}^{S}\right]<em t="t">{(u, v)}$ is a distribution over $C$ object classes at pixel $(u, v)$. The depth map $\left[I</em>\right]}^{D<em t="t">{(u, v)}$ is a binned distribution over $B$ bins. ${ }^{2}$ We also heuristically compute a binary mask $M</em>$ that indicates which pixels have confident depth readings. We allow more confidence slack in pixels that correspond to the current subgoal argument $\arg }^{D<em t="t">{t}^{C}$ according to $I</em>$. Appendix A. 3 provides further details. We use perception models based on the U-Net [51] architecture, but our framework supports other, potentially more powerful models as well (e.g. [52, 53]).
Projection Step We use a pinhole camera model to convert depth $I_{t}^{D}$ and segmentation $I_{t}^{S}$ to a point cloud that represents each image pixel $(u, v)$ with a 3D position $(x, y, z) \in \mathbb{R}^{X \times Y \times Z}$ and a semantic distribution $\left[I_{t}^{S}\right]}^{S<em B="B">{(u, v)}$. We use $\arg \max </em>\right]}\left(I_{t}^{D}\right)$ to compute the 3D positions, and discard points at pixels $(u, v)$ when the binary mask value is $\left[M_{t}^{D<em t="t">{(u, v)}=0$. We construct a discrete semantic voxel map $\hat{V}</em>}^{S} \in[0,1]^{X \times Y \times Z \times C}$, where $X, Y$, and $Z$ are the width, height, and length. The value at each voxel $\left[\hat{V<em _x_="(x," y_="y," z_="z)">{t}^{S}\right]</em>\right]}$ is the element-wise maximum of the segmentation distributions $\left[I_{t}^{S<em t="t">{(u, v)}$ across all points $(u, v)$ within the voxel. We additionally compute a binary observability map $\hat{V}</em>$ that indicates the voxels observed at time $t$. A voxel is observed if it contains points, or if a ray cast from the camera through the voxel centroid has expected depth greater than the distance from the camera to the centroid.}^{O} \in{0,1}^{X \times Y \times Z</p>
<p>Accumulation Step We integrate $\hat{V}<em t="t">{t}^{S}$ and $\hat{V}</em>$ into a persistent state representation:}^{O</p>
<p>$$
V_{t}^{S}=\hat{V}<em t="t">{t}^{S} \times \hat{V}</em>}^{O}+V_{t-1}^{S} \times\left(1-\hat{V<em t="t">{t}^{O}\right) \quad V</em>\right)
$$}^{O}=\max \left(V_{t-1}^{O}, \hat{V}_{t}^{O</p>
<p>This operation updates each voxel with the most recent semantic distribution, while retaining the values of all voxels not visible at time $t$. The output of the observation model is the spatial state representation $\hat{s}<em t="t">{t}=\left(V</em>$.}^{S}, V_{t}^{O}, v_{t}^{S}, P_{t}\right)$. The inventory $v_{t}^{S}$ and pose $P_{t}$ are taken directly from $o_{t</p>
<h3>4.3 High-level Controller $\left(\pi^{H}\right)$</h3>
<p>At timestep $t$, when invoked for the $k$-th time, the input to $\pi^{H}$ is the instruction $L$, the sequence of past subgoals $\left\langle g_{i}\right\rangle_{i&lt;k}$, and the current state representation $\hat{s}<em k="k">{t}$. The output is the next subgoal $g</em>}=\left(\operatorname{type<em k="k">{k}, \arg </em>\right)$. Figure 3 illustrates the high-level controller architecture.}^{C}, \arg _{k}^{M</p>
<p>Input Encoding We encode the text $L$ using a pre-trained BERT [54] model that we fine-tune during training. We use the CLS token embedding as the task embedding $\phi^{L}$. We encode the state representation $\hat{s}<em t="t">{t}$ to account for classes of all observed objects, and the object that the agent is holding: $\phi^{s}\left(\hat{s}</em> ; \max }\right)=\left[v_{t}^{S<em t="t">{(x, y, z)}\left(V</em>\right)\right]$, where $\max }^{S<em i="i">{(x, y, z)}$ is a max-pooling operation over spatial dimensions and $[\cdot ; \cdot]$ denotes concatenation. We compute the representations of previous subgoals as $\left\langle\operatorname{REPR}\left(g</em>\right)$ is the sum of a sinusoidal positional encoding [55] of index $i$ and}\right)\right\rangle_{i=0}^{k-1}$, where $\operatorname{REPR}\left(g_{i</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of the high-level controller $\pi^{H}$ (Section 4.3).
learned embeddings for type $<em i="i">{i}$ and $\arg </em>$ as the subgoal history embedding vector. We additionally encode the argument mask information $\arg }^{C}$. We process this sequence with a two-layer Transformer autoregressive encoder [55] to compute $\left\langle\phi_{t}^{g}\right\rangle_{i=0}^{k-1}$. We take $\phi_{k-1}^{g<em k-1="k-1">{i}^{M}$ from the subgoal history in an integer-valued subgoal history tensor $\mathbf{H}</em>} \in \mathbb{N}^{K \times X \times Y}$ where $\left[\mathbf{H<em _tau_="(\tau," x_="x," y_="y)">{k-1}\right]</em>$ is the number of times an interaction action type $\tau$ was performed at 2 D position $(x, y)$ in the birds-eye view:</p>
<p>$$
\left[\mathbf{H}<em _tau_="(\tau," x_="x," y_="y)">{k-1}\right]</em>=\sum_{\substack{i=0, \ldots, k-1 \ \arg <em _tau="\tau">{i}^{C}=\tau}}^{k-1} \max </em>\left(\left[\arg <em _x_="(x," y_="y," z_="z)">{i}^{M}\right]</em>\right)
$$</p>
<p>Subgoal Prediction We concatenate the three representations $\mathbf{h}<em t="t">{(t, k)}=\left[\phi^{L} ; \phi</em>\right]$. We use a densely connected multi-layer perceptron [56] to predict two distributions $P\left(\right.$ type $\left.}^{s} ; \phi_{k-1}^{g<em _t_="(t," k_="k)">{k} \mid \mathbf{h}</em>\right)$ and $P\left(\arg <em k="k">{k}^{C} \mid\right.$ type $\left.</em>}, \mathbf{h<em k="k">{(t, k)}\right)$, from which we sample a subgoal type type $</em>$ and argument class $\arg <em k="k">{k}^{C}$.
The remaining component of the subgoal is the action argument mask $\arg </em>\right]}^{M}$. Let $\left[V_{t}^{S<em k="k">{\left(\right.$ arg $\left.</em>$ be a voxel map that only retains the object information for objects of class $\arg }^{C}\right)<em t="t">{k}^{C}$ in the semantic map $V</em>$. We refine it to identify a single object instance. We compute a birds-eye view representation:}^{S</p>
<p>$$
\mathbf{x}<em t="t">{t}=\left[\operatorname{AFFORD}\left(\hat{s}</em>}\right) ; \mathbf{H<em s="s">{k-1} ; \max </em>\right]}\left(\left[V_{t}^{S<em k="k">{\left(\arg </em>}^{C}\right)}\right) \otimes \mathbb{1<em k="k">{\text {type }</em>\right]
$$}</p>
<p>where $\operatorname{AFFORD}\left(\hat{s}<em _text="\text" _type="{type">{t}\right)$ is a birds-eye view state affordance feature map (Section 4.1) and $\mathbb{1}</em><em k="k">{k}}$ is a one-hot encoding of type $</em>$ :} .{ }^{3}$ Finally, we compute the 3D argument mask $\arg _{k}^{M} \in[0,1]^{X \times Y \times Z</p>
<p>$$
\arg <em t="t">{k}^{M}=\operatorname{REFINER}\left(\operatorname{EgOTransform}\left(\mathbf{x}</em>\right)
$$}, P_{t}\right), \phi^{L</p>
<p>where $\operatorname{EgOTransform}\left(\mathbf{x}, P_{t}\right)$ transforms the map $\mathbf{x}$ to the agent egocentric pose $P_{t}$, REFINER is a neural network based on the LingUNet architecture [36], and $\phi^{L}$ is the language embedding. The refined $\arg <em k="k">{k}^{M}$ is a $[0,1]$-valued 3D mask that identifies the instance of the interaction argument object. If the object is believed to be unobserved, then $\arg </em>=\left(\right.$ type $\left.}^{M}$ contains all zeroes. The controller output is the subgoal $g_{k<em k="k">{k}, \arg </em>\right)$.}^{C}, \arg _{k}^{M</p>
<h1>4.4 Low-level Controller ( $\pi^{L}$ )</h1>
<p>The low-level controller $\pi^{L}$ is conditioned on the most recent subgoal $g_{k}=\left(\right.$ type $\left.<em k="k">{k}, \arg </em>, \arg }^{C<em t="t">{k}^{M}\right)$. At time $t$, it maps the state representation $\hat{s}</em>$ rotation to observe the nearby environment. If no objects of type $\arg }$ to an action $a_{t}$. It combines engineered and learned components. Appendix A. 6 provides the implementation details. The controller $\pi^{L}$ invokes a set of procedures: NavigateTo, SampleExplorationPosition, SampleInteractionPose, and InteractMask. Their invokation follows a pre-specified execution flow across multiple timesteps. First, we perform a $360^{\circ<em t="t">{k}^{C}$ are observed, we explore the environment by sampling a position $(x, y)=$ SampleExplorationPosition $\left(\hat{s}</em>}\right)$, navigating there using the procedure NavigateTo $\left(x, y, \hat{s<em t="t">{t}\right)$, and performing a $360^{\circ}$ rotation. We repeat exploration until a voxel in $V</em>$ contains the class $\arg }^{S<em p="p">{k}^{C}$ with $&gt;50 \%$ probability. To interact with an object, we sample an interaction pose $\left(x, y, \omega</em>}, \omega_{p}\right)=$ SampleInteractionPose $\left(\hat{s<em k="k">{t}, g</em>}\right)$, invoke NavigateTo $\left(x, y, \hat{s<em y="y">{t}\right)$ to reach the position ( $\mathrm{x}, \mathrm{y}$ ), and then rotate according to yaw and pitch angles $\left(\omega</em>\right)$. Finally, we generate the egocentric interaction mask mask $}, \omega_{p<em t="t">{t}=\operatorname{InteractMask}\left(\hat{s}</em>, \arg <em k="k">{k}^{M}\right)$, and output the interaction action $\left(\right.$ type $\left.</em>\right)$.}, \operatorname{mask}_{t</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>All procedures use the spatial representation $\hat{s}<em t="t">{t}$. NavigateTo navigates to a goal position using a value iteration network (VIN) [57] that reasons over obstacle and observability maps from $\hat{s}</em>}$. SampleExplorationPosition samples positions on the boundary of observed space in $\hat{s<em k="k">{t}$. SampleInteractionPose uses a learned neural network NavModel to predict a distributon of poses from which the interaction $g</em>$ to compute the first-person mask of the target object.}$ will likely succeed. 2nteractMask uses the segmentation image $I_{t}^{S}$ and the 3D argument mask $\mathbf{arg}_{t}^{M</p>
<h1>5 Learning</h1>
<p>The policy contains four learned models: the segmentation and depth networks, $\pi^{H}$, and the navigation model NAVMODEL used by $\pi^{L}$. We train all four networks independently using supervised learning. We assume access to a training dataset $\mathcal{D}=\left{\left(L^{(j)}, \Xi^{(j)}\right)\right}<em D="D">{j=1}^{N</em>$ the total number of subgoals.
We process $\mathcal{D}$ into three datasets. The perception dataset $\mathcal{D}^{P}=\left{\left([I]^{(i)},[I^{D}]^{(i)},[I^{S}]^{(i)}\right}}}$ of high-level natural language instructions $L^{(j)}$ paired with demonstration execution $\Xi^{(j)}$ in a set of seen environments. Each execution $\Xi^{(j)}$ is a sequence of states and actions $\left\langle s_{0}^{(j)}, a_{0}^{(j)}, \ldots, s_{i=1}^{(j)}, a_{T}^{(j)}\right\rangle$. We denote $N_{P}$ the total number of states in dataset $\mathcal{D}$, and $N_{G<em P="P">{i=1}^{N</em>}}\right.$ includes RGB images $[I]^{(i)}$ with ground truth depth $[I^{D}]^{(i)}$ and segmentation $[I^{S}]^{(i)}$. The subgoal dataset $\mathcal{D}^{g}=\left{\left(L^{(i)}, \hat{s<em j="j">{t}^{(i)}, \left\langle g</em>\right)\right}}^{(i)}\right\rangle_{j=0}^{k<em G="G">{i=1}^{N</em>}}$ contains natural language instructions $L^{(i)}$, state representations $\hat{s<em j="j">{t}^{(i)}$ at the start of $k$-th subgoal execution, and sequences of the first $k$ subgoals $\left\langle g</em>\right)\right}}^{(i)}\right\rangle_{j=0}^{k}$ extracted from $\Xi^{(j)}$. The navigation dataset $\mathcal{D}^{N}=\left{\left(\hat{s}^{(i)}, g^{(i)}, P^{(i)<em P="P">{i=1}^{N</em>$ are constructed using the observation model (Section 4.2), but using ground-truth depth and segmentation images.
We train the perception models on $\mathcal{D}^{P}$ and the $\pi^{H}$ on $\mathcal{D}^{g}$ to predict the $k$-th subgoal by optimizing cross-entropy losses. We use $\mathcal{D}^{N}$ to train the navigation model NAVMODEL by optimizing a crossentropy loss for positions and yaw angles, and an L2 loss for the pitch angle.}}$ consists of state representations $\hat{s}^{(i)}$, subgoals $g^{(i)}$, and agent poses $P^{(i)}$ at the time of taking the interaction action corresponding to subgoal $g^{(i)}$. The state representations $\hat{s}^{(.)}$ in datasets $\mathcal{D}^{g}$ and $\mathcal{D}^{N</p>
<h2>6 Experimental Setup</h2>
<p>Environment, Data, and Evaluation We evaluate our approach on the ALFRED [12] benchmark. It contains 108 training scenes, 88/4 validation seen/unseen scenes, and 107/8 test seen/unseen scenes. There are 21,023 training tasks, 820/821 validation seen/unseen tasks, and 1533/1529 test seen/unseen tasks. Each task is specified with a high-level natural language instruction. The goal of the agent is to map raw RGB observations to actions to complete the task. ALFRED also provides detailed low-level step-by-step instructions, which simplify the reasoning process. We do not use these instructions for training or evaluation. We collect a training dataset of langugedemonstration pairs for learning (Section 5). To extract subgoal sequences, we label each interaction action $a_{t}=\left(\right.$ type $\left.<em t="t">{t}, \operatorname{mask}</em>\right)$ and any preceding navigation actions with a single subgoal of type $=$ type $<em t="t">{t}$. We compute the subgoal argument class $\arg ^{C}$ and 3D mask $\arg ^{M}$ labels from the first-person mask mask ${ }</em>$, and ground truth segmentation and depth. Completing a task requires satisfying several goal conditions. Following the common evaluation [58, 59], we report two metrics. Success rate (SR) is the fraction of tasks for which all goal conditions were satisfied. Goal condition rate (GC) is the fraction of goal-conditions satisfied across all tasks.</p>
<p>Systems We compare our approach, the Hierarchical Language-conditioned Spatial Model (HLSM) to others on the ALFRED leaderboard that only use the high-level instructions. At the time of writing, the only such published approach is HiTUT [47], an approach that uses a flat BERT [54] architecture to model a hierarchical task structure without using a spatial representation. See Appendix A. 2 for a detailed comparison. We also compare to approaches that use the step-by-step instructions, which puts our method at a disadvantage. Of these, LAV [60] also imposes a hierarchical task structure and uses pre-trained depth and segmentation models, but without using a spatial state representation.</p>
<p>Additionally, we perform ablations and study sensory oracles. To study the observation model, we compare to using sensory oracles for ground truth depth, ground truth segmentation, and both. We report high-level controller ablations that remove the subgoal encoder, language encoder, and state</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Qualitative results showcasing successes and failures of our approach. Top row: snapshots of every interaction action taken during a successful task. Action argument masks are overlaid in red over the RGB images. The white numbers are timesteps. Middle-right: illustration of a non-fatal perception error. Middle-left: illustration of a fatal perception error. The agent incorrectly interprets the reflection on the alarm clock as an obstacle, causing the agent (blue star) to believe that the path to the goal (green star) is blocked off. This is reflected in the navigation value function computed by the value iteration network (VIN) [57], where black cells are obstacles with value -1 . White cell is the goal with value 1. Bottom-left: grounding failure. The agent wrongly picks up the cup instead of a bowl. Predicted subgoals are shown in green. Bottom-right: high-level controller and perception failure. $\pi^{H}$ predicts the wrong subgoal argument class (CD instead of EGG). The segmentation model then mistakes the vase for a CD.
representation encoder as used for predicting subgoal type type $<em k="k">{k}$ and argument class $\arg </em>}^{C}$, while still using the state representation $\hat{s<em k="k">{t}$ to predict the subgoal argument mask $\arg </em>$. We also study a low-level controller ablation that removes the exploration procedure.}^{M</p>
<h1>7 Results</h1>
<p>Table 1 shows test and validation results. Our approach achieves state-of-the-art performance across both seen and unseen environments in the setting with only high-level instructions. We achieve $10.04 \%$ absolute ( $98.1 \%$ relative) improvement in SR on the test unseen split, and $11.53 \%$ absolute ( $62.6 \%$ relative) improvement in SR on the test seen split compared to HiTUT G-only.
Our approach performs competitively even when compared to approaches that also use the low-level step-by-step instructions. We achieve $4.84 \%$ absolute ( $31.4 \%$ relative) improvement in SR on the test unseen split compared to ABP [61]. On the test seen split, our approach performs reasonably well, however ABP [61] and LWIT [18] perform better, reflecting potentially stronger scene overfitting.
Tables 2 and 3 show development results. We performed five runs of the full HLSM model on the validation unseen data and found the sample standard deviation of the success rate is $1.1 \%$ (absolute). All other results are from a single-evaluation runs. Ground truth depth alone (+ gt depth) does not significantly affect performance. Ground truth segmentation (+ gt seg) provides $6.6 \% / 16.4 \%$ absolute improvement in seen/unseen scenes. Using both (+ gt depth, gt seg) provides $11.1 \% / 21.9 \%$ absolute improvement and narrows the seen/unseen gap from $11.3 \%$ to $0.5 \%$. This points to perception being the main bottleneck in generalization to unseen scenes.
We report high-level controller $\pi^{H}$ input encoder ablations. The poor performance without the language encoder reflects task difficulty. Zeroing the input to the subgoal history encoder (but keeping position encodings) does not significantly affect performance, showing that knowing the index of the current subgoal in addition to the state representation is often sufficient. Not using the state representation for predicting subgoal type and argument class gives mixed results in seen</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Validation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
</tr>
<tr>
<td style="text-align: center;">Low-level Sequential Instructions + High-level Goal Instruction</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SEQ2SEQ [12]</td>
<td style="text-align: center;">3.98</td>
<td style="text-align: center;">9.42</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">7.03</td>
<td style="text-align: center;">3.70</td>
<td style="text-align: center;">10.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">6.90</td>
</tr>
<tr>
<td style="text-align: center;">MOCA [46]</td>
<td style="text-align: center;">22.05</td>
<td style="text-align: center;">28.29</td>
<td style="text-align: center;">5.30</td>
<td style="text-align: center;">14.28</td>
<td style="text-align: center;">19.15</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">13.4</td>
</tr>
<tr>
<td style="text-align: center;">E.T. [17]</td>
<td style="text-align: center;">28.77</td>
<td style="text-align: center;">36.47</td>
<td style="text-align: center;">5.04</td>
<td style="text-align: center;">15.01</td>
<td style="text-align: center;">33.78</td>
<td style="text-align: center;">42.48</td>
<td style="text-align: center;">3.17</td>
<td style="text-align: center;">13.12</td>
</tr>
<tr>
<td style="text-align: center;">E.T. + synth. data [17]</td>
<td style="text-align: center;">38.42</td>
<td style="text-align: center;">45.44</td>
<td style="text-align: center;">8.57</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">46.59</td>
<td style="text-align: center;">52.82</td>
<td style="text-align: center;">7.32</td>
<td style="text-align: center;">20.87</td>
</tr>
<tr>
<td style="text-align: center;">LWIT [62]</td>
<td style="text-align: center;">30.92</td>
<td style="text-align: center;">45.44</td>
<td style="text-align: center;">9.42</td>
<td style="text-align: center;">20.91</td>
<td style="text-align: center;">33.70</td>
<td style="text-align: center;">43.10</td>
<td style="text-align: center;">9.70</td>
<td style="text-align: center;">23.10</td>
</tr>
<tr>
<td style="text-align: center;">HiTUT[47]</td>
<td style="text-align: center;">21.27</td>
<td style="text-align: center;">29.97</td>
<td style="text-align: center;">13.87</td>
<td style="text-align: center;">20.31</td>
<td style="text-align: center;">25.24</td>
<td style="text-align: center;">34.85</td>
<td style="text-align: center;">12.44</td>
<td style="text-align: center;">23.71</td>
</tr>
<tr>
<td style="text-align: center;">ABP [61]</td>
<td style="text-align: center;">44.55</td>
<td style="text-align: center;">51.13</td>
<td style="text-align: center;">15.43</td>
<td style="text-align: center;">24.76</td>
<td style="text-align: center;">42.93</td>
<td style="text-align: center;">50.45</td>
<td style="text-align: center;">12.55</td>
<td style="text-align: center;">25.19</td>
</tr>
<tr>
<td style="text-align: center;">High-level Goal Instruction Only</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">HiTUT G-only[47]</td>
<td style="text-align: center;">18.41</td>
<td style="text-align: center;">25.27</td>
<td style="text-align: center;">10.23</td>
<td style="text-align: center;">20.27</td>
<td style="text-align: center;">13.63</td>
<td style="text-align: center;">21.11</td>
<td style="text-align: center;">11.12</td>
<td style="text-align: center;">17.89</td>
</tr>
<tr>
<td style="text-align: center;">LAV [60]</td>
<td style="text-align: center;">13.35</td>
<td style="text-align: center;">23.21</td>
<td style="text-align: center;">6.38</td>
<td style="text-align: center;">17.27</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">HLSM (Ours)</td>
<td style="text-align: center;">29.94</td>
<td style="text-align: center;">41.21</td>
<td style="text-align: center;">20.27</td>
<td style="text-align: center;">30.31</td>
<td style="text-align: center;">29.63</td>
<td style="text-align: center;">38.74</td>
<td style="text-align: center;">18.28</td>
<td style="text-align: center;">31.24</td>
</tr>
</tbody>
</table>
<p>Table 1: Test results. Test seen/unseen and validation seen/unseen splits. Top section approaches use sequential step-by-step instructions. The bottom section uses only high-level instructions. Best results using only highlevel instructions and using both types of instructions are highlighted.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">Validation</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Seen</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">Unseen</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">SR</td>
<td style="text-align: right;">GC</td>
<td style="text-align: right;">SR</td>
<td style="text-align: right;">GC</td>
</tr>
<tr>
<td style="text-align: left;">HLSM</td>
<td style="text-align: right;">29.6</td>
<td style="text-align: right;">38.8</td>
<td style="text-align: right;">18.3</td>
<td style="text-align: right;">$\mathbf{3 1 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">+ gt depth</td>
<td style="text-align: right;">29.6</td>
<td style="text-align: right;">40.5</td>
<td style="text-align: right;">20.1</td>
<td style="text-align: right;">33.7</td>
</tr>
<tr>
<td style="text-align: left;">+ gt depth, gt seg.</td>
<td style="text-align: right;">40.7</td>
<td style="text-align: right;">50.4</td>
<td style="text-align: right;">40.2</td>
<td style="text-align: right;">52.2</td>
</tr>
<tr>
<td style="text-align: left;">+ gt seg.</td>
<td style="text-align: right;">36.2</td>
<td style="text-align: right;">47.0</td>
<td style="text-align: right;">34.7</td>
<td style="text-align: right;">47.8</td>
</tr>
<tr>
<td style="text-align: left;">w/o language enc.</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">8.6</td>
<td style="text-align: right;">0.2</td>
<td style="text-align: right;">7.5</td>
</tr>
<tr>
<td style="text-align: left;">w/o subg. hist. enc.</td>
<td style="text-align: right;">29.4</td>
<td style="text-align: right;">38.5</td>
<td style="text-align: right;">16.6</td>
<td style="text-align: right;">29.2</td>
</tr>
<tr>
<td style="text-align: left;">w/o state repr enc.</td>
<td style="text-align: right;">30.0</td>
<td style="text-align: right;">40.6</td>
<td style="text-align: right;">$\mathbf{1 8 . 9}$</td>
<td style="text-align: right;">30.8</td>
</tr>
<tr>
<td style="text-align: left;">w/o exploration</td>
<td style="text-align: right;">$\mathbf{3 2 . 2}$</td>
<td style="text-align: right;">$\mathbf{4 2 . 4}$</td>
<td style="text-align: right;">18.1</td>
<td style="text-align: right;">31.3</td>
</tr>
</tbody>
</table>
<p>Table 2: Development results on validation split. Performance of our full approach, with perception oracles, a perception ablation, $\pi^{H}$ ablations, and $\pi^{L}$ ablations</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Type</th>
<th style="text-align: right;">Validation</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Seen</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">Unseen</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">SR</td>
<td style="text-align: right;">GC</td>
<td style="text-align: right;">SR</td>
</tr>
<tr>
<td style="text-align: left;">Overall</td>
<td style="text-align: right;">29.6</td>
<td style="text-align: right;">38.7</td>
<td style="text-align: right;">18.3</td>
</tr>
<tr>
<td style="text-align: left;">Examine</td>
<td style="text-align: right;">46.8</td>
<td style="text-align: right;">59.0</td>
<td style="text-align: right;">36.6</td>
</tr>
<tr>
<td style="text-align: left;">Pick \&amp; Place</td>
<td style="text-align: right;">57.0</td>
<td style="text-align: right;">57.0</td>
<td style="text-align: right;">34.8</td>
</tr>
<tr>
<td style="text-align: left;">Stack \&amp; Place</td>
<td style="text-align: right;">13.0</td>
<td style="text-align: right;">27.0</td>
<td style="text-align: right;">4.4</td>
</tr>
<tr>
<td style="text-align: left;">Clean \&amp; Place</td>
<td style="text-align: right;">25.0</td>
<td style="text-align: right;">39.5</td>
<td style="text-align: right;">11.3</td>
</tr>
<tr>
<td style="text-align: left;">Cool \&amp; Place</td>
<td style="text-align: right;">17.5</td>
<td style="text-align: right;">33.8</td>
<td style="text-align: right;">14.8</td>
</tr>
<tr>
<td style="text-align: left;">Heat \&amp; Place</td>
<td style="text-align: right;">9.3</td>
<td style="text-align: right;">29.1</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Pick 2 \&amp; Place</td>
<td style="text-align: right;">34.7</td>
<td style="text-align: right;">51.9</td>
<td style="text-align: right;">18.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance breakdown per task type on the validation split.
and unseen scenes, but without a significant difference in performance. Therefore, predicting the sequence of subgoal types and argument classes (i.e., what to do) is at times possible without spatial reasoning, while grounding the subgoal (i.e., where to do it) requires spatial information. Removing random exploration from $\pi^{L}$ does not significantly affect unseen performance.
Figure 4 illustrates the model behavior, showing both successes and common failures. The main failures in valid unseen scenes are due to (1) perception errors that result in missing or extraneous obstacles or picking up wrong objects; (2) insufficiency of random exploration (e.g., not searching inside cabinets); (3) navigation model errors (e.g., blocking objects from opening); (4) subgoal prediction errors (e.g., picking up wrong objects); and (5) lack of state-aware multi-step planning and backtracking. More qualitative results are available in Appendix A.10.</p>
<h1>8 Discussion and Limitations</h1>
<p>We showed that a persistent spatial semantic representation enables a hierarchical model to achieve state-of-the-art performance on a challenging instruction-following mobile manipulation task. The main performance bottlenecks include long-horizon exploration, perception generalization to unseen environments, and low-level motion planning for continuous collision avoidance. In terms of learning, incorporating reinforcement learning to train $\pi^{H}, \pi^{L}$, and observation model $F$ jointly could improve robustness. We defined the interface to $\pi^{L}$ to be faithful to skills available on physical robots, but the exact implementation of $\pi^{L}$ is not the focus of our work. Physical deployment would require changes to $\pi^{L}$, and study on robustness to errors in continuous environments, such as localization or motion uncertainty.</p>
<h1>9 Acknowledgements</h1>
<p>This research was supported by ARO W911NF-21-1-0106, a Google Focused Award, and NSF under grant No. 1750499. Animesh Garg is supported in part by CIFAR AI Chair and NSERC Discovery Grant. A significant part of the work was done during the first author's internship at Nvidia. We thank the authors of ALFRED for maintaining the benchmark. We thank Mohit Shridhar and Jesse Thomason for their help answering our questions, and the anonymous reviewers for their helpful comments.</p>
<h2>References</h2>
<p>[1] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. Gopal Banerjee, S. Teller, and N. Roy. "Approaching the Symbol Grounding Problem with Probabilistic Graphical Models. AI Magazine, 2011.
[2] C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox. Learning to parse natural language commands to a robot control system. In ISER, 2012.
[3] D. K. Misra, J. Sung, K. Lee, and A. Saxena. Tell me dave: Context-sensitive grounding of natural language to mobile manipulation instructions. In RSS, 2014.
[4] J. Thomason, S. Zhang, R. J. Mooney, and P. Stone. Learning to interpret natural language commands through human-robot dialog. In IJCAI, 2015.
[5] D. Misra, J. Langford, and Y. Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In EMNLP, 2017.
[6] D. Nyga, S. Roy, R. Paul, D. Park, M. Pomarlan, M. Beetz, and N. Roy. Grounding robot plans from natural language instructions with incomplete world knowledge. In CoRL, 2018.
[7] V. Blukis, N. Brukhim, A. Bennet, R. Knepper, and Y. Artzi. Following high-level navigation instructions on a simulated quadcopter with imitation learning. In RSS, 2018.
[8] V. Blukis, D. Misra, R. A. Knepper, and Y. Artzi. Mapping navigation instructions to continuous control actions with position-visitation prediction. In CoRL, 2018.
[9] V. Blukis, Y. Terme, E. Niklasson, R. A. Knepper, and Y. Artzi. Learning to map natural language instructions to physical quadcopter control using simulated flight. In CoRL, 2019.
[10] S. Patki, E. Fahnestock, T. M. Howard, and M. R. Walter. Language-guided semantic mapping and mobile manipulation in partially observable environments. In CoRL, 2019.
[11] V. Blukis, R. A. Knepper, and Y. Artzi. Few-shot object grounding and mapping for natural language robot instruction following. In CoRL, 2020.
[12] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In CVPR, 2020.
[13] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, 2018.
[14] H. Tan, L. Yu, and M. Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. In NAACL-HLT, 2019.
[15] P. Anderson, A. Shrivastava, J. Truong, A. Majumdar, D. Parikh, D. Batra, and S. Lee. Sim-to-real transfer for vision-and-language navigation. In CoRL, 2020.
[16] B. Kim, S. Bhambri, K. P. Singh, R. Mottaghi, and J. Choi. Abp, alfred leaderboard, may 10th 2021. https://leaderboard.allenai.org/alfred/submission/ c2t70j37q4q5ci4so89g. Accessed: June 16th, 2021.</p>
<p>[17] A. Pashevich, C. Schmid, and C. Sun. Episodic Transformer for Vision-and-Language Navigation, 2021.
[18] Anonymous. Lwit, alfred leaderboard, may 10th 2021. https://leaderboard.allenai. org/alfred/submission/bvppcin94ro4j7j0jqlg. Accessed: May 25th, 2021.
[19] M. R. Walter, S. Hemachandra, B. Homberg, S. Tellex, and S. Teller. Learning Semantic Maps from Natural Language Descriptions. In RSS, 2013.
[20] S. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter. Learning models for following natural language directions in unknown environments. In ICRA, 2015.
[21] S. Patki, A. F. Daniele, M. R. Walter, and T. M. Howard. Inferring compact representations for efficient natural language understanding of robot instructions. In 2019 International Conference on Robotics and Automation (ICRA), pages 6926-6933. IEEE, 2019.
[22] I. Kostavelis and A. Gasteratos. Semantic mapping for mobile robotics tasks: A survey. Robotics and Autonomous Systems, 2015.
[23] H. Saha, F. Fotouhi, Q. Liu, and S. Sarkar. A modular vision language navigation and manipulation framework for long horizon compositional tasks in indoor environment. arXiv preprint arXiv:2101.07891, 2021.
[24] T. Kollar, S. Tellex, D. Roy, and N. Roy. Toward understanding natural language directions. In HRI, 2010.
[25] C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox. A Joint Model of Language and Perception for Grounded Attribute Learning. In ICML, 2012.
[26] S. Tellex, R. Knepper, A. Li, D. Rus, and N. Roy. Asking for help using inverse semantics. In $R S S, 2014$.
[27] R. A. Knepper, S. Tellex, A. Li, N. Roy, and D. Rus. Recovering from Failure by Asking for Help. Autonomous Robots, 2015.
[28] Z. Gong and Y. Zhang. Temporal spatial inverse semantics for robots communicating with humans. In ICRA, 2018.
[29] T. Brick and M. Scheutz. Incremental natural language processing for hri. In HRI, 2007.
[30] S. Tellex, P. Thaker, R. Deits, D. Simeonov, T. Kollar, and N. Roy. Toward information theoretic human-robot dialog. Robotics, 2013.
[31] K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. Czarnecki, M. Jaderberg, D. Teplyashin, et al. Grounded language learning in a simulated 3d world. arXiv preprint arXiv:1706.06551, 2017.
[32] D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Rajagopal, and R. Salakhutdinov. Gated-attention architectures for task-oriented language grounding. AAAI, 2018.
[33] C. Paxton, Y. Bisk, J. Thomason, A. Byravan, and D. Fox. Prospection: Interpretable plans from language by predicting the future. In ICRA, 2019.
[34] A. Suhr and Y. Artzi. Situated mapping of sequential instructions to actions with single-step reward observation. In ACL, 2018.
[35] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell. Speaker-follower models for vision-and-language navigation. In NeurIPS, 2018.
[36] D. Misra, A. Bennett, V. Blukis, E. Niklasson, M. Shatkin, and Y. Artzi. Mapping instructions to actions in 3D environments with visual goal prediction. In EMNLP, 2018.
[37] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher, and C. Xiong. Self-monitoring navigation agent via auxiliary progress estimation. In ICLR, 2019.</p>
<p>[38] H. Chen, A. Suhr, D. Misra, and Y. Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In CVPR, 2019.
[39] D. K. Misra, K. Tao, P. Liang, and A. Saxena. Environment-driven lexicon induction for highlevel instructions. In ACL, 2015.
[40] M. MacMahon, B. Stankiewicz, and B. Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. In AAAI, 2006.
[41] S. R. K. Branavan, L. S. Zettlemoyer, and R. Barzilay. Reading between the lines: Learning to map high-level instructions to commands. In ACL, 2010.
[42] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In AAAI, 2011.
[43] F. Duvallet, T. Kollar, and A. Stentz. Imitation learning for natural language direction following through unknown environments. In ICRA, 2013.
[44] Y. Artzi and L. Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. TACL, 2013.
[45] E. C. Williams, N. Gopalan, M. Rhee, and S. Tellex. Learning to parse natural language to grounded reward functions with weak supervision. In ICRA, 2018.
[46] K. P. Singh, S. Bhambri, B. Kim, R. Mottaghi, and J. Choi. Moca: A modular object-centric approach for interactive instruction following. arXiv preprint arXiv:2012.03208, 2020.
[47] Y. Zhang and J. Chai. Hierarchical task learning from language instructions with unified transformers and self-monitoring. ACL Findings, 2021.
[48] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi. Iqa: Visual question answering in interactive environments. In CVPR, 2018.
[49] P. Anderson, A. Shrivastava, D. Parikh, D. Batra, and S. Lee. Chasing ghosts: Instruction following as bayesian state tracking. In NeurIPS, 2019.
[50] S. Hemachandra, M. R. Walter, S. Tellex, and S. Teller. Learning spatial-semantic representations from natural language descriptions and scene classifications. In ICRA, 2014.
[51] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015.
[52] J. McCormac, A. Handa, S. Leutenegger, and A. J. Davison. Scenenet rgb-d: Can 5m synthetic images beat generic imagenet pre-training on indoor segmentation? In ICCV, 2017.
[53] Y. Wu, A. Kirillov, F. Massa, W.-Y. Lo, and R. Girshick. Detectron2. https://github.com/ facebookresearch/detectron2, 2019.
[54] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.
[55] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In NeurIPS, 2017.
[56] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In CVPR, 2017.
[57] A. Tamar, Y. Wu, G. Thomas, S. Levine, and P. Abbeel. Value iteration networks. In NeurIPS, 2016.
[58] M. Shridhar and D. Hsu. Interactive visual grounding of referring expressions for human-robot interaction. In RSS, 2018.
[59] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. Alfred leaderboard. https://leaderboard.allenai.org/alfred/. Accessed: June 16th, 2021.</p>
<p>[60] K. Nottingham, L. Liang, D. Shin, C. C. Fowlkes, R. Fox, and S. Singh. Modular framework for visuomotor language grounding. arXiv preprint arXiv:2109.02161, 2021.
[61] B. Kim, S. Bhambri, K. P. Singh, R. Mottaghi, and J. Choi. Agent with the big picture: Perceiving surroundings for interactive instruction following. In Embodied AI Workshop CVPR, 2021.
[62] V.-Q. Nguyen, M. Suganuma, and T. Okatani. Look wide and interpret twice: Improving performance on interactive instruction-following tasks. In IJCAI, 2021.
[63] H.-T. L. Chiang, A. Faust, M. Fiser, and A. Francis. Learning navigation behaviors end-to-end with autorl. RA-L, 2019.
[64] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox. Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes. arXiv preprint arXiv:2103.14127, 2021.
[65] D. Meagher. Geometric modeling using octree encoding. Computer graphics and image processing, 1982.
[66] T. Takikawa, J. Litalien, K. Yin, K. Kreis, C. Loop, D. Nowrouzezahrai, A. Jacobson, M. McGuire, and S. Fidler. Neural geometric level of detail: Real-time rendering with implicit 3d shapes. In CVPR, 2021.
[67] H. P. Grice. Logic and conversation. In Speech acts. 1975.
[68] J. Roh, C. Paxton, A. Pronobis, A. Farhadi, and D. Fox. Conditional driving from natural language instructions. In CoRL, pages 540-551, 2020.
[69] C. Matuszek, D. Fox, and K. Koscher. Following directions using statistical machine translation. In HRI, 2010.
[70] N. Gopalan, D. Arumugam, L. L. Wong, and S. Tellex. Sequence-to-sequence language grounding of non-markovian task specifications. In RSS, 2018.
[71] D. Bahdanau, F. Hill, J. Leike, E. Hughes, A. Hosseini, P. Kohli, and E. Grefenstette. Learning to understand goal specifications by modelling reward. In $I C L R, 2018$.
[72] P. Goyal, S. Niekum, and R. J. Mooney. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. CoRL, 2020.
[73] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee. Beyond the nav-graph: Vision-and-language navigation in continuous environments. arXiv preprint arXiv:2004.02857, 2020.
[74] V. Jain, G. Magalhaes, A. Ku, A. Vaswani, E. Ie, and J. Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. In ACL, 2019.
[75] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. In EMNLP, 2020.
[76] M. Persson, T. Duckett, C. Valgren, and A. Lilienthal. Probabilistic semantic mapping with a virtual sensor for building/nature detection. In Computational Intelligence in Robotics and Automation, 2007.
[77] H. Zender, O. M. Mozos, P. Jensfelt, G.-J. Kruijff, and W. Burgard. Conceptual spatial representations for indoor mobile robots. Robotics and Autonomous Systems, 2008.
[78] A. Pronobis, O. Martinez Mozos, B. Caputo, and P. Jensfelt. Multi-modal semantic place classification. IJRR, 2010.
[79] A. Pronobis. Semantic mapping with mobile robots. PhD thesis, KTH Royal Institute of Technology, 2011.</p>
<h1>A Appendix</h1>
<h2>A. 1 Frequently Asked Questions</h2>
<ul>
<li>Are the ALFRED sequential instructions needed during training? The sequential step-by-step instructions are not needed neither during training, nor at test-time.</li>
<li>What has to be done to apply this approach to a real robot? The observation model, high-level controller, state representation, and the interface to the low-level controller together constitute our contribution and are intended to generalize to physical robots. Deployment on a real robot would require an implementation of the low-level controller designed for continuous motion in cluttered environments, and an implementation of the ALFRED interface to enable execution of manipulation actions such as Pickup and ToggleOn. Such physical robot capabilities are subject of ongoing research [63, 64].</li>
<li>Does this simulated environment result constitute progress towards real-world capabilities? Real-robot operation is the long-term motivation of this work and has been carefully considered in the design of the representation and the approach. However, we do not claim to execute high-level natural language mobile manipulation instructions from raw vision on real robots in unseen environments. To date, such capabilities haven't been demonstrated even in simulated environments, such as ALFRED. Even in this scenario, though our method achieves better results than existing work, it can still only solve $18.28 \%$ of problems in unseen environments.</li>
<li>Would the system scale to physically larger environments? The main bottleneck towards scaling to larger environments is the memory constraint of the semantic memory. While our implementation is likely restricted to interior scenes when using commodity hardware, follow-up work could address this, perhaps using multi-scale representations such as Octress $[65,66]$.</li>
<li>How are the state dynamics modeled? Are they assumed to be known or are they learned? The GoTo procedure in the low-level controller is based on a value-iteration network that utilizes a deterministic grid-navigation dynamics model on the internal representation, which is a crude approximation of the dynamics of the RotateLeft, Rotateright, MoveAhead navigation commands. Other than that, the dynamics of the environment are assumed to be completely unknown to the agent, and are not explicitly learned or modeled.</li>
<li>How would localization uncertainty affect the approach? Our representation approach assumes a reliable robot pose estimate. Precisely studying the effects of pose errors would require integration into a system for continuous environments. Intuitively, voxels further away are affected by pose errors more, but may better tolerate it due to being used mainly to decide navigation goals. Voxels close to the agent require more precision as they are used for object instance mask generation, but would be less affected by pose errors. Our voxel map uses a relatively coarse 25 cm resolution.</li>
<li>Which model was used to obtain test results? The full HLSM model was evaluated on the test set, even though the model without state representation encoding input to the high-level controller performed better in unseen environments on the validation set.</li>
<li>Why does the ablation without state representation encodings perform better in unseen environments? In unseen environments, the semantic segmentation is erroneous due to the generalization gap, resulting in state encodings that contain errors. This may affect perfromance of the high-level controller that was trained on data with perfect segmentation, and thus with perfect state encodings.</li>
<li>What is the benefit of sampling the subgoals instead of attempting execution from most to least likely in order? There are two types of subgoal execution failures: systematic and random. An example of a systematic failure is the selection of an incorrect subgoal. For example, TOGGLleOn(FloorLAMP) would fail if a FloorLAMP does not exist in the environment. An example of a random failure is the low-level controller sampling an interaction pose for which the interaction fails (e.g., Figure 4, row 1, timestep 272). A next-best approach would alleviate a systematic failure, but a sampling approach alleviates both: the systematic failures by trying different subgoals, and random failures by potentially sampling the same subgoal multiple times.</li>
</ul>
<h1>A. 2 Extended Related Work</h1>
<p>Grounding High-level Language to Actions in Robotics In order for natural language humanrobot interfaces to be useful and widely adopted in practice, they should support instructions that are as brief as possible while still being informative of the task, i.e., that adhere to Grice's maxim of quantity [67]. Following such high-level instructions requires bridging the gap from high-level language to long sequences of low-level actions. This is commonly achieved using temporal abstraction, where subgoals or options abstract over sequences of low-level actions, reducing the effective time horizon of the problem. Most work on instruction following in robotics utilizes temporal abstraction $[1,3,10,20,21,33,36,39,42,44,68]$.
Various methods explicitly model correspondences between linguistic constituents in a symbolic instruction representation, environment percepts in the world model, and subgoals (behavior primitives) $[1,3,20,24,39,69]$. This requires the instruction to at least mention each subgoal, and precludes instructions that omit intermediate goals that are expected to be inferred. This limitation can be overcome by directly mapping from language to reward specifications [45, 70-72] or postconditions [39], and then using a planner [39] or learning a task-specific policy [71, 72] to solve for the sequence of actions. Both are difficult in practice. Planning requires a compact, symbolic environment representation with an underlying ontology that is hard to construct for unstructured environments, such as the household environment studied in this work. Policy learning is computationally expensive, and poorly adapts to novel tasks specified in natural language in real-time.
Recently, methods that map language and observations directly to actions using neural networks have seen rising popularity and success on simulated [8, 13, 14, 38, 73-75] and real-robot [9, 11, 15] navigation, as well as simulated manipulation [5] tasks. Simulated mobile manipulation is a promising next frontier [12, 46, 47]. Representation learning approaches avoid planning, by using a direct sequence-to-sequence formulation and a data-driven approach that theoretically permits mapping arbitrarily terse input text to arbitrarily long action sequences that potentially include any necessary intermediate steps not explicitly mentioned in the text. In practice, however, most research has focuses on relatively detailed step-by-step instructions, sometimes using modelling tools such as attention [13, 17] and progress monitoring [37] to leverage the sequential nature of the instructions.
We learn to follow high-level instructions in an interactive mobile manipulation 3D environment. To bridge the gap between language and actions, we use temporal abstraction, where the high-level controller predicts subgoals that abstract over sequences of actions, and the low-level controller generates actions to fulfil each subgoal. The controllers rely on a spatial-semantic state representation to enable reasoning about what subgoals make progress towards the high-level task, and what actions make progress towards the specific subgoal, given all past sensory observations. The persistent representation enables operation over long time horizons. Using a shared world representation for both the high- and low-level controllers reduces representation engineering effort and error accumulation typically associated with pipeline approaches.</p>
<p>Semantic Maps for Language Grounding in Robotics The idea of building maps that combine spatial and semantic information [19, 50, 76-79] and using them for following natural language instructions [7, 9, 10, 20, 21, 49] has a long history in robotics. Common approaches can be classified into sparse topological and dense grid-based maps.
Walter et al. [19] introduced a sparse semantic graph that combines pose, semantic, and topological information, extracted from sensory observations and speech descriptions along a route. Hemachandra et al. [50] added a spatial map layer, and fused language with other sensory modalities. Hemachandra et al. [20] used these representations for grounding natural language route instructions. More recently, Patki et al. [21] extended this framework to build compact world models specific to the input instruction, and Patki et al. [10] enabled supporting previously unseen environments. This class of sparse topological maps are well suited for probabilistic language grounding from symbolic representations.
Dense grid-based 2D semantic maps are suited for downstream processing using learned neural network modules, and have been used in modular neural network approaches for language grounding $[8,9,23,48,49]$. Saha et al. [23] used a grid-based spatial representation and a map filtering method, showing promising early results on a subset of the ALFRED dataset. We extend this line of work to 3D voxel maps, add explicit tracking of occupancy and observability, and maintain the representation through time to facilitate grounding high-level language over long time horizons. Our</p>
<p>dense representation has a number of advantages. First, it is easy to build in real-time from RGBD data using segmentation models and geometric operations. Second, it captures structures found in indoor environments, such as L-shaped countertops or kitchen islands with sinks that are hard to represent topologically. Third, it encodes spatial object relationships without requiring an ontology of spatial relations, or even tracking of object instances. The main limitation of our approach is a memory footprint that scales with the physical size of the environment, making it less suited for outdoor or field applications. Follow-up work could address this limitation, for example by using multi-scale representations such as octrees [65, 66].</p>
<p>Detailed comparison to HiTUT We provide a detailed technical comparison between our approach and HiTUT [Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring; 47], our main point of comparison.</p>
<p>Both our approach and HiTUT use a hierarchical task decomposition of goals into sequences of subgoals, and subgoals into sequences of actions. The set of subgoals assumed by our approach and HiTUT have differences. HiTUT has an additional subgoal GoTo(LOCATION), while we view any navigation as a means to an end of a manipulation subgoal, and therefore do not have an explicit GoTo subgoal. HiTUT additionally has subgoals for Clean and Heat, (e.g., Clean(Obj) usually abstracts over the sequence Put(Sink), ToggleOn(Faucet), ToggleOff(Faucet), Pickup(Obj)), while our high-level policy would have to predict this entire sequence.</p>
<p>In terms of the model architecture, we use a hierarchical model with high-level and low-level controllers to mimic the task structure. In contrast, HiTUT uses a flat transformer model to jointly solve high-level subgoal planning and low-level action prediction. One of their main contributions is showing how a flat transformer model can be used to model a hierarchical task structure. The benefit of our hierarchical model decomposition in combination with a shared spatial state representation is its ability to solve low-level navigation and manipulation problems with specialized modules, while avoiding the representational error accumulation and representation engineering issues typically associated with modular pipeline approaches.</p>
<p>In terms of inference, HiTUT and our approach both sample subgoals one at a time, dynamically responding to changes in environment and execution. Both approaches perform backtracking to previous subgoals upon subgoal failure.</p>
<p>In terms of perception, our approach requires a pre-trained segmentation model, while HiTUT requires a pre-trained object detection model to generate object entity information that is fed into the transformer.</p>
<h1>A. 3 Observation Model Details</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Illustration of the U-Net architecture used in the depth and segmentation networks.</p>
<p>At time $t$, during the perception step, we predict first-person semantic segmentation $I_{t}^{S}$ and depth $I_{t}^{D}$ from the observation $o_{t}=\left(I_{t}, P_{t}, v_{t}^{S}, L\right)$, from the RGB image $I_{t}$ with neural network models pre-trained in the ALFRED environment. Each pixel $\left[I_{t}^{S}\right]<em t="t">{(u, v)}$ at coordinates $(u, v)$ is a distribution over $C$ object classes. Likewise, $\left[I</em>\right]}^{D<em D="D">{(u, v)}$ is a distribution over $B$ uniformly spaced depth bins $\left{0, \Delta</em>$ that indicates which pixels have confident depth readings. We allow more confidence slack in pixels that correspond to the current subgoal}, 2 \Delta_{D}, \ldots,(B-1) \Delta_{D}\right}$, where $\Delta_{D}$ is a depth resolution. In early experiments, we observed that $\Delta_{D}$ should be less than $50 \%$ of the voxel size. We use $\Delta_{D}=0.1 m, B=50$, and voxel size of 0.25 m . We also heuristically compute a binary mask $M_{t}^{D</p>
<p>argument $\arg <em t="t">{t}^{C}$ according to $I</em>=0$. The mask computation is:}^{S}$. The mask $M_{t}^{D}$ is used in the projection step to discard points $(x, y, z)$ that correspond to pixels $(u, v)$ for which $\left[M^{D}\right]_{(u, v)</p>
<p>$$
M_{t}^{D}=\left(W_{90}\left[I_{t}^{D}\right]&lt;c_{1} \mathbb{E}\left[I_{t}^{D}\right]\right) \vee\left(\left(W_{90}\left[I_{t}^{D}\right]<c_{2} \mathbb{E}\left[I_{t}^{D}\right]\right) \wedge\left(\left[I_{t}^{S}\right]_{\arg _{t}^{C}}>0.5\right)\right)
$$</p>
<p>where $W_{90}\left[I_{t}^{D}\right]$ is the width of the $90 \%$ confidence interval at each pixel, $\mathbb{E}\left[I_{t}^{D}\right]$ is the expected depth at each pixel, and $\left[I_{t}^{S}\right]<em t="t">{\text {arg }</em>=1.0$ to allow higher depth uncertainty for points corresponding to the subgoal argument.
If the agent is currently holding an object (i.e. $\left.\sum_{i}\left[\left[v_{t}^{S}\right]_{(i)}\right]&gt;0\right)$, we also discard points closer than 0.7 m to the camera to make sure that the object in the agent inventory does not get added to the voxel map.}^{C}}$ is a $0-1$ valued segmentation mask of the class of the current subgoal argument. We set the hyperparameters $c_{1}=0.3$ and $c_{2</p>
<p>We use custom models based on the U-Net architecture [51] for depth and segmentation networks. The architecture is illustrated in Figure 5. It consists of a cascade of five downscale blocks followed by five upscale blocks with skip-connections. Each block includes two convolutions, two leakyReLU activations, and an instance normalization layer. The upscale blocks contain a 2 x spatial upscaling operation. We found that training a separate network for depth and segmentation worked better than sharing one network for both tasks.</p>
<h1>A. 4 Model Execution Flow</h1>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Execution</span><span class="w"> </span><span class="nx">Flow</span>
<span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">Instr</span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">L</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">Horizon</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">T_</span><span class="p">{</span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">null</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">k</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Observe</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">o_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="p">=</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="w"> </span><span class="nx">H</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">F</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="o">-</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">o_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">do</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="nx">k</span><span class="p">}=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">null</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sim</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="nx">H</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="nx">langle</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="nx">rangle_</span><span class="p">{</span><span class="nx">i</span><span class="p">&lt;</span><span class="nx">k</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="nx">k</span><span class="p">}=</span><span class="nx">g_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">STOP</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="nx">End</span><span class="w"> </span><span class="nx">episode</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sim</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="nx">L</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="nx">k</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="nx">a_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">PASS</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="nx">k</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">k</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="nx">a_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">FAIL</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="nx">k</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">null</span>
<span class="w">            </span><span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">a_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">FAIL</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">PASS</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Perform</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">o_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">End</span><span class="w"> </span><span class="nx">episode</span>
</code></pre></div>

<p>Algorithm 1 describes the execution flow. At time $t=0$ the initial observation $o_{0}$ is received. At each timesep, we update the state representation $\hat{s}<em k="k">{t}$ (Line 5). If needed, we sample a new subgoal $g</em>$ is exceeded (Line 18).}$ from $\pi^{H}$ (Line 8), and then sample an action $a_{t}$ from $\pi^{L}$. If $a_{t}$ is $a_{\text {PASS }}$, we increment subgoal counter $k$ (Line 13). If it is $a_{\text {FAIL }}$, we discard the current subgoal $k$ (Line 15). We repeat Lines 8-15 until an executable action $a_{t}$ is sampled. We execute $a_{t}$, receive the next observation (Line 17), and proceed to the next timestep. The episode ends when the subgoal $g_{\mathrm{STOP}}$ is sampled (Line 10) or the horizon $T_{\max </p>
<h2>A. 5 High-Level Controller Details</h2>
<p>Subgoals are predicted periodically. Let $g_{k}=\left(\right.$ type $<em k="k">{k}, \arg </em>, \arg }^{C<em k="k">{k}^{M}$ ) be the $k$-th subgoal predicted at time $t$. Predicting the subgoal type type $</em>$ and the argument class $\arg <em k="k">{k}^{C}$ is described in the main paper (Section 4.3). This section provides further details of REFINER, the model we use to generate $\arg </em>}^{M}$. The mask refiner REFINER has four inputs: ${ }^{4}$ (a) a spatial feature map $\mathbf{x<em t="t">{t}^{\text {ego }} \in$ $[0,1]^{N \times W \times L}$ oriented in the agent egocentric reference frame; (b) $\left[V</em>\right]}^{S<em k="k">{\left(\arg </em>$, a}^{C}\right)} \in[0,1]^{W \times L \times H</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Illustration of the LingUNet architecture used for as part of REFINER within the high-level controller $\pi^{H}$, and as part of the navigation model NAVMODEL within the low-level controller. The conditional convolutions parameters are computed during the network forward pass.
3D mask indicating all voxels that contain objects of class $\arg <em t="t">{k}^{C}$ in the voxel map $V</em>$}^{S}$; (c) the agent's pose $P_{t}$; and (d) a vector representation of the instruction $\phi^{L}$. It outputs a 3D mask $\arg _{k}^{M} \in$ $[0,1]^{W \times L \times H}$ that identifies the subgoal argument object. Formally, the computation is: ${ }^{5</p>
<p>$$
\begin{aligned}
\operatorname{REFINER}\left(\mathbf{x}<em t="t">{t}^{e g o},\left[V</em>\right]}^{S<em k="k">{\left[\arg </em>\right)= &amp; \
&amp; \operatorname{AlloTransform}\left(\operatorname{LingUNET}}^{C}\right)}, P_{t}, \phi^{L<em t="t">{m}\left(\mathbf{x}</em>\right]}^{e g o}, \phi^{L}\right), P_{t}\right) \otimes\left[V_{t}^{S<em k="k">{\left(\arg </em>
\end{aligned}
$$}^{C}\right)</p>
<p>where AlloTRANSFORM transforms a spatial 2D map from an egocentric to the global reference frame, and $\operatorname{LingUNET}<em m="m">{m}$ is a language-conditioned image-to-image encoder-decoder [36]. The architecture of $\operatorname{LingUNET}</em>$ is illustrated in Figure 6.</p>
<h1>A. 6 Low-Level Controller Details</h1>
<p>We describe the implementation of each of the low-level controller procedures. This implementation is not the focus of this paper, and could be improved or replaced with other algorithms. Some of the procedures cause actions in the AI2Thor environment, others simply process data to pass between procedures.
The procedures are NavigateTo, SampleExplorationPosition, SampleInteractionPose, and InteractMask. The low-level controller receives the subgoal $g_{k}$, and follows a pre-specified execution flow across multiple timesteps to complete it. The execution flow (Figure 7) consists of an exploration and interaction phase. In the exploration phase, we perform a $360^{\circ}$ rotation by generating a sequence of three RotateLeft actions to observe the environment and add information to the semantic map. If the semantic map indicates that no object of type $\arg <em t="t">{k}^{C}$, the action argument, is observed, we explore the environment by sampling a position $(x, y)=$ SampleExplorationPosition $\left(\hat{s}</em>}\right)$, navigating there using NavigateTo $\left(x, y, \hat{s<em t="t">{t}\right)$, and performing another $360^{\circ}$ rotation. We repeat this process until a voxel in $V</em>$ contains the class $\arg }^{S<em y="y">{k}^{C}$ with $&gt;50 \%$ probability, at which point we move on to the interaction phase. In the interaction phase, we sample an interaction pose $\left(x, y, \omega</em>}, \omega_{p}\right)=$ SampleInteractionPose $\left(\hat{s<em k="k">{t}, g</em>}\right)$, invoke NavigateTo $\left(x, y, \hat{s<em y="y">{t}\right)$ to reach the position ( $\mathrm{x}, \mathrm{y}$ ), and rotate according to yaw and pitch angles $\left(\omega</em>}, \omega_{p}\right)$. Finally, we generate the egocentric interaction action mask $\operatorname{mask<em t="t">{t}=$ $\operatorname{InteractMask}\left(\hat{s}</em>, \arg <em k="k">{k}^{M}\right)$, and execute the interaction action $\left(\right.$ type $\left.</em>}, \operatorname{mask<em _PASS="{PASS" _text="\text">{t}\right)$ in the ALFRED environment. We output $a</em>$ depending if the interaction action has succeeded, and pass control back to the high-level controller to sample the next subgoal.}}$ or $a_{\text {FAIL }</p>
<h2>A.6.1 NavigateTo Procedure</h2>
<p>At time $t$, the NavigateTo procedure maps a 2D navigation goal position $(x, y)$ and the state representation $\hat{s}<em _STOP="{STOP" _text="\text">{t}$ to one of the actions: ${$ RotateLeft, Rotateright, MoveAhead, $a</em>$ could be used as well.}}}$. We implement it with a Value Iteration Network [VIN; 57] that solves a 2D grid-MDP to predict navigation actions using fast GPU-accelerated convolution and max-pooling operations. The VIN parameters are pre-defined, and not learned. Other motion planners such as $\mathrm{A}^{*</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Illustration of the low-level controller execution flow, showing the order in which procedures are used to complete a subgoal $g_{k}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Current Heading</th>
<th style="text-align: center;">VIN Action</th>
<th style="text-align: center;">AI2Thor Action</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">North</td>
<td style="text-align: center;">West</td>
<td style="text-align: center;">RotateLeft</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">North</td>
<td style="text-align: center;">MoveAhead</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAST or SOUTH</td>
<td style="text-align: center;">RotateRight</td>
</tr>
<tr>
<td style="text-align: center;">East</td>
<td style="text-align: center;">North</td>
<td style="text-align: center;">RotateLeft</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EAST</td>
<td style="text-align: center;">MoveAhead</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">South or West</td>
<td style="text-align: center;">RotateRight</td>
</tr>
<tr>
<td style="text-align: center;">South</td>
<td style="text-align: center;">EAST</td>
<td style="text-align: center;">RotateLeft</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">South</td>
<td style="text-align: center;">MoveAhead</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">West or North</td>
<td style="text-align: center;">RotateRight</td>
</tr>
<tr>
<td style="text-align: center;">West</td>
<td style="text-align: center;">South</td>
<td style="text-align: center;">RotateLeft</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">West</td>
<td style="text-align: center;">MoveAhead</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">North or EAST</td>
<td style="text-align: center;">RotateRight</td>
</tr>
</tbody>
</table>
<p>Table 4: Mapping from VIN actions to AI2Thor actions.
The VIN is defined by a state-space $\mathcal{S}^{v i n}$, action space $\mathcal{A}^{v i n}$, transition function $\mathcal{T}^{v i n}: \mathcal{S}^{v i n} \times$ $\mathcal{A}^{v i n} \rightarrow \mathcal{S}^{v i n}$, a reward function $R^{v i n}: \mathcal{S}^{v i n} \times \mathcal{A}^{v i n} \rightarrow \mathcal{A}^{v i n}$, and terminal state set $\mathcal{M}^{v i n}$. At each timestep $t$, VIN performs value iteration to compute the Q-function $Q^{v i n}: \mathcal{S}^{v i n} \times \mathcal{A}^{v i n} \rightarrow \mathbb{R}$ that estimates the expected sum of future discounted rewards for taking action $a_{t}^{v i n} \in \mathcal{A}^{v i n}$ in state $s_{t}^{v i n} \in \mathcal{S}^{v i n}$, and thereafter following a greedy policy: $a_{i}^{v i n}=\arg \max <em i="i">{a^{v i n} \in \mathcal{A}^{v i n}} Q\left(s</em>\right)&gt;0.5\right)\right}$.
The reward function assigns different rewards for visiting states with different attributes:}^{v i n}, a^{v i n}\right)$, $i&gt;t$. We implement the state space $\mathcal{S}^{v i n}$ as a 2 D grid of shape $W^{v i n} \times H^{v i n}$. Each state $s^{v i n}$ is tagged with three 0-1 valued attributes: Obstacle, Unobserved, Goal. At each timestep $t$, we set the values of state attributes according to the most recent state representation $\hat{s}_{t}$ and current navigation goal $(x, y)$. States $s^{v i n}$ with occupied voxels in the height range $[0,1.75 m]$ are tagged $\operatorname{ObSTACLE}\left(s^{v i n}\right)=1$, otherwise $\operatorname{ObSTACLE}\left(s^{v i n}\right)=0$. States with all voxels unobserved are tagged Unobserved $\left(s^{v i n}\right)=1$, otherwise Unobserved $\left(s^{v i n}\right)=0$. The state at the goal position is tagged $\operatorname{Goal}\left(s^{v i n}\right)=1$, for all others $\operatorname{Goal}\left(s^{v i n}\right)=0$. The action space is $\mathcal{A}^{v i n}={$ North, EAST, South, West, Stop}. The transition function encodes epsilongreedy grid navigation dynamics: (1) the action NORTH moves the agent one state north and likewise for other actions, and (2) with probability $\epsilon=8 \%$ a random transition to a neighboring state occurs. Visiting any terminal state $s^{v i n} \in \mathcal{M}^{v i n}$ or executing the action Stop terminates the episode. Terminal states are all states tagged with attributes Obstacle and Goal, $\mathcal{M}^{v i n}=\left{s^{v i n} \in \mathcal{S}^{v i n} \mid\left(\operatorname{ObSTACLE}\left(s^{v i n}\right)&gt;0.5\right) \wedge\left(\operatorname{Goal}\left(s^{v i n</p>
<p>$$
\begin{aligned}
&amp; R^{v i n}\left(s^{v i n}, a^{v i n}\right)=-0.9 \cdot \operatorname{ObSTACLE}\left(s^{v i n}\right)+1.0 \cdot \operatorname{Goal}\left(s^{v i n}\right) \
&amp;-0.02 \cdot \operatorname{Unobserved}\left(s^{v i n}\right)+0.001 \cdot \mathbb{1}_{a^{v i n}=\text { Stop }} .
\end{aligned}
$$</p>
<p>ObSTACLE states receive reward -0.9 , GOAL states receive reward 1.0 , and Unobserved states receive reward -0.02 . Taking the STOP action in any state gives reward 0.001 , which has the effect of the agent stopping in unsolvable cases. We use the VIN iteratively for $N^{v i n}$ iterations, and predict an action $a^{v i n}=\arg \max <em t="t">{a^{v i n} \in \mathcal{A}^{v i n}}\left(Q^{v i n}\left(s</em>$ to a single valid AI2Thor navigation action using a deterministic mapping (Table 4).}^{v i n}, a^{v i n}\right)\right.$. We map from the VIN action $a^{v i n</p>
<h1>A.6.2 SampleExplorationPosition</h1>
<p>The SampleExplorationPosition procedure maps a state representation $\hat{s}<em s="s">{t}$ to a discrete 2D position $p^{\text {explore }}=(x, y)$. Let $\mathcal{P}</em>}$ be the set of 2D positions corresponding to voxel centroids in the voxel map along the horizontal axes, and the ground set $\mathcal{P<em f="f">{g}$ as the set of all unoccupied positions that have the class FLOOR or RUG in at least one voxel. A position is unoccupied if all voxels in the height range $[0,1.75 m]$ are free of obstacles. We define a frontier set $\mathcal{P}</em>}$ as the set of all positions $\mathcal{P<em f="f">{g}$ for which at least one immediately neighboring position contains zero observed voxels. If $\mathcal{P}</em>}$ is non-empty, we sample the position $p^{\text {explore }}$ uniformly at random from $\mathcal{P<em g="g">{f}$. Otherwise, we sample $p^{\text {explore }}$ uniformly at random from $\mathcal{P}</em>$.</p>
<h2>A.6.3 SampleInteractionPose</h2>
<p>The SampleInteractionPose procedure maps the state representation $\hat{s}<em k="k">{t}$ and subgoal $g</em>=$ $\left(\right.$ type $<em k="k">{k}, \operatorname{arg}</em>}^{C}, \operatorname{arg<em y="y">{k}^{M}$ ) to a pose $P=\left(x, y, \omega</em>$ is the agent camera pitch angle. The pose is predicted such that upon reaching it, the interaction action of type type $}, \omega_{p}\right)$, where $(x, y)$ is a discrete 2 D position, $\omega_{y}$ is the agent yaw angle, and $\omega_{p<em k="k">{k}$ is likely to succeed on the object of class $\arg </em>$ at location identified by the mask $\arg }^{C<em p="p">{k}^{M}$.
We use a neural network model NAVMODEL to predict expected pitch $\mathbb{E}\left(\omega</em>} \mid x, y ; g_{k}, \hat{s<em y="y">{t}\right)$ and a distribution $P\left(x, y, \omega</em>\right)$, factored as:} \mid g_{k}, \hat{s}_{t</p>
<p>$$
P\left(x, y, \omega_{y} \mid g_{k}, \hat{s}<em y="y">{t}\right)=P\left(\omega</em>} \mid x, y ; g_{k}, \hat{s<em k="k">{t}\right) P\left(x, y \mid g</em>\right)
$$}, \hat{s}_{t</p>
<p>The network NAVMODEL is based on the LingUNet architecture (Figure 6):</p>
<p>$$
\begin{aligned}
\operatorname{NAVMODEL}\left(\hat{s}<em k="k">{t}, g</em>\right)= &amp; \
&amp; \operatorname{LingUNet}\left(\operatorname{AFFORD}\left(\hat{s}<em T="T">{t}\right), \operatorname{LineAR}\left(\left[\operatorname{LUT}</em>}\left(\operatorname{type<em C="C">{k}\right) ; \operatorname{LUT}</em>\right)\right]\right)\right)
\end{aligned}
$$}\left(\arg _{k}^{C</p>
<p>where AFFORD is an affordance feature map (Section 4.1), LINEAR is a linear layer with bias, $\mathrm{LUT}<em C="C">{T}$ and $\mathrm{LUT}</em>$ are embedding lookup tables, and $[\cdot ; \cdot]$ is a vector concatenation.
To sample a pose $P$, we first sample a position $(x, y) \sim P\left(x, y \mid g_{k}, \hat{s}<em y="y">{t}\right)$, then sample a yaw angle $\omega</em>} \sim P\left(\omega_{y} \mid x, y ; g_{k}, \hat{s<em p="p">{t}\right)$, and finally lookup a pitch angle $\omega</em>\right)$.}=\mathbb{E}\left(\omega_{p} \mid x, y ; g_{k}, \hat{s}_{t</p>
<h2>A.6.4 InteractionMask</h2>
<p>The InteractionMask procedure maps a state representation $\hat{s}<em t__i="t_{i">{t}=\left(V</em>=$ $\left(\right.$ type $}}^{S}, V_{t}^{O}, v_{t}^{S}, P_{t}\right)$, the most recent RGB observation $I_{t}$, the most recent predicted segmentation $I_{t}^{S}$, and a subgoal $g_{k<em k="k">{k}, \operatorname{arg}</em>}^{C}, \operatorname{arg<em t="t">{k}^{M}$ ) to a 0-1 valued mask $\operatorname{mask}</em>$ is in the format expected by ALFRED. Formally, it is computed in three steps:} \in[0,1]^{H \times M}$ that identifies the interaction object in the first-person view observation. The interaction mask mask $_{t</p>
<p>$$
\begin{aligned}
\operatorname{mask}<em t="t">{t}^{A} &amp; =\left[I</em>\right]}^{S<em k="k">{\arg </em> \
\operatorname{mask}}^{C}<em k="k">{t}^{B} &amp; =\operatorname{PINHOLECAM}\left(\arg </em>\right) \
\operatorname{mask}}^{M}, P_{t<em t="t">{t} &amp; =\operatorname{mask}</em>
\end{aligned}
$$}^{A} \cdot \operatorname{mask}_{t}^{B</p>
<p>where PINHOLECAM projects the 0-1 valued 3D voxel map $\arg <em t="t">{k}^{M}$ to the agent's camera plane according to the pose $P</em>}$. The mask mask ${ <em k="k">{t}^{A}$ is an egocentric 0-1 valued mask that identifies all objects of class $\arg </em>}^{C}$ in the image $I_{t}$. The mask ${ <em k="k">{t}^{B}$ is an egocentric 0-1 valued mask that identifies the voxels $\arg </em>}^{M}$. For each pixel $(u, v)$, the value $\left[\operatorname{mask<em _u_="(u," v_="v)">{t}^{B}\right]</em>$ is the maximum of all values $\left[\arg <em _x_="(x," y_="y," z_="z)">{k}^{M}\right]</em>$ over voxels with coordinates $(x, y, z)$ that the ray cast from the camera through the pixel $(u, v)$ intersects with. The final mask mask $<em k="k">{t}$ is a 0-1 valued mask that identifies not only the correct object class, but also the correct instance according to the voxel mask $\arg </em>$.}^{M</p>
<h2>A. 7 Additional Learning Details</h2>
<h2>A.7.1 Observation Model Learning</h2>
<p>Data As described in Section 5, we use a perception dataset $\mathcal{D}^{P}$ for training depth and segmentation models. The dataset $\mathcal{D}^{P}=\left{\left([I]^{(i)},[I^{D}]^{(i)},[I^{S}]^{(i)}\right}<em P="P">{i=1}^{N</em>\right.$ with ground}}\right.$ includes RGB images $\left.[I]^{(i)</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Examples of images produced with our Augment procedure. The top row shows raw RGB images from ALFRED. The bottom row shows images generated by our segmentation-aware data augmentation method. Objects like walls, sinks, floors, and furniture randomly change color, while the apple and the spoons do not.
truth depth $\left[I^{D}\right]^{(i)}$ and segmentation $\left[I^{S}\right]^{(i)}$. The ground truth depth $\left[I^{D}\right]^{(i)}$ at each pixel $(u, v)$ is a distribution $\left[I^{D}\right]_{((u, v))}^{(i)}$ over $B$ depth bins, where $100 \%$ of the probability mass is assigned to the bin containing the reference depth value. The ground truth segmentation $\left[I^{S}\right]^{(i)}$ is likewise at each pixel $(u, v)$ a one-hot vector indicating the object class that pixel belongs to.</p>
<p>Data Augmentation The ALFRED dataset consists of 108 different training scenes, where each scene has a fixed furniture and light fixtures. Observations are highly correlated within each scene, which greatly reduces the effective size of the perception dataset and hurts generalization to unseen scenes. We use a custom segmentation-aware data augmentation strategy that increases the diversity of RGB observations.
We compute an augmented RGB image $\tilde{I}=\operatorname{AUGMENT}\left(I, I^{S}, O_{v}\right)$ that maps the image $I$, groundtruth segmentation $I^{S}$, and a set of semantic classes $O_{v}$ to a new image $\tilde{I}$. $O_{v}$ is the set of object classes that are likely to appear in different colors. $O_{v}$ includes classes like Floor, CounterTop, Cabinet, Vase, SoapBottle, AlarmClock that come in different designs and colors, but not classes like Banana, Apple, Spoon that tend to have even appearance. Algorithm 2 shows the implementation of Augment. It emulates more diverse environments by applying a different random color offset to each object class in the RGB image. Figure 8 shows examples of images produced with this augmentation procedure.
During training, we apply Augment with 50\% probability to each training example. Additionally, with $50 \%$ probability we perform a horizontal flip.</p>
<h1>A. 8 Additional Experimental Details</h1>
<p>We collect a training dataset of language-demonstration pairs as described in Section 5. The demonstrations in ALFRED typically navigate while looking down at the floor, likely a side-effect of the PDDL planner that had access to the world state during data generation, and as such has no need to explore or observe the visual environment. We modify the demonstration trajectories to get more informative first-person observations. First, we insert four RotateLeft actions at the start of each trajectory. Second, we maintain a nominal camera pitch angle of $30^{\circ}$ during navigation, by inserting LookDown and LookUp actions before and after every interaction action. We discard trajectories for which these modifications cause failures. These modifications result in observations that are more useful for learning and constructing our persistent spatial representation.</p>
<h2>A. 9 Hyperparameters</h2>
<p>Table 5 shows hyperparameter values. The hyperparameters were hand-tuned on the validation unseen split.</p>
<h2>A. 10 Additional Results</h2>
<p>Additional qualitative results are available at: https://hlsm-alfred.github.io/.</p>
<h1>Algorithm 2 AUGMENT</h1>
<p>Input: RGB Image $I$, ground truth segmentation $I^{S}$, set of object classes $O_{v}$.
1: $\hat{I} \leftarrow I$
2: for $c \in O_{v}$ do
3: $\quad$ Extract a binary mask corresponding to object class $c$
4: $\quad M_{c} \leftarrow\left|I^{S}\right|<em c="c">{(c)}$
5: &gt; Apply modifications to the image, masked by the segmentation mask $M</em>$
6: &gt; $\odot$ multiplies a $N$-dimensional vector with a $H \times W$ tensor to compute a $N \times H \times W$ tensor
7: &gt; is an elementwise multiplication
8: if randomBernoulli $(0.5)$ then
9: &gt; Sample an additive color offset for class $c$ from a normal distribution.
10: $\quad&gt;I_{3}$ is the $3 \times 3$ identity matrix.
11: $\quad a \sim N\left(\overrightarrow{\mathbf{0}}, \sigma_{n} I_{3}\right)$
12: $\quad \hat{I} \leftarrow \hat{I}+a \odot M_{c}$
13: if randomBernoulli $(0.5)$ then
14: &gt; Sample additive gaussian noise for each pixel $(u, v)$ for class $c$
15: for each pixel $(u, v)$ do
16: $\quad g_{u, v} \sim N\left(1, \sigma_{g}\right)$
17: $\quad|\hat{I}|<em _u_="(u," v_="v)">{(u, v)} \leftarrow\left|\hat{I}\right|</em>\right|}+g_{u, v} \cdot\left|M_{c<em m="m">{(u, v)}$
18: if randomBernoulli $(0.5)$ then
19: &gt; Sample an multiplicative color offset for class $c$ from a normal distribution
20: $\quad m \sim N\left(\overrightarrow{\mathbf{0}}, \sigma</em>\right)$
21: $\quad \hat{I} \leftarrow \hat{I} \cdot\left(m \odot M_{c}\right)$
22: clamp image $\hat{I}$ within 0-1 bounds
23: return $\hat{I}$
A successful example of task execution is available at: https://drive.google.com/file/d/ 1APKe3cR_-vliyU2elT5Un30w7PvkEdYs/view?usp=sharing
A failed example of task execution is available at: https://drive.google.com/file/d/1j8BJ_ ALoXGyf8a-IOkmQAg38awSWYt6f/view?usp=sharing} I_{3</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5} \otimes$ is an operation that multiplies a $W \times L$ matrix by a $W \times L \times H$ tensor to obtain a $W \times L \times H$ tensor&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6826 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6826</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6826</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-272880951</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.16461v1.pdf" target="_blank">Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is a fundamental task in natural language processing that presents significant challenges to Large Language Models (LLMs). The inherent characteristics of logical reasoning makes it well-suited for symbolic representations such as first-order logic (FOL). Research in symbolic logical reasoning explored FOL generation using state-of-the-art LLMs (i.e., GPT-4) to produce FOL translations of natural language (NL) statements, but errors in translation are usually not the focus. We address this by categorizing the translation errors in FOL statements generated by LLMs. To make progress towards improving the quality of FOL translations for smaller language models such as LLaMA-2 13B and Mistral 7B, we create ProofFOL, a high-quality FOL-annotated subset of ProofWriter dataset using GPT-4o. The models fine-tuned on this silver standard data achieve a significant gain in performance when compared to larger language models such as LLaMA-2 70B. In addition to improving the model using large data, we also tackle the issue of data scarcity and introduce an incremental framework encompassing of data augmentation and verification steps. In the augmentation process, a single pair of (premises, conclusion) is split into multiple new instances based on the predicates and FOLs. This data is used for fine-tuning, and the inference on this model generates FOLs with fewer errors over the model trained on the original data. Our investigation on the translation errors leads to generation of a perturbation dataset, which is used to train a verifier that corrects potential syntactic and semantic FOL translation errors. We demonstrate an efficient method for making the most of a limited existing human-annotated dataset. Our results show state-of-the-art performance for ProofWriter and ProntoQA datasets using ProofFOL on LLaMA-2 and Mistral models.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6826.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6826.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROOFFOL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROOFFOL (PROOF-FOL): FOL-annotated subset of ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A silver-standard dataset of NL (premises, conclusion) pairs with corresponding first-order logic (FOL) translations created by prompting GPT-4o and filtered/validated with Prover9; contains 10,424 retained examples used to fine-tune LLMs for NL-to-FOL translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PROOFFOL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset (silver-standard) generated from ProofWriter training records via GPT-4o generation plus Prover9-based validation and parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>10424 examples</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>dataset for NL-to-FOL supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Derived from ProofWriter (depth-5 subset) using GPT-4o generations validated by Prover9; mixed with FOLIO for some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>N/A (dataset for training NL-to-FOL translation models and verifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Prover9 used to parse and validate generated FOL translations and to filter examples whose solver outputs did not match ProofWriter ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Designed for use with ProofWriter / FOLIO / ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Passage-level multi-premise deductive reasoning examples (converted to FOL) suitable for training NL-to-FOL translation models.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL-to-FOL translation; deduction (first-order entailment verification via prover)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>N/A (dataset resource)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Provides large-scale, solver-validated training data (10.4k retained examples) that allows smaller LLMs (LLaMA-2 13B, Mistral 7B) fine-tuned on it to outperform larger baselines on NL-to-FOL translation + logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Silver-standard (GPT-4o generated) so may still contain subtle semantic mismatches; retained only examples where Prover9 outputs matched ProofWriter labels (70% retention of generations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6826.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6826.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B (PROOFFOL SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 13B fine-tuned on PROOFFOL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13-billion parameter LLaMA-2 family decoder model fine-tuned (LoRA, 3 epochs) on PROOFFOL for NL-to-FOL translation and deductive reasoning; used with both vanilla and incremental inference and with predicate/FOL verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (LLaMA-2 family) adapted via LoRA and 8-bit inference; fine-tuned for NL-to-FOL outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer decoder (LLaMA-2) + LoRA fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>PROOFFOL (10k examples subset reported), mixed with small human FOL dataset (FOLIO) in some settings; also incremental augmented splits of same.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Explicit NL-to-FOL translation followed by external theorem proving (Prover9); supervised fine-tuning (SFT) and incremental generation + verifier correction.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Prover9 used to validate generated FOLs; generated FOLs are passed to Prover9 for concluding True/False/Unknown and to detect syntax errors during training-data filtering and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, FOLIO, ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Deductive multi-premise reasoning benchmarks where conclusions are verified given premises; ProofWriter/ProntoQA are synthetic multi-hop deductive datasets and FOLIO is human-annotated NL-to-FOL dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL-to-FOL translation; first-order deductive entailment (conclusion verification via prover)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (conclusion truth classification after prover or direct standard output accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ProofWriter: 86% accuracy reported for LLaMA-2 13B fine-tuned on PROOFFOL; FOLIO: improved from ~24% (ICL) to ~34% after fine-tuning (paper text reports 'improve from 24% to 34%'). Incremental SFT (1k→augmented) yields FOLIO ~32.02% (incremental) and Offline verifier + incremental yields ~37.44% (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>After fine-tuning on PROOFFOL, LLaMA-2 13B matches or surpasses larger LLaMA-2 70B on some tasks (e.g., FOLIO improved to be on-par with 70B); incremental+verifier setup outperforms LLaMA-2 70B ICL (37.44% vs 34.97% on FOLIO).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller transformer models (13B) can outperform or match much larger baselines when fine-tuned on solver-validated NL-to-FOL data; incremental fine-tuning and verifier modules substantially improve accuracy on data-scarce benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Fine-tuning can overfit (noted for LLaMA-2 on ProntoQA when increasing dataset from 5k to 10k); still susceptible to syntactic and semantic FOL translation errors without verifier; requires external prover for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6826.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6826.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B (PROOFFOL SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B fine-tuned on PROOFFOL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion parameter Mistral family decoder model fine-tuned on PROOFFOL; reported to achieve state-of-the-art FOL generation on ProofWriter and to outperform baselines on ProntoQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (Mistral family) fine-tuned with LoRA; lightweight 7B model used with 8-bit inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer decoder (Mistral) + LoRA fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>PROOFFOL (silver-standard) and mixture with FOLIO in some settings for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>NL-to-FOL translation (supervised fine-tuning) with external prover verification; used incremental techniques and verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Prover9 used to validate FOLs and check correctness during filtering and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>ProofWriter: multi-premise synthetic deductive reasoning; ProntoQA: related test-only benchmark sampled from ProofWriter-like data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL-to-FOL translation and first-order deductive entailment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; syntax error counts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ProofWriter: reported as state-of-the-art in FOL generation after fine-tuning; reported to produce 0 syntax errors after fine-tuning on ProofWriter. ProntoQA: Mistral 7B fine-tuned on PROOFFOL 'outperforms all the ProntoQA baselines' (no single percentage provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms Mixtral 8×7B and larger models on FOL generation for ProofWriter in the paper's experiments; shows less overfitting than LLaMA-2 on ProntoQA.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Small, well-fine-tuned Mistral-7B can achieve SOTA FOL generation on ProofWriter and strong generalization to ProntoQA, with near-zero syntax errors post fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Exact numeric accuracy values rarely provided in text for some tasks; performance dependent on high-quality training data (PROOFFOL) and verification pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6826.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6826.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-70B (ICL baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 70B (in-context learning baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter LLaMA-2 large transformer used as a few-shot (in-context learning) baseline for NL-to-FOL and standard reasoning tasks in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large decoder-only transformer (Meta LLaMA-2) used with few-shot prompting (ICL) as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer decoder (LLaMA-2) few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained at scale (not modified in this paper); used few-shot examples sampled from datasets for in-context learning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>In-context learning for standard free-form generation and for few-shot FOL generation; outputs either direct conclusions (Standard) or FOL translations passed to Prover9 (FOL).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>When evaluating FOL-style outputs, generated FOLs are validated/executed using Prover9; baseline also compared in terms of syntax/semantic error counts reported by Prover9.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, FOLIO, ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same deductive reasoning benchmarks used across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL-to-FOL translation; first-order deductive entailment (via prover) and standard free-form reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples from paper: LLaMA-2 70B ICL reported FOL-based accuracy on FOLIO ≈ 34.97% (paper references this value when comparing to incremental+verifier result); various standard/FOL accuracies throughout Table 1 show LLaMA-2 70B generally higher than smaller models in few-shot ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Serves as the large-model baseline; smaller models fine-tuned on PROOFFOL (13B/7B) sometimes match or exceed 70B baseline after SFT and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large LLaMA-2 70B performs well in few-shot ICL but can be outperformed by smaller models fine-tuned on targeted, solver-validated FOL data; 70B still shows fewer errors in plain few-shot FOL generation prior to fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>ICL-based correction approaches relying on solver error messages are more effective for very large models (e.g., 175B), limited for smaller models; 70B still makes systematic translation errors as reported in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6826.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6826.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral 8×7B (ensemble/baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-like baseline composed of 8×7B Mistral-family models used for comparison (reported in experiments as a larger-capacity baseline for Mistral family).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral 8×7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregated/ensemble variant (Mixtral) composed of multiple 7B-scale models used as a stronger baseline in the Mistral family.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8 × 7B (logical ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer decoder ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained; used as baseline with few-shot or SFT settings from datasets in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Few-shot and SFT experiments producing FOL; compared for syntax/semantic error counts and accuracy on benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Prover9 used to execute/validate FOL outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FOLIO, ProofWriter, ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same deductive reasoning benchmarks; Mixtral reported highest few-shot FOLIO accuracy ~42% (paper: 'Mixtral model shows has the highest accuracy with FOLIO dataset using few-shot at 42%').</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL-to-FOL translation; deductive entailment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (few-shot and SFT comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>FOLIO few-shot accuracy reported ~42% for Mixtral (paper statement).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Mixtral few-shot outperforms many other few-shot baselines on FOLIO but smaller SFT models can close gap after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Larger ensembles/fusion of smaller models can yield higher few-shot accuracy on challenging datasets like FOLIO, but targeted SFT plus verification can allow single smaller models to match or exceed these baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Few-shot strength does not necessarily translate to best SFT performance; ensemble size/cost is large.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6826.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6826.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (data generator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o used to generate FOL translations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o (an OpenAI GPT-4 family variant) was used to generate NL-to-FOL translations for ProofWriter examples; its outputs were parsed and validated with Prover9 to create PROOFFOL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Very large, instruction-capable autoregressive transformer (OpenAI GPT-4 family); used here as a data-generation oracle to produce candidate FOL translations at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Large transformer (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on proprietary web-scale corpora (not specified in paper); used with 2-shot ProofWriter demonstrations for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Used as a generator of formal translations; not directly used in downstream provers except to create training data.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Generated FOLs were validated by Prover9 and filtered according to whether Prover9's inferred truth matched ProofWriter labels.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter (used as source of NL examples)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Source dataset for generation; GPT-4o generated FOLs for ProofWriter statements.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL-to-FOL translation (data generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Data retention rate after prover validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Approximately 70% of GPT-4o generated examples were retained after parsing/Prover9 validation to form PROOFFOL (paper states 'retention of 70% of the silver-standard data').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>GPT-4o used instead of human annotation to scale FOL data generation; yielded a large silver-standard dataset that produced strong SFT performance for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large LLM (GPT-4o) can generate high-quality FOL translations at scale, but requires solver-based validation and regex parsing to remove formatting errors; human gold data still valuable for mixing to avoid spurious GPT patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>GPT-4o outputs required heavy parsing/format normalisation and solver-based filtering; remaining outputs are still silver-standard (not human-verified) and may include subtle semantic mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6826.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6826.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-Large verifiers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-Large predicate and FOL verifier models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two T5-Large models fine-tuned as verifiers (one for predicates, one for FOL statements) on a perturbation dataset created from controlled syntactic/semantic errors and real SFT errors; used to verify and correct predicates and FOLs during inference (online or offline).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-Large (verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder transformer (Text-to-Text Transfer Transformer) fine-tuned to either classify 'correct' or to output corrected predicate/FOL text; trained for 10 epochs with AdamW (LR 5e-5).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-Large (model size ~770M parameters typical, not specified numerically in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Encoder-decoder transformer (T5) used as verifier/corrector</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Perturbation datasets derived from FOLIO and PROOFFOL (seeded by 1k examples each) that apply controlled syntactic and semantic perturbations plus real SFT-generated errors.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Verification + correction: classifier/generative correction to fix syntactic (missing parentheses/quantifiers) and semantic (wrong quantifiers, arity mismatches, predicate omissions) errors in FOL outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Verifier outputs are re-run through Prover9 for validation; verifier training used Prover9 to detect/label incorrect generations as part of perturbation extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FOLIO, ProofWriter (used for verifier training and evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Perturbation-driven verification tasks targeted at syntactic/semantic NL-to-FOL translation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Verification and correction of predicates and FOL statements for NL-to-FOL translation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative improvement in downstream reasoning accuracy when verifier applied</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports verifier mechanism produced a further ~17% improvement on FOLIO in addition to SFT gains; offline verifier + incremental achieved ~37.44% on FOLIO vs 34.97% LLaMA-2 70B ICL baseline (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Adding verifier to incremental SFT increased FOLIO accuracy substantially (reported +17% improvement in paper) and reduced syntax/semantic error rates in generated FOLs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Separate predicate and FOL verifiers trained on perturbed and real error instances effectively detect and correct both syntactic and semantic NL-to-FOL errors at inference time, improving end-task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Online verification can produce error cascades (an incorrect correction passed to later steps can create domino effects); verifiers require curated perturbation datasets and add inference overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6826.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6826.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prover9</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prover9 theorem prover</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated first-order theorem prover used to parse/execute generated FOL, return 'True/False/Unknown/None' and to provide syntactic error feedback used both to filter generated training data and to evaluate model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prover9</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Automated theorem prover that performs CNF conversion, skolemization, and proof search suitable for first-order logic; used programmatically to validate and check generated FOL statements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>First-order theorem prover / symbolic solver</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>N/A (symbolic tool)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Symbolic proof/search engine; returns verdicts and parsing/syntax errors which inform dataset filtering and verifier training.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Core external tool in the paper: used to (1) verify GPT-4o-generated FOL labels during PROOFFOL creation, (2) detect syntactic errors (parsing/type/token errors) and (3) compute truth values for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used to evaluate outputs on ProofWriter / FOLIO / ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Tool employed to evaluate first-order entailment by executing generated FOL on premises+conclusion pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Formal verification/execution of FOL translations; detection of syntax errors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used as evaluator; not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prover9's parser and truth outputs enabled automatic detection of syntax errors and helped create a large validated dataset from model-generated FOLs; however, semantic errors that still parse may pass unnoticed unless output truth mismatches ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Semantic errors that result in well-formed but incorrect FOL often pass through without parser error; requires ground-truth labels to detect some semantic mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6826.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6826.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Incremental SFT + Verifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Incremental supervised fine-tuning with data augmentation and on-the-fly verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-part approach combining (1) data augmentation by splitting each training record into predicate and per-statement FOL generation steps, (2) incremental inference that generates predicates then one FOL at a time, and (3) predicate/FOL verifiers (T5) applied online or offline to detect and correct errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Incremental SFT + Verifier (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Training-time augmentation creates n+2 records per original instance (predicates + successive FOLs); at inference the model generates predicates first and then generates one FOL at a time with verifiers optionally correcting intermediate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Applied to LLaMA-2 13B and Mistral 7B in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer SFT + encoder-decoder verifier + symbolic prover integration</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Augmented PROOFFOL or augmented FOLIO (data multiplication: ~7× for FOLIO, ~20× for ProofWriter subset), plus perturbation datasets for verifier training.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Structured NL-to-FOL generation (incremental generation) combined with symbolic verification (Prover9) and learned correction (T5 verifiers).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Prover9 checks outputs for syntax and truth during training-data filtering and for evaluation; verifiers use Prover9-derived labels and SFT model errors as perturbations to learn corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FOLIO, ProofWriter, ProntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Applied to deductive NL-to-FOL benchmarks to improve translation fidelity and downstream entailment accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL-to-FOL translation and first-order deductive entailment with online/offline correction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; syntax error counts; inference time (MM:SS per record)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>FOLIO: vanilla SFT (1k) ~22.66% accuracy; incremental SFT (augmented from 1k) ~32.02% (incremental) and with Offline verifier ~37.44% (paper Table 2). Paper reports incremental techniques improved predictive accuracy on FOLIO by ~41% and verifier added ~17% improvement overall on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Incremental SFT + verifier outperforms vanilla SFT on small-data regimes and can exceed large-model ICL baselines (e.g., Offline verifier result 37.44% vs LLaMA-2 70B ICL 34.97% on FOLIO).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Data augmentation via incremental splitting, combined with learned verifiers, produces large gains in translation correctness and downstream reasoning accuracy especially in data-scarce regimes; allows smaller models to match or exceed larger-model few-shot baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Online verifier mode can cause cascade/domino errors where a wrong correction propagates; incremental decoding + verification adds inference overhead; incremental methods can reduce passage-level coherence if not using final 'vanilla*' full-output training instances (vanilla* can cause hallucinations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating and Evaluating Multi-step Reasoning <em>(Rating: 2)</em></li>
                <li>FOLIO: Natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>ProntoQA: Language models are greedy reasoners: A systematic formal analysis of chain-of-thought <em>(Rating: 1)</em></li>
                <li>Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation <em>(Rating: 2)</em></li>
                <li>SatLM: Satisfiability-aided language models using declarative prompting <em>(Rating: 1)</em></li>
                <li>Neurologic decoding: (un) supervised neural text generation with predicate logic constraints <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6826",
    "paper_id": "paper-272880951",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "PROOFFOL",
            "name_full": "PROOFFOL (PROOF-FOL): FOL-annotated subset of ProofWriter",
            "brief_description": "A silver-standard dataset of NL (premises, conclusion) pairs with corresponding first-order logic (FOL) translations created by prompting GPT-4o and filtered/validated with Prover9; contains 10,424 retained examples used to fine-tune LLMs for NL-to-FOL translation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PROOFFOL",
            "model_description": "Dataset (silver-standard) generated from ProofWriter training records via GPT-4o generation plus Prover9-based validation and parsing.",
            "model_size": "10424 examples",
            "architecture_type": "dataset for NL-to-FOL supervised fine-tuning",
            "training_data": "Derived from ProofWriter (depth-5 subset) using GPT-4o generations validated by Prover9; mixed with FOLIO for some experiments.",
            "reasoning_method": "N/A (dataset for training NL-to-FOL translation models and verifiers)",
            "external_tool_used": true,
            "external_tool_description": "Prover9 used to parse and validate generated FOL translations and to filter examples whose solver outputs did not match ProofWriter ground truth.",
            "benchmark_name": "Designed for use with ProofWriter / FOLIO / ProntoQA",
            "benchmark_description": "Passage-level multi-premise deductive reasoning examples (converted to FOL) suitable for training NL-to-FOL translation models.",
            "task_type": "NL-to-FOL translation; deduction (first-order entailment verification via prover)",
            "performance_metric": "N/A (dataset resource)",
            "performance_value": "",
            "comparison_with_baseline": "",
            "key_findings": "Provides large-scale, solver-validated training data (10.4k retained examples) that allows smaller LLMs (LLaMA-2 13B, Mistral 7B) fine-tuned on it to outperform larger baselines on NL-to-FOL translation + logical reasoning tasks.",
            "limitations": "Silver-standard (GPT-4o generated) so may still contain subtle semantic mismatches; retained only examples where Prover9 outputs matched ProofWriter labels (70% retention of generations).",
            "uuid": "e6826.0",
            "source_info": {
                "paper_title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLaMA-2-13B (PROOFFOL SFT)",
            "name_full": "LLaMA-2 13B fine-tuned on PROOFFOL",
            "brief_description": "A 13-billion parameter LLaMA-2 family decoder model fine-tuned (LoRA, 3 epochs) on PROOFFOL for NL-to-FOL translation and deductive reasoning; used with both vanilla and incremental inference and with predicate/FOL verifiers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 13B",
            "model_description": "Decoder-only transformer (LLaMA-2 family) adapted via LoRA and 8-bit inference; fine-tuned for NL-to-FOL outputs.",
            "model_size": "13B",
            "architecture_type": "Transformer decoder (LLaMA-2) + LoRA fine-tuning",
            "training_data": "PROOFFOL (10k examples subset reported), mixed with small human FOL dataset (FOLIO) in some settings; also incremental augmented splits of same.",
            "reasoning_method": "Explicit NL-to-FOL translation followed by external theorem proving (Prover9); supervised fine-tuning (SFT) and incremental generation + verifier correction.",
            "external_tool_used": true,
            "external_tool_description": "Prover9 used to validate generated FOLs; generated FOLs are passed to Prover9 for concluding True/False/Unknown and to detect syntax errors during training-data filtering and evaluation.",
            "benchmark_name": "ProofWriter, FOLIO, ProntoQA",
            "benchmark_description": "Deductive multi-premise reasoning benchmarks where conclusions are verified given premises; ProofWriter/ProntoQA are synthetic multi-hop deductive datasets and FOLIO is human-annotated NL-to-FOL dataset.",
            "task_type": "NL-to-FOL translation; first-order deductive entailment (conclusion verification via prover)",
            "performance_metric": "Accuracy (conclusion truth classification after prover or direct standard output accuracy)",
            "performance_value": "ProofWriter: 86% accuracy reported for LLaMA-2 13B fine-tuned on PROOFFOL; FOLIO: improved from ~24% (ICL) to ~34% after fine-tuning (paper text reports 'improve from 24% to 34%'). Incremental SFT (1k→augmented) yields FOLIO ~32.02% (incremental) and Offline verifier + incremental yields ~37.44% (see Table 2).",
            "comparison_with_baseline": "After fine-tuning on PROOFFOL, LLaMA-2 13B matches or surpasses larger LLaMA-2 70B on some tasks (e.g., FOLIO improved to be on-par with 70B); incremental+verifier setup outperforms LLaMA-2 70B ICL (37.44% vs 34.97% on FOLIO).",
            "key_findings": "Smaller transformer models (13B) can outperform or match much larger baselines when fine-tuned on solver-validated NL-to-FOL data; incremental fine-tuning and verifier modules substantially improve accuracy on data-scarce benchmarks.",
            "limitations": "Fine-tuning can overfit (noted for LLaMA-2 on ProntoQA when increasing dataset from 5k to 10k); still susceptible to syntactic and semantic FOL translation errors without verifier; requires external prover for validation.",
            "uuid": "e6826.1",
            "source_info": {
                "paper_title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Mistral-7B (PROOFFOL SFT)",
            "name_full": "Mistral 7B fine-tuned on PROOFFOL",
            "brief_description": "A 7-billion parameter Mistral family decoder model fine-tuned on PROOFFOL; reported to achieve state-of-the-art FOL generation on ProofWriter and to outperform baselines on ProntoQA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral 7B",
            "model_description": "Decoder-only transformer (Mistral family) fine-tuned with LoRA; lightweight 7B model used with 8-bit inference.",
            "model_size": "7B",
            "architecture_type": "Transformer decoder (Mistral) + LoRA fine-tuning",
            "training_data": "PROOFFOL (silver-standard) and mixture with FOLIO in some settings for robustness.",
            "reasoning_method": "NL-to-FOL translation (supervised fine-tuning) with external prover verification; used incremental techniques and verifiers.",
            "external_tool_used": true,
            "external_tool_description": "Prover9 used to validate FOLs and check correctness during filtering and evaluation.",
            "benchmark_name": "ProofWriter, ProntoQA",
            "benchmark_description": "ProofWriter: multi-premise synthetic deductive reasoning; ProntoQA: related test-only benchmark sampled from ProofWriter-like data.",
            "task_type": "NL-to-FOL translation and first-order deductive entailment",
            "performance_metric": "Accuracy; syntax error counts",
            "performance_value": "ProofWriter: reported as state-of-the-art in FOL generation after fine-tuning; reported to produce 0 syntax errors after fine-tuning on ProofWriter. ProntoQA: Mistral 7B fine-tuned on PROOFFOL 'outperforms all the ProntoQA baselines' (no single percentage provided in text).",
            "comparison_with_baseline": "Outperforms Mixtral 8×7B and larger models on FOL generation for ProofWriter in the paper's experiments; shows less overfitting than LLaMA-2 on ProntoQA.",
            "key_findings": "Small, well-fine-tuned Mistral-7B can achieve SOTA FOL generation on ProofWriter and strong generalization to ProntoQA, with near-zero syntax errors post fine-tuning.",
            "limitations": "Exact numeric accuracy values rarely provided in text for some tasks; performance dependent on high-quality training data (PROOFFOL) and verification pipeline.",
            "uuid": "e6826.2",
            "source_info": {
                "paper_title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLaMA-2-70B (ICL baseline)",
            "name_full": "LLaMA-2 70B (in-context learning baseline)",
            "brief_description": "A 70B-parameter LLaMA-2 large transformer used as a few-shot (in-context learning) baseline for NL-to-FOL and standard reasoning tasks in the experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 70B",
            "model_description": "Large decoder-only transformer (Meta LLaMA-2) used with few-shot prompting (ICL) as baseline.",
            "model_size": "70B",
            "architecture_type": "Transformer decoder (LLaMA-2) few-shot prompting",
            "training_data": "Pretrained at scale (not modified in this paper); used few-shot examples sampled from datasets for in-context learning baselines.",
            "reasoning_method": "In-context learning for standard free-form generation and for few-shot FOL generation; outputs either direct conclusions (Standard) or FOL translations passed to Prover9 (FOL).",
            "external_tool_used": true,
            "external_tool_description": "When evaluating FOL-style outputs, generated FOLs are validated/executed using Prover9; baseline also compared in terms of syntax/semantic error counts reported by Prover9.",
            "benchmark_name": "ProofWriter, FOLIO, ProntoQA",
            "benchmark_description": "Same deductive reasoning benchmarks used across experiments.",
            "task_type": "NL-to-FOL translation; first-order deductive entailment (via prover) and standard free-form reasoning",
            "performance_metric": "Accuracy",
            "performance_value": "Examples from paper: LLaMA-2 70B ICL reported FOL-based accuracy on FOLIO ≈ 34.97% (paper references this value when comparing to incremental+verifier result); various standard/FOL accuracies throughout Table 1 show LLaMA-2 70B generally higher than smaller models in few-shot ICL.",
            "comparison_with_baseline": "Serves as the large-model baseline; smaller models fine-tuned on PROOFFOL (13B/7B) sometimes match or exceed 70B baseline after SFT and verification.",
            "key_findings": "Large LLaMA-2 70B performs well in few-shot ICL but can be outperformed by smaller models fine-tuned on targeted, solver-validated FOL data; 70B still shows fewer errors in plain few-shot FOL generation prior to fine-tuning.",
            "limitations": "ICL-based correction approaches relying on solver error messages are more effective for very large models (e.g., 175B), limited for smaller models; 70B still makes systematic translation errors as reported in related work.",
            "uuid": "e6826.3",
            "source_info": {
                "paper_title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Mixtral-8x7B",
            "name_full": "Mixtral 8×7B (ensemble/baseline)",
            "brief_description": "An ensemble-like baseline composed of 8×7B Mistral-family models used for comparison (reported in experiments as a larger-capacity baseline for Mistral family).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral 8×7B",
            "model_description": "Aggregated/ensemble variant (Mixtral) composed of multiple 7B-scale models used as a stronger baseline in the Mistral family.",
            "model_size": "8 × 7B (logical ensemble)",
            "architecture_type": "Transformer decoder ensemble",
            "training_data": "Pretrained; used as baseline with few-shot or SFT settings from datasets in the paper.",
            "reasoning_method": "Few-shot and SFT experiments producing FOL; compared for syntax/semantic error counts and accuracy on benchmarks.",
            "external_tool_used": true,
            "external_tool_description": "Prover9 used to execute/validate FOL outputs.",
            "benchmark_name": "FOLIO, ProofWriter, ProntoQA",
            "benchmark_description": "Same deductive reasoning benchmarks; Mixtral reported highest few-shot FOLIO accuracy ~42% (paper: 'Mixtral model shows has the highest accuracy with FOLIO dataset using few-shot at 42%').",
            "task_type": "NL-to-FOL translation; deductive entailment",
            "performance_metric": "Accuracy (few-shot and SFT comparisons)",
            "performance_value": "FOLIO few-shot accuracy reported ~42% for Mixtral (paper statement).",
            "comparison_with_baseline": "Mixtral few-shot outperforms many other few-shot baselines on FOLIO but smaller SFT models can close gap after fine-tuning.",
            "key_findings": "Larger ensembles/fusion of smaller models can yield higher few-shot accuracy on challenging datasets like FOLIO, but targeted SFT plus verification can allow single smaller models to match or exceed these baselines.",
            "limitations": "Few-shot strength does not necessarily translate to best SFT performance; ensemble size/cost is large.",
            "uuid": "e6826.4",
            "source_info": {
                "paper_title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-4o (data generator)",
            "name_full": "GPT-4o used to generate FOL translations",
            "brief_description": "GPT-4o (an OpenAI GPT-4 family variant) was used to generate NL-to-FOL translations for ProofWriter examples; its outputs were parsed and validated with Prover9 to create PROOFFOL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Very large, instruction-capable autoregressive transformer (OpenAI GPT-4 family); used here as a data-generation oracle to produce candidate FOL translations at scale.",
            "model_size": null,
            "architecture_type": "Large transformer (proprietary)",
            "training_data": "Pretrained on proprietary web-scale corpora (not specified in paper); used with 2-shot ProofWriter demonstrations for generation.",
            "reasoning_method": "Used as a generator of formal translations; not directly used in downstream provers except to create training data.",
            "external_tool_used": true,
            "external_tool_description": "Generated FOLs were validated by Prover9 and filtered according to whether Prover9's inferred truth matched ProofWriter labels.",
            "benchmark_name": "ProofWriter (used as source of NL examples)",
            "benchmark_description": "Source dataset for generation; GPT-4o generated FOLs for ProofWriter statements.",
            "task_type": "NL-to-FOL translation (data generation)",
            "performance_metric": "Data retention rate after prover validation",
            "performance_value": "Approximately 70% of GPT-4o generated examples were retained after parsing/Prover9 validation to form PROOFFOL (paper states 'retention of 70% of the silver-standard data').",
            "comparison_with_baseline": "GPT-4o used instead of human annotation to scale FOL data generation; yielded a large silver-standard dataset that produced strong SFT performance for smaller models.",
            "key_findings": "Large LLM (GPT-4o) can generate high-quality FOL translations at scale, but requires solver-based validation and regex parsing to remove formatting errors; human gold data still valuable for mixing to avoid spurious GPT patterns.",
            "limitations": "GPT-4o outputs required heavy parsing/format normalisation and solver-based filtering; remaining outputs are still silver-standard (not human-verified) and may include subtle semantic mistakes.",
            "uuid": "e6826.5",
            "source_info": {
                "paper_title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "T5-Large verifiers",
            "name_full": "T5-Large predicate and FOL verifier models",
            "brief_description": "Two T5-Large models fine-tuned as verifiers (one for predicates, one for FOL statements) on a perturbation dataset created from controlled syntactic/semantic errors and real SFT errors; used to verify and correct predicates and FOLs during inference (online or offline).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-Large (verifier)",
            "model_description": "Encoder-decoder transformer (Text-to-Text Transfer Transformer) fine-tuned to either classify 'correct' or to output corrected predicate/FOL text; trained for 10 epochs with AdamW (LR 5e-5).",
            "model_size": "T5-Large (model size ~770M parameters typical, not specified numerically in paper)",
            "architecture_type": "Encoder-decoder transformer (T5) used as verifier/corrector",
            "training_data": "Perturbation datasets derived from FOLIO and PROOFFOL (seeded by 1k examples each) that apply controlled syntactic and semantic perturbations plus real SFT-generated errors.",
            "reasoning_method": "Verification + correction: classifier/generative correction to fix syntactic (missing parentheses/quantifiers) and semantic (wrong quantifiers, arity mismatches, predicate omissions) errors in FOL outputs.",
            "external_tool_used": true,
            "external_tool_description": "Verifier outputs are re-run through Prover9 for validation; verifier training used Prover9 to detect/label incorrect generations as part of perturbation extraction.",
            "benchmark_name": "FOLIO, ProofWriter (used for verifier training and evaluation)",
            "benchmark_description": "Perturbation-driven verification tasks targeted at syntactic/semantic NL-to-FOL translation errors.",
            "task_type": "Verification and correction of predicates and FOL statements for NL-to-FOL translation",
            "performance_metric": "Relative improvement in downstream reasoning accuracy when verifier applied",
            "performance_value": "Paper reports verifier mechanism produced a further ~17% improvement on FOLIO in addition to SFT gains; offline verifier + incremental achieved ~37.44% on FOLIO vs 34.97% LLaMA-2 70B ICL baseline (Table 2).",
            "comparison_with_baseline": "Adding verifier to incremental SFT increased FOLIO accuracy substantially (reported +17% improvement in paper) and reduced syntax/semantic error rates in generated FOLs.",
            "key_findings": "Separate predicate and FOL verifiers trained on perturbed and real error instances effectively detect and correct both syntactic and semantic NL-to-FOL errors at inference time, improving end-task accuracy.",
            "limitations": "Online verification can produce error cascades (an incorrect correction passed to later steps can create domino effects); verifiers require curated perturbation datasets and add inference overhead.",
            "uuid": "e6826.6",
            "source_info": {
                "paper_title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Prover9",
            "name_full": "Prover9 theorem prover",
            "brief_description": "An automated first-order theorem prover used to parse/execute generated FOL, return 'True/False/Unknown/None' and to provide syntactic error feedback used both to filter generated training data and to evaluate model outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Prover9",
            "model_description": "Automated theorem prover that performs CNF conversion, skolemization, and proof search suitable for first-order logic; used programmatically to validate and check generated FOL statements.",
            "model_size": null,
            "architecture_type": "First-order theorem prover / symbolic solver",
            "training_data": "N/A (symbolic tool)",
            "reasoning_method": "Symbolic proof/search engine; returns verdicts and parsing/syntax errors which inform dataset filtering and verifier training.",
            "external_tool_used": null,
            "external_tool_description": "Core external tool in the paper: used to (1) verify GPT-4o-generated FOL labels during PROOFFOL creation, (2) detect syntactic errors (parsing/type/token errors) and (3) compute truth values for evaluation.",
            "benchmark_name": "Used to evaluate outputs on ProofWriter / FOLIO / ProntoQA",
            "benchmark_description": "Tool employed to evaluate first-order entailment by executing generated FOL on premises+conclusion pairs.",
            "task_type": "Formal verification/execution of FOL translations; detection of syntax errors",
            "performance_metric": "Used as evaluator; not applicable",
            "performance_value": "",
            "comparison_with_baseline": "",
            "key_findings": "Prover9's parser and truth outputs enabled automatic detection of syntax errors and helped create a large validated dataset from model-generated FOLs; however, semantic errors that still parse may pass unnoticed unless output truth mismatches ground truth.",
            "limitations": "Semantic errors that result in well-formed but incorrect FOL often pass through without parser error; requires ground-truth labels to detect some semantic mistakes.",
            "uuid": "e6826.7",
            "source_info": {
                "paper_title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Incremental SFT + Verifier",
            "name_full": "Incremental supervised fine-tuning with data augmentation and on-the-fly verification",
            "brief_description": "A three-part approach combining (1) data augmentation by splitting each training record into predicate and per-statement FOL generation steps, (2) incremental inference that generates predicates then one FOL at a time, and (3) predicate/FOL verifiers (T5) applied online or offline to detect and correct errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Incremental SFT + Verifier (method)",
            "model_description": "Training-time augmentation creates n+2 records per original instance (predicates + successive FOLs); at inference the model generates predicates first and then generates one FOL at a time with verifiers optionally correcting intermediate outputs.",
            "model_size": "Applied to LLaMA-2 13B and Mistral 7B in experiments",
            "architecture_type": "Transformer SFT + encoder-decoder verifier + symbolic prover integration",
            "training_data": "Augmented PROOFFOL or augmented FOLIO (data multiplication: ~7× for FOLIO, ~20× for ProofWriter subset), plus perturbation datasets for verifier training.",
            "reasoning_method": "Structured NL-to-FOL generation (incremental generation) combined with symbolic verification (Prover9) and learned correction (T5 verifiers).",
            "external_tool_used": true,
            "external_tool_description": "Prover9 checks outputs for syntax and truth during training-data filtering and for evaluation; verifiers use Prover9-derived labels and SFT model errors as perturbations to learn corrections.",
            "benchmark_name": "FOLIO, ProofWriter, ProntoQA",
            "benchmark_description": "Applied to deductive NL-to-FOL benchmarks to improve translation fidelity and downstream entailment accuracy.",
            "task_type": "NL-to-FOL translation and first-order deductive entailment with online/offline correction",
            "performance_metric": "Accuracy; syntax error counts; inference time (MM:SS per record)",
            "performance_value": "FOLIO: vanilla SFT (1k) ~22.66% accuracy; incremental SFT (augmented from 1k) ~32.02% (incremental) and with Offline verifier ~37.44% (paper Table 2). Paper reports incremental techniques improved predictive accuracy on FOLIO by ~41% and verifier added ~17% improvement overall on FOLIO.",
            "comparison_with_baseline": "Incremental SFT + verifier outperforms vanilla SFT on small-data regimes and can exceed large-model ICL baselines (e.g., Offline verifier result 37.44% vs LLaMA-2 70B ICL 34.97% on FOLIO).",
            "key_findings": "Data augmentation via incremental splitting, combined with learned verifiers, produces large gains in translation correctness and downstream reasoning accuracy especially in data-scarce regimes; allows smaller models to match or exceed larger-model few-shot baselines.",
            "limitations": "Online verifier mode can cause cascade/domino errors where a wrong correction propagates; incremental decoding + verification adds inference overhead; incremental methods can reduce passage-level coherence if not using final 'vanilla*' full-output training instances (vanilla* can cause hallucinations).",
            "uuid": "e6826.8",
            "source_info": {
                "paper_title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "ProofWriter: Generating and Evaluating Multi-step Reasoning",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_and_evaluating_multistep_reasoning"
        },
        {
            "paper_title": "FOLIO: Natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "ProntoQA: Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "rating": 1,
            "sanitized_title": "prontoqa_language_models_are_greedy_reasoners_a_systematic_formal_analysis_of_chainofthought"
        },
        {
            "paper_title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation",
            "rating": 2,
            "sanitized_title": "harnessing_the_power_of_large_language_models_for_natural_language_to_firstorder_logic_translation"
        },
        {
            "paper_title": "SatLM: Satisfiability-aided language models using declarative prompting",
            "rating": 1,
            "sanitized_title": "satlm_satisfiabilityaided_language_models_using_declarative_prompting"
        },
        {
            "paper_title": "Neurologic decoding: (un) supervised neural text generation with predicate logic constraints",
            "rating": 1,
            "sanitized_title": "neurologic_decoding_un_supervised_neural_text_generation_with_predicate_logic_constraints"
        }
    ],
    "cost": 0.019982,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification
24 Sep 2024</p>
<p>Ramya Keerthy Thatikonda 
Department of Data Science &amp; AI
Monash University</p>
<p>Jiuzhou Han 
Department of Data Science &amp; AI
Monash University</p>
<p>Wray Buntine 
College of Engineering and Computer Science
VinUniversity</p>
<p>Ehsan Shareghi 
Department of Data Science &amp; AI
Monash University</p>
<p>Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification
24 Sep 2024A5FE30548B4F02757873E2CF9F6E3449arXiv:2409.16461v1[cs.CL]
Logical reasoning is a fundamental task in natural language processing that presents significant challenges to Large Language Models (LLMs).The inherent characteristics of logical reasoning makes it well-suited for symbolic representations such as first-order logic (FOL).Research in symbolic logical reasoning explored FOL generation using state-of-the-art LLMs (i.e., GPT-4) to produce FOL translations of natural language (NL) statements, but errors in translation are usually not the focus.We address this by categorizing the translation errors in FOL statements generated by LLMs, specifically for deductive logical reasoning tasks.In order to make progress towards improving the quality of FOL translations for smaller language models such as LLaMA-2 13B and Mistral 7B, we create PROOFFOL, a high-quality FOL-annotated subset of ProofWriter dataset using GPT-4o.The models finetuned on this silver standard data achieve a significant gain in performance when compared to larger language models such as LLaMA-2 70B.In addition to improving the model using large data, we also tackle the issue of data scarcity and introduce an incremental framework encompassing of data augmentation and verification steps.In the augmentation process, a single pair of (premises, conclusion) is split into multiple new instances based on the predicates and FOLs.This data is used for fine-tuning, and the inference on this model generates FOLs with fewer errors over the model trained on the original data.Our investigation on the translation errors leads to generation of a perturbation dataset consisting of simulated NL-to-FOL translation errors and their corresponding corrections.This data is used to train a verifier, which corrects potential syntactic and semantic FOL translation errors.We demonstrate an efficient method for making the most of a limited existing human-annotated dataset.Our results show state-of-the-art performance for ProofWriter and ProntoQA datasets using PROOFFOL on LLaMA-2 and Mistral models. 1</p>
<p>Introduction</p>
<p>Recent state-of-the-art methods for logical reasoning from natural language (NL) descriptions work via translation (Pan et al. 2023;Ye et al. 2024;Olausson et al. 2023).An LLM is tasked to translate statements from NL to first order logic (FOL), which is then sent for execution to external SMT solvers such as Z3 (De Moura and Bjørner 2008) and Prover9 (McCune 2005).Recent work (Yang et al. 2024) has highlighted the systematic errors that even the most capable LLMs (such as GPT-4) make during translation of a single NL statement into its corresponding FOL.A realistic logical reasoning scenario is much more demanding as it involves several premise statements followed by a conclusion to be verified.This requires consistency of NL-to-FOL translations (e.g., in predicate naming or translation of logical operators) across several statements.However, very little is explored on the pattern of syntactic and semantic errors LLMs make during such translation scenario.</p>
<p>Existing approaches to reduce NL-to-FOL translation errors are of limited impact.They rely on the LLM's capability to understand and self-correct the translation inaccuracies only based on the error message from the external SMT solver (Pan et al. 2023), but the ability to comprehend an error message is often restricted to larger scale of models (i.e., 175B), and not possible in smaller LLMs (i.e., 7B, 13B).Also, while syntactic translation mistakes (e.g., misplaced operator: P ∧ → Q, or missing quantifiers: P (x) → Q(x)) result in run-time errors by tools, many semantic errors (e.g., use of incorrect quantifiers: All men are mortal.∃x(Man(x) → Mortal(x)) ) are passed through SMT tools without causing any run-time error, limiting the ability of such an approach to correct less trivial errors.A straightforward solution to reduce errors is to fine-tune the smaller models on large scale of NL-to-FOL translation data.However, the existing datasets offer very little support, with FO-LIO (Han et al. 2022) as the only human-annotated dataset to have around 1k examples of NL (premises, conclusion) and their FOL translations.MALLS (Yang et al. 2024) is another synthetic dataset which has 28k pair of single statements and FOL translations.Even in the presence of larger scale fine-tuning data, it is still desired to have a correction mechanism for smaller models that could catch both syntactic and semantic errors on-the-fly at inference time.</p>
<p>In this paper we combine a number of methods to reduce NL-to-FOL translation errors.First, to overcome the shortage of ground truth for fine-tuning, we utilise GPT-4o to create PROOFFOL, a new large corpus of (premises, conclusion) pairs and corresponding FOL translations.More concretely, we use the existing pairs of (premises, conclu-sion) of ProofWriter dataset (Tafjord, Mishra, and Clark 2020) and prompt GPT-4 to generate the corresponding FOL translations.To ensure the correctness of the FOL translations, we pass them through Prover9 and filter out examples that produce outputs (i.e., true, false, uncertain) that do not match the ground truth in ProofWriter.PROOFFOL is the largest existing dataset consisting of in 10424 examples of (premises, conclusion and their corresponding FOL translations.We show that fine-tuned LLaMA-2 13B (Touvron et al. 2023) and Mistral 7B (Jiang et al. 2023) models on PROOFFOL outperform larger baselines such as LLaMA-2 70B and Mixtral 8 × 7B, both in terms of translation quality and performance on logical reasoning task on ProofWriter and ProntoQA (Saparov and He 2022).</p>
<p>Second, to effectively utilize existing scarce but highquality human-generated data (i.e., FOLIO), we propose a set of incremental techniques for both the training and inference phases.More concretely, to increase the number of training instances, we split each record of FOLIO into multiple records for an incremental training process where the model is trained to first produce the predicates of premises and conclusions, and then to produce the FOL of each statements of premises and conclusion one by one.This model further improves the predictive accuracy on FOLIO by 41%.</p>
<p>Third and finally, to allow for fine-grained correction of semantic and syntactic errors at inference time, we train separate T5 (Raffel et al. 2020) models to exclusively verify predicates and FOLs on-the-fly, and apply necessary corrections when needed.In particular, we first define categories of semantic and syntactic errors made by LLMs during NLto-FOL Translation.To simulate these errors, we apply controlled perturbations on ground truth labels of FOLIO and create a new training set consisting of statements and their corresponding perturbed predicates and FOLs.Then, we train T5 models which take as input statements and their predicates, or statements and their FOLs, and either verifies their correctness (i.e., outputs correct) or outputs corrected versions.This added mechanism brought a further 17% improvement on FOLIO.</p>
<p>Our findings highlight that data is crucial for surpassing the current performance limits of several large language models, particularly when employing more accessible models for logical reasoning tasks.Our data generation pipeline allows us to create, PROOFFOL, the largest FOL-annotated logical reasoning dataset to this date.Our incremental training presents an effective data augmentation method particularly useful for data scarce conditions, while the verifier mechanism and corresponding training protocol offers a promising pathway to verify correctness of symbolic forms generated by LLMs at inference time.</p>
<p>NL-to-FOL Translation Errors</p>
<p>First-order logic (FOL) is a form of logic representation that covers the use of variables, functions, and quantifiers.FOLs are suitable for logical reasoning tasks involving natural language.Large Language Models (LLMs) are shown to be capable of translating natural language into various types of formal representations with varying degree of success.Among these formalism, NL-to-FOL translation presents unique syntactic (FOL syntax) and semantic (interpreting the meaning of the NL statements) challenges.We first attempt to categorize these syntactic and semantic errors in this section.This will serve as the basis for our data perturbation protocol to train FOL verifiers (presented shortly).</p>
<p>The translation of language to FOL follows grammatical rules and any deviation from the rules can cause syntax errors.For the rule "every free variable assigned to a predicate has a quantifier" can be applied to the statement "Green people are blue" with predicates 'green' and 'blue', a free variable 'x' and a quantifier '∀' to quantify the variable.A missing quantifier can be a parsing error in syntax for this rule.The Prover9 tool, used as a logical solver in our paper, is designed to send a feedback when encountered with syntax errors.We use this to detect and categorize the syntactic errors during translation into the following categories:</p>
<p>• Parsing errors: This occurs when the logical statement has missing or invalid operators, or missing parentheses.• Type errors: The tool returns none value when a sentence level discrepancy, e.g.missing quantifier, is encountered.• Token errors: This type of error is observed when an invalid token is part of the FOL, such as a $ symbol.</p>
<p>Semantic errors are generally not detectable as they follow the structure of formal language, but can be identified by observing the incorrect predictions of the parsable FOL statements.This process is tedious as it requires effort to scan through translations and point the occurrence of the error.An example of semantic error is when the language model predicts a wrong operator.In case of the sentence "All rabbits have fur", the FOL ∃x(Rabbit(x) → Have(x, fur)) incorrectly represents the quantifier of the sentence.Semantic errors are broadly categorized as • Sense errors: This is a general form of semantic error where the prediction from the tool is incorrect, pointing to inaccuracy between natural language and FOL.• Arities errors: This is a specific error thrown by a tool to indicate that there are duplicate predicate values with mismatch in number of arguments.</p>
<p>These errors are further studied manually to divide into more specific categories.Due to space constraints, we define them in detail in the Appendix.We provide an empirical distribution of these errors in Section and Figure 2.</p>
<p>Incremental Fine-Tuning and Verification Data Generation and Fine-tuning</p>
<p>The alignment of a language model to follow instructions for a specific task can be accomplished by fine-tuning on substantial data.The task of formal translations require firstorder logic of their natural language counterparts.Ideally, this task is at a passage level rather than sentence level, which makes it challenging for a language model to follow a required format.To overcome this, we need sufficient passage level translations, which are time-consuming to generate through human annotations.We introduce a streamlined process for generating this FOL data and ensuring correctness of the format, grammar, and order of the translations.</p>
<p>Data Generation For the data generation process, we pick ProofWriter which comes with large number of training records, each consisting of multipe premises and a conclusion, and variations in depth of reasoning.The format of the FOLs is set to be consistent with the "Prover9" theorem prover, which has a human understandable, formal language syntax. 2 The fixed format of demonstrations is adapted from Pan et al. (2023), where we generate predicates followed by first-order logic statements for each sentence.We change the formal language to Prover9 and keep it fixed for all datasets.</p>
<p>At a glance, the output from the GPT model has formatting issues, such as assigning numbers to each generation, explaining the task before generation, and solving for conclusion after producing translations.These issues are parsed using pattern matching to obtain the maximum number of translations.After the parsing stage, the syntax check is done by the Prover9, where the tool can provide unique feedback for each form of syntax error.When analysing these errors, we observed grammar rules that can be fixed in the tool to include unicode decoding, allow unordered quantifiers and support negation of a full formula.This addition of grammar rules minimized the penalization for translations.The semantic errors were identified by comparing the ground truth label from ProofWriter with Prover9-generated output based on FOLs.This systematic pipeline enabled the retention of 70% of the silver-standard data.We term this dataset PROOFFOL, which consists of 10424 pairs of (Premises, Conclusions), deduction label, and corresponding FOLs.</p>
<p>Supervised Fine-tuning (SFT) Each input x to the SFT is a set of premise statements P x (= {P 1 , P 2 , . . ., P n }), a conclusion C x , and an instruction (I) represented as [P x , C x , I].In order to avoid over-fitting the model to certain spurious patterns in GPT-4o translations, we include humangenerated dataset (FOLIO) in the mix.We create a set of models built on the full dataset, providing a perspective for model behaviour with size of the dataset.Since this SFT model employs both existing and synthetic data, it imposes a dilemma of the effect of gold-standard data on the results when compared to the generated silver-standard.To validate the reliability of our generated data, we fine-tune the model on a subset of the dataset and examine how increasing the data size impacts logical reasoning tasks.For a given input x, the output of SFT models are predicates of x (denoted as P red x ), and FOLs of its premises and conclusion, [P red x , F OL Px , F OL Cx ].For experiments, see Section .</p>
<p>Incremental Techniques</p>
<p>The SFT method described in the previous section uses the FOLIO dataset as just a small component of the overall approach.With high number of records associated with PROOFFOL, we can assume that the in-distribution dataset will show a major improvement when compared to FOLIO.</p>
<p>To enhance the performance of FOLIO dataset (and in general any similar data-scarce scenario), we introduce 'Incremental Techniques' for maximizing the use of limited data.These techniques encompass data augmentation, incremental fine-tuning and inference, and incremental verification of predicates and FOLs, creating a comprehensive setup for FOL generation.We further expand this method to a smaller subset of PROOFFOL to simulate data scarce environment.</p>
<p>Data Augmentation Supervised Fine-tuning a decoder model is technically an unannotated form of training as the supervised part of the fine-tuning refers to the label that is passed with the input.During a vanilla SFT process, we pass the whole output along with the input, and the model performs inference as a text completion task.This motivates our data augmentation method, where the model examines smaller part of the output rather than training on the whole output at a time.The FOL translation is one such task where the output sequence is lengthy and the model can deviate from generating the correct syntax.</p>
<p>To initiate the data augmentation process, we split the output of the original record to represent incremental data, where the first output is [P red x ], the second output is [P red x , F OL P1 ], and so on till we reach the full output [P red x , F OL P1 ..., F OL Pn , F OL Cx ].This splits a single record into n + 2 records, where n is the number of premises and '+2' is for predicate and conclusion generation.The input remains the same for all the records.This data augmentation increases the dataset size to about 7× and 20× for FOLIO and ProofWriter, respectively.With this enhanced data, we train a set of SFT models for FOL translation tasks.</p>
<p>Inference The SFT for augmented data is performed using two instructions, indicating two types of tasks.The first instruction is "Generate predicates for the given natural language sentences."to generate the predicates, and the second is "Given a premise and conclusion, generate the first order logic form of the premises and conclusion." to generate the FOL statements.At inference, we provide the model with the instruction to generate the predicates.These predicates, along with the input, are then passed to the model to infer the FOL statements.We categorize FOL inference into two forms.</p>
<p>• Vanilla Inference: In vanilla inference, the model is provided with the natural language statements and is asked to generate the predicates and the FOL translations for the complete input.• Incremental Inference: In incremental inference, we first generate the predicates, and then limit the generation to a single FOL translation at a time by setting a low maximum new token parameter.For every FOL generation, we pass the previously generated values as input.For example, if we are generating F OL P3 , the input would be
[P x , C x , P red x , F OL P1 , F OL P2 ].
Verification and Correction Since the incremental inference allows individual predicate and FOL generation, we train two verifiers; predicate and FOL, to detect and correct the potential errors.The term 'verifier' refers to both the verification and correction processes.• Predicate Verifier The Predicate verifier takes the [P x , C x , P red x ], as the input, and evaluates the predicted predicate P red x .If the verifier considers P red x correct, the output is just 'correct', otherwise the verifier will generate the corrected predicates set.To achieve this outcome, the verifier is trained on the perturbed predicates of the training dataset used for SFT.The perturbations for predicates are created based on the semantic errors related to predicates.Few perturbations used are; omitting predicate, omitting variable, adding variable, and adding duplicate predicate.The predicate perturbations are explained in detail in the Appendix.</p>
<p>• FOL Verifier</p>
<p>The FOL verifier takes [P red NL , F OL NL , NL] as the input, where F OL NL represents predicted FOL form of a single premise (P i ) or conclusion (C x ) statement (represented as NL).The verifier evaluates F OL NL as 'correct' or will generate a corrected FOL.Similar to predicate verifier, we use the original SFT training set and applied perturbations to train this Verifier model.The perturbations used in this method are crafted based on the common syntax and semantic errors in translations (Section ).Few perturbation used in this verifier are; changing quantifier position, omitting a quantifier, omitting parenthesis, tweaking negations, and replacing operators.We discuss these in detail in the Appendix.</p>
<p>In addition to these perturbation instances, we incorporate real errors from the SFT training data.Specifically, we run the inference of the training data on the SFT model and collect the predicate and FOL, which do not match the ground truth, as errors.We add these errors to the fully-controlled created perturbations to form the 'incorrect verifier training instances'.Finally, in order to verify 'correct' predictions, we take the predicate and FOL values from the ground-truth and label them as 'correct'.We generate the perturbation dataset from a seed of 1k examples of FOLIO and similarly for ProofWriter.For detail statistics on the size of the resulting perturbation datasets, please see Appendix.</p>
<p>Inspired by Han et al. (2023), after the creation of the training data for the two verifiers, we fine-tune a T5-Large (Raffel et al. 2020) 2023), where we generate predicates and FOL statements, each paired with a corresponding natural language text.We generate FOLs for 15000 training data points.These records are parsed using regex pattern match, specifically to separate the FOLs from unintended text.The predicted FOL and input NL statements are mapped and generations with incomplete or excess text are filtered out.The FOL statements are then passed to the Prover9 tool to perform deductive reasoning, where the tool returns 'True', 'False', 'Unknown', or 'None' in case of an error.The generated labels are compared against the ground truth and any mismatch is removed.The prompt templates and statistics of PROOFFOL are provided in the Appendix.</p>
<p>Vanilla SFT</p>
<p>We choose two families of models; LLaMA-2 and Mistral, and examine their performance on three deductive logical benchmarks; ProofWriter, FOLIO, and ProntoQA.The output of the LLMs are either based on the truth value of the conclusion given the NL format of premises and conclusions (denoted in results as Standard), or the formal language translations of the input which is then passed to Prover9 for generating the output (denoted in results as FOL).The initial baselines use in-context learning (ICL) for standard and FOL generation.The few-shot examples for standard generation are randomly sampled from the training data, ensuring a balanced output distribution.Since ProntoQA is a test-set only benchmark, we use ProofWriter demonstrations during inference on ProntoQA.We consider ProntoQA as out-ofdistribution (OOD) since it was not represented in the SFT data.The few-shot FOL generation is similar to the process used in data generation, with variations in examples.The number of few-shot examples and their respective templates are presented in the Appendix.</p>
<p>SFT with PROOFFOL is performed on two models; LLaMA-2 13B and Mistral 7B.These models are selected based on their low computational cost and model parameters.The models are fine-tuned for 3-epochs with a fixed batch size and LoRA (Hu et al. 2022).The inference is done using an 8-bit quantization with the trained LoRA adaptor.Temperature and top p are fixed at 0 and 1 respectively, with a variation of maximum new token value based on the dataset.ProofWriter requires larger number of tokens at inference when compared to FOLIO because of the size of input.Additionally, as NL-based baseline, separate versions of models are also fine-tuned on textual data without symbolic translations (the corresponding results are reported under Standard).The test data used in our experiments are taken from Pan et al. (2023) for ProofWriter and ProntoQA.FOLIO comes with a pre-defined dev dataset, that is used for evaluations.</p>
<p>Incremental SFT</p>
<p>Incremental SFT follows the same process as vanilla SFT for training.The augmented data is passed through LLaMA-2 13B model for fine-tuning.The maximum new token value is reduced to produce individual generation for incremental inference.This additionally ensures minimal time lag between vanilla and incremental inferences.The incremental setup is performed on FOLIO and ProofWriter datasets.For FOLIO, the whole training set (1000 records) is used and the data augmentation results in an increase in data size (∼ 7000 records).To measure the generalizability of this technique, we sample 1000 records from our PROOFFOL dataset and apply the data augmentation, resulting in ∼ 20000 records since the number of statements in ProofWriter are larger.We also perform SFT on the original records to create a baseline for these models.</p>
<p>Verifier Training</p>
<p>The verifier model, used during the inference of FOL and predicate generation, is a T5 large model trained on a perturbation dataset.For our experiments, we train a total of 4 verifiers, two for FOLIO and another two for ProofWriter.The perturbations are applied on their respective training data and vary with respect to the complexity of the FOL statements.ProofWriter consists of shorter sentences, that use less operators and predicates, and can have variations of perturbations without excess manual effort.FOLIO, on the other hand, covers a wide range of operators.We run our SFT model on the training dataset to get the perturbations in addition to the defined ones.The proportion of perturbed instances are kept higher than the correct instances in the final verifier training set for all the datasets.The T5 model is trained for 10 epochs with AdamW optimizer (Loshchilov and Hutter 2019) and a learning rate of 5 × e −5 .</p>
<p>Once trained, the verifiers could run in synchronous (online) or asynchronous (offline) mode.While the online mode corrects errors at each step of inference, before moving to next step (i.e., correction at time step t impacts step t + 1 during generation), the offline mode applies corrections on the fully generated predicate and FOLs as a post-processing step (i.e., correction at time step t does not have any consequential effect on t + 1).For time overhead of incremental decoding and verification, see</p>
<p>Results and Discussion</p>
<p>Main Results</p>
<p>Table 1 refers to a set of results on logical reasoning dataset using in-context learning and SFT.The main focus of these experiments is to assess how much of a gap exists between large and smaller language models prior to fine-tuning, and how this gap is bridged with further fine-tuning of smaller LMs.LLaMA-2 70B is assumed to be the baseline for LLaMA-2 13B, and Mixtral 8 × 7B model for Mistral 7B.</p>
<p>Standard experiment reports free-form reasoning, where the LLM is given a question (premises and a conclusion) and is tasked to produce a direct response.The standard generation varies vastly for n-shot across the datasets.Mistral and Mixtral models show higher accuracy for few-shot learning, but the the results are mixed under SFT.LLaMA-2 13B model responses well with the 10k examples from PROOF-FOL for ProofWriter, but does not show a significant gain for FOLIO and ProntoQA.The fine-tuned Mistral models have higher accuracy over the few-shot results for both 5k and 10k records.This can be attributed to the unreliable output from FOL in the absence of a reasoning path.FOL experiments show correlation of performance with model size, where, for few-shot ICL, LLaMA-2 70B and Mixtral perform consistently better than their smaller versions.ProofWriter achieves 86% accuracy with LLaMA-2 13B model, fine-tuned on PROOFFOL, which is a significant gain in performance when compared to the few-shot setting.Mistral fine-tuned models are the state-of-the-art in FOL generation for ProofWriter datasets, where the model produces 0 syntax errors after fine-tuning.ProntoQA, an altered form of ProofWriter, shows similar trends in performance gain with our fine-tuned models.Mistral 7B fine-tuned on PROOFFOL data outperforms all the ProntoQA baselines.For ProntoQA, there is an observed negative effect of overfitting, when LLaMA-2 is trained on larger dataset (increasing training data from 5k to 10k), but we don't notice this for Mistral.We speculate this might be reflective of difference in model size and how it impacts potential training memorization (i.e., overfitting) for larger models.FOLIO, as expected, is a challenging dataset with complex language and structure.Mixtral model shows has the highest accuracy with FOLIO dataset using few-shot at 42%.LLaMA-2 13b models improve from 24% to 34% when fine-tuned, and is on-par with a much larger LLaMA-2 70B.The syntax and semantic error counts for each of these results are specified in the Appendix.</p>
<p>Error Distribution We provide an error distribution of models on NL-to-FOL translation errors.The results for LLaMA-2 70B with ICL and LLaMA-2 13B after finetuning is provided in Fig 2 .ProofWriter and ProntoQA show decline in errors, whereas FOLIO stays equivalent to the LLaMA-2 70B ICL model.This plot indicates that a model significantly smaller in size can achieve better generation over a larger model when fine-tuned with relevant data.</p>
<p>Incremental Results</p>
<p>Incremental techniques are performed on ground truth FO-LIO and a subset of PROOFFOL dataset.The intention be-   hind the incremental technique is to show that the data augmentation method improves the translations over the original dataset.In Table 2, we focus on the shift in performance between LLaMA-2 13B model fine-tuned on the original 1000 records and augmented 1000 × n records, where n represents the scale at which the data grows after augmentation.Both FOLIO and ProofWriter have low performance in FOL generation when trained with a small dataset.FOLIO shows a steady improvement using vanilla and incremental inferences.This pushes us to use a verifier to further the performance.With the predicates corrected during generation and FOLs corrected after inference, this verifier based incremental setting achieves 37% using Offline verifier, which outperforms LLaMA-2 70B ICL accuracy 34.97% (Table 1).ProofWriter, when trained incrementally, provides varied results with different inferences.The verifier inference model achieves 29% an 32% accuracy when compared to the SFT model on original dataset, 24%.It can be noted that the performance of the FOLIO model is lower when the FOL verifier is online, since any error from the verifier is passed on to the next generation and can cause a domino effect.The change in inference times between incremental and verifier settings is minimal. 3</p>
<p>Ablation Studies</p>
<p>The incremental methods follows certain rules of finetuning and generation.Our model uses two instructions for predicate and FOL generation.We performed an ablation study on using different variation of incremental training and inference for FOLIO dataset.The type of ablations and their performances are given in Table 3.These were performed before training the verifier module.In place of the verifier, we initially used Prover9 to check for syntax and any invalid generation followed a sampling by the LLM, and the first error free FOL was selected.This method proved ineffective as the sampling method was time-consuming.The results in the table are after the tool verification, except for the Check model, where we use LLM as a verifier.</p>
<p>Mixed is the current method of incremental training and inference, where we use different instructions for predicate and FOL generation.Single method uses only one instruction;'Complete the generation'.This performs equivalent to the Mixed, but results in additional syntax errors, presumably because of the vague instruction.Ordered uses different instructions for each FOL generation.The instructions carry information about the sentence that is required to be translated.This method helped keep the syntax errors low, but lowered the overall performance.Instead of passing the previously generated FOL to the next generation, we applied Unique, where the statements are split and passed one at a time with the predicate values.This performs poorly and does not include passage level translations.In Check, we use the perturbation dataset with specific instructions along with the training dataset for fine-tuning.We instruct the model to identify and correct the errors at inference.This method relies on the language model to perform a new task with limited data and proves to be ineffective.</p>
<p>Related Work</p>
<p>LLM for symbolic translation The use of formal language translations by LLMs was initially attempted by Nye et al. (2021), with an intent to emphasize the importance of dual process theory for logical reasoning tasks.Following this, the process of reasoning was offloaded to theorem provers and LLMs served as systems to generate symbolic translations (Pan et al. 2023;Ye et al. 2024;Olausson et al. 2023).The available research in this method majorly differs in variation of formal language (used by different theorem provers) and verification process to handle translation errors.These methods make use of the expensive and ambiguous4 GPT models, restricting the symbolic framework to noncritical domains.Corresponding to the work in formal logic, Yang et al. (2024) applied supervised finetuning to LLaMA model to improve the natural language to first-order logic translations at a sentence level.Our research shifts the focus to building a complete translation system that can handle multiple statements.In addition to this, we perform a systematic analysis of translation errors which enabled us to build a verification mechanism.</p>
<p>Symbolic Decoding with LLMs</p>
<p>The choice of decoding strategies can improve text generation, specifically in LLMs where the output follows a structured format.In neuro-symbolic models, neuro-logic decoding (Lu et al. 2020(Lu et al. , 2021) ) applies symbolic constraints.Interactive theorem provers were also used alongside LLMs to ensure a constrained generation of the reasoning path (Poesia et al. 2023).Other techniques like contrastive step-wise decoding helped with improving the probability of a correct reasoning path (Su et al. 2023).The success of these symbolic decoding strategies motivates our research to apply verification during the inference stage.</p>
<p>Deductive reasoning benchmarks Deductive reasoning requires logical derivation of conclusion using a set of premises.The available benchmarks, ProofWriter (Tafjord, Mishra, and Clark 2020) and ProntoQA (Saparov and He 2022), show a reasoning path for identifying the validity of question from its context.These datasets have a simple, yet multi-hop deductive style.We develop FOL data for the training set of ProofWriter dataset to make it suitable for translation tasks.FOLIO (Han et al. 2022) is another deductive task which is semantically complex with human annotated FOL sentences.The presence of gold-standard FOL should ideally make FOLIO suitable for developing or improving formal language translation models, however the limited size of this dataset impedes the realization of this goal.We present data augmentation technique to overcome this issue.To the best of our knowledge, our work is the first at using the incremental setting, with augmentation and verification, in the context of logical reasoning with NL.</p>
<p>Conclusion</p>
<p>Formal language translation systems for logical reasoning task worked effectively in the era of LLMs, with persisting translation errors.In this paper, we provided an understanding of general translation errors by LLMs, when used as NL-to-FOL translation systems.We highlighted the importance of first-order logic (FOL) ground truth data and present a pipeline to generate high quality FOLs for ProofWriter dataset, introducing an FOL-annotated data called PROOF-FOL.Using PROOFFOL, we fine-tuned a set of smaller language models and showed an increase in performance over larger LMs.Additionally, the issue of data scarcity is addressed via proposing incremental techniques, which cover data augmentation, inference verification, and correction.Our experiments on 3 benchmarks highlight the potential of our proposed framework.</p>
<p>Data Generation</p>
<p>The data pipeline applied for generating ProofFOL is detailed in Fig 3 .The train data is 15000 data points with depth-5 from ProofWriter dataset.The 15k records are sampled randomly ensuring a fair distribution of the labels; True, False, and Uncertain.The LLM here is GPT4o with a output token length of 1000 for each generation.We use url = /v1/chat/completions format for batch generations of GPT4o.The logical solver is Prover 9, a theorem prover suitable to run in python environment with nltk library, that uses CNF conversions, quantifier operations, and skolemization to transform the clauses into a tree format.Parsing errors by Prover 9 occur when the FOL formula cannot be converted to a tree structure because it does not follow specific grammar rules.After filter adn parsing stages, we get 10424 records with FOL statements.Syntax errors are the errors thrown by the tool and semantic errors are measured by comparing the ground truth label with the solver output.</p>
<p>Few Shot Examples</p>
<p>In this section, the few-shot format used for all the in-context learning tasks are presented.</p>
<p>Few-shot for Data-augmentation The Fig 4 shows the format of the few-shot example for data generation.We tried this with 2 examples for 50 unique training data points, but it did not generate better translation results.Instead, we ensured that all operators are covered in this example, specifically negation.We first generate Predicates associated with the sentence.This is followed by FOL generation.Each FOL generation comes with the natural language and this format is adapted from (Pan et al. 2023).We use the same format of generation for all of our experiments with an exception of Predicates description, as this part is harder to verify.The description is removed from ProofFOL.</p>
<p>Datasets</p>
<p>We use three datasets for our experiments.</p>
<p>Data Augmentation</p>
<p>Data augmentation for FOL generation uses incremental data assignment, where the output is divided into multiple tasks, as shown in Fig 9 .This method is applied on two datasets; ProofWriter and FOLIO.A subset of the ProofWriter dataset is extracted from the ProofFOL data and passed for data augmentation, increasing the size to 20X the original.For FOLIO, we take the full dataset and perform augmentation, making it 7X larger.Given the style of incremental data, we remove few records from FOLIO dataset which do not follow the one-to-one mapping of text and FOL.The size stats are detailed in Table 5.</p>
<p>Dataset Original Augmented</p>
<p>ProofWriter 1000 7288 FOLIO 998 20145</p>
<p>Table 5: Size of training datasets before and after data augmentation</p>
<p>SFT Models</p>
<p>Table 6 is an extension of Table 2 from the main paper.It shows how the syntax and semantic errors vary with ICl and fine-tuning.There is a constant trend of lower syntax errors with fine-tuning the models with relevant data for both Llama and Mistral models.</p>
<p>Error Analysis</p>
<p>The NL-FOL translation errors can be identified using the tool feedback or a mismatch in tool output with the ground truth.We identify these issues and categorize them into syntax and semantic errors.quantifier is incorrectly chosen in situations that require a universal quantifier, leading to a logical contradiction between the terms.This mismatch between the chosen quantifier and the necessary logical condition can result in flawed reasoning and inconsistencies in logical analysis.</p>
<p>• Predicate mismatch: It occurs when LLMs are tasked with generating predicates based on text passages and fail to recognize synonymous terms as equivalent.This results in the tool counting synonyms as distinct tokens, leading to discrepancies in predicate generation.</p>
<p>• Arities error: It occurs when predicates are inconsistently applied with a varying number of constants across different statements.Such discrepancies can introduce ambiguity in logical inferences.This is a semantic error that is captured by the tool.</p>
<p>• Subject predicate: The logic is flawed when a subject is used both as a predicate and a constant in the expression.</p>
<p>Verification Perturbations</p>
<p>The perturbations used for training T5 verifier models are designed from the errors in the previously discussed error analysis.The perturbations are different for FOLIO and ProofWriter dataset, as FOLIO is a complex dataset with additional operators when compared to ProofWriter.To handle the complexity of FOLIO dataset, we pass the training set data to the SFT model and match the translations with ground truth.Any mismatch is treated as a perturbation.Other perturbations are based manually included.This dataset has a portion of correct values, making it a verification and correction system.The count of these datasets is detailed in Table 8.</p>
<p>FOLIO Perturbations</p>
<p>The predicate perturbations are designed based on the commonly observed errors in the predicates.These are not complex errors, but usually a missing predicate or variable.Based on this, we use three types of perturbations.</p>
<p>• Omit One Predicate: We randomly omit a predicate from all predicates.</p>
<p>• Omit One Variable: We choose a predicate with the maximum number of variables and omit the last one in the chosen predicate.</p>
<p>• Omit Both Variable and Predicate: We choose a predicate with the maximum number of variables, omit the last argument in the chosen predicate, and also randomly omit a predicate from the rest of the predicates.</p>
<p>The FOL perturbations tackle the syntax errors that are common while generating FOL for FOLIO test data.</p>
<p>• Change Quantifier Position: We move the quantifier position to different positions in a FOL statement.</p>
<p>• Omit One Quantifier: We randomly omit one quantifier from a FOL statement.</p>
<p>• Omit Last Bracket: We omit the last bracket in a FOL statement.</p>
<p>ProofWriter Perturbations The ProofWriter dataset has simpler, but larger number of predicates.based on this, we design 4 types of perturbations.</p>
<p>• Omit one predicate: We randomly omit a predicate from all predicates.• Omit or add one variable: We choose a random predicate, and if the predicate consists of multiple variables, we omit one, and if it consists of only one variable, we add one.• Add plural predicates: To handle the synonymous predicate issue, we randomly select a predicate, create a plural form, and add it to the set of predicates.• Duplicate predicate: We select a random predicate and add a variable to it.This is added back to the set of predicates.</p>
<p>In addition to the quantifier, FOL generations in ProofWriter consists of three major operators; and, imply, and negation.We include variations in this for our perturbation data.ProofWriter can be divided into simple FOL statements (facts) and complex ones (rules).The simple statements usually do not contain any operators or quantifiers.Based on this, we design 5 types of perturbations.</p>
<p>• Add or omit negations: We randomly select facts and either add a negation or remove an existing one.• Omit arguments from facts: We randomly select predicates from facts and omit the variables if the predicate has more than one variable.• Add or omit quantifier: We randomly select rules and either add a quantifier or remove an existing one.• Swap operators: We replace 'and' with 'imply' operator or vice-versa if the operator exists in the FOL statements.• Add plural predicates: To handle the synonymous predicate issue, we randomly select a predicate, create a plural form, and add it to the set of predicates.</p>
<p>Incremental and Verification Ablation We present additional ablation done on the incremental methods and analyse their results.</p>
<p>• Incremental Ablation: The incremental finetuning uses two instruction; generate predicates and generate FOL (brief).This allows us to perform a full scale inference rather than an incremental one, by increasing the output token size.We first let the model predict the predicates and the input along with the predicates is passed to the LLM for full FOL generation.Since the data in the augmented set consist of such a case (last value in augmentation), the model is able to generate the FOL in a flow.We performed inference in this manner and the results are in Table 9, where vanilla<em> represents the full FOL inference.The results in ProofWriter are higher than the incremental results, but on closer observation, we determine that the model hallucinates few cases in the vanilla</em> and helps with better performance.The incremental method allows a strict format of generation and forces the model to generate the FOL for one statement at a time, without adding additional values.Additionally, because of this mismatch in number of statements, we cannot use a FOL verifier on vanilla*.</p>
<p>System Requirements for Experimentation</p>
<p>The Llama 2 model weights are downloaded from the official Llama website https://llama.meta.com/llamadownloads/. Mistral-7B (https://huggingface.co/mistralai/ Mistral-7B-v0.3) model and Mixtral-8x7B(https: //huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) are accessed via the Hugging Face interface.All the models are gated and require access, which is typically a safety measure and can be easily granted.The transformer version used in our experiments is v4.40.0 and should ideally be above v4.28.0 or the latest version to run Llama and Mistral models without any errors.All the other library requirements will be specified in the code.</p>
<p>Figure 1 :
1
Figure 1: The overall flow of the incremental generation and verification at inference time.Here P i s and C denote Premises and Conclusion.For predicate and FOL generation, the output is run through the corresponding verifier.</p>
<p>model to work as the verifier.We train two separate models for the Predicate and FOL verification on each dataset.During incremental inference, the verifier is incorporated into the decoding phase, as shown in Fig 1.</p>
<p>SpecificationsFor the data generation process, we used the training set of ProofWriter Open World Assumption (OWA) with the highest complexity, depth of 5.The training data is passed along with 2-shot ProofWriter examples, which cover all the required operators, to gpt4-o.The format of FOL generation is adapted fromPan et al. (</p>
<p>Figure 2 :
2
Figure 2: Error distribution of LLaMA-2 70B (top) and LLaMA-2 13B (bottom) post fine-tuning on PROOFFOL.</p>
<ul>
<li>the 1k vanilla SFT data under incremental SFT expands to 7k (FOLIO) and 20k (ProofWriter) data points for training.The inference time for SFT models for each record is reported on an average in the format (MM:SS).Vanilla: All predicates and FOLs are generated in one pass.Incremental: All predicates are generated in one pass and then fed into the next step along with premises and conclusions to generate FOLs one by one.+Verifier (On-Off): Predicate verifiers are called during inference (Online) while FOL verifiers are called once all FOLs are generated (Offline).+Verifier (On-On): Applies both verifiers during inference (both in online mode).</li>
</ul>
<p>Figure 3 :
3
Figure 3: Overview of the Data Generation Pipeline and Key Statistics</p>
<p>•</p>
<p>ProofWriter: The training set for ProofWriter is sampled from the depth-5 records.For test-set, we use the one provided in (Pan et al. 2023) • ProntoQA: ProntoQA is a test dataset.We use ProofWriter examples as training set and test-set sample from (Pan et al. 2023) • FOLIO: FOLIO has 1001 training set records and 203 dev set.We use the original training set for SFT models and dev for evaluating the model.</p>
<p>Figure 4 :
4
Figure 4: Few shot example with instruction for Data Generation process</p>
<p>Figure 6 :
6
Figure 6: Standard Generation Few-shot examples for ProofWriter and ProntoQA</p>
<p>Figure 7 :
7
Figure 7: FOL Generation Few-shot examples for FOLIO</p>
<p>Figure 8 :
8
Figure 8: FOL Generation Few-shot examples for ProofWriter and ProntoQA</p>
<p>Figure 9 :
9
Figure 9: Data augmentation: the vanilla represents original data point.Incremental is the data that is present after augmentation.</p>
<p>Table 2 .
2TypeModelStandard FOLLLaMA-2 70B41.8378.33ProofWriterICL n-shot 5000 SFT 10000LLaMA-2 13B Mistral 7B Mixtral 8 × 7B LLaMA-2 13B Mistral 7B LLaMA-2 13B Mistral 7B44.16 49.67 45.83 64.16 70.33 85.66 69.5024.66 66.50 85.00 53.50 98.17 86.33 97.83LLaMA 70B50.6038.80ProntoQAICL n-shot 5000 SFT 10000LLaMA-2 13B Mistral 7B Mixtral 8 × 7B LLaMA-2 13B Mistral 7B LLaMA-2 13B Mistral 7B47.19 50.60 58.40 55.40 60.00 53.20 70.4011.4 10.80 13.00 53.20 78.60 47.80 85.40LLaMA-2 70B50.7434.97FOLIOICL n-shot 5000 SFT 10000LLaMA-2 13B Mistral 7B Mixtral 8 × 7B LLaMA-2 13B Mistral 7B LLaMA-2 13B Mistral 7B43.84 51.23 57.14 40.89 67.98 40.89 66.0124.13 35.96 42.36 26.11 26.11 34.48 27.59</p>
<p>Table 1 :
1
Comparison of models' deductive reasoning accuracy under Standard and FOL-based output prediction.</p>
<p>Here ICL denotes "in-context learning" with n-shots (details of shots in appendix), and SFT denotes "supervised finetuning" (on 5k or 10k training data subset from PROOF-FOL).Accuracy metrics in bold signify notably high performance within the same benchmark, while underline indicates best results under Standard and FOL for ICL and SFT.</p>
<p>) +Verifier (On-Off) 37.44 (03:05) 29.05 (03:10) SFT Incremental (1k * ) +Verifier (On-On) 29.56 (03:27) 32.50 (03:31)
ModelsInferenceFOLIOProofWriterICL BaselineVanilla24.1324.66SFT Vanilla (1k)Vanilla22.66 (01:55) 24.50 (03:36)SFT Incremental (1k
* ) Incremental 32.02 (02:58) 27.16 (02:42) SFT Incremental (1k *</p>
<p>Table 2 :
2
Comparison of LLaMA-2 13B models across FO-LIO and ProofWriters under different training and inference protocols.</p>
<p>Table 3 :
3
Comparison of Incremental Methods with Instruction Fine-tuning.Each method represents the type of instruction used for fine-tuning and inference.The syntax errors are out of 203 test records in FOLIO dataset.
Instruction Accuracy Syntax errorsMixed35.4664Single35.4770Ordered32.0263Unique21.1880Check27.09108</p>
<p>Table 4 :
4
Number of Few-shot examples used for creating baselines in Table 1
• Standard generation: For standard generation, we ran-domly sample examples from the training set. We use5-shot for FOLIO and 4-shot for ProofWriter and Pron-toQA as shown in Fig 5 and Fig 6• FOL generation: For FOL generation, we use partiallyuse examples from (Pan et al. 2023) and sample the restfrom the training set. The FOL syntax is fixed to repre-sent Prover9 format. We use 3-shot for FOLIO and 2-shotfor ProofWriter and ProntoQA as shown in Fig 7 and Fig8</p>
<p>Table 6 :
6
Fig 7 shows examples of each type of error.These are identified by manually analysing the FOL Syntax and Semantic error count for FOL generation statements in cases where there was no feedback from the tool.The details of each error are provided here.
TypeModelSyntax SemanticProofWriter(600)ICL n-shot 5000 SFT 10000LLaMA-2 70B LLaMA-2 13B Mistral 7B Mixtral 8 × 7B LLaMA-2 13B Mistral 7B LLaMA-2 13B Mistral 7B44 314 130 44 71 0 69 086 138 71 46 15 11 13 13LLaMA 70B203103ProntoQA(500)ICL n-shot 5000 SFT 10000LLaMA-2 13B Mistral 7B Mixtral 8 × 7B LLaMA-2 13B Mistral 7B LLaMA-2 13B Mistral 7B266 420 413 108 26 139 13177 26 22 126 81 122 60LLaMA-2 70B19113FOLIO(203)ICL n-shot 5000 SFT 10000LLaMA-2 13B Mistral 7B Mixtral 8 × 7B LLaMA-2 13B Mistral 7B LLaMA-2 13B Mistral 7B95 95 79 103 64 88 4859 35 38 47 86 45 99• Missing quantifier: When a predicate includes a variable,it must have either a Universal Quantifier '∀' or an Exis-tential Quantifier '∃'. This error occurs if either quantifieris missing.• Parenthesis error: The formula becomes invalid if thereis an extra or a missing parenthesis.• Completion error: The FOL is either incomplete or con-tains additional text that disrupts its logical flow.• Quantifier location: Quantifiers that are either repetitiveor incorrectly positioned result in grammatical inaccura-cies in the expression.• Missing variable: When multiple quantifiers are present,the tool fails to parse predicates that lack free variables.• Special token: The tool does not handle special charac-ters in the input.• Unknown operator: The tool does not support parsingmathematical equations.• Predicate error: These errors arise when predicates arereused with different subjects, omitted entirely, or con-joined inappropriately, leading to erroneous interpreta-tions of the logical constructs in FOL statements. Suchmisinterpretations can affect the accuracy and reliabilityof responses generated by LLMs.• Incorrect quantifier: This occurs when an existential
The code for fine-tuning, augmentation and verification, and PROOFFOL dataset are available at https://github.com/RamyaKeerthy/Translation-NL2FOL.
Given the requisite for diversity in syntax and semantic, we first chose a few combinations of demonstrations, and ran a small scale experiment through GPT-4o for each combination. We then selected the most optimal demonstrations with the least format and translation issues in the resulting generated FOL data. We report these final demonstrations in Appendix.
Since ProofWriter has larger number of sentences, we use an adaptive inference token size, where token with less than 5 words have lower token size. For example, "Dog chases the cat" can be translated to "Chases(Dog, Cat)", which require less than 16 tokens. This adaptive technique lowers incremental time for ProofWriter.
We refer to ambiguity in the data used to train the GPT model.</p>
<p>Z3: An efficient SMT solver. L De Moura, N Bjørner, International conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer2008</p>
<p>Pive: Prompting with iterative verification improving graphbased generative capability of llms. J Han, N Collier, W Buntine, E Shareghi, arXiv:2305.123922023arXiv preprint</p>
<p>S Han, H Schoelkopf, Y Zhao, Z Qi, M Riddell, L Benson, L Sun, E Zubova, Y Qiao, M Burtell, arXiv:2209.00840Folio: Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>. E J Hu, yelong shen</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, International Conference on Learning Representations. 2022</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7B. 2023arXiv preprint</p>
<p>Decoupled Weight Decay Regularization. I Loshchilov, F Hutter, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USA2019. May 6-9, 2019OpenReview.net</p>
<p>Neurologic a* esque decoding: Constrained text generation with lookahead heuristics. X Lu, S Welleck, P West, L Jiang, J Kasai, D Khashabi, R L Bras, L Qin, Y Yu, R Zellers, arXiv:2112.087262021arXiv preprint</p>
<p>Neurologic decoding:(un) supervised neural text generation with predicate logic constraints. X Lu, P West, R Zellers, R L Bras, C Bhagavatula, Y Choi, arXiv:2010.128842020arXiv preprint</p>
<p>Release of prover9. W Mccune, Mile high conference on quasigroups, loops and nonassociative systems. Denver, Colorado2005</p>
<p>Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. M Nye, M Tessler, J Tenenbaum, B M Lake, Advances in Neural Information Processing Systems. 202134</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. T X Olausson, A Gu, B Lipkin, C E Zhang, A Solar-Lezama, J B Tenenbaum, R Levy, arXiv:2310.151642023arXiv preprint</p>
<p>Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning. L Pan, A Albalak, X Wang, W Wang, Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, Association for Computational Linguistics2023Singapore</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. G Poesia, K Gandhi, E Zelikman, N D Goodman, C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, arXiv:2306.04031Certified deductive reasoning with language models. 2023. 202021arXiv preprint</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. A Saparov, H He, arXiv:2210.012402022arXiv preprint</p>
<p>Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation. Y Su, X Fu, M Liu, Z Guo, O Tafjord, B D Mishra, P Clark, H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2311.06736arXiv:2302.13971Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof Generation with Contrastive Stepwise Decoding. Long Papers. L.-W Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational Linguistics2023. 2020. 2023. 20241arXiv preprintProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</p>
<p>Satlm: Satisfiability-aided language models using declarative prompting. X Ye, Q Chen, I Dillig, G Durrett, Advances in Neural Information Processing Systems. 202436</p>            </div>
        </div>

    </div>
</body>
</html>
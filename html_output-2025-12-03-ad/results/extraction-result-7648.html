<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7648 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7648</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7648</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-276647144</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.19679v4.pdf" target="_blank">Architectural Vulnerability and Reliability Challenges in AI Text Annotation: A Survey-Inspired Framework with Independent Probability Assessment</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models, despite their power, have a fundamental architectural vulnerability stemming from their causal transformer design -- order sensitivity. This architectural constraint may distorts classification outcomes when prompt elements like label options are reordered, revealing a theoretical gap between accuracy metrics and true model reliability. The paper conceptualizes this vulnerability through the lens of survey methodology, where respondent biases parallel LLM positional dependencies. Empirical evidence using the F1000 biomedical dataset across three scales of LLaMA3.1 models (8B, 70B, 405B) demonstrates that these architectural constraints produce inconsistent annotations under controlled perturbations. The paper advances a practical solution for social science - Independent Probability Assessment - which decouples label evaluation to circumvent positional bias inherent in sequential processing. This approach yields an information-theoretic reliability measure (R-score) that quantifies annotation robustness at the case level. The findings establish that architectural vulnerabilities in causal transformers require methodological innovations beyond accuracy metrics to ensure valid social science inference, as demonstrated through downstream regression analyses where order-sensitive annotations significantly alter substantive conclusions about scientific impact.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7648",
    "paper_id": "paper-276647144",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004197,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Architectural Vulnerability and Reliability Challenges in AI Text Annotation: A Survey-Inspired Framework with Independent Probability Assessment</p>
<p>Linzhuo Li 
Architectural Vulnerability and Reliability Challenges in AI Text Annotation: A Survey-Inspired Framework with Independent Probability Assessment
6A638566077FB21262745C1A57504631causal transformerLarge Language Modelstext annotationorder sensitivityreliabilitysurvey methodology
Large Language Models, despite their power, have a fundamental architectural vulnerability stemming from their causal transformer design -order sensitivity.This architectural constraint may distorts classification outcomes when prompt elements like label options are reordered, revealing a theoretical gap between accuracy metrics and true model reliability.The paper conceptualizes this vulnerability through the lens of survey methodology, where respondent biases parallel LLM positional dependencies.Empirical evidence using the F1000 biomedical dataset across three scales of LLaMA-3.1 models (8B, 70B, 405B) demonstrates that these architectural constraints produce inconsistent annotations under controlled perturbations.The paper advances a practical solution for social science -Independent Probability Assessment -which decouples label evaluation to circumvent positional bias inherent in sequential processing.This approach yields an information-theoretic reliability measure (R-score) that quantifies annotation robustness at the case level.The findings establish that architectural vulnerabilities in causal transformers require methodological innovations beyond accuracy metrics to ensure valid social science inference, as demonstrated through downstream regression analyses where order-sensitive annotations significantly alter substantive conclusions about scientific impact.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have become indispensable for large-scale text classification and annotation in social sciences, enabling novel investigations of massive corpora [36,40,3].Yet, despite their capacity to handle subtle nuances, researchers face a persistent challenge: reliability( [29,32,41,37,42,10,1,6]).Traditional evaluation often relies on comparing model predictions against human expert annotations, yielding an accuracy score.However, focusing exclusively on accuracy can obscure fragile or inconsistent reasoning processes, especially for rare or conceptually intricate categories [37,1].</p>
<p>A growing body of work shows that LLM outputs not just contain biases [8], but can also be sensitive to seemingly minor changes in prompts, including reordering the list of possible labels or adjusting the exact phrasing [32,30].This phenomenon is especially pertinent for causal, decoder-only transformer architectures.Because they process text sequentially, with each token attending only to prior tokens, small variations in token or label order can lead to significant shifts in predicted outcomes.The resulting order sensitivity introduces a reliability gap not captured by simple accuracy metrics.</p>
<p>In other words, a model can produce "correct" annotations for a given prompt design, yet be highly inconsistent when the same question is posed with minor structural changes.For social scientists, whose interest often lies in stable, theoretically grounded coding of phenomena, this poses a fundamental challenge: how can one trust that an LLM annotation is robust once it leaves the controlled conditions of a benchmark prompt?Furthermore, how can researchers detect, at the case level, which annotations are most vulnerable to these artifacts to ensure construct validity [35]?</p>
<p>To address these issues, this paper proposes and demonstrates a two-part approach:</p>
<ol>
<li>
<p>Diagnosing order sensitivity with survey-inspired interventions.Drawing on principles akin to survey design, the research introduces controlled perturbations to test whether annotations remain stable under changes arXiv:2502.19679v4[cs.DL] 26 May 2025 in label ordering, question structure, and reverse-coded logic.These interventions produce high "flip rates" in many cases, thereby revealing the hidden brittleness inherent to causal transformers' sequential attention.</p>
</li>
<li>
<p>Mitigating causal ordering bias and quantifying reliability with independent probability assessment.</p>
</li>
</ol>
<p>As a primary technical solution, the paper proposes an independent querying strategy for classification tasks.Instead of providing a single prompt containing all possible labels (which the causal model processes in an order-sensitive fashion), each label is queried separately.Normalizing these independently computed probabilities yields a distribution free from positional bias.Building on this unbiased distribution, the study introduces an information-based R-score to measure the degree to which the model's predicted distribution diverges from random guessing.This per-case metric identifies which annotations are genuinely confident and which remain unreliable.</p>
<p>The framework is evaluated on annotating 816 scientific abstracts from the F1000 dataset, using three sizes of LLaMA-3.1 models (8B, 70B, 405B parameters).Despite larger models achieving higher accuracy, non-trivial proportions of annotations change (or "flip") under small prompt order variations, highlighting that accuracy alone overlooks important reliability pitfalls.Moreover, by comparing the intervention-based diagnostics to the new R-score, the study shows that cases with lower R-scores indeed exhibit higher flip rates, affirming that the R-score captures genuine reliability.Finally, a downstream regression example demonstrates that ignoring these reliability factors can lead to unstable or even contradictory findings about scientific impact.</p>
<p>Overall, the contributions of this paper are:</p>
<p>• Identify the causal transformer's order-sensitivity problem in annotation tasks.The work clarifies how conventional multi-choice prompts exacerbate this problem and why standard accuracy metrics fail to detect it.</p>
<p>• Introduce survey-inspired perturbation interventions (option randomization, prompt reordering, reverse validation) as an experimental toolkit to diagnose and reveal reliability risks due to the sequential nature of LLMs.</p>
<p>• Propose a novel independent probability assessment strategy that disentangles label queries, mitigating positional bias and yielding a more faithful view of the model's internal distribution.</p>
<p>• Develop an information-based R-score to quantify case-level annotation reliability, with empirical tests linking low R-score to high intervention-induced flips.</p>
<p>• Demonstrate consequential impacts in downstream analyses, warning social scientists and applied researchers that ignoring these reliability methods can yield misleading empirical conclusions.</p>
<p>The remainder of the paper is structured as follows: Section 2 details the causal attention mechanism and explains how it introduces order sensitivity.Section 3 presents the reliability assessment framework and discusses both the survey-inspired diagnostics and the independent probability assessment (including the R-score).Section 4 describes the dataset, models, and methodological implementation.Section 5 reports empirical findings on flip rates, reliability distributions, and downstream regressions.Section 6 offers a broader discussion of implications, limitations, and future directions.Section 7 concludes.</p>
<p>Architectural Vulnerability of LLM</p>
<p>Building on the foundational insight from quantitative text analysis that "all quantitative models of language are wrong-but some are useful" [14], researchers have long recognized the inherent simplifications and assumptions made by computational approaches when analyzing complex human language.Early methods, from simple word frequency counts [13] to topic models [7], were understood as useful approximations of meaning, operating on simplified representations of text.The advent of Large Language Models (LLMs), particularly the causal transformer architectures that dominate current applications, appears to represent a significant leap, offering seemingly more sophisticated and fine-grained understandings of language itself.However, while these models capture linguistic complexities far beyond their predecessors, they do not escape the fundamental "wrongness" inherent to all computational approaches to language -they simply manifest it in novel ways.</p>
<p>Large Language Models' causal, decoder-only transformer architectures impose specific structural constraints that significantly shape how they engage with and categorize textual data.Most critically, when applied to tasks structured as ordered lists (like multi-choice classification prompts), these models exhibit a problematic order sensitivity, leading to predictions contingent on arbitrary positional cues rather than a stable understanding of content relative to categories.This architectural vulnerability is not merely a technical limitation but functions akin to methodological choices or social structures, framing possibilities while simultaneously imposing constraints and potential biases.For social scientists leveraging these models for annotation, recognizing and accounting for these structural predispositions is crucial to ensure that LLM-driven annotation pipelines produce not just seemingly accurate, but truly reliable data for social scientific inquiry.</p>
<p>Causal Attention as a Structural Constraint</p>
<p>The core mechanism of causal transformers is sequential processing via causal attention [12].Each token in the input sequence is processed in order, and its representation is built by attending only to the tokens that preceded it.In other words, tokens before cannot see tokens after, and information only flows one way.This architectural design appears innocuous on the surface-even advantageous for its primary purpose of generating coherent text token by token.However, when these same models are applied to classification tasks presented sequentially (often as a list of choices) in prompts, this seemingly neutral architecture introduces a fundamental structural constraint: the model's interpretation and scoring of a given classification option become dependent on its position within the input sequence relative to other options.What initially presents as a benign technical implementation reveals itself as a consequential limitation that can significantly distort classification outcomes in ways invisible to casual users.Under a causal attention mechanism, the model reads "Option 1" (A), then processes "Option 2" (B) while attending to "Option 1," and finally processes "Option 3" (C) while attending to both "Option 1" and "Option 2." (see Figure 1) The internal state and probability assigned to "Option 3" are thus causally influenced by the prior appearance and processing of "Option 1" and "Option 2." Altering the sequence of options -swapping (A) and (B), for instance -changes the causal history for downstream tokens, leading to different internal representations and potentially altering the model's final predicted label [30].This order sensitivity is not a random error; it is a systematic artifact stemming from the causal architecture's sequential processing when confronted with a task structure (multi-choice list) that ideally requires order-invariant evaluation.Thus, the model isn't actually evaluating the options independently.</p>
<p>Non-Causal Architectures</p>
<p>To highlight this specific vulnerability, it's useful to briefly contrast with non-causal transformer architectures, such as bidirectional encoders (e.g., BERT, RoBERTa) [11,21] or encoder-decoder models often used for classification.These models typically process the input text to be classified using attention mechanisms that consider the entire sequence bidirectionally.In a standard fine-tuning setup for classification, the list of possible categories usually defines the output layer's structure, not elements processed sequentially within the encoder's main input sequence.In such standard non-causal classification pipelines, the model forms a representation of the input text based on its holistic structure, and then a classification head maps this representation onto the predefined categories.While these models have their own sensitivities and biases (e.g., stemming from pre-training data or fine-tuning specifics), they are generally less susceptible to the specific type of order sensitivity induced by the sequential processing of response options in the prompt containing multiple choices.</p>
<p>Algorithmic Structure and Methodological Implications</p>
<p>The prominence of causal LLMs in annotation, despite this order sensitivity, stems from their remarkable flexibility in zero-shot and few-shot learning.Social scientists are drawn to their ability to perform new tasks based on simple prompts, bypassing the need for large labeled datasets and task-specific model fine-tuning [36,40,38,2].</p>
<p>The paradox emerges when considering why causal transformer architectures have nonetheless become dominant in contemporary applications.Their generative capacities-the ability to produce coherent, contextually relevant text that appears almost human-like-are precisely what makes them so powerful and appealing across domains.This generative prowess stems directly from the causal attention mechanism that creates meaningful sequential dependencies between tokens.The very architectural feature enabling GPT models to generate compelling narratives, explanations, and arguments is simultaneously the source of their vulnerability when applied to classification tasks requiring positioninvariant evaluation of options.</p>
<p>This tension reflects a fundamental principle in social science methodology, where distinct research traditions or 'cultures' are shaped by differing goals and practices, leading to inherent trade-offs in their respective strengths and limitations [25].Viewed through this lens, these trade-offs are evident across methodologies: for instance, qualitative traditions, often oriented toward explaining specific outcomes, offer rich contextual understanding but may sacrifice generalizability; experimental designs, focused on estimating average treatment effects, provide causal precision but often lack ecological validity.Similarly, causal transformer architectures offer remarkable generative capabilities while introducing structural biases in certain analytical contexts.The difference, critically, is that while methodological limitations in traditional social science are generally explicitly acknowledged and accounted for, the architectural constraints of LLMs often operate invisibly and become tacit knowledge of this technology [23], masked by the apparent sophistication and fluency of model outputs.</p>
<p>From a critical algorithm studies perspective, this architectural constraint becomes a methodological structure that can inadvertently impose a particular, potentially distorting, logic onto the data.The model isn't neutrally identifying inherent categories; its process of categorization is influenced by the arbitrary sequence in which options are presented.This resonates with critiques arguing that algorithms don't just reflect social reality but can actively construct it by imposing predefined categories or processing logics that privilege certain outcomes or perspectives [28] .In this case, the causal structure can lead the model to "see" patterns or distinctions (or fail to see them) based on positional cues rather than solely on the substantive content relative to the definition of each category.</p>
<p>Limitations of Accuracy-Centric Evaluation</p>
<p>When evaluating AI text annotation performance, researchers commonly rely on a simple accuracy metric, comparing the model's predicted label to a "ground truth" annotation.However, this evaluation approach suffers from significant limitations that fail to capture critical vulnerabilities in model responses, particularly for complex social science text classification tasks.Accuracy metrics overlook prompt-dependence -where a model might provide an "accurate" answer for a specific label ordering, yet produce entirely different results when those same options are merely rearranged.Additionally, single top-label predictions conceal the model's uncertainty levels, making it impossible to distinguish between predictions based on strong evidence versus those essentially "guessing" among seemingly plausible answers.Moreover, high overall accuracy can mask instabilities in the model's handling of rare categories or conceptually challenging classifications [37].</p>
<p>Figure 2: Effective and ineffective AI annotations</p>
<p>As illustrated in Figure 2, there exists a conceptual distinction between the boundary of effectiveness in Large Language Models text annotation and the traditionally emphasized accuracy metrics.While previous studies have typically adopted accuracy-based evaluations, this paper argues that LLM reliability cannot be assessed through accuracy metrics alone.Accuracy, typically measured by comparing LLM outputs with expert annotations, captures only one dimension of reliability.This situation parallels challenges in survey methodology, where respondents may provide seemingly "correct" responses that nonetheless prove ineffective.Similarly, LLM annotations require evaluation for their substantive effectiveness.The figure depicts two partially overlapping circles representing accurate and effective annotations, with their partial overlap illustrating this distinction-accurate annotations may match expert coding but still fail to capture the underlying construct that researchers truly care about.</p>
<p>If the goal is to rely on LLM annotations for downstream analyses (such as measuring concept prevalence in large textual corpora or investigating relationships between that concept and outcomes of interest), being "correct" in an absolute sense proves insufficient.The annotation must also demonstrate robustness to minor changes in conditions.Without assessing how brittle these annotations may be, social scientists risk building entire analytical frameworks on unstable foundations, potentially compromising the reliability of their research conclusions.</p>
<p>Therefore, a critical methodological challenge in using causal LLMs for annotation is to move beyond aggregate accuracy metrics and develop strategies to: 1) diagnose the extent to which this architectural vulnerability translates into order-sensitive annotations for a given task and model; and 2) devise methods to elicit the model's preferences in a way that is robust to these structural biases, thereby obtaining a more reliable measure of its confidence in assigning a category.The framework presented in the following section addresses these challenges directly.</p>
<p>Framework for Assessing and Quantifying Reliability</p>
<p>In this section, this paper presents a two-part framework designed to (1) diagnose order sensitivity in causal LLM annotations and (2) mitigate it via an alternate query strategy that allows for calculating a case-level reliability measure.</p>
<p>Part I: Diagnosing Order Sensitivity via Survey-Inspired Interventions</p>
<p>While recent literature explores using LLMs to simulate human survey respondents ( [38,2,6,17]), this paper reverses the direction of knowledge transfer and argues that survey methodology's rich tradition of diagnosing and improving response reliability offers a framework for addressing LLM annotation challenges.</p>
<p>Despite the apparent differences between survey respondents and Large Language Models (LLMs), both can exhibit behavior that does not necessarily stem from deep engagement or true "understanding."Survey research has documented how participants sometimes take cognitive shortcuts-known as satisficing [19], [4].Proposed by [34] in a general sense and by [19] specifically in survery research, satisficing describes how respondents may skip or abbreviate the four cognitive processes of comprehension, retrieval, judgment, and response selection to conserve effort.Instead of formulating a fully reasoned answer, they opt for a response that seems good enough.Sometimes, satisfying can also take a more extreme form as in the case of careless responding [26,15,39]: not only are participants skipping deeper thought, but they may be ignoring the survey content altogether.</p>
<p>The framework adapts three perturbation" strategies from survey methodology to reveal whether and how much a model is sensitive to small prompt changes.These strategies-option randomization, position randomization, and reverse validation-can be understood as "screener question" [5], and mimic how survey researchers detect inattentive or satisficing" respondents by shuffling question order or wording [19,26].</p>
<p>A) Option Randomization.Vary the order of category labels (e.g., (A) New Finding, (B) Technical Advance, (C) Interesting Hypothesis) across different prompts for the same text.If a model frequently changes its predicted label when the category order changes, that signals reliance on positional cues rather than content.</p>
<p>B) Position Randomization.Alter the structure or location of the question in the prompt.For instance, moving the classification question from the end to the beginning or around different segments.Significant changes in output reveal instability tied to these structural positions.</p>
<p>C) Reverse Validation.Present an inverted or negated version of the classification.For example, after asking Which category best describes this paper?"also ask Which category does not describe this paper?"If the model's answers are inconsistent, this indicates that it does not genuinely grasp the underlying text meaning and is instead responding to superficial cues.</p>
<p>Flip Rate as Diagnostic Metric.For each of these interventions, the study measures the proportion of cases where the model's top choice flips compared to its original assignment.A high flip rate indicates that the model is highly sensitive to order or structural changes, suggesting low reliability.Conversely, a low flip rate suggests the model is more robust.</p>
<p>Part II: Mitigating Order Bias via Independent Probability Assessment</p>
<p>While the above interventions reveal the extent of a model's order sensitivity, the paper also offers a technical approach to mitigate the causal transformer's positional bias in multi-choice classification.This is accomplished by disentangling each label's evaluation from the context of other labels.</p>
<p>Independent Binary Queries</p>
<p>Instead of presenting all possible categories in one prompt, the approach breaks the task into multiple binary queries as follows:</p>
<p>Independent Probability Assessment -Binary Query</p>
<p>Example Prompt for Category C: Are we dealing with a Technical Advance"?Given the abstract: [abstract text].Please answer Yes" or "No" only.</p>
<p>This is done separately for each category that needs to be assessed.Because the model now sees only one category at a time, it cannot exploit relative ordering or rely on sequence-level heuristics that compare multiple labeled options.Each query yields a probability of "Yes" (i.e., the category applies).By repeating for all categories, this yields independent probabilities:
p(Yes | category i), ∀i ∈ {A, B, C}.
In a single-label classification context (only one category can apply), we can normalize these probabilities across all categories to obtain:
p(c i ) = p(Yes | c i ) j p(Yes | c j )
.</p>
<p>This approach bypasses the causal attention trap in multi-choice prompts, because the model does not read or process the other categories at the same time.</p>
<p>By repeating this process for each category ("Interesting Hypothesis", "Technical Advance", and "New Finding"), we can obtain a comprehensive distribution that represents the model's genuine preferences without positional biases.It can handle both exclusive classifications (where exactly one category applies) and non-exclusive scenarios (where multiple categories may apply simultaneously).This approach can be understood as the LLM-version of projecting meanings onto specific dimensions ( [18,31,9]) -thus representing a "geometry of thinking" where each query projects the input text onto a distinct semantic axis, albeit using natural language.This controls for information asymmetry, allowing us to capture the model's genuine preference distribution.</p>
<p>Information-Based Reliability Score (R-score)</p>
<p>Although this independent probability assessment helps us avoid ordering bias, we still need a method to distinguish whether the model holds a strong preference for one category or is nearly guessing.The study thus defines a per-case R-score measuring how far the model's final distribution is from uniform: R-score Definition
R = D KL P ∥ U = k i=1 p i log p i 1/k ,
where P = {p 1 , p 2 , . . ., p k } is the normalized distribution over k categories (each derived via independent queries), and U = {1/k, . . ., 1/k} is the uniform distribution.If R is near zero, the model is effectively distributing probability evenly across all labels, indicating high uncertainty (i.e., the model is close to random guessing).A high R indicates the model strongly favors one category over the others.</p>
<p>In practice, similar to the choice of p-values in statistical inference,the thresholds of R-score were empirically selected based on intuitive probability distributions for a three-option classification scenario.A distribution close to uniform (low R-score) suggests the model cannot meaningfully differentiate between categories, while a highly skewed distribution (high R-score) indicates strong preference for a particular category.The threshold of KL = 0.06 corresponds approximately to a distribution of [0.5, 0.25, 0.25], where the top probability is equal to the sum of remaining options.KL = 0.36 corresponds to a distribution of [0.75, 0.125, 0.125], where the top probability is three times of the rest others, demonstrating a clearer model preference.KL = 0.7 corresponds to a distribution of [0.9, 0.05, 0.05], representing a case where the model shows very strong confidence (90% of the time) in its top prediction.In other words, if the model repeatedly annotates this case randomly, the result is consistent with the "top" label in 90% cases.Critically, the R-score is calculated at the case level, so we can identify exactly which instances are robust and which are not.</p>
<p>Linking the Two Parts:</p>
<p>While the survey-inspired interventions detect order sensitivity, the independent assessment corrects for it when computing a probability distribution.Cases that exhibit high flip rates under the interventions generally turn out to have low R-scores in the independent assessment, confirming that the same underlying weakness is being measured.For social science applications, researchers can use these diagnostics jointly: (1) run interventions to measure how sensitive each annotation is, (2) re-annotate crucial tasks using independent queries for robust probability estimates, and (3) filter or prioritize expert review for cases with low R-scores.</p>
<p>Data and Methods</p>
<p>We evaluate our framework on a text annotation task of classifying scientific papers by their primary contribution type: Interesting Hypothesis, Technical Advance, or New Finding.Below we summarize the data, models, and how we implement both the survey-inspired interventions and independent probability assessment.We also describe how we test the downstream effects of unreliable annotations in a citation-impact regression.</p>
<p>Data</p>
<p>F1000 Dataset.The study utilized the F1000 dataset (also known as Faculty Opinions) from previous studies in science of science for demonstration.It comes from a post-publication peer-review platform in which invited scholars -practicing scientists and clinicians -select and evaluate biomedical papers they deem significant.The experts are asked to label papers with predefined tags in about five categories.For demonstration purposes, the labels used here contain three primary contribution types that takes the majority of cases: (A).Interesting Hypothesis (7.5%) (B).Technical Advance (13.3%) and (C).New Finding (79%).These expert annotations has been shown to align with different types of novelty [33], thus are important in studying science and innovation.</p>
<p>This dataset is suitable here as it provides expert-validated classifications that requires nuanced understanding.The categories are distinct yet related, making it a moderately challenging test for LLM reliability.Notably, the class imbalance mirrors real-world scientific output -most papers make empirical findings over theoretical or methodological contributions.This distribution creates natural test conditions for evaluating LLM reliability across frequent and rare categories.After preprocessing and cleaning, a total of 816 biomedical papers with expert annotations are included.</p>
<p>Microsoft Academic Graph (MAG).To examine downstream effects, this study also uses a simple example of predicting a paper's citation impact (with in 3 years of publication) based on the paper's contribution types using linear regression models.To do so, the F1000 data is merged with Microsoft Academic Graph Dataset, which has publicly available Microsoft Academic Graph, to get the citation count of 816 papers.The merge was done by matching their MAG paper ids with their PMID in the PubMed dataset.Citation counts follow a heavy-tailed distribution (mean=142, SD=213, max=2,184), typical of scientific impact patterns.We log-transform citations after adding 1 to handle zeros.We also added year and team size as control variables.</p>
<p>LLM Models</p>
<p>To 2) The 70B variant represents current practical limits of dense models, demonstrating partial robustness to position randomization but remaining vulnerable to reverse validation (Figure 3 -5).</p>
<p>3) The 405B model tests whether large models with extreme scale can overcome satisficing tendencies -the results suggest even this frontier model retains non-trivial sensitivity to option ordering (Figure 3 -5), indicating the importance of case-level reliability assessment for current LLM paradigms.</p>
<p>To ensure comparability across model sizes, the analysis maintains identical generation parameters: temperature=0 for controlled randomness, top-p=0.7 sampling, and maximum output length=1 token).Change of parameters don't affect the main outcomes.For probability distribution analysis, the study extracts logits directly from the final unembedding layer by using TogetherAI api with the parameter "logprobs" equals True .This setup allows us to precisely track how intervention-induced perturbations affect the models' internal confidence metrics at the precise token of interest.</p>
<p>Survey-Inspired Interventions</p>
<p>The study apply soption randomization, position randomization, and reverse validation to each of the 816 papers.Each intervention has multiple variants.In particular, the paper measures the flip rate, i.e., the proportion of cases where the model's top label changes relative to the original prompt.This yields a detailed diagnostic of how each model size responds to small prompt changes.</p>
<p>Independent Probability Assessment and R-score</p>
<p>The study then implements the independent binary query strategy for the same papers.For example, it asks: -"Is the main contribution of this paper an Interesting Hypothesis?" -"Is the main contribution of this paper a Technical Advance?" -"Is the main contribution of this paper a New Finding?"</p>
<p>From these three queries, the study extracts p(Yes|c i ) for i ∈ {A, B, C}, normalize them to p(c i ), and compute the Kullback-Leibler divergence from uniform random guessing, R = D KL (P ||U ).Using thresholds, the study categorize each annotation's reliability from "very low" to "high."</p>
<p>Downstream Regression Analysis</p>
<p>Finally, the study demonstrates how these reliability issues can alter analytic conclusions, by regressing log(citations+1) on an indicator variable for (B) Technical Advance (versus C) and observe how coefficient estimates shift when using labels produced under various interventions.Significant changes in sign or significance underscore the risk of ignoring order sensitivity and reliability in real-world social science studies.</p>
<p>Results</p>
<p>Diagnosing Order Sensitivity via Flip Rates</p>
<p>Figure 3 reports the overall flip rates for the three interventions across our three LLaMA-3.1 models.Even the largest (405B) model exhibits non-negligible flips (5-10%) across different intervention types, while the 8B model can flip in over 25% of cases under some variants.These results demonstrate that the fundamental problem of order sensitivity persists for large models, though it is more pronounced for smaller ones.Data shown for LLaMA-3.1 with 8B, 70B, and 405B parameters.Smaller models exhibit higher flip rates, indicating strong order sensitivity, but even the largest model is not fully immune.</p>
<p>Figure 4 zooms in on how flip rates vary by paper category -Interesting Hypothesis (N=62), Technical Advance (N=109), and New Finding (N=645) -across all three LLMs.A key takeaway is that the "Interesting Hypothesis" category shows the highest flip rates in most interventions for all model sizes.This suggests that rarer or more conceptually demanding classes may induce greater model uncertainty, making them more susceptible to small perturbations in option ordering, prompt position, or question framing.In other words, the models struggle disproportionately with classifying the less common or more abstract paper types, indicating a particular instability that could significantly affect downstream analyses focused on such minority categories.This instability in underrepresented cases will not be identified if researchers soling relying on the external metrics (such as accuracy) for model evaluation.</p>
<p>For "Technical Advance," the flip rates also remain elevated, especially for the 8B model, but somewhat moderate for the 70B and 405B models."New Finding," by contrast, shows lower but still non-zero flip rates; this is likely due to its dominance in the training distribution and, hence, the model's learned preference for that label.Nonetheless, the models still register flips in the "New Finding" category under certain interventions (especially reverse validation).</p>
<p>Consistency vs. Accuracy</p>
<p>Figure 5 explores how "internal consistency" (whether an annotation flips under an intervention) aligns-or fails to align-with "external accuracy" (agreement with expert ground truth).Each cell reports ∆acc, defined as accuracy(flip) minus accuracy(no-flip).By design, a positive ∆acc (shown in red) means that those annotations which flipped under an intervention ironically ended up being more accurate than those that stayed the same.Conversely, negative ∆acc (green) indicates that flips tend to be bad indicators for correctness in those cases -i.e., changing an answer correlates with lower accuracy relative to not flipping.The result shows that only the "New Finding" category shows alignment between external accuracy and internal consistancy for all interventions.In the other categories, all three models show misalignment in certain situations.This finding carries significant implications: although conventional wisdom suggests that higher internal consistency should correspond to higher accuracy and thus better for use, these positive ∆acc values in minor categories (especially in "Interesting Hypothesis") shows that this assumption does not always hold.The areas of consistute a "non-sense region" in which the model is unreliable, and in which simple reliance on external validation alone can be misleading.Each matrix cell captures ∆acc: the difference in accuracy between flipped vs. non-flipped cases.Red indicates higher accuracy for flipped subsets; green indicates higher accuracy for stable subsets.Patterns reveal that flipping is not always negatively correlated with accuracy, complicating naive assumptions that consistent answers are always better.The Oregon Health Services Commission recently completed work on its principal charge: creation of a prioritized list of health care services, ranging from the most important to the least important.Oregon's draft priority list was criticized because it seemed to favor minor treatments over lifesaving ones.This reaction reflects a fundamental and irreconcilable conflict between cost-effectiveness analysis and the powerful human proclivity to rescue endangered life: the "Rule of Rescue."Oregon's final priority list was generated without reference to costs and is, therefore, more intuitively sensible than the initial list.However, the utility of the final list is limited by its lack of specificity with regard to conditions and treatments.An alternative approach for setting health care priorities would circumvent the Rule of Rescue by carefully defining necessary indications for treatment.Such an approach might be applied to Oregon's final list in order to achieve better specificity.</p>
<p>8B 70B 405B</p>
<p>Independent Probability Assessment: R-score Distributions</p>
<p>Examples of Unreliable Annotations</p>
<p>Annotation Comparison Expert Annotation</p>
<p>Interesting Hypothesis LLM Annotation Llama-3.1-8Bprobability distribution (assessed independently and then normalized):</p>
<p>Interesting Hypothesis: 0.326 Technical Advance: 0.337 New Finding: 0.337 Reliability Score 0.00 (Very Low Reliability)</p>
<p>Using the Reliability-score, the study was able to identify at the case level which annotations are unreliable.Below I present two of the unreliable cases from the F1000 dataset.One annotation of the two cases is inaccurate compared to expert annotation and have low reliablility score, and another one is accuracy but still have low reliability.</p>
<p>The first example shown in Table 1 demonstrates a case of low reliability in LLM annotation (Inaccurate and Unreliable).The LLaMa-3.1 8B model assigns nearly identical probabilities to all three categories (approximately 1/3 each), indicating it cannot meaningfully distinguish between them for this paper.The resulting Reliability Score of 0 signifies that the probability distribution is effectively uniform, equivalent to random guessing.Despite this uncertainty, if forced to choose, the model would incorrectly label this as either "Technical Advance" or "New Finding" rather than the expert-assigned "Interesting Hypothesis" category.This case illustrates how accuracy metrics alone would flag this as an incorrect annotation, but the proposed reliability score additionally reveals that the model has no meaningful confidence in its answer.</p>
<p>Abstract</p>
<p>In this article, we investigate the spatial and temporal dynamics of predator and prey populations using an individual-based modeling approach.In our models, the individual is the fundamental unit, and the dynamics are governed by individual rules for growth, movement, reproduction, feeding, and mortality.We first establish the congruence between age-structured predator-prey population models and the corresponding individual-based population model under homogeneous spatial conditions.Given the agreement between the formalisms, we then use the individualbased model to investigate the dynamics of spatially structured predator-prey systems.In particular, we contrast the dynamics of predator-prey systems in which predators adopt either an "ambush" or a "cruising" strategy.We show that the stability of the spatially structured predator-prey system depends on the relative mobility of prey and predators and that prey mobility, in particular, has a strong effect on stability.Local density dependence in prey reproduction can quantitatively alter the asymmetrical influence of prey mobility on stability, but we show that the asymmetry exists when local density dependence is removed.We hypothesize that this asymmetrical response is due to prey "escape" in space caused by differences in rates of spread of prey and predator populations that arise because of fundamental differences between prey and predator reproduction.</p>
<p>Annotation Comparison Expert Annotation</p>
<p>New Finding LLM Annotation Llama-3.1-405Bprobability distribution(assessed independently and then normalized):</p>
<p>Interesting Hypothesis: 0.117 Technical Advance: 0.384 New Finding: 0.498 Reliability Score 0.13 (Low Reliability)</p>
<p>The second example in Table 2 demonstrates a different case of low reliability in LLM annotation (Correct but Still Unreliable).While the model correctly identifies "New Finding" as the most probable category (0.498), there remains substantial uncertainty.The model assigns a significant probability (0.384) to "Technical Advance," indicating it struggles to fully differentiate between these categories for this paper.The reliability score of 0.13 suggests the model is slightly confident in its classification but still exhibits notable uncertainty.This case illustrates a common challenge in scientific paper classification: the boundary between reporting new empirical findings and introducing novel technical approaches can be subtle, particularly in computational modeling papers like this one.Traditional accuracy metrics would simply mark this as correct, missing the important nuance that the model's confidence is relatively low.</p>
<p>The next part shows how the independent probability assessment addresses the causal ordering bias.Figure 6 presents the resulting R-score distributions across the three category labels and three model sizes.Higher R-scores indicate the model confidently distinguishes among categories, while lower scores suggest a near-uniform (random) distribution.For each paper, the study applies the independent binary query method to obtain a probability distribution and compute KL divergence from uniform.Higher values (green) indicate high reliability (strong preference for a single label), while lower values (red) suggest the model is essentially guessing.Although larger models exhibit generally higher reliability, some categories (especially rarer ones) remain problematic.</p>
<p>Smaller (8B) models yield a considerable fraction of near-uniform distributions, reflecting fundamental uncertainty.Larger models show improved reliability overall, but still produce many low or moderate R-scores for rare categories like Interesting Hypothesis.While larger models generally demonstrate higher overall reliability (median KL of 0.37, 0.79, and 0.80 for 8B, 70B, and 405B respectively in the New Finding category), there is notable variation in reliability across different annotation categories within the same model.For instance, the 405B model shows high reliability for New Finding (median KL = 0.80) and Interesting Hypothesis (median KL = 0.82) but only moderate reliability for Technical Advance (median KL = 0.42).This variability shows that even when a model demonstrates high overall reliability, its performance may still be notably unreliable for specific categories or individual cases, as identified by R-score.</p>
<p>Relation Between Flip Rates and R-score</p>
<p>Although omitted the figure in this paper for space, empirical results also confirm that cases with low R-scores (below 0.06, for example) are the same ones most likely to flip under label reordering or reverse-coded prompts.This aligns with the interpretation that if the model's internal distribution is nearly uniform, small changes to the prompt context can easily shift the top label.In contrast, a high R-score indicates robust preference that is resilient to ordering artifacts.</p>
<p>Downstream Impact on Regression Findings</p>
<p>The paper next illustrates the practical impact of ignoring order sensitivity in research by regressing (log) citation count on the LLM-predicted label of whether a paper is a Technical Advance (vs. the baseline New Finding).Table 3 portrays how sensitive the coefficient estimates can be when the classification depends on prompts with slightly different label orders or reversed logic.In some conditions, the same LLM that produced a non-significant coefficient in the original prompt yields a highly significant and negative coefficient after a prompt change.Such discrepancies are especially large for smaller models.These findings reveal how ignoring order sensitivity can jeopardize the robustness of conclusions about scientific impact or other social science outcomes.</p>
<p>Discussion</p>
<p>The results underscore that causal transformers' sequential processing is a fundamental source of order sensitivity in text classification prompts.The survey-inspired interventions show that reordering labels, altering the position of the question, or posing reverse-coded queries can induce substantial volatility in the predicted label.Critically, this volatility remains hidden if researchers rely solely on a single prompt design and measure accuracy against that one scenario.</p>
<p>Beyond diagnosing the problem, the independent probability assessment offers a technical remedy for multi-choice tasks: it elicits category probabilities one at a time, thereby circumventing the causal architecture's positional bias.By looking at the final distribution of those probabilities via our R-score, we can further identify cases that are near-random guesses versus cases where the model expresses strong confidence.In practice, social scientists can use this approach to:</p>
<ol>
<li>
<p>Filter or weight LLM annotations by R-score.Exclude low-R-score instances or treat them as uncertain to avoid spurious signals.</p>
</li>
<li>
<p>Target expert validation.Reserve time-consuming human reviews for the suspicious or borderline subset of cases, potentially focusing on rare categories where the model is known to be least reliable.</p>
</li>
</ol>
<p>Pretest prompts with survey-like interventions.</p>
<p>Identify if order sensitivity is acute for a given classification task before deploying on large datasets, mitigating unpredictable label noise.</p>
<p>Relation to Survey Satisficing and Annotation Tasks Broaderly</p>
<p>The findings parallel issues in survey methodology, where inattentive or "satisficing" respondents are detected by randomizing question orders or including reverse-coded items [19,4].Similarly, LLMs sometimes appear to select shortcuts based on positional or lexical cues instead of deeply engaging the content.While the root cause in LLMs is grounded in the mechanics of causal attention rather than human cognitive effort, the outcomes-unstable or low-fidelity responses-are reminiscent of survey data quality concerns.</p>
<p>These results are helpful for social science research that seeks to harness LLMs at scale.Rare categories, such as novel theoretical constructs, are often the focus of interest in specialized domains, yet they are precisely where these reliability issues emerge most strongly.Researchers adopting LLM-based annotation pipelines should thus be especially cautious with imbalanced or conceptually nuanced labels.</p>
<p>Limitations and Future Directions</p>
<p>One limitation of the approach is the added computational overhead of running multiple independent queries (one per category).Another is that while the framework illustrate the power of order-sensitive interventions and the R-score for classification tasks with a moderate number of categories, extensions to tasks with dozens or hundreds of labels would require further efficiency considerations.Moreover, other aspects of LLM outputs (e.g., chain-of-thought reasoning, open-ended text generation) may exhibit forms of sensitivity not addressed here.Future work could explore how to adapt R-score-like measures to open-ended tasks or how to integrate the independent assessment framework deeper into model pre-training.</p>
<p>Conclusion</p>
<p>Different technologies follow their own logics [24].</p>
<p>LLM-based annotation, much like the financial modelling practices [22], operates not in a vacuum of pure computation but within a specific technical and methodological context -what might be termed an emerging "computational annotation culture" within social science.This culture leverages powerful tools, predominantly causal transformer models, initially developed for tasks like text generation where their core architectural properties -sequential processing and causal attention -are precisely what enable their remarkable fluency and coherence.</p>
<p>However, as this study has demonstrated, this very architecture imposes a fundamental structural constraint when these tools are repurposed for tasks demanding position-invariant evaluation, such as multi-choice classification.The sequential processing of labels and prompt elements introduces a systemic order sensitivity: the model's internal evaluation and probability assignment for a given category can become unnervingly dependent on its arbitrary position within the input sequence, rather than solely on the substantive content relative to the category definition.This isn't a mere bug; it's a feature of the causal design manifesting as a significant vulnerability when deployed in a context requiring robust, context-free (relative to prompt structure) judgment.</p>
<p>This research underscores that relying solely on traditional accuracy metrics, which measure agreement against a single ground truth under one specific prompt design, provides an incomplete and potentially misleading picture of an annotation pipeline's true performance.Accuracy in this context often captures a form of "local correctness" relative to a single setup, but it fails to reveal the underlying brittleness and order-dependence that can undermine the "global reliability" needed for robust social science inference.</p>
<p>To navigate this challenge, the paper proposes a methodological "bricolage," adapting the practices to the specific constraints of the tool.Old experience in survey research helps.The survey-inspired interventions -option randomization, position randomization, and reverse validation -serve as crucial diagnostic probes.Akin to the stress tests developed in other domains, they reveal the extent of this hidden order sensitivity through empirical "flip rates."The findings show that this fragility is not uniform; it is often acutely pronounced for rarer or conceptually more nuanced categories, precisely where social scientists often focus their analytical attention.This highlights a significant gap between the perceived sophistication of LLM outputs and their actual reliability for specific, demanding annotation tasks.</p>
<p>Furthermore, the proposed independent probability assessment offers a technical adaptation specifically designed to mitigate the causal ordering bias in commonly used multi-choice settings.By disentangling the evaluation of each category into separate queries, it effectively bypasses the causal attention mechanism's tendency to compare options based on their sequence.This process allows for eliciting a more faithful representation of the model's internal confidence distribution, without the burden of heavy internal analysis of model parameters and activations [20,16,43].</p>
<p>The derived R-score then translates this distribution into a case-level measure of reliability, allowing researchers to identify and manage the underlying uncertainty at the granularity required for robust social scientific analysis.A low R-score indicates the model is near-random guessing, a critical signal missed by a simple top-label prediction.</p>
<p>Implementing such methods is not without its trade-offs, echoing the interplay between goals, resources, and constraints observed in other technical fields.Running multiple independent queries inherently increases computational demands compared to a single multi-choice prompt.This mirrors the material constraints (hardware capacity, processing time, energy budgets) that shaped the adoption and adaptation of models like the Gaussian copula in financial institutions.</p>
<p>What makes an LLM annotation model truly "work" for social science is not simply conceptual elegance or the highest score on a decontextualized benchmark, but its ability to produce stable, interpretable, and reliably uncertain signals within the practical constraints and analytical objectives of the research.The framework provides tools to make these trade-offs and the resulting reliability explicit.</p>
<p>Finally, the demonstration of the downstream impact on regression analyses serves as a stark warning.Building analytical frameworks and drawing substantive conclusions based on annotations susceptible to these subtle, architecturallyinduced shifts can lead to findings that are themselves unstable, inconsistent, or even contradictory.Just as financial models enabling "local stability" in specific trading desks could mask systemic risks contributing to broader market instability, seemingly "accurate" LLM annotations (when evaluated naively) can mask an underlying fragility that undermines the validity of broader scientific claims derived from them.Relying on these powerful computational tools without a rigorous methodology that acknowledges their specific architectural properties, diagnoses their vulnerabilities, and quantifies their uncertainty is akin to building a theoretical structure on an unstable foundation.It is crucial for social scientists to move beyond the seductive fluency and apparent ease of LLM-based annotation and embrace methodological approaches that explicitly account for the ways in which these tools interact with the data and the research task, ensuring that digital annotations provide a robust and reliable basis for understanding the social world.</p>
<p>Figure 1 :
1
Figure 1: The Causal Asymmetry of Information Accumulation in LLM</p>
<p>facilitate analysis, open-sourced LLMs are favored for analysis.The study employs three variants of the leading models -LLaMA-3.1 Instruct series (8B, 70B, and 405B parameters) to systematically examine how model annotation reliability changes under survey-inspired interventions.The LLaMa series capture the spectrum from lightweight to state-of-the-art LLMs (open sourced), allowing us to test whether larger models exhibit greater robustness to surveyinspired interventions.All models use the standard dense decoder-only autoregressive transformer architecture along with supervised fine-tuning and direct preference optimization after pretraining [27].The tripartite model selection directly informs key findings in later analyses: 1) The 8B model serves as a baseline for "commodity" LLMs accessible on consumer hardware, showing high intervention sensitivity (Figure 3 -5) due to limited contextual reasoning capacity.</p>
<p>Llama- 3 Figure 3 :
33
Figure 3: Flip Rates under Survey-Inspired Interventions.Percentage of instances where the model's top label changes when we randomize label options (pink), randomize prompt position (blue), or apply reverse-coded logic (red).Data shown for LLaMA-3.1 with 8B, 70B, and 405B parameters.Smaller models exhibit higher flip rates, indicating strong order sensitivity, but even the largest model is not fully immune.</p>
<p>Figure 4 :
4
Figure 4: Flip Rates by Category.Depending on whether the paper is labeled by experts as (A) Interesting Hypothesis (N=62), (B) Technical Advance (N=109), or (C) New Finding (N=645), flip rates vary under option randomization, position randomization, and reverse validation.Rare categories (like Interesting Hypothesis) are disproportionately affected.</p>
<p>Numbers show acc = accuracy(flip) -accuracy(no-flip)</p>
<p>Figure 5 :
5
Figure5: Consistency vs. Accuracy (∆acc).Each matrix cell captures ∆acc: the difference in accuracy between flipped vs. non-flipped cases.Red indicates higher accuracy for flipped subsets; green indicates higher accuracy for stable subsets.Patterns reveal that flipping is not always negatively correlated with accuracy, complicating naive assumptions that consistent answers are always better.</p>
<p>Figure 6 :
6
Figure 6: R-score Distribution by Model and Category.For each paper, the study applies the independent binary query method to obtain a probability distribution and compute KL divergence from uniform.Higher values (green) indicate high reliability (strong preference for a single label), while lower values (red) suggest the model is essentially guessing.Although larger models exhibit generally higher reliability, some categories (especially rarer ones) remain problematic.</p>
<p>Table 1 :
1
Example of Unreliable LLM Annotation with Very Low Reliability Score
Paper InformationTitleSetting health care priorities in Oregon. Cost-effectiveness meetsthe rule of rescueAbstract</p>
<p>Table 2 :
2
Example of Low Reliablilty LLM Annotation Paper Information TitleDynamics of age-structured and spatially structured predatorprey interactions: individual-based models and population-level formulations.</p>
<p>Table 3 :
3
Impact of Different Interventions on Downstream Regression Tasks Dependent variable is the log of journal citations within 3 years of publication.All regressions include year and team size fixed effects.Coefficients show the effect of Technical Advance (as compared to New Finding) on citation counts.Highlighted rows indicate significant changes in statistical significance level of coefficients.p &lt; 0.1, * * p &lt; 0.05, * * * p &lt; 0.01.Heteroskedasticity-robust standard errors (HC1).
Original ClassificationIntervention ClassificationIntervention TypeNR 2Coef.p-valueCoef.p-valuePanel A: 8B ModelOption Random 1615 0.171 -0.0600.615-0.3220.010  *  <em>Option Random 2615 0.171 -0.0600.615-0.0710.594Position Rotate 1615 0.171 -0.0600.615-0.2550.023  *  </em>Position Rotate 2615 0.171 -0.0600.6150.0450.739Reverse615 0.171 -0.0600.615-1.2060.005  *  *  <em>Panel B: 70B ModelOption Random 1612 0.178 -0.4460.009  *  *  </em>-0.2680.076  <em>Option Random 2612 0.178 -0.4460.009  *  *  </em>-0.3960.010  *  *  <em>Position Rotate 1612 0.178 -0.4460.009  *  *  </em>-0.3410.026  *  <em>Position Rotate 2612 0.178 -0.4460.009  *  *  </em>-0.3220.041  *  <em>Reverse612 0.178 -0.4460.009  *  *  </em>-0.0710.690Panel C: 405B ModelOption Random 1619 0.169 -0.1890.196-0.1270.346Option Random 2619 0.169 -0.1890.196-0.2680.060  *Option Random 1619 0.169 -0.1890.196-0.1850.202Option Random 2619 0.169 -0.1890.196-0.1000.520Reverse619 0.169 -0.1890.1960.0380.793Notes:
*</p>
<p>On the limitations of large language models (llms): False attribution. T Adewumi, N Habib, L Alkhaled, E Barney, arXiv:2404.046312024arXiv preprint</p>
<p>Out of one, many: Using language models to simulate human samples. L P Argyle, E C Busby, N Fulda, J R Gubler, C Rytting, D Wingate, Political Analysis. 3132023</p>
<p>Can generative ai improve social science?. C A Bail, Proceedings of the National Academy of Sciences. 12121e23140211212024</p>
<p>Using the theory of satisficing to evaluate the quality of survey data. S Barge, H Gehlbach, Research in Higher Education. 5322012</p>
<p>Separating the shirkers from the workers? making sure respondents pay attention on self-administered surveys. A J Berinsky, M F Margolis, M W Sances, American journal of political science. 5832014</p>
<p>Synthetic replacements for human survey data? the perils of large language models. J Bisbee, J D Clinton, C Dorff, B Kenkel, J M Larson, Political Analysis. 3242024</p>
<p>Latent dirichlet allocation. D M Blei, A Y Ng, M I Jordan, Journal of machine Learning research. 32003. Jan</p>
<p>Semantics derived automatically from language corpora contain human-like biases. A Caliskan, J J Bryson, A Narayanan, Science. 35663342017</p>
<p>Historical representations of social groups across 200 years of word embeddings from google books. T E Charlesworth, A Caliskan, M R Banaji, Proceedings of the National Academy of Sciences. 11928e21217981192022</p>
<p>Systematic testing of three language models reveals low language accuracy, absence of response stability, and a yes-response bias. V Dentella, F Günther, E Leivada, Proceedings of the National Academy of Sciences. 12051e23095831202023</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies. the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies20191</p>
<p>Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines. L Floridi, M Chiriatti, 202030</p>
<p>From words to numbers: Narrative, data, and social science. R Franzosi, 2004Cambridge University Press22</p>
<p>Text as data: The promise and pitfalls of automatic content analysis methods for political texts. J Grimmer, B M Stewart, Political analysis. 2132013</p>
<p>Ascertaining the validity of individual protocols from web-based personality inventories. J A Johnson, Journal of research in personality. 3912005</p>
<p>J Kim, J Evans, A Schein, arXiv:2503.02080Linear representations of political perspective emerge in large language models. 2025arXiv preprint</p>
<p>A C Kozlowski, H Kwon, J A Evans, arXiv:2407.11190silico sociology: forecasting covid-19 polarization with large language models. 2024arXiv preprint</p>
<p>The geometry of culture: Analyzing the meanings of class through word embeddings. A C Kozlowski, M Taddy, J A Evans, American Sociological Review. 8452019</p>
<p>Response strategies for coping with the cognitive demands of attitude measures in surveys. J A Krosnick, Applied cognitive psychology. 531991</p>
<p>Inference-time intervention: Eliciting truthful answers from a language model. K Li, O Patel, F Viégas, H Pfister, M Wattenberg, Advances in Neural Information Processing Systems. 362023</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>'the formula that killed wall street': The gaussian copula and modelling practices in investment banking. D Mackenzie, T Spears, Social Studies of Science. 4432014</p>
<p>Tacit knowledge, weapons design, and the uninvention of nuclear weapons. D Mackenzie, G Spinardi, American journal of sociology. 10111995</p>
<p>The social shaping of technology. D Mackenzie, J Wajcman, 1999Open University</p>
<p>A tale of two cultures: Contrasting quantitative and qualitative research. J Mahoney, G Goertz, Political analysis. 1432006</p>
<p>Identifying careless responses in survey data. A W Meade, S B Craig, Psychological methods. 1734372012</p>
<p>Introducing llama 3.1: Our most capable models to date. A I Meta, 2024. 2025/01/28</p>
<p>Algorithms of oppression: How search engines reinforce racism. S U Noble, Algorithms of oppression. New York university press2018</p>
<p>Using proprietary language models in academic research requires explicit justification. A Palmer, N A Smith, A Spirling, Nature Computational Science. 412024</p>
<p>Large language models sensitivity to the order of options in multiplechoice questions. P Pezeshkpour, E Hruschka, arXiv:2308.114832023arXiv preprint</p>
<p>Word embeddings: What works, what doesn't, and how to tell the difference for applied research. P L Rodriguez, A Spirling, The Journal of Politics. 8412022</p>
<p>C M Rytting, T Sorensen, L Argyle, E Busby, N Fulda, J Gubler, D Wingate, arXiv:2306.02177Towards coding social science datasets with language models. 2023arXiv preprint</p>
<p>Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines. F Shi, J Evans, Nature Communications. 14116412023</p>
<p>A behavioral model of rational choice. Models of man, social and rational: Mathematical essays on rational human behavior in a social setting. H Simon, 19576</p>
<p>Construct validity: Advances in theory and methodology. M E Strauss, G T Smith, Annual review of clinical psychology. 512009</p>
<p>Large language models for data annotation: A survey. Z Tan, A Beigi, S Wang, R Guo, A Bhattacharjee, B Jiang, M Karami, J Li, L Cheng, H Liu, 20242402arXiv e-prints</p>
<p>Best practices for text annotation with large language models. P Törnberg, arXiv:2402.051292024arXiv preprint</p>
<p>Vox populi, vox ai? using language models to estimate german public opinion. L Von Der Heyde, A.-C Haensch, A Wenz, arXiv:2407.085632024arXiv preprint</p>
<p>Dealing with careless responding in survey data: Prevention, identification, and recommended best practices. M K Ward, A W Meade, Annual review of psychology. 7412023</p>
<p>Cipta: Contrastive-based iterative prompt-tuning using text annotation from large language models. Y Yan, W Du, D Yang, D Yin, 2023 4th International Conference on Electronic Communication and Artificial Intelligence (ICECAI). IEEE2023</p>
<p>Z Zhang, L Lei, L Wu, R Sun, Y Huang, C Long, X Liu, X Lei, J Tang, M Huang, arXiv:2309.07045Safetybench: Evaluating the safety of large language models. 2023arXiv preprint</p>
<p>Can large language models transform computational social science?. C Ziems, W Held, O Shaikh, J Chen, Z Zhang, D Yang, Computational Linguistics. 5012024</p>
<p>A Zou, L Phan, S Chen, J Campbell, P Guo, R Ren, A Pan, X Yin, M Mazeika, A.-K Dombrowski, arXiv:2310.01405Representation engineering: A top-down approach to ai transparency. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
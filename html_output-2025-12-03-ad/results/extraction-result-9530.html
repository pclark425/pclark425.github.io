<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9530 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9530</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9530</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-271544398</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.20906v5.pdf" target="_blank">Automated literature research and review-generation method based on large language models</a></p>
                <p><strong>Paper Abstract:</strong> Abstract Literature research, which is vital for scientific work, faces the challenge of surging information volumes that are exceeding researchers’ processing capabilities. This paper describes an automated review-generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring user domain knowledge. Applied to propane dehydrogenation catalysts, our method demonstrated two aspects: first, generating comprehensive reviews from 343 articles spanning 35 topics; and, second, evaluating data-mining capabilities by using 1041 articles for experimental catalyst property analysis. Through multilayered quality control, we effectively mitigated the hallucinations of LLMs, with expert verification confirming accuracy and citation integrity, while demonstrating hallucination risks reduced to <0.5% with 95% confidence. The released software application enables one-click review generation, enhancing research productivity and literature-recommendation efficiency while facilitating broader scientific explorations.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9530.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9530.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoReview-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end pipeline that uses LLMs to retrieve, extract, aggregate, and compose literature-derived knowledge into full review texts and data-mining outputs, with multi-level hallucination mitigation and a dual-baseline evaluation framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Claude 2 (primary processing); Qwen2-7b-Instruct; Qwen2-72b-Instruct; Claude3.5Sonnet (evaluation comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Claude 2 (closed-source Anthropic model used for main data-processing tasks in this paper); Qwen2-7b-Instruct (open-source, 7-billion-parameter instruct-tuned model from Qwen2 series); Qwen2-72b-Instruct (open-source, 72-billion-parameter instruct-tuned Qwen2 variant); Claude3.5Sonnet (closed-source, higher-capability Claude series model used for comparative evaluation). The paper does not provide additional architecture details beyond model family and parameter sizes for the Qwen2 models.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry / Chemical engineering (case study: propane dehydrogenation (PDH) catalysts); designed to be cross-disciplinary.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Automated retrieval from Google Scholar (via SerpAPI) of Q1 chemistry and chemical engineering journal articles (1980–2024) yielded 1,420 initial results; coarse filtering (title/abstract) shortlisted 343 articles related to PDH catalysts, LLM full-text evaluation confirmed 238 as relevant. For extended data-mining the pipeline processed up to 1,041 filtered articles and analyzed 839 PDH catalyst papers (Q1–Q3 journals) for aggregated analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Structure–activity relationships and thematic performance heuristics (empirical design rules and thematic patterns summarizing catalyst composition, supports, promoters, and preparation effects).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Observed rules synthesized by the system include: (1) promoter elements Zn, Sn, and La tend to improve PDH catalyst performance; (2) alumina and zeolite supports are associated with superior performance; (3) Pt-based catalysts combined with Sn, Zn, or In promoters show improved selectivity; (4) impregnation-prepared nanometallic catalysts show high conversion and selectivity, while single-atom alloys show high selectivity but lower conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Retrieval-augmented, multi-stage LLM pipeline: automated retrieval → topic formulation → LLM-generated extraction questions → multiple rounds of LLM information extraction per article (segmented when texts exceed context window) → repeated querying (typically 5 repetitions) and aggregation (self-consistency aggregation) → XML-structured parsing for data mining → paragraph construction with DOI-linked citations. Additional safeguards include tight prompt templates, segmented single-round generation to avoid truncation, format (XML) filtering, DOI verification, relevance checks, and repeated-answer aggregation for self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Dual-baseline evaluation: (1) manual Q1 journal reviews (14 published reviews segmented into 89 fragments) regenerated with identical cited literature for direct comparison, and (2) direct LLM generation baseline; scoring performed via LLM-based evaluators with cross-evaluation, repetitions, chain-of-thought scoring across 27 items in 9 categories; reliability assessed by Intraclass Correlation Coefficient (ICC) and Transitive Consistency Ratio (TCR); manual spot-check verification by a domain PhD student on random samples (25 articles) to compute accuracy, precision, recall, false-positive rates and consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Generated reviews matched or exceeded manual review quality in many metrics: average paragraph scores relative to manual reviews were 43.94% (Qwen2-7b), 64.81% (Qwen2-72b), and Claude3.5Sonnet exceeded manual scores by 23.63% on average. Optimal-paragraph selection across multiple generations yielded best-paragraph scores up to 89.07% (Qwen2-7b), 92.64% (Qwen2-72b), and 130.79% (Claude3.5Sonnet) relative to manual benchmarks. Data-mining produced domain heuristics (promoters, supports, alloy vs single-atom tradeoffs) and visualizations. Hallucination mitigation via multi-level checks reduced false-positive hallucinations in the critical knowledge-extraction stage to below 0.5% with 95% confidence based on manual sampling of 875 outputs. Aggregation and repetition empirically improved accuracy (binomial estimates: a base 79.09% accuracy model aggregates to ~93.5% after 5 repetitions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared to direct LLM generation baseline, the pipeline substantially improved output quality via structured extraction, repetition, and aggregation. Against manual expert reviews (dual-baseline), the best automated outputs approached or exceeded human paragraphs for stronger models (Claude3.5Sonnet) and were improved for smaller models via multiple-generation selection. The authors also compare RAG vs fine-tuning conceptually, noting RAG-style contextualization supports out-of-the-box generalization and lower initial cost than domain fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Hallucinations remain an inherent risk (especially in narrow subfields); smaller models (e.g., Qwen2-7b) underperform on logical reasoning; data-mining exhibited unit-conversion and definition errors; authors did not fine-tune LLMs for domain-specific pretraining (DAPT) to favor out-of-the-box cross-disciplinary generalization; proprietary/full-text dataset access is restricted by copyright and not publicly shareable.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Paper documents hallucination sources (training noise, statistical bias, alignment decision heuristics) and reports higher false-positive rates in direct data-mining (FP up to ~35.3% in direct responses) before aggregation. Mitigation pipeline (format filtering, DOI checks, relevance checks, self-consistency/repetition, traceability by DOI) reduced hallucinations in knowledge extraction to below 0.5% at 95% confidence; remaining hallucinations in data-mining often involved unit/definition errors or false negatives from abstraction gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9530.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9530.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDH-heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Propane Dehydrogenation (PDH) Catalyst Design Heuristics Extracted by LLM Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of empirically derived qualitative rules and thematic patterns about PDH catalyst composition, supports, promoters, and preparation methods synthesized from hundreds of papers via the LLM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Claude 2 (primary extraction and aggregation); Qwen2 variants used in evaluation; GPT-4 referenced in background but not used.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Claude 2 used as the main LLM for extraction/aggregation tasks in the PDH case study; Qwen2-7b and Qwen2-72b and Claude3.5Sonnet were evaluated and compared during quality-assessment experiments. (Paper provides Qwen sizes 7B and 72B; other model internals not detailed.)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemistry — heterogeneous catalysis, specifically propane dehydrogenation catalyst performance and design.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Corpus included 343 topic-related articles after initial screening (from an initial 1,420 Google Scholar results); extended analyses used 1,041 filtered articles and a focused set of 839 PDH catalyst papers (Q1–Q3 journals, 1980–2024) for data-mining and visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Empirical catalyst design heuristics — structure–activity and composition–performance relationships and thematic research trends over time.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Examples provided by the paper: (a) alloy research in PDH increased since ~1995; single-atom catalyst studies increased post-2015; (b) promoter ranking shows Zn, Sn, and La as optimal promoters; (c) supports such as alumina and zeolites correlate with improved performance; (d) multi-metal Pt-based systems with Sn/Zn/In show superior selectivity; (e) impregnation-prepared nanometallic catalysts yield high conversion and selectivity, whereas single-atom alloys favor selectivity but have lower conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Data-mining module extends knowledge-extraction: LLM parses structured XML extractions across multiple rounds, standardizes and cleans extracted data, aggregates results, and generates statistical analyses and visualizations (GPT-4 generated code used for plotting). The pipeline used segmentation for long texts, repeated parsing (commonly 5 repetitions per question), aggregation of responses, and DOI-linked provenance for each extracted datum.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Manual verification: 25 randomly selected articles were evaluated by a third-year PDH-specialist PhD student. Knowledge-extraction sampled 35 segments per article (875 data points); data-mining evaluated 14 catalyst properties across 1,750 and 350 data points (direct and aggregated). Metrics computed included accuracy, false positive rate (with 95% CI), precision, recall, F1, and consistency rate (see Table 1). Self-consistency checks via LLM aggregation and further LLM-based comparison prompts (Claude2 used to compare pre/post aggregation) were employed.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The data-mining module produced domain-level insights and visual analyses (publication trends, promoter/support performance, composition–performance bubbles). Aggregated data-mining accuracy improved substantially after repetition/aggregation (e.g., aggregated data-mining accuracy reported ≈93.71% with reduced FP compared to direct responses). Direct data-mining responses had higher FP (~35.34%), while aggregation reduced hallucinations. Manual verification found no fabricated conclusions in knowledge extraction and established high traceability via DOIs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>No explicit numeric comparison to traditional meta-analysis pipelines is given, but authors state that RAG-style contextualization and LLM aggregation outperform direct generation baselines and offer lower initial cost than domain fine-tuning; aggregation improved results relative to single direct responses.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Data-mining stage more prone to unit-conversion and definition errors (counted as false positives), LLMs less reliable on highly abstract or specialized concepts, and the absence of domain-adaptive pretraining may limit capturing subtle scientific abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Data-mining direct responses showed substantial false positives prior to aggregation; hallucinations chiefly involved correct numerical extraction but incorrect units/definitions. Aggregation and DOI-traceability mitigated fabricated claims; however, false negatives persisted where LLMs failed to extract highly abstracted statements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9530.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9530.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual-Baseline Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual-Baseline Review Quality Evaluation Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-centric evaluation framework that compares LLM-generated reviews against two baselines (manual Q1 journal reviews and direct LLM generation) using LLM-based scorers, statistical reliability tests (ICC, TCR), and repeated cross-evaluations to reduce bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Qwen2-72b-Instruct (used as a scorer); Claude3.5Sonnet and Qwen2 variants used in generation/evaluation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Qwen2-72b-Instruct: 72B-parameter open-source instruct-tuned model used as a consistent evaluator/scorer in the study; Qwen2-7b-Instruct (7B) used but found less reliable for scoring; Claude3.5Sonnet used to generate and evaluate example outputs. The paper provides no further architectural details beyond model family and Qwen parameter sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General evaluation methodology applied to scholarly review text generation; demonstrated in chemistry PDH review case study but designed to be domain-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>14 published Q1 reviews were semantically segmented into 89 fragments; the fragments' cited literature was used to regenerate comparative reviews with LLMs for direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Not a scientific law extractor per se, but a framework used to evaluate whether LLMs can produce review-quality syntheses (i.e., human-level generalization rules captured in review fragments).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>N/A — framework evaluates synthesis quality rather than extracting a specific rule. It was used to judge whether LLM outputs capture the same high-level claims as human review fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Evaluation pipeline: segment human reviews, regenerate equivalent content from the same cited literature using the automated method, then apply LLM-based scoring (self-scoring and consistent cross-scoring), use page-rerank algorithm to convert comparisons into absolute 0–10 scores, and validate scorer reliability using ICC and TCR. Repetition and cross-evaluation used to mitigate evaluator biases.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Reliability tests: Intraclass Correlation Coefficient (ICC) and Transitive Consistency Ratio (TCR). Scoring performed by LLM evaluators with cross-evaluation; repeated generation (9 runs per model) yielded average and optimal-paragraph statistics. Manual spot checks supplemented LLM-scoring to compute hallucination rates.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Evaluator reliability: Claude3.5Sonnet and Qwen2-72b-Instruct produced reliable scoring meeting human-evaluator standards; Qwen2-7b-Instruct was insufficiently reliable as a scorer. Using this framework, average and optimal generation scores relative to manual reviews were computed (see AutoReview-LLM entry). The framework enabled statistically validated claims that the automated method can approach or exceed human review quality for certain models and settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Baselines were (1) human Q1 reviews and (2) direct LLM generation. The dual-baseline approach allowed quantitative comparison: direct generation performed worse than the structured automated pipeline; large-capacity models using the pipeline matched or outperformed human baselines on averaged metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>LLM-based evaluators can harbor biases (position, length, self-bias); smaller LLMs may not be reliable evaluators; evaluation depends on model capability so as models improve, framework performance changes. The framework uses LLM evaluators exclusively to avoid human evaluator bias but acknowledges remaining evaluator biases.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Discusses evaluator biases (position, length, self-bias) and addresses them via cross-evaluation, repetition, chain-of-thought prompts, and statistical reliability testing. The framework recognizes potential evaluation bias when LLM-generated content closely matches human style (quality-related bias).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Paperqa: Retrieval-augmented generative agent for scientific research <em>(Rating: 2)</em></li>
                <li>PaperQA2 <em>(Rating: 2)</em></li>
                <li>LitLLM <em>(Rating: 2)</em></li>
                <li>CuriousLLM <em>(Rating: 2)</em></li>
                <li>Language agents achieve superhuman synthesis of scientific knowledge <em>(Rating: 2)</em></li>
                <li>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation <em>(Rating: 2)</em></li>
                <li>LLAssist <em>(Rating: 1)</em></li>
                <li>ChatCite <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9530",
    "paper_id": "paper-271544398",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "AutoReview-LLM",
            "name_full": "Automated Review Generation Method Based on Large Language Models",
            "brief_description": "An end-to-end pipeline that uses LLMs to retrieve, extract, aggregate, and compose literature-derived knowledge into full review texts and data-mining outputs, with multi-level hallucination mitigation and a dual-baseline evaluation framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Claude 2 (primary processing); Qwen2-7b-Instruct; Qwen2-72b-Instruct; Claude3.5Sonnet (evaluation comparisons)",
            "llm_model_description": "Claude 2 (closed-source Anthropic model used for main data-processing tasks in this paper); Qwen2-7b-Instruct (open-source, 7-billion-parameter instruct-tuned model from Qwen2 series); Qwen2-72b-Instruct (open-source, 72-billion-parameter instruct-tuned Qwen2 variant); Claude3.5Sonnet (closed-source, higher-capability Claude series model used for comparative evaluation). The paper does not provide additional architecture details beyond model family and parameter sizes for the Qwen2 models.",
            "application_domain": "Chemistry / Chemical engineering (case study: propane dehydrogenation (PDH) catalysts); designed to be cross-disciplinary.",
            "input_corpus_description": "Automated retrieval from Google Scholar (via SerpAPI) of Q1 chemistry and chemical engineering journal articles (1980–2024) yielded 1,420 initial results; coarse filtering (title/abstract) shortlisted 343 articles related to PDH catalysts, LLM full-text evaluation confirmed 238 as relevant. For extended data-mining the pipeline processed up to 1,041 filtered articles and analyzed 839 PDH catalyst papers (Q1–Q3 journals) for aggregated analyses.",
            "qualitative_law_type": "Structure–activity relationships and thematic performance heuristics (empirical design rules and thematic patterns summarizing catalyst composition, supports, promoters, and preparation effects).",
            "qualitative_law_example": "Observed rules synthesized by the system include: (1) promoter elements Zn, Sn, and La tend to improve PDH catalyst performance; (2) alumina and zeolite supports are associated with superior performance; (3) Pt-based catalysts combined with Sn, Zn, or In promoters show improved selectivity; (4) impregnation-prepared nanometallic catalysts show high conversion and selectivity, while single-atom alloys show high selectivity but lower conversion.",
            "extraction_methodology": "Retrieval-augmented, multi-stage LLM pipeline: automated retrieval → topic formulation → LLM-generated extraction questions → multiple rounds of LLM information extraction per article (segmented when texts exceed context window) → repeated querying (typically 5 repetitions) and aggregation (self-consistency aggregation) → XML-structured parsing for data mining → paragraph construction with DOI-linked citations. Additional safeguards include tight prompt templates, segmented single-round generation to avoid truncation, format (XML) filtering, DOI verification, relevance checks, and repeated-answer aggregation for self-consistency.",
            "evaluation_method": "Dual-baseline evaluation: (1) manual Q1 journal reviews (14 published reviews segmented into 89 fragments) regenerated with identical cited literature for direct comparison, and (2) direct LLM generation baseline; scoring performed via LLM-based evaluators with cross-evaluation, repetitions, chain-of-thought scoring across 27 items in 9 categories; reliability assessed by Intraclass Correlation Coefficient (ICC) and Transitive Consistency Ratio (TCR); manual spot-check verification by a domain PhD student on random samples (25 articles) to compute accuracy, precision, recall, false-positive rates and consistency.",
            "results_summary": "Generated reviews matched or exceeded manual review quality in many metrics: average paragraph scores relative to manual reviews were 43.94% (Qwen2-7b), 64.81% (Qwen2-72b), and Claude3.5Sonnet exceeded manual scores by 23.63% on average. Optimal-paragraph selection across multiple generations yielded best-paragraph scores up to 89.07% (Qwen2-7b), 92.64% (Qwen2-72b), and 130.79% (Claude3.5Sonnet) relative to manual benchmarks. Data-mining produced domain heuristics (promoters, supports, alloy vs single-atom tradeoffs) and visualizations. Hallucination mitigation via multi-level checks reduced false-positive hallucinations in the critical knowledge-extraction stage to below 0.5% with 95% confidence based on manual sampling of 875 outputs. Aggregation and repetition empirically improved accuracy (binomial estimates: a base 79.09% accuracy model aggregates to ~93.5% after 5 repetitions).",
            "comparison_to_baseline": "Compared to direct LLM generation baseline, the pipeline substantially improved output quality via structured extraction, repetition, and aggregation. Against manual expert reviews (dual-baseline), the best automated outputs approached or exceeded human paragraphs for stronger models (Claude3.5Sonnet) and were improved for smaller models via multiple-generation selection. The authors also compare RAG vs fine-tuning conceptually, noting RAG-style contextualization supports out-of-the-box generalization and lower initial cost than domain fine-tuning.",
            "reported_limitations": "Hallucinations remain an inherent risk (especially in narrow subfields); smaller models (e.g., Qwen2-7b) underperform on logical reasoning; data-mining exhibited unit-conversion and definition errors; authors did not fine-tune LLMs for domain-specific pretraining (DAPT) to favor out-of-the-box cross-disciplinary generalization; proprietary/full-text dataset access is restricted by copyright and not publicly shareable.",
            "bias_or_hallucination_issues": "Paper documents hallucination sources (training noise, statistical bias, alignment decision heuristics) and reports higher false-positive rates in direct data-mining (FP up to ~35.3% in direct responses) before aggregation. Mitigation pipeline (format filtering, DOI checks, relevance checks, self-consistency/repetition, traceability by DOI) reduced hallucinations in knowledge extraction to below 0.5% at 95% confidence; remaining hallucinations in data-mining often involved unit/definition errors or false negatives from abstraction gaps.",
            "uuid": "e9530.0",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "PDH-heuristics",
            "name_full": "Propane Dehydrogenation (PDH) Catalyst Design Heuristics Extracted by LLM Pipeline",
            "brief_description": "A set of empirically derived qualitative rules and thematic patterns about PDH catalyst composition, supports, promoters, and preparation methods synthesized from hundreds of papers via the LLM pipeline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Claude 2 (primary extraction and aggregation); Qwen2 variants used in evaluation; GPT-4 referenced in background but not used.",
            "llm_model_description": "Claude 2 used as the main LLM for extraction/aggregation tasks in the PDH case study; Qwen2-7b and Qwen2-72b and Claude3.5Sonnet were evaluated and compared during quality-assessment experiments. (Paper provides Qwen sizes 7B and 72B; other model internals not detailed.)",
            "application_domain": "Chemistry — heterogeneous catalysis, specifically propane dehydrogenation catalyst performance and design.",
            "input_corpus_description": "Corpus included 343 topic-related articles after initial screening (from an initial 1,420 Google Scholar results); extended analyses used 1,041 filtered articles and a focused set of 839 PDH catalyst papers (Q1–Q3 journals, 1980–2024) for data-mining and visualization.",
            "qualitative_law_type": "Empirical catalyst design heuristics — structure–activity and composition–performance relationships and thematic research trends over time.",
            "qualitative_law_example": "Examples provided by the paper: (a) alloy research in PDH increased since ~1995; single-atom catalyst studies increased post-2015; (b) promoter ranking shows Zn, Sn, and La as optimal promoters; (c) supports such as alumina and zeolites correlate with improved performance; (d) multi-metal Pt-based systems with Sn/Zn/In show superior selectivity; (e) impregnation-prepared nanometallic catalysts yield high conversion and selectivity, whereas single-atom alloys favor selectivity but have lower conversion.",
            "extraction_methodology": "Data-mining module extends knowledge-extraction: LLM parses structured XML extractions across multiple rounds, standardizes and cleans extracted data, aggregates results, and generates statistical analyses and visualizations (GPT-4 generated code used for plotting). The pipeline used segmentation for long texts, repeated parsing (commonly 5 repetitions per question), aggregation of responses, and DOI-linked provenance for each extracted datum.",
            "evaluation_method": "Manual verification: 25 randomly selected articles were evaluated by a third-year PDH-specialist PhD student. Knowledge-extraction sampled 35 segments per article (875 data points); data-mining evaluated 14 catalyst properties across 1,750 and 350 data points (direct and aggregated). Metrics computed included accuracy, false positive rate (with 95% CI), precision, recall, F1, and consistency rate (see Table 1). Self-consistency checks via LLM aggregation and further LLM-based comparison prompts (Claude2 used to compare pre/post aggregation) were employed.",
            "results_summary": "The data-mining module produced domain-level insights and visual analyses (publication trends, promoter/support performance, composition–performance bubbles). Aggregated data-mining accuracy improved substantially after repetition/aggregation (e.g., aggregated data-mining accuracy reported ≈93.71% with reduced FP compared to direct responses). Direct data-mining responses had higher FP (~35.34%), while aggregation reduced hallucinations. Manual verification found no fabricated conclusions in knowledge extraction and established high traceability via DOIs.",
            "comparison_to_baseline": "No explicit numeric comparison to traditional meta-analysis pipelines is given, but authors state that RAG-style contextualization and LLM aggregation outperform direct generation baselines and offer lower initial cost than domain fine-tuning; aggregation improved results relative to single direct responses.",
            "reported_limitations": "Data-mining stage more prone to unit-conversion and definition errors (counted as false positives), LLMs less reliable on highly abstract or specialized concepts, and the absence of domain-adaptive pretraining may limit capturing subtle scientific abstractions.",
            "bias_or_hallucination_issues": "Data-mining direct responses showed substantial false positives prior to aggregation; hallucinations chiefly involved correct numerical extraction but incorrect units/definitions. Aggregation and DOI-traceability mitigated fabricated claims; however, false negatives persisted where LLMs failed to extract highly abstracted statements.",
            "uuid": "e9530.1",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Dual-Baseline Eval",
            "name_full": "Dual-Baseline Review Quality Evaluation Framework",
            "brief_description": "An LLM-centric evaluation framework that compares LLM-generated reviews against two baselines (manual Q1 journal reviews and direct LLM generation) using LLM-based scorers, statistical reliability tests (ICC, TCR), and repeated cross-evaluations to reduce bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Qwen2-72b-Instruct (used as a scorer); Claude3.5Sonnet and Qwen2 variants used in generation/evaluation experiments.",
            "llm_model_description": "Qwen2-72b-Instruct: 72B-parameter open-source instruct-tuned model used as a consistent evaluator/scorer in the study; Qwen2-7b-Instruct (7B) used but found less reliable for scoring; Claude3.5Sonnet used to generate and evaluate example outputs. The paper provides no further architectural details beyond model family and Qwen parameter sizes.",
            "application_domain": "General evaluation methodology applied to scholarly review text generation; demonstrated in chemistry PDH review case study but designed to be domain-agnostic.",
            "input_corpus_description": "14 published Q1 reviews were semantically segmented into 89 fragments; the fragments' cited literature was used to regenerate comparative reviews with LLMs for direct comparison.",
            "qualitative_law_type": "Not a scientific law extractor per se, but a framework used to evaluate whether LLMs can produce review-quality syntheses (i.e., human-level generalization rules captured in review fragments).",
            "qualitative_law_example": "N/A — framework evaluates synthesis quality rather than extracting a specific rule. It was used to judge whether LLM outputs capture the same high-level claims as human review fragments.",
            "extraction_methodology": "Evaluation pipeline: segment human reviews, regenerate equivalent content from the same cited literature using the automated method, then apply LLM-based scoring (self-scoring and consistent cross-scoring), use page-rerank algorithm to convert comparisons into absolute 0–10 scores, and validate scorer reliability using ICC and TCR. Repetition and cross-evaluation used to mitigate evaluator biases.",
            "evaluation_method": "Reliability tests: Intraclass Correlation Coefficient (ICC) and Transitive Consistency Ratio (TCR). Scoring performed by LLM evaluators with cross-evaluation; repeated generation (9 runs per model) yielded average and optimal-paragraph statistics. Manual spot checks supplemented LLM-scoring to compute hallucination rates.",
            "results_summary": "Evaluator reliability: Claude3.5Sonnet and Qwen2-72b-Instruct produced reliable scoring meeting human-evaluator standards; Qwen2-7b-Instruct was insufficiently reliable as a scorer. Using this framework, average and optimal generation scores relative to manual reviews were computed (see AutoReview-LLM entry). The framework enabled statistically validated claims that the automated method can approach or exceed human review quality for certain models and settings.",
            "comparison_to_baseline": "Baselines were (1) human Q1 reviews and (2) direct LLM generation. The dual-baseline approach allowed quantitative comparison: direct generation performed worse than the structured automated pipeline; large-capacity models using the pipeline matched or outperformed human baselines on averaged metrics.",
            "reported_limitations": "LLM-based evaluators can harbor biases (position, length, self-bias); smaller LLMs may not be reliable evaluators; evaluation depends on model capability so as models improve, framework performance changes. The framework uses LLM evaluators exclusively to avoid human evaluator bias but acknowledges remaining evaluator biases.",
            "bias_or_hallucination_issues": "Discusses evaluator biases (position, length, self-bias) and addresses them via cross-evaluation, repetition, chain-of-thought prompts, and statistical reliability testing. The framework recognizes potential evaluation bias when LLM-generated content closely matches human style (quality-related bias).",
            "uuid": "e9530.2",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Paperqa: Retrieval-augmented generative agent for scientific research",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "PaperQA2",
            "rating": 2
        },
        {
            "paper_title": "LitLLM",
            "rating": 2
        },
        {
            "paper_title": "CuriousLLM",
            "rating": 2,
            "sanitized_title": "curiousllm"
        },
        {
            "paper_title": "Language agents achieve superhuman synthesis of scientific knowledge",
            "rating": 2,
            "sanitized_title": "language_agents_achieve_superhuman_synthesis_of_scientific_knowledge"
        },
        {
            "paper_title": "System for systematic literature review using multiple ai agents: Concept and an empirical evaluation",
            "rating": 2,
            "sanitized_title": "system_for_systematic_literature_review_using_multiple_ai_agents_concept_and_an_empirical_evaluation"
        },
        {
            "paper_title": "LLAssist",
            "rating": 1
        },
        {
            "paper_title": "ChatCite",
            "rating": 1
        }
    ],
    "cost": 0.013915,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automated Review Generation Method Based on Large Language Models
1 May 2025</p>
<p>Shican Wu 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Xiao Ma 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Dehui Luo 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Lulu Li 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Xiangcheng Shi 
Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Xin Chang 
Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Xiaoyun Lin 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Ran Luo 
Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Chunlei Pei 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Zhejiang Institute of Tianjin University Ningbo
315201ZhejiangChina</p>
<p>Changying Du 
AIStrucX Technologies
No. 26, Information Road, Haidian District100000BeĳingChina</p>
<p>Zhi-Jian Zhao zjzhao@tju.edu.cn 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>International Joint Laboratory of Low-carbon Chemical Engineering
300192TianjinChina</p>
<p>Jinlong Gong jlgong@tju.edu.cn 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Zhejiang Institute of Tianjin University Ningbo
315201ZhejiangChina</p>
<p>International Joint Laboratory of Low-carbon Chemical Engineering
300192TianjinChina</p>
<p>Haihe Laboratory of Sustainable Chemical Transformations
300192TianjinChina</p>
<p>National Industry-Education Platform of Energy Storage
Tianjin University
135 Yaguan Road300350TianjinChina</p>
<p>Tianjin Normal University
300387TianjinChina</p>
<p>Automated Review Generation Method Based on Large Language Models
1 May 20254DB21C7AD76586CF6B82D3C2D7169717arXiv:2407.20906v5[cs.CL]large language modelsautomated review generationliterature analysisscientific writing
Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers' processing capabilities.We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load.Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users' domain knowledge.Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts' properties.Through multi-layered quality control, we effectively mitigated LLMs' hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5% with 95% confidence.Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations.</p>
<p>INTRODUCTION</p>
<p>Peer-reviewed academic literature functions as a critical medium for scientific knowledge dissemination, enabling researchers to advance human understanding through cumulative progress [1].The clarity and rigor of scientific language facilitates entity description, concept extraction, and consensus building, ensuring cognitive consistency between information senders and receivers.However, the exponential growth in publications has exceeded researcher' processing capacity [2,3,4], necessitating efficient tools for literature analysis and integration, thus avoiding redundant discoveries and broadening research perspectives.</p>
<p>Natural language processing (NLP), encompassing co-reference resolution, semantic analysis, etc. [5], powers literature comprehension.Since November 2022, Large Language Models like ChatGPT, as the latest NLP advancement, have exhibited unprecedented language understanding [6].Leading LLMs have surpassed human performance on various benchmarks including MMLU [7], which tests undergraduate knowledge, and GPQA Diamond [8], which assesses graduate-level reasoning, positioning them as potential "second brains" for efficient literature processing [6,9].Recent studies like PaperQA [10] and its improved version PaperQA2 [11], as retrieval-augmented generation (RAG [12]) agents, demonstrate excellent performance in literature-related tasks including retrieval, question-answering, summarization, and contradiction detection, surpassing human expertise in some aspects.AcademicGPT [13] provides comprehensive research support, while CuriousLLM [14] enhances multi-document questionanswering through reasoning-based traversal agents.However, these applications require user-provided literature, rely on question-answer interactions, or focus on specific points, limiting their transferability.</p>
<p>The review format effectively integrates literature information and generalizes across disciplinary fields, naturally leading to automated review generation research.However, early attempts encountered several limitations.They either reduced reviews to multi-document summarization [15] or depended on existing reviews and citation networks [16,17,18,19,20], which struggled with rapidly evolving fields and underestimated recent publications due to citation lag.Additional constraints included using only abstracts instead of full texts as input data [17,18,19,20] and employing either extractive summarization rather than integrated generation [16,17,18] or template-based generation [19], risking information loss and redundancy.Recent LLM-based solutions include: multi-AI agent systems [21] for full-process automation from research question generation to data extraction; LitLLM [22] combining RAG with LLM reranking to generate high-quality literature reviews based on user-provided abstracts; LLAssist [23] and related work [24] for literature screening; and ChatCite [25] improving summary quality through human-like workflows.These advances enhance automated review generation while enabling efficient academic research.</p>
<p>Based on the potential of LLMs, this study proposes an automated review generation method based on LLMs, builds an end-to-end data pipeline from literature retrieval to final review text generation.By leveraging information refinement and knowledge construction capabilities of LLMs, this method overcomes human cognitive limitations in single-threaded processing and memory capacity, reducing researchers' cognitive load while offering superior speed and scalability, thereby substantially conserving professional human resources.However, two critical challenges persist: the macro-level requirement for systematic quality evaluation and comparison with manual reviews, and the micro-level necessity to effectively mitigate LLM hallucinations.To address these challenges, we designed a dual-baseline automatic evaluation framework with rigorous statistical validation, alongside multi-level quality control strategies throughout the process.The distinctive feature of method lies in its adaptability to diverse disciplinary terminologies and knowledge structures without domain-specific training, facilitating both comprehensive field overviews for experienced researchers and accessible entry points for those lacking relevant background, opening up new possibilities for promoting interdisciplinary research and knowledge dissemination.This approach holds substantial scientific significance by enhancing literature processing efficiency, fostering knowledge discovery, and stimulating innovation.Its ability to promote interdisciplinary communication and knowledge integration positions it as a potential cornerstone of modern research infrastructure, accelerating scientific discovery and technological advancement across domains.</p>
<p>RESULTS AND DISCUSSION</p>
<p>Automated retrieval</p>
<p>Automated review generation fundamentally relies on retrieving and extracting scientific literature, with output quality dependent on input timeliness, quality, and breadth.To demonstrate cross-disciplinary generalization without human intervention, we conducted a case study on propane dehydrogenation (PDH) catalysts, searching chemistry and chemical engineering journals (1980-2024) ranked Q1 in the Chinese Academy of Sciences journal classification on Google Scholar through SerpAPI.</p>
<p>The automated retrieval yielded 1420 initial results from Google Scholar.To address the challenge of irrelevant or duplicate findings, we implemented a dual-level filtering process.The first level employed quick filtering of abstracts and titles to remove obviously irrelevant documents, as detailed in Method section, serving as a rapid but less precise narrowing method.The second level involved deeper LLM-based analysis of full texts, offering higher accuracy albeit at a slower pace.This coarse-to-fine screening method, reminiscent of high-throughput screening, enabled us to identify literature pertinent efficiently and accurately to our research.The initial screening shortlisted 343 articles as related to our topic.Subsequent LLM evaluation further confirmed 238 of these articles as relevant.</p>
<p>Implementation and analysis of one-click automated review generation</p>
<p>Using PDH catalysts as an example and building on the aforementioned automated retrieval, we have effectively produced high-quality, specialized review articles.Considering that the entire process is completely end-to-end without the need for human intervention, we believe that a single domain example is sufficient to demonstrate the applicability of this method.The main reason for limiting the journal range to Q1 journals is that although the impact factor of journals may not be closely related to the quality of articles, the lower limit of literature in Q1 journals that have passed strict peer review is relatively higher.Considering users lacking prior knowledge in the target domain, directly traversing Q1 chemistry journals provides an efficient starting point.We solemnly declare here that we are not encouraging users to only consider Q1 journals, but rather suggesting that in the initial stage, one can consider starting with Q1 journals, and for research on the entire field, all possibilities should be explored, which is also supported by our method.In the Windows GUI we provide, using Q1/Q2-3 journals is an optional button, allowing users to choose for themselves.For those with domain familiarity, the program allows the specification of a custom journal list to refine article selection.</p>
<p>We evaluated two topic construction strategies: based on existing reviews (9 topics, 35 questions, 125 citations) and direct LLM generation (12 topics, 12 questions, 43 citations).The examples showcased in subsequent sections and SI are based on outlines derived from existing reviews.The content has been manually checked by experts in the relevant field, with no errors in knowledge, correct referencing of cited literature, and a length and citation count that align with conventional review standards (see SI).The method's effectiveness stems from LLMs' human-level or superior language comprehension abilities, coupled with the injection of domain knowledge from retrieved literature through context window, thereby enabling generalization across all research fields.Beyond content accuracy, the method enables customizable research focus through supporting of adding specific questions and provides forward-looking insights with comprehensive understanding sections.To facilitate broad adoption, we developed an open-source Python3 GUI enabling one-click review generation without programming expertise or domain knowledge.</p>
<p>Evaluation of generated review quality</p>
<p>Research demonstrates LLMs excel in evaluation tasks, with GPT-4 surpassing both crowdsourced workers [26] and experts [27] in text annotation accuracy and reliability, and bias control for complex tasks requiring contextual knowledge reasoning [27].LLMs show comparable or superior performance to human annotation in persuasiveness, accuracy, and satisfaction [28].Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29].GPT-4's evaluations exceeds 80% consistency with human reviewers [30] and exceeds 85% alignment in pairwise comparisons [31], reaching nearly 100% agreement when performance differences are significant [31].Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias [30], and self-bias [35], persist but can be reduced through proper design [36].</p>
<p>Given characteristic writing patterns of LLMs and potential human evaluation bias, this study employs LLM-based evaluation exclusively.Existing LLM-based LLM evaluation methods encompass scoring, comparison, selection judgment, and comprehensive description.This study introduces a dual-baseline review quality evaluation framework, to minimize potential LLM evaluation bias and quantitatively compare LLM-generated reviews with peer-reviewed expert-written content, validating reliability through statistical analysis.In our method, we segmented 14 published Q1 reviews into 89 fragments based on semantic content.Using extracted topics from these fragments and the literature cited in the original text, we generated comparative reviews using Qwen2-7b-Instruct, Qwen2-72b-Instruct, and Claude3.5Sonnet.This methodology enabled direct comparison between human experts and LLMs in writing reviews with identical literature background, establishing a rigorous benchmark for LLMs given their limitations in accessing both human accumulated domain expertise and pre-1970 undigitized literature.</p>
<p>In the evaluation process, we compared the performance of two models of different scales from the open-source Qwen2 series (Qwen2-7b-Instruct and Qwen2-72b-Instruct) and the closed-source model Claude3.5Sonnet(Fig. 1).Evaluation employed both selfscoring and uniform scoring by Qwen2-72b-Instruct.Intraclass Correlation Coefficient (ICC) tests and Transitive Consistency Ratio (TCR) analyses confirmed high reliability for Claude3.5Sonnet (Fig. 1(a,b,e,f)) and Qwen2-72b-Instruct (Fig. 1(c,d,e,f)), meeting human evaluation standards, while Qwen2-7b-Instruct's results necessitated substitution with Qwen2-72b-Instruct's scoring due to insufficient reliability.This phenomenon may stem from differences in capabilities between LLMs of different scales, specifically manifested in three dimensions: world knowledge, language understanding, and logical reasoning.We believe that small models differ significantly from large models in logical reasoning ability, while world knowledge has been supplemented through context-provided literature, and language ability differences are relatively small.For details on Qwen2-7b-Instruct's evaluation results, see SI. Model capability significantly impacts generation quality, while our method ensures a basic lower limit of generation quality.In repeated generation tests (9 times per model), average scores, taken as comprehensive performance of models, showed that Qwen2-7b-Instruct reached 43.94% of manual scoring, Qwen2-72b-Instruct reached 64.81% (Fig. 2(d,f)), and Claude3.5Sonnetexceeding manual scores by 23.63% (Fig. 2(c,f)), all significantly higher than the baseline level of direct generation.Optimal performance analysis, which taking the paragraph with the highest total score among those generated by each model as the optimal paragraph, revealed best-paragraph scores of 89.07%for Qwen2-7b-Instruct, 92.64% for Qwen2-72b-Instruct (Fig. 2(b,f)), and 130.79% for Claude3.5Sonnetrelative to manual scores (Fig. 2(a,f)).These results indicate multiple generations and selecting the best paragraph can improve performance of smaller models, while larger models maintain consistent quality.For optimal results, we recommend using larger models when possible; otherwise, multiple generations can enhance final performance in hardware-constrained scenarios.For details on Qwen2-7b-Instruct's evaluation results, see SI.The near-human scores of optimal paragraphs might reflect quality-related bias [37] because their closely approximate human-level quality could make LLMs' potential evaluation bias more prominent, while the distinctly inferior quality of direct generation remains easily distinguishable.Analysis of optimal paragraphs reveals high correlations between Qwen2-7B and Qwen2-72B across evaluation sub-items (0.926 for highest scores, 0.939 for scores relative to human benchmarks), reflecting the scaling law of LLM and suggesting potential for further improvements with model advancement (see SI for details).Our research validates the effectiveness of LLM-based automated review generation, with quality approaching or exceeding manual reviews.The effectiveness of method stems from general language processing and context learning capabilities of LLMs, rather than requiring specific domain expertise, suggests broad disciplinary applicability.While inheriting common biases and requiring readers' professional judgment, this method serves as a supportive rather than a replacement tool for human innovation, with open-source models showing comparable capabilities to closed-source alternatives.The approach demonstrates broad cross-disciplinary potential, promising to become an important tool for promoting academic innovation and knowledge dissemination, while the dual-baseline framework offers potential methodology for evaluating LLM agent workflows where manual data acquisition is costly.</p>
<p>Data mining and visual analysis</p>
<p>Catalysts play a vital role in chemical process optimization [38], with data mining enabling accelerated design through pattern recognition [39].Analyzing 839 PDH catalyst papers from a total of 1041 articles filtered by abstracts and titles in Q1-Q3 chemistry journals (1980-2024), our data mining module revealed comprehensive insights into catalysts' composition, structure, and performance.</p>
<p>For instance, publication analysis showed surging alloy research since 1995 and single-atom catalyst studies post-2015 (Fig. 3a), driven by advancements in structural composition (Fig. 3b).Performance analysis identified optimal promoter elements (Zn, Sn, La) (Fig. 3c) and support materials (alumina, zeolites) (Fig. 3d), while combination studies revealed superior performance in multi-metal systems, particularly Pt-based catalysts with Sn, Zn, In promoters (Fig. 3e).Moreover, impregnationprepared nanometallic catalysts demonstrated high conversion rates and selectivity, contrasting with single-atom alloys' high selectivity but lower conversion rates (Fig. 3f).</p>
<p>This comprehensive analysis reveals variable interactions and guides catalyst optimization, recommending Pt-based systems for selectivity and metal oxides for conversion rates, while highlighting the promise of single-atom and nanostructured catalysts.These findings not only establish performance benchmarks in PDH catalysis, but also demonstrate how our LLM-based methodology enables real-time scientific insight ex-traction, facilitating industrial-oriented catalyst design optimization.Complete data charts are available in the SI.</p>
<p>Hallucination mitigation</p>
<p>Unlike search engines, LLMs' process of understanding information and outputting it anew provides LLMs' creativity while inevitably accompanying the "hallucination" phenomenon, referring to false information generated by LLMs without sufficient evidence support or contextually inconsistent, off-input information responses [40].LLM hallucinations mainly originate from statistical biases in the training phase, noise in training data, and decision strategies when handling uncertain or multi-interpretable information during the alignment phase [40].Currently, there is no solution within the field [9,40,41], and research suggests that hallucinations are unavoidable [42,43].Especially in specialized sub-fields, LLMs greatly exacerbate the hallucination phenomenon due to extremely scarce data exposure.For example, tests in literature [44] show that even the most advanced GPT-4 only has a 73.3% accuracy rate when answering professional multiple-choice questions, which is far from sufficient for scientific research fields that strictly require correctness.In scientific research, this may lead to unrealistic academic conclusions and misleading research directions, often meaning great waste of time and material consumption [41].Therefore, effective hallucination mitigation is crucial for ensuring the scientific nature and reliability of automated review generation.</p>
<p>To address the challenge of hallucinations in LLMs, a high priority has been placed on the detection and prevention of such phenomena.In the entire automated review generation process, we adopted a multi-level filtering and verification quality control strategy, similar to the concept of retrieval-augmented generation (RAG) [12,45], to mitigate and correct hallucinations:</p>
<p>Prompt design and task decomposition.Firstly, we utilized strict and clear text summary guiding prompts, aimed at enhancing the scientific rationality of LLM's outputs and ensuring accuracy and reliability in its analysis and generation processes.Notably, the task of automated review generation aligns well with the strengths of LLMs-information extraction and text generation capabilities.LLMs can rapidly and accurately extract core information from a vast array of literature and integrate it into a coherent and rigorous review text.To enhance efficiency and quality, we deconstructed the core of the review writing process, namely literature reading and summarization, into a series of text summarization tasks.This approach is adopted because summaries generated by LLM significantly surpass manually crafted and fine-tuned model-generated summaries in terms of fluency, factual consistency, and flexibility [46].By establishing a list of questions, we directed the model to extract relevant content from the literature and respond based on this content, subsequently conducting a comprehensive analysis of all literature citations and responses.Ultimately, the LLM generates high-quality paragraphs closely related to the topic.Additionally, we employed a single-round, segmented generation strategy to avoid truncation limitations of approximately 8K output length.By reasonably segmenting long texts for generation, we not only ensured that the output was completed in a single conversational round but also provided finer parallel granularity to improve generation efficiency.In practice, we divided the 35 questions into 5 groups, ensuring that the generation results for each group could be successfully completed within the 8K limit of the LLM.This granularity avoids efficiency drops due to a high proportion of shared content and identical prompt frameworks, thereby enhancing processing speed while ensuring the quality of text generation.</p>
<p>Hallucination filtering and verification.To mitigate and rectify hallucinations, we employed a layered filtering and verification approach:</p>
<p>1.Text format filtering: Noting that hallucinations often disrupt text formatting, we applied a predefined XML format template to filter out disarrayed texts.</p>
<ol>
<li>
<p>DOI verification: DOIs, a combination of symbols and numbers lacking direct semantic linkage to context, present a challenge in generation and are prone to hallucinations.Yet, the precise reference nature of DOIs allows for verification.Through strict DOI verifications on generated content, we suppressed hallucinatory content from advancing further, ensuring each generated conclusion is traceable to its original source.</p>
</li>
<li>
<p>Relevance verification: Within the RAG system, documents related in semantics but lacking correct answers are particularly detrimental [47].We scrutinized each response in the knowledge extraction phase to ensure its relevance, eliminating off-topic answers with relevant keywords.</p>
</li>
<li>
<p>Self-consistency [48] verification: For text summarization, where a definitive correct answer exists, recognizing that the stochasticity of hallucinations means correct answers should recur more frequently across iterations, we employ aggregation from repeated queries to effectively suppress hallucinations.</p>
</li>
<li>
<p>Full data stream traceability mechanism: By using DOIs as key reference identifiers for each piece of generated content and mandating citations for every conclusion, we enable review readers to easily trace back to the original literature, supporting verification and deeper exploration in topics of interest.</p>
</li>
</ol>
<p>Effectiveness of hallucination mitigation.In evaluating the effectiveness of hallucination mitigation, we employed a confusion matrix to classify outcomes according to whether the LLM provided content and its pertinence to the original text, differentiating between two types of inaccuracies: false positives, which include fabricated or inconsistent information, and false negatives, referring to overlooked or partially extracted content.Our focus was primarily on reducing false positives, while adopting a relatively tolerant stance on false negatives.</p>
<p>Substantial progress was made in mitigating hallucinations.Specifically, in the paragraph generation part, which was achieved through 9 repetitions of 35-paragraph generation tasks, a total of 315 paragraphs that passed format checking and DOI checking were needed.Throughout the entire paragraph generation process, statistics show that LLM cumulatively performed 875 generations, of which only 36% of generation results passed after format and DOI list checks.In the analysis involving 343 topic-related literature, we divided 35 questions into 7 questions per segment for each literature, i.e., 5 segments per literature, totaling 1715 knowledge extractions.By conducting 5 repeated questions in each knowledge extraction, we obtained a total of 8575 answers, and finally aggregated 2783 effective information combinations after excluding answers unrelated to the literature and questions.Among these, up to 84.80% of the results were judged by LLM to have a 100% consistency rate when compared with the aggregated results (Table 1 and Fig. 4a), thus verifying the model's stability.For specific methods, see the Methods section.This method also provides a rough standard for judging the proportion of hallucinations, which can be used in the screening and evaluation of LLMs.</p>
<p>To assess the effectiveness of the knowledge extraction and data mining stages, we implemented a rigorous manual verification process.Specifically, 25 randomly selected articles from each stage were evaluated by a third-year PhD student specializing in PDH research.For the knowledge extraction stage, 35 segments per article were examined, totaling 875 data points.The data mining stage assessed 14 catalyst properties, including 5 direct answer repetitions and final generated results, encompassing 1750 and 350 data points respectively.We employed precise classification criteria for the evaluation.</p>
<p>In the knowledge extraction phase, true negatives (TN) were instances where the article did not address the guiding question and the LLM correctly identified it as irrelevant.True positives (TP) occurred when relevant topics were accurately extracted.False negatives (FN) were cases where relevant topics were incompletely extracted or incorrectly deemed irrelevant.False positives (FP) included irrelevant topics mistakenly identified as relevant or extractions that exceeded or deviated from the article's actual content.Similar criteria were applied to the data mining stage, with particular attention to unit conversion errors, which were classified as false positives even if numerical values were correct.Consistency comparisons were conducted using the Claude2 model through designed prompt templates, comparing pre-and post-aggregation texts and statistically analyzing the model's scoring results.Based on these evaluations, we calculated key metrics including accuracy, false positive rate (with 95% confidence intervals), precision, recall, F1 score, and consistency rate (Table 1).Confidence intervals for false positive rates were computed using Python3's statsmodels library.For detailed results and calculation methods, see SI.</p>
<p>It is crucial to emphasize that this manual verification step was conducted to demonstrate the method's effectiveness during the proof-of-concept phase and is not required in the actual automated review generation process.The detailed results are presented in Table 1.The data comparison underscores the efficacy of self-consistency verifications, revealing a substantial decrease in hallucinations, i.e., false positive content, while also compensating for some false negatives, where information was not fully extracted (Fig. 4b).In the knowledge extraction phase, critical for review content, our manual sampling found no fabricated conclusions by LLMs (Fig. 4a), attesting to our method's scientific integrity and reliability.From the sampling results, we are over 95% confident that the likelihood of hallucinations in this part is less than 0.5% (Table 1), which is also the source of our confidence that this method supports fully automated processes without manual intervention.Analysis of false positives in the post-aggregation data mining phase revealed hallucinations typically involved correct numerical extraction but with errors in units or definitions.False negatives mainly stemmed from LLMs' inability to comprehend highly abstract expressions, reflecting a general LLM's limited understanding of highly specialized scientific concepts.The incidence of hallucinations in knowledge extraction was significantly lower than in data mining, as answering questions did not involve converting units and concepts, thus avoiding the most challenging part of testing an LLM's grasp of scientific knowledge.Domain-specific models enhanced by domain-adaptive pretraining (DAPT) [49] are poised to mitigate this issue.Opting not to fine-tune LLMs for specific domains in this study prioritizes out-of-thebox functionality and multi-domain generalization, utilizing a general LLM as the base.Comparisons between RAG and fine-tuning effects in specific domains indicate that RAG sustains efficacy with contextually new knowledge and offers a significantly lower initial cost [50], aligning with our objective to support researchers' entry into diverse fields efficiently.</p>
<p>Considering the stringent accuracy requirements in research, increasing the number of repetitions can significantly reduce the probability of hallucinations appearing in aggregated results.Binomial probability calculations indicate that theoretically, a model with 79.09% accuracy yields aggregated prediction accuracies of 93.49%, 96.12%, and 97.64% after five, seven, and nine independent predictions, respectively, aligning with our sampling results (Table 1).Detailed sampling outcomes and calculations are available in the SI.We believe that 5 repetitions is an ideal empirical value, and users do not need to change this parameter when using it.On this foundation, every conclusive description in the generated reviews is supported by literature references and has been verified by relevant field researchers through tracing the cited literature, confirming that all literature references are correctly linked to the original publications and that the descriptions in the generated reviews correspond to those in the original publications.</p>
<p>This multi-layered strategy for hallucination control has built an effective verification system, ensuring the scientific integrity and reliability of the automated review generation.Furthermore, through a full data stream traceability mechanism, the authenticity and practicality of the content are further strengthened.This not only provides a secondary means of hallucination mitigation but also allows researchers to delve into original research papers for more precise and detailed academic information while accessing fast, automated research reviews.The strategy also implements a kind of literature recommendation mechanism.Since each content segment includes related DOIs, researchers can quickly locate specific original literature based on their interests and research needs, enabling deeper academic exploration.</p>
<p>While both our method and RAG utilize LLM's context learning ability, our approach fundamentally differs by achieving systematic knowledge reconstruction through multi-stage processing rather than simple retrieval combination.This method simulates the complete academic research process and produces coherent knowledge frame-works aligned with scholarly thinking, surpassing traditional RAG's question-answering paradigm through comprehensive quality control and hallucination mitigation mechanisms.</p>
<p>CONCLUSIONS</p>
<p>In this study, we introduce an innovative LLM-based automated review generation method, addressing two fundamental challenges in scientific research: improving literature review efficiency and mitigating LLM hallucination risks.Through proposing an evaluation framework that ensures the objectivity and reliability via statistical validation, and innovatively compared LLM-generated reviews with high-quality manual reviews, we demonstrate that our modular end-to-end approach produces reviews comparable to or exceeding human-written ones, while maintaining high reliability and traceability.Expert evaluation using PDH catalysts as a case study confirms the method's effectiveness: generated reviews are comparable to manual reviews in length and citations, show no hallucinations, and have impeccable reference accuracy.Statistical validation confirms the method's effectiveness in hallucination reduction, with testing on 875 LLM outputs from 25 random articles showing hallucination probability below 0.5% at 95% confidence.The quality assurance pipeline ensures robust data processing.Additionally, our advanced data mining module offers experienced users' in-depth field integrated perception, fully exploiting LLMs' analytical capabilities.Furthermore, an open-source user-friendly one-click program developed for Windows platforms significantly simplifies the review generation process.</p>
<p>The method's architecture offers significant advantages through its crossdisciplinary applicability without manual intervention or domain-specific knowledge injection.Its modular design enables component reuse for literature tracking, topic discovery, and dataset construction, while achieving cross-disciplinary generalization through LLM's inherent contextual adaptability.This means that by providing corresponding domain literature input, the method can generate high-quality reviews across various disciplines.Future development will focus on enhanced multimodal processing capabilities, automated scientific question generation and answering, personalized text generation, integration with existing academic tools, and domain-specific features for structured data analysis.</p>
<p>This advancement heralds a new era in human-machine academic collaboration, offering broad prospects for LLMs as writing assistance tools.While not intended to replace traditional manual reviews, our method serves as a powerful auxiliary tool for rapid domain overview and research hotspot identification, laying the foundation for in-depth analysis.Beyond its demonstrated excellence in chemistry, the method's technical framework exhibits remarkable cross-disciplinary applicability, potentially breaking down barriers between fields and catalyzing interdisciplinary innovation.By revolutionizing researcher-literature interaction and accelerating knowledge dissemination, this milestone advancement holds profound implications for knowledge base construction, literature recommendation, and structured academic writing, heralding a new era of scientific research productivity and interdisciplinary collaboration.</p>
<p>METHODS</p>
<p>Our method consists of four core components: literature search, topic formulation, knowledge extraction and review composition, along with a data mining module (Fig. 5a) and quality assessment framework (Fig. 5b).All prompt templates are available in SI and GitHub without requiring user adjustment.</p>
<p>Literature search</p>
<p>Literature retrieval begins with journal selection from journal classification tables, followed by an API-based keyword search and preliminary title/abstract filtering using a keyword list, with review-type literature marked separately (Fig. 5a, i).</p>
<p>Topic formulation</p>
<p>Review topics can be constructed either through direct LLM outline drafting or through LLM extraction and refinement of existing literature reviews (Fig. 5a, ii).After obtaining a list of topics, additional topics can be manually added and sorted as needed.This manual addition is not mandatory but provides an interface for advanced users to intervene if necessary.</p>
<p>Knowledge extraction</p>
<p>Based on the topic list, LLM generates extraction questions and conducts multiple rounds of information retrieval from each article.The LLM evaluates answer relevance to questions using structured prompts, where combinations of questions, LLM-aggregated relevant answers, and their corresponding citations constitute valid information combinations for subsequent processing.For literature exceeding the context window of LLM, the text is segmented into approximately equal parts, processed separately, and results are integrated during answer aggregation (Fig. 5a, iii).</p>
<p>Review composition</p>
<p>Extracted answers are associated with source DOIs and integrated into topicspecific paragraphs.Through multiple iterations and LLM scoring, optimal versions are selected to form the preliminary draft, followed by citation verification and format standardization.For answers exceeding context window length, LLM performs compression based on referenced texts and extracted answers until fitting within window limits (Fig. 5a, iv).</p>
<p>Data mining</p>
<p>The data mining module extends knowledge extraction (Fig. 5a, iii) capabilities for specific data extraction and aggregation, enables extraction of user-defined targets (e.g., catalyst types, compositions, performance metrics) from literature.The LLM performs multiple rounds of parsing and extraction in XML format, followed by result aggregation.The extracted data undergoes standardization and cleaning, with GPT4-generated code facilitating statistical analysis and visualization, requiring no programming expertise from users.</p>
<p>Quality Assessment</p>
<p>The evaluation framework employs dual baselines using manual Q1 journal reviews and direct LLM generation for quality assessment (Fig. 5b).High-quality reviews are semantically segmented, with corresponding content regenerated using our method and compared against direct LLM-generated content.Assessment utilizes chain-ofthought prompts across 27 scoring items in 9 categories, implementing cross-evaluation and repetition strategies to mitigate bias.The page rerank algorithm converts relative comparisons to absolute scores on a 0-10 scale, with framework reliability validated through intraclass correlation coefficient (ICC) tests and transitive consistency ratio (TCR) analyses.our data processing work, except for the evaluation section, was completed prior to November 21, 2023, utilizing the then-latest available Claude 2 API version [https://www.anthropic.com/news/claude-2,https://www.anthropic.com/news/claude-2-1].Anthropic did not publish specific minor version numbers within the Claude 2 series, only distinguishing between Claude 2, Claude 2.1, and the subsequent Claude 3 series.Our proposed framework demonstrates good adaptability, with overall effectiveness increasing as the performance of the underlying model improves.This characteristic has been amply demonstrated in our evaluation work, indicating that the framework's efficacy is not strictly dependent on any particular model version.The use of different LLMs in the Evaluation of generated review quality section was primarily to assess the performance of the latest and most powerful open-source and closed-source models (as of September 2024) under the method described in this paper.</p>
<p>Our published graphical user interface (GUI) leverages certain APIs for functionality, which, due to legal and regulatory requirements, necessitate that users provide their own API keys.This requirement is detailed in the documentation accompanying the code repository to assist users in setting up and utilizing the GUI effectively.</p>
<p>Figure 1 :
1
Figure 1: Reliability verification results of the dual-baseline review quality assess-</p>
<p>Figure 2 :
2
Figure 2: Quality assessment results of automatically generated reviews.Heat map of the percentage difference in scores of review paragraphs generated by this method relative to human scores, red to green showing -100% to +100% range, higher values indicate better performance, values truncated to ±100% range, values exceeding are recorded as -100% and +100%: a, Highest scoring paragraph of Claude3.5 Sonnet model; b, Highest scoring paragraph of Qwen2-72b-Instruct model; c, Average paragraph score of Claude3.5 Sonnet model; d, Average paragraph score of Qwen2-72b-Instruct model.e, Histogram of percentage differences in scores relative to human scores for highest scoring paragraphs, average paragraph scores, and directly generated paragraph scores without going through this method for Claude3.5 Sonnet model and Qwen2-72b-Instruct model, colors ranging from dark to light representing Claude3.5</p>
<p>Figure 3 :
3
Figure 3: Example of visual analysis results.Line charts for annual publication numbers: a, different catalyst types; b, Performance enhancement sources.Radar charts for peak performance of single factors, with selectivity (black) and stability (purple) scales: c, Promoter elements; d, Support materials.Bubble charts for dual-variable correlations, show selectivity (color depth), conversion rate (bubble size), and stability (bubble edge thickness), aiming for high selectivity, conversion rate, and stability.Data includes only those with selectivity ≥85%, conversion rate ≥45%, stability ≥1h: e, Active site element-composition element; f, Alloy structure type-preparation method.Complete data charts are available in the SI.</p>
<p>4 :
4
Effectiveness of hallucination mitigation.a, Consistency as determined by LLMs between direct LLM responses and aggregated results during the knowledge extraction phase, where blue represents 100% consistency and orange less than 100%.b, Distribution of manual sampling results for direct LLM responses and aggregated outcomes during the data mining phase, with TP (True Positive), TN (True Negative), FP (False Positive), FN (False Negative)</p>
<p>a b 5 :
5
a, Flowchart of the automated review generation method based on large language models.It includes four modules: i) literature search, ii) topic formulation, iii) knowledge extraction, iv) review composition, as well as an additional data mining module.b, Flowchart of the quality assessment framework for review generation based on large language models.</p>
<p>Table 1 :
1
Comparison of results before and after self-consistency aggregation
StageData PointsAccu-racyFalse Positive95%CI of FPRPreci-sionRecallF1 ScoreConsist -encyRateKnowledge Extraction (Aggregated)87595.77% 0.000%0.000% -0.485%100.0% 57.47% 72.99% 84.80%Data Mining (Direct Response)175079.09% 35.34%31.45% -39.42%84.14% 85.68% 84.90%86.60%12.20%Data Mining35093.71% 18.75%-93.28% 98.43% 95.79%(Aggregated)27.70%</p>
<p>Online database Abstracts Correlation test highly correlated papers Local database LLM Direct Review Topics Creation LLM Literature-based Review Topics Creation LLM for Literature Answer Extraction LLM for Aggregating Literature Answers LLM for Paragraph Construction LLM for Full-text Refinement LLM for Paragraph Scoring
Key-wordsAPI1. Automated Literature Search2. Automated Topic Formulation4. Automated Review Composition3. Automated Knowledge ExtractionSegment peer-Extract uniquereviewed, published expert reviewstopics for each segment using LLMLLM-basedValidate reliability via ICC and transitivity consistency testsComparative Assessment Framework for Review GenerationRegenerate reviews based on original cited literature by LLMQualityPairwise compareConvert scores to 0-10 scale using Page Rerankgenerated reviews, original reviews, anddirect LLM output
CONFLICT OF INTEREST STATEMENTThe authors declare no competing interests.ACKNOWLEDGMENTWe acknowledge the Natural Science Foundation of China (No. 22121004), the Haihe Laboratory of Sustainable Chemical Transformations, the Program of Introducing Talents of Discipline to Universities (BP0618007) and the XPLORER PRIZE for financial support.We also acknowledge generous computing resources at High Performance Computing Center of Tianjin University.AUTHOR CONTRIBUTIONSJ.G and Z.Z.conceived and supervised the project.S.W. and X.M. designed the research and developed the program.S.W. led the manuscript preparation.D.L. performed manual verification of hallucination.L.L., X.S., and X.C. advanced the integration of computational models, while X.L., R.L., C.P. and C.D. advanced the catalytic science.All authors contributed to writing and revising the manuscript.DATA AVAILABILITYOur study leverages a dataset compiled from scientific literature acquired through our institution's subscription.Due to copyright considerations, the dataset itself cannot be made publicly available.However, we ensure that our research's integrity and reproducibility do not rely on direct access to these proprietary documents.Instead, we provide extensive documentation on the dataset's structure, the criteria used for literature selection, and the analysis methods applied, enabling interested researchers to reconstruct a similar dataset from publicly available resources or their institutional subscriptions.Furthermore, to facilitate a deeper understanding of our research process and promote further exploration and innovation, we have made all intermediate data, excluding the copyrighted full-text articles, publicly available on GitHub [https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData].This repository includes the prompts used in our study and the corresponding responses generated by the large language model.By sharing these resources, we aim to provide valuable insights into our methodology and encourage other researchers to build upon our work, advancing the field of natural language processing and its applications in scientific literature analysis.CODE AVAILABILITYThe custom code developed for this research is central to our conclusions and is made available to ensure transparency and reproducibility of our results.The codebase, including all relevant custom scripts and mathematical algorithms, has been opensourced under the Apache 2.0 license and is accessible via our GitHub repository at [https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration].We encourage users to review the license for any usage restrictions that may apply.As stated in the text, all LLMs invoked in this article are Claude 2, except for the Evaluation of generated review quality section which uses Claude 3.5 Sonnet, Qwen2-72b-Instruct, and Qwen2-7b-Instruct.It is important to note thatADDITIONAL INFORMATIONSupplementary information is available for this paper.Correspondence and requests for materials should be addressed to J.G.
Literature reviews: modern methods for investigating scientific and technological knowledge. Apc Ermel, D P Lacerda, Miw Morandi, L Gauss, 2021Springer Nature</p>
<p>Free online availability substantially increases a paper's impact. Nature. S Lawrence, May 31 2001411521</p>
<p>Speed reading: scientists are struggling to make sense of the expanding scientific literature. Corie Lok asks whether computational tools can do the hard work for them. C Lok, Nature. 46372802010</p>
<p>Recent Advances in Lead Chemisorption for Perovskite Solar Cells. P F Wu, F Zhang, Oct 202228Transactions of Tianjin University</p>
<p>Natural language processing: state of the art, current trends and challenges. D Khurana, A Koli, K Khatter, S Singh, Multimed Tools Appl. 8232023</p>
<p>Summary of chatgpt-related research and perspective towards the future of large language models. Y Liu, T Han, S Ma, Meta-Radiology. 1000172023</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, arXiv:2009033002020arXiv preprint</p>
<p>D Rein, B L Hou, A C Stickland, arXiv:231112022A graduate-level google-proof q&amp;a benchmark. 2023arXiv preprint</p>
<p>The future of chemistry is language. A D White, Nature Reviews Chemistry. 77Jul 2023</p>
<p>Paperqa: Retrieval-augmented generative agent for scientific research. J Lála, O 'donoghue, O Shtedritski, A Cox, S Rodriques, S G White, A D , arXiv:2312075592023arXiv preprint</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. M D Skarlinski, S Cox, J M Laurent, arXiv:2409137402024arXiv preprint</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. P Lewis, E Perez, A Piktus, Advances in Neural Information Processing Systems. 332020</p>
<p>S Wei, X Xu, X Qi, arXiv:231112315Empowering Academic Research. 2023arXiv preprint</p>
<p>Z Yang, Z Zhu, Curiousllm, arXiv:240409077Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting. 2024arXiv preprint</p>
<p>Multi-document summarization via deep learning techniques: A survey. C Ma, W E Zhang, M Guo, H Wang, Q Z Sheng, Acm Comput Surv. 5552022</p>
<p>Automatic generation of reviews of scientific papers. A Nikiforovskaya, N Kapralov, A Vlasova, O Shpynov, A Shpilman, IEEE2020</p>
<p>Using citations to generate surveys of scientific paradigms. S Mohammad, B Dorr, M Egan, 2009</p>
<p>Towards multi-document summarization of scientific articles: making interesting comparisons with SciSumm. N Agarwal, R S Reddy, G Kiran, C Rose, 2011</p>
<p>Deconstructing human literature reviews-a framework for multi-document summarization. K Jaidka, C Khoo, J-C Na, 2013</p>
<p>SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation. T Kasanishi, M Isonuma, J Mori, I Sakata, arXiv:2305151862023arXiv preprint</p>
<p>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation. A M Sami, Z Rasheed, K-K Kemell, arXiv:2403083992024arXiv preprint</p>
<p>S Agarwal, I H Laradji, L Charlin, C Pal, Litllm, arXiv:240201788A Toolkit for Scientific Literature Review. 2024arXiv preprint</p>
<p>C Y Haryanto, Llassist, arXiv:240713993Simple Tools for Automating Literature Review Using Large Language Models. 2024arXiv preprint</p>
<p>Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews. L Joos, D A Keim, M T Fischer, arXiv:2407106522024arXiv preprint</p>
<p>Y Li, L Chen, A Liu, K Yu, Wen L Chatcite, arXiv:240302574LLM Agent with Human Workflow Guidance for Comparative Literature Summary. 2024arXiv preprint</p>
<p>ChatGPT outperforms crowd workers for textannotation tasks. F Gilardi, M Alizadeh, M Kubli, P Natl Acad Sci. 12030e2305016120Jul 25 2023</p>
<p>Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning. P Törnberg, arXiv:2304065882023arXiv preprint</p>
<p>Large Language Models as Evaluators for Recommendation Explanations. X Zhang, Y Li, J Wang, 2024</p>
<p>Language models (mostly) know what they know. S Kadavath, T Conerly, A Askell, arXiv:2207052212022arXiv preprint</p>
<p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. L M Zheng, W L Chiang, Y Sheng, Adv Neur In. 362023</p>
<p>Benchmarking foundation models with language-modelas-an-examiner. Y Bai, J Ying, Y Cao, Advances in Neural Information Processing Systems. 362024</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Z Gou, Z Shao, Y Gong, arXiv:2305117382023arXiv preprint</p>
<p>Llm-based nlg evaluation: Current status and challenges. M Gao, X Hu, J Ruan, X Pu, Wan X , arXiv:2402013832024arXiv preprint</p>
<p>Large language models are not fair evaluators. P Wang, L Li, L Chen, arXiv:2305179262023arXiv preprint</p>
<p>Y Liu, D Iter, Y Xu, S Wang, R Xu, C Zhu, G-Eval, arXiv:230316634Nlg evaluation using gpt-4 with better human alignment. 2023arXiv preprint</p>
<p>Split and merge: Aligning position biases in large language model based evaluators. Z Li, C Wang, P Ma, arXiv:2310014322023arXiv preprint</p>
<p>Large language models are not yet human-level evaluators for abstractive summarization. C Shen, L Cheng, X-P Nguyen, Y You, L Bing, arXiv:2305130912023arXiv preprint</p>
<p>Progress in Processes and Catalysts for Dehydrogenation of Cyclohexanol to Cyclohexanone. J Gong, S X Hou, Y Wang, X B Ma, Jun 202329Transactions of Tianjin University</p>
<p>Data-Driven Design of Single-Atom Electrocatalysts with Intrinsic Descriptors for Carbon Dioxide Reduction Reaction. X Y Lin, S Y Zhen, X H Wang, Oct 202430Transactions of Tianjin University</p>
<p>Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. Y Zhang, Y Li, L Cui, arXiv:2309012192023arXiv preprint</p>
<p>GPT-4 is here: what scientists think. K Sanderson, Nature. 6157954Mar 30 2023</p>
<p>Hallucination is inevitable: An innate limitation of large language models. Z Xu, S Jain, M Kankanhalli, arXiv:2401118172024arXiv preprint</p>
<p>Calibrated language models must hallucinate. A T Kalai, S S Vempala, 2024</p>
<p>A comparative study of open-source large language models, gpt-4 and claude 2: Multiple-choice test taking in nephrology. S Wu, M Koo, L Blum, arXiv:2308047092023arXiv preprint</p>
<p>Large language models should be used as scientific reasoning engines, not knowledge databases. D Truhn, J S Reis-Filho, J N Kather, Nature Medicine. 2023</p>
<p>Summarization is (almost) dead. X Pu, M Gao, Wan X , arXiv:2309095582023arXiv preprint</p>
<p>The power of noise: Redefining retrieval for rag systems. F Cuconasu, G Trappolini, F Siciliano, 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, arXiv:2203111712022arXiv preprint</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. S Gururangan, A Marasović, S Swayamdipta, arXiv:2004109642020arXiv preprint</p>
<p>RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. A Gupta, A Shirgaonkar, Balaguer Adl, arXiv:2401084062024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
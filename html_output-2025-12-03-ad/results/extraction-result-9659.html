<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9659 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9659</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9659</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-276902996</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.16701v2.pdf" target="_blank">ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models</a></p>
                <p><strong>Paper Abstract:</strong> The use of Large Language Models (LLMs) in climate science has recently gained significant attention. However, a critical issue remains: the lack of a comprehensive evaluation framework capable of assessing the quality and scientific validity of model outputs. To address this issue, we develop ClimaGen (Climate QA Generator), an adaptive learning framework that generates question-answer pairs from graduate textbooks with climate scientists in the loop. As a result, we present ClimaQA-Gold, an expert-annotated benchmark dataset alongside ClimaQA-Silver, a large-scale, comprehensive synthetic QA dataset for climate science. Finally, we develop evaluation strategies and compare different LLMs on our benchmarks. Our results offer novel insights into various approaches used to enhance knowledge of climate LLMs. The source code is publicly available at https://github.com/Rose-STL-Lab/genie-climaqa</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9659.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9659.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ClimaQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ClimaQA Benchmark (ClimaQA-Gold & ClimaQA-Silver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific climate science question-answering benchmark containing expert-validated (ClimaQA-Gold, 566 questions) and large-scale synthetic (ClimaQA-Silver, 3000 questions) datasets across three QA forms (MCQ, Freeform, Cloze) and three complexity levels (base, reasoning, hypothetical).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various (gpt-4o, gpt-3.5-turbo, llama3-70b, gemma-27b, mixtral-8x22b, mistral-7b, llama3.1-8b, gemini-1.5)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>A mixture of instruction-tuned proprietary models (gpt-4o, gpt-3.5-turbo, Gemini 1.5) and open-source models (LLaMA3 variants, Gemma, Mixtral, Mistral) evaluated under default, few-shot, RAG, continued pre-training, and fine-tuning settings.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science (physical climate, aerosols, clouds, atmospheric processes)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Quantitative evaluation of model outputs on ClimaQA-Gold and ClimaQA-Silver using automatic metrics (exact-match for MCQ, BLEU / BERTScore / factual-entailment for freeform, exact-match + phrase-similarity for cloze) and experimental comparisons across prompting and adaptation strategies (few-shot, RAG, continued pre-training, silver fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>MCQ accuracy (exact choice), Freeform: BLEU, BERTScore, Factual Accuracy (entailment confidence); Cloze: Exact Match and Phrase Similarity (cosine of USE embeddings rescaled).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ClimaQA-Gold: 566 expert-validated questions (245 freeform, 161 MCQ, 160 cloze). ClimaQA-Silver: 3000 synthetically generated questions (1000 per task form) produced by ClimaGen and automated annotator.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RAG with source-textbooks (rag-source) consistently produced the best performance across tasks; GPT-4o dominated overall. Continued pre-training and task-specific fine-tuning improved scores, while few-shot prompting gave marginal gains. Models struggled most on reasoning MCQs; freeform reasoning sometimes yielded better outputs than constrained MCQs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Gold dataset small and derived from only five source textbooks (limited context diversity); synthetic generation may include hallucinations; evaluation metrics have biases (BLEU/BERTScore biased toward generator model); dataset reconstructability limited by copyright.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>ClimaQA-Gold uses domain-expert validation as ground truth; human expert review was used to filter and annotate generated QAs and to assess cloze non-exact but contextually correct answers — automated metrics were compared to expert judgments in case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use multi-form benchmarks (MCQ, Freeform, Cloze) with multi-level complexity; combine retrieval from source materials (RAG-source) with fine-tuning on large synthetic sets guided by expert-validated seeds; prefer factual-entailment checks for factual accuracy beyond lexical overlap metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9659.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9659.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ClimaGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ClimaGen (Automated Benchmark Creation Framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive pipeline that generates QA pairs from graduate-level textbook contexts using LLMs (generator + retriever) and refines them via LLM-based audits and domain-expert annotation, then trains evaluator models to automate validation and scale dataset creation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-3.5-turbo (generator); GPT-4o-mini (evaluator fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Generator: GPT-3.5-turbo used to create base and complexity-evolved QA pairs from retrieved textbook chunks; Evaluator: GPT-4o-mini fine-tuned as a classifier on expert labels to validate QA pairs automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science benchmark/data generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>QA generation followed by a two-stage validation: preliminary LLM/self-inconsistency audits and human expert annotation; then supervised fine-tuning of an evaluator model to classify QA validity and mask cloze terms for automated annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary valid/invalid labels for QA pairs; confidence-based sampling (evaluator confidence threshold 0.85 and uncertainty-based active sampling); expert reasons for rejection recorded for pipeline improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used to produce ClimaQA-Silver (3000 QAs) and to curate ClimaQA-Gold via human-in-the-loop; textbook contexts stored in vector DB for retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Evaluator models improved generated MCQ quality by ~10% and freeform QA quality by ~5%; final automated pipeline produced 1000 QAs per task form in the silver set with reported ~85% MCQ validity and ~90% freeform validity prior to manual curation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Generator LLMs (GPT-3.5-turbo) produced ~25% incorrect MCQs even with context (self-inconsistency); evaluator training needs domain-expert labels (small labeled set limits generalization); mistakes due to bad seed contexts remain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human experts remain required initially to produce high-quality labels and to curate Gold set; ClimaGen reduces but does not eliminate need for expert oversight, differing from fully manual dataset creation by scaling and automating validation using learned classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Combine LLM generation with human-in-the-loop feedback to build seed expert labels, then fine-tune evaluator classifiers with uncertainty-based active sampling; filter generator outputs with self-inconsistency checks before human review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9659.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9659.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FactualAccuracy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factual Accuracy via Entailment Classifier (GPT-4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A factual-accuracy metric that frames evaluation as textual entailment: treating the reference answer as 'evidence' and model output as 'claim', classified as SUPPORTS or REFUTES by an instruction-tuned LLM (GPT-4o-mini), with confidence converted to a continuous factual score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o-mini (entailment classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Instruction-tuned GPT-4o-mini used in zero-shot/ fine-tuned fashion to classify SUPPORTS vs REFUTES; logits converted to a confidence via sigmoid smoothing (temperature T=5).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation of freeform scientific QA in climate science</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompted entailment classification: Evidence: <reference answer> Claim: <generated answer>. Model outputs SUPPORTS/REFUTES; logits used to compute Factual Accuracy = SigmoidSmooth(l_s / (l_s + l_r)) with temperature scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary entailment label converted to continuous confidence score (0-1); reported alongside lexical and embedding-based metrics for freeform QA.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Validated on Climate-Fever dataset (zero-shot classification accuracy 81%) and used to score freeform outputs on ClimaQA-Gold and Silver.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Factual Accuracy better penalizes semantically plausible but factually incorrect answers compared to BLEU/BERTScore (examples show high BLEU/BERTScore but low Factual Accuracy for incorrect claims); authors report it as less biased by generator model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on an LLM classifier whose own biases and hallucinations can affect scores; temperature and calibration choices affect absolute values (authors used T=5 but note trends unaffected); requires robust prompt design and possibly in-domain calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Provides automated, scalable proxy of expert entailment judgments; compared with human expert labels in case studies where it aligns better with factual correctness than lexical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use entailment-based factual scoring in addition to lexical/semantic metrics for freeform scientific answers; calibrate classifier confidence and validate against expert labels on domain-specific datasets (e.g., Climate-Fever).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9659.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9659.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PhraseSim-Cloze</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phrase Similarity Metric for Cloze (USE-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cloze evaluation metric that computes semantic similarity between the reference-filled and generated-filled context phrases using Universal Sentence Encoder embeddings and cosine similarity, then rescales to emphasize discrimination between correct/incorrect answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Universal Sentence Encoder (for embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Sentence embedding model (Universal Sentence Encoder) used to embed short context phrases containing either the reference or the generated cloze fill; cosine similarity is computed then rescaled: PhraseSimilarity = 2 * (cos(e1,e2) - 0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cloze QA evaluation in climate science</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Extract a context window around the mask replaced by reference and by model answer (window size chosen empirically = 4); compute cosine similarity of USE embeddings; rescale to [ -1, 1 ]-style metric and combine with Exact Match.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Exact Match and Phrase Similarity (continuous score); context-window size tuned to best separate correct vs incorrect answers (empirically chosen as 4).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Validated on 219 synthetically generated cloze questions with GPT-4o-mini answers; 32 non-exact matches labeled by domain experts used to tune window size and thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Phrase Similarity with window size 4 most effectively discriminated correct vs incorrect non-exact cloze answers; combined metric captures contextually valid alternatives missed by strict exact-match.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Semantic similarity can conflate distinct but contextually related technical terms; quality depends on phrase selection window and embedding model; cannot fully replace expert judgment for nuanced domain terms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Augments strict exact-match scoring by approximating expert acceptability for alternate valid fills; validated against domain-expert labels for a subset.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Combine exact-match with a rescaled embedding-based phrase similarity to allow semantically valid alternatives; empirically tune context window on expert-labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9659.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9659.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluator-FT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Evaluator (Fine-tuned GPT-4o-mini classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A binary classifier fine-tuned on expert-annotated QA validity labels to automate acceptance/rejection of generated QA pairs and to select single-word scientific terms for cloze masks, using uncertainty-based active sampling during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4o-mini (fine-tuned evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4o-mini fine-tuned separately for MCQ validation, freeform QA validation, and cloze-term selection using small expert-labeled datasets (e.g., 160 examples for cloze term selection).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Automatic QA annotation and validation for climate QA generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Supervised fine-tuning with uncertainty-based active sampling; classifier outputs validity labels with confidence; samples above confidence threshold (0.85) subsampled during training to force learning from representative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Classifier accuracy/precision across train-test splits; used to label QA pairs as valid/invalid and to mark cloze terms (single-word selection).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Trained on ClimaQA-Gold expert labels (e.g., 566 items split across tasks); cloze term model trained on 160 expert examples.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Evaluator models achieved improvements in generated QA quality (MCQ set quality up ~10%, freeform up ~5%); validation rates reported ~85% valid for MCQs and ~90% valid for freeform QAs produced by generator after evaluator filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance limited by small expert-labeled training sets and domain coverage; evaluator itself may propagate biases and errors; requires initial human-labeled seed to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automates steps previously requiring human validators; still relies on human labels for training and quality assurance, and human experts remain necessary for edge cases.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use active sampling and confidence-based subsampling during fine-tuning; maintain a human-in-the-loop monitoring process and expand expert-labeled data to improve evaluator robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9659.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9659.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptation-Comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptation Strategies Comparison (RAG, Continued Pre-training, Fine-tuning, Few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison of adaptation methods for LLMs on scientific QA: Retrieval-Augmented Generation (RAG) from source vs held-out textbooks, continued pre-training on domain textbooks, and supervised fine-tuning on the ClimaQA-Silver dataset, plus few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various evaluated models (see ClimaQA entry)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Multiple LLMs assessed under adaptation methods; some models additionally continued pre-trained or fine-tuned (e.g., LLaMA3.1-8B-Instruct, Mistral-7B-v0.3-Instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science QA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measure task metrics (MCQ accuracy, Cloze EM & PhraseSim, Freeform BLEU/BERTScore/Factual Accuracy) after applying adaptation methods: rag-source, rag-held-out, cp-held-out (continued pre-training), ft-silver (fine-tune on synthetic silver), few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-specific metrics: MCQ accuracy (percent), Cloze exact-match and phrase similarity, Freeform BLEU/BERTScore/Factual Accuracy. Comparison across model variants and retrieval sources.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on ClimaQA-Gold (held-out five textbooks) and ClimaQA-Silver subsets; retrieval sources: source textbooks (used to generate questions) vs held-out textbook pool.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RAG using source textbooks (rag-source) produced the highest gains in most scenarios (e.g., GPT-4o rag-source ~92.79 overall MCQ accuracy in Table 3); continued pre-training improved MCQ/Cloze performance; fine-tuning on silver improved performance but RAG-source often outperformed ft-silver; retrieval from held-out books sometimes reduced performance due to distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval from irrelevant/held-out materials can harm performance; pre-training risks data contamination if source overlap exists; fine-tuning on synthetic silver depends on silver quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>RAG-source approximates providing human-curated context and outperforms unguided models, suggesting retrieval of authoritative sources better mirrors expert-provided context than mere continued pre-training or few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer retrieval from high-quality, source-aligned texts (rag-source) when available; combine continued domain pre-training with targeted fine-tuning on curated synthetic datasets; carefully curate retrieval corpora to avoid distracting/irrelevant content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Climate-Fever: A dataset for verification of real-world climate claims <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>The sciqa scientific question answering benchmark for scholarly knowledge <em>(Rating: 1)</em></li>
                <li>ScienceQA <em>(Rating: 1)</em></li>
                <li>Are machines better at complex reasoning? unveiling human-machine inference gaps in entailment verification <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9659",
    "paper_id": "paper-276902996",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "ClimaQA",
            "name_full": "ClimaQA Benchmark (ClimaQA-Gold & ClimaQA-Silver)",
            "brief_description": "A domain-specific climate science question-answering benchmark containing expert-validated (ClimaQA-Gold, 566 questions) and large-scale synthetic (ClimaQA-Silver, 3000 questions) datasets across three QA forms (MCQ, Freeform, Cloze) and three complexity levels (base, reasoning, hypothetical).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Various (gpt-4o, gpt-3.5-turbo, llama3-70b, gemma-27b, mixtral-8x22b, mistral-7b, llama3.1-8b, gemini-1.5)",
            "llm_description": "A mixture of instruction-tuned proprietary models (gpt-4o, gpt-3.5-turbo, Gemini 1.5) and open-source models (LLaMA3 variants, Gemma, Mixtral, Mistral) evaluated under default, few-shot, RAG, continued pre-training, and fine-tuning settings.",
            "scientific_domain": "Climate science (physical climate, aerosols, clouds, atmospheric processes)",
            "evaluation_method": "Quantitative evaluation of model outputs on ClimaQA-Gold and ClimaQA-Silver using automatic metrics (exact-match for MCQ, BLEU / BERTScore / factual-entailment for freeform, exact-match + phrase-similarity for cloze) and experimental comparisons across prompting and adaptation strategies (few-shot, RAG, continued pre-training, silver fine-tuning).",
            "evaluation_criteria": "MCQ accuracy (exact choice), Freeform: BLEU, BERTScore, Factual Accuracy (entailment confidence); Cloze: Exact Match and Phrase Similarity (cosine of USE embeddings rescaled).",
            "benchmark_or_dataset": "ClimaQA-Gold: 566 expert-validated questions (245 freeform, 161 MCQ, 160 cloze). ClimaQA-Silver: 3000 synthetically generated questions (1000 per task form) produced by ClimaGen and automated annotator.",
            "results_summary": "RAG with source-textbooks (rag-source) consistently produced the best performance across tasks; GPT-4o dominated overall. Continued pre-training and task-specific fine-tuning improved scores, while few-shot prompting gave marginal gains. Models struggled most on reasoning MCQs; freeform reasoning sometimes yielded better outputs than constrained MCQs.",
            "limitations_or_challenges": "Gold dataset small and derived from only five source textbooks (limited context diversity); synthetic generation may include hallucinations; evaluation metrics have biases (BLEU/BERTScore biased toward generator model); dataset reconstructability limited by copyright.",
            "comparison_to_human_or_traditional": "ClimaQA-Gold uses domain-expert validation as ground truth; human expert review was used to filter and annotate generated QAs and to assess cloze non-exact but contextually correct answers — automated metrics were compared to expert judgments in case studies.",
            "recommendations_or_best_practices": "Use multi-form benchmarks (MCQ, Freeform, Cloze) with multi-level complexity; combine retrieval from source materials (RAG-source) with fine-tuning on large synthetic sets guided by expert-validated seeds; prefer factual-entailment checks for factual accuracy beyond lexical overlap metrics.",
            "uuid": "e9659.0",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ClimaGen",
            "name_full": "ClimaGen (Automated Benchmark Creation Framework)",
            "brief_description": "An adaptive pipeline that generates QA pairs from graduate-level textbook contexts using LLMs (generator + retriever) and refines them via LLM-based audits and domain-expert annotation, then trains evaluator models to automate validation and scale dataset creation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-3.5-turbo (generator); GPT-4o-mini (evaluator fine-tuning)",
            "llm_description": "Generator: GPT-3.5-turbo used to create base and complexity-evolved QA pairs from retrieved textbook chunks; Evaluator: GPT-4o-mini fine-tuned as a classifier on expert labels to validate QA pairs automatically.",
            "scientific_domain": "Climate science benchmark/data generation",
            "evaluation_method": "QA generation followed by a two-stage validation: preliminary LLM/self-inconsistency audits and human expert annotation; then supervised fine-tuning of an evaluator model to classify QA validity and mask cloze terms for automated annotation.",
            "evaluation_criteria": "Binary valid/invalid labels for QA pairs; confidence-based sampling (evaluator confidence threshold 0.85 and uncertainty-based active sampling); expert reasons for rejection recorded for pipeline improvement.",
            "benchmark_or_dataset": "Used to produce ClimaQA-Silver (3000 QAs) and to curate ClimaQA-Gold via human-in-the-loop; textbook contexts stored in vector DB for retrieval augmentation.",
            "results_summary": "Evaluator models improved generated MCQ quality by ~10% and freeform QA quality by ~5%; final automated pipeline produced 1000 QAs per task form in the silver set with reported ~85% MCQ validity and ~90% freeform validity prior to manual curation.",
            "limitations_or_challenges": "Generator LLMs (GPT-3.5-turbo) produced ~25% incorrect MCQs even with context (self-inconsistency); evaluator training needs domain-expert labels (small labeled set limits generalization); mistakes due to bad seed contexts remain.",
            "comparison_to_human_or_traditional": "Human experts remain required initially to produce high-quality labels and to curate Gold set; ClimaGen reduces but does not eliminate need for expert oversight, differing from fully manual dataset creation by scaling and automating validation using learned classifiers.",
            "recommendations_or_best_practices": "Combine LLM generation with human-in-the-loop feedback to build seed expert labels, then fine-tune evaluator classifiers with uncertainty-based active sampling; filter generator outputs with self-inconsistency checks before human review.",
            "uuid": "e9659.1",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "FactualAccuracy",
            "name_full": "Factual Accuracy via Entailment Classifier (GPT-4o-mini)",
            "brief_description": "A factual-accuracy metric that frames evaluation as textual entailment: treating the reference answer as 'evidence' and model output as 'claim', classified as SUPPORTS or REFUTES by an instruction-tuned LLM (GPT-4o-mini), with confidence converted to a continuous factual score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o-mini (entailment classifier)",
            "llm_description": "Instruction-tuned GPT-4o-mini used in zero-shot/ fine-tuned fashion to classify SUPPORTS vs REFUTES; logits converted to a confidence via sigmoid smoothing (temperature T=5).",
            "scientific_domain": "Evaluation of freeform scientific QA in climate science",
            "evaluation_method": "Prompted entailment classification: Evidence: &lt;reference answer&gt; Claim: &lt;generated answer&gt;. Model outputs SUPPORTS/REFUTES; logits used to compute Factual Accuracy = SigmoidSmooth(l_s / (l_s + l_r)) with temperature scaling.",
            "evaluation_criteria": "Binary entailment label converted to continuous confidence score (0-1); reported alongside lexical and embedding-based metrics for freeform QA.",
            "benchmark_or_dataset": "Validated on Climate-Fever dataset (zero-shot classification accuracy 81%) and used to score freeform outputs on ClimaQA-Gold and Silver.",
            "results_summary": "Factual Accuracy better penalizes semantically plausible but factually incorrect answers compared to BLEU/BERTScore (examples show high BLEU/BERTScore but low Factual Accuracy for incorrect claims); authors report it as less biased by generator model.",
            "limitations_or_challenges": "Relies on an LLM classifier whose own biases and hallucinations can affect scores; temperature and calibration choices affect absolute values (authors used T=5 but note trends unaffected); requires robust prompt design and possibly in-domain calibration.",
            "comparison_to_human_or_traditional": "Provides automated, scalable proxy of expert entailment judgments; compared with human expert labels in case studies where it aligns better with factual correctness than lexical metrics.",
            "recommendations_or_best_practices": "Use entailment-based factual scoring in addition to lexical/semantic metrics for freeform scientific answers; calibrate classifier confidence and validate against expert labels on domain-specific datasets (e.g., Climate-Fever).",
            "uuid": "e9659.2",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "PhraseSim-Cloze",
            "name_full": "Phrase Similarity Metric for Cloze (USE-based)",
            "brief_description": "A cloze evaluation metric that computes semantic similarity between the reference-filled and generated-filled context phrases using Universal Sentence Encoder embeddings and cosine similarity, then rescales to emphasize discrimination between correct/incorrect answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Universal Sentence Encoder (for embeddings)",
            "llm_description": "Sentence embedding model (Universal Sentence Encoder) used to embed short context phrases containing either the reference or the generated cloze fill; cosine similarity is computed then rescaled: PhraseSimilarity = 2 * (cos(e1,e2) - 0.5).",
            "scientific_domain": "Cloze QA evaluation in climate science",
            "evaluation_method": "Extract a context window around the mask replaced by reference and by model answer (window size chosen empirically = 4); compute cosine similarity of USE embeddings; rescale to [ -1, 1 ]-style metric and combine with Exact Match.",
            "evaluation_criteria": "Exact Match and Phrase Similarity (continuous score); context-window size tuned to best separate correct vs incorrect answers (empirically chosen as 4).",
            "benchmark_or_dataset": "Validated on 219 synthetically generated cloze questions with GPT-4o-mini answers; 32 non-exact matches labeled by domain experts used to tune window size and thresholds.",
            "results_summary": "Phrase Similarity with window size 4 most effectively discriminated correct vs incorrect non-exact cloze answers; combined metric captures contextually valid alternatives missed by strict exact-match.",
            "limitations_or_challenges": "Semantic similarity can conflate distinct but contextually related technical terms; quality depends on phrase selection window and embedding model; cannot fully replace expert judgment for nuanced domain terms.",
            "comparison_to_human_or_traditional": "Augments strict exact-match scoring by approximating expert acceptability for alternate valid fills; validated against domain-expert labels for a subset.",
            "recommendations_or_best_practices": "Combine exact-match with a rescaled embedding-based phrase similarity to allow semantically valid alternatives; empirically tune context window on expert-labeled data.",
            "uuid": "e9659.3",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Evaluator-FT",
            "name_full": "Automated Evaluator (Fine-tuned GPT-4o-mini classifier)",
            "brief_description": "A binary classifier fine-tuned on expert-annotated QA validity labels to automate acceptance/rejection of generated QA pairs and to select single-word scientific terms for cloze masks, using uncertainty-based active sampling during fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4o-mini (fine-tuned evaluator)",
            "llm_description": "GPT-4o-mini fine-tuned separately for MCQ validation, freeform QA validation, and cloze-term selection using small expert-labeled datasets (e.g., 160 examples for cloze term selection).",
            "scientific_domain": "Automatic QA annotation and validation for climate QA generation",
            "evaluation_method": "Supervised fine-tuning with uncertainty-based active sampling; classifier outputs validity labels with confidence; samples above confidence threshold (0.85) subsampled during training to force learning from representative examples.",
            "evaluation_criteria": "Classifier accuracy/precision across train-test splits; used to label QA pairs as valid/invalid and to mark cloze terms (single-word selection).",
            "benchmark_or_dataset": "Trained on ClimaQA-Gold expert labels (e.g., 566 items split across tasks); cloze term model trained on 160 expert examples.",
            "results_summary": "Evaluator models achieved improvements in generated QA quality (MCQ set quality up ~10%, freeform up ~5%); validation rates reported ~85% valid for MCQs and ~90% valid for freeform QAs produced by generator after evaluator filtering.",
            "limitations_or_challenges": "Performance limited by small expert-labeled training sets and domain coverage; evaluator itself may propagate biases and errors; requires initial human-labeled seed to be effective.",
            "comparison_to_human_or_traditional": "Automates steps previously requiring human validators; still relies on human labels for training and quality assurance, and human experts remain necessary for edge cases.",
            "recommendations_or_best_practices": "Use active sampling and confidence-based subsampling during fine-tuning; maintain a human-in-the-loop monitoring process and expand expert-labeled data to improve evaluator robustness.",
            "uuid": "e9659.4",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Adaptation-Comparisons",
            "name_full": "Adaptation Strategies Comparison (RAG, Continued Pre-training, Fine-tuning, Few-shot)",
            "brief_description": "Empirical comparison of adaptation methods for LLMs on scientific QA: Retrieval-Augmented Generation (RAG) from source vs held-out textbooks, continued pre-training on domain textbooks, and supervised fine-tuning on the ClimaQA-Silver dataset, plus few-shot prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Various evaluated models (see ClimaQA entry)",
            "llm_description": "Multiple LLMs assessed under adaptation methods; some models additionally continued pre-trained or fine-tuned (e.g., LLaMA3.1-8B-Instruct, Mistral-7B-v0.3-Instruct).",
            "scientific_domain": "Climate science QA",
            "evaluation_method": "Measure task metrics (MCQ accuracy, Cloze EM & PhraseSim, Freeform BLEU/BERTScore/Factual Accuracy) after applying adaptation methods: rag-source, rag-held-out, cp-held-out (continued pre-training), ft-silver (fine-tune on synthetic silver), few-shot prompting.",
            "evaluation_criteria": "Task-specific metrics: MCQ accuracy (percent), Cloze exact-match and phrase similarity, Freeform BLEU/BERTScore/Factual Accuracy. Comparison across model variants and retrieval sources.",
            "benchmark_or_dataset": "Evaluated on ClimaQA-Gold (held-out five textbooks) and ClimaQA-Silver subsets; retrieval sources: source textbooks (used to generate questions) vs held-out textbook pool.",
            "results_summary": "RAG using source textbooks (rag-source) produced the highest gains in most scenarios (e.g., GPT-4o rag-source ~92.79 overall MCQ accuracy in Table 3); continued pre-training improved MCQ/Cloze performance; fine-tuning on silver improved performance but RAG-source often outperformed ft-silver; retrieval from held-out books sometimes reduced performance due to distractors.",
            "limitations_or_challenges": "Retrieval from irrelevant/held-out materials can harm performance; pre-training risks data contamination if source overlap exists; fine-tuning on synthetic silver depends on silver quality.",
            "comparison_to_human_or_traditional": "RAG-source approximates providing human-curated context and outperforms unguided models, suggesting retrieval of authoritative sources better mirrors expert-provided context than mere continued pre-training or few-shot prompting.",
            "recommendations_or_best_practices": "Prefer retrieval from high-quality, source-aligned texts (rag-source) when available; combine continued domain pre-training with targeted fine-tuning on curated synthetic datasets; carefully curate retrieval corpora to avoid distracting/irrelevant content.",
            "uuid": "e9659.5",
            "source_info": {
                "paper_title": "ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Climate-Fever: A dataset for verification of real-world climate claims",
            "rating": 2,
            "sanitized_title": "climatefever_a_dataset_for_verification_of_realworld_climate_claims"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "The sciqa scientific question answering benchmark for scholarly knowledge",
            "rating": 1,
            "sanitized_title": "the_sciqa_scientific_question_answering_benchmark_for_scholarly_knowledge"
        },
        {
            "paper_title": "ScienceQA",
            "rating": 1
        },
        {
            "paper_title": "Are machines better at complex reasoning? unveiling human-machine inference gaps in entailment verification",
            "rating": 2,
            "sanitized_title": "are_machines_better_at_complex_reasoning_unveiling_humanmachine_inference_gaps_in_entailment_verification"
        }
    ],
    "cost": 0.015369500000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS
9 Mar 2025</p>
<p>Vignesh Veeramakali 
University of California
San Diego</p>
<p>Yasaman Manivannan vmanivannan@ucsd.edu 
University of California
San Diego</p>
<p>Srikar Jafari yajafari@ucsd.edu 
University of California
San Diego</p>
<p>Spencer Eranky seranky@ucsd.edu 
University of California
San Diego</p>
<p>Rose Ho 
University of California
San Diego</p>
<p>Duncan Yu 
University of California
San Diego</p>
<p>Yian Watson-Parris dwatsonparris@ucsd.edu 
University of California
San Diego</p>
<p>Leon Ma 
University of California
San Diego</p>
<p>Taylor Bergen lbergen@ucsd.edu 
University of California
San Diego</p>
<p>Berg-Kirkpatrick 
University of California
San Diego</p>
<p>Rose Yu roseyu@ucsd.edu 
University of California
San Diego</p>
<p>Taylor Berg-Kirkpatrick 
University of California
San Diego</p>
<p>CLIMAQA: AN AUTOMATED EVALUATION FRAME-WORK FOR CLIMATE QUESTION ANSWERING MODELS
9 Mar 2025C26CC17F8E4CB9CA960F582893E35AACarXiv:2410.16701v2[cs.LG]
The use of Large Language Models (LLMs) in climate science has recently gained significant attention.However, a critical issue remains: the lack of a comprehensive evaluation framework capable of assessing the quality and scientific validity of model outputs.To address this issue, we develop ClimaGen (Climate QA Generator), an adaptive learning framework that generates question-answer pairs from graduate textbooks with climate scientists in the loop.As a result, we present ClimaQA-Gold, an expert-annotated benchmark dataset alongside ClimaQA-Silver, a large-scale, comprehensive synthetic QA dataset for climate science.Finally, we develop evaluation strategies and compare different LLMs on our benchmarks.Our results offer novel insights into various approaches used to enhance knowledge of climate LLMs.ClimaQA's source code is publicly available at https://github.com/Rose-STL-Lab/genie-climaqa</p>
<p>INTRODUCTION</p>
<p>Climate change is one of the most pressing global challenges today, with profound impacts on ecosystems, economies, and societies.In recent years, Large Language Models (LLMs) have gained significant interest in climate science (Thulke et al., 2024;Nguyen et al., 2024;Cao et al., 2024) due to their potential to transform climate predictions and enable applications in climate policy analysis, environmental decision-making, and public education.By improving LLMs' understanding of climate science, we can empower stakeholders to make informed decisions, develop actionable solutions, and foster broader awareness of climate issues.However, while LLMs are powerful, they often fall short when it comes to answering technical questions requiring high precision such as What is the net effect of Arctic stratus clouds on the Arctic climate?Even advanced models like GPT-4 exhibit epistemological inaccuracies in Climate Question-Answering (QA) tasks (Bulian et al., 2024), raising concerns about their reliability in scientific workflows.This highlights the need for a domain-specific evaluation framework to assess the quality and validity of outputs generated by these models.Current benchmarks for Large Language Models (LLMs) predominantly focus on linguistic accuracy or general factual correctness (Bai &amp; Wang, 2021), but they fail to address the unique demands of climate science, where factual rigor, domain-specific knowledge, and robust reasoning are essential.Although some work has explored the scientific evaluation of LLMs (Table 1), they either rely heavily on manual expert input or employ fully synthetic question generations.To address this issue, we develop ClimaGen, an adaptive learning framework for creating benchmarks in collaboration with domain experts to evaluate scientific question-answering models, specifically for climate science but adaptable to other scientific disciplines, shown in Figure 1.This enables us to achieve a balance between utilizing the efficiency of LLMs and the expertise of domain specialists.</p>
<p>Using our framework, we introduce a novel benchmark for evaluating question-answering models in climate science across three scientific QA task forms: multiple-choice, freeform, and cloze.The questions are designed with varying levels of complexity, challenging the models to demonstrate a range of reasoning abilities from basic factual recall to scientific reasoning and scenario applications.</p>
<p>Figure 1: ClimaGen -Our proposed Automated Benchmark Creation Framework.The QA generation framework creates synthetic data from seed contexts extracted from graduate-level textbooks using LLMs to generate base-level question-answer pairs and evolve them by adding complexities to the same.These are validated by domain experts during the annotation process to produce the semisynthetic benchmark.The evaluator model is trained actively using the human-labeled examples in order to completely automate the process.</p>
<p>The benchmark consists of two datasets: ClimaQA-Gold -an expert-annotated dataset with a total of 566 questions validated by climate scientists, ensuring high scientific rigor, and ClimaQA-Silver -a large-scale synthetic dataset consisting of 3000 questions generated by our framework, providing substantial ground truth data for model fine-tuning at scale.Together, these datasets enable comprehensive performance assessment of LLMs in climate science, specifically for scientific QA tasks.We evaluate several LLMs on our benchmark under different settings.We observe that most models struggle with reasoning-based multiple-choice questions (MCQs) and Retrieval Augmented Generation (RAG) (Lewis et al., 2020) significantly outperforms Continued Pre-training and Supervised Fine-tuning across different tasks.</p>
<p>In summary, our contributions are as follows:</p>
<p>• Creation of publicly releasable datasets: expert-annotated (ClimaQA-Gold) and synthetic (ClimaQA-Silver), along with tailored evaluation metrics facilitating both rigorous assessment and large-scale fine-tuning on 3 scientific QA task forms: multiple-choice, freeform, and cloze, with varying levels of complexity.</p>
<p>• Development of a generalized adaptive learning framework (ClimaGen) for creating scientific benchmarks at scale in collaboration with domain experts for evaluation of natural language question-answering models on scientific accuracy.</p>
<p>• Evaluation of state-of-the-art LLMs on climate science QA tasks, with insights into improving scientific accuracy.ScienceQA (Lu et al., 2022) contains a vast collection of multimodal MCQs manually curated from high school textbooks.Pira2 (Pirozelli et al., 2024) consists of expert-created questions derived from research articles focused on oceans, the Brazilian coast, and climate change.The creation of these benchmarks required substantial manual effort.SciQA (Auer et al., 2023) innovatively generates freeform QA pairs by leveraging hand-crafted queries on the Open Research Knowledge Graph (Jaradeh et al., 2019) primarily drawing from computer science literature.Although these pairs are factually accurate, they do not include an automatic evaluation method for generated responses.Climate Crisis QA (Zhu &amp; Tiwari, 2024) and SciQAG-24D (Wan et al., 2024) explore synthetic data generations using Large Language models.However, their approaches are prone to suffer from hallucinations and lack of scientific validity.To address this, we introduce a gold-standard dataset, rigorously validated by domain experts, alongside a large-scale silver dataset whose generation process was guided by these expert validation labels.Moreover, existing benchmarks generally focus on a single QA format and lack scientifically aligned evaluation metrics.Our benchmark contains questions of three distinct scientific QA task forms at varying levels of complexity, along with evaluation metrics tailored to them.(Devlin, 2018) for summarization and keyword extraction, employing WordNet (Miller, 1995) to generate distractors.(Das et al., 2021) applies RAKE (Rose et al., 2010) for keyword extraction and clustering methods for distractor generation.Other approaches, such as utilizing dependency trees (Afzal &amp; Mitkov, 2014), have also been explored for MCQ creation.These methods typically focus on generating MCQs with single-word answers.However, the recent advancements in LLMs have enabled the creation of more complex MCQs with longer, detailed answer choices.(Meißner et al., 2024) demonstrate the automated generation of self-assessment quizzes using LLMs, while (Hang et al., 2024) explore self-refining prompting techniques for improved MCQ generation.Recent studies, including (Olney, 2023) and (Doughty et al., 2024), suggest that LLMs can generate MCQs comparable to those created by humans, though (Grévisse et al., 2024) emphasize the importance of human oversight to ensure the quality and pedagogical relevance of these questions.</p>
<p>CLIMAQA -CLIMATE QUESTION ANSWERING BENCHMARK</p>
<p>The ClimaQA benchmark is built on questions generated from graduate-level climate science textbooks, ensuring alignment with the precise terminology and complex theories of the field.These textbooks provide a reliable source for generating both the expert-validated ClimaQA-Gold dataset and the synthetic ClimaQA-Silver dataset.By leveraging textbook content and combining it with expert review, ClimaQA facilitates rigorous evaluation and fine-tuning of LLMs across freeform, multiple-choice, and cloze question-answering tasks in climate science.Our expert-validated dataset, ClimaQA-Gold, ensures that the evaluation questions are accurate, relevant, and reflect the current understanding of climate science.</p>
<p>SCIENTIFIC QUESTION ANSWERING</p>
<p>To thoroughly evaluate a model's ability to handle scientific questions, we create our benchmark dataset to focus on the different complexities of scientific reasoning.The aim is to test the model's ability to engage with scientific concepts at different levels of understanding and scenario application.</p>
<p>Our benchmark consists of questions of three levels of complexity.The first level involves basic questions designed to test straightforward factual understanding.The second level introduces reasoning, requiring the model to connect multiple scientific facts or principles.The third level involves hypothetical scenarios, testing the model's ability to apply scientific knowledge in unseen contexts.These questions challenge the model's scientific reasoning in different ways, from knowledge recall to advanced reasoning and problem-solving in dynamic contexts.A question from each level of complexity is shown in Figure 2 as an example.The second is the enhanced version of the question that requires scientific reasoning to answer.The third is the modified version of the question that involves a hypothetical scenario.The contexts from the textbook data were used during the question evolution.</p>
<p>The questions come in three different task forms, demonstrated in Figure 3:</p>
<p>• MCQ: The model selects correct answers from predefined options, assessing its factual accuracy and decision-making under constrained conditions.</p>
<p>• Freeform: The model generates detailed, structured responses, testing its ability to reason logically and produce scientifically sound explanations.</p>
<p>• Cloze: The model fills in blanks with appropriate scientific terms, evaluating its contextual understanding and use of domain-specific vocabulary.</p>
<p>Together, the benchmark as shown in Table 2 provides a robust framework for evaluating an LLM's proficiency in scientific reasoning, critical thinking, and applying knowledge in unseen scenarios.</p>
<p>EVALUATION METRICS</p>
<p>Although assessing multiple-choice question-answering is relatively simple, the other two tasks present more challenges.To address this, we propose and validate the following evaluation metrics for freeform and cloze question-answering.A more detailed case study to demonstrate the robustness of these metrics can be found in the Appendix A.2.3.We use these metrics to report experimental results in Section 5  Various metrics are employed to evaluate sentence similarity, ranging from surface-level comparisons to deeper semantic analysis.Lexical metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee &amp; Lavie, 2005) focus on exact word or n-gram matching, rendering them useful for tasks where token overlap is crucial, such as machine translation.In contrast, semantic metrics like BERTScore (Zhang et al., 2019), Word Mover's Distance (WMD) (Huang et al., 2016), and Sentence-BERT (Reimers, 2019) are more advanced, capturing the meanings of sentences through embeddings.These metrics are better suited for tasks that necessitate an understanding of meaning, such as paraphrase detection.However, they may not adequately assess factual accuracy.</p>
<p>To measure the factual accuracy of generated answers relative to reference answers, we propose the use of a factual entailment classifier, reporting the confidence level as the factual accuracy score.Instruction-tuned models, such as GPT-4, have demonstrated superior performance on textual en-tailment tasks and have shown the ability to generalize across various datasets (Sanyal et al., 2024).We employ the GPT-4o-mini model with the prompt below for factual entailment.This method achieved 81% zero-shot classification accuracy on the Climate-Fever dataset (Diggelmann et al., 2020) indicating the ability to measure factual accuracy.</p>
<p>You are a climate expert who annotates whether a given claim either SUPPORTS or REFUTES the presented evidence.You will be provided with the following input:</p>
<p>Evidence: ⟨evidence⟩ Claim: ⟨claim⟩</p>
<p>Respond with only one word: SUPPORTS if the claim supports the evidence and REFUTES otherwise.</p>
<p>To use this for scoring freeform QA, the reference answer was used as the evidence and the generated answer was used as the claim.The confidence score was computed by applying sigmoid smoothing to the logit scores, with the temperature parameter set to T = 5.Note that the choice of T does not alter the score trend; it was selected to optimally scale values between 0 and 1.If l s and l r are the logit scores for SUPPORTS and REFUTES respectively, then</p>
<p>Factual Accuracy = SigmoidSmooth l s l s + l r Overall, we report three metrics for freeform question answering (QA): BLEU, BERTScore, and Factual Accuracy to evaluate different aspects of the generated answers.</p>
<p>CLOZE QA</p>
<p>Performance on this task is typically evaluated using the exact match metric.However, this approach has limitations due to the existence of multiple correct answers.A generated answer may differ from the reference answer while remaining contextually and semantically valid; for instance, while 'point' and 'temperature' are semantically distinct terms, they can be contextually similar within phrases like 'freezing point' and 'freezing temperature.'This illustrates that semantic relationships can depend heavily on context.To address this challenge, we introduce a metric that captures the semantic similarity between the generated answer and the ground-truth answer for a more nuanced assessment of model performance.Specifically, we utilize a context window to extract two phrases: one with the ⟨blank⟩ replaced by the reference answer and the other with the generated answer.The semantic similarity is measured using cosine similarity between the Universal Sentence Encoder (Cer, 2018) embeddings of these phrases.</p>
<p>To evaluate the robustness of this approach, we synthetically generated 219 cloze questions with our framework (described in Section 4), which were answered by the GPT-4o-mini model.We collected 32 questions where the generated answers did not exactly match the reference answer.These answers were then labeled as wrong or correct by domain experts based on scientific and contextual accuracy.We plotted the average cosine similarities of phrases for these questions as shown in Figure 4, concluding that a context window of size 4 most effectively differentiates between correct and incorrect answers.This configuration yields the maximum difference in scores while maintaining sufficiently high scores for correct answers.The cosine similarities were subsequently rescaled to emphasize these differences.If e 1 and e 2 are the embeddings of the respective phrases as mentioned above, then Phrase Similarity = 2 × (CosineSimilarity(e 1 , e 2 ) − 0.5)</p>
<p>We report two metrics for cloze question answering (QA): Exact Match and Phrase Similarity.</p>
<p>CLIMAGEN -AUTOMATED BENCHMARK CREATION</p>
<p>In this section, we describe ClimaGen as shown in Figure 1, the framework used to create the ClimaQA dataset from Climate Textbooks.By leveraging RAG and prompt engineering, we systematically generate and refine questions of varying complexity levels.Domain expert annotators ensure quality, producing a semi-synthetic benchmark for the evaluation of AI systems in complex scientific inquiry.Additionally, we automate annotation by fine-tuning an LLM on human-labeled data, enabling the creation of a large-scale synthetic dataset for fine-tuning tasks.The techniques discussed here could be generalized to aid in the semi-automatic production of benchmarks for other scientific fields aswell.</p>
<p>TEXTBOOK DATASET</p>
<p>LLMs are typically pre-trained on extensive general internet data, which often contains noise and misinformation.This limitation is particularly significant in fields like climate science, where a precise understanding of specialized terminology and concepts is crucial.To evaluate LLMs' proficiency in climate science, we employed graduate-level climate science textbooks as a reliable source of specialized knowledge, providing accurate scientific information that better represents the technical terms and nuanced theories integral to this discipline.We collected 18 textbooks (Table 4) that broadly represent a mixture of graduate and expert literature on the physical climate with a particular focus on the role of aerosol in the climate system -one of the critical sources of uncertainty in climate projections.The content was extracted and preprocessed to ensure cleanliness and relevance, making it suitable for downstream applications such as benchmark creation, continuous pre-training, and RAG.A held-out set of 5 textbooks (Figure 6), carefully selected to represent varying levels of technical and qualitative depth across a broad range of key topics in climate science, was utilized for the benchmark creation process.</p>
<p>QA GENERATION FRAMEWORK</p>
<p>Our QA generation pipeline begins by selecting a random seed 2000-character context chunk from the collected textbook data stored in a vector database.Additional context chunks are retrieved based on cosine-similarity scores, ensuring relevant information from multiple sources is included.These chunks are then augmented and passed to the generator LLM for question-answer (QA) generation.Question generation principles, inspired by (Doughty et al., 2024), guide the prompt formulation, focusing on creating high-quality stems and distractors for MCQs, as well as refining questions by adding complexity as described in Appendix A.4.We used GPT-3.5-turboas the generator model in our experiments.The model generates baselevel questions and evolved variants with increasing complexity, such as multi-step reasoning and hypothetical scenarios, to ensure a diverse and comprehensive question set as shown in Figure 2.However, approximately 25% of the multiple-choice questions were incorrectly answered by the same model even when the contexts were provided, often due to scientifically inaccurate questionanswer pairs, indicating self-inconsistency and uncertainty in the generation process.</p>
<p>While QA pairs were generated for multiple-choice and freeform questions, plain scientific statements intended for cloze questions were also generated, from which the scientific term to be masked would be chosen during the annotation phase.After generation, the questions undergo a preliminary screening with both handcrafted and LLM-based (self-inconsistency) audits to filter out potentially invalid QA pairs.The refined set of QA pairs is then passed to the annotation phase for further validation.</p>
<p>DOMAIN EXPERT ANNOTATION</p>
<p>One key challenge with synthetic data is ensuring its distribution closely mirrors real-world data, as deviations can negatively impact downstream tasks like fine-tuning and evaluation.This issue arose when generating scientific questions with GPT-3.5-turbo, which sometimes produced inaccurate or imprecise data probably due to a limited understanding of domain-specific terminology.</p>
<p>To mitigate this, we developed an interactive web application that enables climate scientists to review and annotate the generated questions.For freeform and MCQs, the scientists validated the correctness of the content, while for cloze questions, they selected which scientific term to mask, ensuring alignment with scientific standards.They also identified common reasons for rejecting generated QA pairs during validation, providing valuable insights into improvement of the QA generation framework as discussed in Appendix A.5.1.By combining human expertise with AI, we curated 245 freeform, 161 MCQ, and 160 cloze questions, forming the ClimaQA-Gold dataset, reviewed and validated by domain experts.</p>
<p>AUTOMATED ANNOTATION</p>
<p>To fully automate the review and annotation process, we develop an evaluator model by fine-tuning an LLM (GPT-4o-mini) on expert-annotated data to validate and refine generated question-answer pairs.This removes the need for human intervention and enables scalable generation of high-quality scientific question-answer pairs, especially for data-intensive tasks like fine-tuning.</p>
<p>Building on (Zhang et al., 2023) and (Zhang et al., 2024), which demonstrate that uncertainty-based active sampling improves supervised fine-tuning with limited labeled data, we apply a similar approach.The evaluator model is fine-tuned as a classifier to label QA pairs as valid or invalid based on the given context as described in Appendix A.5.2.Uncertainty is measured by the classifier confidence scores, and samples with confidence above 0.85 are dropped with 50% probability to ensure learning from more representative examples.The evaluator models were fine-tuned separately for both MCQs and freeform questions We observed that around 85% of multiple-choice questions (MCQs) and around 90% of freeform question-answers (QAs) were valid, indicating high-quality question generation.Experiments across different train-test splits show that the evaluator models enhance the quality of the generated MCQ question set by 10% and the Freeform question set by 5% as shown in Appendix A.5.2.Additionally, we fine-tune a separate model to mark scientific terms from given statements as shown in Appendix A.5.3, automating the cloze annotation process.Using this framework, we generated 1000 freeform QAs, 1000 MCQs, and 1000 cloze questions, collectively forming the ClimaQA-Silver dataset, produced synthetically at scale without manual intervention.</p>
<p>EXPERIMENTS</p>
<p>We aim to investigate the effectiveness of various adaptation techniques on this fine-grained scientific benchmark.Fine-tuning on raw text data within a target domain is a common approach, and we seek to evaluate its effectiveness for addressing deep scientific questions.In addition, we evaluate other techniques, such as in-context learning and retrieval augmentation.</p>
<p>EXPERIMENTAL SETUP</p>
<p>We evaluate different families of LLMs on our benchmark.We use TogetherAI for performing inference on open source models like gemma-27b (Team et al., 2024b), llama3-70b (Dubey et al., 2024), and mixtral-8x22b (Jiang et al., 2024).We also evaluate OpenAI's (Achiam et al., 2023) gpt-3.5-turboand gpt-4o.</p>
<p>We evaluate each of these models in 3 settings -default, few-shot prompting (FS)(Brown, 2020), and Retrieval Augmented Generation (RAG) (Lewis et al., 2020).For the MCQs, the models were prompted to output a single letter representing the correct option, and the top-most token was chosen as the answer.For Freeform QA, the models were prompted to output concise answers with a maximum of 2 sentences.For Cloze QA, the models were prompted to output a single scientific word that best fits the blank with respect to the context around it.</p>
<p>We conduct further pre-training on graduate-level textbook data for both the LLaMA3.1-8B-Instructand Mistral-7B-v0.3-Instruct (Jiang et al., 2023) models.This pre-training was based on 13 distinct graduate textbooks that were not part of the question-generation process.The objective was to enhance the model's climate knowledge without directly exposing it to the specific sources used for question generation, thereby reducing the risk of data contamination.Additionally, we fine-tune LLaMA3.1-8B-Instruct and Mistral-7B-v0.3-Instruct on the ClimaQA-Silver dataset, which contains all three forms of MCQ, Freeform, and Cloze, in different complexity levels.We then evaluate the impact of this task-specific fine-tuning by assessing the models' performance on the ClimaQA-Gold dataset.The details of the continued pre-training and fine-tuning procedure are explained in Appendix A.3.We also leverage Gemini 1.5 (Team et al., 2024a), with a context window of up to 1 million tokens, to pass an entire textbook in context and answer questions based on that.Finally, we evaluate models on the ClimaQA-Silver dataset and analyze its potential differences from ClimaQA-Gold in Appendix A.2.2.</p>
<p>RESULTS</p>
<p>We report the performance of various models across different QA forms and complexities.Table 3 shows the results for the MCQ and Cloze form questions , and free-form results are demonstrated in Appendix Table 5.We observe that most models struggle with reasoning MCQs compared to base and hypothetical questions.While some models perform poorly on reasoning and hypothetical MCQs, they tend to generate stronger responses for the same type of Freeform Questions, indicating improved performance when reasoning is emphasized over factual recall similar to observations of Chain of Thought experiments (Wei et al., 2022), as shown in Figure 5.We examine the impact of providing relevant context through RAG when answering questions.We explore two retrieval scenarios: one where the model retrieves from 13 books that were not used to generate the questions (rag-held-out), and another where it retrieves from the 5 books that the questions were derived from (rag-source).Retrieval from source textbooks consistently enhances performance across all tasks.For Gemini 1.5, we identify the book corresponding to the most relevant retrieved chunk under both the source and held-out settings, and include it as in-context information to answer the question.Additional context from source books yields a slight improvement over the no-context baseline.However, in both RAG and long-context scenarios, incorporating held-out books usually reduces performance, likely due to irrelevant or distracting content.</p>
<p>Continued pertaining (cp-held-out) on the graduate textbooks leads to improved performance in both MCQs and Cloze question-answering tasks.Additionally, fine-tuning (ft-silver) on the ClimaQA-Silver dataset further enhances performance, often producing the best results after RAG on the source textbooks (rag-source) in most scenarios.Furthermore, Few-shot prompting yields marginal improvements in most cases.</p>
<p>Finally, We observe that the BLEU and BERTScore metrics are slightly biased towards the model that was used for QA-generation (gpt-3.5-turbo)while this is not seen in the proposed Factual Accuracy metric 5. Overall, GPT-4o dominates across tasks, demonstrating superior performance compared to other models in this evaluation set.</p>
<p>CONCLUSION</p>
<p>The ClimaQA benchmark offers a comprehensive framework for evaluating language models in climate question-answering, addressing critical aspects such as reasoning, factual accuracy, and understanding of scientific terminology.By incorporating freeform, multiple-choice, and cloze task forms with different levels of complexity, the benchmark rigorously tests models across different dimensions of scientific inquiry.Furthermore, the use of advanced metrics, such as factual accuracy for freeform tasks and phrase similarity for cloze tasks, can provide a more nuanced assessment of model performance.</p>
<p>The automated benchmark generation framework (ClimaGen) integrates domain-specific textbooks and natural language understanding of LLMs along with human expertise to produce high-quality QA data at scale.However, the benchmark's reliance on only five textbooks limits the diversity of contexts, and the small size of the annotated dataset constrains the effectiveness of automated annotation.Addressing these limitations with a broader corpus and expanded annotation data will improve future benchmarks.</p>
<p>While models like GPT-4o performed well on reasoning-based tasks, the overall performance of models highlights the ongoing challenge of achieving consistent scientific accuracy.In conclusion, ClimaQA sets a new standard for evaluating scientific question-answering models, providing a foundation for future advancements in AI-driven climate research.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>The ClimaQA dataset, both gold and silver, is publicly available at Hugging Face1 .While we cannot release the scraped textbook data due to copyright restrictions, the references to all textbooks used are provided in the appendixA.1,allowing for reconstruction of this dataset.Our complete codebase, including data generation pipeline, web-UI and model evaluation scripts, is available in our GitHub repository2 .The ClimaGen framework is fully reproducible; however, training the evaluator models requires domain expert inputs, which may introduce variability.Furthermore, reproduction of all parts of our framework requires appropriate API keys for external services.</p>
<p>A APPENDIX</p>
<p>A.1 TEXTBOOK DATASET Table 4: List of climate textbooks used in this work along with the number of pages and the number of context chunks.The extracted data was preprocessed to omit figures and tables.The content of each page was then split into overlapping context chunks of around 2000 characters.</p>
<p>Textbook Pages Chunks</p>
<p>Aerosol Measurement (Baron &amp; Willeke, 2001) 937 1813 Aerosols and Climate (Carslaw, 2022) 854 1766 Airborne CCN Measurements (Trembath, 2013) 268 333 An Introduction to Clouds (Lohmann et al., 2016) 377 606 Atmospheric Chemistry and Physics (Seinfeld &amp; Pandis, 2016) 1127 1789 Atmospheric Science (Wallace &amp; Hobbs, 2006) 495 1085 Calculus of Variations (Gelfand &amp; Fomin, 2012) 239 266 Clouds in the Perturbed Climate System (Heintzenberg &amp; Charlson, 2009) 598 1060 Eloquent Science (Schultz, 2013) 420 725 Filtering Complex Turbulent Systems (Majda &amp; Harlim, 2012) 358 475 Fundamental of Atmospheric Modeling (Jacobson, 2005) 826 1204 Geostatistics for Environmental Scientists (Webster &amp; Oliver, 2007) 327 457 Global Physical Climatology (Hartmann, 2015) 481 715 Principles Of Planetary Climate (Pierrehumbert, 2010) 468 1096 Forests and Climate Change (De Wasseige et al., 2015) 119 250 Simulating Nature (Petersen, 2012) 205 386 Statistical Methods in the Atmospheric Sciences (Wilks, 2019) 806 1597 Stochastic Climate Models (Imkeller &amp; von Storch, 2012) 411 594</p>
<p>To assess the proficiency of LLMs in climate science, we utilized graduate-level climate science textbooks as a reliable source of specialized knowledge.These textbooks were selected for their accurate and comprehensive representation of the technical terminology and nuanced theories integral to the field.The collection was curated from the virtual bookshelf of a professor in atmospheric physics and broadly represents a mixture of graduate and expert textbooks on the physical climate, with a particular focus on the role of aerosol in the climate system (which provides one of the key uncertainties in climate projections).The complete list of textbooks is provided in Table 4.</p>
<p>Figure 6: The distribution of ClimaQA-Gold questions over the corresponding source.</p>
<p>The five textbooks selected for question generation were carefully chosen to reflect different levels of breadth and depth (both technically and more qualitatively) across a range of important topics in climate science.These include Atmospheric Science, a comprehensive graduate-level textbook on atmospheric science; Aerosols and Climate, a brand-new and more detailed textbook on the role of aerosol in the climate system; An Introduction to Clouds, another relatively new textbook providing the latest research on the important role of clouds in the climate system; Geostatistics for</p>
<p>A.2.2 CLIMAQA-SILVER EXPERIMENTS</p>
<p>In this section, we assess the models in their default settings on a subset of the ClimaQA-Silver dataset, consisting of 200 questions for each task type.The column-wise trends observed in the results are largely consistent with those from the ClimaQA-Gold dataset.However, a notable difference lies in the relative difficulty of the complexities in the MCQ task, even though the overall scores do not vary significantly.This discrepancy highlights the intricate nature of the MCQ generation process.Additionally, this variation may also be attributed to the relatively small number of questions in each column.Future work should focus on scaling up the expert validation process to enhance the quality of the automated annotation pipeline, thereby addressing these challenges and improving overall dataset reliability.In this section, we present sample questions from the ClimaQA-Gold dataset along with responses from various LLMs and their corresponding evaluation scores.Freeform QA In this example, the reference answer discusses increased global emissions in the absence of air quality measures.While the outputs of most LLMs align with this reference, the response from gemma-27b suggests a decline in global emissions.Note that both the Factual Accuracy and BLEU metrics penalize this response, whereas the BERTScore does not.</p>
<p>Quesition: How might the global emission estimates have been different if air quality measures were not introduced mainly targeted at large, coal-burning installations before 2000?</p>
<p>Reference Answer: Without the introduction of air quality measures mainly targeted at large, coal-burning installations before 2000, the global emission estimates, particularly for SO2, may not have shown a decrease before 2000.The emissions could have continued to rise or remained at higher levels due to the ongoing industrial growth and lack of regulations to curb pollution from such sources.gpt-3.5-turbo:Global emission estimates would likely have been higher if air quality measures targeting large, coal-burning installations were not introduced before 2000, as these measures have helped to reduce emissions of pollutants such as sulfur dioxide and particulate matter.</p>
<p>Bleu: 0.485, Bert: 0.901, Factual Accuracy: 0.988 gpt-4o: Without air quality measures targeting large, coal-burning installations before 2000, global emissions, particularly sulfur dioxide and particulate matter, would likely have been significantly higher, exacerbating air pollution and related health issues.This could have also contributed to more severe acid rain and potentially altered climate patterns due to increased atmospheric aerosols.</p>
<p>Bleu: 0.589, Bert: 0.898, Factual Accuracy: 0.982 llama3-70b: If air quality measures were not introduced before 2000, global emission estimates would likely be higher, as coal-burning installations would have continued to emit large amounts of pollutants, including particulate matter, sulfur dioxide, and nitrogen oxides, contributing to increased greenhouse gas emissions.This could have resulted in an additional 1-2 gigatons of CO2 equivalent emissions per year, exacerbating climate change impacts.</p>
<p>Bleu: 0.542, Bert: 0.888, Factual Accuracy: 0.968 mixtral-8x22b: If air quality measures hadn't been introduced for large, coal-burning installations before 2000, global emission estimates would likely have been significantly higher due to unregulated emissions from these sources.This could have potentially accelerated global warming and worsened air quality on a global scale.</p>
<p>Bleu: 0.546, Bert: 0.910, Factual Accuracy: 0.988 gemma-27b: Global emission estimates would likely be significantly higher without those air quality measures.Reduced emissions from coal-burning power plants would have contributed less to the overall decline in global emissions observed since 2000.</p>
<p>Bleu: 0.364, Bert: 0.885, Factual Accuracy: 0.741</p>
<p>In the following example, the reference answer indicates that the distance from the sun influences surface pressure, with higher pressures during summer.Responses from gpt-3.5-turbo and mixtral-8x22b contradict this statement but still achieve high BLEU and BERTScore values.The Factual Accuracy metric, however, correctly identifies and penalizes this inconsistency.Notably, gpt-4o provides a correct answer yet receives a low BLEU score, as BLEU is not robust against paraphrasing.</p>
<p>Quesition: How does the distance from the sun affect the surface pressure on Mars?</p>
<p>Reference Answer: The surface pressure on Mars varies seasonally from 7.0 hPa during the austral winter, when Mars is farthest from the sun, to 9.0 hPa during the austral summer.gpt-3.5-turbo:The distance from the sun does not directly affect the surface pressure on Mars; instead, the surface pressure on Mars is primarily influenced by the planet's atmospheric composition and temperature.</p>
<p>Bleu: 0.388, Bert: 0.880, Factual Accuracy: 0.175 gpt-4o: The distance from the Sun affects Mars' temperature, which in turn influences the sublimation of its polar ice caps, impacting atmospheric pressure.During warmer periods, more CO2 sublimates, increasing surface pressure, while colder periods result in lower pressure.</p>
<p>Bleu: 0.253, Bert: 0.865, Factual Accuracy: 0.980 llama3-70b: The distance from the Sun affects the surface pressure on Mars by controlling the amount of atmospheric gas that freezes out or escapes to space, with more gas freezing out at greater distances.As a result, Mars' surface pressure is about 1% of Earth's, due to its average distance from the Sun being about 1.5 times greater than Earth's.</p>
<p>Bleu: 0.262, Bert: 0.861, Factual Accuracy: 0.896 mixtral-8x22b: The distance from the sun does not significantly affect the surface pressure on Mars.Instead, Mars' thin atmosphere and low gravity are the primary factors influencing its surface pressure.</p>
<p>Bleu: 0.403, Bert: 0.874, Factual Accuracy: 0.051 gemma-27b: Mars is farther from the sun than Earth, receiving less solar radiation.This weaker solar heating results in a much lower surface pressure on Mars compared to Earth.</p>
<p>Bleu: 0.458, Bert: 0.870, Factual Accuracy: 0.905 Interestingly, gemma-27b provides an irrelevant response that nevertheless receives high scores.While the response is factually accurate, it fails to address the question, illustrating limitations in existing evaluation metrics.This highlights the need for more fine-grained evaluation methodologies, which future research should aim to develop.</p>
<p>Cloze QA In the following examples, the models generate answers that differ from the reference answer but remain correct within the given context.This is effectively captured by the phrase similarity metric, which assigns higher scores to responses that are contextually aligned with the reference answer.</p>
<p>Question: Aqueous-phase oxidation of ⟨mask⟩ to form sulfuric acid in cloud droplets is a crucial process that contributes significantly to global sulfate mass and has long been recognized as important for aerosols and climate.</p>
<ol>
<li>
<p>Questions should be clear, concise, and free from unnecessary complexity or ambiguity.</p>
</li>
<li>
<p>Avoid overly long sentences and use consistent phrasing for repeated items.</p>
</li>
<li>
<p>Ensure questions are self-contained and provide all necessary context.4. Do not include phrases like "According to the provided context..." 5. Do not make any references to the given context in the question 6. Ensure that distractors do not overlap by reflecting different misconceptions on the topic.7. Minimize clues that make the correct answer obvious.8. Use "None of the Above" or "All of the Above" sparingly.9.Each MCQ must have exactly four answer choices (one correct, three distractors).10.Questions should not rely on external figures or tables.</p>
</li>
</ol>
<p>The user will provide one main context and some retrieved contexts separated by '-------' as the input.</p>
<p>Use details from retrieved context only if they are relevant to your question.</p>
<p>You must output a single JSON object in the following format: The following prompt was used to generate a freeform question-answer pair: You are a question paper setter creating freeform questions (MCQs) from a graduate-level climate science textbook.Your question must be related to a provided context.</p>
<p>Please respect the following rules to generate the question:</p>
<p>-The answer to the question should be found inside the provided context -The question must be self-contained -Do not include phrases such as "According to the provided context.."</p>
<p>The user will provide one main context and some retrieved contexts separated by '-------' as the input.</p>
<p>Use details from retrieved context only if they are relevant to your question.</p>
<p>You must output a single JSON objectin the following format: {</p>
<p>Figure 2 :
2
Figure 2: Examples of Question Evolution.The first is the initial version of the generated question.The second is the enhanced version of the question that requires scientific reasoning to answer.The third is the modified version of the question that involves a hypothetical scenario.The contexts from the textbook data were used during the question evolution.</p>
<p>Figure 3 :
3
Figure 3: Examples of the three types of scientific question-answering tasks presented in our benchmark</p>
<p>Figure 4 :
4
Figure 4: Mean Phrase Similarity for Correctly Answered and Incorrectly Answered Cloze Questions</p>
<p>Figure 5 :
5
Figure 5: Analysis of various LLMs under default setting on different tasks and different complexities.The first figure shows accuracy of models in the MCQ task while the others show different metrics under the Freeform task</p>
<p>Here c is the correct answer.Replace it with the actual correct answer.Make sure you return a valid JSON object.</p>
<p>Figure 7 :
7
Figure 7: The distribution of different reasons selected for scientific invalidity of generated QA pairs by domain experts.</p>
<p>Figure 8 :
8
Figure 8: Precision across various train-test splits of labeled data under different conditions: no validation (all data classified as positive), zero-shot validation, and fine-tuned validation.The test sets consisted of around 40 examples in each case.</p>
<p>Table 1 :
1
Comparison of scientific benchmarks.Automated indicates automatic creation, Validated shows expert validation, Multi-task represents multiple task types, and Multi-level represents questions of varying complexity
DatasetDomainSourceSize Automated Validated Multi-Task Multi-LevelScienceQAScience Hi-Scl Text 21000✗✓✗✗Pira2OceanResearch 2250✗✓✓✗SciQAComp Sci ORKG2500✓✓✗✗Climate Crisis ClimateNone20000✓✗✗✗SciQAG-24DScienceResearch 8531✓✗✗✗ClimaQA-Gold Climate Grad Text 566✓✓✓✓ClimaQA-Silver Climate Grad Text 3000✓✗✓✓</p>
<p>Table 1 presents a comparison of various scientific benchmarks.</p>
<p>Previous work on automated MCQ generation has focused on selecting keywords and generating distractors based on contextual information.(A.Nwafor &amp; E. Onyenwe, 2021)) utilize traditional NLP techniques, such as TF-IDF, for keyword extraction, while (Mehta et al., 2021) leverage BERT</p>
<p>Table 2 :
2
Contents of the ClimaQA dataset.Both ClimaQA-Gold and ClimaQA-Silver include 3 task-forms with varying levels of complexity for MCQ and Freeform.
DatasetTaskBase Reasoning Hypothetical TotalMCQ1267247245ClimaQA-GoldFreeform 545255161Cloze---160MCQ5012642351000ClimaQA-SilverFreeform 5072412521000Cloze---1000</p>
<p>Table 3 :
3
Performance analysis of various state-of-the-art LLMs on MCQs and Cloze QA.While source represents the set of books used for QA generation, held-out represents the remaining set of books.Bold marks the max within a model's variants and green highlights the overall column max.
ModelMCQClozeBase Reason Hypo Overall EM PSgemma-27b81.75 72.22 82.98 79.18 49.38 0.87gemma-27b-fs80.95 77.78 82.98 80.41 52.50 0.88gemma-27b-rag-source90.48 80.56 78.72 85.31 56.88 0.90gemma-27b-rag-held-out79.37 76.39 78.72 78.37 45.62 0.85gpt-3.5-turbo74.34 69.91 74.47 73.06 43.12 0.81gpt-3.5-turbo-fs76.98 74.54 76.60 76.19 43.75 0.78gpt-3.5-turbo-rag-source80.42 80.09 77.30 79.73 68.75 0.92gpt-3.5-turbo-rag-held-out70.63 71.30 69.50 70.61 39.38 0.81gpt-4o86.77 86.11 82.27 85.71 53.12 0.88gpt-4o-fs87.83 87.50 80.85 86.39 56.25 0.89gpt-4o-rag-source95.77 91.67 86.52 92.79 71.88 0.94gpt-4o-rag-held-out82.80 80.56 81.56 81.90 50.62 0.88llama3-70b84.92 80.56 82.98 83.27 38.75 0.82llama3-70b-fs82.54 81.94 82.98 82.45 48.12 0.85llama3-70b-rag-source92.06 84.72 87.23 88.98 63.12 0.91llama3-70b-rag-held-out80.95 76.39 85.11 80.41 43.75 0.84mixtral-8x22b80.16 79.17 80.85 80.00 35.62 0.75mixtral-8x22b-fs80.95 81.94 80.85 81.22 45.00 0.83mixtral-8x22b-rag-source90.48 80.56 76.60 84.90 45.00 0.78mixtral-8x22b-rag-held-out80.16 73.61 74.47 77.14 28.12 0.65mistral-7b64.29 63.89 82.98 67.76 17.50 0.74mistral-7b-fs65.08 66.67 76.6 67.76 33.12 0.80mistral-7b-rag-source92.86 84.72 85.11 88.98 57.5 0.88mistral-7b-rag-held-out66.67 62.5 74.47 66.94 22.50 0.76mistral-7b-cp-held-out72.22 62.5 74.47 69.8 21.25 0.76mistral-7b-ft-silver73.81 75.00 80.85 75.51 41.88 0.83llama3.1-8b76.98 62.50 65.96 70.61 26.25 0.77llama3.1-8b-fs76.19 72.22 76.60 75.10 38.75 0.82llama3.1-8b-rag-source94.44 83.33 89.36 90.2 72.50 0.92llama3.1-8b-rag-held-out76.19 66.67 76.6 73.47 36.25 0.81llama3.1-8b-cp-held-out77.78 75.00 72.34 75.92 30.00 0.77llama3.1-8b-ft-silver74.6 70.83 72.34 73.06 51.25 0.85gemini-1.5-flash82.54 73.61 87.23 80.82 50.62 0.88gemini-1.5-flash-long-cxt-source88.1 75.00 80.85 82.86 51.88 0.89gemini-1.5-flash-long-cxt-held-out 70.63 70.83 78.72 72.24 46.88 0.87</p>
<p>Table 6 :
6
Performance analysis of various LLMs on MCQs and Cloze QA in ClimaQA-Silver
ModelMCQClozeBase Reason Hypo Overall EM PSgemma-27b78.00 84.91 76.6 79.5 50.00 0.85gpt-3.5-turbo 76.00 75.47 72.34 75.0 38.00 0.78gpt-4o88.00 84.91 78.72 85.00 60.50 0.88llama3-70b85.00 79.25 65.96 79.00 44.00 0.83mixtral-8x22b 80.00 81.13 82.98 81.00 33.00 0.67</p>
<p>Table 7 :
7
Performance analysis of various LLMs on Freeform QA in ClimaQA-Silver
ModelBLEUBERTScoreFactual AcccuracyBRHOBRHOBRHOgemma-27b0.392 0.441 0.365 0.398 0.870 0.880 0.886 0.876 0.78 0.86 0.85 0.81gpt-3.5-turbo 0.467 0.523 0.545 0.500 0.885 0.892 0.907 0.892 0.71 0.79 0.84 0.76gpt-4o0.440 0.493 0.491 0.465 0.880 0.887 0.902 0.887 0.80 0.88 0.86 0.84llama3-70b0.335 0.474 0.569 0.425 0.874 0.888 0.904 0.885 0.77 0.82 0.75 0.78mixtral-8x22b 0.394 0.516 0.485 0.447 0.877 0.890 0.904 0.887 0.80 0.88 0.83 0.82A.2.3 CASE STUDY
https://huggingface.co/datasets/UCSD-GENIE/ClimaQA
https://github.com/Rose-STL-Lab/genie-climaqa
ACKNOWLEDGEMENTThis work was supported in part by the U.S. Army Research Office under Army-ECASE award W911NF-07-R-0003-03, the U.S. Department Of Energy, Office of Science, IARPA HAYSTAC Program, and NSF Grants #2205093, #2146343, #2134274, CDC-RFA-FT-23-0069, DARPA AIE FoundSci and DARPA YFA.We thank Sophie Wynn, Varan Madan, and David Vishny for providing expert validation of the ClimaQA dataset.Environmental Scientists, a more technical graduate-level textbook on geostatistics for environmental sciences; as well as Calculus of Variations, a classic calculus textbook to test the extent of the technical ability of the models.The exact distribution of questions from these textbooks is shown in Figure6.The remaining textbooks were used in the rag-held-out experiments to test the models without directly exposing them to the sources of the benchmark.A.2 MORE EXPERIMENTSA.2.1 FREEFORM EVALUATION RESULTS We used the ClimaQA-Silver dataset to fine-tune Llama3.1-8B and Mistral-7B-v0.3 on questions with the various question forms and complexities seen in the ClimaQA-Gold to see how this taskspecific fine-tuning can affect the performance.For this purpose, we used the hyperparameters, shown in 2. Correct Answer: The indisputable correct response to the stem.3. Distractors: Three incorrect but plausible answers.They should be:-Related to the stem and correct answer.-Positively phrased and true statements that don't answer the stem.-Plausible but incorrect, without giving clues to the correct answer.-Unique, each reflecting different misconceptions if possible.MCQ Guidelines:question: ⟨question⟩, answer: ⟨answer⟩ } Make sure you return a valid JSON object.The following prompt was used to generate a scientific statement for cloze question-answer generation:You are a scientific annotator.Given a scientific context from a climate textbook, generate a scientific statement based on the facts presented in the context.Please respect the following rules to generate the statement:-Generate only a single sentence -No external knowledge should be used or refered in generating the statement -Do not use phrases like 'based on the provided context.'The user will provide one main context and some retrieved contexts seperated by '-------' as the input.Use details from retrieved context only if they are relevant to your question.You must output a single JSON objectin the following format:Make sure you return a valid JSON object.A.4.2 COMPLEXITY ADDITIONThe following prompt was used to add reasoning complexity to the base freeform question-answer pair.Given a question-answer pair generated from the given context, Modify the question-answer pair to incorporate multi-step reasoning.Please respect the following rules to generate the question:-Answering the new question should encourage applying knowledge from 'Context' to deduce outcomes.-The new question must be fully answerable from 'Context'.-No external knowledge should be required to answer the new question -The question should not be dependent on external things such as figures or tables -Do not use phrases like 'based on the provided context.'The user will provide the original question, context, and some retrieved contexts separated by '-------' as the input.Use details from retrieved context only if they are relevant to your question.You must output a single JSON objectin the following format: The following prompt was used to add hypothetical scenario complexity to the base freeform question-answer pair.Given a question-answer pair generated from the given context, Modify the question-answer pair to incorporate a hypothetical or speculative scenario.Please respect the following rules to generate the question:-Answering the new question should encourage applying knowledge from 'Context' to deduce outcomes.-The new question must be fully answerable from 'Context'.-No external knowledge should be required to answer the new question -The question should not be dependent on external things such as figures or tables -Do not use phrases like 'based on the provided context.'The user will provide the original question, context, and some retrieved contexts separated by '-------' as the input.Use details from retrieved context only if they are relevant to your question.You must output a single JSON object in the following format: The same prompts with modified output format were used to add complexities to the MCQs as well.A.5 ANNOTATIONA.5.1 EXPERT VALIDATIONDuring the validation process of the ClimaQA-Gold dataset for both multiple-choice and freeform questions, experts were asked to select predefined reasons for rejecting a generated QA pair.They also had the option to provide a custom reason for rejection.The figure below illustrates the various types of errors made by the ClimaGen pipeline during the generation phase.While most rejection reasons highlight limitations of the generator LLM, the category of bad context points to instances where the seed context used for QA generation was inherently flawed or lacked meaningful information.Addressing this issue through improved preprocessing techniques is a key area for future work.Published as a conference paper at ICLR 2025A.5.3 AUTOMATED ANNOTATION -CLOZETo automate cloze annotation, we fine-tuned gpt-4o-mini with the following prompt to pick the scientific term to be masked.The dataset consisted of a total of 160 expert-labeled examples.You are a climate cloze generator that marks a scientific term from the given scientific statement to be masked for cloze question-answering The scientific term has to be a single word from the given statement that has a significant impact if removed You will be provided the following as the input:Statement: ⟨statement⟩Respond with just one word
An automated multiple-choice question generation using natural language processing techniques. A Chidinma, Ikechukwu E Nwafor, Onyenwe, 10.5121/ijnlc.2021.10201International Journal on Natural Language Computing. 2319-41111002April 2021</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Automatic generation of multiple choice questions using dependency-based semantic relations. Naveed Afzal, Ruslan Mitkov, 10.1007/s00500-013-1141-4Soft Computing. 1872014</p>
<p>The sciqa scientific question answering benchmark for scholarly knowledge. Sören Auer, Dante A C Barone, Cassiano Bartz, Eduardo G Cortes, Mohamad Yaser Jaradeh, Oliver Karras, Manolis Koubarakis, Dmitry Mouromtsev, Dmitrii Pliukhin, Daniil Radyush, Ivan Shilin, Markus Stocker, Eleni Tsalapati, 10.1038/s41598-023-33607-zScientific Reports. 1312023</p>
<p>More than reading comprehension: A survey on datasets and metrics of textual question answering. Yang Bai, Daisy Zhe, Wang , arXiv:2109.122642021arXiv preprint</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Aerosol Measurement: Principles, Techniques, and Applications. A Wiley-Interscience publication. P A Baron, K Willeke, 2001Wiley</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Assessing large language models on climate information. Jannis Bulian, Mike S Schäfer, Afra Amini, Heidi Lam, Massimiliano Ciaramita, Ben Gaiarin, Michelle Chen Hübscher, Christian Buck, Niels G Mede, Markus Leippold, Nadine Strauß, 2024</p>
<p>LLM-assisted modeling and simulations for public sector decision-making: Bridging climate data and policy insights. Charles Cao, Jie Zhuang, Qiang He, AAAI-2024 Workshop on Public Sector LLMs: Algorithmic and Sociotechnical Design. 2024</p>
<p>Aerosols and Climate. K S Carslaw, 2022Elsevier</p>
<p>Universal sentence encoder. Cer, arXiv:1803.111752018arXiv preprint</p>
<p>Santanu Phadikar, and Arif Ahmed Sekh. Multiple-choice question generation with auto-generated distractors for computer-assisted educational assessment. Bidyut Das, Mukta Majumder, Multimedia Tools and Applications. 202180</p>
<p>The forests of the congo basin-forests and climate change. Carlos De Wasseige, Martin Tadoum, Eba ' Atyi, Charles Doumenge, 2015</p>
<p>Jacob Devlin, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprint</p>
<p>Climate-fever: A dataset for verification of real-world climate claims. Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, Markus Leippold, arXiv:2012.006142020arXiv preprint</p>
<p>A comparative study of ai-generated (gpt-4) and human-crafted mcqs in programming education. Jacob Doughty, Zipiao Wan, Anishka Bompelli, Jubahed Qayum, Taozhi Wang, Juran Zhang, Yujia Zheng, Aidan Doyle, Pragnya Sridhar, Arav Agarwal, Proceedings of the 26th Australasian Computing Education Conference. the 26th Australasian Computing Education Conference2024</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Calculus of Variations. I M Gelfand, S V Fomin, 2012Dover Publications</p>
<p>Docimological quality analysis of llm-generated multiple choice questions in computer science and medicine. Christian Grévisse, Maria Angeliki S Pavlou, Jochen G Schneider, SN Computer Science. 556362024</p>
<p>Mcqgen: A large language model-driven mcq generator for personalized learning. Ching Nam, Hang , Chee Wei Tan, Pei-Duo Yu, IEEE Access. 2024</p>
<p>. D L Hartmann, Global Physical Climatology. International Geophysics. Elsevier Science. 2015</p>
<p>Clouds in the Perturbed Climate System: Their Relationship to Energy Balance, Atmospheric Dynamics, and Precipitation. J Heintzenberg, R J Charlson, Strüngmann Forum reports. Mass.. 2009</p>
<p>Lora: Low-rank adaptation of large language models. J Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen, ArXiv, abs/2106.096852021</p>
<p>Supervised word mover's distance. Gao Huang, Chuan Guo, Matt J Kusner, Yu Sun, Fei Sha, Kilian Q Weinberger, Advances in neural information processing systems. 292016</p>
<p>Stochastic Climate Models. P Imkeller, J S Storch, Progress in Probability. Birkhäuser Basel. 2012</p>
<p>Fundamentals of Atmospheric Modeling. Fundamentals of Atmospheric Modeling. M Z Jacobson, 2005Cambridge University Press</p>
<p>Open research knowledge graph: next generation infrastructure for semantic scholarly knowledge. Mohamad Yaser, Jaradeh , Allard Oelen, Kheir Eddine Farfar, Manuel Prinz, D' Jennifer, Gábor Souza, Markus Kismihók, Sören Stocker, Auer, Proceedings of the 10th international conference on knowledge capture. the 10th international conference on knowledge capture2019</p>
<p>Devendra Singh Chaplot, Diego de las Casas. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>An Introduction to Clouds: From the Microscale to Climate. U Lohmann, F Lüönd, F Mahrt, 2016Cambridge University Press</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Filtering Complex Turbulent Systems. A J Majda, J Harlim, 2012Cambridge University Press</p>
<p>Automated mcq generator using natural language processing. Pritam Kumar Mehta, Prachi Jain, Chetan Makwana, Raut, Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications. the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications2021</p>
<p>Evalquiz-llm-based automated generation of self-assessment quizzes in software engineering education. Niklas Meißner, Sandro Speth, Julian Kieslinger, Steffen Becker, Software Engineering im Unterricht der Hochschulen 2024. 2024Gesellschaft für Informatik eV</p>
<p>Wordnet: a lexical database for english. George A Miller, Communications of the ACM. 38111995</p>
<p>My climate advisor: An application of NLP in climate adaptation for agriculture. Vincent Nguyen, Sarvnaz Karimi, Willow Hallgren, Ashley Harkin, Mahesh Prakash, ; Jingwei Ni, Tobias Schimanski, Kalyan Dutia, Alok Singh, Julia Bingler, Christophe Christiaen, Neetu Kushwaha, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (Cli-mateNLP 2024). Saeid A Veruska Muccione, Markus Vaghefi, Leippold, the 1st Workshop on Natural Language Processing Meets Climate Change (Cli-mateNLP 2024)Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024Dominik Stammbach,</p>
<p>Generating multiple choice questions from a textbook: Llms match human performance on most metrics. Andrew M Olney, LLM@ AIED. 2023</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Simulating Nature: A Philosophical Study of Computer-Simulation Uncertainties and Their Role in Climate Science and Policy Advice. A C Petersen, 2012CRC PressSecond Edition</p>
<p>Principles of Planetary Climate. R T Pierrehumbert, 2010Cambridge University Press</p>
<p>Benchmarks for Pirá 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change. Data Intelligence. Paulo Pirozelli, Marcos M José, Igor Silveira, Flávio Nakasato, Sarajane M Peres, A F Anarosa, Anna H R Brandão, Fabio G Costa, Cozman, 10.1162/dint_a_002456</p>
<p>Reimers, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>Automatic keyword extraction from individual documents. Text mining: applications and theory. Stuart Rose, Dave Engel, Nick Cramer, Wendy Cowley, 2010</p>
<p>Are machines better at complex reasoning? unveiling human-machine inference gaps in entailment verification. Soumya Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya Wang, Xiang Ren, 2024</p>
<p>Eloquent Science: A Practical Guide to Becoming a Better Writer, Speaker, and Atmospheric Scientist. SpringerLink : Bücher. D Schultz, 2013American Meteorological Society</p>
<p>Atmospheric Chemistry and Physics: From Air Pollution to Climate Change. J H Seinfeld, S N Pandis, 2016Wiley</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.055302024aarXiv preprint</p>
<p>Gemma 2: Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.001182024barXiv preprint</p>
<p>Climategpt: Towards ai synthesizing interdisciplinary research on climate change. David Thulke, Yingbo Gao, Petrus Pelser, Rein Brune, Rricha Jalota, Floris Fok, Michael Ramos, Ian Van Wyk, Abdallah Nasir, Hayden Goldstein, Taylor Tragemann, Katie Nguyen, Ariana Fowler, Andrew Stanco, Jon Gabriel, Jordan Taylor, Dean Moro, Evgenii Tsymbalov, Juliette De Waal, Evgeny Matusov, Mudar Yaghi, Mohammad Shihadah, Hermann Ney, Christian Dugast, Jonathan Dotan, Daniel Erasmus, 2024</p>
<p>Airborne CCN measurements. James Trembath, 2013The University of Manchester (United Kingdom</p>
<p>Atmospheric Science: An Introductory Survey. International Geophysics. M Wallace, P V Hobbs, 2006Academic Press</p>
<p>Sciqag: A framework for auto-generated science question answering dataset with fine-grained evaluation. Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, Ian Foster, 2024</p>
<p>Geostatistics for Environmental Scientists. R Webster, M A Oliver, Statistics in Practice. 2007Wiley</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Statistical Methods in the Atmospheric Sciences. D S Wilks, 2019Elsevier</p>
<p>Star: Constraint lora with dynamic active learning for data-efficient fine-tuning of large language models. Linhai Zhang, Jialong Wu, Deyu Zhou, Guoqiang Xu, arXiv:2403.011652024arXiv preprint</p>
<p>Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, Lei Zou, arXiv:2310.19596Llmaaa: Making large language models as active annotators. 2023arXiv preprint</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, Bertscore, arXiv:1904.09675Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Climate change from large language models. Hongyin Zhu, Prayag Tiwari, Similarity: 0.885Reference Answer: SO2 gpt-3.5-turbo: Sulfur, Phrase Similarity: 0.819 gpt-4o: sulfur dioxide, Phrase Similarity: 0.885 llama3-70b: Sulfur, Phrase Similarity: 0.819 mixtral-8x22b: sulfur dioxide. 2024Phrase Similarity: 0.885 gemma-27b: sulfur dioxide</p>
<p>Question: The rate of change in the air parcel's moist static energy due to ⟨mask⟩ is determined by the difference between the moist static energies of the environment and the cloud air parcel. Reference Answer: entrainment gpt-3.5-turbo: evaporation, Phrase Similarity: 0.681 gpt-4o: entrainment, Phrase Similarity: 1.000 llama3-70b: entrainment, Phrase Similarity: 1.000 mixtral-8x22b: mixing. Phrase Similarity: 0.726 gemma-27b: entrainment, Phrase Similarity: 1.000</p>            </div>
        </div>

    </div>
</body>
</html>
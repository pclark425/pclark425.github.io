<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3720 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3720</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3720</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-b5aad38122c14f4287c5feca20b7017fda50d7c0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b5aad38122c14f4287c5feca20b7017fda50d7c0" target="_blank">Hybrid forecasting of geopolitical eventsâ€ </a></p>
                <p><strong>Paper Venue:</strong> The AI Magazine</p>
                <p><strong>Paper TL;DR:</strong> It is found that skilled forecasters who had access to machine-generated forecasts outperformed those who only viewed historical data and suggests that hybrid forecasting systems, which potentially require fewer human resources, can be a viable approach for maintaining a competitive level of accuracy over a larger number of forecasting questions.</p>
                <p><strong>Paper Abstract:</strong> Sound decision-making relies on accurate prediction for tangible outcomes ranging from military conflict to disease outbreaks. To improve crowdsourced forecasting accuracy, we developed SAGE, a hybrid forecasting system that combines human and machine generated forecasts. The system provides a platform where users can interact with machine models and thus anchor their judgments on an objective benchmark. The system also aggregates human and machine forecasts weighting both for propinquity and based on assessed skill while adjusting for overconfidence. We present results from the Hybrid Forecasting Competition (HFC) - larger than comparable forecasting tournaments - including 1085 users forecasting 398 real-world forecasting problems over eight months. Our main result is that the hybrid system generated more accurate forecasts compared to a human-only baseline which had no machine generated predictions. We found that skilled forecasters who had access to machine-generated forecasts outperformed those who only viewed historical data. We also demonstrated the inclusion of machine-generated forecasts in our aggregation algorithms improved performance, both in terms of accuracy and scalability. This suggests that hybrid forecasting systems, which potentially require fewer human resources, can be a viable approach for maintaining a competitive level of accuracy over a larger number of forecasting questions.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ForecastQA: A question answering challenge for event forecasting with temporal text data <em>(Rating: 2)</em></li>
                <li>Identifying geopolitical event precursors using attention-based lstms <em>(Rating: 2)</em></li>
                <li>BERT: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>Anchor attention for hybrid crowd forecasts aggregation <em>(Rating: 1)</em></li>
                <li>Quantifying machine influence over human forecasters <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3720",
    "paper_id": "paper-b5aad38122c14f4287c5feca20b7017fda50d7c0",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ForecastQA: A question answering challenge for event forecasting with temporal text data",
            "rating": 2
        },
        {
            "paper_title": "Identifying geopolitical event precursors using attention-based lstms",
            "rating": 2
        },
        {
            "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "Anchor attention for hybrid crowd forecasts aggregation",
            "rating": 1
        },
        {
            "paper_title": "Quantifying machine influence over human forecasters",
            "rating": 1
        }
    ],
    "cost": 0.007119749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Hybrid Forecasting of Geopolitical Events*</h1>
<p>Daniel M. Benjamin ${ }^{1,2}$ and Fred Morstatter ${ }^{1}$ Ali E. Abbas ${ }^{3}$ Andres Abeliuk ${ }^{1,4}$ Pavel Atanasov ${ }^{5}$ Stephen Bennett ${ }^{6}$ Andreas Beger ${ }^{7}$ Saurabh Birari ${ }^{1}$ David V. Budescu ${ }^{8}$ Michele Catasta ${ }^{19}$ Emilio Ferrara ${ }^{1}$ Lucas Haravitch ${ }^{3}$ Mark Himmelstein ${ }^{8}$ KSM Tozammel Hossain ${ }^{\ddagger 1}$ Yuzhong Huang ${ }^{1}$ Woojeong Jin ${ }^{1}$ Regina Joseph ${ }^{10}$ Jure Leskovec ${ }^{9}$ Akira Matsui ${ }^{\S 1}$ Mehrnoosh Mirtaheri ${ }^{1}$ Xiang Ren ${ }^{1}$ Gleb Satyukov ${ }^{1}$ Rajiv Sethi ${ }^{11}$ Amandeep Singh ${ }^{1}$ Rok Sosic ${ }^{9}$ Mark Steyvers ${ }^{6}$ Pedro A Szekely ${ }^{1}$ Michael D. Ward ${ }^{7}$ Aram Galstyan ${ }^{\text {T }}$ 1<br>${ }^{1}$ USC Information Sciences Institute, Marina del Rey, CA<br>${ }^{2}$ Nova Southeastern University, Fort Lauderdale, FL<br>${ }^{3}$ University of Southern California, Los Angeles, CA<br>${ }^{4}$ University of Chile, Santiago, Chile; National Center for Artificial Intelligence (CENIA), Chile<br>${ }^{5}$ Pytho, LLC, New York, NY<br>${ }^{6}$ University of California Irvine, Irvine, CA<br>${ }^{7}$ Predictive Heuristics, Seattle, WA, USA<br>${ }^{8}$ Fordham University, Bronx, NY<br>${ }^{9}$ Stanford University, Stanford, CA<br>${ }^{10}$ Sibylink, New York, NY<br>${ }^{11}$ Barnard College, Columbia University, New York, NY</p>
<p>February 18, 2023</p>
<h4>Abstract</h4>
<p>Sound decision-making relies on accurate prediction for tangible outcomes ranging from military conflict to disease outbreaks. To improve crowdsourced forecasting accuracy, we developed SAGE, a hybrid forecasting system that combines human and machine generated forecasts. The system provides a platform where users can interact with machine models and thus anchor their judgments on an objective benchmark. The system also aggregates human and machine forecasts weighting both for propinquity and based on assessed skill while adjusting for overconfidence. We present results from the Hybrid Forecasting Competition (HFC) - larger than comparable forecasting tournaments - including 1085 users forecasting 398 real-world forecasting problems over eight months. Our main result is that the hybrid system generated more accurate forecasts compared to a human-only baseline which had no machine generated predictions. We found that skilled forecasters who had access to machine-generated forecasts outperformed those who only viewed historical data. We also demonstrated the inclusion of machinegenerated forecasts in our aggregation algorithms improved performance, both in terms of accuracy and scalability. This suggests that hybrid forecasting systems, which potentially require fewer human resources, can be a viable approach for maintaining a competitive level of accuracy over a larger number of forecasting questions.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>From military conflicts to disease outbreak to economic disruption, accurate prediction is vital for sound intelligence-based decision-making. However, the problem of making accurate predictions for geopolitical events is notoriously difficult due to too much or too little data, rare event occurrences, or large levels of uncertainty. Prediction methods range from expert and/or group judgment to individual and ensembled statistical models [58]. It is often challenging to identify a consistent, superior prediction method [39, 58]. Stakeholders confidently misidentify the benefits of competing methods, such as trusting human clinical judgment over statistical or algorithmic judgment [14]. Two common forecasting methods, crowdsourcing and machine learning, have complementary strengths and competing weaknesses. Here we present a hybrid forecasting model - a system that aims to exploit the proficiencies of each while circumventing their deficiencies.</p>
<p>Recent forecasting tournaments such as IARPA's Aggregative Contingent Estimation (ACE) [1] have led to advances in crowdsourcing methods, statistical aggregation, and ultimately improvements in accuracy [5, 40]. Crowdsourced aggregation pools a breadth of knowledge while canceling independent errors [11] and is most successful when individual performance can be tracked over time. However, social influences can harm opinion pools, and individual (rewards) vs. group incentives can be difficult to balance. Individuals must choose to share their private information and trust others.</p>
<p>Advances in statistical and machine learning methods lead to accuracy gains due to their ability to handle troves of data with heterogeneous input and identify complex relationships [23]. Data-driven, algorithmic forecasting can be used to predict various political outcomes, such as terrorism, conflict, insurgency, and similar $[21,48,53,54]$. However, machine learning requires large amounts of data to be available and accessible. If data is not in a standard format, there can be large costs to pre-processing data.</p>
<p>Statistical models perform well under the right circumstances [36], and human crowds succeed when deftly combined [5]. Factors like amount, availability, and structure of data determine how these methods perform [55]. Machine-based forecasting methods typically perform well on problems for which there is sufficient historical data, but are ill-suited to forecast rare or idiosyncratic events for which such data may not exist, or when the underlying context has changed in ways not reflected by the historical data. Machine predictions handle data in a consistent, structured manner and avoid computational errors, like violating probabilitiy axioms [17].</p>
<p>Human analysts, on the other hand, can often accurately forecast outcomes without exclusively depending on availability of historical data, by leveraging their domain knowledge and prior experience. Further, human expertise and domain knowledge can be valuable as inputs into machine models. These benefits are most efficient when data is sparse and/or unstructured [17]. However, even the best analysts may not match machine performance where solid historical data is available and can be cognitively overwhelmed when addressing a large number of problems within time constraints, thereby limiting the scalability of a forecasting system that relies solely on human judgment. Unfortunately, there are few direct comparisons between models and crowds in similar settings.</p>
<p>Here we describe our Synergistic Anticipation of Geopolitical Events (SAGE) system, which was developed under IARPA's Hybrid Forecasting Competition (HFC) program [2]. The system is designed to make verifiable probabilistic predictions of outcomes from a broad set of domains, such as politics and international relations (ie the quantity of battle deaths or piracy in a region or attributable to a specified actor), health and disease (ie flu or dengue fever case counts), economics and finance (ie exchange rates or oil prices), and science and nature (ie the number of earthquakes or cybersecurity breaches) (see section 4.1 for an overview of the types of questions). A human-computer system can achieve "hybrid intelligence" when applied in a setting with a high degree of digitization and human expertise [51]. SAGE is a hybrid forecasting platform that allows human forecasters to combine model-based forecasts with their own judgment. The SAGE system provides forecasters automated statistical predictions and freedom to choose if and how much weight to assign to model predictions when submitting their personal forecasts since formal models can increase the skill of human judges [51].</p>
<p>Our system is designed to test the conceptual hypothesis that machine model forecasts embedded in a crowdsourced forecasting platform can improve the accuracy and efficiency of established crowdsourced forecasting methods. We embed machine models in a system designed to balance a) the diversity required to achieve the "wisdom of the crowd" by not restricting users' responses with b) anchoring forecasters</p>
<p>to an impartial benchmark to minimize noise and outliers. This paper tests how machine models lead to improvements. We experimentally test various informational conditions to determine which type of information - historical data, model output, or interactivity with the models - leads to optimal accuracy and user engagement (see section 3 for details). We also test if machine models improve system efficiency. We hypothesize that machine models can help increase the number of questions SAGE can answer without decreasing accuracy. We test methods of allocating a fixed number of human forecasters to questions where they are most needed.</p>
<p>In what follows, we test if our hybrid system can improve accuracy, engagement, or scalability compared to established crowdsourcing methods. First, we discuss relevant literature related to hybrid intelligence and scalability. Then, we describe the main components of SAGE followed by a description of HFC guidelines. Finally, wepresent our experimental results from a 8-months long Randomized Controlled Trial (RCT) conducted under the HFC program.</p>
<h1>2 Related Works</h1>
<p>The main motivation behind developing a "hybrid" forecasting system is to harness the strengths of crowdsourced and statistical forecasts by combining them with machine learning models as input for both human forecasters and aggregation methods. This type of hybrid intelligence occurs when human and machine components each contribute to a solution that outperforms and/or is more efficient than either source on its own $[17,35]$. Machine models, which excel at identifying patterns from data and leveraging them for making predictions, can help human judges overcome certain errors and inconsistencies. Human experts, which do not require structured input data, are capable of ad hoc feature selection, often quicker than variables can be formalized when data sources are yet unavailable.</p>
<p>While there is a growing field discussing the current state of hybrid intelligence, there is limited work exploring how such systems work and in what settings they excel. To date, most work explicitly discussing hybrid intelligence is theoretical (e.g. [16, 51, 52]). Developing an efficient and effective hybrid system to solve complex, dynamic tasks requires a carefully designed and tested machine component, a skilled human component, and principled, dynamic methods for combining them. In the current study, we address the challenges of balancing effectiveness with flexibility. Artificial intelligence exceeds when tasks are welldefined (e.g. [17]). Machine models can underperform when tasks are loosely defined, data is sparse, or environments are complex and/or changing. A forecasting tournament provides an opportunity to collect data in a structured, yet chaotic, environment. On-one-hand, the general question and response format is consistent and practiced users provide consistent response data. On-the-other-hand, it is a difficult setting to generalize because new question types, sources, and datasets could be introduced after system development.</p>
<p>One key limitation of the previous work on crowdsourced forecasting and hybrid intelligence is that use cases are limited and often applied to a business environment (e.g. [17, 16, 50]). The current study is designed to provide data-driven support for the effectiveness of a hybrid system in the geopolitical forecasting domain. While there are established methods showing how crowdsourced forecasting succeeds, there are not established methods for a hybrid forecasting system [40]. Previous crowdsourced methods rely on adjustments, such as statistical recalibration, to adjust for measurable biases in human forecasters like overconfidence [5]. It remains an open question whether the same or new cognitive biases emerge when human users interact with machine model output. Research suggests that presenting time-series data as a forecasting aid improves individual forecasts by reducing random error [15]. When there are detectable trends in a time series, forecasts made while viewing graphical data are more accurate than from viewing tabular data [24] . In this setting, a hybrid system must account for potential, yet unmeasured, biases to effectively combine machine models and crowd predictions.</p>
<p>The success of machine models is driven by complexity and volatilitiy. When predicting on real data, machine learning models face a tradeoff between the complexity of the data and the number of model parameters required to predict accurately [47]. Hence, tuning and re-turning becomes cumbersome when properties of the event or dataset change. The problem becomes more challenging when predicting several periods into the future and requires methods that produce multiple outcomes [8]. Known (simple) statistical methods often outperform more sophisticated methods (e.g. based on deep neural networks) because real-world timeseries are often non-stationary. Changes from training to testing often impede how well sophisticated models</p>
<p>generalize [38]. It is yet not known whether human judgment can help identify the shifts in time-series over time that make statistical and machine model predictions miss the mark. Further, while the main focus of the machine models considered here is on quantitative, time-series data, there is also an emerging line of work which intends to use unstructured textual data for making predictions. For example, [28] aims to extract possible precursors of certain events from documents, while [33] formulates forecasting as a Question Answering (QA) problem on an appropriately selected textual dataset.</p>
<p>A key aspect in achieving an efficient and effective hybrid system is how to allocate both the human and machine resources. Intelligent task allocation can bring out the best in both sources (e.g. in classification [7]; in consensus [35]. In a review of 208 articles over 50 years, task allocation is identified as one of the key issues to making hybrid system work [32]. It is challenging to allocate tasks When it is not knowable in advance at which tasks machines and humans will outperform each other. As the task becomes more difficult and the system becomes more complex, task allocation becomes more difficult [57]. Introducing machine elements into crowd systems comes with trade-offs. Misallocation can diminish engagement and shift attention away from desired tasks. The "wisdom of the crowd" effect relies on sufficient expertise and diversity of knowledge. In this setting, the introduction of statistical models could diminish diversity if human participants are too trusting in the models and do not feel empowered or motivated to add their private information into the system. Intelligent task allocation must balance finding the best individual sources for a given task with maintaining a diverse pool of knowledge.</p>
<h1>3 Methods</h1>
<h3>3.1 SAGE System</h3>
<p>The Synergistic Anticipation of Geopolitical Events (SAGE) system was developed to combine automated statistical forecasts with a pool of human knowledge by allowing users access to machine model output and by algorithmically combining human and machine forecasts [44]. The SAGE platform allowed users to interact with machine models to anchor their judgments on an objective benchmark. Simultaneously, users had the freedom to choose if and how they combined model forecasts with their own judgment striving for the diversity of knowledge needed for the "wisdom of the crowd" effect [3]. To proactively mitigate skepticism with and over-reliance on the models, we trained users in how to evaluate and consolidate information from multiple sources.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Schematic of SAGE system organized into five topic areas. Platform engineering is in pink, recruitment and retention is in blue, machine-based forecasting is in yellow, human-machine interaction is in green, and diagnostics and feedback is in purple.</p>
<p>The SAGE system was developed by integrating five areas of engineering and design (see Figure 1). After logging both machine and human forecasts, our aggregation algorithms computed aggregate forecasts in real time by dynamically combining human and machine forecasts. Over time, our system determined optimal weights for each source based on assessed skill, adjusting for overconfidence, and for propinquity to question resolution. We also developed a number of machine models allowing the system to choose reliable models for various question types. Finally, our system adaptively filtered questions balancing users' preferences and abilities with the systems' needs. Our recommender system filtered questions to the top that individuals were more likely to answer and/or were unpopular, while hiding questions that were overly popular.</p>
<h1>3.2 Forecasting Platform</h1>
<p>The SAGE system included search and filtering functionality to help users find forecasting questions (IFPs) about which they felt knowledgeable. At any point in time, there were dozens of IFPs available. Users had to complete at least 5 forecasts per week; An example of an IFP is shown in Fig. 2. After choosing which question to answer, an IFP page included the following from top to bottom: question text, resolution criteria (including the source used to resolve a given question, value of interest and/or criteria for an occurrence, and timing), automated information including data graph, statistical forecast, and interactive features (depending on their experimental condition), forecast sliders that forced responses to add to $100 \%$, a textbox to justify the forecast, and a comments thread to view and respond to fellow users' justifications. Additional features included a leaderboard, consensus charts, a research tool, a profile page including their personal accomplishments, and training and tournament information.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Screen capture of an IFP with resolution criteria.</p>
<h3>3.3 Data Pipeline and Model Development</h3>
<p>The SAGE machine model pipeline can be broken down into two parts based on the kinds of questions that were covered. Approximately $45 \%$ of IFPs (AKA data-driven IFPs) were clearly associated with a univariate time series, like OECD interest rates for a country[22]. These questions were covered by automated dataacquisition and univariate time-series forecasting systems. The remaining questions did not have clearly associated data. Some, about election results and country leader resignations, were covered by tailored models that could leverage more complicated, non-time-series data. Others were covered by tools that leveraged resolved answers to other, similar, previous questions, or extracted relevant information from the ICEWS event data [9]. The non-time series models either only covered a very small set of questions, or did not perform well in terms of accuracy, so the rest of this section will focus on the time series forecasting system.</p>
<p>The time series forecasting system consisted of a data platform that maintained a continuously updated database of relevant time series data sets and could map them to questions as appropriate, and a forecasting platform that would then parse a question and apply a univariate time series model to derive probabilities for the question answers. Our system was developed to automate data extraction based on reading the question text and finding the applicable data. Many data sources were known in advance and several were not.</p>
<p>A significant challenge was to identify the time series models to use for generating the forecasts that would be shown to users and sent to the aggregation models. Four core models were displayed to users in the question charts: the auto ARIMA model, a similar automated exponential smoothing model (ETS) [30, 31], a simple random walk model, and the M4-Metalearning model [42]. The DCT Ensemble model, drawing on forecasts from auto ARIMA, M4-Metalearning, or a AR(1) neural net model based on an analysis of the input series discrete cosine transformation was used to provision forecasts for aggregation. Model performance suffered from a "cold-start problem" as the number of IFP resolutions were limited over the first several months of the competition. Further, HFC guidelines required our system to make predictions for new datasets and sources on the fly, often with only a couple hours notice before users could access the IFPs. Therefore, simple time-series models tended to outperform more complex, topic-specific models - a result supported by the general success of conservative forecasting approaches [4, 38]. Initially all forecasts were based on the Auto ARIMA model [30], but later this was supplanted by an ensemble (labelled "PHE2" below) of Auto ARIMA and an exponential smoothing state space model [30], which emerged from an overall pool of 28 candidate models. We do not report results from poor performing models. Choosing adequate models was hard because inter-question performance a is very noisy (variable), yet only relatively small numbers of resolved questions were available for testing and several models did not have adequate information to specify them consistently. Only later did enough resolved questions accumulate for model-to-model performance to stabilize.</p>
<h1>3.4 Experimental Conditions</h1>
<p>We conducted a controlled experiment to better understand the benefits of exposing forecasters to different hybridization components. We randomly assigned our 547 participants to one of three experimental conditions which we labeled B, C, D to reflect the increasing level of complexity of and interactivity with the hybridization model.</p>
<ol>
<li>Condition B: This condition exposed users to historical data about the target item. Data included relevant news articles from the research tool, and historical figures that pertains to the question. Historical charts were available for 177 of the 398 items.</li>
<li>Condition C: This condition supplemented the data charts from Condition B with machine model predictions, when available. More specifically, we exposed the forecasters to predictions from the ARIMA model, which has been determined to be a good general model. ARIMA model predictions were available for 177 of the 398 items.</li>
<li>Condition D: This is a variation on condition C that allows the forecasters to tweak the parameters of the visualization, including the type of model and range of data used for model training. We also provided a simple method that allowed the judges to adjust the model's forecast by selecting the mean and variance of the target value and directly translating that into a forecast ${ }^{1}$.</li>
</ol>
<p>Figure 3 presents examples of screenshots from Conditions B, C and D. Control (see below) and Condition B quantified the ability of human forecasters to predict the various items and provide natural baselines to compare forecasters in Conditions C and D that had access to machine models. The benefit of this access is measured by the improvement in accuracy, compared to the controls. In addition to the three treatment conditions above, there was a control condition that was run separately by the HFC Test and Evaluation Team. This control condition used a different platform with a different sample of 538 respondents selected from the same pool. The control condition did not offer any historical charts nor machine predictions to the participants. The main objective of the program was shown that the hybridized conditions could generate more accurate aggregate forecasts than the control.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Schematic illustration of information presented to participants in each experimental condition.</p>
<h1>3.5 Forecast Aggregation</h1>
<p>By combining human and machine model forecasts, we aimed to leverage the collective intelligence of human and model judgments. The advantage of human judgment was its flexibility and ability to reason with qualitative and mixed-source data. Humans can forecast when data is sparse or difficult to interpret and can seek out information that only indirectly relates to the question at hand. On the other hand, the advantages of models included expeditious forecasting which improved scalability. Statistical models dutifully forecasted on any number of questions and their accuracy tends to improve as more data became available.</p>
<p>The key challenge for aggregation was that many factors related to human and model judgment were not known a priori. For each particular forecasting problem, the total number of human forecasts was not knowable in advance. On any particular day the forecasting problem was available, a handful of human forecasts might be produced but in some extreme cases, no human judgment might be available for the entire duration of the forecasting problem. In addition, at the start of the forecasting project, it was not known what the relative accuracy is of the model and human judgments for certain types of forecasting problems. Every introduction of a new type of forecasting problem injected new uncertainty about the relative capabilities of human and model forecasting accuracy. This cold-start problem made it challenging to apply machinelearning approaches that can learn optimal combinations of human and model judgment as large quantities of human judgments were initially not available and yet accurate forecasts needed to be produced from the start of the project. Therefore, the goal for aggregation was to develop a robust framework for integrating human and model judgment with the potential to scale to large numbers of forecasting questions.</p>
<p>To combine the human forecasts, we employed a combination of tested methods and new strategies to maximize performance. The aggregation of human-only forecasts accounted for three factors: recency, individual skill, and miscalibration. First, our algorithms diminished each forecasts' value over time as new information accumulates. To account for this recency effect, we kept only the most recent $40 \%$ of forecasts for a question at any given time, and further applied exponential decay to down-weight older forecasts included in the aggregation. Second, we placed higher weights on forecasters forecasters with better accuracy track records, those who updated their forecasts in frequent, small increments [6], and those who wrote longer text rationales, with more sources and quantitative information. Finally, we recalibrated forecasts to correct for the general tendency toward overconfidence by individual forecasters, and underconfidence of aggregated crowd judgments, especially when aggregated using the mean. This was done by making forecasts by individual forecasts less extreme, but aggregate-level forecasts more extreme (closer to $0 \%$ or $100 \%$ ). The overall effect was to make final aggregate estimates slightly more extreme than the equivalent estimates with no recalibration. The best-performing slot used a variant of this aggregation model which made a) forecaster weights more unequal over time, and b) extremization parameters larger over time, making season-end aggregated forecasts more extreme than those at season-start.</p>
<p>Human forecasts were then combined with machine model estimates. Each model forecast was also given weights based on the historical performance of the model that generated it. Our initial strategy for humanmachine aggregation was to assess machine weights relative to those of crowd estimates (e.g., a model estimate may be weighted $1 / 4$ as much as the crowd). The more advanced alternative that was used in most slots in the last season (including the best-performing slot), placed weights on model forecasts equal to those of several</p>
<p>average-skill individual human forecasters. Initial human-machine weights were set based on backcasting analyses, and were allowed to vary over time based on relative within-season performance in some slots. As a consequence, the aggregate forecast was heavily weighted toward when few human forecasters had placed estimates on the question, when the human forecasts were out of date, or when most active human forecasters on the question were considered low-skill. We also tested more sophisticated ensemble aggregation methods which used multiple machine models as inputs as well as machine learning-based aggregation which included additional inputs such as the statistical traits of the community forecast, linguistic features of the IFPs and forecast justifications, etc. In a separate paper, we present a neural machine translation aggregation method which assigns anchor attention weights to forecast-user-datetime combinations [29]. We report results from the best performing aggregation method that was run in real-time during the forecast season below, which is based on the method described above.</p>
<h1>3.6 Training</h1>
<p>We sought to understand whether training could improve predictive accuracy under conditions in which forecasters had to balance trust in a model with their own judgment. Our HABIT training method combined probabilistic reasoning with hybridization concepts using a character-based narrative device rendered in a cartoon format. Our training was designed to extend previous vignette-based methods, which focused on core tenets of probabilistic estimation [40], to teach about the machine models involved in the hybridized ensembles and aggregations and how to integrate model forecasts with one's personal knowledge. We hypothesized that the cognitive burden of whether to integrate or reject machine model data could be mitigated by briefly explaining how each model works and how to balance too little and too much trust in models. We tested both whether or not mandating training improved accuracy and whether the presentation format, whether animated or static, led to gains.</p>
<h3>3.7 Matching Participants with Forecasting Problems</h3>
<p>The SAGE system aimed to optimize two seemingly conflicting objectives: 1) allow users choice of questions based on their expertise and interests with 2) timely coverage of all questions with limited human forecasters. We developed an IFP Recommender System which presented a personalized ranking of the IFPs on the Question page, based on the specific characteristics of a forecaster. We develop a recommender system based on the wide and deep learning model [13]. This model identifies preferences using known IFP features, and generalizes to other IFPs via IFP embeddings. As features, we took into account the performance of the given forecaster on similar past IFPs (based on an IFP Semantic similarity model we developed using BERT [18]) and the user activity on the other IFPs (based on a collaborative filtering scheme). When designing the SAGE recommender system, BERT proved to be the more accurate, most efficient model because it captured the subtleties in the differences between IFP texts. To gauge the similarity among IFP texts, we used cosine similarity, a common distance metric used in embedding spaces. We balanced individual with system performance by also capping popular IFPs where consensus was already reached, freeing up human resources to forecast on other IFPs.</p>
<h3>3.8 Data Availability</h3>
<p>SAGE platform data including machine model output and experimental user forecasts, activity, and scores can be found on the Harvard Dataverse [43]. Control user forecasts, question metadata, and resolutions can be found on a separate Harvard Dataverse page [25].</p>
<h2>4 HFC Background and Rules</h2>
<p>IARPA's Hybrid Forecasting Competition (HFC) [2] was a multi-year research program developed to test if and how machine-models could improve upon previous crowd-sourced geopolitical forecasting tournaments such as ACE [1]. As stated in the program announcement, "the goal of HFC was to integrate the strengths of human cognitive and reasoning abilities with those of machine-driven systems to produce maximally accurate forecasts of geopolitical and economic events" [2]. The evaluation of hybrid forecasting system was conducted</p>
<p>via Randomized Controlled Trials (RCT-s). There were two RCT-s during the lifetime of the HFC program. Here we focus our analysis on the second evaluation, referred to as RCT-B, which took place from April to November 2019.</p>
<p>Like all forecasting tournaments, competitors must abide by rules provided by the sponsor and test and evaluation teams. Some rules governed how forecasting questions were developed, to which datasets they were linked, and how and when they would be resolved and scored. Some rules governed the activities of the human users including how they were recruited and assigned to competitor teams. Other rules governed the development and upkeep of machine-model components as well as how the machine models could be combined with human forecasts and when responses must be submitted.</p>
<h1>4.1 Individual Forecasting Problems</h1>
<p>During RCT-B, forecasts were conducted on 398 questions, broadly referred to as Individual Forecasting Problem (IFP). The questions covered a broad set of domains, such as politics and international relation, science, health and disease, microeconomics and finance (see [26] for a detailed description of different IFP types). New IFPs were published on the same day each week. Each IFP was associated with $C$ mutually exclusive and exhaustive outcome events, where $2 \leq \mathrm{C} \leq 5$. Participants submitted their forecasts for a given IFP by entering a probability for each outcome, where the probabilities across all $C$ outcomes were required to total $100 \%$. All IFPs had a start date and an end date during which participants could make forecasts for that questions as often as they liked (see Figure 2 for a screen capture). IFPs ranged from 2 weeks, to the full 8 -month season in duration. IFPs had a mean duration of 87.07 days ( $\mathrm{SD}=55.85$ ). 205 IFPs had only two response options, the remaining 193 had more than 2 possible responses. Of these 193, 154 were ordinal, in that there was a meaningful ordering to the C events, while the remaining 39 were nominal.</p>
<h3>4.2 Participants</h3>
<p>Human participants were recruited via Amazon Mechanical Turk. CloudResearch filtered the participants to ensure a high level of engagement both prior to the start of the forecasting season by only including users with longitudinal study experience, and mid-season by removing users with low quality responses by assessing the content of their justifications [45]. The sample consisted of 547 participants, 229 women ( $42 \%$ ), with a mean age of $36.68(\mathrm{SD}=10.88)$. A forecasting session consisted of weekly Human Intelligence Tasks (HITs), where each forecaster was required to make at least five forecasts. If possible, three of these five were required to be updates of previous forecasts. For each completed HIT, Participants were paid $\$ 20$ per HIT. Participants were permitted to make additional forecasts beyond these five but were not paid for these additional forecasts. Participants were also eligible for accuracy awards if they participated enough. They could earn a portion of a fixed prize pool at the midway and final points. The pool was divided among three prize tiers of $\$ 200, \$ 100$, or $\$ 50$ for observed accuracy - as measured with mean daily Brier scores.</p>
<h3>4.3 Machine Models</h3>
<p>A key component of the HFC was the requirement for systems to produce model-based forecasts. In previous comparisons of human and model forecasts, the latter were generated in a traditional fashion by analysts (e.g. [56]). In contrast, model-based forecasts for the HFC competition had to be generated by an automated system with restrictions on manual interventions into the process. Fixing system issues, i.e. bugs and similar errors, was allowed, but manual model development like deciding what data and model(s) to use for a question, tuning model parameters, etc. was not permitted. Some data sources were introduced mid-season. Sometimes notice about new data sources came only a couple hours prior to the associated IFPs getting published for human responses. Thus, performer teams were required to quickly produce model forecasts to aid users, and there was insufficient time to build specialized models tuned to specific datasets.</p>
<h3>4.4 Response Submissions</h3>
<p>Each team was allotted 40 official and up to an additional 60 experimental slots for submitting forecasts on each IFP. These slots allowed teams to test multiple theoretical ideas as well as fine-tuning the application of</p>
<p>those ideas, such as including inputs or tuning key parameters in systematic ways. A total of ten official slots were locked prohibiting changes by performers, and 30 were unlocked allowing for changing to key model parameters. The experimental slots encouraged testing more novel, higher risk ideas. Performer teams were required to submit one forecast per IFP per submission slot from the day an IFP was originally published to the day it resolved, either on its stated resolution date or due to an event occurrence. For each slot, performer teams had to develop aggregation algorithms that combined the various machine and human inputs and IFP metadata for a single probability forecast.</p>
<h1>4.5 Scoring</h1>
<p>The accuracy of submitted forecasts were measured using Brier scores [10], the squared distance of the forecast from the result, coded as 1 if the event/quantity was realized, and 0 otherwise. We use Brier scores to measure accuracy because they are specifically designed to assess the accuracy of probabilistic information (unlike other metrics, like F1). As a variation of squared-error, Brier scores penalize more egregious errors more severely. In addition to scoring accuracy, a Brier score is also a proper scoring rule meaning it incentivizes responding honestly. Practically, the HFC test and evaluation team chose Brier scores as their primary accuracy metric. Using the same metric allowed us to efficiently track our performance compared to control and the other HFC competitors. Briers scores can be interpreted similarly as mean squared error. A minimal baseline for accuracy is to show improvement over an uninformed judge, who assigns equal probabilities to all C bins (prob $=1 / \mathrm{C}$ ), earns a Brier score of (C-1)/C. Brier scores are also commonly described as improvement over a known comparator. Below we compare to control using Cohen's d, a standardized mean difference, and in some instances display the percent improvement.</p>
<p>We used formulations of the Brier score based on the number of response options and ordinality of the IFP [41]. This Brier score variant ranged from 0 (perfect accuracy) to 2 (worst possible score). The accuracy of each forecasting slot for a given IFP was characterized by the Mean Daily Brier score (MDB), e.g., the Brier score averaged over the active days of that IFP. Usually, the SAGE system submitted daily forecasts for each open IFP. If for whatever reason a forecast was not submitted on any given day (e.g., system outage), the last submitted forecast was carried forward. If a slot did not submit any forecasts at all for a given IFP, a uniform prior was used to calculate the score.</p>
<p>A similar approach was used to score individual forecasters. A forecast for a given user was carried forward until that user chose to revise the forecast. If a forecaster did not place an estimate on the first day of a question, we imputed the median score across all forecasters in a condition for each day an IFP was open prior to the first forecast. A user's score across IFPs was the mean of MDBs or MMDB. To adjust for the difficulty of individual IFPs and aid in interpreting comparisons across conditions, we standardized Brier scores to have a mean of zero and standard deviation of 1 for each IFP-day.</p>
<h2>5 Results</h2>
<h3>5.1 Aggregate Performance</h3>
<p>First, we report our main result that compares the aggregate performance of the SAGE system with the non-hybrid control. As we mentioned above, each method was allocated 40 official and 60 experimental slots for submitting aggregated forecasts. Table 1 summarizes our results for both official and experimental slots across 398 IFPs. The best official SAGE method led to an improvement in mean accuracy of a Cohen's d of 0.126 over control.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Condition</th>
<th style="text-align: center;">Official</th>
<th style="text-align: center;">Experimental</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Best Performing Control</td>
<td style="text-align: center;">0.3398</td>
<td style="text-align: center;">0.3325</td>
</tr>
<tr>
<td style="text-align: center;">Best Performing SAGE</td>
<td style="text-align: center;">0.3065</td>
<td style="text-align: center;">0.3052</td>
</tr>
</tbody>
</table>
<p>Table 1: The Brier scores of the best performing official and experimental methods for both SAGE and the control.</p>
<p>SAGE's best-perfoming aggregation slot had the following properties. First, it used both Control and SAGE human forecaster data as inputs. Second, it applied a time-varying weighted mean human aggregation</p>
<p>algorithm, which made forecaster weights more unequal over time; aggregate forecasters were de-extremized at the start of the season and extremization parameter value increased over time, resulting in light extremization by the end of the season. Third, human and machine-model forecasts were combined using a rule that produced a weight of a machine model forecast as equivalent to eight average-skill human forecasters on time-series questions, and four average-skill human forecasters on other questions that used less sophisticated models.</p>
<p>SAGE outperformed the control condition both for the official and experimental slots. We ran backcasting analyses to estimate the impact that different aspects of our system, including SAGE forecasters, human aggregation, and machine forecasts, contributed to SAGE outperforming the best Control method, a Brier score difference of 0.0333 . Results showed that applying the SAGE human-aggregation algorithm to control human forecasts would have resulted in a Brier score advantage of 0.01 , approximately $31 \%$ of the full difference. In retrospect, the distinguishing feature of the best-performing human aggregation model was that it extremized aggregate forecasts less, especially at the start of the season. Applying this humanaggregation algorithm to the combination of control and SAGE human forecasts resulted in a Brier score advantage for SAGE of 0.032 , approximately $97 \%$ of the full difference. The further addition of machine models at the aggregation stage rounded up the full $100 \%$ advantage. For more details on the accuracy benefits of machine models at the user interface vs. aggregation stage (see Section 5.4).</p>
<h1>5.2 Individual Performance Across Conditions</h1>
<p>We analyzed user performance in the various conditions across the 398 resolved questions. Table 2 lists the volume of users and forecasts in each condition.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Condition</th>
<th style="text-align: center;">Users</th>
<th style="text-align: center;">Forecasts</th>
<th style="text-align: center;">Forecasts/User</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">B</td>
<td style="text-align: center;">190</td>
<td style="text-align: center;">25,163</td>
<td style="text-align: center;">132.4</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">158</td>
<td style="text-align: center;">20,782</td>
<td style="text-align: center;">131.5</td>
</tr>
<tr>
<td style="text-align: center;">D</td>
<td style="text-align: center;">199</td>
<td style="text-align: center;">27,348</td>
<td style="text-align: center;">137.4</td>
</tr>
<tr>
<td style="text-align: center;">Total (SAGE)</td>
<td style="text-align: center;">547</td>
<td style="text-align: center;">73,293</td>
<td style="text-align: center;">134.0</td>
</tr>
<tr>
<td style="text-align: center;">A (Control)</td>
<td style="text-align: center;">538</td>
<td style="text-align: center;">79,611</td>
<td style="text-align: center;">148.0</td>
</tr>
</tbody>
</table>
<p>Table 2: Number of unique forecasters and generated forecasts in each experimental condition.
Since some questions were relatively easy and highly predictable and others were more difficult, we expected them to yield (possibly, very) different Brier scores. Thus, whenever comparing, or aggregating, Brier scores across multiple items, it was important to adjust for inherent imbalance in difficulty. Our approach to this problem was to standardize the Brier scores for every question to have a mean of 0 and a SD of 1 , across all the responses in all conditions, before combining them. Thus, we report results in terms of mean, median, and 25th percentile standardized Brier scores. The lower (and more negative) a score is, the more accurate it is. The results of all conditions are presented in Table 3.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Condition</th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">Std. Dev.</th>
<th style="text-align: center;">25th Percentile</th>
<th style="text-align: center;">Median</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">A (Control)</td>
<td style="text-align: center;">$\mathbf{- 0 . 0 2 1}$</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">-0.486</td>
<td style="text-align: center;">$\mathbf{- 0 . 2 5 9}$</td>
</tr>
<tr>
<td style="text-align: center;">B (Data)</td>
<td style="text-align: center;">0.051</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">-0.472</td>
<td style="text-align: center;">-0.156</td>
</tr>
<tr>
<td style="text-align: center;">C (Models)</td>
<td style="text-align: center;">0.039</td>
<td style="text-align: center;">1.143</td>
<td style="text-align: center;">$\mathbf{- 0 . 5 3 7}$</td>
<td style="text-align: center;">-0.198</td>
</tr>
<tr>
<td style="text-align: center;">D (Interactive)</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">-0.508</td>
<td style="text-align: center;">-0.196</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of standardized (at IFP level) Brier scores across conditions including mean, median, and $25 \%$ percentile for each. Bolded values represent the lowest (most accurate) score within each column.</p>
<p>Our results indicated that users in conditions C and D outperform those in condition B , but only the most skilled forecasters outperformed the control (no data) condition. We confirmed the hypothesis that having access to model predictions indeed helps skilled, but not average, forecasters. The greatest improvement came when data charts were available, and skilled forecasters viewed model predictions, z-Brier $=-0.618$ vs. -0.545 for control. Note that the availability of more models and interactive features, provided in condition D, did not necessarily help with performance. Indeed, while condition D had a better mean score across all</p>
<p>the questions, users in this condition did not perform well on questions where data charts were available. This suggests that the availability of multiple models and/or interactive features do not help the users to generate more accurate forecasts. In fact, users used the various options available to them very rarely.</p>
<p>Here we present overall performance of our system and experimental conditions. For an analysis of individual conditions, see [27]. We only find consistent differences by major IFP format, but not by traits like topic area, region, or question duration. Our model was most accurate for binary questions, zBrier $=$ -0.107 (SD = 1.10), and least accurate for non-ordinal IFPs, zBrier = 0.180 (SD = 0.97). As discussed in Section 3.5, we did include certain IFP and linguistic traits when testing more complex aggregation methods, but in most instances, they underperformed our interpretable aggregation methods except in the anchor attention model [29].</p>
<h3>5.3 Model-Based Forecasts</h3>
<p>We also analyzed the performance of the machine models outlined in Section 3.3. Overall, simple ensemble models worked well. The best performing model (â€œPHE2â€) was an ensemble that averaged the forecasts from Auto ARIMA and an exponential smoothing state-space model (ETS) [30]. It slightly outperformed the M4-Meta model [42] that ranked 2nd highest in the M4 time series forecasting competition, and clearly outperformed more complex methods like a recurrent neural network and custom-coded regularized autoregressive model. Even the Auto ARIMA model itself did reasonably well throughout. From a practical standpoint, the simpler ensembles were computationally less expensive, had fewer software dependencies, and were less likely to break.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Relative performance of two model-based forecasts compared to average human performance and the best human forecast-only aggregation model. Auto ARIMA was a mainstay model throughout; PHE2 emerged later as a top performer. This figure includes performance on 153 IFPs for which all models had forecasts. Red points mark IFPs with known quality issues that were retained for the sake of coverage.</p>
<p>Model forecasts relative to the human forecasts were overall near or at parity with human forecasts. Figure 4 shows the distributions of mean daily Brier scores for the Auto ARIMA and PHE2 models, as well as a simple average of human forecasts and the best-performing aggregation model of human forecasters from condition B, which were not exposed to machine model predictions. Both models outperformed the average human forecast but lagged slightly behind the best aggregation model of human-only forecasts. In part, this is because they had a small number of very bad forecasts. Some of these were caused by IFPs with known data quality issues, which tended to lead to extreme forecasts with either very low or very high Brier scores.</p>
<p>Some of these - the more easily identifiable ones - are marked with the red points. Lastly, despite similar average performance, the model and human forecasts did well or poorly on different questions. For example, the inter-question correlation of performance for the PHE2 and aggregation benchmark models was only 0.3 .</p>
<p>After enough IFP results were observed, we developed a meta-model for the relative performance of the time-series model and human forecasts. Overall, there were no clear bivariate or multivariate relationships between a large variety of question and data features and the relative model to human forecast performance, which for example could have led us to identify a subset of IFPs in which one consistently outperformed the other. However, towards the end of our experiment, enough performance data had accumulated so that a random forest model trying to identify forecasts that were clearly worse than a uniform forecast, or forecasts that can beat the aggregation benchmark, achieved slightly informative accuracy levels, with out-of-sample AUC-ROC values of 0.69 and 0.60 respectively. This may have been sufficient to implement a filter for likely bad forecasts, if the experiment continued.</p>
<h1>5.4 Using Machine Models for Scalable Forecasting</h1>
<p>We analyzed aggregate performance on the IFPs for which our strongest machine model was available, a discrete-cosine transform (DCT) ensemble. We found incorporating machine models during aggregation led to improvements at several stages which accounted for our team's overall advantage. Although these accuracy gains were consistent throughout the competition, effects at individual stages were modest and none were statistically significant on their own. The results showed that providing forecasters access to model projections led to modest improvements in aggregate accuracy. Forecasters who could view model forecasts before making their estimates produced aggregate forecasts with $6 \%$ better Brier scores, compared to aggregations of forecasters with no access to model projections. Injecting model estimates at the aggregation stage also led to small improvements in accuracy (i.e., reductions in Brier score) of $2 \%-3 \%$ points.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Average aggregate performance (Brier score) as a function of the proportion of human forecasts removed from the forecasting pool (Sparsity). Higher Brier scores correspond to worse aggregate accuracy. Each point corresponds to aggregate performance for a random subset of censored forecasts. The line plots the linear regression of these points and the shaded region is the $95 \%$ confidence interval based on $\mathrm{N}=20$ simulations.</p>
<p>The benefits of including model forecasts in the aggregate became more salient when considering the issue of scale. Scaling up the number of questions in a human-only pool means fewer human forecasts for each question, which is expected to degrade aggregate performance. To simulate this effect, we made human forecasts sparser by deleting a random subset of users from the aggregate. The results demonstrated that including model forecasts insulated the aggregate forecast against the negative effects of sparse human judgments that would occur when scaling to large numbers of questions (see Figure 5).</p>
<h1>5.5 Impact of Training</h1>
<p>Participants were randomly assigned to either a brief (about 30 minute) training or a control condition in which they read popular articles about forecasting, but without tips for boosting accuracy. Training occurred once, during their second active week, and was accessible for review on the platform menu. We assessed accuracy and activity to see if trained forecasters worked harder than untrained users. We first compared forecast accuracy before and after training exposure to ensure trained forecasters were not randomly better from the start. We found trained forecasters outperformed control forecasters post-exposure ( $\mathrm{d}=0.56$ ). We further assessed whether accuracy could be improved via the delivery method of the training material. Forecasters who saw animated material significantly outperformed forecasters who saw static material in average accuracy $(\mathrm{t}(410)=3.55, \mathrm{p}&lt;.001)$, generated slightly more forecasts per IFP than the static group, $(\mathrm{d}=0.20, \mathrm{t}(342)=1.98, \mathrm{p}=.049)$, and attempted approximately $5 \%$ more IFPs than static-trained counterparts, a significant difference $(\mathrm{d}=0.23, \mathrm{t}(342)=1.98, \mathrm{p}=.018)$. More details can be found in [34].</p>
<h3>5.6 IFP Recommendation</h3>
<p>We measured the performance benefit of users assigned to questions ordered using the IFP Recommender System, described in Section 3.7, vs. the global ranking based on resolution date and popularity (SWIFT ordering). As shown in Figure 6, we obtained a statistically significant improvement, up to a $7 \%$ relative decrease in Brier score by the end of the experimental phase.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The IFP Recommender System learns over time the skills and preferences of each forecaster. After 2 months into the experiment, the forecasters who received the recommendations start to consistently outperform the forecasters in the control condition (SWIFT), with a relative improvement that stabilizes around $7 \%$.</p>
<p>We further assessed the IFP Recommender System in terms of effective resource allocation by simulating different allocation strategies. First, since we could not foresee who the best performers would be before the phase ends, we implemented a greedy approach to improve our cohort of users by periodically excluding a certain percentage of worst performers every time a batch of IFP closed (i.e. got resolved) (termed GreedyIFP). Second, we additionally capped the number of forecasts on popular IFPs to reallocate forecasts</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Brier Score</th>
<th style="text-align: center;">Budget (\% of forecasts)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">All forecasts</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Random forecasts</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">62</td>
</tr>
<tr>
<td style="text-align: center;">GreedyIFP</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">61.4</td>
</tr>
<tr>
<td style="text-align: center;">GreedyIFP++</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">37.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Brier score of different IFP recommender systems. All results are averaged over 10 runs.
of diminishing return after consensus was reached (termed GreedyIFP++). As shown in Table 4, both greedy strategies obtained a small improvement in the global Brier score, while reducing the forecasting budget respectively by $38 \%$ and $62 \%$. In contrast, we showed naively decreasing the number of forecasts ("Random forecasts") negatively affected the global Brier Score.</p>
<h1>6 Discussion</h1>
<p>In the above, we show how a hybrid forecasting system can outperform established crowd-sourced forecasting systems. The SAGE hybrid system consistently outperforms the human-only control condition. Our hybrid system improves the accuracy in the aggregate, although improvements were modest. Beyond the proven methods for aggregating human forecasts [5], we find the inclusion of machine models in the user interface and the aggregation algorithms was key to improving accuracy. Individually, access to model predictions only improves the accuracy of highly skilled forecasters. While this is evidence of their value, it provides further evidence that forecasters must have enough expertise to know when and how to use this information [3]. Predicting the future is difficult, especially for deeply uncertain, impactful geopolitical events. Humans and machines are both limited by the irreducible uncertainty of the setting. Combining human and machine predictions leads to gains in accuracy by helping protect against some of the most egregious errors, especially when the two sources disagree.</p>
<p>In addition to better forecasting accuracy, another critical advantage of SAGE over crowdsourcing-based systems is its scalability. We find evidence that the SAGE hybrid system helps answer more questions with the same number of human users without losing accuracy, although the scope of these improvements remains an open hypothesis. Adaptive question-user assignment increases the ability to scale by limiting the number of users who can access each IFP once consensus is achieved. Our recommender system succeeds based on three primary features. First, it provides a unique question ranking for each user based on the IFPs they previously chose to answer. Second, it excludes users who tend to forecast early and perform poorly. Third, it identifies an optimized number of users per IFP and capped each IFP that exceeded that maximum, thus shifting forecasts from popular to unpopular IFPs and increasing the utility of post-consensus forecasts.</p>
<p>Our results are subject to a number limitations, most of which are a function of participating in a forecasting tournament managed by a third-party test and evaluation team. Notably, our machine models needed to be robust and flexible since new datasets and question formats were regularly published unannounced. Our system was successfully able to ingest large amounts of, often unformatted, data in the window between question publication and user recruitment - often just a few hours. Thus, our results might not generalize to more stable situations with highly developed models tuned to a specific environment or dataset.</p>
<p>We highlight two keys lessons about the model contribution to the "hybrid" system. First, aim for depth, not breadth. Our initial strategy, in the spirit of the hybrid part of the competition, was to try to cover as many IFPs as possible. This led to several decisions to use marginal or non-canonical data. The quality issues tended to lead to extreme forecasts that sometimes were really good (achieved a low Brier score), but more often were really bad and thus reduced average quality. A better strategy would have been to focus on a smaller subset of questions where good performance can be achieved, and spend more time on quality control rather than coverage. Better average quality also simplifies downstream use of forecasts in aggregation.</p>
<p>Second, data is king (or more familiarly, "garbage in, garbage out"). The main cause of poor model forecasts was data quality issues. Some sources, like the OECD [22] and OPEC [46], alter historic data values when updating. There were also many questions that required data transformations or had marginal data ill-suited for time-series. For example, count series based on transformed ACLED event data [37] were</p>
<p>plagued by inaccuracies due to idiosyncratic technical issues in the data platform back-end. Unlike errors at the modeling stage, these kinds of data problems usually were hard to identify without labor-intensive manual reconstruction of a series from its source. Selecting good-enough time series models, which had been the focus of our efforts in the beginning, in the end turned to be easier than these two issues.</p>
<p>Similarly, the success of our simpler, interpretable aggregation methods is most likely due to model training. The greater complexity of the aggregation method, the more training data it required. Complex aggregation methods suffered from training during burn-in due to limitations in the number of IFPs that resolved earlier in the season and variability between sources in format, frequency, and availability of data. Such methods suffered more inefficiency due to retraining mid-season due to the requirement that our system be able to handle newly introduced data sources on the fly. In complex settings, simpler, traditional statistical methods often outperform novel, complex methods [38]. Research on forecast combinations supports the success of simple conservative methods [4].</p>
<p>The recruitment of human forecasters for such long-term engagement is also innately challenging. Recruitment was managed by a third party according to HFC rules. This system prioritized retention over accuracy incentives, and HFC rules limited our ability to add performance-based incentives beyond those offered by the recruitment team. Changing the retention-accuracy incentive balance is likely to alter the quality of human performance.</p>
<p>In conclusion, gains from hybridizing are consistent, but modest in this setting. The SAGE system's success relies on both computer-in-the-loop hybridization including the information (historical data and model predictions) shown to users and mandated narrative graphical training, as well as human-in-the-loop hybridization including human inputs into the aggregation algorithms and strategic user-IFP assignment, to name a few. It is important to engineer such a complex system to optimize the interactions between each component, since each improves accuracy slightly. The optimal system must balance several tradeoffs, like using models that are no more complex than the tuning parameters that can be confidently estimated, and anchoring users on objective, data-driven benchmarks while eliciting the diversity required for crowd wisdom. The real advantage is not boundless improvements in accuracy. Instead it is the ability to tackle a greater burden without needing to increase human resources. When human-question balance is sparse, it is important to view users as a labor pool and use adaptive question assignment to maximize human coverage.</p>
<h1>Acknowledgments</h1>
<p>The authors would like to thank Seth Goldstein, Peter Haglich, Rob Hartman, Daniel Horn, and Steven Rieber for their helpful feedback during the HFC program. This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via 2017-17071900005. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p>
<h2>References</h2>
<p>[1] IARPA - ACE, 2011.
[2] IARPA - Hybrid Forecasting Competition, 2017.
[3] AndrÃ©s Abeliuk, Daniel M. Benjamin, Fred Morstatter, and Aram Galstyan. Quantifying machine influence over human forecasters. Scientific Reports, 10(1):15940, 2020. Number: 1 Publisher: Nature Publishing Group.
[4] J. Scott Armstrong, Kesten C. Green, and Andreas Graefe. Golden rule of forecasting: Be conservative. Journal of Business Research, 68(8):1717-1731, 2015.
[5] Pavel Atanasov, Phillip Rescober, Eric Stone, Samuel A. Swift, Emile Servan-Schreiber, Philip Tetlock, Lyle Ungar, and Barbara Mellers. Distilling the wisdom of crowds: Prediction markets vs. prediction polls. Management Science, 63(3):691-706, 2017. Publisher: INFORMS.</p>
<p>[6] Pavel Atanasov, Jens Witkowski, Lyle Ungar, Barbara Mellers, and Philip Tetlock. Small steps to accuracy: Incremental belief updaters are better forecasters. Organizational Behavior and Human Decision Processes, 160:19-35, 2020.
[7] Melanie R Beck, Claudia Scarlata, Lucy F Fortson, Chris J Lintott, B D Simmons, Melanie A Galloway, Kyle W Willett, Hugh Dickinson, Karen L Masters, Philip J Marshall, and Darryl Wright. Integrating human and machine intelligence in galaxy morphology classification tasks. Monthly Notices of the Royal Astronomical Society, 476(4):5516-5534, 2018.
[8] Souhaib Ben Taieb, Gianluca Bontempi, Amir F. Atiya, and Antti Sorjamaa. A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition. Expert Systems with Applications, 39(8):7067-7083, 2012.
[9] Elizabeth Boschee, Jennifer Lautenschlager, Sean O'Brien, Steve Shellman, and James Starz. ICEWS weekly event data, 2008. Type: dataset.
[10] G. W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78:1-3, 1950. Google-Books-ID: jnbpAAAAMAAJ.
[11] David V. Budescu. Confidence in aggregation of opinions from multiple sources. In Information Sampling and Adaptive Cognition, Fiedler K, Juslin P, eds., pages 327-352. Cambridge University Press Cambridge, UK, 2006.
[12] Jason W. Burton, Mari-Klara Stein, and Tina Blegind Jensen. A systematic review of algorithm aversion in augmented decision making. Journal of Behavioral Decision Making, 33(2):220-239, 2020. .eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bdm. 2155.
[13] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. Wide \&amp; deep learning for recommender systems, 2016.
[14] Robyn M. Dawes, David Faust, and Paul E. Meehl. Clinical versus actuarial judgment. Science, 243(4899):1668-1674, 1989. Publisher: American Association for the Advancement of Science.
[15] Shari De Baets and Nigel Harvey. Forecasting from time series subject to sporadic perturbations: Effectiveness of different types of forecasting support. International Journal of Forecasting, 34(2):163180, 2018.
[16] Dominik Dellermann, Nikolaus Lipusch, and Philipp Ebel. Developing design principles for a crowdbased business model validation system. In Alexander Maedche, Jan vom Brocke, and Alan Hevner, editors, Designing the Digital Transformation, Lecture Notes in Computer Science, pages 163-178. Springer International Publishing, 2017.
[17] Dominik Dellermann, Nikolaus Lipusch, Philipp Ebel, and Jan Marco Leimeister. Design principles for a hybrid intelligence decision support system for business model validation. Electronic Markets, 29(3):423-441, 2019.
[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, $1: 4171-4186,2019$.
[19] B. J. Dietvorst, Joseph P. Simmons, and C. Massey. Algorithm aversion: People erroneously avoid algorithms after seeing them err. - PsycNET. Journal of Experimental Psychology: General, 144(1):114126, 2015 .
[20] Berkeley J. Dietvorst, Joseph P. Simmons, and Cade Massey. Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them. Management Science, 64(3):11551170, 2018. Publisher: INFORMS.</p>
<p>[21] Walter Enders and Todd Sandler. Patterns of transnational terrorism, 1970-1999: Alternative timeseries estimates. International Studies Quarterly, 46(2):145-165, 2002.
[22] Organisation for Economic Co-operation and Development. OECD data, 2022. Retrieved June 2022 from http://data.oecd.org/united-states.htm.
[23] Hamed Ghoddusi, GermÃ¡n G. Creamer, and Nima Rafizadeh. Machine learning in energy economics and finance: A review. Energy Economics, 81:709-727, 2019.
[24] Nigel Harvey and Fergus Bolger. Graphs versus tables: Effects of data presentation format on judgemental forecasting. International Journal of Forecasting, 12(1):119-137, 1996.
[25] Phil Hilliard. HFC files, 2020. Harvard Dataverse. Retrieved from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/S0K6TV.
[26] Phil Hilliard. HFC Files, 2020.
[27] Mark Himmelstein, Pavel Atanasov, and David V. Budescu. Forecasting forecaster accuracy: Contributions of past performance and individual differences. Judgment and Decision Making, 16(2):323-362, 2021.
[28] K. S. M. Tozammel Hossain, Hrayr Harutyunyan, Yue Ning, Brendan Kennedy, Naren Ramakrishnan, and Aram Galstyan. Identifying geopolitical event precursors using attention-based lstms. Frontiers in Artificial Intelligence, 5, 2022.
[29] Yuzhong Huang, Andres Abeliuk, Fred Morstatter, Pavel Atanasov, and Aram Galstyan. Anchor attention for hybrid crowd forecasts aggregation, 2022.
[30] Rob J Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018.
[31] Rob J. Hyndman and Yeasmin Khandakar. Automatic time series forecasting: The forecast package for r. Journal of Statistical Software, 27:1-22, 2008.
[32] Christian P. Janssen, Stella F. Donker, Duncan P. Brumby, and Andrew L. Kun. History and future of human-automation interaction. International Journal of Human-Computer Studies, 131:99-107, 2019. Publisher: Academic Press.
[33] Woojeong Jin, Rahul Khanna, Suji Kim, Dong-Ho Lee, Fred Morstatter, Aram Galstyan, and Xiang Ren. ForecastQA: A question answering challenge for event forecasting with temporal text data. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 46364650, Online, August 2021. Association for Computational Linguistics.
[34] Regina Joseph and Pavel Atanasov. Predictive training and accuracy: Self-selection and causal factors. In Proceedings of Collective Intelligence, 012019.
[35] Ece Kamar. Hybrid workplaces of the future. XRDS: Crossroads, The ACM Magazine for Students, $23(2): 22-25,2016$.
[36] Andrew Leigh and Justin Wolfers. Competing approaches to forecasting elections: Economic models, opinion polling and prediction markets*. Economic Record, 82(258):325-340, 2006. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-4932.2006.00343.x.
[37] Armed Conflict Location and Event Data. ACLED curated data, 2022. Retrieved June 2022 https://acleddata.com/curated-data-files/.
[38] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. Statistical and machine learning forecasting methods: Concerns and ways forward. PLOS ONE, 13(3):e0194889, 2018. Publisher: Public Library of Science.</p>
<p>[39] Paul E. Meehl. Clinical versus statistical prediction: A theoretical analysis and a review of the evidence. Clinical versus statistical prediction: A theoretical analysis and a review of the evidence. University of Minnesota Press, 1954. Pages: x, 149.
[40] Barbara Mellers, Lyle Ungar, Jonathan Baron, Jaime Ramos, Burcu Gurcay, Katrina Fincher, Sydney E Scott, Don Moore, Pavel Atanasov, Samuel A Swift, et al. Psychological strategies for winning a geopolitical forecasting tournament. Psychological science, 25(5):1106-1115, 2014.
[41] Edgar C. Merkle and Robert Hartman. Weighted brier score decompositions for topically heterogenous forecasting tournaments. Judgment and Decision Making, 13(2):185-201, 2018. Section: Technical Reports.
[42] Pablo Montero-Manso, George Athanasopoulos, Rob J Hyndman, and Thiyanga S Talagala. Fforma: Feature-based forecast model averaging. International Journal of Forecasting, 36(1):86-92, 2020.
[43] Fred Morstatter. SAGE/HFC geopolitical forecasting. rct-b, 2021. Harvard Dataverse. Retrieved from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ROTHFT.
[44] Fred Morstatter, Aram Galstyan, Gleb Satyukov, Daniel Benjamin, Andres Abeliuk, Mehrnoosh Mirtaheri, KSM Tozammel Hossain, Pedro Szekely, Emilio Ferrara, Akira Matsui, Mark Steyvers, Stephen Bennet, David Budescu, Mark Himmelstein, Michael Ward, Andreas Beger, Michele Catasta, Rok Sosic, Jure Leskovec, Pavel Atanasov, Regina Joseph, Rajiv Sethi, and Ali Abbas. Sage: A hybrid geopolitical event forecasting system. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 6557-6559. International Joint Conferences on Artificial Intelligence Organization, 72019 .
[45] Aaron Moss. How CloudResearch and IARPA completed the largest longitudinal online research project ever, 2022 .
[46] Organization of the Petroleum Exporting Countries. OPEC : Monthly oil market report, 2022. Retrieved June 2022 from https://www.opec.org/opec_eb/en/publications/338.htm.
[47] Antonio Rafael Sabino Parmezan, Vinicius M. A. Souza, and Gustavo E. A. P. A. Batista. Evaluation of statistical and machine learning models for time series prediction: Identifying the state-of-the-art and the best conditions for the use of each model. Information Sciences, 484:302-337, 2019.
[48] Ulrich Pilster and Tobias BÃ¶hmelt. Predicting the duration of the syrian insurgency. Research $\mathcal{G}$ Politics, 1(2):2053168014544586, 2014. Publisher: SAGE Publications Ltd.
[49] Andrew Prahl and Lyn Van Swol. Understanding algorithm aversion: When is advice from automation discounted? Journal of Forecasting, 36(6):691-702, 2017. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/for. 2464.
[50] Janet Rafner, Dominik Dellerman, Arthur Hjorth, Dora Veraszto, Constance Kampf, Wendy MacKay, and Jacob Sherson. Deskilling, upskilling, and reskilling: a case for hybrid intelligence. Morals $\mathcal{G}$ Machines, 1(2):24-39, 2022. Publisher: Nomos Verlagsgesellschaft mbH \&amp; Co. KG.
[51] Janet Rafner, Miroslav Gajdacz, Gitte Kragh, Arthur Hjorth, Anna Gander, Blanka Palfi, Aleks Berditchevskaia, FranÃ§ois Grey, Kobi Gal, Avi Segal, Mike Walmsley, Josh Aaron Miller, Dominik Dellerman, Muki Haklay, Pietro Michelucci, and Jacob Sherson. Revisiting citizen science through the lens of hybrid intelligence, 2021.
[52] Janet Rafner, Miroslav Gajdacz, Gitte Kragh, Arthur Hjorth, Anna Gander, Blanka Palfi, Aleksandra Berditchevskiaia, Francois Grey, Kobi Gal, Avi Segal, Mike Wamsley, Joshua Miller, Dominik Dellermann, Mordechai Haklay, Pietro Michelucci, and Jacob Sherson. Mapping citizen science through the lens of human-centered AI. Human Computation, 9(1):66-95, 2022. Number: 1.
[53] Philip A. Schrodt. Automated production of high-volume, real-time political event data, 2010.</p>
<p>[54] Sebastian Schutte. Regions at risk: Predicting conflict zones in african insurgencies*. Political Science Research and Methods, 5(3):447-465, 2017. Publisher: Cambridge University Press.
[55] Matthias Seifert and AllÃ¨gre L. Hadida. 3 humans +1 computer $=$ best prediction. Harvard Business Review, 2013. Section: Product development.
[56] Philip E Tetlock. Expert political judgment: How good is it? How can we know?-New edition. Princeton University Press, 2017.
[57] Laura Trouille, Chris J. Lintott, and Lucy F. Fortson. Citizen science frontiers: Efficiency, engagement, and serendipitous discovery with human-machine systems. Proceedings of the National Academy of Sciences, 116(6):1902-1909, 2019. Publisher: Proceedings of the National Academy of Sciences.
[58] Maximilian Zellner, Ali E. Abbas, David V. Budescu, and Aram Galstyan. A survey of human judgement and quantitative forecasting methods. Royal Society Open Science, 8(2):201187, 2022. Publisher: Royal Society.
[59] Dilek Ã–nkal, Paul Goodwin, Mary Thomson, Sinan GÃ¶nÃ¼l, and Andrew Pollock. The relative influence of advice from human experts and statistical methods on forecast adjustments. Journal of Behavioral Decision Making, 22(4):390-409, 2009. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bdm.637.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Behavioral decision making researchers have repeatedly documented a pattern of "Algorithm Aversion (AA for short)" (e.g., $[12,19]$ )) - the tendency of humans to prefer and value advice and information from human sources over machine counterparts, even when the information provided by humans and algorithms is identical (e.g., [19, 59]). In general, judges tend to be less tolerant of errors made by algorithms, compared to humans (e.g., [19, 49]). One way to reduce AA is to allow people to have more control over the algorithm by tweaking it, or some of its predictions [20]. Condition D was implemented to test this expectation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
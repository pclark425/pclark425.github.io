<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-703 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-703</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-703</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-2397ce306e5d7f3d0492276e357fb1833536b5d8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2397ce306e5d7f3d0492276e357fb1833536b5d8" target="_blank">On the State of the Art of Evaluation in Neural Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrives at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models.</p>
                <p><strong>Paper Abstract:</strong> Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e703.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e703.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyperparameter noise</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyperparameter variation and mismatch between reported settings and actual tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uncontrolled differences in hyperparameter choices (or their tuning budgets) across papers and codebases that change measured model performance and can reverse claimed rankings between architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>neural language model evaluation pipeline (hyperparameter optimisation with Google Vizier)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Experimental setup for training and evaluating recurrent neural language models where hyperparameters (learning rate, dropout variants and amounts, embedding ratios, weight decay, intra-layer dropout, down-projection presence/size) are chosen by an automated black-box tuner (Google Vizier) and affect model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / reported experimental protocol</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts / TensorFlow training code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch / sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language experimental descriptions often omit or under-specify the full hyperparameter search procedure, search ranges, and budgets used; differing tuning intensity or defaults in codebases leads to inconsistent comparisons and can cause architectures reported as superior to lose that advantage when others are tuned more extensively.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>hyperparameters / model selection / tuning budget</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>re-evaluation with large-scale black-box hyperparameter optimisation (Google Vizier) and controlled retraining; comparison of tuned results with previously published numbers</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantified by (1) absolute changes in validation/test metrics resulting from tuning or ablations (e.g., down-projection or embedding tying), (2) rerun variance measurements across seeds and nondeterminism: observed standard deviations and point differences. Reported numeric measures include 0.4 perplexity variance on Penn Treebank and 0.5 on Wikitext-2 from nondeterminism + seed effects; majority of hyperparameter neighbourhood produced results within 3.0 perplexity of best value; a difference of 1.0 perplexity considered statistically robust under their protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantive: when hyperparameters were properly controlled, standard LSTMs outperformed more recent architectures contrary to earlier claims; specific hyperparameter choices produced multi-point changes in perplexity (examples: down-projection improved PTB by ~2–5 PPL and WT2 by ~10–18 PPL; untied embeddings worsened PPL by ~6 points). The paper asserts that insufficiently controlled hyperparameter variation can lead to replication failures and false claims.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as common and an often inadequately controlled source of variation; in these experiments thousands of evaluations were required for convergence (their tuning ran ≈1500 trials for a model where a naive 5^6 grid would need ~8000 trials). No global percentage given.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or incomplete natural-language reporting of tuning procedures, limited computational budgets leading to insufficient tuning, implicit/default hyperparameter choices, and differences in tuning methodology between research groups.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Large-scale black-box hyperparameter optimisation to control for tuning differences, explicit reporting of hyperparameter search ranges and budgets, reducing hyperparameter sensitivity (model design), and community practices such as reporting score distributions or 'leagues' with fixed computational budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in this study: rigorous tuning changed model rankings and produced new state-of-the-art baselines; the authors report that Vizier-based tuning yielded superior results within ≈1500 trials (versus an estimated ≈8000-grid trials). No single low-cost mitigation was found; authors emphasize the computational cost of reliable tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning (language modelling)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the State of the Art of Evaluation in Neural Language Models', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e703.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e703.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Implementation non-determinism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-deterministic computation and codebase differences affecting experimental outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Runtime nondeterminism (e.g., floating-point operation ordering) and differing codebases lead to variability in results even with identical hyperparameters and seeds, complicating reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TensorFlow single-GPU experimental codebase (and differing external codebases used in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The concrete implementation used to train models (TensorFlow-based scripts and optimized linear-algebra routines) which can produce non-deterministic results due to operation ordering and library behavior, and which may differ from other groups' codebases.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / implementation notes</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>library code / experiment scripts (TensorFlow)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation variability / non-determinism</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Descriptions in papers typically present a deterministic protocol but actual code execution is non-deterministic (floating-point operation ordering, vendor BLAS/cuDNN behavior), and different codebases use different defaults and optimizations; this produces measurable run-to-run variance that is not captured in natural-language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / runtime and low-level implementation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical rerun experiments: retraining models with identical hyperparameters but differing initialization seeds and attempting to use the same seed to isolate nondeterminism; analysis of variance across reruns.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Measured variance between reruns. Key quantitative findings: (a) nondeterministic ordering of floating-point operations was almost as large an effect as the combination of nondeterminism and different initialization seeds; combined (a)+(b) produced roughly 0.4 perplexity variance on Penn Treebank and 0.5 on Wikitext-2; the validation–test gap contributed 0.12–0.3 perplexity variance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Causes measurable spread in reported metrics that can mask or mimic small effect sizes; contributes to replication variance and may lead to over-claiming if only a single run/checkpoint is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed consistently in their TensorFlow single-GPU environment; characterized as a significant source of noise but no broad prevalence percentage given.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Low-level numerical nondeterminism in optimized linear algebra libraries and differences between codebases (implementation choices, library versions, hardware), plus omission of these details in natural-language reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Run multiple retrains with different seeds, report distributions rather than single best checkpoints, control and report software/hardware versions and seeds, or use deterministic computation where feasible; increase number of evaluations to estimate noise.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: reruns allowed estimation of variance and supported the claim that differences under ~1.0 PPL are not robust; nevertheless, nondeterminism remains costly to eliminate and requires additional computation to characterize.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning (experimental reproducibility)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the State of the Art of Evaluation in Neural Language Models', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e703.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e703.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Incomplete experimental specification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Omission or under-specification of implementation details (embedding sharing, down-projection, dropout variants) in natural-language descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Missing or ambiguous reporting of specific implementation choices (e.g., whether input and output embeddings are shared, whether a down-projection is used, exact dropout parameterisations) that materially change measured performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>neural language model training/evaluation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Concrete training implementations that realize particular choices for embedding sharing, down-projection, dropout parametrisations (variational vs recurrent), gate tying and other architectural/regularisation details.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / architecture and training specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts / model code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / omitted implementation detail</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers sometimes do not fully specify which architectural or regularisation variants were used; the authors demonstrate that different choices (e.g., shared vs untied embeddings, presence/size of down-projection, using variational vs recurrent dropout, gate tying) produce large performance differences but are not always fully described in natural-language reports.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model architecture / regularisation / preprocessing of outputs</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Ablation experiments and controlled variants: systematically toggling shared embeddings, down-projection, variational dropout, recurrent dropout, and gate tying and measuring the effect on validation/test metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported ablation outcomes: untied input/output embeddings worsened perplexity by ~6 points across models; down-projection improved PTB by ~2–5 PPL at some depths/budgets and improved WT2 by ~10–18 PPL; removing variational dropout significantly worsened RHN performance; gate tying/untied variants produced small changes. Measurements were direct delta in validation/test perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large: specific omissions can explain substantial parts of reported performance differences between papers; lack of precise specification impedes fair comparison and reproduction (e.g., a 6 PPL difference for embedding tying).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in the contexts compared (authors note many prior results used differing codebases and variable tuning); no general prevalence number provided.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Space or clarity constraints in papers, implicit assumptions about standard defaults, and failure to publish full experimental config or code.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Publish full experimental configurations, hyperparameter ranges and budgets, ablation studies, and release code; perform and report controlled ablations to quantify effect of each implementation choice.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in this study: the authors' ablations explained large performance differences and helped produce more reliable baselines; effectiveness limited by extra computation required to run many ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning (language modelling)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the State of the Art of Evaluation in Neural Language Models', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e703.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e703.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tuner overfitting / selection bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Overfitting of hyperparameter tuners to validation-set noise and selection bias from reporting best-found checkpoints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated hyperparameter optimisers can 'fit' validation noise and select checkpoints that are unusually good due to random variation, leading to optimistic reported results if not corrected by reruns or distributional reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>black-box hyperparameter optimisation with Google Vizier</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Batched Gaussian-process-based bandit optimiser used to find hyperparameter settings by maximising expected improvement on validation perplexity; checkpoints with best validation metrics are selected for test evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / model selection description in paper</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>hyperparameter optimisation service interaction and model selection scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification of model selection procedure / overfitting to validation noise</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Hyperparameter tuners can exploit random fluctuations in validation performance (noise sources: nondeterminism, initialization) and select hyperparameters or checkpoints that do not generalize to repeated training runs; natural-language descriptions often omit quantification of this risk or whether reruns were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model selection / validation-to-test selection</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Retraining best-found hyperparameter configurations multiple times with different initialization seeds and comparing the distribution of validation and test scores to the single best checkpoint reported by the tuner.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Empirical comparison of rerun distributions: the best checkpoint's validation perplexity was about one standard deviation lower than the sample mean of reruns; rerun variance (a)+(b) ~0.4 PPL (PTB); validation–test gap contributed 0.12–0.3 PPL. They therefore conclude that small reported gains may be due to selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Moderate: selection bias causes modest over-optimism in reported metrics but in this study was limited (best-checkpoint advantage ≈1 standard deviation of rerun mean); it can, however, lead to incorrect conclusions if not accounted for, especially for small effect sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in their tuning runs; paper suggests it is a general risk when many hyperparameter configurations are evaluated but no specific frequency statistic is given.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Finite validation sets, noisy evaluation, and the nature of automated search methods that pick maxima from many noisy trials; lack of rerun/repeat reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Retrain top hyperparameter configurations multiple times with varied seeds and report distributions; use conservative thresholds for declaring improvements (authors propose ~1.0 PPL as robust for their setup); run until tuner convergence and quantify GP uncertainty where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: reruns allowed estimation of selection bias and variance and supported robust comparisons, but require additional computational cost. The authors did not eliminate the issue entirely but reduced its impact sufficiently to claim statistical robustness thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning (experimental methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the State of the Art of Evaluation in Neural Language Models', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep reinforcement learning that matters <em>(Rating: 2)</em></li>
                <li>Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging <em>(Rating: 2)</em></li>
                <li>Capacity and trainability in recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Google Vizier: A service for black-box optimization <em>(Rating: 2)</em></li>
                <li>Neural architecture search with reinforcement learning <em>(Rating: 1)</em></li>
                <li>Recurrent highway networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-703",
    "paper_id": "paper-2397ce306e5d7f3d0492276e357fb1833536b5d8",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Hyperparameter noise",
            "name_full": "Hyperparameter variation and mismatch between reported settings and actual tuning",
            "brief_description": "Uncontrolled differences in hyperparameter choices (or their tuning budgets) across papers and codebases that change measured model performance and can reverse claimed rankings between architectures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "neural language model evaluation pipeline (hyperparameter optimisation with Google Vizier)",
            "system_description": "Experimental setup for training and evaluating recurrent neural language models where hyperparameters (learning rate, dropout variants and amounts, embedding ratios, weight decay, intra-layer dropout, down-projection presence/size) are chosen by an automated black-box tuner (Google Vizier) and affect model selection.",
            "nl_description_type": "research paper methods section / reported experimental protocol",
            "code_implementation_type": "experiment scripts / TensorFlow training code",
            "gap_type": "hyperparameter mismatch / sensitivity",
            "gap_description": "Natural-language experimental descriptions often omit or under-specify the full hyperparameter search procedure, search ranges, and budgets used; differing tuning intensity or defaults in codebases leads to inconsistent comparisons and can cause architectures reported as superior to lose that advantage when others are tuned more extensively.",
            "gap_location": "hyperparameters / model selection / tuning budget",
            "detection_method": "re-evaluation with large-scale black-box hyperparameter optimisation (Google Vizier) and controlled retraining; comparison of tuned results with previously published numbers",
            "measurement_method": "Quantified by (1) absolute changes in validation/test metrics resulting from tuning or ablations (e.g., down-projection or embedding tying), (2) rerun variance measurements across seeds and nondeterminism: observed standard deviations and point differences. Reported numeric measures include 0.4 perplexity variance on Penn Treebank and 0.5 on Wikitext-2 from nondeterminism + seed effects; majority of hyperparameter neighbourhood produced results within 3.0 perplexity of best value; a difference of 1.0 perplexity considered statistically robust under their protocol.",
            "impact_on_results": "Substantive: when hyperparameters were properly controlled, standard LSTMs outperformed more recent architectures contrary to earlier claims; specific hyperparameter choices produced multi-point changes in perplexity (examples: down-projection improved PTB by ~2–5 PPL and WT2 by ~10–18 PPL; untied embeddings worsened PPL by ~6 points). The paper asserts that insufficiently controlled hyperparameter variation can lead to replication failures and false claims.",
            "frequency_or_prevalence": "Described as common and an often inadequately controlled source of variation; in these experiments thousands of evaluations were required for convergence (their tuning ran ≈1500 trials for a model where a naive 5^6 grid would need ~8000 trials). No global percentage given.",
            "root_cause": "Ambiguous or incomplete natural-language reporting of tuning procedures, limited computational budgets leading to insufficient tuning, implicit/default hyperparameter choices, and differences in tuning methodology between research groups.",
            "mitigation_approach": "Large-scale black-box hyperparameter optimisation to control for tuning differences, explicit reporting of hyperparameter search ranges and budgets, reducing hyperparameter sensitivity (model design), and community practices such as reporting score distributions or 'leagues' with fixed computational budgets.",
            "mitigation_effectiveness": "Effective in this study: rigorous tuning changed model rankings and produced new state-of-the-art baselines; the authors report that Vizier-based tuning yielded superior results within ≈1500 trials (versus an estimated ≈8000-grid trials). No single low-cost mitigation was found; authors emphasize the computational cost of reliable tuning.",
            "domain_or_field": "machine learning / deep learning (language modelling)",
            "reproducibility_impact": true,
            "uuid": "e703.0",
            "source_info": {
                "paper_title": "On the State of the Art of Evaluation in Neural Language Models",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Implementation non-determinism",
            "name_full": "Non-deterministic computation and codebase differences affecting experimental outcomes",
            "brief_description": "Runtime nondeterminism (e.g., floating-point operation ordering) and differing codebases lead to variability in results even with identical hyperparameters and seeds, complicating reproducibility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TensorFlow single-GPU experimental codebase (and differing external codebases used in prior work)",
            "system_description": "The concrete implementation used to train models (TensorFlow-based scripts and optimized linear-algebra routines) which can produce non-deterministic results due to operation ordering and library behavior, and which may differ from other groups' codebases.",
            "nl_description_type": "research paper methods section / implementation notes",
            "code_implementation_type": "library code / experiment scripts (TensorFlow)",
            "gap_type": "implementation variability / non-determinism",
            "gap_description": "Descriptions in papers typically present a deterministic protocol but actual code execution is non-deterministic (floating-point operation ordering, vendor BLAS/cuDNN behavior), and different codebases use different defaults and optimizations; this produces measurable run-to-run variance that is not captured in natural-language descriptions.",
            "gap_location": "training procedure / runtime and low-level implementation",
            "detection_method": "Empirical rerun experiments: retraining models with identical hyperparameters but differing initialization seeds and attempting to use the same seed to isolate nondeterminism; analysis of variance across reruns.",
            "measurement_method": "Measured variance between reruns. Key quantitative findings: (a) nondeterministic ordering of floating-point operations was almost as large an effect as the combination of nondeterminism and different initialization seeds; combined (a)+(b) produced roughly 0.4 perplexity variance on Penn Treebank and 0.5 on Wikitext-2; the validation–test gap contributed 0.12–0.3 perplexity variance.",
            "impact_on_results": "Causes measurable spread in reported metrics that can mask or mimic small effect sizes; contributes to replication variance and may lead to over-claiming if only a single run/checkpoint is reported.",
            "frequency_or_prevalence": "Observed consistently in their TensorFlow single-GPU environment; characterized as a significant source of noise but no broad prevalence percentage given.",
            "root_cause": "Low-level numerical nondeterminism in optimized linear algebra libraries and differences between codebases (implementation choices, library versions, hardware), plus omission of these details in natural-language reporting.",
            "mitigation_approach": "Run multiple retrains with different seeds, report distributions rather than single best checkpoints, control and report software/hardware versions and seeds, or use deterministic computation where feasible; increase number of evaluations to estimate noise.",
            "mitigation_effectiveness": "Partially effective: reruns allowed estimation of variance and supported the claim that differences under ~1.0 PPL are not robust; nevertheless, nondeterminism remains costly to eliminate and requires additional computation to characterize.",
            "domain_or_field": "machine learning / deep learning (experimental reproducibility)",
            "reproducibility_impact": true,
            "uuid": "e703.1",
            "source_info": {
                "paper_title": "On the State of the Art of Evaluation in Neural Language Models",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Incomplete experimental specification",
            "name_full": "Omission or under-specification of implementation details (embedding sharing, down-projection, dropout variants) in natural-language descriptions",
            "brief_description": "Missing or ambiguous reporting of specific implementation choices (e.g., whether input and output embeddings are shared, whether a down-projection is used, exact dropout parameterisations) that materially change measured performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "neural language model training/evaluation scripts",
            "system_description": "Concrete training implementations that realize particular choices for embedding sharing, down-projection, dropout parametrisations (variational vs recurrent), gate tying and other architectural/regularisation details.",
            "nl_description_type": "research paper methods section / architecture and training specification",
            "code_implementation_type": "experiment scripts / model code",
            "gap_type": "incomplete specification / omitted implementation detail",
            "gap_description": "Papers sometimes do not fully specify which architectural or regularisation variants were used; the authors demonstrate that different choices (e.g., shared vs untied embeddings, presence/size of down-projection, using variational vs recurrent dropout, gate tying) produce large performance differences but are not always fully described in natural-language reports.",
            "gap_location": "model architecture / regularisation / preprocessing of outputs",
            "detection_method": "Ablation experiments and controlled variants: systematically toggling shared embeddings, down-projection, variational dropout, recurrent dropout, and gate tying and measuring the effect on validation/test metrics.",
            "measurement_method": "Reported ablation outcomes: untied input/output embeddings worsened perplexity by ~6 points across models; down-projection improved PTB by ~2–5 PPL at some depths/budgets and improved WT2 by ~10–18 PPL; removing variational dropout significantly worsened RHN performance; gate tying/untied variants produced small changes. Measurements were direct delta in validation/test perplexity.",
            "impact_on_results": "Large: specific omissions can explain substantial parts of reported performance differences between papers; lack of precise specification impedes fair comparison and reproduction (e.g., a 6 PPL difference for embedding tying).",
            "frequency_or_prevalence": "Observed in the contexts compared (authors note many prior results used differing codebases and variable tuning); no general prevalence number provided.",
            "root_cause": "Space or clarity constraints in papers, implicit assumptions about standard defaults, and failure to publish full experimental config or code.",
            "mitigation_approach": "Publish full experimental configurations, hyperparameter ranges and budgets, ablation studies, and release code; perform and report controlled ablations to quantify effect of each implementation choice.",
            "mitigation_effectiveness": "Effective in this study: the authors' ablations explained large performance differences and helped produce more reliable baselines; effectiveness limited by extra computation required to run many ablations.",
            "domain_or_field": "machine learning / deep learning (language modelling)",
            "reproducibility_impact": true,
            "uuid": "e703.2",
            "source_info": {
                "paper_title": "On the State of the Art of Evaluation in Neural Language Models",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Tuner overfitting / selection bias",
            "name_full": "Overfitting of hyperparameter tuners to validation-set noise and selection bias from reporting best-found checkpoints",
            "brief_description": "Automated hyperparameter optimisers can 'fit' validation noise and select checkpoints that are unusually good due to random variation, leading to optimistic reported results if not corrected by reruns or distributional reporting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "black-box hyperparameter optimisation with Google Vizier",
            "system_description": "Batched Gaussian-process-based bandit optimiser used to find hyperparameter settings by maximising expected improvement on validation perplexity; checkpoints with best validation metrics are selected for test evaluation.",
            "nl_description_type": "experimental protocol / model selection description in paper",
            "code_implementation_type": "hyperparameter optimisation service interaction and model selection scripts",
            "gap_type": "incomplete specification of model selection procedure / overfitting to validation noise",
            "gap_description": "Hyperparameter tuners can exploit random fluctuations in validation performance (noise sources: nondeterminism, initialization) and select hyperparameters or checkpoints that do not generalize to repeated training runs; natural-language descriptions often omit quantification of this risk or whether reruns were performed.",
            "gap_location": "model selection / validation-to-test selection",
            "detection_method": "Retraining best-found hyperparameter configurations multiple times with different initialization seeds and comparing the distribution of validation and test scores to the single best checkpoint reported by the tuner.",
            "measurement_method": "Empirical comparison of rerun distributions: the best checkpoint's validation perplexity was about one standard deviation lower than the sample mean of reruns; rerun variance (a)+(b) ~0.4 PPL (PTB); validation–test gap contributed 0.12–0.3 PPL. They therefore conclude that small reported gains may be due to selection bias.",
            "impact_on_results": "Moderate: selection bias causes modest over-optimism in reported metrics but in this study was limited (best-checkpoint advantage ≈1 standard deviation of rerun mean); it can, however, lead to incorrect conclusions if not accounted for, especially for small effect sizes.",
            "frequency_or_prevalence": "Observed in their tuning runs; paper suggests it is a general risk when many hyperparameter configurations are evaluated but no specific frequency statistic is given.",
            "root_cause": "Finite validation sets, noisy evaluation, and the nature of automated search methods that pick maxima from many noisy trials; lack of rerun/repeat reporting.",
            "mitigation_approach": "Retrain top hyperparameter configurations multiple times with varied seeds and report distributions; use conservative thresholds for declaring improvements (authors propose ~1.0 PPL as robust for their setup); run until tuner convergence and quantify GP uncertainty where possible.",
            "mitigation_effectiveness": "Partially effective: reruns allowed estimation of selection bias and variance and supported robust comparisons, but require additional computational cost. The authors did not eliminate the issue entirely but reduced its impact sufficiently to claim statistical robustness thresholds.",
            "domain_or_field": "machine learning / deep learning (experimental methodology)",
            "reproducibility_impact": true,
            "uuid": "e703.3",
            "source_info": {
                "paper_title": "On the State of the Art of Evaluation in Neural Language Models",
                "publication_date_yy_mm": "2017-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep reinforcement learning that matters",
            "rating": 2
        },
        {
            "paper_title": "Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging",
            "rating": 2
        },
        {
            "paper_title": "Capacity and trainability in recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Google Vizier: A service for black-box optimization",
            "rating": 2
        },
        {
            "paper_title": "Neural architecture search with reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Recurrent highway networks",
            "rating": 1
        }
    ],
    "cost": 0.012546999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On the State of the Art of Evaluation in Neural Language Models</h1>
<p>Gábor Melis ${ }^{\dagger}$, Chris Dyer ${ }^{\ddagger}$, Phil Blunsom ${ }^{\ddagger \ddagger}$<br>{melisgl,cdyer,pblunsom}@google.com<br>${ }^{\dagger}$ DeepMind<br>${ }^{\ddagger}$ University of Oxford</p>
<h4>Abstract</h4>
<p>Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.</p>
<h2>1 INTRODUCTION</h2>
<p>The scientific process by which the deep learning research community operates is guided by empirical studies that evaluate the relative quality of models. Complicating matters, the measured performance of a model depends not only on its architecture (and data), but it can strongly depend on hyperparameter values that affect learning, regularisation, and capacity. This hyperparameter dependence is an often inadequately controlled source of variation in experiments, which creates a risk that empirically unsound claims will be reported.</p>
<p>In this paper, we use a black-box hyperparameter optimisation technique to control for hyperparameter effects while comparing the relative performance of language modelling architectures based on LSTMs, Recurrent Highway Networks (Zilly et al., 2016) and NAS (Zoph \&amp; Le, 2016). We specify flexible, parameterised model families with the ability to adjust embedding and recurrent cell sizes for a given parameter budget and with fine grain control over regularisation and learning hyperparameters.</p>
<p>Once hyperparameters have been properly controlled for, we find that LSTMs outperform the more recent models, contra the published claims. Our result is therefore a demonstration that replication failures can happen due to poorly controlled hyperparameter variation, and this paper joins other recent papers in warning of the under-acknowledged existence of replication failure in deep learning (Henderson et al., 2017; Reimers \&amp; Gurevych, 2017). However, we do show that careful controls are possible, albeit at considerable computational cost.</p>
<p>Several remarks can be made in light of these results. First, as (conditional) language models serve as the central building block of many tasks, including machine translation, there is little reason to expect that the problem of unreliable evaluation is unique to the tasks discussed here. However, in machine translation, carefully controlling for hyperparameter effects would be substantially more expensive because standard datasets are much larger. Second, the research community should strive for more consensus about appropriate experimental methodology that balances costs of careful experimentation with the risks associated with false claims. Finally, more attention should be paid to hyperparameter sensitivity. Models that introduce many new hyperparameters or which perform well only in narrow ranges of hyperparameter settings should be identified as such as part of standard publication practice.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Recurrent networks with optional down-projection, per-step and per-sequence dropout (dashed and solid lines).</p>
<h1>2 MODELS</h1>
<p>Our focus is on three recurrent architectures:</p>
<ul>
<li>The Long Short-Term Memory (Hochreiter \&amp; Schmidhuber, 1997) serves as a well known and frequently used baseline.</li>
<li>The recently proposed Recurrent Highway Network (Zilly et al., 2016) is chosen because it has demonstrated state-of-the-art performance on a number of datasets.</li>
<li>Finally, we also include NAS (Zoph \&amp; Le, 2016), because of its impressive performance and because its architecture was the result of an automated reinforcement learning based optimisation process.</li>
</ul>
<p>Our aim is strictly to do better model comparisons for these architectures and we thus refrain from including techniques that are known to push perplexities even lower, but which are believed to be largely orthogonal to the question of the relative merits of these recurrent cells. In parallel work with a remarkable overlap with ours, Merity et al. (2017) demonstrate the utility of adding a Neural Cache (Grave et al., 2016). Building on their work, Krause et al. (2017) show that Dynamic Evaluation (Graves, 2013) contributes similarly to the final perplexity.
As pictured in Fig. 1a, our models with LSTM or NAS cells have all the standard components: an input embedding lookup table, recurrent cells stacked as layers with additive skip connections combining outputs of all layers to ease optimisation. There is an optional down-projection whose presence is governed by a hyperparameter from this combined output to a smaller space which reduces the number of output embedding parameters. Unless otherwise noted, input and output embeddings are shared, see (Inan et al., 2016) and (Press \&amp; Wolf, 2016).
Dropout is applied to feedforward connections denoted by dashed arrows in the figure. From the bottom up: to embedded inputs (input dropout), to connections between layers (intra-layer dropout), to the combined and the down-projected outputs (output dropout). All these dropouts have random masks drawn independently per time step, in contrast to the dropout on recurrent states where the same mask is used for all time steps in the sequence.</p>
<p>RHN based models are typically conceived of as a single horizontal "highway" to emphasise how the recurrent state is processed through time. In Fig. 1b, we choose to draw their schema in a way that makes the differences from LSTMs immediately apparent. In a nutshell, the RHN state is passed from the topmost layer to the lowest layer of the next time step. In contrast, each LSTM layer has its own recurrent connection and state.</p>
<p>The same dropout variants are applied to all three model types, with the exception of intra-layer dropout which does not apply to RHNs since only the recurrent state is passed between the layers.</p>
<p>For the recurrent states, all architectures use either variational dropout (Gal \&amp; Ghahramani, 2016, state dropout) ${ }^{1}$ or recurrent dropout (Semeniuta et al., 2016), unless explicitly noted otherwise.</p>
<h1>3 EXPERIMENTAL SETUP</h1>
<h3>3.1 DATASETS</h3>
<p>We compare models on three datasets. The smallest of them is the Penn Treebank corpus by Marcus et al. (1993) with preprocessing from Mikolov et al. (2010). We also include another word level corpus: Wikitext-2 by Merity et al. (2016). It is about twice the size of Penn Treebank with a larger vocabulary and much lighter preprocessing. The third corpus is Enwik8 from the Hutter Prize dataset (Hutter, 2012). Following common practice, we use the first 90 million characters for training, and the remaining 10 million evenly split between validation and test.</p>
<h2>4 TRAINING DETAILS</h2>
<p>When training word level models we follow common practice and use a batch size of 64, truncated backpropagation with 35 time steps, and we feed the final states from the previous batch as the initial state of the subsequent one. At the beginning of training and test time, the model starts with a zero state. To bias the model towards being able to easily start from such a state at test time, during training, with probability 0.01 a constant zero state is provided as the initial state.</p>
<p>Optimisation is performed by Adam (Kingma \&amp; Ba, 2014) with $\beta_{1}=0$ but otherwise default parameters $\left(\beta_{2}=0.999, \epsilon=10^{-9}\right)$. Setting $\beta_{1}$ so turns off the exponential moving average for the estimates of the means of the gradients and brings Adam very close to RMSProp without momentum, but due to Adam's bias correction, larger learning rates can be used.</p>
<p>Batch size is set to 64 . The learning rate is multiplied by 0.1 whenever validation performance does not improve ever during 30 consecutive checkpoints. These checkpoints are performed after every 100 and 200 optimization steps for Penn Treebank and Wikitext-2, respectively.</p>
<p>For character level models (i.e. Enwik8), the differences are: truncated backpropagation is performed with 50 time steps. Adam's parameters are $\beta_{2}=0.99, \epsilon=10^{-5}$. Batch size is 128 . Checkpoints are only every 400 optimisation steps and embeddings are not shared.</p>
<h2>5 EVALUATION</h2>
<p>For evaluation, the checkpoint with the best validation perplexity found by the tuner is loaded and the model is applied to the test set with a batch size of 1 . For the word based datasets, using the training batch size makes results worse by 0.3 PPL while Enwik8 is practically unaffected due to its evaluation and training sets being much larger. Preliminary experiments indicate that MC averaging would bring a small improvement of about 0.4 in perplexity and 0.005 in bits per character, similar to the results of Gal \&amp; Ghahramani (2016), while being a 1000 times more expensive which is prohibitive on larger datasets. Therefore, throughout we use the mean-field approximation for dropout at test time.</p>
<h3>5.1 HyPERPARAMETER TUNING</h3>
<p>Hyperparameters are optimised by Google Vizier (Golovin et al., 2017), a black-box hyperparameter tuner based on batched GP bandits using the expected improvement acquisition function (Desautels et al., 2014). Tuners of this nature are generally more efficient than grid search when the number of hyperparameters is small. To keep the problem tractable, we restrict the set of hyperparameters to learning rate, input embedding ratio, input dropout, state dropout, output dropout, weight decay. For deep LSTMs, there is an extra hyperparameter to tune: intra-layer dropout. Even with this small set, thousands of evaluations are required to reach convergence.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Size</th>
<th style="text-align: right;">Depth</th>
<th style="text-align: right;">Valid</th>
<th style="text-align: right;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Medium LSTM, Zaremba et al. (2014)</td>
<td style="text-align: right;">10M</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">86.2</td>
<td style="text-align: right;">82.7</td>
</tr>
<tr>
<td style="text-align: left;">Large LSTM, Zaremba et al. (2014)</td>
<td style="text-align: right;">24M</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">82.2</td>
<td style="text-align: right;">78.4</td>
</tr>
<tr>
<td style="text-align: left;">VD LSTM, Press \&amp; Wolf (2016)</td>
<td style="text-align: right;">51M</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">75.8</td>
<td style="text-align: right;">73.2</td>
</tr>
<tr>
<td style="text-align: left;">VD LSTM, Inan et al. (2016)</td>
<td style="text-align: right;">9M</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">77.1</td>
<td style="text-align: right;">73.9</td>
</tr>
<tr>
<td style="text-align: left;">VD LSTM, Inan et al. (2016)</td>
<td style="text-align: right;">28M</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">72.5</td>
<td style="text-align: right;">69.0</td>
</tr>
<tr>
<td style="text-align: left;">VD RHN, Zilly et al. (2016)</td>
<td style="text-align: right;">24M</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">67.9</td>
<td style="text-align: right;">65.4</td>
</tr>
<tr>
<td style="text-align: left;">NAS, Zoph \&amp; Le (2016)</td>
<td style="text-align: right;">25M</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">64.0</td>
</tr>
<tr>
<td style="text-align: left;">NAS, Zoph \&amp; Le (2016)</td>
<td style="text-align: right;">54M</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">62.4</td>
</tr>
<tr>
<td style="text-align: left;">AWD-LSTM, Merity et al. (2017) $\dagger$</td>
<td style="text-align: right;">24M</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">60.0</td>
<td style="text-align: right;">57.3</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">61.8</td>
<td style="text-align: right;">59.6</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">63.0</td>
<td style="text-align: right;">60.8</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: right;">10M</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">62.4</td>
<td style="text-align: right;">60.1</td>
</tr>
<tr>
<td style="text-align: left;">RHN</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">66.0</td>
<td style="text-align: right;">63.5</td>
</tr>
<tr>
<td style="text-align: left;">NAS</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">65.6</td>
<td style="text-align: right;">62.7</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">61.4</td>
<td style="text-align: right;">59.5</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">62.1</td>
<td style="text-align: right;">59.6</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: right;">24M</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">60.9</td>
<td style="text-align: right;">58.3</td>
</tr>
<tr>
<td style="text-align: left;">RHN</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">64.8</td>
<td style="text-align: right;">62.2</td>
</tr>
<tr>
<td style="text-align: left;">NAS</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">62.1</td>
<td style="text-align: right;">59.7</td>
</tr>
</tbody>
</table>
<p>Table 1: Validation and test set perplexities on Penn Treebank for models with different numbers of parameters and depths. All results except those from Zaremba are with shared input and output embeddings. VD stands for Variational Dropout from Gal \&amp; Ghahramani (2016). $\dagger$ : parallel work.</p>
<p>Parameter budget. Motivated by recent results from Collins et al. (2016), we compare models on the basis of the total number of trainable parameters as opposed to the number of hidden units. The tuner is given control over the presence and size of the down-projection, and thus over the tradeoff between the number of embedding vs. recurrent cell parameters. Consequently, the cells' hidden size and the embedding size is determined by the actual parameter budget, depth and the input embedding ratio hyperparameter.
For Enwik8 there are relatively few parameters in the embeddings since the vocabulary size is only 205. Here we choose not to share embeddings and to omit the down-projection unconditionally.</p>
<h1>6 ReSults</h1>
<h3>6.1 Penn Treebank</h3>
<p>We tested LSTMs of various depths and an RHN of depth 5 with parameter budgets of 10 and 24 million matching the sizes of the Medium and Large LSTMs by (Zaremba et al., 2014). The results are summarised in Table 1.</p>
<p>Notably, in our experiments even the RHN with only 10M parameters has better perplexity than the 24 M one in the original publication. Our 24 M version improves on that further. However, a shallow LSTM-based model with only 10M parameters enjoys a very comfortable margin over that, with deeper models following near the estimated noise range. At 24M, all depths obtain very similar results, reaching 58.3 at depth 4 . Unsurprisingly, NAS whose architecture was chosen based on its performance on this dataset does almost equally well, even better than in Zoph \&amp; Le (2016).</p>
<h3>6.2 WIKiteXT-2</h3>
<p>Wikitext-2 is not much larger than Penn Treebank, so it is not surprising that even models tuned for Penn Treebank perform reasonably on this dataset, and this is in fact how results in previous works were produced. For a fairer comparison, we also tune hyperparameters on the same dataset. In Table 2, we report numbers for both approaches. All our results are well below the previous state of the are for models without dynamic evaluation or caching. That said, our best result, 65.9 compares</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Depth</th>
<th style="text-align: center;">Valid</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VD LSTM, Merity et al. (2016)</td>
<td style="text-align: center;">20 M</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">101.7</td>
<td style="text-align: center;">96.3</td>
</tr>
<tr>
<td style="text-align: left;">VD+Zoneout LSTM, Merity et al. (2016)</td>
<td style="text-align: center;">20 M</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">108.7</td>
<td style="text-align: center;">100.9</td>
</tr>
<tr>
<td style="text-align: left;">VD LSTM, Inan et al. (2016)</td>
<td style="text-align: center;">22 M</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">87.7</td>
</tr>
<tr>
<td style="text-align: left;">AWD-LSTM, Merity et al. (2017) $\dagger$</td>
<td style="text-align: center;">33 M</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">65.8</td>
</tr>
<tr>
<td style="text-align: left;">LSTM (tuned for PTB)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">83.2</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">69.1</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">10 M</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">70.7</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: left;">RHN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">79.5</td>
</tr>
<tr>
<td style="text-align: left;">NAS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">75.9</td>
</tr>
<tr>
<td style="text-align: left;">LSTM (tuned for PTB)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">76.3</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">65.9</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">24 M</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">65.9</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">67.6</td>
</tr>
<tr>
<td style="text-align: left;">RHN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">75.6</td>
</tr>
<tr>
<td style="text-align: left;">NAS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">69.8</td>
</tr>
</tbody>
</table>
<p>Table 2: Validation and test set perplexities on Wikitext-2. All results are with shared input and output embeddings. $\dagger$ : parallel work.
favourably even to the Neural Cache Grave et al., 2016) whose innovations are fairly orthogonal to the base model.</p>
<p>Shallow LSTMs do especially well here. Deeper models have gradually degrading perplexity, with RHNs lagging all of them by a significant margin. NAS is not quite up there with the LSTM suggesting its architecture might have overfitted to Penn Treebank, but data for deeper variants would be necessary to draw this conclusion.</p>
<h1>6.3 ENWIK8</h1>
<p>In contrast to the previous datasets, our numbers on this task (reported in BPC, following convetion) are slightly off the state of the art. This is most likely due to optimisation being limited to 14 epochs which is about a tenth of what the model of Zilly et al. (2016) was trained for. Nevertheless, we match their smaller RHN with our models which are very close to each other. NAS lags the other models by a surprising margin at this task.</p>
<h2>7 ANALYSIS</h2>
<p>On two of the three datasets, we improved previous results substantially by careful model specification and hyperparameter optimisation, but the improvement for RHNs is much smaller compared to that for LSTMs. While it cannot be ruled out that our particular setup somehow favours LSTMs, we believe it is more likely that this effect arises due to the original RHN experimental condition having been tuned more extensively (this is nearly unavoidable during model development).</p>
<p>Naturally, NAS benefitted only to a limited degree from our tuning, since the numbers of Zoph \&amp; Le (2016) were already produced by employing similar regularisation methods and a grid search. The small edge can be attributed to the suboptimality of grid search (see Section 7.3).</p>
<p>In summary, the three recurrent cell architectures are closely matched on all three datasets, with minuscule differences on Enwik8 where regularisation matters the least. These results support the claims of Collins et al. (2016), that capacities of various cells are very similar and their apparent differences result from trainability and regularisation. While comparing three similar architectures cannot prove this point, the inclusion of NAS certainly gives it more credence. This way we have two of the best human designed and one machine optimised cell that was the top performer among thousands of candidates.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Depth</th>
<th style="text-align: center;">Valid</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Stacked LSTM, Graves (2013)</td>
<td style="text-align: center;">21 M</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.67</td>
</tr>
<tr>
<td style="text-align: left;">Grid LSTM, Kalchbrenner et al. (2015)</td>
<td style="text-align: center;">17 M</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.47</td>
</tr>
<tr>
<td style="text-align: left;">MI-LSTM, Wu et al. (2016)</td>
<td style="text-align: center;">17 M</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.44</td>
</tr>
<tr>
<td style="text-align: left;">LN HM-LSTM, Chung et al. (2016)</td>
<td style="text-align: center;">35 M</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.32</td>
</tr>
<tr>
<td style="text-align: left;">ByteNet, Kalchbrenner et al. (2016)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.31</td>
</tr>
<tr>
<td style="text-align: left;">VD RHN, Zilly et al. (2016)</td>
<td style="text-align: center;">23 M</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.31</td>
</tr>
<tr>
<td style="text-align: left;">VD RHN, Zilly et al. (2016)</td>
<td style="text-align: center;">21 M</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.30</td>
</tr>
<tr>
<td style="text-align: left;">VD RHN, Zilly et al. (2016)</td>
<td style="text-align: center;">46 M</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.27</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">27 M</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1.29</td>
<td style="text-align: center;">1.31</td>
</tr>
<tr>
<td style="text-align: left;">RHN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1.30</td>
<td style="text-align: center;">1.31</td>
</tr>
<tr>
<td style="text-align: left;">NAS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1.38</td>
<td style="text-align: center;">1.40</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">46 M</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1.28</td>
<td style="text-align: center;">1.30</td>
</tr>
<tr>
<td style="text-align: left;">RHN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1.29</td>
<td style="text-align: center;">1.30</td>
</tr>
<tr>
<td style="text-align: left;">NAS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1.32</td>
<td style="text-align: center;">1.33</td>
</tr>
</tbody>
</table>
<p>Table 3: Validation and test set BPCs on Enwik8 from the Hutter Prize dataset.</p>
<h1>7.1 The Effect of Individual Features</h1>
<p>Down-projection was found to be very beneficial by the tuner for some depth/budget combinations. On Penn Treebank, it improved results by about 2-5 perplexity points at depths 1 and 2 at 10M, and depth 1 at 24 M , possibly by equipping the recurrent cells with more capacity. The very same models benefited from down-projection on Wikitext-2, but even more so with gaps of about 10-18 points which is readily explained by the larger vocabulary size.</p>
<p>We further measured the contribution of other features of the models in a series of experiments. See Table 4. To limit the number of resource used, in these experiments only individual features were evaluated (not their combinations) on Penn Treebank at the best depth for each architecture (LSTM or RHN) and parameter budget ( 10 M or 24 M ) as determined above.</p>
<p>First, we untied input and output embeddings which made perplexities worse by about 6 points across the board which is consistent with the results of Inan et al. (2016).</p>
<p>Second, without variational dropout the RHN models suffer quite a bit since there remains no dropout at all in between the layers. The deep LSTM also sees a similar loss of perplexity as having intra-layer dropout does not in itself provide enough regularisation.</p>
<p>Third, we were also interested in how recurrent dropout (Semeniuta et al., 2016) would perform in lieu of variational dropout. Dropout masks were shared between time steps in both methods, and our results indicate no consistent advantage to either of them.</p>
<h3>7.2 Model Selection</h3>
<p>With a large number of hyperparameter combinations evaluated, the question of how much the tuner overfits arises. There are multiple sources of noise in play,
(a) non-deterministic ordering of floating-point operations in optimised linear algebra routines,
(b) different initialisation seeds,
(c) the validation and test sets being finite samples from a infinite population.</p>
<p>To assess the severity of these issues, we conducted the following experiment: models with the best hyperparameter settings for Penn Treebank and Wikitext-2 were retrained from scratch with various initialisation seeds and the validation and test scores were recorded. If during tuning, a model just got a lucky run due to a combination of (a) and (b), then retraining with the same hyperparameters but with different seeds would fail to reproduce the same good results.</p>
<p>There are a few notable things about the results. First, in our environment (Tensorflow with a single GPU) even with the same seed as the one used by the tuner, the effect of (a) is almost as large as that of (a) and (b) combined. Second, the variance induced by (a) and (b) together is roughly equivalent to an absolute difference of 0.4 in perplexity on Penn Treebank and 0.5 on Wikitext-2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Size 10M</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Size 24M</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Depth</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Depth</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr>
<td style="text-align: left;">- Shared Embeddings</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">63.2</td>
</tr>
<tr>
<td style="text-align: left;">- Variational Dropout</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">64.5</td>
</tr>
<tr>
<td style="text-align: left;">+ Recurrent Dropout</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">62.9</td>
</tr>
<tr>
<td style="text-align: left;">+ Untied gates</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">61.3</td>
</tr>
<tr>
<td style="text-align: left;">+ Tied gates</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">58.0</td>
</tr>
<tr>
<td style="text-align: left;">RHN</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">62.2</td>
</tr>
<tr>
<td style="text-align: left;">- Shared Embeddings</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">64.6</td>
</tr>
<tr>
<td style="text-align: left;">- Variational Dropout</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: left;">+ Recurrent Dropout</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">61.0</td>
</tr>
</tbody>
</table>
<p>Table 4: Validation and test set perplexities on Penn Treebank for variants of our best LSTM and RHN models of two sizes.</p>
<p>Third, the validation perplexities of the best checkpoints are about one standard deviation lower than the sample mean of the reruns, so the tuner could fit the noise only to a limited degree.</p>
<p>Because we treat our corpora as a single sequence, test set contents are not i.i.d., and we cannot apply techniques such as the bootstrap to assess (c). Instead, we looked at the gap between validation and test scores as a proxy and observed that it is very stable, contributing variance of $0.12-0.3$ perplexity to the final results on Penn Treebank and Wikitext-2, respectively.</p>
<p>We have not explicitly dealt with the unknown uncertainty remaining in the Gaussian Process that may affect model comparisons, apart from running it until apparent convergence. All in all, our findings suggest that a gap in perplexity of 1.0 is a statistically robust difference between models trained in this way on these datasets. The distribution of results was approximately normal with roughly the same variance for all models, so we still report numbers in a tabular form instead of plotting the distribution of results, for example in a violin plot (Hintze \&amp; Nelson, 1998).</p>
<h1>7.3 SENSITIVITY</h1>
<p>To further verify that the best hyperparameter setting found by the tuner is not a fluke, we plotted the validation loss against the hyperparameter settings. Fig. 2 shows one such typical plot, for a 4-layer LSTM. We manually restricted the ranges around the best hyperparameter values to around 15-25\% of the entire tuneable range, and observed that the vast majority of settings in that neighbourhood produced perplexities within 3.0 of the best value. Widening the ranges further leads to quickly deteriorating results.</p>
<p>Satisfied that the hyperparameter surface is well behaved, we considered whether the same results could have possibly been achieved with a simple grid search. Omitting input embedding ratio because the tuner found having a down-projection suboptimal almost non-conditionally for this model, there remain six hyperparameters to tune. If there were 5 possible values on the grid for each hyperparameter (with one value in every $20 \%$ interval), then we would need $6^{5}$, nearly 8000 trials to get within 3.0 of the best perplexity achieved by the tuner in about 1500 trials.</p>
<h3>7.4 TYING LSTM GATES</h3>
<p>Normally, LSTMs have two independent gates controlling the retention of cell state and the admission of updates (Eq. 1). A minor variant which reduces the number of parameters at the loss of some flexibility is to tie the input and forget gates as in Eq. 2. A possible middle ground that keeps the number of parameters the same but ensures that values of the cell state $c$ remain in $[-1,1]$ is to cap</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Average per-word negative log-likelihoods of hyperparameter combinations in the neighbourhood of the best solution for a 4-layer LSTM with 24M weights on the Penn Treebank dataset.
the input gate as in Eq. 3.</p>
<p>$$
\begin{aligned}
&amp; \mathbf{c}<em t="t">{t}=\mathbf{f}</em>} \odot \mathbf{c<em t="t">{t-1}+\mathbf{i}</em>} \odot \mathbf{j<em t="t">{t} \
&amp; \mathbf{c}</em>}=\mathbf{f<em t-1="t-1">{t} \odot \mathbf{c}</em>}+\left(1-\mathbf{f<em t="t">{t}\right) \odot \mathbf{j}</em> \
&amp; \mathbf{c}<em t="t">{t}=\mathbf{f}</em>} \odot \mathbf{c<em t="t">{t-1}+\min \left(1-\mathbf{f}</em>}, \mathbf{i<em t="t">{t}\right) \odot \mathbf{j}</em>
\end{aligned}
$$</p>
<p>Where the equations are based on the formulation of Sak et al. (2014). All LSTM models in this paper use the third variant, except those titled "Untied gates" and "Tied gates" in Table 4 corresponding to Eq. 1 and 2, respectively.</p>
<p>The results show that LSTMs are insensitive to these changes and the results vary only slightly even though more hidden units are allocated to the tied version to fill its parameter budget. Finally, the numbers suggest that deep LSTMs benefit from bounded cell states.</p>
<h1>8 CONCLUSION</h1>
<p>During the transitional period when deep neural language models began to supplant their shallower predecessors, effect sizes tended to be large, and robust conclusions about the value of the modelling innovations could be made, even in the presence of poorly controlled "hyperparameter noise." However, now that the neural revolution is in full swing, researchers must often compare competing deep architectures. In this regime, effect sizes tend to be much smaller, and more methodological care is required to produce reliable results. Furthermore, with so much work carried out in parallel by a growing research community, the costs of faulty conclusions are increased.</p>
<p>Although we can draw attention to this problem, this paper does not offer a practical methodological solution beyond establishing reliable baselines that can be the benchmarks for subsequent work. Still, we demonstrate how, with a huge amount of computation, noise levels of various origins can be carefully estimated and models meaningfully compared. This apparent tradeoff between the amount of computation and the reliability of results seems to lie at the heart of the matter. Solutions to the methodological challenges must therefore make model evaluation cheaper by, for instance, reducing the number of hyperparameters and the sensitivity of models to them, employing better hyperparameter optimisation strategies, or by defining "leagues" with predefined computational budgets for a single model representing different points on the tradeoff curve.</p>
<h2>REFERENCES</h2>
<p>Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. CoRR, abs/1609.01704, 2016. URL http://arxiv.org/abs/1609.01704.</p>
<p>Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo. Capacity and trainability in recurrent neural networks. arXiv preprint arXiv:1611.09913, 2016.</p>
<p>Thomas Desautels, Andreas Krause, and Joel W. Burdick. Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization. Journal of Machine Learning Research, 15: 4053-4103, 2014. URL http://jmlr.org/papers/v15/desautels14a.html.</p>
<p>Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 1019-1027, 2016.</p>
<p>Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Sculley. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1487-1495. ACM, 2017.</p>
<p>Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. CoRR, abs/1612.04426, 2016. URL http://arxiv.org/abs/1612. 04426 .</p>
<p>Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL http://arxiv.org/abs/1308.0850.</p>
<p>Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017.</p>
<p>Jerry L Hintze and Ray D Nelson. Violin plots: a box plot-density trace synergism. The American Statistician, 52(2):181-184, 1998.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9 (8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.doi.org/10.1162/neco.1997.9.8.1735.</p>
<p>Marcus Hutter. The human knowledge compression contest. 2012.
Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. CoRR, abs/1611.01462, 2016. URL http://arxiv. org/abs/1611.01462.</p>
<p>Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. CoRR, abs/1507.01526, 2015. URL http://arxiv.org/abs/1507.01526.</p>
<p>Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. CoRR, abs/1610.10099, 2016. URL http://arxiv.org/abs/1610.10099.</p>
<p>Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural sequence models. arXiv preprint arXiv:1709.07432, 2017.</p>
<p>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The Penn treebank. Computational linguistics, 19(2):313-330, 1993.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843.</p>
<p>Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. CoRR, abs/1708.02182, 2017. URL http://arxiv.org/abs/1708. 02182 .</p>
<p>Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, volume 2, pp. 3, 2010.</p>
<p>Ofir Press and Lior Wolf. Using the output embedding to improve language models. CoRR, abs/1608.05859, 2016. URL http://arxiv.org/abs/1608.05859.</p>
<p>Nils Reimers and Iryna Gurevych. Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging. CoRR, abs/1707.09861, 2017. URL http: / / arxiv.org/abs/1707.09861.</p>
<p>Hasim Sak, Andrew W. Senior, and Françoise Beaufays. Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition. CoRR, abs/1402.1128, 2014. URL http://arxiv.org/abs/1402.1128.</p>
<p>Stanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth. Recurrent dropout without memory loss. CoRR, abs/1603.05118, 2016. URL http://arxiv.org/abs/1603.05118.</p>
<p>Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan Salakhutdinov. On multiplicative integration with recurrent neural networks. CoRR, abs/1606.06630, 2016. URL http://arxiv.org/abs/1606.06630.</p>
<p>Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. CoRR, abs/1409.2329, 2014. URL http://arxiv.org/abs/1409.2329.</p>
<p>Julian G. Zilly, Rupesh Kumar Srivastava, Jan Koutník, and Jürgen Schmidhuber. Recurrent highway networks. CoRR, abs/1607.03474, 2016. URL http://arxiv.org/abs/1607. 03474 .</p>
<p>Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Of the two parameterisations, we used the one in which there is further sharing of masks between gates rather than independent noise for the gates.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
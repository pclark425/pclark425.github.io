<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6948 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6948</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6948</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-264590585</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.18679v1.pdf" target="_blank">N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics</a></p>
                <p><strong>Paper Abstract:</strong> We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model’s own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6948.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6948.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>N-CRITICS (Factual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics (factual-hallucination experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative, model-agnostic self-refinement method that sends a generator output to an ensemble of (open-source) LLM critics, aggregates their critiques, and uses that feedback to prompt the generator to produce a refined answer; repeated until a stopping criterion or max iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-70b (generator); WizardLM-70b and LLaMA-70b (critics)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer-based LLaMA-family models; LLaMA-70b used as the primary generator; WizardLM-70b and LLaMA-70b used as critics for the factual experiments (paper reports that smaller models like Koala-13b/Vicuna-13b did not help for this task).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>N-CRITICS (ensemble critique / self-refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate initial answer; send generated output (and input) to an ensemble of LLM critics (including the generator optionally); collect and aggregate critiques (evidence, error points); use aggregated feedback to rewrite/refine the prompt and regenerate the answer; repeat iteratively until max refinements or early-stop rule (no change for two consecutive corrections).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (iterative external-critic feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Factual-hallucination reduction (TriviaQA, AmbigNQ, HotpotQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain and multi-hop QA datasets used to measure factual accuracy / hallucinations in generated answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact Match (EM) and F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Reported aggregate numbers in paper: N-CRITICS reported numeric results as: 78.02, 84.67, 50.93, 62.54, 43.13, 52.56 (presented in the paper's Table 2 as the post-refinement EM/F1 values across the three datasets; paper text states N-CRITICS outperforms baselines on F1 across all three datasets and improves EM on TriviaQA and HotpotQA).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Critiques from smaller models (Koala-13b, Vicuna-13b) did not improve and sometimes reduced quality for factual tasks, so the method depended on larger critic models; early-stop rule used when answer unchanged for two consecutive corrections. Also computational overhead from multiple large-model calls.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6948.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6948.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>N-CRITICS (Toxicity)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics (toxicity-reduction experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same ensemble-critics iterative refinement framework applied to reduce toxicity of generations by having critics identify toxic elements and instruct the generator to produce a safer rewrite.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Koala-13b, Vicuna-13b, Wizard-13b (generators/critics used in toxicity experiments); Perspective API used as toxicity scorer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source chat models (13B parameter class) used as generation models and as part of the critic ensemble; Perspective API used as an external toxicity-evaluation tool.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B (Koala, Vicuna, Wizard variants used in toxicity experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>N-CRITICS (ensemble critique for detoxification)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial continuation; send generated text to an ensemble of LLM critics and to the Perspective toxicity API to receive toxicity labels/percentages and feedback; use aggregated feedback to prompt the generator to produce a healthier rewrite; iterate until toxicity falls below threshold or max iterations reached.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (iterative external-critic feedback with external toxicology scorer)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Toxicity reduction (REALTOXICITYPROMPTS subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detoxification measured on 1,700 prompts sampled from REALTOXICITYPROMPTS to evaluate whether iterative critique and refinement reduce toxic continuations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Mean toxicity score (Perspective API); additionally diversity metrics dist-2 and dist-3 reported for lexical diversity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Paper reports substantial reduction in toxicity; example refined output had a new toxicity score of 0.035 (Perspective API) in the manuscript; Table 1 in paper shows N-CRITICS outperforms other methods/proprietary LLMs on mean toxicity (exact baseline numbers not reproducibly presented in plain text).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Diminishing returns: improvements plateau beyond four critics; computational overhead from calling multiple LLM critics; stopping criterion used (toxicity < 10% or max iterations). Dependence on Perspective API as external scorer and on the quality of open-source critic models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6948.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6948.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (Madaan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-correction approach where the same pre-trained LLM both generates outputs and generates feedback for its own prior outputs; feedback is used to prompt the model to refine answers without external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model generates an answer, then the same model generates critiques of that answer; the critiques are fed back to the model to produce a refined answer; iterative self-feedback loop.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (self-generated feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned in related work as an approach that automates refinement without human intervention; paper cites it but does not evaluate it directly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6948.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6948.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion (Shinn et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-improvement framework that pairs model-generated feedback with an episodic memory buffer to avoid repeating past errors and improve future performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (episodic memory + self-refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model produces outputs and receives verbal feedback; an episodic memory buffer stores past feedback and outputs to prevent repeating mistakes and to guide further refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect with memory</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned in related work; not directly evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6948.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6948.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRITIC (Gou et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses tool-interactive critics (e.g., search, calculators, code interpreters) to generate critiques about a model's output which are then used to refine the original output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>CRITIC (tool-interactive critiquing)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use specialized external tools (search, calculators, code execution, etc.) to generate diagnostic feedback for a model's output; feed that feedback to the generator to produce corrected outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (external-tool critiques)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Referenced and compared to in paper; authors used CRITIC's reported numbers for comparison but did not run CRITIC themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Refine: Iterative Refinement with Self-Feedback <em>(Rating: 2)</em></li>
                <li>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-Edit: Fault-aware code editor for code generation <em>(Rating: 1)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6948",
    "paper_id": "paper-264590585",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "N-CRITICS (Factual)",
            "name_full": "N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics (factual-hallucination experiments)",
            "brief_description": "An iterative, model-agnostic self-refinement method that sends a generator output to an ensemble of (open-source) LLM critics, aggregates their critiques, and uses that feedback to prompt the generator to produce a refined answer; repeated until a stopping criterion or max iterations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-70b (generator); WizardLM-70b and LLaMA-70b (critics)",
            "model_description": "Open-source transformer-based LLaMA-family models; LLaMA-70b used as the primary generator; WizardLM-70b and LLaMA-70b used as critics for the factual experiments (paper reports that smaller models like Koala-13b/Vicuna-13b did not help for this task).",
            "model_size": "70B",
            "reflection_method_name": "N-CRITICS (ensemble critique / self-refinement)",
            "reflection_method_description": "Generate initial answer; send generated output (and input) to an ensemble of LLM critics (including the generator optionally); collect and aggregate critiques (evidence, error points); use aggregated feedback to rewrite/refine the prompt and regenerate the answer; repeat iteratively until max refinements or early-stop rule (no change for two consecutive corrections).",
            "iteration_type": "generate-then-reflect (iterative external-critic feedback)",
            "num_iterations": 3,
            "task_name": "Factual-hallucination reduction (TriviaQA, AmbigNQ, HotpotQA)",
            "task_description": "Open-domain and multi-hop QA datasets used to measure factual accuracy / hallucinations in generated answers.",
            "evaluation_metric": "Exact Match (EM) and F1",
            "performance_before_reflection": null,
            "performance_after_reflection": "Reported aggregate numbers in paper: N-CRITICS reported numeric results as: 78.02, 84.67, 50.93, 62.54, 43.13, 52.56 (presented in the paper's Table 2 as the post-refinement EM/F1 values across the three datasets; paper text states N-CRITICS outperforms baselines on F1 across all three datasets and improves EM on TriviaQA and HotpotQA).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Critiques from smaller models (Koala-13b, Vicuna-13b) did not improve and sometimes reduced quality for factual tasks, so the method depended on larger critic models; early-stop rule used when answer unchanged for two consecutive corrections. Also computational overhead from multiple large-model calls.",
            "uuid": "e6948.0",
            "source_info": {
                "paper_title": "N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "N-CRITICS (Toxicity)",
            "name_full": "N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics (toxicity-reduction experiments)",
            "brief_description": "Same ensemble-critics iterative refinement framework applied to reduce toxicity of generations by having critics identify toxic elements and instruct the generator to produce a safer rewrite.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Koala-13b, Vicuna-13b, Wizard-13b (generators/critics used in toxicity experiments); Perspective API used as toxicity scorer",
            "model_description": "Open-source chat models (13B parameter class) used as generation models and as part of the critic ensemble; Perspective API used as an external toxicity-evaluation tool.",
            "model_size": "13B (Koala, Vicuna, Wizard variants used in toxicity experiments)",
            "reflection_method_name": "N-CRITICS (ensemble critique for detoxification)",
            "reflection_method_description": "Generate an initial continuation; send generated text to an ensemble of LLM critics and to the Perspective toxicity API to receive toxicity labels/percentages and feedback; use aggregated feedback to prompt the generator to produce a healthier rewrite; iterate until toxicity falls below threshold or max iterations reached.",
            "iteration_type": "generate-then-reflect (iterative external-critic feedback with external toxicology scorer)",
            "num_iterations": 4,
            "task_name": "Toxicity reduction (REALTOXICITYPROMPTS subset)",
            "task_description": "Detoxification measured on 1,700 prompts sampled from REALTOXICITYPROMPTS to evaluate whether iterative critique and refinement reduce toxic continuations.",
            "evaluation_metric": "Mean toxicity score (Perspective API); additionally diversity metrics dist-2 and dist-3 reported for lexical diversity",
            "performance_before_reflection": null,
            "performance_after_reflection": "Paper reports substantial reduction in toxicity; example refined output had a new toxicity score of 0.035 (Perspective API) in the manuscript; Table 1 in paper shows N-CRITICS outperforms other methods/proprietary LLMs on mean toxicity (exact baseline numbers not reproducibly presented in plain text).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Diminishing returns: improvements plateau beyond four critics; computational overhead from calling multiple LLM critics; stopping criterion used (toxicity &lt; 10% or max iterations). Dependence on Perspective API as external scorer and on the quality of open-source critic models.",
            "uuid": "e6948.1",
            "source_info": {
                "paper_title": "N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-Refine (Madaan et al.)",
            "name_full": "Self-Refine: Iterative Refinement with Self-Feedback",
            "brief_description": "An iterative self-correction approach where the same pre-trained LLM both generates outputs and generates feedback for its own prior outputs; feedback is used to prompt the model to refine answers without external tools.",
            "citation_title": "Self-Refine: Iterative Refinement with Self-Feedback.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Self-Refine (self-feedback)",
            "reflection_method_description": "Model generates an answer, then the same model generates critiques of that answer; the critiques are fed back to the model to produce a refined answer; iterative self-feedback loop.",
            "iteration_type": "generate-then-reflect (self-generated feedback)",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned in related work as an approach that automates refinement without human intervention; paper cites it but does not evaluate it directly.",
            "uuid": "e6948.2",
            "source_info": {
                "paper_title": "N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Reflexion (Shinn et al.)",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "An iterative self-improvement framework that pairs model-generated feedback with an episodic memory buffer to avoid repeating past errors and improve future performance.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Reflexion (episodic memory + self-refinement)",
            "reflection_method_description": "Model produces outputs and receives verbal feedback; an episodic memory buffer stores past feedback and outputs to prevent repeating mistakes and to guide further refinements.",
            "iteration_type": "generate-then-reflect with memory",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned in related work; not directly evaluated in this paper.",
            "uuid": "e6948.3",
            "source_info": {
                "paper_title": "N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CRITIC (Gou et al.)",
            "name_full": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
            "brief_description": "A method that uses tool-interactive critics (e.g., search, calculators, code interpreters) to generate critiques about a model's output which are then used to refine the original output.",
            "citation_title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "CRITIC (tool-interactive critiquing)",
            "reflection_method_description": "Use specialized external tools (search, calculators, code execution, etc.) to generate diagnostic feedback for a model's output; feed that feedback to the generator to produce corrected outputs.",
            "iteration_type": "generate-then-reflect (external-tool critiques)",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Referenced and compared to in paper; authors used CRITIC's reported numbers for comparison but did not run CRITIC themselves.",
            "uuid": "e6948.4",
            "source_info": {
                "paper_title": "N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
            "rating": 2,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Self-Edit: Fault-aware code editor for code generation",
            "rating": 1,
            "sanitized_title": "selfedit_faultaware_code_editor_for_code_generation"
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 1,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        }
    ],
    "cost": 0.011664250000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics</p>
<p>Sajad Mousavi 
Ricardo Luna Gutierrez 
Desik Rengarajan 
AshwinVineet Gundecha 
Ramesh Babu 
Avisek Naug 
Antonio Guillen 
Soumyendu Sarkar 
N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics
18FEF6B2F78FB7DA8ED265E656DE3885
We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination.This method involves refining model outputs through an ensemble of critics and the model's own feedback.Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics.Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns.We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.</p>
<p>Introduction</p>
<p>The recent impressive results achieved by LLMs have led to a substantial surge in their utilization and investigation (OpenAI, 2023;Touvron et al., 2023;Chowdhery et al., 2022;Zhao et al., 2023).However, as a consequence of this heightened exposure, it is important to ensure their accuracy and safety.This concern is particularly significant in light of the demonstrated potential for LLMs to exhibit unfaithful and toxic behavior (Deshpande et al., 2023;Lin et al., 2022;Maynez et al., 2020).</p>
<p>Various methods have been proposed to tackle this problem (Pan et al., 2023).From training-time correction (Xu et al., 2023;Liu and Liu, 2021;Li et al., 2019;Jauregi Unanue et al., 2021;Zelikman et al., 2022;Huang et al., 2022) to post output generation refinement (Madaan et al., 2023;Shinn et al., 2023;Zhang et al., 2023;Pan et al., 2023;Du et al., 2022;Yu et al., 2023;Gou et al., 2023;Paul et al., 2023;Le et al., 2022;Akyurek et al., 2023), these methods have shown the impact that iterative self-refinement and proper feedback can have on the performance of LLMs.</p>
<p>While numerous prior research endeavors have focused on iterative feedback, generated from external tools or the LLM itself, there exists a gap in the exploration of the potential benefits arising from an ensemble of distinct general-purpose Large Language Models (LLMs).Such an ensemble could evaluate LLM-generated output and subsequently offer feedback, which can help to reduce toxicity and rectify factual errors.</p>
<p>Taking inspiration from human behavior, where seeking feedback from others is a common practice for improvement, we introduce N-CRITICS, a self-correction framework designed for LLMs.N-CRITICS leverages an ensemble of critics, each represented by a distinct LLM, which can provide new evidence or feedback to correct the reasoning of the main generation model.In our framework, the</p>
<p>Input BlackBox LLM</p>
<p>Ouput</p>
<p>Refine</p>
<p>Self Critique Feedback generated output of an LLM is supplied to the ensemble of critics, and subsequently, we aggregate the critiques provided by the ensemble.These collected critiques are then used to prompt the generator to regenerate the output, taking into consideration the feedback received.This iterative process is repeated for a predefined number of cycles or until further refinement is deemed unnecessary.Figure 1 shows our overall approach.</p>
<p>Contrary to previous approaches, N-CRITICS is built on open-source models and does not rely on proprietary models and tools such as GPT (OpenAI, 2023) or Google search.We evaluate N-CRITICS on the REALTOXICITYPROMPTS (Gehman et al., 2020) dataset for toxicity.The AmbigNQ (Min et al., 2020), TriviaQA (Joshi et al., 2017) and HotpotQA (Yang et al., 2018) datasets were used to test factual hallucination.We show that N-CRITICS is able to improve the original output of the LLM model, increasing its accuracy and reducing toxicity.</p>
<p>Figure 1: Workflow of the N-CRITICS approach.The process begins with initializing the initial context and output, after which feedback is generated by engaging with an ensemble of opensource LLMs.This feedback then guides the refinement of the output through iterative steps until a satisfactory result is achieved or a stopping criterion is met.</p>
<p>Related Work</p>
<p>Refinements of LLM outputs can be achieved through various methods, involving both human and machine-generated verification and feedback.One effective technique is Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), which fine-tunes LLMs to generate more compelling outputs for human consumption.Similarly, the use of iterative human feedback to refine model outputs has been explored in Du et al. (2022).While these approaches have been shown to improve the performance of LLM models, they rely on human intervention, which can be expensive and time-consuming.</p>
<p>To address this limitation, alternative automated approaches that do not require human intervention during the refinement process have been proposed (Pan et al., 2023).For instance, Self-Refine (Madaan et al., 2023) introduced an approach that involves utilizing a pre-trained Large Language Model (LLM) for comprehensive end-to-end self-correction.In this approach, the same LLM serves a dual role, both as the generator of output and as the provider of feedback.This generated feedback is subsequently employed to prompt the model to iteratively refine its previous output based on the received feedback.Similarly, Reflexion (Shinn et al., 2023) adopts iterative self-correction and proposes the incorporation of an episodic memory buffer to enhance performance.This memory buffer stores past feedback and corresponding model outputs with the aim of preventing the repetition of previous errors.</p>
<p>On the other hand, employing external tools as sources of feedback has also been a subject of exploration.Self-Edit (Zhang et al., 2023) executes code generated by an LLM in a code interpreter and provides the execution results as feedback.Logic-LM (Pan et al., 2023) proposes to address logical reasoning through a two-step process.Firstly, an LLM translates a natural language problem into a symbolic formulation, and subsequently, a deterministic symbolic solver is deployed to conduct inference on this formulation.The error messages returned by the solver are harnessed as feedback to refine the LLM-generated output.RARR (Du et al., 2022) and REFEED (Yu et al., 2023) leverage an external corpus of collected documents to look for evidence that corroborates or contradicts the outputs generated by the models.This acquired evidence is subsequently employed for model refinement.CRITIC (Gou et al., 2023) proposes the use of a suite of specialized tools for a variety of tasks such as code interpreters, calculators, or search engines to generate critics for the LLM's generated output.Moreover, approaches such as REFINER (Paul et al., 2023), CodeRL (Le et al., 2022) and RL4F (Akyurek et al., 2023) propose to train a specialized critic to provide feedback to the generator model.</p>
<p>In contrast to these approaches, our approach harnesses the collective knowledge of multiple generalpurpose LLMs, including the primary LLM in use.Moreover, in our work, we only use models that are part of the open-source community, which differ from many of the previous work which uses proprietary generation models and feedback tools.Our ensemble-based strategy enables automatic LLM output correction across various domains via an iterative process of feedback and correction, without training or fine-tuning, showcasing the versatility and effectiveness of our approach.</p>
<p>Algorithm 1 N-CRITICS algorithm end if 8: Refine the input prompt based on critiques to form x ′ .9: Obtain the improved output yi+1 by feeding x ′ to M. 10: Increment i by 1. 11: end while
Input: Prompt x, primary LLM model M, ensemble of external LLMs L = [L1, L2, L3, . . . ,</p>
<p>N-CRITICS: Ensemble of Critics</p>
<p>The capacity for LLMs to emulate the human self-correction process poses an engaging research query.Humans typically use a dual strategy: self-reflection and seeking external feedback when grappling with intricate thoughts or texts.Although LLMs have advanced impressively, their initial outputs often require improvement.Our proposal, inspired by the human text refinement process, presents two main strategies: self-reflection and critiquing.The overarching objective is to elevate the quality of LLM-generated content.An outline of our proposed framework, N-CRITICS, is depicted in Figure 1.</p>
<p>The methodology operates on an iterative feedback mechanism that doesn't require supervised training.Let x be a given input, the LLM, denoted as M, produces an initial output y0.This output y0 is then circulated through M itself and other LLMs for evaluation.The feedback obtained from the ensemble of critics is compiled, guiding M to refine its output into yi, where i is the iteration count.The iterations persist until a specific stopping criterion, relevant to the given task, is fulfilled.The N-CRITICS approach is elaborated in Algorithm 1.</p>
<p>Setting the Initial Context and Output: Our methodology draws from existing self-correction strategies (Pan et al., 2023), implementing chain-of-thought reasoning combined with few-shot and zero-shot prompting for in-context learning.For a given input x, a set of handcrafted examples are combined into the input prompt, directing the LLMs to yield an initial output y0.Depending on the nature of the task, the zero-shot strategy might be incorporated, enhancing the input prompt with "Let's think step by step" (Kojima et al., 2022).An example of such prompts, used during the refinement for the factual hallucination and toxicity reduction tasks, can be observed in Figure 2.</p>
<p>Feedback Generation from Ensemble of Critics: For feedback procurement, we harness multiple open-source LLMs.With the input x and its corresponding initial output y0, the primary model M interacts with other LLMs to evaluate y0.This feedback mechanism is tailored according to the specific task.Figure 2 displays instances of the prompts and the feedback acquired during the refinement for tasks like toxicity reduction and factual hallucination.</p>
<p>Output Correction via Feedback:</p>
<p>The critiquing strategy tackles the noted inconsistencies and issues prevalent in LLM outputs.It mirrors the human practices of consulting experts or tools to assess and improve upon initial drafts.The critique process starts with the LLM's initial output, which is then assessed by appropriate tools to evaluate its various dimensions.Feedback from this assessment is used to revise the output.This iterative mechanism, leveraging both introspective and external feedback, fine-tunes LLM-generated content, bridging the gap between machine-generated and human-curated content.</p>
<p>Experiments and Results</p>
<p>We evaluate N-CRITICS on two distinct tasks: Toxicity reduction, which focuses on improving the overall health (and safety) of the LLM model output, and factual hallucination reduction, which ensures the accuracy of generated content.For our assessments, we leverage several open-sourced base LLMs, which include: LLaMA-70b (Touvron et al., 2023): A widely recognized LLM by Meta., WizardLM-70b and 13b (Xu et al., 2023): A variant of LLaMA trained with the innovative Evol-Instruct method on intricate instruction data., Koala-13b (Geng et al., 2023): This model is fine-tuned on dialogue data extracted from the web and utilizes Meta's LLaMA as its foundation., and Vicuna-13b (Chiang et al., 2023): An open-sourced chatbot that's been fine-tuned using conversations from ShareGPT, a platform where users share their ChatGPT dialogues.</p>
<p>In our experiments, we consistently set the temperature parameter to p = 0.7 to encourage diverse outputs.We also cap the number of iterations, itr, at 4. Notably, we present the results of previous state-of-the-art approaches as originally reported.Replicating their outcomes would necessitate extensive training and inference using LLMs.Furthermore, the APIs for their LLM models are not freely accessible.</p>
<p>Factual Hallucination</p>
<p>Q: What was advertised with Eva Herzagovia using the slogan hello boys?A: Let's think step by step.Eva Herzagovia was advertised with the slogan hello boys.So the answer is: Eva Herzagovia.(Main Model Original Answer)</p>
<p>Is there any problem with answer 'A:'?Let's ask model 1: &gt; Q: What was advertised with Eva Herzagovia using the slogan hello boys?&gt; A: Let's think step by step.Eva Herzagovia was advertised with the slogan hello boys.So the answer is: Eva Herzagovia.</p>
<p>Feedback:</p>
<p>No, the answer is not correct.The correct answer should be the product or service that Eva Herzagovia was promoting with the slogan "hello boys," not Eva Herzagovia herself.(Model 1 Feedback)</p>
<p>Let's ask model 2: &gt; Q: What was advertised with Eva Herzagovia using the slogan hello boys?&gt; A: Let's think step by step.Eva Herzagovia was advertised with the slogan hello boys.So the answer is: Eva Herzagovia.</p>
<p>Feedback: No, the answer provided is not factually correct.Eva Herzagovia was not advertised with the slogan "hello boys."The correct answer is Wonderbra, as she was featured in a Wonderbra ad campaign with that slogan.(Model 2 Feedback)</p>
<p>Given the previous feedback from model 1 and model 2, revisit your previous answer.</p>
<p>Question: What was advertised with Eva Herzagovia using the slogan hello boys?</p>
<p>The product that was advertised with Eva Herzagovia using the slogan "helloboys" was Wonderbra.So the answer is: Wonderbra.(Main Model Refined Answer)</p>
<p>Toxicity Reduction</p>
<p>The input: "Your call is very important to us..." we are told, by automated voices that don't give a</p>
<p>The current toxic text continuation of input: damn about our frustrations or our time, just like the endless hold music that never seems to end, it's all a part of the modern customer service experience.(Main Model Original Answer)</p>
<p>The feedback: The current toxic text continuation has 57% toxicity of PROFANITY.Also, another oracle says that the text is more closely associated with Profanity, with 45%.Additionally, another source says that the text is closely associated with the category of Profanity.Also, another augur says that the text "damn about our frustrations or our time, just like the endless hold music that never seems to end, it's all a part of the modern customer service experience" can be categorized as "Toxicity" with a percentage of 25%.(Model 1,2, 3 and Main Model Feedback)</p>
<p>Considering the mentioned feedback, give a new healthier, safer, and more fluent continuation that does not contain toxicity, insult, profanity, identity attack, threats sexually explicit, etc. Ensure not to repeat the provided input text.</p>
<p>Start the new continuation only after "--&gt;" and ensure not repeat the previous toxic continuation text: "Your call is very important to us..." we are told, by automated voices that don't give a--&gt; thought to our frustrations or the value of our time.Instead, they guide us through an endless hold music experience, just like the never-ending loop of an eternal wait.This is the modern customer service experience, where every interaction is designed to make us feel valued and app reciated.(Main Model Refined Answer) New toxicity: 0.035 Toxicity Reduction: We evaluate the capacity of our proposed method, N-CRITICS, to mitigate potential toxicity in LLM-generated content.Using a random sample of 1,700 prompts from the non-toxic section of REALTOXICITYPROMPTS (Gehman et al., 2020)-a dataset deliberately designed to elicit toxic responses-we gauge the effectiveness of our technique.For a comprehensive assessment of toxicity levels, we employ the PERSPECTIVE API 2 in conjunction with three notable chatbots: Koala-13 (Geng et al., 2023), Vicuna-13 (Chiang et al., 2023), and Wizard-13b (Xu et al., 2023).To evaluate our results, we compute the mean toxicity across all the finalized outputs and present both dist-2 and dist-3 scores, representing the uniqueness of bigrams and trigrams respectively.The refinement process is halted either when the content's overall toxicity drops below 10% or when the maximum iteration limit is reached.</p>
<p>Table 1 showcases the performance of N-CRITICS relative to other leading methods.Evidently, our approach substantially reduces the toxicity levels in the LLM-generated outputs while ensuring diversity.Remarkably, N-CRITICS outperforms proprietary LLMs such as ChatGPT and supervised methods that necessitate training steps and data.In addition, we examined the impact of iterative correction as well as the number of LLMs as critics.Figure 3 traces the detoxification trajectory across iterations for varying numbers of critics.It is observable that as both the iteration frequency and the number of critics rise, the toxicity diminishes.However, beyond four critics, the improvement plateaus, suggesting the LLM might have tapped into its maximal capability for generating non-toxic content, regardless of additional feedback.</p>
<p>Factual Hallucination Reduction:</p>
<p>To assess the effectiveness of our proposed model in addressing the task of reducing factual hallucinations, we conducted experiments using three distinct datasets: TriviaQA (Joshi et al., 2017), AmbigNQ (Min et al., 2020) and HotpotQA (Yang et al., 2018).From each of these datasets, we randomly selected 400 samples for evaluation.We set a maximum of 3 refinement intentions.Moreover, as in shown CRITIC (Gou et al., 2023), we make an early stop in the refinement loop if the generated answer did not change for two consecutive corrections.We report the results of these experiments in terms of two key metrics: Exact Match (EM) and F1 scores.</p>
<p>The EM score is determined by assessing whether the model's prediction precisely matches the characters of the correct answer(s).If there is an exact match between the predicted output and the true answer(s), the EM score is set to 1; otherwise, it is assigned a value of 0.</p>
<p>We used Llama-70b (Touvron et al., 2023) as the base generator.For critics, we used WizardLM-70b (Xu et al., 2023) and Llama-70b.During the development of N-CRITICS, we found that the critiques from small models such as Koala-13b or Vicuna-13b did not help improve or even reduce the quality of the generated output for this task, hence the dependency on larger models used in our experimentation.</p>
<p>Table 2 shows the results obtained for these experiments.N-CRITICS effectively rectifies untruthful facts across all three distinct datasets.Moreover, N-CRITICS outperforms state-of-the-art methods in terms of F1 scores on all three datasets.For EM, N-CRITICS exhibits superior performance on TriviaQA and HotpotQA.Furthermore, we conducted an analysis to assess the impact of including additional critics and refinement iterations on the quality of the generated output.For this analysis we used TriviaQA.As illustrated in Figure 4, N-CRITICS achieves improvements in performance with the incorporation of additional critics and refinement iterations.(Gou et al., 2023).(Joshi et al., 2017) Limitations: Our work with N-CRITICS presents several noteworthy findings, yet it is not without its limitations.Dependence on Open-Sourced LLMs: A significant aspect of our method is its reliance on feedback from open-sourced LLMs.This means the quality and effectiveness of N-CRITICS are inherently tied to the caliber of these models.Shared biases or flaws among these LLMs could potentially influence the refined outputs.Nevertheless, with the rapid growth in the open-sourced LLM community, we believe some of these concerns may be alleviated in the future.</p>
<p>Computational Load: The iterative feedback and refinement process, especially when soliciting insights from multiple LLMs, can add computational overhead.As a consequence, the refinement process might experience delays, especially in resource-constrained environments.Language Specificity: Our current experiments predominantly revolve around English datasets.As such, the method's effectiveness and applicability in non-English contexts remain unknown.</p>
<p>N-Critic</p>
<p>F1</p>
<p>GPT-2 0.527 0.85 0.85 PPLM (Dathathri et al., 2019) 0.520 0.86 0.86 GeDi (Krause et al., 2021) 0.363 0.84 0.83 DEXPERT (Liu et al., 2021) 0.314 0.84 0.84 DAPT (Gururangan et al., 2020) 0.428 0.84 0.84 PPO (Lu et al., 2022) 0.218 0.79 0.82 QUARK (Lu et al., 2022) 0.196 0.80 0.84 Self-Correct (Welleck et al., 2022) 0</p>
<p>Conclusion</p>
<p>We introduced N-CRITICS, an innovative method leveraging feedback from open-sourced LLMs to iteratively refine model outputs, setting it apart from current self-refinement approaches (also, their underlying models are not free to use).Our evaluations across diverse tasks, ranging from hallucination and factual error mitigation to toxicity reduction, consistently underscore the merit of employing critiques from various LLMs to strengthen overall LLM performance.Looking ahead, we aim to broaden our evaluative lens to capture a wider array of errors, specifically those tied to flawed code and instances of unfaithful reasoning-where the conclusion strays from the established reasoning trajectory.While our current research predominantly centered on English datasets, a strategic expansion into multilingual tasks remains on our agenda as well.</p>
<p>Figure 2 :
2
Figure2: Left: Example prompt used for factual hallucination refinement in N-CRITICS.N-CRITICS evaluates the truthfulness and accuracy of a generated answer and collects feedback from its ensemble of critics.It utilizes this feedback to guide the generation of new improved output, with the ultimate goal of improving the truthfulness and accuracy of the response.Right: Example prompt for toxicity reduction with N-CRITICS.Feedback, identifying toxic elements in the output, is gathered from the ensemble (including the primary LLM) and used to guide the main LLM in mitigating such issues.</p>
<p>Figure 3 :
3
Figure 3: Left: Detoxification progress over iterations, Right: Variations in detoxification reduction across different critic numbers (0-4).</p>
<p>2 https://www.perspectiveapi.com/</p>
<p>Figure 4 :
4
Figure 4: Left: Factual hallucination reduction progress over iterations, Right: Variations in factual hallucination reduction across different critic numbers (0-2).Both evaluations were done using the TriviaQA dataset(Joshi et al., 2017)</p>
<p>Table 1 :
1
Toxicity reduction results.
N-Critic</p>
<p>Table 2 :
2
Factual hallucination reduction results.The results of the methods evaluated on ChatGPT are taken from CRITIC</p>
<p>.15 79.35 48.79 60.03 41.75 50.91
.1710.800.83ChatGPTChatGPT (OpenAI, 2023)0.3250.770.76CRITIC (Gou et al., 2023)0.1730.780.77Wizard-13BVanilla0.2130.937 0.892N-CRITICS0.0680.944 0.922Vanilla70.479.335.152.423.236.6CoT (Wei et al., 2022)72.979.244.258.633.746.1ReACT (Yao et al., 2023)63.769.847.661.234.947.9CRITIC (Gou et al., 2023) 75.181.750.064.938.750.5Llama-70bVanilla73
N-CRITICS 78.02 84.67 50.93 62.54 43.13 52.56</p>
<p>arXiv:2303.08774OpenAI, Gpt-4 technical report. 2023</p>
<p>H Touvron, T Lavril, G Izacard, X M M , -A L T L B R N G E H F A A R A J E Grave, G Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, P Schuh, K Shi, S Tsvyashchenko, J Maynez, A Rao, P Barnes, Y Tay, N Shazeer, V Prabhakaran, E Reif, N Du, B Hutchinson, R Pope, J Bradbury, J Austin, M Isard, G Gur-Ari, P Yin, T Duke, A Levskaya, S Ghemawat, S Dev, H Michalewski, X Garcia, V Misra, K Robinson, L Fedus, D Zhou, D Ippolito, D Luan, H Lim, B Zoph, A Spiridonov, R Sepassi, D Dohan, S Agrawal, M Omernick, arXiv:2204.02311Scaling language modeling with pathways. M Dai, T S Pillai, M Pellat, A Lewkowycz, E Moreira, R Child, O Polozov, K Lee, Z Zhou, X Wang, B Saeta, M Diaz, O Firat, M Catasta, J Wei, K Meier-Hellstern, D Eck, J Dean, Petrov, N. Fiedel, Palm2022</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J.-Y Nie, J.-R Wen, arXiv:2303.18223A survey of large language models. 2023</p>
<p>A Deshpande, V Murahari, T Rajpurohit, A Kalyan, K Narasimhan, arXiv:2304.05335Toxicity in chatgpt: Analyzing persona-assigned language models. 2023</p>
<p>Measuring how models mimic human falsehoods. S Lin, J Hilton, O Evans, Truthfulqa , arXiv:2109.079582022</p>
<p>On faithfulness and factuality in abstractive summarization. J Maynez, S Narayan, B Bohnet, R Mcdonald, 10.18653/v1/2020.acl-main.173Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020</p>
<p>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies. L Pan, M Saxon, W Xu, D Nathani, X Wang, W Y Wang, arXiv:2308.031882023</p>
<p>SESCORE2: Learning text generation evaluation via synthesizing realistic mistakes. W Xu, X Qian, M Wang, L Li, W Y Wang, 10.18653/v1/2023.acl-long.283Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>SimCLS: A simple framework for contrastive learning of abstractive summarization. Y Liu, P Liu, 10.18653/v1/2021.acl-short.135Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20212Short Papers)</p>
<p>Deep reinforcement learning with distributional semantic rewards for abstractive summarization. S Li, D Lei, P Qin, W Y Wang, 10.18653/v1/D19-1623Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>BERTTune: Fine-tuning neural machine translation with BERTScore. I Jauregi Unanue, J Parnell, M Piccardi, 10.18653/v1/2021.acl-short.115Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20212Short Papers)</p>
<p>Star: Bootstrapping reasoning with reasoning. E Zelikman, Y Wu, J Mu, N Goodman, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Large language models can self-improve. J Huang, S S Gu, L Hou, Y Wu, X Wang, H Yu, J Han, arXiv:2210.116102022</p>
<p>Self-Refine: Iterative Refinement with Self-Feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Gupta, B P Majumder, K Hermann, S Welleck, A Yazdanbakhsh, P Clark, arXiv:2303.176512023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, B Labash, A Gopinath, K Narasimhan, S Yao, arXiv:2303.113662023</p>
<p>Self-edit: Fault-aware code editor for code generation. K Zhang, Z Li, J Li, G Li, Z Jin, arXiv:2305.040872023</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. L Pan, A Albalak, X Wang, W Y Wang, arXiv:2305.122952023</p>
<p>Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision. W Du, Z M Kim, V Raheja, D Kumar, D Kang, Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants. the First Workshop on Intelligent and Interactive Writing AssistantsDublin, IrelandAssociation for Computational LinguisticsIn2Writing 2022. 2022</p>
<p>. 10.18653/v1/2022.in2writing-1.14</p>
<p>Improving language models via plug-and-play retrieval feedback. W Yu, Z Zhang, Z Liang, M Jiang, A Sabharwal, arXiv:2305.140022023</p>
<p>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. Z Gou, Z Shao, Y Gong, Y Shen, Y Yang, N Duan, W Chen, arXiv:2305.117382023</p>
<p>D Paul, M Ismayilzada, M Peyrard, B Borges, A Bosselut, R West, B Faltings, arXiv:2304.01904Refiner: Reasoning feedback on intermediate representations. 2023</p>
<p>Coderl: Mastering code generation through pretrained models and deep reinforcement learning. H Le, Y Wang, A D Gotmare, S Savarese, S C H Hoi, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>RL4F: Generating natural language feedback with reinforcement learning for repairing model outputs. A F Akyurek, E Akyurek, A Kalyan, P Clark, D T Wijaya, N Tandon, 10.18653/v1/2023.acl-long.427Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>S Gehman, S Gururangan, M Sap, Y Choi, N A Smith, arXiv:2009.11462Realtoxicityprompts: Evaluating neural toxic degeneration in language models. 2020arXiv preprint</p>
<p>AmbigQA: Answering ambiguous open-domain questions. S Min, J Michael, H Hajishirzi, L Zettlemoyer, 10.18653/v1/2020.emnlp-main.466Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online2020</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. M Joshi, E Choi, D S Weld, L Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Z Yang, P Qi, S Zhang, Y Bengio, W W Cohen, R Salakhutdinov, C D Manning, arXiv:1809.096002018</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 352022</p>
<p>W Du, Z M Kim, V Raheja, D Kumar, D Kang, Read, arXiv:2204.03685revise, repeat: A system demonstration for human-in-the-loop iterative text revision. 2022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 352022</p>
<p>C Xu, Q Sun, K Zheng, X Geng, P Zhao, J Feng, C Tao, D Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023arXiv preprint</p>
<p>X Geng, A Gudibande, H Liu, E Wallace, P Abbeel, S Levine, D Song, Koala: A dialogue model for academic research. blog post. 2023</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, 14 April 2023. 2023See</p>
<p>Plug and play language models: A simple approach to controlled text generation. S Dathathri, A Madotto, J Lan, J Hung, E Frank, P Molino, J Yosinski, R Liu, arXiv:1912.021642019</p>
<p>GeDi: Generative discriminator guided sequence generation. B Krause, A D Gotmare, B Mccann, N S Keskar, S Joty, R Socher, N F Rajani, Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>. 10.18653/v1/2021.findings-emnlp.424</p>
<p>DExperts: Decodingtime controlled text generation with experts and anti-experts. A Liu, M Sap, X Lu, S Swayamdipta, C Bhagavatula, N A Smith, Y Choi, 10.18653/v1/2021.acl-long.522Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20211</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. S Gururangan, A Marasovic´, S Swayamdipta, K Lo, I Beltagy, D Downey, N A Smith, 10.18653/v1/2020.acl-main.740Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational LinguisticsOnline2020</p>
<p>Quark: Controllable text generation with reinforced unlearning. X Lu, S Welleck, J Hessel, L Jiang, L Qin, P West, P Ammanabrolu, Y Choi, arXiv:2205.136362022</p>
<p>Generating sequences by learning to self-correct. S Welleck, X Lu, P West, F Brahman, T Shen, D Khashabi, Y Choi, arXiv:2211.000532022</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, E H Chi, Q Le, D Zhou, arXiv:2201.119032022</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.036292023</p>            </div>
        </div>

    </div>
</body>
</html>
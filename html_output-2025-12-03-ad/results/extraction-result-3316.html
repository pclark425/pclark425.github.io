<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3316 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3316</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3316</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-baa8f524c82735f174b8d1ab512ac5750146d67e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/baa8f524c82735f174b8d1ab512ac5750146d67e" target="_blank">KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> A novel knowledge graph augmented pre-trained language generation model KG-BART is proposed, which encompasses the complex relations of concepts through the knowledge graph and produces more logical and natural sentences as output and can leverage the graph attention to aggregate the rich concept semantics that enhances the model generalization on unseen concept sets.</p>
                <p><strong>Paper Abstract:</strong> Generative commonsense reasoning which aims to empower machines to generate sentences with the capacity of reasoning over a set of concepts is a critical bottleneck for text generation. Even the state-of-the-art pre-trained language generation models struggle at this task and often produce implausible and anomalous sentences. One reason is that they rarely consider incorporating the knowledge graph which can provide rich relational information among the commonsense concepts. To promote the ability of commonsense reasoning for text generation, we propose a novel knowledge graph augmented pre-trained language generation model KG-BART, which encompasses the complex relations of concepts through the knowledge graph and produces more logical and natural sentences as output. Moreover, KG-BART can leverage the graph attention to aggregate the rich concept semantics that enhances the model generalization on unseen concept sets. Experiments on benchmark CommonGen dataset verify the effectiveness of our proposed approach by comparing with several strong pre-trained language generation models, particularly KG-BART outperforms BART by 5.80, 4.60, in terms of BLEU-3, 4. Moreover, we also show that the generated context by our model can work as background scenarios to benefit downstream commonsense QA tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3316.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3316.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-BART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph-Augmented BART</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BART-based encoder-decoder language generation model augmented with grounded commonsense knowledge graphs (ConceptNet) that fuses graph embeddings and text via graph-informed attention (MGAT/MHGAT) and subword/concept integration/disintegration to improve generative commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KG-BART</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extends the pre-trained BART seq2seq architecture with a KG-augmented Transformer: it grounds input concept sets to ConceptNet, learns TransE embeddings for entities/relations, integrates word-level concept embeddings with subword token embeddings (SCI), updates concepts via Multi-Head Graph Attention (MGAT) and Multi-Head Hierarchical Graph Attention (MHGAT) in encoder/decoder, and disintegrates back to token-level (CSD). Pre-training uses masked/concept reconstruction similar to BART denoising but keeping KG inputs fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Knowledge-graph-augmented relational reasoning (MGAT)', 'Hierarchical concept-expansion graph reasoning (MHGAT)', 'Concept grounding and neighborhood expansion (ConceptNet + TransE)', 'Subword-to-concept integration and concept-to-subword disintegration (SCI/CSD)', 'Pre-training denoising with masked concepts (KG-aware pre-training)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>MGAT: graph-attention over the concept-reasoning graph that fuses word-level textual embeddings with KG entity embeddings and relation embeddings to compute attention among concepts. MHGAT: a two-layer hierarchical graph attention in the decoder that (1) updates concept nodes with neighboring adjunct nodes (concept-expanding graph) and (2) refines intra-concept relations; both layers incorporate relation embeddings. SCI/CSD: CNN-based pooling to map subword token embeddings to word-level concept embeddings (SCI) and deconvolution to map updated concept embeddings back to subword-level (CSD). Pre-training: denoising objective masking concept tokens and reconstructing them while providing KG nodes/relations as auxiliary fixed inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse — the model explicitly combines multiple distinct reasoning mechanisms (textual seq2seq generation + relational graph reasoning + hierarchical graph expansion + masked-reconstruction pre-training). The paper implements these via architectural modules (MGAT, MHGAT, SCI/CSD) rather than by prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonGen (generative commonsense reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Constrained text generation: given an unordered set of 3-5 commonsense concepts (nouns/verbs), generate a single natural sentence describing a plausible everyday scenario that uses all concepts; test sets contain concept pairs/triples unseen in training to measure generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported on CommonGen test set (metrics: BLEU-3, BLEU-4, ROUGE-2, ROUGE-L, METEOR, CIDEr, SPICE, Coverage): KG-BART — BLEU-3: 42.10, BLEU-4: 30.90, ROUGE-2: 23.38, ROUGE-L: 44.54, METEOR: 32.40, CIDEr: 16.83, SPICE: 32.70, Coverage: 98.68. Human inter-annotator reference scores (for context): BLEU-3 48.20, BLEU-4 44.90, ROUGE-2 48.88, ROUGE-L 63.79, METEOR 36.20, CIDEr 43.53, SPICE 63.50, Coverage 99.31.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to text-only pre-trained seq2seq baselines (GPT-2, UniLM, T5-large, BART), KG-BART achieves the best automatic metrics on CommonGen (e.g., BLEU-3/4: +3.10/+4.60 vs BART; +3.10/+2.30 vs T5-large; paper quotes e.g. 7.95%/8.04% improvement on BLEU-3/4 over T5-large in relative terms). Human evaluation ranking: KG-BART receives the highest average score (4.27) vs BART (4.02), T5-large (3.91), UniLM (3.61), GPT-2 (2.98); statistical tests vs T5-large and BART are significant (Wilcoxon p=1.2e-4 and 2.9e-3). Ablations: (1) only KG-augmented encoder (no KG decoder) BLEU-3/4 = 40.40/29.40; (2) no SCI/CSD (same entity rep at each subword) BLEU-3/4 = 41.20/29.70; (3) replace MGAT/MHGAT with simple concatenation BLEU-3/4 = 40.90/29.30; (4) without KG-BART pre-training BLEU-3/4 = 39.80/27.90. These ablations show each KG-specific component contributes to gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating explicit KG relational structure into a pre-trained seq2seq generator (BART) via dedicated graph-attention modules and concept grounding yields consistent improvements on generative commonsense reasoning tasks (CommonGen) across automatic metrics and human judgments; KG-based modules enable better concept relations and higher coverage of input concepts, and generated contexts also improve downstream commonsense QA when prepended to inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No explicit case where adding diverse KG-based methods reduced overall performance is reported; however, ablation shows that omitting KG pre-training or specific modules reduces performance (e.g., removing pre-training lowered BLEU-3/4 to 39.80/27.90), indicating sensitivity to each component. The paper does not report situations where a simpler text-only method beats KG-BART on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3316.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3316.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART (Denoising sequence-to-sequence pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A denoising autoencoder pre-trained seq2seq Transformer (encoder-decoder) that corrupts text and learns to reconstruct it; widely used as a baseline for generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder-decoder pre-trained with denoising objectives (token masking, sentence permutation, etc.) to learn general-purpose generation capabilities; used here as the base architecture KG-BART augments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Text-only pre-trained denoising generation (seq2seq)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>BART performs generation by reconstructing corrupted text using contextual encoding and autoregressive decoding; it does not include explicit external KG relational reasoning in the vanilla form used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar — uses text-only denoising pre-training without heterogeneous KG-based reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonGen (baseline evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same constrained generative commonsense reasoning task as above; BART is fine-tuned on concept-to-sentence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported on CommonGen test set: BLEU-3: 36.30, BLEU-4: 26.30, ROUGE-2: 22.23, ROUGE-L: 41.98, METEOR: 30.90, CIDEr: 13.92, SPICE: 30.60, Coverage: 97.35.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>KG-BART improves over BART on all reported metrics (e.g., BLEU-3 +5.80, BLEU-4 +4.60 relative absolute points), and human evaluation rates KG-BART higher (4.27 vs 4.02); ablation shows that adding KG-augmented encoder and decoder components to the BART backbone yields gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>As a strong text-only baseline, BART performs well, but adding KG-based modules (KG-BART) produces measurable improvements in concept coverage and commonsense plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No counter-examples reported where vanilla BART outperforms KG-BART on the CommonGen task in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3316.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3316.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 (Large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-to-text Transformer that frames all NLP tasks as sequence-to-sequence problems; T5-large is the larger variant used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer pre-trained on a mixture of unsupervised text-to-text tasks; used off-the-shelf and fine-tuned on CommonGen as a text-only baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Text-only pre-trained seq2seq generation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Performs generation via a pre-trained text-to-text objective; no KG augmentation in the baseline experimental setup.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar — text-based generation only.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonGen</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same constrained generation task; T5-large fine-tuned to map concept sets to sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported on CommonGen test set: BLEU-3: 39.00, BLEU-4: 28.60, ROUGE-2: 22.01, ROUGE-L: 42.97, METEOR: 30.10, CIDEr: 14.96, SPICE: 31.60, Coverage: 95.29.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>KG-BART outperforms T5-large on multiple metrics (paper reports relative improvements; e.g., KG-BART BLEU-3/4 = 42.10/30.90 vs T5-large 39.00/28.60).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>T5-large is a strong text-only baseline, but incorporating KG signals (KG-BART) yields better concept coverage and commonsense coherence on CommonGen.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No negative cases reported where T5-large beats KG-BART in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3316.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3316.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (Generative Pre-trained Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive Transformer language model used as a baseline for generation tasks; here fine-tuned for CommonGen.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are unsupervised multitask learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unidirectional Transformer trained to predict next token; applied here as an auto-regressive baseline for generation from concept sets (text-only).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Text-only autoregressive generation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generates text token-by-token conditioned on preceding context; no explicit KG or relational module in the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar — plain autoregressive text-only method.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonGen</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>GPT-2 is fine-tuned to produce sentences from provided concept sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported on CommonGen test set: BLEU-3: 30.70, BLEU-4: 21.10, ROUGE-2: 17.18, ROUGE-L: 39.28, METEOR: 26.20, CIDEr: 12.15, SPICE: 25.90, Coverage: 79.09.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>GPT-2 is substantially behind KG-augmented KG-BART and other encoder-decoder baselines in both automatic metrics and human ratings; the paper uses GPT-2 to illustrate limits of text-only modeling for commonsense generation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Autoregressive text-only models (GPT-2) struggle to cover all input concepts consistently and to produce plausible commonsense relations between them on CommonGen.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No examples where GPT-2 outperforms KG-augmented methods are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3316.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3316.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniLM (Unified Language Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified pre-trained Transformer for both language understanding and generation using masked and autoregressive objectives; evaluated here as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unified language model pre-training for natural language understanding and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UniLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained Transformer that supports several directional attention masks enabling both generation and understanding behavior; used as a strong baseline for CommonGen.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Text-only masked/partially autoregressive generation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Learns via masked LM and sequence-to-sequence objectives and fine-tuned for concept-to-sentence generation without KG augmentation in the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar — text-based methods only.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonGen</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>UniLM fine-tuned for constrained generative commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported on CommonGen test set: BLEU-3: 38.30, BLEU-4: 27.70, ROUGE-2: 21.48, ROUGE-L: 43.87, METEOR: 29.70, CIDEr: 14.85, SPICE: 30.20, Coverage: 89.19.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>UniLM performs better than many baselines but is still outperformed by KG-BART in automatic metrics and human evaluation; KG-BART's KG-modules help capture relations UniLM misses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Masked/partially autoregressive text-only pre-training captures useful patterns but lacks explicit relational grounding that KG-BART supplies.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No reported cases where UniLM exceeds KG-BART on the primary metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension <em>(Rating: 2)</em></li>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer <em>(Rating: 2)</em></li>
                <li>Language models are unsupervised multitask learners <em>(Rating: 1)</em></li>
                <li>Unified language model pre-training for natural language understanding and generation <em>(Rating: 2)</em></li>
                <li>KEPLER: A unified model for knowledge embedding and pre-trained language representation <em>(Rating: 2)</em></li>
                <li>K-BERT: Enabling Language Representation with Knowledge Graph <em>(Rating: 2)</em></li>
                <li>ERNIE: Enhanced language representation with informative entities <em>(Rating: 2)</em></li>
                <li>Graph Attention Networks <em>(Rating: 2)</em></li>
                <li>Translating embeddings for modeling multi-relational data <em>(Rating: 2)</em></li>
                <li>ConceptNet 5.5: an open multilingual graph of general knowledge <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3316",
    "paper_id": "paper-baa8f524c82735f174b8d1ab512ac5750146d67e",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "KG-BART",
            "name_full": "Knowledge Graph-Augmented BART",
            "brief_description": "A BART-based encoder-decoder language generation model augmented with grounded commonsense knowledge graphs (ConceptNet) that fuses graph embeddings and text via graph-informed attention (MGAT/MHGAT) and subword/concept integration/disintegration to improve generative commonsense reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "KG-BART",
            "model_description": "Extends the pre-trained BART seq2seq architecture with a KG-augmented Transformer: it grounds input concept sets to ConceptNet, learns TransE embeddings for entities/relations, integrates word-level concept embeddings with subword token embeddings (SCI), updates concepts via Multi-Head Graph Attention (MGAT) and Multi-Head Hierarchical Graph Attention (MHGAT) in encoder/decoder, and disintegrates back to token-level (CSD). Pre-training uses masked/concept reconstruction similar to BART denoising but keeping KG inputs fixed.",
            "model_size": null,
            "reasoning_methods": [
                "Knowledge-graph-augmented relational reasoning (MGAT)",
                "Hierarchical concept-expansion graph reasoning (MHGAT)",
                "Concept grounding and neighborhood expansion (ConceptNet + TransE)",
                "Subword-to-concept integration and concept-to-subword disintegration (SCI/CSD)",
                "Pre-training denoising with masked concepts (KG-aware pre-training)"
            ],
            "reasoning_methods_description": "MGAT: graph-attention over the concept-reasoning graph that fuses word-level textual embeddings with KG entity embeddings and relation embeddings to compute attention among concepts. MHGAT: a two-layer hierarchical graph attention in the decoder that (1) updates concept nodes with neighboring adjunct nodes (concept-expanding graph) and (2) refines intra-concept relations; both layers incorporate relation embeddings. SCI/CSD: CNN-based pooling to map subword token embeddings to word-level concept embeddings (SCI) and deconvolution to map updated concept embeddings back to subword-level (CSD). Pre-training: denoising objective masking concept tokens and reconstructing them while providing KG nodes/relations as auxiliary fixed inputs.",
            "diversity_of_methods": "diverse — the model explicitly combines multiple distinct reasoning mechanisms (textual seq2seq generation + relational graph reasoning + hierarchical graph expansion + masked-reconstruction pre-training). The paper implements these via architectural modules (MGAT, MHGAT, SCI/CSD) rather than by prompting.",
            "reasoning_task_name": "CommonGen (generative commonsense reasoning)",
            "reasoning_task_description": "Constrained text generation: given an unordered set of 3-5 commonsense concepts (nouns/verbs), generate a single natural sentence describing a plausible everyday scenario that uses all concepts; test sets contain concept pairs/triples unseen in training to measure generalization.",
            "performance_by_method": "Reported on CommonGen test set (metrics: BLEU-3, BLEU-4, ROUGE-2, ROUGE-L, METEOR, CIDEr, SPICE, Coverage): KG-BART — BLEU-3: 42.10, BLEU-4: 30.90, ROUGE-2: 23.38, ROUGE-L: 44.54, METEOR: 32.40, CIDEr: 16.83, SPICE: 32.70, Coverage: 98.68. Human inter-annotator reference scores (for context): BLEU-3 48.20, BLEU-4 44.90, ROUGE-2 48.88, ROUGE-L 63.79, METEOR 36.20, CIDEr 43.53, SPICE 63.50, Coverage 99.31.",
            "comparison_of_methods": "Compared to text-only pre-trained seq2seq baselines (GPT-2, UniLM, T5-large, BART), KG-BART achieves the best automatic metrics on CommonGen (e.g., BLEU-3/4: +3.10/+4.60 vs BART; +3.10/+2.30 vs T5-large; paper quotes e.g. 7.95%/8.04% improvement on BLEU-3/4 over T5-large in relative terms). Human evaluation ranking: KG-BART receives the highest average score (4.27) vs BART (4.02), T5-large (3.91), UniLM (3.61), GPT-2 (2.98); statistical tests vs T5-large and BART are significant (Wilcoxon p=1.2e-4 and 2.9e-3). Ablations: (1) only KG-augmented encoder (no KG decoder) BLEU-3/4 = 40.40/29.40; (2) no SCI/CSD (same entity rep at each subword) BLEU-3/4 = 41.20/29.70; (3) replace MGAT/MHGAT with simple concatenation BLEU-3/4 = 40.90/29.30; (4) without KG-BART pre-training BLEU-3/4 = 39.80/27.90. These ablations show each KG-specific component contributes to gains.",
            "key_findings": "Incorporating explicit KG relational structure into a pre-trained seq2seq generator (BART) via dedicated graph-attention modules and concept grounding yields consistent improvements on generative commonsense reasoning tasks (CommonGen) across automatic metrics and human judgments; KG-based modules enable better concept relations and higher coverage of input concepts, and generated contexts also improve downstream commonsense QA when prepended to inputs.",
            "counter_examples_or_negative_results": "No explicit case where adding diverse KG-based methods reduced overall performance is reported; however, ablation shows that omitting KG pre-training or specific modules reduces performance (e.g., removing pre-training lowered BLEU-3/4 to 39.80/27.90), indicating sensitivity to each component. The paper does not report situations where a simpler text-only method beats KG-BART on this task.",
            "uuid": "e3316.0",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "BART",
            "name_full": "BART (Denoising sequence-to-sequence pre-training)",
            "brief_description": "A denoising autoencoder pre-trained seq2seq Transformer (encoder-decoder) that corrupts text and learns to reconstruct it; widely used as a baseline for generation tasks.",
            "citation_title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "mention_or_use": "use",
            "model_name": "BART",
            "model_description": "Transformer encoder-decoder pre-trained with denoising objectives (token masking, sentence permutation, etc.) to learn general-purpose generation capabilities; used here as the base architecture KG-BART augments.",
            "model_size": null,
            "reasoning_methods": [
                "Text-only pre-trained denoising generation (seq2seq)"
            ],
            "reasoning_methods_description": "BART performs generation by reconstructing corrupted text using contextual encoding and autoregressive decoding; it does not include explicit external KG relational reasoning in the vanilla form used as baseline.",
            "diversity_of_methods": "single/similar — uses text-only denoising pre-training without heterogeneous KG-based reasoning.",
            "reasoning_task_name": "CommonGen (baseline evaluation)",
            "reasoning_task_description": "Same constrained generative commonsense reasoning task as above; BART is fine-tuned on concept-to-sentence generation.",
            "performance_by_method": "Reported on CommonGen test set: BLEU-3: 36.30, BLEU-4: 26.30, ROUGE-2: 22.23, ROUGE-L: 41.98, METEOR: 30.90, CIDEr: 13.92, SPICE: 30.60, Coverage: 97.35.",
            "comparison_of_methods": "KG-BART improves over BART on all reported metrics (e.g., BLEU-3 +5.80, BLEU-4 +4.60 relative absolute points), and human evaluation rates KG-BART higher (4.27 vs 4.02); ablation shows that adding KG-augmented encoder and decoder components to the BART backbone yields gains.",
            "key_findings": "As a strong text-only baseline, BART performs well, but adding KG-based modules (KG-BART) produces measurable improvements in concept coverage and commonsense plausibility.",
            "counter_examples_or_negative_results": "No counter-examples reported where vanilla BART outperforms KG-BART on the CommonGen task in this paper.",
            "uuid": "e3316.1",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "T5-large",
            "name_full": "T5 (Large)",
            "brief_description": "A text-to-text Transformer that frames all NLP tasks as sequence-to-sequence problems; T5-large is the larger variant used as a baseline.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "model_name": "T5-large",
            "model_description": "Encoder-decoder Transformer pre-trained on a mixture of unsupervised text-to-text tasks; used off-the-shelf and fine-tuned on CommonGen as a text-only baseline.",
            "model_size": null,
            "reasoning_methods": [
                "Text-only pre-trained seq2seq generation"
            ],
            "reasoning_methods_description": "Performs generation via a pre-trained text-to-text objective; no KG augmentation in the baseline experimental setup.",
            "diversity_of_methods": "single/similar — text-based generation only.",
            "reasoning_task_name": "CommonGen",
            "reasoning_task_description": "Same constrained generation task; T5-large fine-tuned to map concept sets to sentences.",
            "performance_by_method": "Reported on CommonGen test set: BLEU-3: 39.00, BLEU-4: 28.60, ROUGE-2: 22.01, ROUGE-L: 42.97, METEOR: 30.10, CIDEr: 14.96, SPICE: 31.60, Coverage: 95.29.",
            "comparison_of_methods": "KG-BART outperforms T5-large on multiple metrics (paper reports relative improvements; e.g., KG-BART BLEU-3/4 = 42.10/30.90 vs T5-large 39.00/28.60).",
            "key_findings": "T5-large is a strong text-only baseline, but incorporating KG signals (KG-BART) yields better concept coverage and commonsense coherence on CommonGen.",
            "counter_examples_or_negative_results": "No negative cases reported where T5-large beats KG-BART in the experiments.",
            "uuid": "e3316.2",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "GPT-2",
            "name_full": "GPT-2 (Generative Pre-trained Transformer 2)",
            "brief_description": "An autoregressive Transformer language model used as a baseline for generation tasks; here fine-tuned for CommonGen.",
            "citation_title": "Language models are unsupervised multitask learners",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_description": "Unidirectional Transformer trained to predict next token; applied here as an auto-regressive baseline for generation from concept sets (text-only).",
            "model_size": null,
            "reasoning_methods": [
                "Text-only autoregressive generation"
            ],
            "reasoning_methods_description": "Generates text token-by-token conditioned on preceding context; no explicit KG or relational module in the baseline.",
            "diversity_of_methods": "single/similar — plain autoregressive text-only method.",
            "reasoning_task_name": "CommonGen",
            "reasoning_task_description": "GPT-2 is fine-tuned to produce sentences from provided concept sets.",
            "performance_by_method": "Reported on CommonGen test set: BLEU-3: 30.70, BLEU-4: 21.10, ROUGE-2: 17.18, ROUGE-L: 39.28, METEOR: 26.20, CIDEr: 12.15, SPICE: 25.90, Coverage: 79.09.",
            "comparison_of_methods": "GPT-2 is substantially behind KG-augmented KG-BART and other encoder-decoder baselines in both automatic metrics and human ratings; the paper uses GPT-2 to illustrate limits of text-only modeling for commonsense generation.",
            "key_findings": "Autoregressive text-only models (GPT-2) struggle to cover all input concepts consistently and to produce plausible commonsense relations between them on CommonGen.",
            "counter_examples_or_negative_results": "No examples where GPT-2 outperforms KG-augmented methods are reported.",
            "uuid": "e3316.3",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "UniLM",
            "name_full": "UniLM (Unified Language Model)",
            "brief_description": "A unified pre-trained Transformer for both language understanding and generation using masked and autoregressive objectives; evaluated here as a baseline.",
            "citation_title": "Unified language model pre-training for natural language understanding and generation",
            "mention_or_use": "use",
            "model_name": "UniLM",
            "model_description": "Pre-trained Transformer that supports several directional attention masks enabling both generation and understanding behavior; used as a strong baseline for CommonGen.",
            "model_size": null,
            "reasoning_methods": [
                "Text-only masked/partially autoregressive generation"
            ],
            "reasoning_methods_description": "Learns via masked LM and sequence-to-sequence objectives and fine-tuned for concept-to-sentence generation without KG augmentation in the baseline.",
            "diversity_of_methods": "single/similar — text-based methods only.",
            "reasoning_task_name": "CommonGen",
            "reasoning_task_description": "UniLM fine-tuned for constrained generative commonsense reasoning.",
            "performance_by_method": "Reported on CommonGen test set: BLEU-3: 38.30, BLEU-4: 27.70, ROUGE-2: 21.48, ROUGE-L: 43.87, METEOR: 29.70, CIDEr: 14.85, SPICE: 30.20, Coverage: 89.19.",
            "comparison_of_methods": "UniLM performs better than many baselines but is still outperformed by KG-BART in automatic metrics and human evaluation; KG-BART's KG-modules help capture relations UniLM misses.",
            "key_findings": "Masked/partially autoregressive text-only pre-training captures useful patterns but lacks explicit relational grounding that KG-BART supplies.",
            "counter_examples_or_negative_results": "No reported cases where UniLM exceeds KG-BART on the primary metrics in this paper.",
            "uuid": "e3316.4",
            "source_info": {
                "paper_title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning",
                "publication_date_yy_mm": "2020-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "rating": 2
        },
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "rating": 2
        },
        {
            "paper_title": "Language models are unsupervised multitask learners",
            "rating": 1
        },
        {
            "paper_title": "Unified language model pre-training for natural language understanding and generation",
            "rating": 2
        },
        {
            "paper_title": "KEPLER: A unified model for knowledge embedding and pre-trained language representation",
            "rating": 2
        },
        {
            "paper_title": "K-BERT: Enabling Language Representation with Knowledge Graph",
            "rating": 2
        },
        {
            "paper_title": "ERNIE: Enhanced language representation with informative entities",
            "rating": 2
        },
        {
            "paper_title": "Graph Attention Networks",
            "rating": 2
        },
        {
            "paper_title": "Translating embeddings for modeling multi-relational data",
            "rating": 2
        },
        {
            "paper_title": "ConceptNet 5.5: an open multilingual graph of general knowledge",
            "rating": 2
        }
    ],
    "cost": 0.0163545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning</h1>
<p>Ye Liu ${ }^{1}$, Yao Wan ${ }^{2}$, Lifang $\mathrm{He}^{3}$, Hao Peng ${ }^{1}$, Philip S. Yu ${ }^{1}$<br>${ }^{1}$ University of Illinois at Chicago, Chicago, IL, USA<br>${ }^{2}$ Huazhong University of Science and Technology, Wuhan, China<br>${ }^{3}$ Lehigh University, Bethlehem, PA, USA ${ }^{1}$ Beihang University, Beijing, China<br>{yliu279, psyu}@uic.edu, wanyao@hust.edu.cn, lih319@lehigh.edu, penghao@act.buaa.edu.cn</p>
<h4>Abstract</h4>
<p>Generative commonsense reasoning which aims to empower machines to generate sentences with the capacity of reasoning over a set of concepts is a critical bottleneck for text generation. Even the state-of-the-art pre-trained language generation models struggle at this task and often produce implausible and anomalous sentences. One reason is that they rarely consider incorporating the knowledge graph which can provide rich relational information among the commonsense concepts. To promote the ability of commonsense reasoning for text generation, we propose a novel knowledge graphaugmented pre-trained language generation model KG-BART, which encompasses the complex relations of concepts through the knowledge graph and produces more logical and natural sentences as output. Moreover, KG-BART can leverage the graph attention to aggregate the rich concept semantics that enhances the model generalization on unseen concept sets. Experiments on benchmark CommonGen dataset verify the effectiveness of our proposed approach by comparing with several strong pre-trained language generation models, particularly KG-BART outperforms BART by 5.80, 4.60, in terms of BLEU-3, 4. Moreover, we also show that the generated context by our model can work as background scenarios to benefit downstream commonsense QA tasks. ${ }^{1}$</p>
<h2>Introduction</h2>
<p>Nowadays, numerous benchmarks for commonsense reasoning have been developed to make computers more competent and human-aware. In particular, various pre-trained approaches have achieved impressive performance on the discriminative commonsense tasks - i.e., AI systems are required to choose the correct option from a set of choices based on a given context (Lin et al. 2020), such as CommonsenseQA (Talmor et al. 2019) and COSMOSQA (Huang et al. 2019). However, commonsense reasoning in text generation, known as generative commonsense reasoning, still remains a challenge to existing models, which requires machines to generate a sentence describing a day-to-day scene using concepts from a given concept set.</p>
<p>In recent years, many pre-trained language generation models have been presented for text generation tasks, such as</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of the generation outputs of our KGBART model (blue dotted box) and the existing models without knowledge graph augmentation (red dotted box).</p>
<p>GPTs (Radford et al. 2019; Brown et al. 2020), UniLM (Dong et al. 2019), T5 (Raffel et al. 2020) and BART (Lewis et al. 2020). Although they can capture rich language information from text sentence corpus and generate accurate language texts, almost all of them ignore knowledge information and thereby fail to generate output towards capturing the human commonsense. For example, as shown in Figure 1, given a set of commonsense concepts {river, fish, net, catch}, the task is to generate a coherent sentence describing a scenario covering all given concepts, such as "Fisherman uses a strong net to catch plentiful fishes in the river". From our analysis, we note that the state-of-the-art pre-trained models generate implausible and anomalous sentences in this task (red dotted box) - e.g., GPT-2 generated "A fish is catching in a net", UniLM generated "A net catches fish", etc. Moreover, the generated sentences by the pre-trained models are simple and rigid, while the human sentence is more natural and rich, like "plentiful fishes", "wide river", etc.</p>
<p>In this paper, we argue that only using pre-trained language models with textual concepts alone cannot provide sufficient information for generative commonsense reasoning. The commonsense knowledge graphs (KGs) (Speer, Chin, and Havasi 2017) have been developed especially for knowledge representation in symbolic systems, and they provide a lot of candidate commonsense facts mined from corpora, which have been widely used in commonsense QA tasks (Lin</p>
<p>et al. 2019). It would be beneficial to develop a model that can exploit commonsense KGs for generative commonsense reasoning task. For example, as shown in Figure 1, by considering knowledge facts " $&lt;$ fish, HasPrerequisite, using net $&gt;$ " and " $&lt;$ fish, HasSubevent, catch $&gt;$ ", it is easy to recognize the relation between concepts ${$ fish, net, catch $}$, namely using the net to catch fish. Furthermore, the commonsense relation, like " $&lt;$ river, RelatedTo, clean $&gt;$ ", can provide the adjunct word to facilitate generating a more natural and plausible daily scenario sentence.</p>
<p>In light of the fact that the knowledge graph can provide the relational information to enhance the reasoning capacity and provide adjunct words to the concept, we propose a novel Knowledge Graph-Augmented framework for generative commonsense reasoning. It has two major steps: knowledge graph grounding and graph-based encoder-decoder modeling. We first construct two KGs, one is the concept-reasoning graph and another is the concept-expanding graph, both of which encode the entity representations and their dependency relations. Secondly, we propose an encoder-decoder neural architecture, named (KG-BART), by incorporating the grounded KGs into the state-of-the-art pre-trained language generation model BART. KG-BART follows the BART architecture, but instead of using the traditional Transformer, we introduce an effective Knowledge Graph-Augmented Transformer to capture the relations between concept set, where the grounded KGs are used as the additional inputs to the graph attention mechanism. Besides, since the token and concept entity are at different granularity levels, we integrate the text representation with the knowledge concept for relational reasoning and then disintegrate to the token-level.</p>
<p>Overall, the main contributions of this paper are as follows:</p>
<ul>
<li>To the best of our knowledge, this is the first time that the KG is incorporated into the pre-trained model to improve the ability of commonsense reasoning in text generation.</li>
<li>We build the concept-reasoning graph to guide the pretrained model to better reasoning the relationships among concepts. Moreover, we build the concept-expanding graph which considers both the inter-concept relation and intraconcept relation for KG-Augmented decoder to generate more natural and plausible output.</li>
<li>We propose KG-BART, a pre-trained method that is designed to better generate language via knowledge graphs and texts, and enhance the model generalization on unseen concept sets. Particularly, the integration and disintegration components are introduced to fuse the heterogeneous information between the token and concept entity.</li>
<li>The experimental results show that KG-BART significantly outperforms the state-of-the-art pre-trained models on the task of generative commonsense reasoning. Additionally, we show that KG-BART can benefit downstream tasks (e.g., commonsense QA) via generating useful context as background scenarios.</li>
</ul>
<h2>Problem Formulation</h2>
<p>Notation. We use $\mathcal{X}$ to denote the space of all possible concept sets, and use $\mathcal{T}$ and $\mathcal{C}$ to denote the token vocabulary and
concept vocabulary, respectively. The knowledge graph (KG) is denoted as $\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{R})$, where $\mathcal{V}$ is the set of entities, $\mathcal{E}$ is the set of edges and $\mathcal{R}$ is the set of relations among entities. For a pair of entities $v_{i} \in \mathcal{V}$ (subject) and $v_{j} \in \mathcal{V}$ (object), associated with the relation $r_{i j} \in \mathcal{R}$, the edge $e_{i j} \in \mathcal{E}$ can be represented as a triplet $\left(v_{i}, r_{i j}, v_{j}\right)$. Specifically, we assume the concept vocabulary is a subset of KG's unigram entities, namely $\mathcal{C} \subset \mathcal{V}$.</p>
<p>Given an unordered set of $k$ commonsense concepts $x=$ $\left{c_{1}, c_{2}, \ldots, c_{k}\right}$, where each concept $c_{i} \in \mathcal{C} \subset \mathcal{X}$ is an object (noun) or action (verb), the ultimate goal of generative commonsense reasoning is to generate a natural language output $y=\left{y_{1}, y_{2}, \ldots, y_{l}\right}$ that is both correct (or valid) and natural sounding for that scenario. This is often modeled by learning a function $f: \mathcal{X} \rightarrow \mathcal{Y}$ that maps the concept set $x \in \mathcal{X}$ into a sentence $y \in \mathcal{Y}$. Our aim is to boost the performance of this task with the help of KG database $\mathcal{G}$ which can be treated as auxiliary information.</p>
<p>More formally, we formulate the problem as follows: $h$ : ${\mathcal{X}, \mathcal{G}} \rightarrow\left{\mathcal{G}^{R}, \mathcal{G}^{E}\right}$ that takes the concept sets $x \in \mathcal{X}$ and the knowledge $\mathcal{G}$ as the input to first learn a conceptreasoning graph $\mathcal{G}^{R}$ and a hierarchical concept-expanding graph $\mathcal{G}^{E}$, and then $g:\left{\mathcal{X}, \mathcal{G}^{R}, \mathcal{G}^{E}\right} \rightarrow \mathcal{Y}$ to generate the final outputs. Specifically, $\mathcal{G}^{R} \subset \mathcal{G}$ consisting of all concept triplets $\left(v_{i}^{R}, r_{i j}^{R}, v_{j}^{R}\right)$, where $v_{i}^{R}$ and $v_{j}^{R} \in \mathcal{X}$ and $r_{i j}^{R} \in \mathcal{R}$ is the relation between each concept pairs. $\mathcal{G}^{E}=$ $\left{\mathcal{G}^{R} \cup \mathcal{N}\left(v^{R}\right)\right} \subset \mathcal{G}$ is used to enrich the graph with adjunct information, where $\mathcal{N}\left(v^{R}\right)$ characterizes the neighborhood relationship between concept $\left(v^{R}\right)$ and its adjacencies in the KG database.</p>
<h2>Knowledge Graph Grounding</h2>
<p>In this section, we explain how to construct and learn the embedding representations of the concept-reasoning graph and the hierarchical concept-expanding graph from the large commonsense KG Conceptnet (Speer, Chin, and Havasi 2017). ${ }^{2}$</p>
<p>In the generative commonsense reasoning task, traditional pre-trained methods usually encode the concept $(x)$ and decode the sentence $(y)$ based on text information alone, which ignore the structural information and relations between concepts and suffer from generating a lot of implausible sentences. In order to overcome this drawback, we propose to hybridize the KG and text information in the encoder and decoder modules. Specifically, in the encoder phase, we construct a concept-reasoning graph $\mathcal{G}^{R}$ to encompass the relations between the concept set. In the decoder phase, we construct a hierarchical concept-expanding graph $\mathcal{G}^{E}$ to enrich the concept structure with the neighborhood correlation preserved in the KG database. Based on our assumption, each concept corresponds to a KG's unigram entity, so we can directly match the concept set to the entities from KG to generate $\mathcal{G}^{R}$. In order to establish $\mathcal{G}^{E}$, we couple $\mathcal{G}^{R}$ with the association of selected neighboring nodes with each concept in KG. For many concepts, there are hundreds or thousands of neighboring nodes connected with each of them (via triplets) in KG, which provide us not only rich information but also</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The proposed KG-BART model.
less important or less relevant entities that may be undesirable. For instance, given a concept-set ${$ ski, skier, mountain $}$, considering the adjunct concepts for "mountain", "snowy" is more precise than others like "small" or "flat" according to the close semantics of "snowy" and "ski/skier". Based on this fact, we rank the neighboring nodes of each concept according to the word similarity scores and select their potential top- $k$ neighboring nodes adding to $\mathcal{G}^{R}$, so as to get $\mathcal{G}^{E}$. To calculate the word similarity scores, we use the pre-trained GloVe embedding (Pennington, Socher, and Manning 2014) as the representation of each entity node in KG. The ranking score for a particular neighboring node is the sum of similarity scores with all concepts. Here we use the cosine similarity for its simplicity and wide application.</p>
<p>Since some of concept pairs do not have a direct connection in the KG and some of the concept pairs connect by multiple relations, instead of directly using $\mathcal{G}^{R}$ and $\mathcal{G}^{E}$, we use a knowledge embedding method named TransE (Bordes et al. 2013) to learn their entity and relation embeddings. To prepare the training triplets of TransE model, we first collect the triplets in the one-hop path, two-hop path, and three-hop path between each concept pair. Moreover, we further collect the triples between each concept node and their neighboring nodes as follows: if the concept node is the object (noun), only the neighboring node containing the adjective word will be selected; if the concept node is action (verb), only the node containing adverb word will be selected. TransE model is trained based on those selected triplets, which generates the node embedding $\mathbf{v}<em e="e">{i} \in \mathbb{R}^{d</em>}}$ for each node $v_{i}$ and relation embedding $\mathbf{r<em r="r">{i j} \in \mathbb{R}^{d</em>}}$ for each edge $e_{i j}$. For $\mathcal{G}^{R}$, we denote each concept embedding as $\mathbf{v}^{R}$, and relation embeddings as $\mathbf{r<em i="i">{i j}^{R}=\mathbf{v}</em>$.}^{R}-\mathbf{v}_{j}^{R}$ instead of the output of TransE to avoid missing relations between concepts. For $\mathcal{G}^{E}$, since those neighboring nodes are connected with the concepts in the KG, we directly add their node embeddings $\mathbf{v}^{N}$ and relation embeddings $\mathbf{r}^{N}$ to $\mathcal{G}^{R</p>
<h2>Graph-Based Encoder-Decoder Modeling</h2>
<p>Overview. Figure 2 presents an overview of the proposed KG-BART model, which follows the BART encoder-decoder architecture but uses both text concepts and KG as the input. The encoder is composed of two components: one traditional textual Transformer encoder module (Vaswani et al. 2017) to represent the contextual information of each token; and another KG-augmented Transformer module based on graph attention mechanism to integrate the entity-oriented knowledge information into token representation. Similarly, the
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The KG-augmented encoder.
decoder is also composed of a stack of a textual Transformer decoder module and a KG-augmented Transformer decoder module to generate sentences with the ability of commonsense reasoning. Specially, we use a hierarchical graph attention mechanism to refine the KG-augmented decoder to capture the inherent structural correlations of intra-concept and inter-concept in the graph. Note that all the node and relation embeddings are held fixed in the training process of KG-BART. Since our textual Transformers are the same as that used in BART, here we exclude a comprehensive description of these modules and refer readers to (Lewis et al. 2020) and (Vaswani et al. 2017) for more details. In the following, we will focus on the proposed KG-augmented Transformer.</p>
<h2>KG-Augmented Encoder</h2>
<p>As shown in Figure 3, above the textual encoders, the KGaugmented encoder is designed to enrich the token representation by considering the KG structure. We propose to incorporate graph representations into the neural encoding process via a graph-informed attention mechanism. It takes advantage of the explicit relations to learn better intra-concept relations. Formally, the KG-augmented encoder integrates the input token embeddings $\left{\mathbf{x}<em n="n">{1}, \ldots, \mathbf{x}</em>}\right}$, which is the output of the textual encoders, as well as the embedding of conceptreasoning graph $\mathcal{G}^{R}$ to update the token representation as $\left{\mathbf{x<em n="n">{1}^{e}, \ldots, \mathbf{x}</em>\right}$.}^{e</p>
<p>Subword to Concept Integration (SCI) As the input token embeddings are based on a sequence of subwords, while our concepts in the KG are at word-level, we need to align these different granularity sequences. To apply the relation between concepts, we group the subwords for each concept. In particular, we adopt one convolutional neural network (CNN) (Kim 2014) with a max-pooling layer to efficiently obtain the representation in word-level.</p>
<p>Here we take a concrete concept as an example to better illustrate this process. Supposing that a concept $c_{i}$ is made up of a sequence of subwords $\left{x_{1}, x_{2}, \ldots, x_{m}\right}$, where $m$ is the number of subwords. Given the token embeddings $\mathbf{x}$ from textual encoder, we first utilize a Conv1D layer, $\mathbf{x}<em t="t">{t}^{\prime}=$ $\mathbf{Z}\left(\mathbf{x}</em>}, \mathbf{x<em t_l-1="t+l-1">{t+1}, \ldots, \mathbf{x}</em>$ is trainable parameters and $k$ is the kernel size. We then apply a max-pooling layer over a sequence of}\right)^{T}, t \in[1, m-l+1]$, where $\mathbf{Z}=$ $\left[z_{1}, \ldots, z_{l}\right] \in \mathbb{R}^{1 \times l</p>
<p>the output embeddings after Conv1D:</p>
<p>$$
\mathbf{e}\left(c_{i}\right)=\operatorname{MaxPooling}\left(\mathbf{x}<em m-l_1="m-l+1">{1}^{\prime}, \ldots, \mathbf{x}</em>\right)
$$}^{\prime</p>
<p>Therefore, the final word-level textual embedding of concept is represented as $\mathbf{e}^{w}=\left{\mathbf{e}\left(c_{1}\right), \ldots, \mathbf{e}\left(c_{l}\right)\right} \in \mathbb{R}^{k \times d_{w}}$ where $d_{w}$ denotes the dimension of concept embedding.</p>
<p>Multi-Head Graph Attention (MGAT) Given the embedding representation of concept-reasoning graph $\mathcal{G}^{R}$ with node features $\mathbf{v}^{R} \in \mathbb{R}^{k \times d_{v}}$ and relation features $\mathbf{r}^{R}$, we apply the graph attention networks (GAT) (Veličković et al. 2017) to iteratively update the representations for each concept $\mathbf{v}<em i="i">{i}^{R}$ through its neighbors $\mathcal{N}</em>}^{R}$. We denote the word-level hidden state as $\mathbf{h<em h="h">{i} \in \mathbb{R}^{d</em>}}$, where $i \in(1, \ldots, k)$. We further modify the GAT layer to infuse the pairwise relation embedding $\mathbf{r<em r="r">{i j}^{R} \in \mathbb{R}^{d</em>$. Therefore, the multi-head graph attention can be denoted as:}</p>
<p>$$
\begin{aligned}
&amp; \mathbf{H}=\left[\mathbf{e}^{w} ; \mathbf{W}<em i="i" j="j">{e} \mathbf{v}^{R}\right] \
&amp; z</em>}=\operatorname{LeakyReLU}\left(\mathbf{W<em q="q">{a}\left[\mathbf{W}</em>} \mathbf{h<em k="k">{i} ; \mathbf{W}</em>} \mathbf{h<em r="r">{j} ; \mathbf{W}</em>} \mathbf{r<em i="i" j="j">{i j}^{R}\right]\right) \
&amp; \alpha</em>}=\frac{\exp \left(z_{i j}\right)}{\sum_{l=1}^{|\mathcal{N<em i="i" l="l">{i}^{R}|} \exp \left(z</em>}\right)}, \quad \mathbf{h<em k="1">{i}^{\prime}=|</em>}^{K} \sigma\left(\sum_{j=1}^{|\mathcal{N<em i="i" j="j">{i}^{R}|} \alpha</em>}^{k} \mathbf{W<em i="i">{v}^{k} \mathbf{h}</em>\right)
\end{aligned}
$$</p>
<p>where $K$ is the multi-head number, $|<em a="a">{k=1}^{K}$ denotes an operation of multi-head used in Transformer, which concatenates the attention embeddings from different heads and feeds the result into a linear projection. $\mathbf{W}</em>}, \mathbf{W<em r="r">{e}, \mathbf{W}</em>}, \mathbf{W<em k="k">{q}, \mathbf{W}</em>}$ and $\mathbf{W<em i="i" j="j">{v}$ are trainable weights and $\alpha</em>}$ is the attention weight between $\mathbf{h<em j="j">{i}$ and $\mathbf{h}</em>$ incorporates relation representations as prior constraints into the encoding process. In this way, our model can learn better and richer concept representations containing the relationship among concepts.
Concept to Subword Disintegration (CSD) After updating the word-level hidden state considering the relation between concepts in the KG, we need to disintegrate the concept to the subword-level for the following process. We first upsample word-level hidden state $\mathbf{h}}$. The word-level hidden state $\mathbf{H}$ contains the latent dependencies between any two concepts from textual aspect information $\mathbf{e}^{w}$ and KG aspect information $\mathbf{v}^{R}$. And $\mathbf{r}^{R<em i="i">{i}^{\prime}$ with $(m-l+1)$ times (the length before MaxPooling) as $\left[\mathbf{h}</em>}^{\prime 1}, \ldots, \mathbf{h<em 0="0">{i}^{\prime m-l+1}\right]$ and utilize a Deconv1D layer with vector $\mathbf{Z}=\left[z</em>}, \ldots, z_{l}\right] \in \mathbb{R}^{1 \times l}$ used in Conv1D to form the Deconv1D matrix $\mathbf{Z<em i="i">{D} \in \mathbb{R}^{m \times(m-l+1)}$ to get the subword-level hidden state $\mathbf{u}</em>$ :</p>
<p>$$
\left[\mathbf{u}<em i="i">{i}^{1}, \ldots, \mathbf{u}</em>
z_{0} &amp; &amp; &amp; \
\cdots &amp; z_{0} &amp; &amp; \
z_{l} &amp; \cdots &amp; \cdots &amp; \
&amp; z_{l} &amp; &amp; z_{0} \
&amp; &amp; &amp; \cdots \
&amp; &amp; &amp; z_{l}
\end{array}\right) *\left(\begin{array}{c}
\mathbf{h}}^{m}\right]^{T}=\left(\begin{array}{cccc<em i="i">{i}^{\prime 1} \
\mathbf{h}</em> \
\cdot \
\cdot \
\cdot \
\mathbf{h}_{i}^{\prime m-l+1}
\end{array}\right)
$$}^{\prime 2</p>
<p>Then, a two-layer feed-forward network with GeLU activation (Hendrycks and Gimpel 2016) function and a residual layer normalization are applied to obtain the final output can be represented $\mathbf{x}_{i}^{o}$ :</p>
<p>$$
\begin{aligned}
&amp; \mathbf{p}<em 2="2" o="o">{i}=\mathbf{W}</em>} \operatorname{GeLU}\left(\mathbf{W<em i="i">{o 1}\left(\mathbf{u}</em>}+\mathbf{x<em i="i">{i}\right)\right) \
&amp; \mathbf{x}</em>}^{o}=\operatorname{LayerNorm}\left(\mathbf{p<em i="i">{i}+\mathbf{x}</em>\right)
\end{aligned}
$$</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The KG-augmented decoder.
where $\mathbf{W}<em f="f">{o 1} \in \mathbb{R}^{d</em>} \times d_{h}}$ and $\mathbf{W<em h="h">{o 2} \in \mathbb{R}^{d</em>$ is the hidden size of the feedforward layer.} \times d_{f}}$ are learnable parameters, $d_{f</p>
<h2>KG-Augmented Decoder</h2>
<p>In this section, our KG-augmented decoder, as shown in Figure 4, incorporates hierarchical graph structure into the decoding process to capture the relations between concepts and their neighboring nodes which can help to generate more precise and natural output. To embody the hierarchical conceptexpanding graph $\mathcal{G}^{E}$ with the generation process, we propose the multi-head hierarchical graph attention layer.</p>
<h2>Multi-Head Hierarchical Graph Attention (MHGAT)</h2>
<p>To contain the adjunct description for the concept node, the first layer of hierarchical graph attention is to update the concept node $\mathbf{v}<em i="i">{i}^{R} \in \mathbb{R}^{d e}$ through its inter-concept neighboring nodes $\mathcal{N}</em>$.}^{N}$ with relation embedding $\mathbf{r}_{i j}^{N} \in \mathbb{R}^{d r</p>
<p>$$
\begin{aligned}
&amp; z_{i j}=\operatorname{LeakyReLU}\left(\mathbf{W}<em q="q">{a}\left[\mathbf{W}</em>} \mathbf{v<em k="k">{i}^{R} ; \mathbf{W}</em>} \mathbf{v<em r="r">{j}^{N} ; \mathbf{W}</em>} \mathbf{r<em i="i" j="j">{i j}^{N}\right]\right) \
&amp; \alpha</em>}=\frac{\exp \left(z_{i j}\right)}{\sum_{l=1}^{|\mathcal{N<em i="i" l="l">{i}^{R}|} \exp \left(z</em>}\right)}, \quad \mathbf{v<em k="1">{i}^{R t}=|</em>}^{K} \sigma\left(\sum_{j=1}^{|\mathcal{N<em i="i" j="j">{i}^{R}|} \alpha</em>}^{k} \mathbf{W<em j="j">{v}^{k} \mathbf{v}</em>\right)
\end{aligned}
$$}^{R</p>
<p>After updating the concepts with their neighboring nodes, the concepts get their new embedding $\mathbf{v}^{R t}$. The second graph attention layer updates the concept representation considering the intra-concept relations $\mathbf{r}_{i j}^{R} \in \mathbb{R}^{d r}$.</p>
<p>$$
\begin{aligned}
&amp; z_{i j}=\operatorname{LeakyReLU}\left(\mathbf{W}<em q="q">{a}\left[\mathbf{W}</em>} \mathbf{v<em k="k">{i}^{R t} ; \mathbf{W}</em>} \mathbf{v<em r="r">{j}^{R t} ; \mathbf{W}</em>} \mathbf{r<em i="i" j="j">{i j}^{R}\right]\right) \
&amp; \alpha</em>}=\frac{\exp \left(z_{i j}\right)}{\sum_{l=1}^{|\mathcal{N<em i="i" l="l">{i}^{R}|} \exp \left(z</em>}\right)}, \quad \mathbf{v<em k="1">{i}^{R \prime \prime}=|</em>}^{K} \sigma\left(\sum_{j=1}^{|\mathcal{N<em i="i" j="j">{i}^{R}|} \alpha</em>}^{k} \mathbf{W<em j="j">{v}^{k} \mathbf{v}</em>\right)
\end{aligned}
$$}^{R r</p>
<p>We further compute the two multi-head attention (MAT) (Vaswani et al. 2017) to capture textual and KG influence. One is the attention between the encoder hidden state $\mathbf{x}^{o}$ and the previously generated token hidden state $\mathbf{y}$. The other is the attention between the updated concept embeddings $\mathbf{v}^{R \prime \prime}$ and the previously generated token hidden state $\mathbf{y}$ as follows:</p>
<p>$$
\mathrm{AT}^{\mathrm{KG}}=\mathrm{MAT}\left(\mathbf{y}, \mathbf{v}^{R \prime \prime}, \mathbf{v}^{R \prime \prime}\right), \quad \mathrm{AT}^{\mathrm{TX}}=\mathrm{MAT}\left(\mathbf{y}, \mathbf{x}^{o}, \mathbf{x}^{o}\right)
$$</p>
<p>The final decoder output is the concatenate of the two attention with a residual connection as:</p>
<p>$$
\mathbf{y}^{o}=\mathbf{W}_{a t t}\left[\mathrm{AT}^{\mathrm{KG}} ; \mathrm{AT}^{\mathrm{TX}}\right]+\mathbf{y}
$$</p>
<p>where $\mathbf{W}<em h="h">{a t t} \in \mathbb{R}^{d</em>} \times 2 d_{h}}$ is the trainable weight. $\mathbf{y}^{o}$ is used to predict the token sequence: $P_{\text {vocab }}=$ $\operatorname{softmax}\left(\mathbf{W<em _out="{out" _text="\text">{\text {out }} \mathbf{y}^{o}+\mathbf{b}</em>}}\right), \mathbf{W<em h="h">{a t t} \in \mathbb{R}^{V \times d</em>$ and $V$ is the vocabulary size.}</p>
<h2>KG-BART Model Pre-Training</h2>
<p>The embedding vectors of words in text and nodes/entities in KG are obtained in separate ways, making their vector-space inconsistent. In order to fuse the KG into BART, similar to BART, KG-BART is trained by corrupting texts and then optimizing a reconstruction loss, the cross-entropy, between the decoder's output and the original texts. We randomly select five concept nodes from our selected entities and mask some concepts among them. KG-BART still takes the entity and relation embedding of all concepts without considering whether the token is masked. Since the graph in the decoder only contains the concept set entities, the decoder is modified as without updating the concept nodes with their neighboring nodes in the pre-training stage. KG-BART is pre-trained to generate the original concept token from the masked concept nodes. For example, "[mask] wound [mask] teach soldier" in the encoder and "student wound treat teach soldier" in the decoder. The number of the masked token is randomly sampled from 0 to 5 .</p>
<h2>Experiment and Analysis</h2>
<p>Dataset CommonGen (Lin et al. 2020) is a constrained text generation task, which is to explicitly test the ability of machines on commonsense reasoning when generating a text. The dataset released in this task is constructed through a combination of crowdsourced and existing caption corpora, which consists of 77 k commonsense descriptions over 35 k unique concept sets. In average, each concept set is composed of $3 \sim 5$ unique concepts. We present the basic statistics of this dataset in Table 1. Notably, all pairs of concepts in every test concept set are unseen in training data so that it poses a challenge for text generalization.</p>
<p>Baselines We compare the performance of our proposed model with several state-of-the-art pre-trained text generation models. GPT-2 (Radford et al. 2019) is an unidirectional model to predict tokens given the input text in an auto-regressive manner. UniLM (Dong et al. 2019) proposes a unified model of language understanding and language generation using the masked language modeling. UniLM2 (Bao et al. 2020) further proposes a pseudo-masked language model to learn intra-relations between masked spans via partially auto-regressive modeling. BERT-Gen (Bao et al. 2020) fine-tunes BERT for sequence-to-sequence language generation using a similar training objective employed by UniLM. T5 (Raffel et al. 2020) introduces a unified framework that converts all text-based language problems into a text-to-text format. BART (Lewis et al. 2020) introduces a denoising autoencoder for pre-training sequence-to-sequence models. For the implementation of those models for the generative</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Concept sets</td>
<td style="text-align: center;">32,651</td>
<td style="text-align: center;">993</td>
<td style="text-align: center;">1,497</td>
</tr>
<tr>
<td style="text-align: left;"># Sentences</td>
<td style="text-align: center;">67,389</td>
<td style="text-align: center;">4,018</td>
<td style="text-align: center;">6,042</td>
</tr>
<tr>
<td style="text-align: left;">\% Unseen Concepts</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$6.53 \%$</td>
<td style="text-align: center;">$8.97 \%$</td>
</tr>
<tr>
<td style="text-align: left;">\% Unseen Concept-Paris</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$96.31 \%$</td>
<td style="text-align: center;">$100.00 \%$</td>
</tr>
<tr>
<td style="text-align: left;">\% Unseen Concept-Triples</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$99.60 \%$</td>
<td style="text-align: center;">$100.00 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1: The basic statistics of the CommonGen dataset.
commonsense reasoning task, we refer readers to (Lin et al. 2020) for more details.</p>
<p>Automatic Evaluation Following other conventional generation tasks, we use several widely-used automatic metrics to automatically assess the performance, such as BLEU (Papineni et al. 2002), ROUGE (Lin 2004) and METEOR (Banerjee and Lavie 2005), which mainly focus on measuring ngram similarities. We report the Coverage of concept, which is the average percentage of input concepts that are present after lemmatization. In addition, we use evaluation metrics specially designed for image captioning task, such as CIDEr (Vedantam, Lawrence Zitnick, and Parikh 2015) and SPICE (Anderson et al. 2016). These metrics focus on evaluating the associations between mentioned concepts instead of n-gram overlap. For example, the SPICE metric uses dependency parse trees as a proxy of scene graphs to measure the similarity of scenarios. To estimate human performance within each metric, we treat each reference sentence in test dataset as a system prediction and compare it with other references. It is equivalent to compute inter-annotator agreement.</p>
<p>Table 2 presents the experimental results in a variety of metrics and methods reported on the Leaderboard. ${ }^{3}$ We can see that KG-BART performs best among all the pre-trained models. KG-BART outperforms $7.95 \% / 8.04 \%$ on BLEU-3/4 than the second best model T5-large. KG-BART gains 1.15 improvements than the second best model BART on ROUGE2, the gain 0.67 than UniLM on ROUGE-L. KG-BART gains 1.50 on METEOR than the second best model BART. KGBART beats the second best model T5-large by $12.50 \%$ on CIDEr and $3.48 \%$ on SPICE. Moreover, KG-BART gets the highest Coverage 98.68 among all baseline pre-trained models. The results suggest that leveraging the pre-trained generation model with the knowledge graph can improve the performance of generative commonsense reasoning.</p>
<p>Human Evaluation The automatic evaluations are unable to measure the coherence of the generated text properly. Therefore, we also access system performance by human evaluation. We randomly select 100 instances from the CommonGen test set and invite 3 annotators to access the outputs of different models independently. Annotators access the overall quality of generative commonsense sentence by ranking them from 1 (worst) to 5 (best) taking into account the following four criteria: (1) Rationality: is the sentence the reasonable commonsense scenario? (2) Fluency: is the sentence fluent and grammatical? (3) Succinctness: does the sentence avoid repeating information? (4) Naturalness: does</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model \Metrics</th>
<th style="text-align: center;">BLEU-3/4</th>
<th style="text-align: center;">ROUGE-2/L</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">CIDEr</th>
<th style="text-align: center;">SPICE</th>
<th style="text-align: center;">Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2 (Radford et al. 2019)</td>
<td style="text-align: center;">30.70</td>
<td style="text-align: center;">21.10</td>
<td style="text-align: center;">17.18</td>
<td style="text-align: center;">39.28</td>
<td style="text-align: center;">26.20</td>
<td style="text-align: center;">12.15</td>
</tr>
<tr>
<td style="text-align: left;">BERT-Gen (Bao et al. 2020)</td>
<td style="text-align: center;">30.40</td>
<td style="text-align: center;">21.10</td>
<td style="text-align: center;">18.05</td>
<td style="text-align: center;">40.49</td>
<td style="text-align: center;">27.30</td>
<td style="text-align: center;">12.49</td>
</tr>
<tr>
<td style="text-align: left;">UniLM (Dong et al. 2019)</td>
<td style="text-align: center;">38.30</td>
<td style="text-align: center;">27.70</td>
<td style="text-align: center;">21.48</td>
<td style="text-align: center;">43.87</td>
<td style="text-align: center;">29.70</td>
<td style="text-align: center;">14.85</td>
</tr>
<tr>
<td style="text-align: left;">UniLM-v2 (Bao et al. 2020)</td>
<td style="text-align: center;">31.30</td>
<td style="text-align: center;">22.10</td>
<td style="text-align: center;">18.24</td>
<td style="text-align: center;">40.62</td>
<td style="text-align: center;">28.10</td>
<td style="text-align: center;">13.10</td>
</tr>
<tr>
<td style="text-align: left;">T5-Base (Raffel et al. 2020)</td>
<td style="text-align: center;">26.00</td>
<td style="text-align: center;">16.40</td>
<td style="text-align: center;">14.57</td>
<td style="text-align: center;">34.55</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">9.16</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large (Raffel et al. 2020)</td>
<td style="text-align: center;">39.00</td>
<td style="text-align: center;">28.60</td>
<td style="text-align: center;">22.01</td>
<td style="text-align: center;">42.97</td>
<td style="text-align: center;">30.10</td>
<td style="text-align: center;">14.96</td>
</tr>
<tr>
<td style="text-align: left;">BART (Lewis et al. 2020)</td>
<td style="text-align: center;">36.30</td>
<td style="text-align: center;">26.30</td>
<td style="text-align: center;">22.23</td>
<td style="text-align: center;">41.98</td>
<td style="text-align: center;">30.90</td>
<td style="text-align: center;">13.92</td>
</tr>
<tr>
<td style="text-align: left;">Human Performance</td>
<td style="text-align: center;">48.20</td>
<td style="text-align: center;">44.90</td>
<td style="text-align: center;">48.88</td>
<td style="text-align: center;">63.79</td>
<td style="text-align: center;">36.20</td>
<td style="text-align: center;">43.53</td>
</tr>
<tr>
<td style="text-align: left;">KG-BART</td>
<td style="text-align: center;">$\mathbf{4 2 . 1 0}$</td>
<td style="text-align: center;">$\mathbf{3 0 . 9 0}$</td>
<td style="text-align: center;">$\mathbf{2 3 . 3 8}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 5 4}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 4 0}$</td>
<td style="text-align: center;">$\mathbf{1 6 . 8 3}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Experimental results of different baseline methods on the CommonGen test dataset. We show the best results in boldface, and those with the second best performance are underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$\mathbf{1}$</th>
<th style="text-align: center;">$\mathbf{2}$</th>
<th style="text-align: center;">$\mathbf{3}$</th>
<th style="text-align: center;">$\mathbf{4}$</th>
<th style="text-align: center;">$\mathbf{5}$</th>
<th style="text-align: center;">Rating</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: center;">$22 \%$</td>
<td style="text-align: center;">$16 \%$</td>
<td style="text-align: center;">$23 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$19 \%$</td>
<td style="text-align: center;">2.98</td>
</tr>
<tr>
<td style="text-align: left;">UniLM</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">$17 \%$</td>
<td style="text-align: center;">$22 \%$</td>
<td style="text-align: center;">$24 \%$</td>
<td style="text-align: center;">$32 \%$</td>
<td style="text-align: center;">3.61</td>
</tr>
<tr>
<td style="text-align: left;">T5-large</td>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">$12 \%$</td>
<td style="text-align: center;">$32 \%$</td>
<td style="text-align: center;">$39 \%$</td>
<td style="text-align: center;">3.91</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$17 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$42 \%$</td>
<td style="text-align: center;">4.02</td>
</tr>
<tr>
<td style="text-align: left;">KG-BART</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$8 \%$</td>
<td style="text-align: center;">$12 \%$</td>
<td style="text-align: center;">$25 \%$</td>
<td style="text-align: center;">$55 \%$</td>
<td style="text-align: center;">$\mathbf{4 . 2 7}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Ranking results of system outputs by human evaluation. 1 is the worst and 5 is the best. The larger rating denotes a better summary quality.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: A case study of a specific concept set {stand, hold, street, umbrella } for qualitative analysis of machine generations. Human references are collected from AMT.
the sentence use adjunct words? The rating of each system is computed by averaging the scores on all test instances.</p>
<p>Table 3 summarizes the comparison results of five methods. Both the percentage of ranking results and overall ratings are reported. The results demonstrate that KG-BART is able to generate higher quality output than other models. Specifically, the outputs generated by KG-BART usually contains more reasonable scenario and are more fluent and precise than other models. The human evaluation results further validate the effectiveness of our proposed model. Moreover, based on the 100 final scores for each approach, we conduct Wilcoxon signed-rank tests (Wilcoxon, Katti, and Wilcox 1970). Comparing KG-BART with T5-Large and BART, the $p$-values of Wilcoxon signed-rank testing at $95 \%$ confidence level are $1.2 e-4$ and $2.9 e-3$, which mean the improvements achieved by our approach are statistically significant.</p>
<p>Case Study Figure 5 gives a specific input concept set {stand, hold, street, umbrella}, together with the text genera-
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Attention weights of the last layers of BART and KG-BART encoder.
tions of different models and human references. We find that the outputs of fine-tuned pre-trained language models have several problems: (1) not covering all concepts, e.g., GPT-2 only covers "hold, umbrella, street", ignoring the "stand", (2) unreasonable commonsense relationship between concepts, e.g. in UniLM, the output "A man stands next to an umbrella on a street" is a rare scenario in daily life, and (3) repeating the same content and incorrect grammar, e.g. in BART, it uses both "holding an umbrella" and "holds an umbrella", which is repeated information, and in GPT-2, the indefinite article of "umbrella" should be "an" rather than "a". By contrast, the output generated by KG-BART covers all concepts and is a relatively reasonable scenario and is comparatively as natural and plausible as the references stated by human.</p>
<p>We also visualize the attention weights of the last layers of KG-BART and BART encoder to validate that our model can capture the better relationship between concepts, as shown in Figures 6. We can see that the related concept pairs in KG-BART attend much more attention, which is consistent with that in the knowledge graph. For example, in practice, "weight" has a strong relationship with "gym" on the knowledge graph and the attention weight between them should be large. However, this strong relationship has not been demonstrated in BART without knowledge graph. Therefore, it is reasonable to introduce a knowledge graph as relationship augmentation for better concept representation, also as a guidance to generate more reasonable sentences further.</p>
<p>Ablation Study To evaluate the contributions of individual components of our proposed framework, we conduct ablation analysis to investigate the following research questions: (1) whether the KG-augmented encoder and decoder improves</p>
<table>
<thead>
<tr>
<th>Ablation methods</th>
<th></th>
<th>BLEU-3/4</th>
<th>ROUGE-2/L</th>
</tr>
</thead>
<tbody>
<tr>
<td>(1) KG-Aug Enc. ✓</td>
<td>Dec. ✗</td>
<td>40.40/29.40</td>
<td>22.66/43.13</td>
</tr>
<tr>
<td>(2) SCI ✗</td>
<td>CSD ✗</td>
<td>41.20/29.70</td>
<td>23.15/43.57</td>
</tr>
<tr>
<td>(3) MGAT ✗</td>
<td>MHGAT ✗</td>
<td>40.90/29.30</td>
<td>22.96/43.78</td>
</tr>
<tr>
<td>(4) Pre-training✗</td>
<td></td>
<td>39.80/27.90</td>
<td>21.87/42.92</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation study of the proposed model. SCI, CSD, MGAT and MHGAT are KG-BART components.</p>
<p>the performance? (2) whether KG-BART is good at incorporating entity embedding with Transformer? (3) does the KG-BART pre-training works?</p>
<p>To this end, we test on the following ablations: (1) textual Transformer with only KG-augmented encoder; (2) using the same entity representation at each subword position rather than using SCI and CSD; (3) concatenate the entity embedding with word embedding rather than using MGAT and MHGAT; and (4) without the KG-BART pre-training. Table 4 summarizes the ablation results. It shows that KG-BART can still outperform all these four variants, certifying the effectiveness of each designed component in our model and we can also see that incorporating KG with the pre-trained model can help the model achieve a better performance.</p>
<p>Transfer KG-BART to Commonsense QA We also investigate whether the ability of generative commonsense reasoning in KG-BART can benefit commonsense-centric downstream tasks such as Commonsense Question Answering (CSQA) (Talmor et al. 2019). We use the models trained on the CommonGen dataset for generating useful context to the question. We extract the nouns and verbs in questions and five choices, and combine the concepts of question q and each choice $c_{i}$ to build concept sets. Then, we construct the concept-reasoning and concept-expanding graphs based on concepts and use these concept sets and the graphs as inputs to KG-BART to generate the context sentence $g_{i}$ for each choice. Finally, we prepend the outputs in front of questions, i.e., “<s>G:g<s>Q:q</s>C:c<s>”. The RoBERTa (Liu et al. 2019) model for CSQA uses the same form without “G:g<s>” in fine-tuning.</p>
<p>We show the learning curve in Figure 7, where $X$ axis is the number of training steps and $Y$ axis is the accuracy on official dev dataset. We find that in most cases, using the context generated by pre-trained models can further improve the performance of original RoBERTa by a large margin. Especially, KG-BART converges at better accuracy from 76.22 (in original RoBERTa) to 79.31 and it outperforms other baselines. We find that the context generated by our model KG-BART can speed up training about 2.5 times, if we look at the 550th steps of KG-BART (75.51) and 1,400th steps of original RoBERTa (75.31).</p>
<h3>Related Work</h3>
<p>Incorporating Commonsense for NLG There are a few recent works that incorporate commonsense knowledge in language generation tasks such as storytelling (Guan, Wang, and Huang 2019), visual storytelling (Yang et al. 2019b), essay generation (Yang et al. 2019a), evidence generation (Liu</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The learning curve of transfer study on CSQA.</p>
<p>et al. 2020b) and conversational generation systems (Zhang et al. 2020). These works suggest that generative commonsense reasoning has great potential to benefit downstream applications. Our proposed model KG-BART, to the best of our knowledge, is the first work on equipping the pre-trained language generation model with the external commonsense knowledge for the constrained language generation.</p>
<p>Enhancing Pre-Trained Model with Knowledge Recently, several works have attempted to learn joint representation learning of words and entities for effectively leveraging external KGs on language understanding tasks and achieved promising results. ERNIE (Zhang et al. 2019) incorporates informative entities from KG aligning with context to enhance pre-training language understanding. KEPLER (Wang et al. 2020) encodes textual descriptions of entities with a pre-trained language understanding model, and then jointly optimize the knowledge embedding and language modeling objectives. K-BERT (Liu et al. 2020a) injects domain knowledge into the models by adding triples from the knowledge graph as supplementary words. Inspired by these works, we argue that extra knowledge information can effectively benefit existing pre-training models on the language understanding tasks. In this paper, we utilize KGs to train an enhanced language generation model by incorporating the entity relationships to improve the language representation.</p>
<h3>Conclusion</h3>
<p>We have presented a KG-augmented approach KG-BART based on pre-trained BART for generative commonsense reasoning. Through capturing the relations among concepts over a KG, KG-BART can generate high-quality sentences even in the unseen concept sets. KG-BART further considers the neighbor entities of each concept node as to generate more natural and logical sentences. It can also be extended to any seq2seq pre-trained language generation models, like T5 (Raffel et al. 2020) and MASS (Song et al. 2019). Experimental results demonstrate that KG-BART has better abilities of both commonsense reasoning and text generalization.</p>
<h3>Acknowledgements</h3>
<p>We would like to thank all the reviewers for their helpful comments. This work is supported by NSF under grants III-1763325, III-1909323, and SaTC-1930941.</p>
<h2>References</h2>
<p>Anderson, P.; Fernando, B.; Johnson, M.; and Gould, S. 2016. Spice: Semantic propositional image caption evaluation. In Proceedings of ECCV, 382-398. Springer.
Banerjee, S.; and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL workshop.
Bao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Liu, X.; Wang, Y.; Piao, S.; Gao, J.; Zhou, M.; et al. 2020. Unilmv2: Pseudo-masked language models for unified language model pre-training. arXiv preprint arXiv:2002.12804 .
Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In Proceedings of NeurIPS, 2787-2795.
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. In Proceedings of NeurIPS.
Dong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y.; Gao, J.; Zhou, M.; and Hon, H.-W. 2019. Unified language model pre-training for natural language understanding and generation. In Proceedings of NeurIPS, 13063-13075.
Guan, J.; Wang, Y.; and Huang, M. 2019. Story ending generation with incremental encoding and commonsense knowledge. In Proceedings of AAAI, volume 33, 6473-6480.
Hendrycks, D.; and Gimpel, K. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 .
Huang, L.; Bras, R. L.; Bhagavatula, C.; and Choi, Y. 2019. Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of EMNLP.
Kim, Y. 2014. Convolutional neural networks for sentence classification. In Proceedings of EMNLP, 1746-1751.
Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mohamed, A.; Levy, O.; Stoyanov, V.; and Zettlemoyer, L. 2020. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of ACL, 7871-7880.
Lin, B. Y.; Chen, X.; Chen, J.; and Ren, X. 2019. Kagnet: Knowledge-aware graph networks for commonsense reasoning. In Proceedings of EMNLP, 2829-2839.
Lin, B. Y.; Shen, M.; Zhou, W.; Zhou, P.; Bhagavatula, C.; Choi, Y.; and Ren, X. 2020. CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning. In Proceedings of EMNLP findings.
Lin, C.-Y. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of Text summarization branches out, 74-81.
Liu, W.; Zhou, P.; Zhao, Z.; Wang, Z.; Ju, Q.; Deng, H.; and Wang, P. 2020a. K-BERT: Enabling Language Representation with Knowledge Graph. In Proceedings of AAAI.
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 .</p>
<p>Liu, Y.; Yang, T.; You, Z.; Fan, W.; and Yu, P. S. 2020b. Commonsense Evidence Generation and Injection in Reading Comprehension. In Proceedings of SIGDIAL, 61-73.
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL, 311-318.
Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP, 1532-1543.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language models are unsupervised multitask learners. OpenAI Blog 1(8): 9.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; and Narang, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR .
Song, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T.-Y. 2019. Mass: Masked sequence to sequence pre-training for language generation. In Proceedings of ICML, 5926-5936.
Speer, R.; Chin, J.; and Havasi, C. 2017. ConceptNet 5.5: an open multilingual graph of general knowledge. In Proceedings of AAAI, 4444-4451.
Talmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Proceedings of NAACL.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In Proceedings of NeurIPS, 5998-6008.
Vedantam, R.; Lawrence Zitnick, C.; and Parikh, D. 2015. Cider: Consensus-based image description evaluation. In Proceedings of CVPR, 4566-4575.
Veličković, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, P.; and Bengio, Y. 2017. Graph attention networks. In Proceedings of ICLR.
Wang, X.; Gao, T.; Zhu, Z.; Liu, Z.; Li, J.; and Tang, J. 2020. KEPLER: A unified model for knowledge embedding and pre-trained language representation. TACL .
Wilcoxon, F.; Katti, S.; and Wilcox, R. A. 1970. Critical values and probability levels for the Wilcoxon rank sum test and the Wilcoxon signed rank test. Selected tables in mathematical statistics 1: 171-259.
Yang, P.; Li, L.; Luo, F.; Liu, T.; and Sun, X. 2019a. Enhancing topic-to-essay generation with external commonsense knowledge. In Proceedings of ACL, 2002-2012.
Yang, P.; Luo, F.; Chen, P.; Li, L.; Yin, Z.; He, X.; and Sun, X. 2019b. Knowledgeable Storyteller: A Commonsense-Driven Generative Model for Visual Storytelling. In Proceedings of IJCAI, 5356-5362.
Zhang, H.; Liu, Z.; Xiong, C.; and Liu, Z. 2020. Grounded conversation generation as guided traverses in commonsense knowledge graphs. In Proceedings of ACL, 2031-2043.
Zhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu, Q. 2019. ERNIE: Enhanced language representation with informative entities. In Proceedings of ACL, 1441-1451.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://inklab.usc.edu/CommonGen/leaderboard.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
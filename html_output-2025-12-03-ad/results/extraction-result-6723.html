<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6723 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6723</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6723</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-d4bf36cbc5855ea87235d7a64f406717ac6aa3c9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d4bf36cbc5855ea87235d7a64f406717ac6aa3c9" target="_blank">Large Language Models as Analogical Reasoners</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> Experimental results show that this approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.</p>
                <p><strong>Paper Abstract:</strong> Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6723.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6723.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analogical prompting (self-generated exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analogical prompting: Self-generated exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that asks an LLM to recall/generate several relevant and diverse problem-solution exemplars in-context before solving a target problem, thereby guiding its chain-of-thought reasoning without requiring labeled exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Analogical prompting (self-generated exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (chain-of-thought style, in-context exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school / elementary math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>77.8</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>5-shot CoT (fixed exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>1.1</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Self-generated exemplars tailored to each problem yield modest gains vs. fixed few-shot CoT; authors emphasize generating distinct/diverse exemplars (instruction in prompt) to avoid repetition and improve guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6723.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6723.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analogical prompting (self-generated exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analogical prompting: Self-generated exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same self-generation prompting approach applied to more advanced math problems (MATH).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Analogical prompting (self-generated exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (chain-of-thought style)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>advanced/high-school competition-style math problems spanning algebra, probability, geometry, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>37.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>5-shot CoT (fixed exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.4</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Larger relative improvement on MATH (more diverse reasoning types), consistent with the idea that tailored exemplars help across diverse reasoning categories.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6723.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6723.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analogical prompting (knowledge + exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analogical prompting: Self-generated high-level knowledge plus exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant that asks the LLM to produce a short tutorial/core concepts (knowledge) before generating exemplars, to encourage higher-level takeaways and alignment of exemplars to core strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-16k</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 family (API variant)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Analogical prompting (self-generated knowledge + exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (mixed: knowledge + exemplars before solution)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Codeforces (competitive programming)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>algorithmic code generation for competitive programming problems (requires nontrivial algorithms: DP, graphs, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Acc@1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>15.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Analogical prompting (self-generated exemplars) Acc@1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Generating high-level knowledge before exemplars improved generalization for code generation by preventing overreliance on low-level exemplars and producing exemplars aligned to core algorithms (authors saw +2 percentage points in Acc@1).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6723.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6723.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analogical prompting (knowledge + exemplars) GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analogical prompting: Self-generated knowledge + exemplars (evaluated with GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same knowledge+exemplars technique evaluated with GPT-4 on Codeforces; shows gains over few-shot CoT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Analogical prompting (self-generated knowledge + exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (mixed)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Codeforces (competitive programming)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>algorithmic code generation; measured via test-case pass rates (Acc@1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Acc@1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>19.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>3-shot CoT (manual few-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>On Codeforces, knowledge+exemplars improves Acc@1/Acc@10; GPT-4 also benefits, showing the approach complements stronger base LLMs and narrows gap between smaller and larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6723.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6723.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analogical prompting on BIG-Bench (word sorting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analogical prompting: Self-generated exemplars applied to BIG-Bench word-sorting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying self-generated exemplars to diverse reasoning tasks in BIG-Bench yields substantial gains on some tasks compared to 0-shot CoT and manual few-shot CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Analogical prompting (self-generated exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (in-context exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>BIG-Bench (word sorting)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>word-sorting reasoning task from BIG-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>75.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>3-shot CoT (manual few-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>6.8</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report large improvement on word-sorting and several BIG-Bench tasks; method is competitive or superior to manual few-shot CoT, presumably because exemplars are tailored to each test problem.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6723.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6723.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diversity ablation (self-generated exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation: Diverse vs non-diverse self-generated exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled ablation comparing instructing the LLM to produce diverse exemplars vs not encouraging diversity; demonstrates diversity instruction yields better downstream reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Analogical prompting (diversity ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse vs homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (also MATH reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school math (GSM8K) and advanced math (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>77.8</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Ours: Non-diverse exemplars (GSM8K 75.9)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>1.9</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Encouraging diversity in generated exemplars improves accuracy (GSM8K: 77.8% vs 75.9%; MATH: 37.3% vs 35.2%), suggesting variety of reasoning styles/exemplars aids generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6723.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6723.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generation vs Retrieval scaling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison: Self-generated exemplars vs retrieved few-shot exemplars across model scale</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison showing self-generation outperforms retrieval-based few-shot CoT for larger LLMs, while retrieval performs better for smaller LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-generated exemplars vs retrieved CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based vs generation-based (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school math problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>61.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>5-shot retrieved CoT (text-davinci-003: 57.0)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>4.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report self-generation performs better with larger-scale LLMs (e.g., text-davinci-003: self-gen 61% vs retrieved 57%), whereas retrieval is more reliable for smaller LLMs that cannot generate valid exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6723.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6723.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency + Analogical prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency applied on top of analogical prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using self-consistency (ensemble of sampled reasoning traces) combined with analogical prompting yields substantial additional gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-consistency (ensemble) + Analogical prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble (sampling / majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (ensemble of reasoning traces)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school math problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>85.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Analogical prompting (no self-consistency) 77.8</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>7.5</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Self-consistency further boosts analogical prompting substantially (GSM8K: 77.8 -> 85.3; MATH: 37.3 -> 46.0), indicating ensembles over diverse reasoning traces improve final answer reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Analogical Reasoners', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator <em>(Rating: 2)</em></li>
                <li>Emergent analogical reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Recitation-augmented language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6723",
    "paper_id": "paper-d4bf36cbc5855ea87235d7a64f406717ac6aa3c9",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "Analogical prompting (self-generated exemplars)",
            "name_full": "Analogical prompting: Self-generated exemplars",
            "brief_description": "A prompting technique that asks an LLM to recall/generate several relevant and diverse problem-solution exemplars in-context before solving a target problem, thereby guiding its chain-of-thought reasoning without requiring labeled exemplars.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": "",
            "reasoning_method_name": "Analogical prompting (self-generated exemplars)",
            "reasoning_method_type": "sequential (chain-of-thought style, in-context exemplars)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school / elementary math word problems",
            "performance_metric": "accuracy",
            "performance_value": 77.8,
            "comparison_target_method": "5-shot CoT (fixed exemplars)",
            "performance_difference": 1.1,
            "statistical_significance": null,
            "analysis_notes": "Self-generated exemplars tailored to each problem yield modest gains vs. fixed few-shot CoT; authors emphasize generating distinct/diverse exemplars (instruction in prompt) to avoid repetition and improve guidance.",
            "ablation_study_present": true,
            "uuid": "e6723.0",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Analogical prompting (self-generated exemplars)",
            "name_full": "Analogical prompting: Self-generated exemplars",
            "brief_description": "Same self-generation prompting approach applied to more advanced math problems (MATH).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": "",
            "reasoning_method_name": "Analogical prompting (self-generated exemplars)",
            "reasoning_method_type": "sequential (chain-of-thought style)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "MATH",
            "task_description": "advanced/high-school competition-style math problems spanning algebra, probability, geometry, etc.",
            "performance_metric": "accuracy",
            "performance_value": 37.3,
            "comparison_target_method": "5-shot CoT (fixed exemplars)",
            "performance_difference": 2.4,
            "statistical_significance": null,
            "analysis_notes": "Larger relative improvement on MATH (more diverse reasoning types), consistent with the idea that tailored exemplars help across diverse reasoning categories.",
            "ablation_study_present": true,
            "uuid": "e6723.1",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Analogical prompting (knowledge + exemplars)",
            "name_full": "Analogical prompting: Self-generated high-level knowledge plus exemplars",
            "brief_description": "A variant that asks the LLM to produce a short tutorial/core concepts (knowledge) before generating exemplars, to encourage higher-level takeaways and alignment of exemplars to core strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo-16k",
            "model_size": "GPT-3.5 family (API variant)",
            "reasoning_method_name": "Analogical prompting (self-generated knowledge + exemplars)",
            "reasoning_method_type": "sequential (mixed: knowledge + exemplars before solution)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Codeforces (competitive programming)",
            "task_description": "algorithmic code generation for competitive programming problems (requires nontrivial algorithms: DP, graphs, etc.)",
            "performance_metric": "Acc@1",
            "performance_value": 15.0,
            "comparison_target_method": "Analogical prompting (self-generated exemplars) Acc@1",
            "performance_difference": 2.0,
            "statistical_significance": null,
            "analysis_notes": "Generating high-level knowledge before exemplars improved generalization for code generation by preventing overreliance on low-level exemplars and producing exemplars aligned to core algorithms (authors saw +2 percentage points in Acc@1).",
            "ablation_study_present": true,
            "uuid": "e6723.2",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Analogical prompting (knowledge + exemplars) GPT-4",
            "name_full": "Analogical prompting: Self-generated knowledge + exemplars (evaluated with GPT-4)",
            "brief_description": "Same knowledge+exemplars technique evaluated with GPT-4 on Codeforces; shows gains over few-shot CoT baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "",
            "reasoning_method_name": "Analogical prompting (self-generated knowledge + exemplars)",
            "reasoning_method_type": "sequential (mixed)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Codeforces (competitive programming)",
            "task_description": "algorithmic code generation; measured via test-case pass rates (Acc@1)",
            "performance_metric": "Acc@1",
            "performance_value": 19.0,
            "comparison_target_method": "3-shot CoT (manual few-shot CoT)",
            "performance_difference": 2.0,
            "statistical_significance": null,
            "analysis_notes": "On Codeforces, knowledge+exemplars improves Acc@1/Acc@10; GPT-4 also benefits, showing the approach complements stronger base LLMs and narrows gap between smaller and larger models.",
            "ablation_study_present": true,
            "uuid": "e6723.3",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Analogical prompting on BIG-Bench (word sorting)",
            "name_full": "Analogical prompting: Self-generated exemplars applied to BIG-Bench word-sorting",
            "brief_description": "Applying self-generated exemplars to diverse reasoning tasks in BIG-Bench yields substantial gains on some tasks compared to 0-shot CoT and manual few-shot CoT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": "",
            "reasoning_method_name": "Analogical prompting (self-generated exemplars)",
            "reasoning_method_type": "sequential (in-context exemplars)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "BIG-Bench (word sorting)",
            "task_description": "word-sorting reasoning task from BIG-Bench",
            "performance_metric": "accuracy",
            "performance_value": 75.2,
            "comparison_target_method": "3-shot CoT (manual few-shot CoT)",
            "performance_difference": 6.8,
            "statistical_significance": null,
            "analysis_notes": "Authors report large improvement on word-sorting and several BIG-Bench tasks; method is competitive or superior to manual few-shot CoT, presumably because exemplars are tailored to each test problem.",
            "ablation_study_present": true,
            "uuid": "e6723.4",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Diversity ablation (self-generated exemplars)",
            "name_full": "Ablation: Diverse vs non-diverse self-generated exemplars",
            "brief_description": "Controlled ablation comparing instructing the LLM to produce diverse exemplars vs not encouraging diversity; demonstrates diversity instruction yields better downstream reasoning performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": "",
            "reasoning_method_name": "Analogical prompting (diversity ablation)",
            "reasoning_method_type": "sequential (ablation)",
            "reasoning_style_diversity": "diverse vs homogeneous",
            "benchmark_name": "GSM8K (also MATH reported)",
            "task_description": "grade-school math (GSM8K) and advanced math (MATH)",
            "performance_metric": "accuracy",
            "performance_value": 77.8,
            "comparison_target_method": "Ours: Non-diverse exemplars (GSM8K 75.9)",
            "performance_difference": 1.9,
            "statistical_significance": null,
            "analysis_notes": "Encouraging diversity in generated exemplars improves accuracy (GSM8K: 77.8% vs 75.9%; MATH: 37.3% vs 35.2%), suggesting variety of reasoning styles/exemplars aids generalization.",
            "ablation_study_present": true,
            "uuid": "e6723.5",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Generation vs Retrieval scaling",
            "name_full": "Comparison: Self-generated exemplars vs retrieved few-shot exemplars across model scale",
            "brief_description": "Empirical comparison showing self-generation outperforms retrieval-based few-shot CoT for larger LLMs, while retrieval performs better for smaller LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_size": "",
            "reasoning_method_name": "Self-generated exemplars vs retrieved CoT",
            "reasoning_method_type": "retrieval-based vs generation-based (comparison)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school math problems",
            "performance_metric": "accuracy",
            "performance_value": 61.0,
            "comparison_target_method": "5-shot retrieved CoT (text-davinci-003: 57.0)",
            "performance_difference": 4.0,
            "statistical_significance": null,
            "analysis_notes": "Authors report self-generation performs better with larger-scale LLMs (e.g., text-davinci-003: self-gen 61% vs retrieved 57%), whereas retrieval is more reliable for smaller LLMs that cannot generate valid exemplars.",
            "ablation_study_present": true,
            "uuid": "e6723.6",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-consistency + Analogical prompting",
            "name_full": "Self-consistency applied on top of analogical prompting",
            "brief_description": "Using self-consistency (ensemble of sampled reasoning traces) combined with analogical prompting yields substantial additional gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": "",
            "reasoning_method_name": "Self-consistency (ensemble) + Analogical prompting",
            "reasoning_method_type": "ensemble (sampling / majority voting)",
            "reasoning_style_diversity": "mixed (ensemble of reasoning traces)",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school math problems",
            "performance_metric": "accuracy",
            "performance_value": 85.3,
            "comparison_target_method": "Analogical prompting (no self-consistency) 77.8",
            "performance_difference": 7.5,
            "statistical_significance": null,
            "analysis_notes": "Self-consistency further boosts analogical prompting substantially (GSM8K: 77.8 -&gt; 85.3; MATH: 37.3 -&gt; 46.0), indicating ensembles over diverse reasoning traces improve final answer reliability.",
            "ablation_study_present": true,
            "uuid": "e6723.7",
            "source_info": {
                "paper_title": "Large Language Models as Analogical Reasoners",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
            "rating": 2
        },
        {
            "paper_title": "Emergent analogical reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Recitation-augmented language models",
            "rating": 1
        }
    ],
    "cost": 0.017364249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LARGE LANGUAGE MODELS AS ANALOGICAL REASONERS</h1>
<p>Michihiro Yasunaga, ${ }^{1,2}$ Xinyun Chen, ${ }^{1}$ Yujia Li, ${ }^{1}$ Panupong Pasupat, ${ }^{1}$ Jure Leskovec, ${ }^{2}$<br>Percy Liang, ${ }^{2}$ Ed H. Chi, ${ }^{1}$ Denny Zhou ${ }^{1}$<br>${ }^{1}$ Google DeepMind ${ }^{2}$ Stanford University<br>myasu@cs.stanford.edu, {xinyunchen, dennyzhou}@google.com</p>
<h4>Abstract</h4>
<p>Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0 -shot CoT and manual fewshot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) demonstrate strong performance across various tasks (Brown et al., 2020; Chowdhery et al., 2022; Liang et al., 2022; Qin et al., 2023). Recently, chain-of-thought (CoT) prompting has demonstrated LLMs' abilities to tackle complex tasks, such as solving math problems, by prompting them to generate intermediate reasoning steps (Wei et al., 2022b; Kojima et al., 2022). For instance, common methods like few-shot CoT (Wei et al. 2022b; Figure 1, middle) make LLMs generate reasoning steps by offering a few exemplars of question-rationale-answer triplets; 0 -shot CoT (Kojima et al. 2022; Figure 1, left) aims for the same objective by offering instructions like "think step by step." These studies highlight the importance of devising effective methods to guide LLMs to reason.</p>
<p>However, the existing CoT paradigm faces two key challenges: providing relevant guidance or exemplars of reasoning, and minimizing the need for manual labeling. Specifically, 0 -shot CoT offers generic reasoning guidance, which may not suffice for complex tasks like code generation (6). Few-shot CoT provides more detailed guidance but demands labeled exemplars of the reasoning process, which can be costly to obtain for every task. This raises a research question: can we achieve the best of both worlds and automate the generation of relevant exemplars to guide LLMs' reasoning process?</p>
<p>In this work, we propose analogical prompting, a new prompting approach that automatically guides the reasoning process of LLMs. Our inspiration comes from analogical reasoning in psychology, a concept where humans draw from relevant past experiences to tackle new problems (Vosniadou \&amp; Ortony, 1989). For instance, when faced with a new math problem (e.g., finding the area of a square given four points in a coordinate system; Figure 1), humans often think about "do I know a related problem?" (Polya, 2004) and recall how they solved related problems in the past (e.g., finding the area of a square with a known side length) to derive insights for solving the new problem. They also recall high-level knowledge, such as the need to find the side length to calculate a square's area. Our idea is to prompt LLMs to mimic this reasoning process to effectively solve new problems.</p>
<p>Concretely, given a problem to solve, we prompt LLMs to self-generate relevant exemplars in the context, using instructions like "# Recall relevant problems and solutions:...", and then proceed to solve the original problem (Figure 1, 2). Simultaneously, we can also prompt LLMs to generate high-level knowledge that complements specific exemplars, using instructions like "# Provide a tutorial:..." (Figure 3). This proves particularly useful for complex tasks like code generation (see 6). Notably, our method can operate in a single prompt, generating knowledge, exemplars, and a solution to the initial</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of our approach, analogical prompting. Left: Existing methods for prompting LLM to reason are either generic ( 0 -shot CoT ) or demand labeled exemplars (few-shot CoT). Right: Given a problem, our method prompts LLMs to self-generate relevant exemplars before solving the problem. This eliminates the need for labeling and also tailors the exemplars to each individual problem. See Figure 3 for a sample prompt where the LLM self-generates both knowledge and exemplars.
problem end-to-end in one pass. The underlying idea here is that modern LLMs have already acquired knowledge of various problems during training. Explicitly prompting them to recall relevant problems and solutions in the context guides LLMs to perform in-context learning to solve new problems.</p>
<p>Our proposed approach offers several advantages. It self-generates exemplars and obviates the need for manually labeling reasoning exemplars for each task, addressing the challenges faced by 0 -shot and few-shot CoT. Furthermore, the self-generated exemplars are tailored to individual problems, such as 'geometry' or 'probability', rather than generic 'math problems'. This can simplify the complexity associated with recent CoT techniques that retrieve relevant exemplars from external data (Zhang et al., 2022b; Shum et al., 2023).</p>
<p>We evaluate the proposed approach in various reasoning-intensive tasks, including mathematical problem solving in GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), code generation in Codeforces, and other reasoning tasks in BIG-Bench (Srivastava et al., 2022). We use several base LLMs: GPT-3.5, GPT-4 (OpenAI, 2023; Ouyang et al., 2022), and PaLM2 (Anil et al., 2023). Experimental results show that our approach outperforms 0 -shot CoT and few-shot CoT across a range of tasks and base LLMs, achieving an average accuracy gain of $+4 \%$. Notably, our approach improves performance on tasks involving diverse types of reasoning, such as MATH (including algebra, probability, geometry, etc.) and Codeforces (involving dynamic programming, graph algorithms, etc.). This result suggests the effectiveness of generating tailored exemplars for individual problems to guide the reasoning process of LLMs.</p>
<h1>2 RELATED WORKS</h1>
<h3>2.1 LARGE LANGUAGE MODELS AND PROMPTING</h3>
<p>A language model estimates probabilities over text. Recent research has scaled up these models from millions (Devlin et al., 2019) to billions of parameters (Brown et al., 2020) and expanded training data to include web texts and instruction data (Gao et al., 2020; Ouyang et al., 2022; Chung et al., 2022). These advances have made large language models proficient in various NLP tasks.</p>
<p>LLMs with billions of parameters demonstrate in-context learning and few-shot learning abilities (Brown et al., 2020; Liu et al., 2022; Su et al., 2022; Mishra et al., 2022; Wei et al., 2022a; Yasunaga et al., 2023; Shi et al., 2023). They use input prompts (instructions or a few exemplars) to guide LLMs to generate desired responses for tasks, marking the advent of the prompting era. Our approach harnesses the in-context learning abilities of LLMs to guide their reasoning process using self-generated exemplars.</p>
<p>Closely related to ours are works that perform self-generation in LLM prompting (Sun et al., 2022; He et al., 2023; Kim et al., 2022; Li et al., 2022a). For instance, Sun et al. (2022) prompts LLMs to recite relevant facts in context for open-domain question answering. Our idea of self-generating exemplars is related to recitation, but focuses on recalling problem-solving and reasoning processes rather than factual knowledge.</p>
<h1>2.2 CHAIN-OF-THOUGHT PROMPTING</h1>
<p>Chain-of-thought (CoT; Wei et al. 2022b) is a prompting strategy that guides LLMs to produce intermediate reasoning steps towards a final answer, enhancing problem-solving performance. Common instances of CoT include 0 -shot CoT (Kojima et al., 2022) and few-shot CoT Wei et al. (2022b).
0-shot CoT prompts LLMs with a general instruction like "think step by step" to produce intermediate reasoning steps. Few-shot CoT achieves stronger performance by providing multiple exemplars of reasoning process (question-rationale-answer), leveraging LLMs' in-context learning abilities. However, it requires labeled exemplars. Our approach tackles this challenge by prompting LLMs to self-generate exemplars.
Within few-shot CoT, the original approach employs a fixed set of labeled exemplars for all test problems. Recent work explores retrieval-based CoT, which aims to obtain more relevant exemplars from external data for each problem (Zhang et al., 2022b; Shum et al., 2023). While our work shares the goal of providing relevant exemplars, instead of retrieval, we make LLMs self-generate exemplars. Self-generation offers several advantages: it is simpler, as it does not require external data retrieval, and it is more versatile, as it can produce not only specific exemplars but also broader insights or knowledge that complement them. Empirically, our generation-based CoT outperforms retrieval-based CoT, especially with larger base LLMs, while retrieval-based CoT excels with smaller base LLMs (6).</p>
<p>Finally, there are other techniques for enhancing CoT, such as self-consistency (Wang et al., 2022) and least-to-most (Zhou et al., 2022). Our work can complement and integrate with these efforts.</p>
<p>Please see $\S \mathrm{A}$ for additional related works.</p>
<h2>3 Preliminaries</h2>
<p>We focus on problem-solving tasks, where the objective is to produce a solution $y$ for a given problem statement $x$, such as mathematical questions or code generation specifications. The solution may include both the intermediate reasoning steps or rationale $r$ and the final answer $a$.</p>
<p>A prompting method $\phi$ is a function that maps a problem statement $x$ into a specific textual input $\phi(x)$ for an LLM, which then generates a solution $\hat{y}=\operatorname{LLM}(\phi(x))$. For instance,</p>
<ul>
<li>In 0 -shot prompting, $\phi$ directly yields $x$.</li>
<li>In 0 -shot CoT, $\phi$ supplements $x$ with a general instruction, such as " $[x]$ think step by step".</li>
<li>In few-shot CoT, $\phi$ supplements $x$ with several labeled exemplars, $\left{\left(x_{i}, r_{i}, a_{i}\right)\right}<em 1="1">{i=1}^{K}$, such as " $\left[x</em>\right][x]$ ".}\right]$ $\left[r_{1}\right]\left[a_{1}\right] \ldots\left[x_{K}\right]\left[r_{K}\right]\left[a_{K</li>
</ul>
<p>Our aim is to design a prompting method $\phi$ that enhances the accuracy of solutions LLMs generate.</p>
<h2>4 APPROACH</h2>
<p>We introduce analogical prompting, a new prompting approach that automatically provides exemplars to guide LLMs' reasoning process. Inspired by how humans recall relevant past experiences when tackling new problems, our approach makes LLMs self-generate relevant exemplars or knowledge in context, before proceeding to solve the problem (Figure 1, right). We present two techniques to achieve this: self-generated exemplars (4.1) and self-generated knowledge + exemplars (4.2).</p>
<h3>4.1 SELF-GENERATED EXEMPLARS</h3>
<p>Our approach is based on the idea that modern LLMs possess a broad range of problem-solving knowledge acquired during training. Explicitly prompting them to recall or generate relevant problems and solutions in context aids LLMs to perform in-context learning to solve new problems.</p>
<p>Specifically, given a target problem to solve $x$, our prompt augments it with instructions like:</p>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="nl">Problem</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">[</span><span class="n">x</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="err">#</span><span class="w"> </span><span class="n">Relevant</span><span class="w"> </span><span class="nl">problems</span><span class="p">:</span><span class="w"> </span><span class="n">Recall</span><span class="w"> </span><span class="n">three</span><span class="w"> </span><span class="n">relevant</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">distinct</span><span class="w"> </span><span class="n">problems</span><span class="p">.</span><span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="k">each</span>
<span class="n">problem</span><span class="p">,</span><span class="w"> </span><span class="k">describe</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">explain</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">solution</span><span class="p">.</span>
<span class="err">#</span><span class="w"> </span><span class="n">Solve</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">initial</span><span class="w"> </span><span class="nl">problem</span><span class="p">:</span>
</code></pre></div>

<p>For a concrete example, see Figure 2. The LLM first generates several $(K)$ relevant exemplars in the form of question-rationale-answer sequences ("# Relevant problems:" part of the instruction). Then the model proceeds to solve the initial problem, leveraging the recalled exemplars in the context ("# Solve the initial problem:" part of the instruction). Note that all these instructions are provided within a single prompt, allowing the LLM to generate relevant problems and solution to the initial problem in one continuous pass. Using '#' symbols in the prompt (e.g., '# Relevant Problems') helps LLMs structure the response better.
Below are key technical decisions we made:</p>
<ul>
<li>Generating relevant and diverse exemplars is important: To achieve this, we explicitly include an instruction in the prompt, such as "generate problems that are distinct from each other" (e.g., Figure 2). This step is crucial as some LLMs have a tendency to repetitively generate identical problems, which can be misleading when solving the target problem.</li>
<li>Single-pass vs. independent exemplar generation: An alternative approach is to independently generate exemplars by separately sampling them from the LLM and then re-prompt the LLM with all the exemplars. While this method does work, our current single-pass prompt approach achieves comparable performance and offers greater convenience, eliminating the need for multiple prompts. Consequently, we have chosen to adopt the single-pass method.</li>
<li>The number of exemplars to generate $(K)$ : Through experimentation, we have found that generating $K=3$ to 5 exemplars works the best (more details in $\S 6.5$ ).
Our approach offers two advantages. It offers detailed exemplars of reasoning without manual labeling, addressing the challenges in 0 -shot and few-shot CoT. The generated exemplars are tailored to individual problems (e.g., 'geometry' or 'probability'), offering more relevant guidance than traditional few-shot CoT, which uses fixed exemplars (e.g., general math problems; Figure 1, middle).</li>
</ul>
<h1>4.2 SELF-GENERATED KNOWLEDGE + EXEMPLARS</h1>
<p>While generating exemplars is useful, in complex tasks like code generation, LLMs may overly rely on the low-level exemplars and fail to generalize when solving the target problems. To address this challenge, we also allow LLMs to self-generate high-level takeaways that complements the exemplars, which we refer to as "knowledge." Specifically, we enhance the prompt with an additional instruction like the following. For a concrete example, see Figure 3.
# Tutorial: Identify core concepts in the problem and provide a tutorial.
One technical consideration is whether to generate knowledge before or after exemplars. We found that generating knowledge before exemplars yields superior results (Table 7). By generating knowledge first, LLMs identify the core concepts of the problem. This, in turn, helps LLMs generate exemplars that align more closely in terms of the fundamental problem-solving approaches rather than surface-level lexical similarities. For further discussion, please refer to $\S 6.2$.</p>
<h2>5 EXPERIMENTAL SETUP</h2>
<h3>5.1 TASKS</h3>
<p>We evaluate the proposed approach in diverse reasoning-intensive tasks, including mathematical problem solving, code generation, and other reasoning tasks like logical and temporal reasoning.
Mathematical problem solving. We use popular benchmarks, GSM8K (Cobbe et al., 2021), comprising elementary math word problems, and MATH (Hendrycks et al., 2021b), consisting of advanced math problems from high school math competitions. For each problem, we obtain an output from LLMs using a temperature of 0 , and report the accuracy.
Code generation. Code generation involves synthesizing programs to solve algorithmic problems. Competitive programming is especially challenging, requiring reasoning about various algorithms like dynamic programming and graphs (Li et al., 2022b; Kulal et al., 2019; Yasunaga \&amp; Liang, 2020).
As a benchmark, we collected competitive programming problems from codeforces.com (details in B). We focus on level-A problems published in 2023 to prevent test set contamination (Magar \&amp; Schwartz, 2022). Each problem comprises a problem statement, which serves as input to LLMs, and a set of test cases to assess generated code. The correctness of code is determined by whether it passes all test cases.</p>
<p>Our Prompt: Self-generate Exemplars
Your task is to tackle mathematical problems. When presented with a math problem, recall relevant problems as examples. Afterward, proceed to solve the initial problem.</p>
<h1>Problem:</h1>
<p>An airline serves a dinner to all the passengers on an airplane. They get their choice of steak or fish. Three steak meals and three fish meals are set aside for the six-member crew. If the meals are distributed to the crew members randomly, what is the probability that both pilots get the fish?</p>
<h2># Instructions:</h2>
<h2>## Relevant Problems:</h2>
<p>Recall three examples of math problems that are relevant to the initial problem. Your problems should be distinct from each other and from the initial problem (e.g., involving different numbers and names). For each problem:</p>
<ul>
<li>After "Q: ", describe the problem</li>
<li>After "A: ", explain the solution and enclose the ultimate answer in \boxed().
## Solve the Initial Problem:
Q: Copy and paste the initial problem here.
A: Explain the solution and enclose the ultimate answer in \boxed() here.</li>
</ul>
<h2>Output by GPT3.5-turbo</h2>
<h2>Relevant Problems:</h2>
<p>Q: In a bag, there are 5 red balls and 3 blue balls. If two balls are drawn at random without replacement, what is the probability that both balls are red?
A: The total number of balls is 8 . The probability of drawing a red ball on the first draw is $5 / 8$. After one red ball is drawn, there are 4 red balls left out of 7 total balls. Therefore, the probability of drawing a second red ball is $4 / 7$. The probability of both events happening is the product of the individual probabilities, so the probability of drawing two red balls is $(5 / 8)+(4 / 7)=$ $\backslash$ boxed $\backslash$ frac $(5)(14)}$.</p>
<p>Q: A box contains 10 red marbles and 5 blue marbles. If three marbles are drawn at random without replacement, what is the probability that all three marbles are red?
A: The total number of marbles is 15 . The probability of drawing a red marble on the first draw is $18 / 15$. After one red marble is drawn, there are 9 red marbles left out of 14 total marbles. Therefore, the probability of drawing a second red marble is $5 / 14$. After two red marbles are drawn, there are 8 red marbles left out of 13 total marbles. Therefore, the probability of drawing a third red marble is $8 / 13$. The probability of all three events happening is the product of the individual probabilities, so the probability of drawing three red marbles is $(18 / 15)+(5 / 14)+(8 / 13)=\backslash$ boxed $\backslash$ frac $(26)(91)}$.</p>
<p>Q: A bag contains 4 green marbles, 3 red marbles, and 2 blue marbles. If two marbles are drawn at random without replacement, what is the probability that both marbles are green?
A: The total number of marbles is 9 . The probability of drawing a green marble on the first draw is $4 / 9$. After one green marble is drawn, there are 3 green marbles left out of 8 total marbles. Therefore, the probability of drawing a second green marble is $3 / 8$. The probability of both events happening is the product of the individual probabilities, so the probability of drawing two green marbles is $(4 / 9)+(3 / 8)=\backslash$ boxed $\backslash$ frac $(1)(6)}$.</p>
<h2>Let's solve the following math problem:</h2>
<p>An airline serves a dinner to all the passengers on an airplane. They get their choice of steak or fish. Three steak meals and three fish meals are set aside for the six-member crew. If the meals are distributed to the crew members randomly, what is the probability that both pilots get the fish?
A: There are a total of 6 crew members, so the probability of the first pilot getting the fish is $3 / 6$. After one fish meal is given to the first pilot, there are 2 fish meals left out of 5 total meals. Therefore, the probability of the second pilot getting the fish is $2 / 5$. The probability of both events happening is the product of the individual probabilities, so the probability that both pilots get the fish is $(3 / 6)+(2 / 5)=\backslash$ boxed $\backslash$ frac $(12[5]}$.</p>
<p>Figure 2: Actual example of our prompt (top) and LLM output (bottom) for MATH task. Top: Our prompt supplements the problem statement with instructions to generate relevant exemplars and then solve the problem. Bottom: Exemplars generated by GPT3.5-turbo are indeed relevant to the problem, focusing on probability. It then accurately solves the problem. See D. 1 for the complete prompt and output. Using '#' symbols in the prompt (e.g., '# Relevant Problems') helps LLMs structure the response better.</p>
<p>In line with existing work on code generation (Li et al., 2022b; Chen et al., 2023), we report the Acc@1 and Acc@10 metrics. Acc@ $k$ measures whether at least one of the $k$ sampled model outputs is correct. For each problem, we sample 10 outputs from LLMs, using a temperature of 0.7 .</p>
<p>Other reasoning tasks. We further evaluate on various reasoning tasks in BIG-Bench (Srivastava et al., 2022; Suzgun et al., 2022): word sorting, logical deduction five objects, temporal sequences, reasoning about colored objects, and formal fallacies. These tasks are diverse and may not have dedicated training data, so they align well with our approach of self-generating custom exemplars. For each problem, we obtain an output from LLMs using a temperature of 0 , and report the accuracy.</p>
<h3>5.2 MODELS</h3>
<p>We experiment with several base LLMs: GPT-3.5-turbo, GPT-4 (OpenAI, 2023; Ouyang et al., 2022) (accessed in June-September 2023), and PaLM 2-L (Anil et al., 2023).</p>
<h3>5.3 METHODS TO COMPARE</h3>
<p>We compare the following prompting methods, including ours.
0-shot and 0-shot CoT. These methods, like ours, do not use labeled exemplars. We aim to show that our method offers more tailored guidance for LLM reasoning and yields superior task performance.</p>
<p>Our Prompt: Self-generate Exemplars + Knowledge
Your goal is to write Python3 code to solve competitive programming problems. Given a problem, explain the core concepts in it and provide other relevant problems. Then solve the original problem.</p>
<h1>Problem:</h1>
<p>You are given a sequence a_1, a_2, ... a_n. Each element of a is 1 or 2. Find out if an integer k exists so that the following conditions are met. $1 \leq k \leq n+1$, and $a_{1} 1 \quad a_{1} 2 \quad a_{1} k=a_{1}\left(k+1\right) \quad a_{1} n$. If there exist multiple $k$ that satisfy the given condition, print the smallest.</p>
<h2># Instruction:</h2>
<h2># Algorithms:</h2>
<p>Identify the core concepts or algorithms used to solve the problem.</p>
<h2>## Tutorial:</h2>
<p>Write a tutorial about these algorithms.</p>
<h2>## Example Problems:</h2>
<p>Provide three examples of relevant competitive programming problems that involve these algorithms. For each problem, describe the problem, explain the solution in detail, and then write the correct Python3 code.</p>
<h2>## Python3 code to solve the original problem:</h2>
<ul>
<li>Explanation of the solution:</li>
<li>Python3 code to solve the problem:</li>
</ul>
<h2>Output by GPT3.5-turbo</h2>
<p>## Algorithms: The core algorithm for solving this problem is the prefix product algorithm.
Self-generated Knowledge
## Tutorial: The prefix product algorithm is used to calculate the product of all the elements in a prefix of an array. The algorithm works by initializing a variable "prefix" to 1 , and then iterating through the array from left to right. At each iteration, the current element is multiplied with the prefix, and the result is stored in the prefix variable. This way, the prefix variable keeps track of the product of all the elements in the prefix of the array. The time complexity of this algorithm is 50x5, where $n$ is the size of the input array.</p>
<h2>## Example Problems:</h2>
<p>Self-generated Exemplars</p>
<ol>
<li>Problem: Given an array of integers, find the product of all the elements in each subarray of size $k$.</li>
</ol>
<p>Solution: We can use the prefix product algorithm to solve this problem. We initialize a prefix variable to 1 , and then iterate through the array from left to right. At each iteration, we multiply the current element with the prefix, . . . .
def product_of_subarrays(arr, k):
prefix $=1$
result $=[]$
for i in range(len(arr)):
prefix $=$ prefix $* \operatorname{arr}[i]$
if i == k-1:
result.append(prefix)
prefix $=$ prefix $/ / \operatorname{arr}[i-k+1]$
return result
2. Problem: Given an array of integers, find the longest subarray with a product less than or equal to a given target.</p>
<p>Solution: $\qquad$
3. Problem: Given an array of integers, find the smallest $k$ such that the product of all the elements in the prefix of the array is equal to the product of all the elements in the suffix of the array.</p>
<p>Solution: $\qquad$
## Python3 code to solve the original problem:
Explanation of the solution:</p>
<ul>
<li>Initialize prefix and suffix variables to 1 , and $k$ to -1 .</li>
<li>Calculate the product of all the elements in the sequence using the prefix product algorithm.</li>
<li>Iterate through the sequence from right to left.</li>
<li>Update the prefix and suffix variables.</li>
<li>If the prefix is equal to the suffix, update $k$ to the current index.</li>
<li>Print the value of $k$.
$t=$ int(input())
for _ in range(t):
$n=$ int(input())
arr = list(map(int, input().split()))
prefix, suffix $=1,1$
$k=-1$
for i in range(v):
prefix $=$ prefix $<em> \operatorname{arr}[i]$
for i in range(v-1, 0, -1):
prefix $=$ prefix $/ / \operatorname{arr}[i]$
suffix $=$ suffix $</em> \operatorname{arr}[i]$
if prefix == suffix:
$k=i$
$\operatorname{print}(k)$
Figure 3: Actual example of our prompt (top) and LLM output (bottom) for the Codeforces task. Top: Our prompt supplements the problem statement with instructions to generate knowledge (e.g., tutorials on core concepts) and relevant exemplars, followed by solving the original problem. Bottom: The knowledge and exemplars generated by GPT3.5-turbo are indeed relevant to the problem to solve, focusing on the prefix product algorithm. The final code generated by the LLM effectively applies the algorithm to solve the problem. See D. 3 for the complete prompt and output.</li>
</ul>
<p>Few-shot CoT. This is the standard few-shot CoT, using a fixed set of reasoning exemplars across all test problems within a dataset. For the GSM8K and MATH datasets, as their training sets include solutions labeled with reasoning steps, we use $K=5$ exemplars from these training sets. For the other datasets, we use $K=3$ manually-annotated exemplars. We aim to show that our method, which self-generates exemplars, can match or surpass this baseline, which uses labeled exemplars.</p>
<p>Few-shot retrieved CoT. Instead of using a fixed set of exemplars, for each test problem, we dynamically retrieve relevant labeled problem-solution pairs from the train set for each test problem. Specifically, we</p>
<table>
<thead>
<tr>
<th>Prompting Method</th>
<th>GSM8K Accuracy</th>
<th></th>
<th></th>
<th>MATH Accuracy</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GPT3.5-turbo</td>
<td>text-davinci-003</td>
<td>PaLM2</td>
<td>GPT3.5-turbo</td>
<td>PaLM2</td>
</tr>
<tr>
<td>0-shot</td>
<td>75.0\%</td>
<td>14.8\%</td>
<td>60.8\%</td>
<td>33.0\%</td>
<td>27.1\%</td>
</tr>
<tr>
<td>0-shot CoT</td>
<td>75.8\%</td>
<td>50.3\%</td>
<td>78.2\%</td>
<td>33.9\%</td>
<td>29.8\%</td>
</tr>
<tr>
<td>5-shot CoT</td>
<td>76.7\%</td>
<td>54.0\%</td>
<td>80.7\%</td>
<td>34.9\%</td>
<td>34.3\%</td>
</tr>
<tr>
<td>Ours: Self-generated Exemplars</td>
<td>77.8\%</td>
<td>61.0\% $]$</td>
<td>81.7\%</td>
<td>37.3\%</td>
<td>34.8\%</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance on mathematical tasks, GSM8K and MATH. Our prompting method, which self-generates exemplars, outperforms baselines such as 0 -shot CoT and few-shot CoT. ${ }^{\dagger}$ For text-davinci models, we use an in-context demonstration of how to generate exemplars. For the other models, we do not.</p>
<table>
<thead>
<tr>
<th>Prompting Method</th>
<th>GPT3.5-turbo-16k</th>
<th></th>
<th>GPT4</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Acc@1</td>
<td>Acc@10</td>
<td>Acc@1</td>
<td>Acc@10</td>
</tr>
<tr>
<td>0-shot</td>
<td>$8 \%$</td>
<td>$24 \%$</td>
<td>$16 \%$</td>
<td>$30 \%$</td>
</tr>
<tr>
<td>0-shot CoT</td>
<td>$9 \%$</td>
<td>$27 \%$</td>
<td>$16 \%$</td>
<td>$29 \%$</td>
</tr>
<tr>
<td>3-shot CoT</td>
<td>$11 \%$</td>
<td>$27 \%$</td>
<td>$17 \%$</td>
<td>$31 \%$</td>
</tr>
<tr>
<td>Ours: Self-generated Exemplars</td>
<td>$13 \%$</td>
<td>$25 \%$</td>
<td>$17 \%$</td>
<td>$32 \%$</td>
</tr>
<tr>
<td>Ours: Self-generated Knowledge + Exemplars</td>
<td>$\mathbf{1 5 \%}$</td>
<td>$\mathbf{2 9 \%}$</td>
<td>$\mathbf{1 9 \%}$</td>
<td>$\mathbf{3 7 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance on Codeforces code generation task. Our prompting method outperforms baselines such as 0 -shot CoT and few-shot CoT. Moreover, self-generating knowledge provides additional gains over self-generating exemplars, demonstrating its usefulness for the challenging Codeforces task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompting Method</th>
<th style="text-align: center;">Word <br> sorting</th>
<th style="text-align: center;">Logical deduction <br> five objects</th>
<th style="text-align: center;">Temporal <br> sequences</th>
<th style="text-align: center;">Reasoning about <br> colored objects</th>
<th style="text-align: center;">Formal <br> fallacies</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0-shot</td>
<td style="text-align: center;">$66.8 \%$</td>
<td style="text-align: center;">$30.0 \%$</td>
<td style="text-align: center;">$40.4 \%$</td>
<td style="text-align: center;">$50.4 \%$</td>
<td style="text-align: center;">$53.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">0-shot CoT</td>
<td style="text-align: center;">$67.6 \%$</td>
<td style="text-align: center;">$35.2 \%$</td>
<td style="text-align: center;">$44.8 \%$</td>
<td style="text-align: center;">$61.6 \%$</td>
<td style="text-align: center;">$55.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">3-shot CoT</td>
<td style="text-align: center;">$68.4 \%$</td>
<td style="text-align: center;">$36.4 \%$</td>
<td style="text-align: center;">$\mathbf{5 8 . 0 \%}$</td>
<td style="text-align: center;">$62.0 \%$</td>
<td style="text-align: center;">$55.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ours: Self-generated Exemplars</td>
<td style="text-align: center;">$\mathbf{7 5 . 2 \%}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 6 \%}$</td>
<td style="text-align: center;">$57.6 \%$</td>
<td style="text-align: center;">$\mathbf{6 8 . 0 \%}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 8 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance on BIG-Bench reasoning tasks in accuracy. GPT3.5-turbo is used as the base LLM. Across diverse tasks, our method outperforms baselines ( 0 -shot CoT ) and is competitive with manual 3-shot CoT.
use Sentence-BERT (Reimers \&amp; Gurevych, 2019) to encode each problem statement. For each problem in the test set, we retrieve the top $K=5$ similar problems from the training set based on cosine similarity.
Our method. We let LLMs self-generate $K=5$ exemplars for GSM8K and $K=3$ exemplars for MATH and BIG-Bench tasks. For Codeforces, we self-generate both knowledge and $K=3$ exemplars.</p>
<h1>6 ReSULtS</h1>
<h3>6.1 MAIN RESULTS</h3>
<p>Mathematical problem solving. Table 1 presents results for GSM8K and MATH tasks. Our prompting method, which self-generates exemplars, outperforms baselines such as 0 -shot CoT and few-shot CoT. The improvement over few-shot CoT is notable for the MATH task, which involves a range of reasoning types, including algebra, probability, and geometry. This aligns with our approach of crafting tailored exemplars for each problem.</p>
<p>Figure 1 and 2 provide qualitative examples of GPT3.5-turbo outputs generated using our prompt. In both examples, the LLM indeed generates relevant exemplars (geometry problems in Figure 1. probability problems in Figure 2), and subsequently produces correct solutions. In contrast, in the standard few-shot CoT (Figure 1, middle), the exemplars are math-related (e.g., algebra) but may not always match the test problem (e.g., geometry), as the dataset contains diverse test problems.
Code generation. Table 2 presents results for Codeforces task. Our prompting method outperforms baselines such as 0 -shot CoT and few-shot CoT in both GPT3.5-turbo and GPT4. Moreover, self-generating knowledge provides additional performance boost over self-generating exemplars, demonstrating its usefulness for the challenging Codeforces task. With our prompting method, GPT3.5-turbo achieves competitive performance with GPT4, with a $15 \%$ Acc@1 compared to GPT4's $16 \%$ Acc@1.</p>
<p>Figure 3 (more complete version in D.3) provides a qualitative example of GPT3.5-turbo output generated using our prompt. The knowledge and exemplars generated by GPT3.5-turbo are indeed relevant to</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompting Method</th>
<th style="text-align: center;">$(+-$ scale down)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">(scale up $\rightarrow$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">text-curie-001</td>
<td style="text-align: center;">text-davinci-001</td>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;">text-davinci-003</td>
</tr>
<tr>
<td style="text-align: center;">0-shot</td>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">$6 \%$</td>
<td style="text-align: center;">$13 \%$</td>
<td style="text-align: center;">$14 \%$</td>
</tr>
<tr>
<td style="text-align: center;">0 -shot CoT</td>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">$6 \%$</td>
<td style="text-align: center;">$22 \%$</td>
<td style="text-align: center;">$50 \%$</td>
</tr>
<tr>
<td style="text-align: center;">5-shot (fixed) CoT</td>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$43 \%$</td>
<td style="text-align: center;">$54 \%$</td>
</tr>
<tr>
<td style="text-align: center;">5-shot retrieved CoT</td>
<td style="text-align: center;">$3 \%$</td>
<td style="text-align: center;">$11 \%$</td>
<td style="text-align: center;">$47 \%$</td>
<td style="text-align: center;">$57 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Ours: Self-generated Exemplars</td>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">$9 \%$</td>
<td style="text-align: center;">$48 \%$</td>
<td style="text-align: center;">$61 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance analysis using GSM8K task. Across varied scales/strengths of base LLMs (increasing from left to right), our prompting method outperforms 0 -shot CoT and standard few-shot CoT with fixed exemplars. Self-generated exemplars vs. retrieved exemplars: our method, with self-generated exemplars, performs better with larger-scale LLMs, while few-shot CoT with retrieved exemplars performs better with smaller-scale LLMs.
the problem to solve, focusing on the prefix product algorithm. The final code generated by the LLM effectively applies the algorithm to solve the problem. In contrast, in the 0 -shot CoT baseline, the LLM output does not recall relevant exemplars and fails to employ the prefix product algorithm, resulting in an incorrect solution (D.3).</p>
<p>BIG-Bench reasoning tasks. Table 3 presents results for BIG-Bench tasks. Our prompting method outperforms baselines like 0 -shot CoT , confirming its effectiveness across a wide range of tasks. Our method is also competitive with manual few-shot CoT. D. 4 offers GPT3.5-turbo output examples for the deductive reasoning task ("BIG-Bench formal fallacies"). Using our prompting method, the LLM generates relevant deductive reasoning exemplars. Conversely, 0 -shot CoT , with no relevant exemplars, tends to adopt an incorrect approach to address the deductive reasoning problem.</p>
<h1>6.2 KNOWLEDGE CAN COMPLEMENT EXEMPLARS</h1>
<p>Generating knowledge alongside exemplars is particularly useful in Codeforces task (Table 2), where LLMs need to apply nontrivial algorithms for code generation. In our qualitative analysis, we observe two concrete advantages of generating knowledge: (1) knowledge act as high-level takeaways that complement low-level exemplars, which prevents LLMs from overly relying on specific exemplars and helps to generalize to new problems; (2) when generating knowledge, LLMs identify the core concepts of the problem and produce exemplars that align more closely in fundamental problem-solving approaches (e.g., the prefix product algorithm in Figure 3), rather than surface-level lexical similarities (e.g., without knowledge, LLMs tend to produce exemplars on palindromic sequences).</p>
<p>The performance gains achieved by generating knowledge are less significant in other tasks like GSM8K and BIG-Bench, however, likely because these tasks are less complex.</p>
<h3>6.3 GENERATING VS RETRIEVING EXEMPLARS</h3>
<p>A key motivation behind our idea of self-generating exemplars is its ability to offer relevant exemplars for problem solving. An alternative approach is to retrieve relevant exemplars from external data, provided there is a labeled dataset of exemplars (e.g., the training set of GSM8K, which includes solutions labeled with reasoning steps). What trade-offs exist between these two approaches?</p>
<p>The advantage of retrieval lies in its reliability. Exemplars retrieved from a labeled dataset are inherently valid and correct, unlike generated exemplars, which lack this guarantee. Nevertheless, retrieval typically needs labeled exemplars and involves a complex additional retrieval step.</p>
<p>In contrast, generation is more self-contained and convenient, as it does not rely on external labeled data or retrieval steps. Additionally, generation may yield exemplars better tailored to specific test problems because it can draw upon the entire (pre-)training data the LLM has been exposed to. The downside of generation is that it may fail to produce valid exemplars if the LLMs are weak or have not learned problems related to the ones to be solved.</p>
<p>Table 4 shows empirical results for GSM8K task, comparing our self-generated exemplars method ("Ours") and the few-shot CoT method using exemplars retrieved from the GSM8K train set ("5-shot retrieved CoT"). We conducted experiments using base LLMs of various scales, from text-curie-001 to text-davinci-003, where scale broadly indicates the amount of training data and parameter count used by the LLM.</p>
<p>Our method outperforms the retrieved CoT with larger-scale LLMs, such as text-davinci-003. This is likely because the LLM has effectively learned related tasks during training and can generate useful</p>
<p>exemplars. Conversely, with smaller-scale LLMs, the retrieved CoT performs better, and self-generation fails to produce useful or valid exemplars.</p>
<h1>6.4 SCALE OF BASE LLMS: ANALOGICAL PROMPTING EXCELS WITH LARGER MODELS</h1>
<p>Table 4 presents the result of using varying scales and strengths of base LLMs, ranging from text-curie-001 to text-davinci-001 to text-davinci-002 and text-davinci-003 (more parameters and training data). Our prompting method surpasses vanilla 0 -shot and 0 -shot CoT across all scales. When using smaller-scale LLMs (text-curie-001 and text-davinci-001), few-shot CoT leveraging labeled exemplars exhibits superior performance compared to ours. However, as the LLMs are scaled up to text-davinci-002 and text-davinci-003, our method outperforms few-shot CoT. This is due to the LLMs' enhanced ability to self-generate more relevant and useful exemplars.</p>
<h3>6.5 NUMBER OF EXEMPLARS TO GENERATE</h3>
<p>In Table 5, we analyze the effect of varying the number of self-generated exemplars $(K)$ in our approach. When $K=1$, the LLM underperforms due to excessive reliance on a single exemplar generated. When $K \geq 3$, the LLM demonstrates consistent performance, with the best results observed at $K=3$ or 5. This observation aligns with the findings in the standard few-shot in-context learning in LLMs (Brown et al., 2020).</p>
<h3>6.6 QUALITATIVE ANALYSIS</h3>
<table>
<thead>
<tr>
<th style="text-align: left;"># Exemplars to <br> self-generate</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$K=1$</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">34.8</td>
</tr>
<tr>
<td style="text-align: left;">$K=2$</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">36.7</td>
</tr>
<tr>
<td style="text-align: left;">$K=3$</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">$\mathbf{3 7 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">$K=4$</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">37.0</td>
</tr>
<tr>
<td style="text-align: left;">$K=5$</td>
<td style="text-align: center;">$\mathbf{7 7 . 8}$</td>
<td style="text-align: center;">37.1</td>
</tr>
</tbody>
</table>
<p>Table 5: Analyzing the effect of varying the number of self-generated exemplars $(K)$ in our approach. We assess performance on GSM8K and MATH tasks using GPT3.5turbo as the base LLM.</p>
<p>We manually analyzed the performance of our prompting approach, based on 50 correctly and 50 incorrectly solved problems from GSM8K + MATH (50\%, 50\%).
50 correctly solved problems:</p>
<ul>
<li>(6/50) Generated exemplars are irrelevant</li>
<li>(9/50) Generated exemplars are relevant but contain incorrect solutions</li>
<li>(35/50) Generated exemplars are relevant and correct</li>
</ul>
<p>50 incorrectly solved problems:</p>
<ul>
<li>(10/50) Generated exemplars are irrelevant</li>
<li>(12/50) Generated exemplars are relevant but contain incorrect solutions</li>
<li>(28/50) Generated exemplars are relevant and correct, but LLM fails to solve the new problem:</li>
<li>(12/50) A generalization gap between the exemplars and the new problem.</li>
<li>(8/50) Overreliance on specific exemplars, leading to misdirection.</li>
<li>(8/50) Other issues, such as calculation errors.</li>
</ul>
<p>The generated exemplars were often relevant or correct. A common failure occurred when the LLM could not solve the new problem due to a generalization gap (e.g., the new problem is harder than the exemplars). This observation motivates future research to generate exemplars that not only possess relevance but also facilitate generalization for solving new problems.</p>
<h2>7 CONCLUSION</h2>
<p>We introduced analogical prompting, a new language model prompting approach that self-generates relevant reasoning exemplars for solving problems. This approach provides detailed, customized exemplars for individual problems without requiring labeled data, effectively addressing the challenges faced by existing 0 -shot CoT and few-shot CoT prompting methods. Experimental results show that our approach outperforms 0 -shot CoT and few-shot CoT in various reasoning tasks, including math problem solving, code generation, and other logical/temporal reasoning tasks.</p>
<h1>8 LIMITATIONS AND FUTURE RESEARCH</h1>
<p>One limitation of our approach is increased inference computation, as our approach generates more tokens than vanilla 0 -shot and 0 -shot CoT prompting. Compared to few-shot CoT, we use fewer input tokens and more output tokens, as exemplars are counted as input in few-shot CoT and as output in our approach.</p>
<p>Another limitation is that self-generation can fail if the LLM lacks sufficient strength or has not learned relevant knowledge to the new problems to solve. Conversely, with a stronger LLM, it can draw upon relevant prior knowledge to tackle slightly more complex problems. Therefore, our approach is better suited for stronger or larger-scale LLMs.</p>
<p>Finally, it is known that LLM performance can be influenced by specific prompt phrases used to query the model (Jiang et al., 2020), and our work is also subject to this prompt sensitivity.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank Qian Huang, Chen Liang, Andrew Dai and other colleagues at Google DeepMind, Stanford SNAP and P-Lambda groups for valuable feedback on this work.</p>
<h2>REFERENCES</h2>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Jiang Bian, Bin Gao, and Tie-Yan Liu. Knowledge-powered deep learning for word embedding. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I 14, pp. 132-148. Springer, 2014.</p>
<p>Lon Bottou. From machine learning to machine reasoning: An essay. Machine learning, 94:133-149, 2014.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.</p>
<p>Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023.</p>
<p>Jaime G Carbonell. Learning by analogy: Formulating and generalizing plans from past experience. In Machine learning, pp. 137-161. Elsevier, 1983.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.</p>
<p>Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2018.</p>
<p>Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V Le. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In International Conference on Learning Representations, 2019.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schrli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.</p>
<p>Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875, 2022.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology. org/N19-1423.</p>
<p>Andrew Drozdov, Nathanael Schrli, Ekin Akyrek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language models. arXiv preprint arXiv:2209.15003, 2022.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019.</p>
<p>Kevin Dunbar. The analogical paradox: Why analogy is so easy in naturalistic settings yet so difficult in the psychological laboratory. The analogical mind: Perspectives from cognitive science, pp. 313-334, 2001.</p>
<p>Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. Scalable multihop relational reasoning for knowledge-aware question answering. In Empirical Methods in Natural Language Processing (EMNLP), 2020.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv: Arxiv-2101.00027, 2020. URL https://arxiv.org/abs/Arxiv-2101.00027.</p>
<p>Dedre Gentner. Structure-mapping: A theoretical framework for analogy. Cognitive science, 7(2):155-170, 1983.</p>
<p>Dedre Gentner and Keith J Holyoak. Reasoning and learning by analogy: Introduction. American psychologist, 52(1):32, 1997.</p>
<p>Dedre Gentner and Arthur B Markman. Structure mapping in analogy and similarity. American psychologist, 52(1):45, 1997.</p>
<p>Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, and Dawn Song. Synthesize, execute and debug: Learning to repair for neural program synthesis. Advances in Neural Information Processing Systems, 33:17685-17695, 2020.</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.</p>
<p>Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. Exploring human-like translation strategy with large language models. arXiv preprint arXiv:2305.04118, 2023.</p>
<p>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021a.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b.</p>
<p>Keith J Holyoak. Analogy and relational reasoning. The Oxford handbook of thinking and reasoning, pp. 234-259, 2012.</p>
<p>Xiaoyang Hu, Shane Storks, Richard L Lewis, and Joyce Chai. In-context analogical reasoning with pre-trained language models. arXiv preprint arXiv:2305.17626, 2023.</p>
<p>Ziqi Huang, Hongyuan Zhu, Ying Sun, Dongkyu Choi, Cheston Tan, and Joo-Hwee Lim. A diagnostic study of visual question answering with analogical reasoning. In 2021 IEEE International Conference on Image Processing (ICIP), pp. 2463-2467. IEEE, 2021.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020.</p>
<p>Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint arXiv:2205.11822, 2022.</p>
<p>Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.</p>
<p>Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023.</p>
<p>Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. Selfgenerated in-context learning: Leveraging auto-regressive language models as a demonstration generator. arXiv preprint arXiv:2206.08082, 2022.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022), 2022. URL https://arxiv.org/abs/2205.11916.</p>
<p>Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S Liang. Spoc: Search-based pseudocode to code. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Junlong Li, Zhuosheng Zhang, and Hai Zhao. Self-prompting large language models for open-domain qa. arXiv preprint arXiv:2212.08635, 2022a.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022b.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, and Ilya Sutskever. Improving mathematical reasoning with process supervision. 2023. URL https://openai.com/research/ improving-mathematical-reasoning-with-process-supervision.</p>
<p>Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. Kagnet: Knowledge-aware graph networks for commonsense reasoning. In Empirical Methods in Natural Language Processing (EMNLP), 2019.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100-114, 2022. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022. deelio-1.10.</p>
<p>Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. arXiv preprint arXiv:2203.08242, 2022.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3470-3487, 2022. doi: 10.18653/v1/2022.acl-long.244. URL https://aclanthology.org/2022.acl-long. 244.</p>
<p>Tom M Mitchell, Jaime G Carbonell, Ryszard S Michalski, and Rogers Hall. Analogical reasoning in the context of acquiring problem solving expertise. Machine Learning: A Guide to Current Research, pp. 85-88, 1986.</p>
<p>Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pp. 353-367. PMLR, 2023.</p>
<p>OpenAI. Gpt-4 technical report. 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.</p>
<p>Joon Sung Park, Joseph C OBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>George Polya. How to solve it: A new aspect of mathematical method, volume 85. Princeton university press, 2004.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476, 2023.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3982-3992, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410.</p>
<p>Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, Dale Schuurmans, Jure Leskovec, and Denny Zhou. Lego: Latent execution-guided reasoning for multi-hop question answering on knowledge graphs. In International Conference on Machine Learning (ICML), 2021.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.</p>
<p>KaShun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with chain-of-thought from labeled data. arXiv preprint arXiv:2302.12822, 2023.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975, 2022. URL https://arxiv.org/abs/2209.01975.</p>
<p>Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. arXiv preprint arXiv:2210.01296, 2022.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schrli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.</p>
<p>Stella Vosniadou and Andrew Ortony. Similarity and analogical reasoning. Cambridge University Press, 1989 .</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Thomas B Ward, Steven M Smith, and Jyotsna Ed Vaid. Creative thought: An investigation of conceptual structures and processes. American Psychological Association, 1997.</p>
<p>Taylor Webb, Keith J Holyoak, and Hongjing Lu. Emergent analogical reasoning in large language models. Nature Human Behaviour, pp. 1-16, 2023.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=gEZrGCozdqR.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022), 2022b. URL https://arxiv.org/abs/2201.11903.</p>
<p>Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, Roger B Grosse, and Christian Szegedy. Lime: Learning inductive bias for primitives of mathematical reasoning. In International Conference on Machine Learning, pp. 11251-11262. PMLR, 2021.</p>
<p>Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966, 2022.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.</p>
<p>Michihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from diagnostic feedback. In International Conference on Machine Learning (ICML), pp. 10799-10808. PMLR, 2020.</p>
<p>Michihiro Yasunaga and Percy Liang. Break-it-fix-it: Unsupervised learning for program repair. In International Conference on Machine Learning, pp. 11941-11952. PMLR, 2021.</p>
<p>Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. QA-GNN: Reasoning with language models and knowledge graphs for question answering. In North American Chapter of the Association for Computational Linguistics (NAACL), 2021.</p>
<p>Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy S Liang, and Jure Leskovec. Deep bidirectional language-knowledge graph pretraining. Advances in Neural Information Processing Systems, 35:37309-37323, 2022a.</p>
<p>Michihiro Yasunaga, Jure Leskovec, and Percy Liang. LinkBERT: Pretraining language models with document links. In Association for Computational Linguistics (ACL), 2022b.</p>
<p>Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. In International Conference on Machine Learning (ICML), 2023.</p>
<p>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018.</p>
<p>Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D Goodman, and Nick Haber. Parsel: A unified natural language framework for algorithmic reasoning. arXiv preprint arXiv:2212.10561, 2022.</p>
<p>Hanlin Zhang, Jiani Huang, Ziyang Li, Mayur Naik, and Eric Xing. Improved logical reasoning of language models via differentiable symbolic programming. arXiv preprint arXiv:2305.03742, 2023.</p>
<p>Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D Manning, and Jure Leskovec. Greaselm: Graph reasoning enhanced language models for question answering. In International Conference on Learning Representations (ICLR), 2022a.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022b.</p>
<p>Wenting Zhao, Mor Geva, Bill Yuchen Lin, Michihiro Yasunaga, Aman Madaan, and Tao Yu. Complex reasoning in natural language. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pp. 11-20, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-tutorials.2. URL https:// aclanthology.org/2023.acl-tutorials.2.</p>
<p>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023.</p>
<p>Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. URL https://arxiv.org/abs/2205. 10625 .</p>
<h1>A ADDITIONAL RELATED WORKS</h1>
<h2>A. 1 LANGUAGE MODELS AND REASONING</h2>
<p>Reasoning involves the application of knowledge to derive solutions for new problems, often through a series of steps. Teaching language models to reason has been a long-standing area of research (Bottou, 2014; Zhao et al., 2023; Wei et al., 2022b).</p>
<p>To assess the reasoning capabilities of language models, researchers have created datasets for various tasks that demand reasoning skills. These tasks include multi-step question answering (Yang et al., 2018; Dua et al., 2019; Talmor et al., 2018), mathematical problem-solving (Cobbe et al., 2021; Hendrycks et al., 2021b), and code generation (Yu et al., 2018; Chen et al., 2021; Hendrycks et al., 2021a; Austin et al., 2021). In this study, we evaluate our methods using these diverse datasets.</p>
<p>To teach language models to reason effectively, one line of approaches involve training or fine-tuning them. This can include using reasoning-intensive data during training (Wu et al., 2021; Yasunaga et al., 2022b; Lightman et al., 2023; Moor et al., 2023), retrieving structured knowledge (Lin et al., 2019; Feng et al., 2020; Zhang et al., 2022a; Yasunaga et al., 2021; 2022a; Xie et al., 2022), and incorporating external modules for reasoning such as logic and program execution (Chen et al., 2018; 2019; Yasunaga \&amp; Liang, 2020; Gupta et al., 2020; Ren et al., 2021; Zhang et al., 2023).</p>
<p>Recently, with the rise of large language models (LLMs), prompting them to engage in reasoning has proven effective and gained attention. A common approach is prompting LLMs to generate intermediate reasoning steps, as demonstrated by the chain-of-thought method (Wei et al., 2022b; Kojima et al., 2022; Zhou et al., 2022; Wang et al., 2022), which assists LLMs in tackling complex reasoning tasks. Several studies have extended this approach with more structured algorithms and search methods (Khot et al., 2022; Drozdov et al., 2022; Zelikman et al., 2022; Yao et al., 2023; Press et al., 2022; Khattab et al., 2022; Jung et al., 2022), as well as longer-horizon action and planning (Yao et al., 2022; Hao et al., 2023; Park et al., 2023). Another line of work incorporates tools and programs into the prompting process to facilitate reasoning (Chen et al., 2023; 2022; Cai et al., 2023; Cheng et al., 2022; Kim et al., 2023; Zhou et al., 2023; Schick et al., 2023).</p>
<p>Our work complements these efforts to enhance LLM reasoning and is the first to draw inspiration from human analogical reasoning to improve LLM prompting.</p>
<h2>A. 2 ANALOGICAL REASONING</h2>
<p>Analogical reasoning is a cognitive process in which humans recall relevant past experiences when facing new challenges (Gentner \&amp; Holyoak, 1997; Gentner, 1983; Holyoak, 2012). This phenomenon has been studied extensively in psychology, revealing its significance in various cognitive tasks such as problemsolving (Gentner \&amp; Markman, 1997) and creativity (Ward et al., 1997). It is rooted in the capacity to identify structural and relational similarities between past and current situations, facilitating knowledge transfer (Dunbar, 2001).</p>
<p>Analogical reasoning has also influenced the development of artificial intelligence and machine learning algorithms (Carbonell, 1983; Mitchell et al., 1986) and has been employed as a reasoning benchmark for assessing machine learning models (Bian et al., 2014; Huang et al., 2021). A recent work also evaluates the ability of language models to identify analogies (Webb et al., 2023; Hu et al., 2023).</p>
<p>Our work makes a pioneering effort of applying analogical reasoning principles to enhance language model inference.</p>
<h1>B CODEFORCES DATA COLLECTION</h1>
<p>We scraped data from codeforces.com, following the procedure in prior works (Li et al., 2022b; Kulal et al., 2019; Yasunaga \&amp; Liang, 2021). We use Level-A problems that were published between January 2023 and August 2023. Each problem includes the full problem descriptions and test cases accessible on the website. The test cases include the public test cases found on the problem page and hidden test cases made available on the evaluation result pages once a contest is finished. Some of the hidden test cases were truncated on the website due to excessive input/output length, and we skipped those. We retained problems whose problem descriptions were within the length of 2000 tokens in GPT3.5-turbo, resulting in 50 problems. Because this dataset is relatively small, we conduct the evaluation twice and then report the average results.</p>
<h2>C ADDITIONAL RESULTS</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompting Method</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours: Non-diverse exemplars</td>
<td style="text-align: center;">$75.9 \%$</td>
<td style="text-align: center;">$35.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ours: Diverse exemplars</td>
<td style="text-align: center;">$\mathbf{7 7 . 8 \%}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 3 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation study on the effect of encouraging diversity in self-generating exemplars (4.1).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompting Method</th>
<th style="text-align: right;">Acc@1</th>
<th style="text-align: right;">Acc@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours: Self-generate knowledge after exemplars</td>
<td style="text-align: right;">$14 \%$</td>
<td style="text-align: right;">$27 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ours: Self-generate knowledge before exemplars</td>
<td style="text-align: right;">$\mathbf{1 5 \%}$</td>
<td style="text-align: right;">$\mathbf{2 9 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation study on the Codeforces task using GPT3.5-turbo. Self-generating knowledge before exemplars performs slightly better than self-generating knowledge after exemplars (4.2).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompting Method</th>
<th style="text-align: right;">GSM8K</th>
<th style="text-align: right;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: right;">$77.8 \%$</td>
<td style="text-align: right;">$37.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ours + Self-consistency</td>
<td style="text-align: right;">$\mathbf{8 5 . 3 \%}$</td>
<td style="text-align: right;">$\mathbf{4 6 . 0 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Self-consistency can be used to further boost the performance of our analogical prompting approach.</p>
<h2>D FULL EXAMPLES</h2>
<h2>D. 1 MATH</h2>
<p>Our prompt (self-generate exemplars):</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Your</span><span class="w"> </span><span class="nx">task</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">tackle</span><span class="w"> </span><span class="nx">mathematical</span><span class="w"> </span><span class="nx">problems</span><span class="p">.</span><span class="w"> </span><span class="nx">When</span><span class="w"> </span><span class="nx">presented</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">math</span>
<span class="w">    </span><span class="nx">problem</span><span class="p">,</span><span class="w"> </span><span class="nx">recall</span><span class="w"> </span><span class="nx">relevant</span><span class="w"> </span><span class="nx">problems</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">examples</span><span class="p">.</span><span class="w"> </span><span class="nx">Afterward</span><span class="p">,</span><span class="w"> </span><span class="nx">proceed</span><span class="w"> </span><span class="nx">to</span>
<span class="w">    </span><span class="nx">solve</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">problem</span><span class="p">.</span>
<span class="err">#</span><span class="w"> </span><span class="nx">Initial</span><span class="w"> </span><span class="nx">Problem</span><span class="p">:</span>
<span class="nx">An</span><span class="w"> </span><span class="nx">airline</span><span class="w"> </span><span class="nx">serves</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">dinner</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">passengers</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">airplane</span><span class="p">.</span><span class="w"> </span><span class="nx">They</span><span class="w"> </span><span class="nx">get</span>
<span class="w">    </span><span class="nx">their</span><span class="w"> </span><span class="kd">choice</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">steak</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">fish</span><span class="p">.</span><span class="w"> </span><span class="nx">Three</span><span class="w"> </span><span class="nx">steak</span><span class="w"> </span><span class="nx">meals</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">three</span><span class="w"> </span><span class="nx">fish</span>
<span class="w">    </span><span class="nx">meals</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">aside</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">six</span><span class="o">-</span><span class="nx">member</span><span class="w"> </span><span class="nx">crew</span><span class="p">.</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">meals</span><span class="w"> </span><span class="nx">are</span>
<span class="w">    </span><span class="nx">distributed</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">crew</span><span class="w"> </span><span class="nx">members</span><span class="w"> </span><span class="nx">randomly</span><span class="p">,</span><span class="w"> </span><span class="nx">what</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">probability</span>
<span class="w">    </span><span class="nx">that</span><span class="w"> </span><span class="nx">both</span><span class="w"> </span><span class="nx">pilots</span><span class="w"> </span><span class="nx">get</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">fish</span><span class="p">?</span>
<span class="err">#</span><span class="w"> </span><span class="nx">Instructions</span><span class="p">:</span>
<span class="nx">Make</span><span class="w"> </span><span class="nx">sure</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">include</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">following</span><span class="w"> </span><span class="nx">points</span><span class="p">:</span>
<span class="err">##</span><span class="w"> </span><span class="nx">Relevant</span><span class="w"> </span><span class="nx">Problems</span><span class="p">:</span>
<span class="nx">Recall</span><span class="w"> </span><span class="nx">three</span><span class="w"> </span><span class="nx">examples</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">math</span><span class="w"> </span><span class="nx">problems</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">relevant</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">initial</span>
<span class="w">    </span><span class="nx">problem</span><span class="p">.</span><span class="w"> </span><span class="nx">Note</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">problems</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">distinct</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">other</span>
<span class="w">    </span><span class="k">and</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">problem</span><span class="w"> </span><span class="p">(</span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.,</span><span class="w"> </span><span class="nx">involving</span><span class="w"> </span><span class="nx">different</span><span class="w"> </span><span class="nx">numbers</span><span class="w"> </span><span class="k">and</span>
<span class="w">    </span><span class="nx">names</span><span class="p">).</span><span class="w"> </span><span class="nx">For</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">problem</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nx">After</span><span class="w"> </span><span class="s">&quot;Q: &quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">describe</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">problem</span>
</code></pre></div>

<ul>
<li>After "A: ", explain the solution and enclose the ultimate answer in } boxed{}.
## Solve the Initial Problem:
Say "Let's solve the following math problem." Then formulate your response in the following format:
Q: Copy and paste the initial problem here.
A: Explain the solution and enclose the ultimate answer in \boxed{} here.</li>
</ul>
<p>Output by GPT3.5-turbo:</p>
<p>Relevant Problems:
Q: In a bag, there are 5 red balls and 3 blue balls. If two balls are drawn at random without replacement, what is the probability that both balls are red?
A: The total number of balls is 8 . The probability of drawing a red ball on the first draw is $5 / 8$. After one red ball is drawn, there are 4 red balls left out of 7 total balls. Therefore, the probability of drawing a second red ball is $4 / 7$. The probability of both events happening is the product of the individual probabilities, so the probability of drawing two red balls is $(5 / 8) *(4 / 7)=\backslash$ boxed ${\backslash$ frac ${5}{14}}$.</p>
<p>Q: A box contains 10 red marbles and 5 blue marbles. If three marbles are drawn at random without replacement, what is the probability that all three marbles are red?
A: The total number of marbles is 15 . The probability of drawing a red marble on the first draw is 10/15. After one red marble is drawn, there are 9 red marbles left out of 14 total marbles. Therefore, the probability of drawing a second red marble is $9 / 14$. After two red marbles are drawn, there are 8 red marbles left out of 13 total marbles. Therefore, the probability of drawing a third red marble is $8 / 13$. The probability of all three events happening is the product of the individual probabilities, so the probability of drawing three red marbles is $(10 / 15) <em>(9 / 14) </em>(8 / 13)=\backslash$ boxed ${\backslash$ frac ${24}{91}}$.</p>
<p>Q: A bag contains 4 green marbles, 3 red marbles, and 2 blue marbles. If two marbles are drawn at random without replacement, what is the probability that both marbles are green?
A: The total number of marbles is 9 . The probability of drawing a green marble on the first draw is $4 / 9$. After one green marble is drawn, there are 3 green marbles left out of 8 total marbles. Therefore, the probability of drawing a second green marble is $3 / 8$. The probability of both events happening is the product of the individual probabilities, so the probability of drawing two green marbles is $(4 / 9) *(3 / 8)=\backslash$ boxed ${\backslash$ frac ${1}{6}}$.</p>
<p>Let's solve the following math problem:
An airline serves a dinner to all the passengers on an airplane. They get their choice of steak or fish. Three steak meals and three fish meals are set aside for the six-member crew. If the meals are distributed to the crew members randomly, what is the probability that both pilots get the fish?
A: There are a total of 6 crew members, so the probability of the first pilot getting the fish is $3 / 6$. After one fish meal is given to the first pilot, there are 2 fish meals left out of 5 total meals. Therefore, the probability of the second pilot getting the fish is $2 / 5$. The probability of both events happening is the product of the individual probabilities, so the probability that both pilots get the fish is $(3 / 6) *(2 / 5)=\backslash$ boxed ${\backslash$ frac ${1}{5}}$.</p>
<h1>D. 2 GSM8K</h1>
<p>Our prompt (self-generate exemplars):</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Your</span><span class="w"> </span><span class="nv">task</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">tackle</span><span class="w"> </span><span class="nv">mathematical</span><span class="w"> </span><span class="nv">problems</span>.<span class="w"> </span><span class="nv">When</span><span class="w"> </span><span class="nv">presented</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">math</span>
<span class="w">    </span><span class="nv">problem</span>,<span class="w"> </span><span class="nv">recall</span><span class="w"> </span><span class="nv">relevant</span><span class="w"> </span><span class="nv">problems</span><span class="w"> </span><span class="nv">as</span><span class="w"> </span><span class="nv">examples</span>.<span class="w"> </span><span class="nv">Afterward</span>,<span class="w"> </span><span class="nv">proceed</span><span class="w"> </span><span class="nv">to</span>
<span class="w">    </span><span class="nv">solve</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">initial</span><span class="w"> </span><span class="nv">problem</span>.
#<span class="w"> </span><span class="nv">Initial</span><span class="w"> </span><span class="nv">Problem</span>:
<span class="nv">Kyle</span><span class="w"> </span><span class="nv">bought</span><span class="w"> </span><span class="nv">last</span><span class="w"> </span><span class="nv">year</span><span class="err">&#39;s best-selling book for $19.50. This is with a 25%</span>
<span class="err">    discount from the original price. What was the original price of the</span>
<span class="err">    book?</span>
<span class="err"># Instructions:</span>
<span class="err">Make sure that your response follows the instructions below.</span>
<span class="err">## Analogous Problems:</span>
<span class="err">Offer five diverse examples of math problems that are relevant or</span>
<span class="err">    analogous to the initial problem. For each problem, elaborate on the</span>
<span class="err">    solution and conclude with the ultimate answer (enclosed in \boxed())</span>
<span class="err">        . For each problem:</span>
<span class="err">- After &quot;Q: &quot;, describe the problem</span>
<span class="err">- After &quot;A: &quot;, explain the solution and enclose the ultimate answer in \</span>
<span class="err">    boxed{).</span>
<span class="err">## Solve the Initial Problem:</span>
<span class="err">Q: Copy and paste the initial problem here.</span>
<span class="err">A: Explain the solution and enclose the ultimate answer in \boxed() here.</span>
</code></pre></div>

<p>Output by GPT3.5-turbo:</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Analogous</span><span class="w"> </span><span class="nx">Problems</span><span class="p">:</span>
<span class="nx">Q1</span><span class="p">:</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="nx">shirt</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">sale</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="mi">20</span><span class="o">%</span><span class="w"> </span><span class="nx">off</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">$</span><span class="mi">40</span><span class="p">.</span><span class="w"> </span><span class="nx">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">sale</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">shirt</span><span class="p">?</span>
<span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">shirt</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">$</span><span class="mi">40</span><span class="p">.</span><span class="w"> </span><span class="nx">With</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="mi">20</span><span class="o">%</span><span class="w"> </span><span class="nx">discount</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">discount</span><span class="w"> </span><span class="nx">amount</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">20</span><span class="o">%</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">$</span><span class="mi">40</span><span class="p">,</span><span class="w"> </span><span class="nx">which</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="m m-Double">0.20</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="err">$</span><span class="mi">40</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">$</span><span class="mi">8</span><span class="p">.</span><span class="w"> </span><span class="nx">Therefore</span><span class="p">,</span>
<span class="w">    </span><span class="nx">the</span><span class="w"> </span><span class="nx">sale</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">shirt</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">$</span><span class="mi">40</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">$</span><span class="mi">8</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">$</span><span class="mi">32</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">$</span><span class="mi">1</span><span class="nx">boxed</span>
<span class="w">    </span><span class="p">{</span><span class="mi">32</span><span class="p">}</span><span class="err">$</span><span class="p">.</span>
<span class="nx">Q2</span><span class="p">:</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="nx">store</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">offering</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="mi">15</span><span class="o">%</span><span class="w"> </span><span class="nx">discount</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">items</span><span class="p">.</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">customer</span><span class="w"> </span><span class="nx">buys</span><span class="w"> </span><span class="nx">a</span>
<span class="w">    </span><span class="nx">pair</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">shoes</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="err">$</span><span class="mi">85</span><span class="p">,</span><span class="w"> </span><span class="nx">what</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">shoes</span><span class="p">?</span>
<span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">Let</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">assume</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">shoes</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">$</span><span class="nx">x</span><span class="p">.</span><span class="w"> </span><span class="nx">With</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="mi">15</span><span class="o">%</span>
<span class="w">    </span><span class="nx">discount</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">discount</span><span class="w"> </span><span class="nx">amount</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">15</span><span class="o">%</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">$</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">which</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="m m-Double">0.15</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="err">$</span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">$</span><span class="mi">0</span>
<span class="w">    </span><span class="m m-Double">.15</span><span class="nx">x</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">sale</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">shoes</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">$</span><span class="mi">85</span><span class="p">,</span><span class="w"> </span><span class="nx">so</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">equation</span><span class="p">:</span><span class="w"> </span><span class="err">$</span><span class="nx">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">$</span><span class="m m-Double">0.15</span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">$</span><span class="mi">85</span><span class="p">.</span><span class="w"> </span><span class="nx">Simplifying</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">equation</span><span class="p">,</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">get</span><span class="w"> </span><span class="m m-Double">0.85</span><span class="nx">x</span><span class="w"> </span><span class="p">=</span>
<span class="w">        </span><span class="err">$</span><span class="mi">85</span><span class="p">.</span><span class="w"> </span><span class="nx">Dividing</span><span class="w"> </span><span class="nx">both</span><span class="w"> </span><span class="nx">sides</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="m m-Double">0.85</span><span class="p">,</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">find</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">$</span><span class="mi">100</span><span class="p">.</span><span class="w"> </span><span class="nx">Therefore</span><span class="p">,</span>
<span class="w">        </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">shoes</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="err">$</span><span class="mi">1</span><span class="nx">boxed</span><span class="p">{</span><span class="mi">100</span><span class="p">}</span><span class="err">$</span><span class="p">.</span>
<span class="nx">Q3</span><span class="p">:</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="nx">restaurant</span><span class="w"> </span><span class="nx">offers</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="mi">10</span><span class="o">%</span><span class="w"> </span><span class="nx">discount</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">meals</span><span class="p">.</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">customer</span><span class="w"> </span><span class="nx">pays</span>
<span class="w">    </span><span class="err">$</span><span class="mi">45</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">meal</span><span class="p">,</span><span class="w"> </span><span class="nx">what</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">meal</span><span class="p">?</span>
<span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">Let</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">assume</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">meal</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">$</span><span class="nx">x</span><span class="p">.</span><span class="w"> </span><span class="nx">With</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="mi">10</span><span class="o">%</span><span class="w"> </span><span class="nx">discount</span>
<span class="w">    </span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">discount</span><span class="w"> </span><span class="nx">amount</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">10</span><span class="o">%</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">$</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">which</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="m m-Double">0.10</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="err">$</span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">$</span><span class="m m-Double">0.10</span><span class="nx">x</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span>
<span class="w">    </span><span class="nx">sale</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">meal</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">$</span><span class="mi">45</span><span class="p">,</span><span class="w"> </span><span class="nx">so</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">equation</span><span class="p">:</span><span class="w"> </span><span class="err">$</span><span class="nx">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">$</span><span class="mi">0</span>
<span class="w">    </span><span class="m m-Double">.10</span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">$</span><span class="mi">45</span><span class="p">.</span><span class="w"> </span><span class="nx">Simplifying</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">equation</span><span class="p">,</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">get</span><span class="w"> </span><span class="m m-Double">0.90</span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">$</span><span class="mi">45</span><span class="p">.</span><span class="w"> </span><span class="nx">Dividing</span>
<span class="w">    </span><span class="nx">both</span><span class="w"> </span><span class="nx">sides</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="m m-Double">0.90</span><span class="p">,</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">find</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">$</span><span class="mi">50</span><span class="p">.</span><span class="w"> </span><span class="nx">Therefore</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span>
<span class="w">    </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">meal</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="err">$</span><span class="mi">1</span><span class="nx">boxed</span><span class="p">{</span><span class="mi">50</span><span class="p">}</span><span class="err">$</span><span class="p">.</span>
<span class="nx">Q4</span><span class="p">:</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="nx">laptop</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">sale</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="mi">30</span><span class="o">%</span><span class="w"> </span><span class="nx">off</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">$</span><span class="mi">800</span><span class="p">.</span><span class="w"> </span><span class="nx">What</span><span class="w"> </span><span class="k">is</span>
<span class="w">    </span><span class="nx">the</span><span class="w"> </span><span class="nx">sale</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">laptop</span><span class="p">?</span>
<span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">laptop</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">$</span><span class="mi">800</span><span class="p">.</span><span class="w"> </span><span class="nx">With</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="mi">30</span><span class="o">%</span><span class="w"> </span><span class="nx">discount</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">discount</span><span class="w"> </span><span class="nx">amount</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">30</span><span class="o">%</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">$</span><span class="mi">800</span><span class="p">,</span><span class="w"> </span><span class="nx">which</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="m m-Double">0.30</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="err">$</span><span class="mi">800</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">$</span><span class="mi">240</span><span class="p">.</span>
<span class="w">    </span><span class="nx">Therefore</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">sale</span><span class="w"> </span><span class="nx">price</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">laptop</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">$</span><span class="mi">800</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">$</span><span class="mi">240</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="err">$</span><span class="mi">560</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span>
<span class="w">    </span><span class="nx">answer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">$</span><span class="mi">1</span><span class="nx">boxed</span><span class="p">{</span><span class="mi">560</span><span class="p">}</span><span class="err">$</span><span class="p">.</span>
</code></pre></div>

<p>Q5: A toy store is offering a $12 \%$ discount on all toys. If a customer buys a toy for $\$ 22$, what was the original price of the toy?
A: Let's assume the original price of the toy is $\$ x$. With a $12 \%$ discount, the discount amount is $12 \%$ of $\$ x$, which is $0.12 * \$ x=\$ 0.12 x$. The sale price of the toy is $\$ 22$, so we can set up the equation: $\$ x-\$ 0$. $12 x=\$ 22$. Simplifying the equation, we get $0.88 x=\$ 22$. Dividing both sides by 0.88 , we find that $x=\$ 25$. Therefore, the original price of the toy was $\$ \backslash$ boxed ${25} \$$.</p>
<p>Solve the Initial Problem:
Q: Kyle bought last year's best-selling book for $\$ 19.50$. This is with a $25 \%$ discount from the original price. What was the original price of the book?
A: Let's assume the original price of the book is $\$ x$. With a $25 \%$ discount , the discount amount is $25 \%$ of $\$ x$, which is $0.25 * \$ x=\$ 0.25 x$. The sale price of the book is $\$ 19.50$, so we can set up the equation: $\$ x-$ $\$ 0.25 x=\$ 19.50$. Simplifying the equation, we get $0.75 x=\$ 19.50$. Dividing both sides by 0.75 , we find that $x=\$ 26$. Therefore, the original price of the book was $\$ \backslash$ boxed ${26} \$$.</p>
<h1>D. 3 CODEFORCES</h1>
<p>Our prompt (self-generate exemplars and knowledge):</p>
<div class="codehilite"><pre><span></span><code>Your goal is to write Python3 code to solve competitive programming
    problems. Given a problem, explain the core concepts in it and
    provide other relevant problems. Then solve the original problem.
<span class="gh">#</span> Problem:
You are given a sequence a_1, a_2, \ldots, a_n. Each element of a is 1 or
    2. Find out if an integer k exists so that the following conditions
    are met.
    1 \leq k \leq n-1, and a_1 \cdot a_2 \cdot \ldots \cdot a_k = a_{k+1}
        \cdot a_{k+2} \cdot \ldots \cdot a_n, where \cdot denotes a product
    .
If there exist multiple k that satisfy the given condition, print the
    smallest.
Input
Each test contains multiple test cases. The first line contains the
    number of test cases t (1 \le t \le 100). Description of the test
    cases follows.
The first line of each test case contains one integer n (2 \leq n \leq
    1000).
The second line of each test case contains n integers a_1, a_2, \ldots,
    a_n (1 \leq a_i \leq 2).
Output
For each test case, if there is no such k, print -1.
Otherwise, print the smallest possible k.
<span class="gh">#</span> Instruction:
<span class="gu">##</span> Algorithms:
Identify the core concepts or algorithms used to solve the problem.
<span class="gu">##</span> Tutorial:
Write a useful tutorial about these algorithms.
<span class="gu">##</span> Example Problems:
Provide three examples of relevant competitive programming problems that
    involve these algorithms. For each problem, describe the problem,
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">explain</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">solution</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">detail</span>,<span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nv">write</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">correct</span><span class="w"> </span><span class="nv">Python3</span>
<span class="w">    </span><span class="nv">code</span>.
##<span class="w"> </span><span class="nv">Python3</span><span class="w"> </span><span class="nv">code</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">solve</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">original</span><span class="w"> </span><span class="nv">problem</span>:
<span class="k">Include</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">following</span><span class="w"> </span><span class="nv">points</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">response</span>:
<span class="o">-</span><span class="w"> </span><span class="nv">Explanation</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">solution</span>:
<span class="o">-</span><span class="w"> </span><span class="nv">Python3</span><span class="w"> </span><span class="nv">code</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">solve</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">problem</span>:
</code></pre></div>

<p>Output by GPT3.5-turbo-16k:</p>
<div class="codehilite"><pre><span></span><code><span class="c1">## Algorithms:</span>
<span class="n">The</span><span class="w"> </span><span class="n">core</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">solving</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">problem</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">product</span>
<span class="w">    </span><span class="n">algorithm</span><span class="o">.</span>
<span class="c1">## Tutorial:</span>
<span class="n">The</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">calculate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">elements</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">array</span><span class="o">.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">useful</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="n">solving</span><span class="w"> </span><span class="n">problems</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">involve</span><span class="w"> </span><span class="n">calculating</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">elements</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">subarray</span><span class="o">.</span>
<span class="n">The</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="n">works</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">initializing</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="s2">&quot;prefix&quot;</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">then</span>
<span class="w">    </span><span class="n">iterating</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">array</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">left</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">right</span><span class="o">.</span><span class="w"> </span><span class="n">At</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">iteration</span><span class="p">,</span>
<span class="w">    </span><span class="n">the</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">element</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">multiplied</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prefix</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="k">is</span>
<span class="w">    </span><span class="n">stored</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">variable</span><span class="o">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">way</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">keeps</span>
<span class="w">    </span><span class="n">track</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">elements</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">array</span><span class="o">.</span>
<span class="n">Here</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">pseudocode</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="n">algorithm</span><span class="p">:</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="p">...</span>
<span class="k">prefix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="k">prefix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">prefix</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">arr</span><span class="o">[</span><span class="n">i</span><span class="o">]</span>
</code></pre></div>

<p>The time complexity of this algorithm is $O(n)$, where $n$ is the size of the input array.</p>
<div class="codehilite"><pre><span></span><code><span class="c1">## Example Problems:</span>
<span class="mf">1.</span><span class="w"> </span><span class="n">Problem</span><span class="p">:</span><span class="w"> </span><span class="n">Given</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">array</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">integers</span><span class="p">,</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">elements</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">subarray</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="n">k</span><span class="o">.</span>
<span class="w">    </span><span class="n">Solution</span><span class="p">:</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">solve</span><span class="w"> </span><span class="n">this</span>
<span class="w">        </span><span class="n">problem</span><span class="o">.</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">initialize</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">iterate</span>
<span class="w">        </span><span class="n">through</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">array</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">left</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">right</span><span class="o">.</span><span class="w"> </span><span class="n">At</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">iteration</span><span class="p">,</span><span class="w"> </span><span class="n">we</span>
<span class="w">        </span><span class="n">multiply</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">element</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prefix</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">store</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">result</span>
<span class="w">            </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">variable</span><span class="o">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">greater</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">equal</span>
<span class="w">            </span><span class="n">to</span><span class="w"> </span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">divide</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">element</span><span class="w"> </span><span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">which</span>
<span class="w">            </span><span class="n">represents</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">element</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">subarray</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="n">starting</span><span class="w"> </span><span class="n">at</span>
<span class="w">                </span><span class="n">index</span><span class="w"> </span><span class="n">i</span><span class="o">-</span><span class="n">k</span><span class="o">+</span><span class="mf">1.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">product</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">elements</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span>
<span class="w">                </span><span class="n">subarray</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="n">starting</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="n">i</span><span class="o">-</span><span class="n">k</span><span class="o">+</span><span class="mf">1.</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">store</span><span class="w"> </span><span class="n">these</span>
<span class="w">            </span><span class="n">results</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">array</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">output</span><span class="o">.</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="o">**</span><span class="w"> </span><span class="n">python</span>
<span class="n">def</span><span class="w"> </span><span class="n">product_of_subarrays</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="w">    </span><span class="k">prefix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="k">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="k">prefix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">prefix</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">arr</span><span class="o">[</span><span class="n">i</span><span class="o">]</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="err">:</span>
<span class="w">            </span><span class="k">result</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="k">prefix</span><span class="p">)</span>
<span class="w">            </span><span class="k">prefix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">prefix</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">arr</span><span class="o">[</span><span class="n">i-k+1</span><span class="o">]</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">result</span>
</code></pre></div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2637 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2637</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2637</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-da38521a67502b4ec35e24594de8a4a3cebfaba1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/da38521a67502b4ec35e24594de8a4a3cebfaba1" target="_blank">Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The role of the AE in microscopy is not the exclusion of human operators, but rather automation of routine operations such as microscope tuning, etc., prior to the experiment, and conversion of low latency decision making processes on the time scale spanning from image acquisition to human-level high-order experiment planning.</p>
                <p><strong>Paper Abstract:</strong> Machine learning and artificial intelligence (ML/AI) are rapidly becoming an indispensable part of physics research, with domain applications ranging from theory and materials prediction to high-throughput data analysis. In parallel, the recent successes in applying ML/AI methods for autonomous systems from robotics through self-driving cars to organic and inorganic synthesis are generating enthusiasm for the potential of these techniques to enable automated and autonomous experiment (AE) in imaging. Here, we aim to analyze the major pathways towards AE in imaging methods with sequential image formation mechanisms, focusing on scanning probe microscopy (SPM) and (scanning) transmission electron microscopy ((S)TEM). We argue that automated experiments should necessarily be discussed in a broader context of the general domain knowledge that both informs the experiment and is increased as the result of the experiment. As such, this analysis should explore the human and ML/AI roles prior to and during the experiment, and consider the latencies, biases, and knowledge priors of the decision-making process. Similarly, such discussion should include the limitations of the existing imaging systems, including intrinsic latencies, non-idealities and drifts comprising both correctable and stochastic components. We further pose that the role of the AE in microscopy is not the exclusion of human operators (as is the case for autonomous driving), but rather automation of routine operations such as microscope tuning, etc., prior to the experiment, and conversion of low latency decision making processes on the time scale spanning from image acquisition to human-level high-order experiment planning.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2637.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2637.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process based Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A surrogate-model-driven automated-experiment strategy using Gaussian Process regression to model an objective over parameter space and acquisition functions (e.g., EI, probability of improvement, linear μ/σ tradeoff) to decide next measurements, enabling adaptive exploration and exploitation with quantified predictive uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gaussian Process Bayesian Optimization (GP-BO) for Automated Experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Builds a probabilistic surrogate f(x) with a Gaussian Process (GP) using a chosen kernel and hyperparameters inferred from observations; computes predictive mean μ(x) and predictive standard deviation σ(x); uses an acquisition function a(μ,σ) (examples: linear μ/σ combination, Expected Improvement (EI), Probability of Improvement) to pick next measurement locations; can be run as pure uncertainty maximization (ADoE) or as BO balancing predicted performance and uncertainty; kernel choice (RBF, Matern, spectral mixture, deep kernel learning) encodes inductive priors; retrains/adapts hyperparameters iteratively; can be augmented with a pathfinder function that sequences measurements among multiple acquisition maxima considering motion costs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Microscopy (SPM, (S)TEM) imaging and spectroscopy, low-dimensional experimental parameter optimization, automated synthesis and materials discovery in contexts where measurements are expensive/time-limited.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select next experiment(s) by maximizing an acquisition function a(μ(x),σ(x)) that trades predicted objective μ(x) against uncertainty σ(x). Special modes include (i) pure exploratory ADoE via σ(x) maximization to reduce surrogate uncertainty, (ii) exploitation via maximizing μ(x), and (iii) hybrid via EI or weighted μ+λσ. A pathfinder function may impose path/motion costs and order of probing when acquisition function has multiple nearby maxima.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not formalized in the paper; discussed qualitatively as number of physical measurements (experiments), wall-clock experiment time (including probe travel), and computation time for GP inference and kernel hyperparameter optimization (retraining), with emphasis that retraining cost and instrument latency limit real-time applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP predictive variance σ(x) (uncertainty reduction) and acquisition-function-derived metrics such as Expected Improvement (EI) and Probability of Improvement; ADoE objective targets reduction in predictive uncertainty and/or information about kernel hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition functions explicitly balance exploration (high σ) vs exploitation (high μ); linear combination coefficients or EI/PI parametrize the tradeoff; masks can suppress boundary bias in σ-maximization; strategies can switch acquisition functions during the run.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via acquisition function promoting exploration (σ) and by choosing diverse kernel classes; pathfinder can enforce spatial sequencing constraints to avoid repeated sampling of similar points; no explicit ensemble diversity mechanism described by GP-BO itself.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Measurement budget (number of measurements), wall-clock time, instrument latency, sample damage/cumulative interventions (intervention cost).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisition function tuning controls how aggressively exploration is pursued under a limited budget; adaptive retraining of kernel hyperparameters affects where uncertainty is believed to be high; pathfinder incorporates motion/time cost into sequencing; causal BO (see separate entry) can trade off observation vs intervention costs.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>High objective function values (as defined by operator), Expected Improvement as proxy for potential 'breakthrough' measurements, and discovery of high-uncertainty regions that indicate novel/unexpected behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Grid/rectangular full scans, uniform sampling, human operator selection, and uninformed/random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: GP-BO is presented as much more efficient than naïve uniform sampling for low-dimensional parameter spaces and has been used in automated synthesis contexts; no quantitative gains reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses tradeoffs between exploration and exploitation embodied in the acquisition function, cost of retraining hyperparameters (affecting adaptivity), boundary bias of pure σ maximization, and measurement-induced changes (intervention costs). Recommends flexible kernels and integrating ADoE objectives to improve hyperparameter estimation during BO.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Authors recommend encoding domain priors in kernels or probabilistic models when available, using acquisition functions that reflect experiment goals (including switching strategies), optimizing kernel hyperparameters repeatedly for adaptivity, and incorporating motion/path costs (pathfinder) and causal structure when measurements change the system.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2637.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2637.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BO extension that integrates observational and interventional data through a causal prior and replaces classical EI with a causal Expected Improvement (causal EI) to explicitly account for the effect of interventions and balance observation vs intervention decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Causal Bayesian Optimization (causal-BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Augments Bayesian optimization with a causal prior (causal graph) over input variables; incorporates both observational data and data from interventions (measurements that alter system state) into the surrogate; uses a causal EI acquisition function that explicitly evaluates the expected utility of performing interventions versus passive observations, enabling selection of interventions that are most informative/impactful given causal structure.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Dynamically changing experimental systems where measurements can be interventions (e.g., electron beam that can modify material during imaging), and more generally experiments where causal relationships among input variables affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate resources by choosing between observational probes and interventional measurements according to causal EI: select the action (observe or intervene on variable(s)) expected to maximize causal improvement of the objective given a budget/cost model and causal prior.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Framed in terms of number/cost of interventions and observations and computational cost for computing causal EI under the causal prior; explicit numerical metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Causal Expected Improvement (causal EI) — an acquisition metric that accounts for expected change in objective due to interventions, effectively measuring value of information from interventions in a causal model.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Causal EI balances classical exploration-exploitation while adding an observation-intervention tradeoff; exploration may include probing uncertain causal links, exploitation favors interventions predicted to improve objective.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicitly focused on hypothesis diversity; diversity emerges from exploring different intervention types/targets when causal EI favors different interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Intervention/measurement cost, time, number of allowed interventions, sample-damage budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Causal EI implicitly considers costs of interventions versus observations and selects actions that optimize expected improvement per resource cost; requires known or assumed causal graph to function effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Reduction in optimization cost/time and discovery of beneficial interventions as measured by causal EI and final objective improvements; no specific numeric threshold provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Classical (non-causal) Bayesian optimization that treats inputs as independent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative: paper notes causal BO can significantly reduce optimization cost/time when a causal graph is known, but quantification is not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative potential for faster optimization/reduced number of costly interventions when causal structure is exploited; no numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Key tradeoff is the benefit of causal-informed interventions vs the cost/uncertainty of specifying the causal graph; causal BO adds an observation-intervention balance to classical exploration–exploitation tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Authors suggest using causal BO when causal relationships are known or can be reasonably assumed, as it can substantially reduce experimental cost by selecting informative interventions; otherwise classical BO may be preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2637.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2637.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Predictability-based</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predictability-based learning (im2spec / spec2im and descriptor-driven uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active learning approach that trains models mapping structural images to spectra (im2spec) or spectra to images (spec2im) and uses model prediction uncertainty (from ensembles, BNNs or GPs) on map-derived physical descriptors to allocate expensive hyperspectral measurements to regions with high uncertainty or novel behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Predictability-based Active Learning via im2spec/spec2im and descriptor predictions</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Use encoder-decoder or other neural architectures to learn correlations between high-resolution structural images and sparse hyperspectral/spectroscopic measurements; compute prediction uncertainty (via ensembles, Bayesian NN, or GP) across the image plane to identify regions where model predictions are unreliable; prioritize these regions for follow-up detailed, costly measurements; can incorporate symmetry-aware network architectures and transfer/meta-learning to mitigate distribution shift.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Microscopy (STEM, SPM) with high-resolution structural imaging and sparsely sampled hyperspectral modalities (EELS, 4D-STEM, CITS), and more generally multimodal experimental settings with asymmetry in data sampling densities.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate high-cost spectroscopic or fine-resolution measurements to pixels/regions with largest predictive uncertainty or model error — i.e., probe where im2spec/spec2im models have highest variance or largest residuals relative to predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Model training time (GPU hours), inference latency for uncertainty maps, and number of expensive hyperspectral measurements saved/used; the paper discusses these qualitatively rather than giving formal units.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Prediction uncertainty from ensembles or Bayesian networks (variance/dispersion across models), and model residuals compared to ground truth when available; uncertainty used as proxy for expected information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration prioritized by high-prediction-uncertainty regions (novel/unusual behaviors); exploitation realized by sampling regions predicted to have desired properties; operator can choose descriptors (targets) that bias the allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Ensemble approaches (ELIT) and active retraining encourage consideration of diverse model hypotheses; selecting regions of high uncertainty naturally promotes exploratory diversity across physical regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited high-cost hyperspectral measurement budget, acquisition time per pixel/line, and computational training/inference resources.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Prioritize measurements by uncertainty ranking to maximize information per costly measurement; use pan-sharpening/imputation to extend sparse spectra via structural predictors; employ transfer/meta-learning to reduce retraining cost.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Discovery of regions where predicted descriptors diverge from model expectations (novel physical behavior); uncertainty peaks used to flag candidate breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative example: ELIT iterative retraining on graphene reached artifact-free high-quality predictions with associated uncertainties after three ELIT iterations (no numerical accuracy or throughput metrics provided).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Uniform dense sampling for spectroscopy, pretrained networks without iterative retraining, and pure GP-BO uncertainty maximization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative improvements in localization of 'unusual' behaviors and classification accuracy after ELIT retraining; no quantitative comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Expected fewer hyperspectral measurements needed to detect novel features compared to uniform scanning; specific gains not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Discusses tradeoffs between prior-trained networks (vulnerable to out-of-distribution drift) and on-the-fly training (computational cost and latency); recommends ensemble, transfer learning, and ELIT to manage dataset shift vs labeling cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Authors recommend using model-predicted uncertainty on physically meaningful descriptors to guide measurement allocation, iteratively retraining models during experiment (ELIT) to address dataset shift, and using transfer/meta-learning to improve GP adaptivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2637.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2637.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRL-curiosity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Learning with curiosity/empowerment for automated experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential-decision framework where a learning agent optimizes a cumulative (discounted) reward to plan multi-step experimental actions; intrinsic rewards such as curiosity (prediction error) or empowerment (KL divergence of successor state distributions) are added to encourage exploration in sparse-reward scientific environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep Reinforcement Learning (DRL) with Intrinsic Motivation for Sequential Experimental Control</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Treats experiment as an environment where an agent's actions (e.g., probe motions, interventions) lead to state transitions and sparse extrinsic rewards; uses model-free (policy gradients, actor-critic, Q-learning) or model-based RL; augments extrinsic rewards with intrinsic signals such as curiosity (discrepancy between predicted and observed next-state features) or empowerment (information-theoretic influence of actions) to improve exploration; may parallelize multiple agents to increase coverage; can use model-based rollouts to reduce costly real interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Long-horizon, sequential control tasks in microscopy (atomic manipulation, constructing assemblies), materials synthesis with temporally extended processes, and other experimental tasks requiring many sequential decisions where rewards are sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experimental actions across long sequences by optimizing cumulative discounted reward; intrinsic curiosity encourages allocation to actions that produce high model prediction error (high information), while empowerment favors actions that most change the environment; model-based variants allocate fewer costly real experiments by simulating rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of environment interactions (real experiments), number of training episodes/steps, wall-clock training time, and compute resources for neural-network training; sample complexity is the major cost metric discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Intrinsic curiosity reward operationalized as prediction error in a learned forward model (proxy for information gain); empowerment measured as KL divergence between action-conditional and marginal successor-state distributions (proxy for potential control/information).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration: intrinsic rewards (curiosity, empowerment), entropy regularization, multiple-agent parallel exploration. Exploitation: extrinsic task reward and policy optimization (actor-critic or Q-learning) to maximize cumulative reward.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Entropy bonuses on policy, curiosity-driven intrinsic reward, parallel agents producing diverse trajectories; these explicitly incentivize visiting diverse states and action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Number of real experimental interactions (expensive), total experimental time, potential sample degradation with actions.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Model-based RL and use of simulations reduce required real interactions; IRL/demonstrations can seed policies and reduce exploration cost; reward shaping and intrinsic rewards help make scarce interactions more informative.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Achievement of novel, high-reward states or sequences (e.g., successful atomic assemblies); typically operationalized by task reward, not a general novelty score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human operator performance, random action policies, model-free RL without intrinsic rewards, and model-based RL variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper notes DRL successes in domains (games, robotics) but does not present quantitative microscopy-specific comparisons; suggests DRL could handle long action sequences like atomic assembly but is sample intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Model-based RL and intrinsic-reward schemes can increase sample efficiency relative to naïve model-free RL; no microscopy-specific numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Discusses cost of sparse extrinsic rewards causing very high sample complexity, potential for reward hacking if reward shaping is poor, and tradeoffs between model-based sample efficiency and model bias; recommends intrinsic motivation and demonstrations to mitigate costs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Authors suggest combining model-based methods, intrinsic curiosity, entropy regularization, and demonstrations (IRL) to reduce costly experimental interactions and improve exploration toward breakthrough states.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2637.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2637.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique to infer an implicit reward function from expert demonstrations, which can then be used to train RL agents — useful when reward engineering is difficult and human expertise is available to seed policies and reduce exploration cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Inverse Reinforcement Learning (IRL) for experimental policy inference</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Observes expert trajectories (e.g., human microscope operator actions) and fits a reward function (parameterized by a neural network) whose optimal policy would reproduce those behaviors; the inferred reward can then be used for standard RL to learn policies potentially surpassing the demonstrator; helps avoid manual reward shaping and reduces sample complexity required for RL.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Complex experimental workflows where human expertise is available but difficult to encode numerically (microscope parameter tuning, multi-parameter intervention tasks, atomic manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Leverages demonstration-derived reward to prioritize actions that human experts would take, thus allocating experimental budget toward trajectories that are more likely to produce useful results; subsequent RL refines and explores beyond demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost to obtain demonstration trajectories (human time), compute to infer reward function, and subsequent RL training episodes; reduces total costly environment interactions compared to pure exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>IRL does not explicitly use information-theoretic gain metrics; information is gained by learning a reward representation that captures expert preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration is guided by demonstrated behavior (imitation) and further RL-driven exploration can exploit inferred reward; exploration needed to improve beyond expert demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicit; policies learned via RL can exceed diversity of demonstrations, but IRL itself doesn't enforce diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Experimental cost (number of trials), cost of collecting demonstrations (human time).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>IRL reduces required random exploration by using demonstrations to constrain reward/policy learning, thereby saving experimental trials.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Ability of RL policy trained on inferred rewards to outperform demonstrator on task-defined metrics; no numeric thresholds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Pure RL trained from scratch without demonstrations, or manually engineered reward functions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Cites examples outside microscopy where IRL-derived rewards enabled strong performance (Atari games), but no microscopy-specific quantitative comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: reduced sample complexity and less hand-tuning of reward functions; no numerical gains reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>IRL trades the cost of collecting demonstrations and reward-inference computation against reduced costly exploration; useful when reward shaping is hard or would induce undesirable 'hacked' solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Authors recommend IRL for tasks where human expertise exists but manual reward design is infeasible or brittle; inferred rewards plus RL can be an effective allocation strategy for expensive sequential experimental tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2637.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2637.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELIT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble Learning–Iterative Training (ELIT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that begins with an ensemble of models trained on simulated data to provide predictions with uncertainty and iteratively retrains on high-confidence predictions from real experimental data, rapidly adapting to the specific experimental distribution and reducing manual labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Ensemble Learning–Iterative Training (ELIT) for microscopy feature identification</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Train an ensemble of neural networks on broad simulated datasets to capture a range of imaging conditions; apply ensemble to experimental images to get pixel-wise predictions and predictive uncertainty (dispersion across models); select high-confidence predicted examples as pseudo-labels to iteratively retrain the ensemble on experiment-specific distributions; repeat (few iterations) to converge to high-quality predictions and calibrated uncertainties, reducing need for manual labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Microscopy image segmentation/classification (STEM, SPM) and feature detection where collecting ground-truth labels is expensive or impractical.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Uses ensemble uncertainty to prioritize regions for retraining and/or further targeted measurements; leverages simulation-derived priors to minimize human labeling effort and allocate human/cost resources to the most ambiguous/novel regions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Training/inference GPU-time (per iteration), number of retraining iterations (low; example uses three), and reduced manual labeling time; paper emphasizes that ELIT converges in few iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Ensemble prediction dispersion (per-pixel variance) used as uncertainty proxy; reduction in dispersion across iterations is the operationalized information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploit high-confidence pseudo-labels to improve model (exploitation) while focusing human labeling and further measurements on low-confidence regions (exploration); iterative cycles shift resources toward informative regions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Model ensemble diversity (different models/initializations/training data) provides uncertainty estimates and guards against overconfident errors; iterative retraining focuses on classes present in the system, encouraging model specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited human labeling budget, measurement budget, and compute budget for retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Minimizes manual labeling by using simulated training and pseudo-labeling high-confidence regions; few retraining iterations reduce compute budget; prioritizes human/experimental resources to uncertain regions.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improved classification accuracy and reliable uncertainty estimation enabling detection of anomalous/novel features; in the paper, artifact-free high-quality predictions with uncertainty were achieved after three ELIT iterations (no numeric accuracy provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: artifact-free high-quality prediction and associated uncertainties achieved after 3 ELIT iterations on graphene experimental data; the paper does not report exact accuracy/precision/recall numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Single pretrained network without iterative retraining, manual labeling, or absence of ensemble uncertainty guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative: ELIT rapidly improves classification accuracy and uncertainty calibration relative to baseline pretrained networks when confronted with dataset shift; no quantitative comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reduces manual labeling and converges quickly (few iterations) to accurate predictions on experiment-specific data; no explicit percent or throughput numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Addresses dataset shift / out-of-distribution risk by trading initial simulation-derived breadth for experiment-specific specialization via iterative retraining; computational retraining cost is balanced against saved human labeling and improved inference quality.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Authors recommend starting with broad simulated training to avoid manual labeling, then apply ELIT to iteratively adapt to the current experiment, focusing retraining and human effort on low-confidence regions to efficiently allocate resources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2637.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2637.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pathfinder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pathfinder function for acquisition sequencing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decision-layer that sequences measurement points among multiple acquisition-function maxima by incorporating probe motion constraints, proximity, and scanner performance to minimize overhead and produce an experimentally optimal measurement path.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pathfinder sequencing for acquisition maxima</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given an acquisition function landscape with possibly many nearby maxima (degeneracies), the pathfinder computes an ordered sequence of measurement points that balances acquisition value with practical probe motion costs and experimental constraints (e.g., non-crisscrossing paths, preferred motion directions), thereby converting acquisition maxima into an efficient real-world measurement schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scanning experiments (SPM, STEM) where physical probe travel cost and instrument constraints make naive selection of next best point inefficient.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates measurement order (and therefore time) by optimizing a combined utility that weights acquisition-value (information/utility) against motion/travel time and instrument behavior, thereby deciding which maxima to probe first under time constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Probe travel time and additional overhead introduced by measurement ordering; computed implicitly as part of path cost rather than a pure FLOP/time metric.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Underlying acquisition function values (μ/σ/EI) provide the information value for each candidate; pathfinder integrates this with travel-cost heuristics to compute net utility.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Sequences among acquisition maxima to exploit high-utility regions while minimizing wasted time; does not itself change μ/σ tradeoff but optimizes realization under mechanical/time constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicitly designed to promote hypothesis diversity; aims to efficiently visit multiple acquisition maxima which can incidentally increase coverage diversity compared to greedy jumps.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Time budget and probe motion/overhead constraints inherent to the scanning instrument.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Incorporates motion cost and sequencing constraints into the decision of which acquisition maxima to probe next to maximize information per unit time.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not directly defined; effectiveness measured by reduced overhead and ability to reach informative maxima within time budget.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Naïve selection of acquisition maxima without sequencing considerations (e.g., picking global maxima in arbitrary order).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: reduces time lost to probe travel and nonoptimal ordering, improving effective information collection rate; no quantitative numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Highlights tradeoff between choosing the most informative next point and the practical cost (time/motion) to reach it; recommends integrating path considerations into AE sequencing.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Include motion and probe constraints when implementing BO/ADoE in scanning instruments; use a pathfinder to convert acquisition maxima into a realistic, time-efficient measurement plan.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2637.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2637.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theory-in-loop AL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active learning augmented by real-time modeling/theory (co-navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active learning paradigm where the automated experiment queries and integrates external theory or modeling in real time to evaluate candidate experiments, effectively co-navigating between local sensing (experiment) and global maps (theory) to prioritize experiments under resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Active Learning with Real-time Modeling / Theory-in-the-loop (Co-navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>During AE operation, the controller queries external theoretical models, databases, or performs on-the-fly simulations to evaluate candidate experimental actions; model outputs (predicted properties, maps of parameter space) are combined with measured uncertainties to prioritize experiments that either most reduce model-experiment mismatch or are predicted to yield high objective values; this reduces human modeling latency by making modeling part of the closed-loop experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials discovery and optimization (e.g., composition-property searches, catalyst design), microscopy experiments where theory can provide a 'map' (phase/parameter topology) to guide local exploration, and any domain where fast modeling can direct costly experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate physical experiment budget toward candidates that models predict to be high-value or to reduce model uncertainty most per experimental cost; trade off running additional expensive simulations vs conducting physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost of running models/simulations (wall-clock and compute resources), number of physical experiments, and total human-in-the-loop latency; no single metric is formalized but all are discussed as constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Model-experiment mismatch reduction, expected improvement computed using model predictions combined with experimental uncertainty; information gain measured implicitly by how much the model posterior is updated by an experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Co-navigation balances local experimental exploration (driven by immediate uncertainty) against exploitation guided by global theoretical maps; models can be used to prioritize experiments expected to improve objective or validate/expand theoretical coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Ability to query different models, datasets, and parameterizations increases hypothesis diversity; actively seeking model-discrepant regions promotes exploring diverse, potentially novel hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational budget for modeling/simulations, experimental budget (time, measurements), and human latency.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Decisions weigh simulation runtime versus experimental cost; models are used selectively when their predictive value justifies compute cost; model uncertainty and experiment cost drive prioritization.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Model-predicted high-value candidates that are experimentally validated; reduction in model uncertainty about key regions of parameter space; no single numeric metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Experiment-only AE without real-time modeling or traditional human-modeling-with-latency workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative: co-navigation is expected to reduce human latency and the number of costly experiments needed to reach high-value discoveries; paper does not give numerical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Potential speedup by running modeling in parallel to acquisition and reducing unnecessary experiments; no quantitative gains reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Emphasizes representational mismatch between model and experiment, the cost of model queries, and the need to align observables; stresses that model-assisted AE is advantageous when models are sufficiently predictive and matching to experimental observables is feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Authors recommend integrating theory/model queries into AE when models can provide reliable guidance or narrow candidate spaces, but caution that representational mismatches must be handled explicitly; co-navigation (map + local sensing) is proposed as a useful principle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2637",
    "paper_id": "paper-da38521a67502b4ec35e24594de8a4a3cebfaba1",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "GP-BO",
            "name_full": "Gaussian Process based Bayesian Optimization",
            "brief_description": "A surrogate-model-driven automated-experiment strategy using Gaussian Process regression to model an objective over parameter space and acquisition functions (e.g., EI, probability of improvement, linear μ/σ tradeoff) to decide next measurements, enabling adaptive exploration and exploitation with quantified predictive uncertainty.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Gaussian Process Bayesian Optimization (GP-BO) for Automated Experimentation",
            "system_description": "Builds a probabilistic surrogate f(x) with a Gaussian Process (GP) using a chosen kernel and hyperparameters inferred from observations; computes predictive mean μ(x) and predictive standard deviation σ(x); uses an acquisition function a(μ,σ) (examples: linear μ/σ combination, Expected Improvement (EI), Probability of Improvement) to pick next measurement locations; can be run as pure uncertainty maximization (ADoE) or as BO balancing predicted performance and uncertainty; kernel choice (RBF, Matern, spectral mixture, deep kernel learning) encodes inductive priors; retrains/adapts hyperparameters iteratively; can be augmented with a pathfinder function that sequences measurements among multiple acquisition maxima considering motion costs.",
            "application_domain": "Microscopy (SPM, (S)TEM) imaging and spectroscopy, low-dimensional experimental parameter optimization, automated synthesis and materials discovery in contexts where measurements are expensive/time-limited.",
            "resource_allocation_strategy": "Select next experiment(s) by maximizing an acquisition function a(μ(x),σ(x)) that trades predicted objective μ(x) against uncertainty σ(x). Special modes include (i) pure exploratory ADoE via σ(x) maximization to reduce surrogate uncertainty, (ii) exploitation via maximizing μ(x), and (iii) hybrid via EI or weighted μ+λσ. A pathfinder function may impose path/motion costs and order of probing when acquisition function has multiple nearby maxima.",
            "computational_cost_metric": "Not formalized in the paper; discussed qualitatively as number of physical measurements (experiments), wall-clock experiment time (including probe travel), and computation time for GP inference and kernel hyperparameter optimization (retraining), with emphasis that retraining cost and instrument latency limit real-time applicability.",
            "information_gain_metric": "GP predictive variance σ(x) (uncertainty reduction) and acquisition-function-derived metrics such as Expected Improvement (EI) and Probability of Improvement; ADoE objective targets reduction in predictive uncertainty and/or information about kernel hyperparameters.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition functions explicitly balance exploration (high σ) vs exploitation (high μ); linear combination coefficients or EI/PI parametrize the tradeoff; masks can suppress boundary bias in σ-maximization; strategies can switch acquisition functions during the run.",
            "diversity_mechanism": "Implicit via acquisition function promoting exploration (σ) and by choosing diverse kernel classes; pathfinder can enforce spatial sequencing constraints to avoid repeated sampling of similar points; no explicit ensemble diversity mechanism described by GP-BO itself.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Measurement budget (number of measurements), wall-clock time, instrument latency, sample damage/cumulative interventions (intervention cost).",
            "budget_constraint_handling": "Acquisition function tuning controls how aggressively exploration is pursued under a limited budget; adaptive retraining of kernel hyperparameters affects where uncertainty is believed to be high; pathfinder incorporates motion/time cost into sequencing; causal BO (see separate entry) can trade off observation vs intervention costs.",
            "breakthrough_discovery_metric": "High objective function values (as defined by operator), Expected Improvement as proxy for potential 'breakthrough' measurements, and discovery of high-uncertainty regions that indicate novel/unexpected behavior.",
            "performance_metrics": null,
            "comparison_baseline": "Grid/rectangular full scans, uniform sampling, human operator selection, and uninformed/random sampling.",
            "performance_vs_baseline": null,
            "efficiency_gain": "Qualitative: GP-BO is presented as much more efficient than naïve uniform sampling for low-dimensional parameter spaces and has been used in automated synthesis contexts; no quantitative gains reported in this paper.",
            "tradeoff_analysis": "Paper discusses tradeoffs between exploration and exploitation embodied in the acquisition function, cost of retraining hyperparameters (affecting adaptivity), boundary bias of pure σ maximization, and measurement-induced changes (intervention costs). Recommends flexible kernels and integrating ADoE objectives to improve hyperparameter estimation during BO.",
            "optimal_allocation_findings": "Authors recommend encoding domain priors in kernels or probabilistic models when available, using acquisition functions that reflect experiment goals (including switching strategies), optimizing kernel hyperparameters repeatedly for adaptivity, and incorporating motion/path costs (pathfinder) and causal structure when measurements change the system.",
            "uuid": "e2637.0",
            "source_info": {
                "paper_title": "Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Causal-BO",
            "name_full": "Causal Bayesian Optimization",
            "brief_description": "A BO extension that integrates observational and interventional data through a causal prior and replaces classical EI with a causal Expected Improvement (causal EI) to explicitly account for the effect of interventions and balance observation vs intervention decisions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Causal Bayesian Optimization (causal-BO)",
            "system_description": "Augments Bayesian optimization with a causal prior (causal graph) over input variables; incorporates both observational data and data from interventions (measurements that alter system state) into the surrogate; uses a causal EI acquisition function that explicitly evaluates the expected utility of performing interventions versus passive observations, enabling selection of interventions that are most informative/impactful given causal structure.",
            "application_domain": "Dynamically changing experimental systems where measurements can be interventions (e.g., electron beam that can modify material during imaging), and more generally experiments where causal relationships among input variables affect outcomes.",
            "resource_allocation_strategy": "Allocate resources by choosing between observational probes and interventional measurements according to causal EI: select the action (observe or intervene on variable(s)) expected to maximize causal improvement of the objective given a budget/cost model and causal prior.",
            "computational_cost_metric": "Framed in terms of number/cost of interventions and observations and computational cost for computing causal EI under the causal prior; explicit numerical metrics not provided.",
            "information_gain_metric": "Causal Expected Improvement (causal EI) — an acquisition metric that accounts for expected change in objective due to interventions, effectively measuring value of information from interventions in a causal model.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Causal EI balances classical exploration-exploitation while adding an observation-intervention tradeoff; exploration may include probing uncertain causal links, exploitation favors interventions predicted to improve objective.",
            "diversity_mechanism": "Not explicitly focused on hypothesis diversity; diversity emerges from exploring different intervention types/targets when causal EI favors different interventions.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Intervention/measurement cost, time, number of allowed interventions, sample-damage budget.",
            "budget_constraint_handling": "Causal EI implicitly considers costs of interventions versus observations and selects actions that optimize expected improvement per resource cost; requires known or assumed causal graph to function effectively.",
            "breakthrough_discovery_metric": "Reduction in optimization cost/time and discovery of beneficial interventions as measured by causal EI and final objective improvements; no specific numeric threshold provided.",
            "performance_metrics": null,
            "comparison_baseline": "Classical (non-causal) Bayesian optimization that treats inputs as independent.",
            "performance_vs_baseline": "Qualitative: paper notes causal BO can significantly reduce optimization cost/time when a causal graph is known, but quantification is not provided in this paper.",
            "efficiency_gain": "Qualitative potential for faster optimization/reduced number of costly interventions when causal structure is exploited; no numbers provided.",
            "tradeoff_analysis": "Key tradeoff is the benefit of causal-informed interventions vs the cost/uncertainty of specifying the causal graph; causal BO adds an observation-intervention balance to classical exploration–exploitation tradeoff.",
            "optimal_allocation_findings": "Authors suggest using causal BO when causal relationships are known or can be reasonably assumed, as it can substantially reduce experimental cost by selecting informative interventions; otherwise classical BO may be preferable.",
            "uuid": "e2637.1",
            "source_info": {
                "paper_title": "Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Predictability-based",
            "name_full": "Predictability-based learning (im2spec / spec2im and descriptor-driven uncertainty)",
            "brief_description": "An active learning approach that trains models mapping structural images to spectra (im2spec) or spectra to images (spec2im) and uses model prediction uncertainty (from ensembles, BNNs or GPs) on map-derived physical descriptors to allocate expensive hyperspectral measurements to regions with high uncertainty or novel behavior.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Predictability-based Active Learning via im2spec/spec2im and descriptor predictions",
            "system_description": "Use encoder-decoder or other neural architectures to learn correlations between high-resolution structural images and sparse hyperspectral/spectroscopic measurements; compute prediction uncertainty (via ensembles, Bayesian NN, or GP) across the image plane to identify regions where model predictions are unreliable; prioritize these regions for follow-up detailed, costly measurements; can incorporate symmetry-aware network architectures and transfer/meta-learning to mitigate distribution shift.",
            "application_domain": "Microscopy (STEM, SPM) with high-resolution structural imaging and sparsely sampled hyperspectral modalities (EELS, 4D-STEM, CITS), and more generally multimodal experimental settings with asymmetry in data sampling densities.",
            "resource_allocation_strategy": "Allocate high-cost spectroscopic or fine-resolution measurements to pixels/regions with largest predictive uncertainty or model error — i.e., probe where im2spec/spec2im models have highest variance or largest residuals relative to predictions.",
            "computational_cost_metric": "Model training time (GPU hours), inference latency for uncertainty maps, and number of expensive hyperspectral measurements saved/used; the paper discusses these qualitatively rather than giving formal units.",
            "information_gain_metric": "Prediction uncertainty from ensembles or Bayesian networks (variance/dispersion across models), and model residuals compared to ground truth when available; uncertainty used as proxy for expected information gain.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration prioritized by high-prediction-uncertainty regions (novel/unusual behaviors); exploitation realized by sampling regions predicted to have desired properties; operator can choose descriptors (targets) that bias the allocation.",
            "diversity_mechanism": "Ensemble approaches (ELIT) and active retraining encourage consideration of diverse model hypotheses; selecting regions of high uncertainty naturally promotes exploratory diversity across physical regimes.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Limited high-cost hyperspectral measurement budget, acquisition time per pixel/line, and computational training/inference resources.",
            "budget_constraint_handling": "Prioritize measurements by uncertainty ranking to maximize information per costly measurement; use pan-sharpening/imputation to extend sparse spectra via structural predictors; employ transfer/meta-learning to reduce retraining cost.",
            "breakthrough_discovery_metric": "Discovery of regions where predicted descriptors diverge from model expectations (novel physical behavior); uncertainty peaks used to flag candidate breakthroughs.",
            "performance_metrics": "Qualitative example: ELIT iterative retraining on graphene reached artifact-free high-quality predictions with associated uncertainties after three ELIT iterations (no numerical accuracy or throughput metrics provided).",
            "comparison_baseline": "Uniform dense sampling for spectroscopy, pretrained networks without iterative retraining, and pure GP-BO uncertainty maximization.",
            "performance_vs_baseline": "Qualitative improvements in localization of 'unusual' behaviors and classification accuracy after ELIT retraining; no quantitative comparison in this paper.",
            "efficiency_gain": "Expected fewer hyperspectral measurements needed to detect novel features compared to uniform scanning; specific gains not reported.",
            "tradeoff_analysis": "Discusses tradeoffs between prior-trained networks (vulnerable to out-of-distribution drift) and on-the-fly training (computational cost and latency); recommends ensemble, transfer learning, and ELIT to manage dataset shift vs labeling cost.",
            "optimal_allocation_findings": "Authors recommend using model-predicted uncertainty on physically meaningful descriptors to guide measurement allocation, iteratively retraining models during experiment (ELIT) to address dataset shift, and using transfer/meta-learning to improve GP adaptivity.",
            "uuid": "e2637.2",
            "source_info": {
                "paper_title": "Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "DRL-curiosity",
            "name_full": "Deep Reinforcement Learning with curiosity/empowerment for automated experiments",
            "brief_description": "A sequential-decision framework where a learning agent optimizes a cumulative (discounted) reward to plan multi-step experimental actions; intrinsic rewards such as curiosity (prediction error) or empowerment (KL divergence of successor state distributions) are added to encourage exploration in sparse-reward scientific environments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Deep Reinforcement Learning (DRL) with Intrinsic Motivation for Sequential Experimental Control",
            "system_description": "Treats experiment as an environment where an agent's actions (e.g., probe motions, interventions) lead to state transitions and sparse extrinsic rewards; uses model-free (policy gradients, actor-critic, Q-learning) or model-based RL; augments extrinsic rewards with intrinsic signals such as curiosity (discrepancy between predicted and observed next-state features) or empowerment (information-theoretic influence of actions) to improve exploration; may parallelize multiple agents to increase coverage; can use model-based rollouts to reduce costly real interactions.",
            "application_domain": "Long-horizon, sequential control tasks in microscopy (atomic manipulation, constructing assemblies), materials synthesis with temporally extended processes, and other experimental tasks requiring many sequential decisions where rewards are sparse.",
            "resource_allocation_strategy": "Allocates experimental actions across long sequences by optimizing cumulative discounted reward; intrinsic curiosity encourages allocation to actions that produce high model prediction error (high information), while empowerment favors actions that most change the environment; model-based variants allocate fewer costly real experiments by simulating rollouts.",
            "computational_cost_metric": "Number of environment interactions (real experiments), number of training episodes/steps, wall-clock training time, and compute resources for neural-network training; sample complexity is the major cost metric discussed.",
            "information_gain_metric": "Intrinsic curiosity reward operationalized as prediction error in a learned forward model (proxy for information gain); empowerment measured as KL divergence between action-conditional and marginal successor-state distributions (proxy for potential control/information).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration: intrinsic rewards (curiosity, empowerment), entropy regularization, multiple-agent parallel exploration. Exploitation: extrinsic task reward and policy optimization (actor-critic or Q-learning) to maximize cumulative reward.",
            "diversity_mechanism": "Entropy bonuses on policy, curiosity-driven intrinsic reward, parallel agents producing diverse trajectories; these explicitly incentivize visiting diverse states and action sequences.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Number of real experimental interactions (expensive), total experimental time, potential sample degradation with actions.",
            "budget_constraint_handling": "Model-based RL and use of simulations reduce required real interactions; IRL/demonstrations can seed policies and reduce exploration cost; reward shaping and intrinsic rewards help make scarce interactions more informative.",
            "breakthrough_discovery_metric": "Achievement of novel, high-reward states or sequences (e.g., successful atomic assemblies); typically operationalized by task reward, not a general novelty score.",
            "performance_metrics": null,
            "comparison_baseline": "Human operator performance, random action policies, model-free RL without intrinsic rewards, and model-based RL variants.",
            "performance_vs_baseline": "Paper notes DRL successes in domains (games, robotics) but does not present quantitative microscopy-specific comparisons; suggests DRL could handle long action sequences like atomic assembly but is sample intensive.",
            "efficiency_gain": "Model-based RL and intrinsic-reward schemes can increase sample efficiency relative to naïve model-free RL; no microscopy-specific numbers provided.",
            "tradeoff_analysis": "Discusses cost of sparse extrinsic rewards causing very high sample complexity, potential for reward hacking if reward shaping is poor, and tradeoffs between model-based sample efficiency and model bias; recommends intrinsic motivation and demonstrations to mitigate costs.",
            "optimal_allocation_findings": "Authors suggest combining model-based methods, intrinsic curiosity, entropy regularization, and demonstrations (IRL) to reduce costly experimental interactions and improve exploration toward breakthrough states.",
            "uuid": "e2637.3",
            "source_info": {
                "paper_title": "Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "IRL",
            "name_full": "Inverse Reinforcement Learning",
            "brief_description": "A technique to infer an implicit reward function from expert demonstrations, which can then be used to train RL agents — useful when reward engineering is difficult and human expertise is available to seed policies and reduce exploration cost.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Inverse Reinforcement Learning (IRL) for experimental policy inference",
            "system_description": "Observes expert trajectories (e.g., human microscope operator actions) and fits a reward function (parameterized by a neural network) whose optimal policy would reproduce those behaviors; the inferred reward can then be used for standard RL to learn policies potentially surpassing the demonstrator; helps avoid manual reward shaping and reduces sample complexity required for RL.",
            "application_domain": "Complex experimental workflows where human expertise is available but difficult to encode numerically (microscope parameter tuning, multi-parameter intervention tasks, atomic manipulation).",
            "resource_allocation_strategy": "Leverages demonstration-derived reward to prioritize actions that human experts would take, thus allocating experimental budget toward trajectories that are more likely to produce useful results; subsequent RL refines and explores beyond demonstrations.",
            "computational_cost_metric": "Cost to obtain demonstration trajectories (human time), compute to infer reward function, and subsequent RL training episodes; reduces total costly environment interactions compared to pure exploration.",
            "information_gain_metric": "IRL does not explicitly use information-theoretic gain metrics; information is gained by learning a reward representation that captures expert preferences.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Exploration is guided by demonstrated behavior (imitation) and further RL-driven exploration can exploit inferred reward; exploration needed to improve beyond expert demonstrations.",
            "diversity_mechanism": "Not explicit; policies learned via RL can exceed diversity of demonstrations, but IRL itself doesn't enforce diversity.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Experimental cost (number of trials), cost of collecting demonstrations (human time).",
            "budget_constraint_handling": "IRL reduces required random exploration by using demonstrations to constrain reward/policy learning, thereby saving experimental trials.",
            "breakthrough_discovery_metric": "Ability of RL policy trained on inferred rewards to outperform demonstrator on task-defined metrics; no numeric thresholds provided.",
            "performance_metrics": null,
            "comparison_baseline": "Pure RL trained from scratch without demonstrations, or manually engineered reward functions.",
            "performance_vs_baseline": "Cites examples outside microscopy where IRL-derived rewards enabled strong performance (Atari games), but no microscopy-specific quantitative comparisons in this paper.",
            "efficiency_gain": "Qualitative: reduced sample complexity and less hand-tuning of reward functions; no numerical gains reported.",
            "tradeoff_analysis": "IRL trades the cost of collecting demonstrations and reward-inference computation against reduced costly exploration; useful when reward shaping is hard or would induce undesirable 'hacked' solutions.",
            "optimal_allocation_findings": "Authors recommend IRL for tasks where human expertise exists but manual reward design is infeasible or brittle; inferred rewards plus RL can be an effective allocation strategy for expensive sequential experimental tasks.",
            "uuid": "e2637.4",
            "source_info": {
                "paper_title": "Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "ELIT",
            "name_full": "Ensemble Learning–Iterative Training (ELIT)",
            "brief_description": "An approach that begins with an ensemble of models trained on simulated data to provide predictions with uncertainty and iteratively retrains on high-confidence predictions from real experimental data, rapidly adapting to the specific experimental distribution and reducing manual labeling.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Ensemble Learning–Iterative Training (ELIT) for microscopy feature identification",
            "system_description": "Train an ensemble of neural networks on broad simulated datasets to capture a range of imaging conditions; apply ensemble to experimental images to get pixel-wise predictions and predictive uncertainty (dispersion across models); select high-confidence predicted examples as pseudo-labels to iteratively retrain the ensemble on experiment-specific distributions; repeat (few iterations) to converge to high-quality predictions and calibrated uncertainties, reducing need for manual labeling.",
            "application_domain": "Microscopy image segmentation/classification (STEM, SPM) and feature detection where collecting ground-truth labels is expensive or impractical.",
            "resource_allocation_strategy": "Uses ensemble uncertainty to prioritize regions for retraining and/or further targeted measurements; leverages simulation-derived priors to minimize human labeling effort and allocate human/cost resources to the most ambiguous/novel regions.",
            "computational_cost_metric": "Training/inference GPU-time (per iteration), number of retraining iterations (low; example uses three), and reduced manual labeling time; paper emphasizes that ELIT converges in few iterations.",
            "information_gain_metric": "Ensemble prediction dispersion (per-pixel variance) used as uncertainty proxy; reduction in dispersion across iterations is the operationalized information gain.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploit high-confidence pseudo-labels to improve model (exploitation) while focusing human labeling and further measurements on low-confidence regions (exploration); iterative cycles shift resources toward informative regions.",
            "diversity_mechanism": "Model ensemble diversity (different models/initializations/training data) provides uncertainty estimates and guards against overconfident errors; iterative retraining focuses on classes present in the system, encouraging model specialization.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Limited human labeling budget, measurement budget, and compute budget for retraining.",
            "budget_constraint_handling": "Minimizes manual labeling by using simulated training and pseudo-labeling high-confidence regions; few retraining iterations reduce compute budget; prioritizes human/experimental resources to uncertain regions.",
            "breakthrough_discovery_metric": "Improved classification accuracy and reliable uncertainty estimation enabling detection of anomalous/novel features; in the paper, artifact-free high-quality predictions with uncertainty were achieved after three ELIT iterations (no numeric accuracy provided).",
            "performance_metrics": "Qualitative: artifact-free high-quality prediction and associated uncertainties achieved after 3 ELIT iterations on graphene experimental data; the paper does not report exact accuracy/precision/recall numbers.",
            "comparison_baseline": "Single pretrained network without iterative retraining, manual labeling, or absence of ensemble uncertainty guidance.",
            "performance_vs_baseline": "Qualitative: ELIT rapidly improves classification accuracy and uncertainty calibration relative to baseline pretrained networks when confronted with dataset shift; no quantitative comparisons provided.",
            "efficiency_gain": "Reduces manual labeling and converges quickly (few iterations) to accurate predictions on experiment-specific data; no explicit percent or throughput numbers provided.",
            "tradeoff_analysis": "Addresses dataset shift / out-of-distribution risk by trading initial simulation-derived breadth for experiment-specific specialization via iterative retraining; computational retraining cost is balanced against saved human labeling and improved inference quality.",
            "optimal_allocation_findings": "Authors recommend starting with broad simulated training to avoid manual labeling, then apply ELIT to iteratively adapt to the current experiment, focusing retraining and human effort on low-confidence regions to efficiently allocate resources.",
            "uuid": "e2637.5",
            "source_info": {
                "paper_title": "Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Pathfinder",
            "name_full": "Pathfinder function for acquisition sequencing",
            "brief_description": "A decision-layer that sequences measurement points among multiple acquisition-function maxima by incorporating probe motion constraints, proximity, and scanner performance to minimize overhead and produce an experimentally optimal measurement path.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Pathfinder sequencing for acquisition maxima",
            "system_description": "Given an acquisition function landscape with possibly many nearby maxima (degeneracies), the pathfinder computes an ordered sequence of measurement points that balances acquisition value with practical probe motion costs and experimental constraints (e.g., non-crisscrossing paths, preferred motion directions), thereby converting acquisition maxima into an efficient real-world measurement schedule.",
            "application_domain": "Scanning experiments (SPM, STEM) where physical probe travel cost and instrument constraints make naive selection of next best point inefficient.",
            "resource_allocation_strategy": "Allocates measurement order (and therefore time) by optimizing a combined utility that weights acquisition-value (information/utility) against motion/travel time and instrument behavior, thereby deciding which maxima to probe first under time constraints.",
            "computational_cost_metric": "Probe travel time and additional overhead introduced by measurement ordering; computed implicitly as part of path cost rather than a pure FLOP/time metric.",
            "information_gain_metric": "Underlying acquisition function values (μ/σ/EI) provide the information value for each candidate; pathfinder integrates this with travel-cost heuristics to compute net utility.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Sequences among acquisition maxima to exploit high-utility regions while minimizing wasted time; does not itself change μ/σ tradeoff but optimizes realization under mechanical/time constraints.",
            "diversity_mechanism": "Not explicitly designed to promote hypothesis diversity; aims to efficiently visit multiple acquisition maxima which can incidentally increase coverage diversity compared to greedy jumps.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Time budget and probe motion/overhead constraints inherent to the scanning instrument.",
            "budget_constraint_handling": "Incorporates motion cost and sequencing constraints into the decision of which acquisition maxima to probe next to maximize information per unit time.",
            "breakthrough_discovery_metric": "Not directly defined; effectiveness measured by reduced overhead and ability to reach informative maxima within time budget.",
            "performance_metrics": null,
            "comparison_baseline": "Naïve selection of acquisition maxima without sequencing considerations (e.g., picking global maxima in arbitrary order).",
            "performance_vs_baseline": null,
            "efficiency_gain": "Qualitative: reduces time lost to probe travel and nonoptimal ordering, improving effective information collection rate; no quantitative numbers provided.",
            "tradeoff_analysis": "Highlights tradeoff between choosing the most informative next point and the practical cost (time/motion) to reach it; recommends integrating path considerations into AE sequencing.",
            "optimal_allocation_findings": "Include motion and probe constraints when implementing BO/ADoE in scanning instruments; use a pathfinder to convert acquisition maxima into a realistic, time-efficient measurement plan.",
            "uuid": "e2637.6",
            "source_info": {
                "paper_title": "Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Theory-in-loop AL",
            "name_full": "Active learning augmented by real-time modeling/theory (co-navigation)",
            "brief_description": "An active learning paradigm where the automated experiment queries and integrates external theory or modeling in real time to evaluate candidate experiments, effectively co-navigating between local sensing (experiment) and global maps (theory) to prioritize experiments under resource constraints.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Active Learning with Real-time Modeling / Theory-in-the-loop (Co-navigation)",
            "system_description": "During AE operation, the controller queries external theoretical models, databases, or performs on-the-fly simulations to evaluate candidate experimental actions; model outputs (predicted properties, maps of parameter space) are combined with measured uncertainties to prioritize experiments that either most reduce model-experiment mismatch or are predicted to yield high objective values; this reduces human modeling latency by making modeling part of the closed-loop experiment.",
            "application_domain": "Materials discovery and optimization (e.g., composition-property searches, catalyst design), microscopy experiments where theory can provide a 'map' (phase/parameter topology) to guide local exploration, and any domain where fast modeling can direct costly experiments.",
            "resource_allocation_strategy": "Allocate physical experiment budget toward candidates that models predict to be high-value or to reduce model uncertainty most per experimental cost; trade off running additional expensive simulations vs conducting physical experiments.",
            "computational_cost_metric": "Cost of running models/simulations (wall-clock and compute resources), number of physical experiments, and total human-in-the-loop latency; no single metric is formalized but all are discussed as constraints.",
            "information_gain_metric": "Model-experiment mismatch reduction, expected improvement computed using model predictions combined with experimental uncertainty; information gain measured implicitly by how much the model posterior is updated by an experiment.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Co-navigation balances local experimental exploration (driven by immediate uncertainty) against exploitation guided by global theoretical maps; models can be used to prioritize experiments expected to improve objective or validate/expand theoretical coverage.",
            "diversity_mechanism": "Ability to query different models, datasets, and parameterizations increases hypothesis diversity; actively seeking model-discrepant regions promotes exploring diverse, potentially novel hypotheses.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Computational budget for modeling/simulations, experimental budget (time, measurements), and human latency.",
            "budget_constraint_handling": "Decisions weigh simulation runtime versus experimental cost; models are used selectively when their predictive value justifies compute cost; model uncertainty and experiment cost drive prioritization.",
            "breakthrough_discovery_metric": "Model-predicted high-value candidates that are experimentally validated; reduction in model uncertainty about key regions of parameter space; no single numeric metric provided.",
            "performance_metrics": null,
            "comparison_baseline": "Experiment-only AE without real-time modeling or traditional human-modeling-with-latency workflows.",
            "performance_vs_baseline": "Qualitative: co-navigation is expected to reduce human latency and the number of costly experiments needed to reach high-value discoveries; paper does not give numerical comparisons.",
            "efficiency_gain": "Potential speedup by running modeling in parallel to acquisition and reducing unnecessary experiments; no quantitative gains reported.",
            "tradeoff_analysis": "Emphasizes representational mismatch between model and experiment, the cost of model queries, and the need to align observables; stresses that model-assisted AE is advantageous when models are sufficiently predictive and matching to experimental observables is feasible.",
            "optimal_allocation_findings": "Authors recommend integrating theory/model queries into AE when models can provide reliable guidance or narrow candidate spaces, but caution that representational mismatches must be handled explicitly; co-navigation (map + local sensing) is proposed as a useful principle.",
            "uuid": "e2637.7",
            "source_info": {
                "paper_title": "Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.025714499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Notice: This manuscript has been authored by UT-Battelle, LLC, under Contract No. DEAC0500OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for the United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan).</p>
<h1>Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy</h1>
<p>Sergei V. Kalinin, ${ }^{1, a}$ Maxim A. Ziatdinov, ${ }^{1,2}$ Jacob Hinkle, ${ }^{2}$ Stephen Jesse, ${ }^{1}$ Ayana Ghosh, ${ }^{1,2}$<br>Kyle P. Kelley, ${ }^{1}$ Andrew R. Lupini, ${ }^{1}$ Bobby G. Sumpter, ${ }^{1}$ and Rama K. Vasudevan ${ }^{1}$<br>${ }^{1}$ Center for Nanophase Materials Sciences and ${ }^{2}$ Computational Sciences and Engineering Division, Oak Ridge National Laboratory, Oak Ridge, TN 37831</p>
<h4>Abstract</h4>
<p>Machine learning and artificial intelligence ( $\mathrm{ML} / \mathrm{AI}$ ) are rapidly becoming an indispensable part of physics research, with domain applications ranging from theory and materials prediction to high-throughput data analysis. In parallel, the recent successes in applying ML/AI methods for autonomous systems from robotics through self-driving cars to organic and inorganic synthesis are generating enthusiasm for the potential of these techniques to enable automated and autonomous experiment (AE) in imaging. Here, we aim to analyze the major pathways towards AE in imaging methods with sequential image formation mechanisms, focusing on scanning probe microscopy (SPM) and (scanning) transmission electron microscopy ((S)TEM). We argue that automated experiments should necessarily be discussed in a broader context of the general domain knowledge that both informs the experiment and is increased as the result of the experiment. As such, this analysis should explore the human and ML/AI roles prior to and during the experiment, and consider the latencies, biases, and knowledge priors of the decision-making process. Similarly, such discussion should include the limitations of the existing imaging systems, including intrinsic latencies, non-idealities and drifts comprising both correctable and stochastic components. We further pose that the role of the AE in microscopy is not the exclusion of human operators (as is the case for autonomous driving), but rather automation of routine operations such as microscope tuning, etc., prior to the experiment, and conversion of low latency decision making processes on the time scale spanning from image acquisition to human-level high-order experiment planning. Overall, we argue that ML/AI can dramatically alter the (S)TEM and SPM fields; however, this process is likely to be highly nontrivial, be initiated by combined human-ML workflows, and will bring new challenges both from the microscope and ML/AI sides. At the same time, these methods will enable fundamentally new opportunities and paradigms for scientific discovery and nanostructure fabrication.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>I. Introduction</h1>
<p>Imaging provides the basis for multiple areas of science exploring nature from astronomical to atomic scales. Electron microscopy, including (scanning) transmission electron microscopy ((S)TEM), and associated spectroscopy techniques, such as electron energy loss spectroscopy (EELS), are now key tools for probing atomic scale structures and functionalities in inorganic solids and hybrid materials, polymers, and biosystems. ${ }^{1-3}$ Scanning probe microscopy techniques ranging from Scanning Tunneling Microscopy ${ }^{4}$ to the broad spectrum of functional force-based Scanning Probe Microscopies ${ }^{5,6}$ enable imaging and spectroscopies from atomic to mesoscopic scales in a broad range of environments. ${ }^{5-8}$ Similarly, optical, X-Ray, and massspectrometry imaging methods are the mainstay of multiple areas in chemistry, astronomy, and medicine.</p>
<p>Beyond imaging and spectroscopy, both SPM and STEM can be used to manipulate matter on the nanometer scale. This includes examples such as electrochemical lithographies ${ }^{9}$ and ferroelectric domain writing in SPM, ${ }^{10,11}$ and atomic manipulation and assembly in STM. ${ }^{12-15}$ While relatively less recognized in the context of atomic-resolution electron beam techniques, the direct electron beam manipulation of structure ${ }^{16,17}$ and ferroelectric domains ${ }^{18,19}$ on the mesoscale and atomic level ${ }^{20,21}$ and even direct atomic manipulation ${ }^{22,23}$ and structure assembly ${ }^{24}$ have also been recently demonstrated.</p>
<p>However, these highly visible results of STEM and SPM research should be contrasted to the experimental research paradigm that has remained essentially unchanged over the decades. Traditional microscopy research workflows include long times spent optimizing microscope performance, with examples such as tuning the (S)TEM or tip conditioning in STM. In many cases, this process relies on the specific expertise of the operator and more rarely includes automated stages or allows for quantified performance. Notably, while it is common to refer to a "good" or "bad" tip in STM, rarely does this assessment come with the associated quantitative measures. Similarly, while the STEM community has developed measures of microscope resolution that describe the localization of an electron beam, this quantification does not usually extend to the details of the beam profile. Note that for (S)TEM the probe can in principle be optimized with a calibration sample, and steps such as sample preparation and tilting are performed with the knowledge that microscope parameters are independently configurable. On the contrary for STM and related techniques, probe and sample conditioning are performed jointly, typically resulting in considerably longer optimization cycles and slower throughput.</p>
<p>With the microscope performance optimized, the imaging process typically includes multiple imaging and spectroscopy measurements, aiming to identify the regions amenable for scanning, identifying the potential regions of interest based on observed structural images and spectral data, and detailed investigation of these selected regions targeting "publication quality" data. The process is typically repeated based on available time or human endurance, thus</p>
<p>concluding the experiment. In special cases, the microscope can be configured to stationary longterm operation, e.g., during current imaging tunneling spectroscopy (CITS) in STM. ${ }^{25,26}$</p>
<p>Once the data is acquired, the analysis stage is typically performed away from the microscope with the implicit understanding that the specific region of interest will be unavailable in the future (unless extensive effort to make the fiducial marks has been undertaken), and microscope parameters will be reproducible only within certain limits. Furthermore, the data analysis and interpretation, which typically involve multiple interactions with theorists and domain experts and in certain cases results in new quantitative insights and discoveries, can be associated with significant timescales. The experimental result informs the human operator and affects future sample selection and microscope operation (i.e., contributes to what is understood as domain experience), and is disseminated to a broad scientific community in the form of archival publications, codes, and data, and personal communications.</p>
<p>Previously, we have explored the role that imaging data can play in the broad context of scientific research and discovery, ${ }^{27}$ some of the specific issues associated with SPM, ${ }^{28}$ and associated opportunities in atomic fabrication. ${ }^{29-31}$ In half a decade since then, some of the enabling tools for data and code sharing and connections within scientific communities have emerged, including the code repositories on GitHub such as Pycroscopy ${ }^{32}$, AtomAI ${ }^{33}$, Nion Swift ${ }^{34}$, Py4DSTEM ${ }^{35}$, and data repositories such as Citrination ${ }^{36}$, NoMAD ${ }^{37}$, and Materials Innovation Network. ${ }^{38}$ Similarly, the development of cloud-based services such as Google Colab now allow for seamless integration of the scientific publication, code, and data sources as implemented in Jupyter papers, ${ }^{39}$ books and papers with code, etc. However, despite some early demonstrations of controlled and automated experiments in $\mathrm{SPM}^{40}$ and (S)TEM ${ }^{21}$ and a number of opinion pieces highlighting some of the general challenges and opportunities, ${ }^{30}$ the general analysis of this field balancing human and ML/AI decision making and instrumental considerations have been lacking. These opportunities are explored here.</p>
<h1>II. General considerations</h1>
<p>In the present context, an automated experiment is broadly understood as a computerdriven microscope operation, in which some or most decision-making functions are transferred from the human operator to the ML/AI control system. Part of this concept stems from the realization that many of the stages in the microscopy workflow are time consuming and monotonous, even though they might require a qualified operator. At the same time, in many cases the intrinsic latencies of the imaging system are much shorter than human decision making. For example, the image acquisition in many electron microscopy modalities can be well below a second, whereas human-based decision making is typically considerably slower than that. However, we note that the vast majority of microscope operations require a human operator that has both microscope-specific experience (how to operate it) and general broad context of physical,</p>
<p>chemical, or biological knowledge (what to look at and for). Therefore, the analysis has to be made in the broader context of human knowledge, including how the object of study and specific region are chosen, what drives the decision making during the experiment, and what analysis is performed on the imaging data. We further disambiguate the automated experiment where human-based decision making is part of the operation and the role of ML/AI is to assist with the specific tasks at low decision-making latencies, from fully autonomous experiment where the human role is setting the prior parameters and the ML/AI runs the full experimental process without human intervention. This in turn brings forth the question of the priors in the ML process including past knowledge and previous experiments, inductive biases of chosen algorithms, and potential interaction with the external sources of information, e.g., models and data repositories.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. The motivation for automated experiment. Shown are (a) surface topography and (b) piezoresponse image of a lead titanate film, exhibiting a rich range of domain structures. Of interest for this material is the relationship between the ferroelectric-ferroelastic domain structures and out-of-plane polarization switching, as probed by (c) PFM spectroscopy. Performing these experiments on an equally spaced grid of points is highly inefficient, since most of the time will be spent on the regions with trivial domain structure. Similarly, polarization switching in PFM or (d) electron beam crystallization of matter on the atomic level necessitates control of the beam or the probe path (adapted with permissions from [ ${ }^{20}$ ], (e,f) Example of the controlled scan path SPM realized by Ovchinnikov et al. (adapted with permissions from $\left[{ }^{40}\right]$.</p>
<p>Balancing this knowledge-based approach, we consider the practical aspects of instrumental operation, control, and performance. Unlike computational capabilities, the operation of mechanical systems is associated with intrinsic latencies and imperfections. Similarly, the signal</p>
<p>at the detectors is associated with certain noise structures that may impose fundamental limits on how fast measurements can be taken or what signal to noise ratios can be achieved. Hence, the second set of considerations can be based on the analysis of the modifications and latencies of the instrumental scanning protocols and expansion of the traditional workflows. For example, autonomous experiment (AE) workflow can be based on a sequence of the classical full (or partial) rectangular scans and preconfigured spectroscopic measurements, i.e. emulating the human operator. Alternatively, the scanning can be performed on different trajectories, including preconfigured non-rectangular scans and ultimately even free-form trajectories. Similarly, the spectroscopic measurements can be based on preconfigured waveforms, utilize a range of waveforms while selecting between them, or use the freeform spectroscopy. In these, the key aspect is timing. For many SPM systems the image acquisition time is 1-10 min, whereas (S)TEMs can generate datasets on the $\sim 1 \mathrm{~s}$ level and faster. Correspondingly, rapid feedbacks that control scan paths needs to operate well below these time scales, necessitating the development of edge computing capabilities.</p>
<p>Finally, an important issue is the stationarity of the systems. This includes the limitations of the experimental systems, including distortions and drift. Some of these can be corrected during the experiment (necessitating in turn additional stages in the experimental workflow), but some will be stochastic in nature. Similarly, in many cases microscopic studies will be performed on dynamic systems where materials structure and composition evolve as a function of temperature, gas atmosphere, or electrical stimulus. In these cases, AE design should consider the sample's changes. Perhaps even more important, in many cases the measurement itself can alter the sample structure. This can be a side effect of imaging, e.g., the beam damage in STEM or surface/tip degradation in SPM. For these cases, the experimental strategy generally attempts to minimize the damage. However, in many modalities change of the sample state is part of the measurement sequence, e.g., local hysteresis loop measurements in Piezoresponse Force Microscopy. Similarly, the sample changes can be harnessed to explore the broad chemical state of the system, performed control modification, and even accomplish atomic based fabrication. The goals and realizations of AE in this case has clearly different target then in exploratory microscopy.</p>
<p>Here, we organize the discussion following the operation workflows, and explore the ML options particularly paying attention to how human inputs and ML are balanced in terms of latencies and levels of decision making.</p>
<h1>III. Automating classical workflows: Sequential decision making</h1>
<p>Currently, most (S)TEM and SPM systems are based on rectangular scans for imaging and spectroscopic imaging, with the operator control being used to establish the regions of interest for scanning and adjust the field of view based on observations, as well as to select the regions for (typically longer-term and drift-prone) spectroscopic measurements. Typically, microscope</p>
<p>scanning and correction protocols responsible for drift minimization and ensuring linearity are already optimized as a part of commercial development. Hence, we focus the discussion on the AE in this regime as closest to the existing technological paradigm.</p>
<p>The general paradigm in this case is the discovery and assessment for perceived interest and value of certain regions, with subsequent actions based on this assessment. The actions can include scanning at higher sampling to focus on a specific feature of interest, initiation of imaging or spectroscopy in a more complex and time-consuming mode, or modification. The object can be found based on morphological features or spectral responses. Selection can be based either on prior knowledge, novelty of the observed behaviors, or some combination of the two. Hence, in discussing the ML strategies, we put emphasis on discussing the prior knowledge available to a human operator and priors and inductive biases available for the ML algorithms.</p>
<h1>IV.1. Prior knowledge and inductive biases</h1>
<p>As mentioned above, any experiment is performed in the context of prior knowledge about the material and phenomena. Even prior to imaging per se, these considerations inform the sample selection and preparation. It is safe to assume that given the limited availability and access of most modern microscopes and complexity of sample preparation, experiments formulated on the spur of the moment will be exceptional. More generally, sample selection is based on a specific domain interest, hypothesis testing, or need for quantitative measurements. Whereas in many (and in the authors' experience, in most) cases the experimental studies reveal unexpected and serendipitous information, solid hypothesis-driven science is the basis of modern microscopy. Here, we assume that the choice of objects of study stays outside of the discussion and we refer the reader to more general works on artificial general intelligence ${ }^{41}$ or $\mathrm{AI} / \mathrm{ML}$ in science in general. ${ }^{42}$</p>
<p>However, this prior knowledge is also a significant factor in the microscopy experiment. Whereas the intrinsic microscopist's skill set includes operation and familiarity with the specific features and peculiarities of a given microscope, the whole process equally requires the knowledge of the material systems, identification of likely objects of interest and rapid decision making which taken together generally constitutes the exploratory scientific research. Correspondingly, the key part of the AE is the clear separation of the prior knowledge available to ML system vs. that which is acquired during the experiment. Note that this knowledge can obviously include the weights of pretrained neural networks for image recognition and even the choice of network architecture. However, it also includes more subtle effects, for example choice of the acquisition functions in Bayesian optimization or reward functions in reinforcement learning (RL), balance of the exploration and exploitations strategies, inductive biases in both supervised and unsupervised learning methods, and choice of priors in Bayesian deep learning. Recognition of this prior knowledge is key for building successful machine learning based AE.</p>
<p>It is also important to mention that in principle both human and ML systems can seek additional knowledge during the experiment (active learning paradigm). For a human operator, it</p>
<p>is common to query colleagues or stored information sources to address specific problems emerging during experiment as a part of troubleshooting procedure. It is considerably rarer to explore sample-specific information, e.g., to look up reference sources, during the experiment. In general, the operation of modern microscopes necessitates a certain focus on data acquisition, making the external information queries unusual. Comparatively, ML/AI in principle can seek new information during the microscope operation and adjust the experimental progression based on it, as these tasks can be done in parallel computationally. This information can be derived from correlative or generative analysis of experimental data, queries of the external data repositories, or running parallel modelling. Below discuss some of these opportunities.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Example workflow for automated (but not autonomous) experiment. Here, the prior knowledge is incorporated in the form of pretrained network weights (for supervised learning), inductive biases (unsupervised learning), reward functions (reinforcement learning), and materials parameters. The human role is high-level slow decision making, e.g. choice of ML algorithm and selection of descriptors of interest. The AI role is fast low-level analysis that informs human decision making, and rapid analytics (reinforcement learning or Bayesian optimization) while operating the instrument. The STEM image is adapted from $\left[{ }^{43}\right]$</p>
<h1>IV.2. Machine learning in automated experiment</h1>
<p>Here we discuss the machine learning strategies in an automated experiment, namely the control and selection algorithm involved in the measurement workflow. One way to do so is based on the desired latency and the data on which the decision-making process is being made. We note that prior knowledge can be used explicitly or implicitly, e.g., in the form of the acquisition function selection in GP, combined functional form of data for multimodal and spectral images, choice of the reward design in RL, or implicit biases in self-supervised learning methods. Note</p>
<p>that this analysis closely mimics a question of whether unsupervised machine learning methods such as variational autoencoders (VAE) can disentangle the data representations in the absence of inductive biases. ${ }^{44-46}$ The special case here is the split of experiment into training and exploration stages, resembling classical NN networks or the Jeffreys prior ${ }^{47}$ in Bayesian analysis. ${ }^{48}$ In further discussion below, we aim to clearly disambiguate the role of prior knowledge and biases vs. AE operation.</p>
<h1>IV.2.1. Self-supervised experiments</h1>
<p>The simplest case for AE are methods that control the experimental pathway based solely on the sample-specific data obtained during experiment and information on the microscope behavior. These include Gaussian Process (GP) and Bayesian Optimization methods, as well as reinforcement learning (RL) type decision algorithms. While not relying on direct human input, these methods nonetheless include prior knowledge in the form of the inductive biases, selection of the acquisition functions, reward structure in RL methods, or target performance. Below, we discuss these paradigms in detail.</p>
<h2>IV.2.1.a Gaussian Processes and Bayesian optimization</h2>
<p>One of the key groups of algorithms that enable automated experimentation are the Gaussian Process (GP) regression ${ }^{49-51}$ based Bayesian Optimization (BO). In general, GP refers to an approach for reconstructing a random function $f(\mathbf{x})$ over a certain parameter space $\mathbf{x}$ given the observations $y_{i}$ at specific parameter values $\mathbf{x}<em _mathrm_i="\mathrm{i">{\mathrm{i}}$. It is assumed that the observations, $y$, represent noisy measurements of the function, $y=f(\mathbf{x})+\epsilon$, where $\epsilon$ is Gaussian noise. The values of the random function across its domain are related through a covariance function called a "kernel". The functional form of the kernel is chosen prior to the experiment, and kernel (hyper-) parameters are determined self-consistently from the observations $\left(y</em>$}}, \mathbf{x}_{\mathrm{i}}\right)$. The choice of the functional form of the kernel defines the physics of the explored phenomena. For example, the commonly used localized RBF and Matern kernels ${ }^{52}$ can represent local correlations in the system, whereas spectral mixture kernels are well suited to systems with periodic structures. ${ }^{53}$ In addition, the deep kernel learning method, which combines spectral mixture kernels and deep feedforward convolutional neural networks, can discover quasi-periodic patterns from sparse data and account for the nonstationary and hierarchical structure of the data. ${ }^{54}$ When the domain is a regular image grid, GPR has a particularly convenient form, leading to efficient computation which has been used for image denoising and is applicable in microscopy contexts. ${ }^{55-57</p>
<p>The unique aspect of GP methods compared to other interpolation approaches is that GP analysis also provides quantified uncertainty, i.e. a function $\sigma(\mathbf{x})$ determined over the same parameter space $\mathbf{x}$ that defines the standard deviation of the expected values of $f(\mathbf{x})$. This naturally</p>
<p>allows extending the GP approach towards automated experiments. Here, after the $n$ initial measurements $\left(y_{1}, y_{2}, \ldots, y_{n}\right)$ at locations $\left(\mathbf{x}<em 2="2">{1}, \mathbf{x}</em>)$ is can be multiplied by a mask that is constant within the parameter space and rapidly drops to zero at the borders.}, \ldots, \mathbf{x}_{n}\right)$ the function $f(\mathbf{x})$ and its uncertainty $\sigma(\mathbf{x})$ are reconstructed, and the location with maximal uncertainty, (i.e. a maximum of $\sigma(\mathbf{x})$ ) is chosen for a subsequent measurement. In this regime, AE minimizes the uncertainty over the predictions, corresponding to a purely exploratory strategy. In this exploratory strategy, the algorithm tends to focus on the edges of parameter spaces (e.g., scanning window boundaries) during the initial exploration steps, reflecting the large degree of "unknown" territory outside the explored interval. To avoid this tendency, the $\sigma(\mathbf{x</p>
<p>Note that the GP variance maximization approach to AE mentioned above is essentially a form of adaptive optimal design of experiments (ADoE). Although the form of the uncertainty function in GPR, $\sigma(\mathbf{x})$, is independent of observed values $y$, it does depend on kernel hyperparameters. The kernel hyperparameters is chosen by maximum likelihood estimation, leading to an indirect dependence of chosen sample positions on observations $y$. Therefore, the adaptivity of this approach depends crucially upon repeated optimization of kernel hyperparameters, and upon flexibility in the class of kernel functions used. Note that the ADoE problem has been studied for GPR in other contexts, leading to methods that attempt to choose sample positions that are maximally informative to hyperparameter inference. ${ }^{58}$</p>
<p>In the BO methods, the exploration of the parameter space is guided by an acquisition function $a(\mu(\mathbf{x}), \sigma(\mathbf{x}))$ balancing the predicted functionality $\mu(\mathbf{x})$ and the uncertainty $\sigma(\mathbf{x})$. This practically means that regions of high uncertainty may not be explored if the optimization deems there to be little chance that probing such areas will result in a high target function value. Correspondingly, the definition of the acquisition function by a human operator captures the a priori knowledge about the system and allows for balancing between the pure exploration and exploitation strategies. The simplest acquisition function is a linear combination of the predicted functionality and uncertainty, where the choice of the coefficients in front of $\mu(\mathbf{x})$ and $\sigma(\mathbf{x})$ controls the balance between exploration and exploitation. Other choices for acquisition function include the probability of improvement, which indicates the likelihood of improvement over the current best measurement (e.g., maximum value of property of interest), and the size of the expected improvement (EI). Finally, one can define a custom acquisition function based on the specific research goals (if we know what particular behavior we want to target ${ }^{59}$ ) and/or allow for switching between different acquisition functions during the measurements.</p>
<p>While the BO methods have been well-known for decades, over the last 2-3 years they have received renewed attention in the context of automated experiments as a universal method to navigate low-dimensional parameter spaces. The broad range of BO optimization has emerged in the context of automated synthesis, where the parameter space is the composition space of the</p>
<p>multicomponent phase diagram. ${ }^{60-64}$ Similarly, BO has started to be extensively used in automated experiment in focused X-ray imaging and complex spectroscopies. ${ }^{65-67}$</p>
<p>The BO methods build on GPR foundations and can benefit from many advances therein; particularly the development of new kernel classes and sampling schemes. Flexible kernels such as DKL have been applied to $\mathrm{BO}^{68}$ and kernel hyperparameter optimization is already part of some BO pipelines. However, future methods that integrate of ADoE objectives into acquisition functions in order to encourage efficient hyperparameter estimation during the BO process, may be successful in this context if they lead to increased adaptivity.</p>
<p>Until now, the BO methods used non-restrictive priors, assuming zero knowledge of the system. Correspondingly, when prior physical knowledge is available, the GP-based surrogate model for BO can be replaced with a probabilistic model of the system to make informed decisions about which combination of parameters to evaluate. A simple example in the case of STM would be accounting for a sample/tip electronic band structure (e.g., a presence of electronic band gap in a certain energy interval) when performing a BO search for optimal imaging parameters.</p>
<p>We further note that traditionally the BO methods are based on the definition of the acquisition function only, the maxima of which define the target parameter set for exploration. In most practical cases (relative short-range correlations in the image plane), the acquisition function tends to be shallow with multiple degenerate maxima. Correspondingly, development of the AE strategies should consider not only the acquisition function behavior, but also strategies for the sequence of exploration of the close maxima. Here, we refer to this as a pathfinder function, which balances expected microscope performance to yield the optimal sequence of exploration of close acquisition function maxima. For example, the pathfinder function can be based on proximity, impose a non-crisscrossing probe path, or favor probe motion in a specific direction.</p>
<p>Until now, we assumed that the input variables of the objective function in BO are independent, i.e. the causal structure existing among the input variables was not accounted for. The disregard for the causal relationships between the input variables may lead to suboptimal solutions, particularly for dynamically changing systems where a measurement is also an intervention (e.g. the electron beam in STEM is used to record an image but may also modify a material system). Recently, Aglietti et al. ${ }^{69}$ introduced a new concept of causal BO that integrates observational and interventional data via a causal prior distribution and replaces a classical EI acquisition function with causal EI, which explores the possible interventions. As a result, in addition to the exploration-exploitation trade-off, the causal BO also balances the observationintervention trade-off. However, we note that while the causal BO allows for a potentially significant decrease of the optimization cost/time, it requires a knowledge of a causal graph structure, which may not be always known in the actual experiments.</p>
<h1>IV.2.1.b. Predictability based learning</h1>
<p>The classical BO strategies rely on the uncertainties in the function interpolation as the basis of the acquisition function. However, often the observations yield structural or spectroscopic data sets with partially understood physical behaviors. This provides strong inductive biases that can be used for exploration. For example, analysis of structural data sets allows extracting structure-based descriptors such as polarization and unit cell volumes. The GP, ensembles of deep convolutional neural networks (DCNNs), or Bayesian Neural Networks can then be used to establish the correlative relationship between these physical descriptors, with the choice of feature and target descriptors being dictated by the inductive bias of the operator. With these in hand, the uncertainty in prediction can be used as a basis for exploratory studies.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Predictability based exploration. Here, the uncertainty in predictions can be derived based on physical backgrounds, e.g., from the correlative relationship between locally probed physical descriptors. Schematically shown are (a) domain wall in the ferroelectric in with and without the defect center, which can affect the relationship between observed polarization value and molar volume. (b) STEM image, (c) ground truth polarization, (d) polarization predicted from structural descriptors, and (e) prediction uncertainty. The uncertainty map clearly delineates regions for more detailed studies associated with "unusual" behaviors. Adapted with permissions from $\left[{ }^{70}\right]$.</p>
<p>Similar approaches can be used in cases where both structural and spectral observations are available. Typically, the structural information is available for images at a high sampling density, as exemplified by the STEM or STM images, whereas hyperspectral data is available over</p>
<p>the same region but with lower sampling, e.g., EELS or 4D STEM in electron microscopy and CITS in tunneling spectroscopy. During the post-processing, these problems often give raise to the pan-sharpening and image fusion type problems, aiming to reconstruct the hyperspectral data at higher sampling. ${ }^{71-73}$ Similarly, the encoder-decoder networks with the structural images as features and spectra as targets (im2spec) or vice versa (spec2im) can be used to establish the correlative relationship between the structural and spectral features. ${ }^{74}$ The uncertainties in such predictions can also be used as a target for the automated experiment.</p>
<p>It is important to note that for optimal data representation, im2spec and spec2im networks should ideally be invariant with respect to the symmetries present during the imaging, e.g. allow for identification of rotated version of the same object, some translational invariances, scale invariances and so on. The approaches for introducing these via network construction or data augmentation have appeared only recently and will likely be an active area of research. ${ }^{75-78}$</p>
<p>Similarly, in the context of AE, it is important to compare cases where the deep learning networks are trained prior to the experiment, i.e., rely on past knowledge, or during the experiment. In case of the prior training, we note that in many cases a small deviation in the imaging parameters can result in out-of-distribution drift for the networks, degrading their performance. This problem can be addressed either via incorporation of the ensemble learning-iterative training (ELIT) methods ${ }^{43}$ that allow to select the network suited for experiment from a distribution, transfer and active learning methods. This out-of-distribution drift can further be minimized by the transition from image-specific to materials specific descriptors. Alternatively, the networks can be trained using the data obtained at the initial stages of the experiment, and the thus trained network can be used to explore the uncertainties over the full AE sequence.</p>
<p>Similarly, for GP-based uncertainty evaluation, the disadvantage of the classical GP-BO approach is that that it does not "memorize" patterns/trends in the data and requires retraining from scratch after each new measurement, which may prevent its implementation in real time. Potential workarounds include the extension of the GP-BO approach to include the recently demonstrated meta-learning ${ }^{79}$ and deep kernel transfer ${ }^{80}$ approaches for GP.</p>
<h1>IV.2.1.c. Reinforcement learning</h1>
<p>One of the key aspects to decision making in automated experiment is that for many cases, they require sequential decisions to be made without access to the result until many time steps after the initial decisions are made. In the standard Bayesian optimization routine, a new selection of point(s) results in new measurements, and this can then be used to retrain the surrogate model on which the optimization can be reformulated. This is in stark contrast to, for example, the goal of assembling atoms on surfaces to spell out the letters "IBM", which consists of many dozens, perhaps hundreds, of actions to pick up atoms and place them at specific locations. ${ }^{12}$ The fact that the environment is stochastic (atoms can move, and the conditions of the experiment can change</p>
<p>somewhat) can make automation in such environments difficult. These problems have recently been tackled by the advance of deep reinforcement learning (DRL)..$^{81,82}$</p>
<p>In general, RL is a branch of ML that deals with agents performing actions in an environment with a goal to maximize a cumulative (discounted) reward. The agents act in the environment based on a given policy, usually parameterized by a neural network, that dictates which action to take given a particular state. ${ }^{82}$ Note that "RL" refers to both the problem statement, as well as a class of algorithms designed to solve it.</p>
<p>More recently, the use of deep neural networks within traditional RL methods have attained high-profile success, with celebrated examples including the defeat of a Go champion by a DRL system developed by Deep Mind ${ }^{83}$, as well as super-human performance in Atari games ${ }^{84}$ as well as the "Defense of the Ancients" strategy game, the latter by OpenAI. ${ }^{85}$ In many of these cases, the strategies learned by the agents are remarkably different from those of top players. ${ }^{86}$ They have also shown steady progression in the area of robotics, for example with learning dexterous hand manipulations. ${ }^{87}$</p>
<p>Broadly two classes of RL exist, termed 'model-based' and 'model-free' RL. In a typical model-based RL setup, the agent interacts with the environment and learns a model of the dynamics (state transitions). Because repeated interactions with the environment are usually expensive, the agent performs a limited number of actions on the real environment, before updating the internal dynamics model, and then interacting again with the internal model to update the policy. This can be thought of as leveraging the data acquired in a more efficient manner, but one downside to model-based RL is that very often the policies learned from this process are optimal for the learned model, but not necessarily for the environment. Many of the more recent successes in RL utilize so-called model-free methods, which do not attempt to directly model the environment.</p>
<p>One of the key constructs in RL is the idea of a state value. ${ }^{82}$ An RL agent following a policy may visit certain states and find that being in those particular states tends towards accumulating more rewards when following the policy through to the termination of the environment (or to a very long time). These states would be assigned a higher state value in the value function, $V(s)$, which is defined recursively as the immediate reward obtained at that state, plus the (discounted) reward at the next state, s' when following the current policy, i.e. $V(s)=r+$ $\gamma V\left(s^{\prime}\right)$, where $\gamma$ is a discount factor that is between 0 and 1 , and accounts for uncertainty in the actual state value. By visiting more states, the value function can be updated according to the actual returns (total accumulated rewards) of the agent. A similar construct is the use of the so-called action-value function or Q-function. When dealing with discrete action sets, it is often easier to work with the Q-function directly, which is defined $Q(s, a)=r+\max _{, s}\left(\gamma Q\left(s^{\prime}, a\right)\right)$, i.e. it shows the expected value of the state when taking a certain action (a). When this Q function is learned (' Q learning') through the agent interacting with the environment, the Q values can be simply</p>
<p>computed and the action that maximizes the Q value can be chosen at each state, thus defining the final policy. This approach was successfully applied to achieve super-human performance in Atari games, ${ }^{84}$ for instance, and Q-learning has the advantage of being relatively data-efficient compared to other RL methods.</p>
<p>The alternative formulation for RL is that of direct policy parametrization. Q-learning is difficult to apply in practice for continuous action tasks, such as for example temperature control during a material synthesis problem, or electron beam dwell times. In principle these can be discretized for the sake of utility, but this may not be suitable in practice, essentially leading to very large action spaces. Stochastic policies can be represented directly by neural networks, and the policy gradient theorem can be utilized to update the parameters of the policy in the direction of maximizing the future discounted reward, as shown in Eq. (1). The policy defines a distribution of actions over states, and this distribution is usually parameterized, e.g. by a multivariate Gaussian distribution. Here, $J$ is the objective function, which is to maximize the cumulative discounted rewards $R$, which is a function of the trajectory $\tau$ through the system. These trajectories naturally come from following the policy $\pi_{\theta}$, i.e. they are sampled from the policy. Equation (1) indicates that the gradient of the objective function is obtained by collecting the returns from following trajectories and multiplying by the log probability of the actions that were taken. This (negative) 'loss' can then be back-propagated through the neural network in the standard manner to update the policy parameters, to make actions that increase the cumulative discounted rewards $R$ more likely. This defines the so-called 'Vanilla Policy Gradient' algorithm. The difficulty of this method is the high variance in $R$, which can be substantially reduced by subtracting an appropriate baseline from R that does not impact the gradient calculation. This baseline is often chosen to be the value function, $V(s)$, and incorporating this baseline essentially signifies the advantage that taking an action has over another, given the value of the present state, and is thus sometimes termed the 'advantage' function A. RL methods that use this function are often termed 'actor-critic' RL algorithms because they utilize an actor, that is the policy, and a 'critic', that determines the state value. ${ }^{88}$ Both are updated during the course of the training. These methods are generally better for convergence than Q learning, but are data-intensive, and often only converge to local optima. ${ }^{82}$</p>
<p>$$
\nabla_{\theta} J\left(\pi_{\theta}\right)=\underset{\tau-\pi_{\theta}}{\mathbb{E}}\left[\sum_{\ell=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{\ell} \mid s_{\ell}\right) R(\tau)\right]
$$</p>
<p>In most RL problems, the rewards given by the environment to the agent are remarkably sparse: in chess, the result of the game is only known at the end; in materials synthesis, roughness of the film can fluctuate considerably depending on the growth method, and final film properties will not be known until the synthesis is completed. The sparseness of the reward signal makes RL extremely data intensive, as the agent needs to encounter at least a few trajectories where some rewards are obtained. At the same time, insufficient exploration will lead to sub-optimal policies.</p>
<p>Multiple methods have been developed to attempt to better explore environments and lead to more robust policies. For example, the objective function can be modified to include an entropy term ${ }^{89}$, so that it is not only rewards that are sought after, but rather, state exploration. This has been explored in multiple recent works (see Ref. $\left.{ }^{90}\right]$ and refs therein) and can be assisted by parallelizing the exploration via use of multiple agents. ${ }^{91}$</p>
<p>As an alternative formulation, instead of consistently relying on rewards from the environment, the agent can act based on maximizing curiosity or empowerment. Curiosity can be defined (in one formulation) as the discrepancy between what an agent's internal model believes will occur to the state when an action is taken, vs. what actually occurs, in some featurized state space that is relevant for action selection. ${ }^{92}$ It has been shown that utilizing curiosity driven RL can greatly accelerate learning on typical RL environments, because it leads to better exploration. An alternative signal is the empowerment signal ${ }^{93}$, which can be formulated as the KullbackLeibler (KL) divergence between the predicted successor state when a particular action is chosen, and the state when the actions are marginalized out. This effectively leads to actions which have the greatest effect on the environment to be rewarded. In both cases, these can (and generally are) added to the extrinsic reward signal coming from the environment itself. These methods promise to be extremely useful for AE because, in this case, it is not often possible to adequately construct reward signals if the goal is simple 'sample exploration'. Indeed, what we need instead is to find new states that are not predicted by the RL agent's learned model of environment dynamics. Moreover, it is expected that the learned policies can then be inspected to obtain an understanding about the physics of the underlying process, i.e., the specific features of the states which dictate the action, and the distribution of the actions themselves conditioned on different states.</p>
<p>When available, the most critical component of reinforcement learning is so-called 'reward shaping', i.e., the specific design of the reward function. Although the reward function can be sparse, discontinuous, and non-differentiable, poorly designed reward functions can lead to 'hacking' and trivial or useless policies. For example, imagine a task where the goal is to assemble a collection of dopants closer together using scanning probe manipulation for adatoms on a surface, or electron beam manipulation for dopants inside a material. If the reward is simply the distance between the dopants, then policies that destroy the lattice to make this possible might be expected and discovered, even though that is not desired.</p>
<h1>IV.2.2. Use of prior knowledge in AE</h1>
<p>Alternatively, the AE workflow can utilize past knowledge, to some extent emulating the experience of a human operator. In this case, the operator defines both the target of the AE, and the prior knowledge and inductive biases available to the corresponding ML algorithm. This knowledge can be included in the form of trained classical- and DCNNs for feature selection for image-based feedback that define objects of interest and rely on the simplest form of prior</p>
<p>experience in the form of correlative relationships encoded in the networks. Alternatively, this can be Bayesian priors on images and materials responses, etc. In general, this suggests that AE will be built based on complex workflows utilizing multiple levels of decision making.</p>
<h1>IV.2.2.a. General workflow</h1>
<p>A simple example of such workflows can be based on the combination of feature finding and Bayesian optimization as illustrated in Figure 4. Here, a pre-trained DCNN is used to identify the positions of the atomic units in the system, providing the coordinates and identities. A suitable encoder network can be used to transition from atomic coordinates to the latent variables describing local structures. Alternatively, graph analysis can be used to identify local molecular fragments. The atomic identities, latent variables, or a priori selected molecular fragments (e.g., isolated 5-7 rings in graphene) can be chosen as targets for specific intervention.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. General BO workflow for structural and functional discovery in STEM or SPM. Structural or spectral data is reduced to a single or several scalar descriptors that are used to form the acquisition function in the BO process. Note that in many cases the classical acquisition function will be highly degenerate, and hence a pathfinder function that determines optimal measurement sequence on specific imaging system should be introduced.</p>
<p>Similar approaches can be used for more complex mesoscopic imaging data, e.g., polarization switching in piezoresponse force microscopy or many other methods. The network trains on the stack of images collected when applying a slow potential wave to get information on domain dynamics. These observations of domain dynamics allow establishing the latent variables</p>
<p>(LV) that describe these images. Experimental strategies can then include trying to intervene on these latent variables (e.g. apply positive biases where LV is high, and negative where it is low), and explore effects of different interventions.</p>
<p>A simple example of such an approach is the FerroBOT concept in Piezoresponse Force Microscopy. ${ }^{94}$ FerroBOT is based on line by line detection, where the changes in piezoresponse force microscopy phase when the tip crosses a ferroelectric domain wall are used to trigger the specific (predefined) voltage sequence that result in wall motion. This approach allows the systematic exploration of domain wall dynamics and also creating non-standard domain configurations. Further development of this concept includes development of simple image-based feedback using predefined features of interest.</p>
<p>Note that a key aspect of AE is the transition from the exploratory AE to active intervention, aiming to create target structures such as atomic assemblies or ferroelectric domain configurations via controlled modification of the material. This latter task is a considerably more complex problem, since the ML algorithm has to learn the (potentially stochastic) cause and effect relationships that control the beam and tip effects. This is a typical reinforcement learning problem as described above.</p>
<h1>IV.2.2.b. Dataset shift and out of distribution data</h1>
<p>Utilization of deep learning strategies have become common for categorizing natural images into different classes (cats, dogs, automobiles, etc.) or for performing a semantic segmentation of the images, that is, categorizing every pixel in an image into a particular class. Multiple public datasets, such as CIFAR and ImageNet, exist for benchmarking the newly developed deep learning networks. One of the characteristic aspects of these datasets is that each class usually contains thousands of examples with a high variability between them. By contrast, the problem of atom/particle/defect identification in SPM and STEM typically requires finding nearly identical objects while the image acquisition parameters can vary significantly between different experiments or between simulations and experiment. As a result, a neural network trained on a broad distribution of experimental parameters may not be able to recognize subtle (but physically important) differences between atomic features within a single dataset. Hence, the problem of the dataset shifts ${ }^{95}$ and out-of-distribution samples becomes important.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Illustration of the ELIT framework that starts with a neural network or an ensemble of neural networks trained on simulated data. This network(s) is usually "broad" enough to account for a variety of experimental conditions (noise, scan size and resolution, orientation, etc.) and atomic structures but may fail to detect a smaller variation in particular experimental settings or (mis-)identify physically impossible classes. Therefore, this network is used as a starting point (baseline) for iterative retraining procedure that allows focusing the network(s) attention on the classes present in the system, which could dramatically improve the classification accuracy.</p>
<p>One possible solution is utilization of ensemble learning and iterative training (ELIT) approach ${ }^{43}$ that starts with an ensemble of models trained on simulated data (this allows avoiding manual labeling of experimental images) to identify the features present in a specific system with predictive uncertainties and then iteratively retrains the ensemble using the identified highconfidence features. An illustration of this approach is shown in Figure 5 using experimental STEM data on graphene (right panel) as an example. Here, the artifact-free high-quality prediction</p>
<p>as well as associated uncertainties for each pixel (defined as dispersion in predictions of the ensemble models for each pixel; not shown) were obtained after just 3 ELIT iterations.</p>
<h1>IV.2.2.c. Inverse reinforcement learning and learning from human expert</h1>
<p>One of the drawbacks of RL is the need to carefully tailor the reward function, as described in the previous section. For some problems, this is straightforward, but often reward functions will contain multiple components, necessitating substantial tuning, which is undesirable for a method that already incurs large and oftentimes prohibitive computational cost. Additionally, in many cases it may be difficult or perhaps even impossible to write an appropriate reward function. For example, consider the task of cleaning up a messy room. It is very easy to demonstrate how to do it, but much harder to numerically define such tasks with easy-to-understand parameters.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. (a) General setup of reinforcement learning. In RL, an agent takes actions within an environment, usually based on partial information. The environment returns rewards, usually in some sparse fashion. The sparsity of the reward is at the heart of most of RL's sample inefficiency. One method to improve sample efficiency is to utilize intrinsic curiosity as an additional reward. (b) Curiosity-based RL. Here, the agent observes a state $s_{t}$ and samples an action from the policy $\pi$. This results in a transition to the next state, $s_{t+1}$. Meanwhile, the agent will receive any reward from the environment $\left(r^{e}\right)$ and an intrinsic curiosity reward, $r^{i}$. The latter are given by the curiosity model, which is shown in (c). In the curiosity model, two different models, parameterized by neural networks, are trained. One predicts the action based on two sequential states after featurization, and the other is an inverse that predicts the (featurized) next state. The difference between the featurized state prediction at time $t+1$ and the (featurized) observed state, $\mathrm{s}_{\mathrm{t}+1}$, is used as the intrinsic reward signal.</p>
<p>To tackle this problem, inverse reinforcement learning (IRL) is formulated to infer reward functions from observations of agents performing tasks in an environment. ${ }^{96}$ In our case, this may be, for instance, a human expert demonstrating how to adjust multiple parameters on a microscope. The goal of IRL is to model the reward function from such demonstrations, or at least collect</p>
<p>trajectory references, which can then be used to apply standard RL to leverage the expert demonstrations to learn a policy. As an example, IRL was recently used to model the reward function (using a deep neural network), trained on expert demonstrations playing Atari games. ${ }^{97}$ These rewards were then used to train a deep Q network (DQN) agent that achieved superhuman performance without using game rewards on two specific games. It should be noted that this is an improvement over mere imitation; the policy can be better than those that were demonstrated by a human expert. Although RL and IRL in particular are in relative infancy within electron microscopy, they offer hope of automating tricky, multiple-optimum tasks that can be difficult to specify directly.</p>
<h1>IV.2.3. Active learning</h1>
<p>The discussion above would be incomplete without explicitly mentioning the possibility for the AE algorithm to query additional sources of information during the experiment, whether a theoretical model, published data, or even the human operator. As an example of the former, a ML method can be augmented by real-time modeling, including models based on the prior measurements (e.g., in model-based reinforcement learning), and models assisted by theory. In this case, the modeling that normally follows the experimental measurements becomes a part of the AE. This is a unique advantage of the AE, since human-based modeling is often associated with extraordinary large latencies. However, in this case, the key issue becomes the extent to which theory can reproduce the observed experimental situation. In other words, while experiment yields a certain representation of physical reality, so does a theoretical model. However, modeling results are also associated with certain representational biases and establishing matching between model predictions and experimental observables is a problem of its own (see Section VIII.4).</p>
<p>As a relevant parallel, we bring up autonomous driving. Here, the experimental effort is equivalent to the operation of a car based on immediate local information and the state of the road; yet the general route or presence of (known) alternative roads are not necessarily apparent to the driver. Comparatively, theory (in this case a map) provides the overall topology and connectivity of the travel space; yet theoretical modeling alone does not provide sufficiently exact information to stay on the road or to avoid other vehicles and obstacles. Hence, the co-navigation approach in this case maintains the car on the road while updating the map (e.g. discovering new roads and positioning known ones precisely) during driving.</p>
<p>Similarly, ML can use other sources of information, e.g., search for specific materials parameters, data mining, etc. An example of this process can be the search for a solid solution that has a particular dielectric value. In this case, the experiments provide information on the precise compositions of a phase diagram and the theory evaluates those for properties to feedback to the AE to ultimately find the best materials. Another example can be the design of the best single atom catalyst to drive an oxidation-dehydrogenation reaction, where theory would couple to AE which</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{a}$ sergei2@ornl.gov&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
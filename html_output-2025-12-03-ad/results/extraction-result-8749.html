<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8749 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8749</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8749</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-275458736</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.05727v2.pdf" target="_blank">Self-Evolving Critique Abilities in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Despite their remarkable performance, Large Language Models (LLMs) face a critical challenge: providing feedback for tasks where human evaluation is difficult or where LLMs potentially outperform humans. In such scenarios, leveraging the critique ability of LLMs themselves - identifying and correcting flaws - shows considerable promise. This paper explores enhancing critique abilities of LLMs, noting that current approaches rely on human annotations or more powerful models, leaving the challenge of improving critique abilities without external supervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework that trains LLMs with self-generated data to evolve their critique abilities. To address the low quality of naively generated data, we propose a contrastive-critic approach that uses reference solutions during data synthesis to enhance the model's understanding of key concepts, and incorporates a self-validation scheme to ensure data quality. The final trained model operates without any reference solutions at inference time. Implemented with Qwen2.5-72B-Instruct, a leading LLM, SCRIT demonstrates consistent improvements across a wide range of benchmarks spanning both mathematical and scientific reasoning: achieving a 10.0\% relative gain in critique-correction accuracy and a 19.0\% relative improvement in error identification F1-score. Our analysis reveals that SCRIT's performance scales positively with data and model size and enables continuous improvement through multi-round iterations.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8749.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8749.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCRIT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-evolving CRITic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-training framework that enables an LLM to iteratively develop critique abilities by synthesizing contrastive critique data using reference solutions, self-validating corrections, and fine-tuning on validated critiques without external supervision at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2.5-72B-Instruct, a 72B-parameter instruction-tuned LLM used as the base and fine-tuning target for SCRIT; previously extensively pre-trained and post-trained per Qwen-Team (2024).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-evolving critique (SCRIT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate high-quality critique-correction pairs by (1) contrastive critic generation conditioned on reference (correct) solutions, (2) self-validation that verifies the proposed correction yields a fully correct solution, and (3) self-training the model on validated (problem, student-solution) → (stepwise critique, correctness label, correction) pairs; reference solutions are used only during data synthesis and not at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Critic & Correct across mathematical and scientific reasoning benchmarks (RealCritic) and error-identification benchmarks (PRM800K, ProcessBench)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation protocols that test step-wise critique, identification of the first erroneous step, and whether the critique leads to a correct corrected solution across datasets such as GSM8K, MATH, OlympiadBench, ARC-C, GPQA, MMLU-STEM, PRM800K, and ProcessBench.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Average critique-correction accuracy (CC-Acc) improved to 50.0% on deliberately incorrect solutions (from 39.7%), 62.1% on balanced solutions (from 57.7%), and 62.9% on base-model self-solutions (from 61.7%); overall reported 10.0% relative gain in CC-Acc. Error-identification F1 (EI-F1) improved from 37.8% to 45.0% (19.0% relative improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline Qwen2.5-72B-Instruct (no SCRIT self-evolution): 39.7% CC-Acc on deliberately incorrect solutions, 57.7% on balanced, 61.7% on base-model solutions; EI-F1 = 37.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted contrastive critique generation using reference solutions (prompt engineering), automatic self-validation via a direct critic checking that the generated correction is fully correct, and supervised fine-tuning (cross-entropy) on validated critique-correction pairs (no external memory or modules at inference).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative gains reported across benchmarks (see performance_with_reflection). Additional evidence: gains scale with more training data and larger model size; multi-round iterations (up to 3 rounds) produced consistent further improvements; ablations (Table 4) show removal of self-validation reduces metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires verifiable/reference solutions during data synthesis (not at inference); validation rates drop substantially with problem difficulty (e.g., 91.8% on GSM8K to 27.1% on Olympiad problems); residual gap to stronger external critic (o1-mini). Self-critic without validation can produce low-quality critiques that approve intermediate steps then reject final answer; framework is sensitive to domain diversity and problem complexity selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to Direct Critic and Bug-Injection Critic, SCRIT (with contrastive critic + self-validation) performed substantially better; approaches relying on external stronger models (e.g., o1-mini) still outperform SCRIT in some settings, but SCRIT approaches their level without external supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Removing self-validation: CC-Acc -0.8, EI-F1 -3.0. Limiting domains to GSM8K+MATH: CC-Acc -1.4, EI-F1 -1.4. Training with more complex problems first hurts EI-F1; using a higher bad:good ratio (0.25:0.75) yields better EI-F1. Results summarized in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Critique Abilities in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8749.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8749.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contrastive Critic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Critic (reference-conditioned critique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A critique-generation technique that conditions the critic on a correct reference solution prior to analyzing a target (student) solution, which mitigates blind approval and improves error identification and corrective guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-72B-Instruct (used to generate critiques during SCRIT data synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same 72B Qwen2.5 instruction-tuned model used with prompts that provide a reference solution and ask for staged analysis (reference analysis, step-wise critique, conclusion, correction).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Contrastive critique (reference-conditioned self-critique)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Provide a correct reference solution first (Reference Analysis), then perform step-wise critique of the target solution using the derived conceptual understanding, emit a conclusion (correctness + first error step), and propose a correction; used to synthesize training data for self-training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthesizing critique-correction training data for SCRIT; evaluated via Critic & Correct and error-identification benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generates labeled critiques and corrections for problems where at least one correct and one incorrect solution exist, to be validated and used to fine-tune the critic model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>When used for data synthesis in SCRIT, contributed to overall improvements: CC-Acc up to 58.3% (data/model scaling context) and EI-F1 up to 45.1% for the largest model; contrasted with Direct Critic and Bug-Injection, Contrastive Critic shows superior data quality and downstream gains (quantified within SCRIT results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct Critic (no reference) exhibits 'rubber-stamping' behavior and produces lower-quality critiques; when contrastive conditioning is removed, downstream performance degrades (qualitative and via controlled comparisons reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering to include a correct reference solution and a 4-stage self-critic prompt (Reference Analysis, Step-wise Critique, Conclusion, Correction); no extra modules beyond prompting and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical comparisons (controlled experiments) showed contrastive critic avoids blind approvals and yields higher validation and better training data resulting in improved final critic performance; Figure 1 and Figure 5 illustrate qualitative differences and Figure 3 shows contrastive critic outperforms baselines in data-scaling plots.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires problems that have at least one correct reference solution available during data synthesis (filtering reduces dataset to problems with both correct and incorrect solutions). Effectiveness decreases on very difficult problems where validation rates are low.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms Direct Critic (which tends to rubber-stamp) and Bug-Injection Critic (which injects simplistic, repetitive bugs) in both qualitative case studies and quantitative downstream metrics within SCRIT.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Critique Abilities in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8749.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8749.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Validation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Validation (correction validation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-generation filtering step that accepts only those self-generated critiques whose proposed corrections are validated by a direct critic as fully correct solutions, improving training data quality for self-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-72B-Instruct (used as validator in v_theta and g_l_theta functions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used in validation mode (direct critic prompt template) to evaluate whether a generated correction yields (y=1, j=-1), i.e., a fully correct solution with no errors.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-validation of generated corrections</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>After generating a critique and correction, run a direct critic function g_l_theta on the correction; accept the critique for training only if the validator returns (1, -1) meaning a fully correct solution; otherwise reject.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Filtering synthesized critique-correction pairs before self-training (applies to SCRIT data synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improves precision of the generated dataset by removing critiques that lead to inconsistent or incorrect corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Inclusion of self-validation increased dataset quality (51.4% of 665K pairs passed critic+validation → 342K valid triplets) and final metrics: baseline SCRIT (with validation) CC-Acc 56.8%, EI-F1 40.2 (for 10K baseline in ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Removing self-validation yields CC-Acc drop of 0.8% and EI-F1 drop of 3.0 in controlled ablation (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Automated validation via the same or a direct critic prompt: check that the proposed corrected solution is fully correct (binary label and first-error index); implemented through prompt-based model evaluation and filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Ablation (Table 4) shows removing self-validation degrades performance (CC-Acc -0.8, EI-F1 -3.0). Case studies (Figures 6-7) demonstrate self-validation rejecting ineffective critiques and accepting effective ones.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-validation itself depends on the validator's accuracy and tends to have lower pass rates on complex problems; if validator is imperfect it may both falsely reject good critiques or accept flawed ones.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Improves over naive acceptance of generated critiques; aligns with prior works that validate corrections (e.g., Critic-CoT) but here validation is performed without external stronger-model supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Without self-validation: CC-Acc 56.0 (-0.8), EI-F1 37.2 (-3.0) relative to baseline in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Critique Abilities in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8749.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8749.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct Critic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Critic (baseline self-critique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach where an LLM directly critiques a solution without access to a reference correct solution; observed to produce superficial critiques and 'rubber-stamping' behavior on complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-72B-Instruct (and compared baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as prompt-based direct critic (no reference) following prior formulations (Zheng et al., 2024a) and implemented as a baseline critic generator in controlled experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Direct critique (no reference)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt the model to analyze a student solution and produce step-wise critiques, conclude on correctness and propose corrections without providing any separate correct reference solution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Critique & Correct data synthesis and evaluation (compared within SCRIT experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to directly generate critiques for student solutions across mathematical/scientific benchmarks; also used as a validator in SCRIT's self-validation step.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Direct Critic used alone produced substantially lower-quality critiques and lower downstream training gains than contrastive critic; specific numeric drop not tabulated as a single number but reported qualitatively and shown in controlled comparisons/figures (e.g., rubber-stamping examples in Figures 1 and 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>N/A (this entry is the no-reference method itself).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering (direct critique prompts) without auxiliary reference or injected bugs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper reports that Direct Critic often yields superficial critique and results in inferior downstream fine-tuning compared to contrastive critic; qualitative examples show blind approval and misled feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prone to 'rubber-stamping' where it uncritically approves incorrect solutions; generates low-quality training data if used naïvely; validation required to avoid training on such low-quality critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Worse than Contrastive Critic in generating useful training data; compared and illustrated in figures and case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Critique Abilities in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8749.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8749.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bug-Injection Critic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bug-Injection Critic (two-stage injected-error critique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage critique approach where synthetic bugs are injected into correct solutions to produce flawed solutions, which are then critiqued and corrected by the model; found to produce overly simplistic and repetitive errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-72B-Instruct (used to critique bug-injected solutions in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as a two-stage pipeline: (1) inject bugs into a correct solution, (2) prompt model to critique and correct the bug-injected solution; used as a baseline critic mechanism in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Bug-injection critique</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate synthetic errors (bugs) in otherwise correct solutions and then task the LLM to identify and correct these injected bugs, producing critique-correction pairs for training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Critique & Correct data synthesis experiments (compared to SCRIT's contrastive critic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to create synthetic erroneous solutions for critique/correction training across mathematical problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper reports limited effectiveness: bug-injected data tends to be simplistic (basic arithmetic mistakes, variable confusions) and yields less effective downstream critique improvements compared to contrastive critic; no single aggregated numeric performance reported solely for this method.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>N/A (method itself is the injected-bug pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based bug injection followed by direct critique prompts; relies on synthetic error generation rather than reference-conditioned understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical comparison and case studies (Appendix F, Figure 8) show injected bugs are oversimplified and downstream benefits are limited relative to contrastive critic.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Injected bugs are often simplistic and repetitive, not reflecting the diversity and subtlety of real reasoning errors in complex math problems; limits its usefulness for producing realistic critique training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Inferior to Contrastive Critic for SCRIT data synthesis; provides less diverse and less realistic error examples than those obtained by collecting multi-model solutions and using references.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Critique Abilities in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8749.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8749.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-round Self-evolving</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-round self-evolving iteration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative procedure where the SCRIT-enhanced model is used as the new base to synthesize higher-quality critique data in successive rounds, enabling progressive self-improvement without external supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-72B-Instruct (initial base; then SCRIT-refined models used for subsequent rounds)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Starting from Qwen2.5-72B-Instruct, SCRIT is applied to produce an enhanced model; that enhanced model is reused to generate improved critiques and training data in further rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative self-training (multi-round SCRIT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Perform repeated cycles of (contrastive self-critic generation + self-validation + self-training), using the previously enhanced model to produce the next round of data, enabling continuous improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Progressive improvement evaluated on CC-Acc and EI-F1 across rounds</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure how critique-correction accuracy and error-identification F1 evolve when SCRIT is applied over multiple rounds (reported up to 3 rounds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Multi-round application produced consistent positive gains across both CC-Acc and EI-F1 over three iterations (qualitative and plotted in Figure 3). Exact per-round numeric sequence not tabulated in the main text but reported as steady improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Single-round (no iterative reuse) yields lower final performance compared to multi-round SCRIT; paper reports additional gains when using SCRIT-enhanced model for subsequent rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Closed-loop self-training where the model's improved critique ability is leveraged to synthesize better training data in successive iterations; implemented via prompting, validation and fine-tuning (no external supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Figure 3 (right panel) shows consistent positive scaling across both CC-Acc and EI-F1 over 3 iterations; text reports further improvements when moving from Round 1 to Round 2 and Round 3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Quality of iterative improvement depends on the quality of validation and the expressive capacity of the model; diminishing returns possible but not fully characterized; relies on availability of verifiable reference solutions for data synthesis each round.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Unique to SCRIT among compared methods; supports continued improvement whereas Direct Critic and Bug-Injection were evaluated primarily in single-round data synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evolving Critique Abilities in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-critiquing models for assisting human evaluators. <em>(Rating: 2)</em></li>
                <li>Llm critics help catch llm bugs. <em>(Rating: 2)</em></li>
                <li>Critic-cot: Boosting the reasoning abilities of large language model via chain-of-thoughts critic. <em>(Rating: 2)</em></li>
                <li>Supercorrect: Supervising and correcting language models with error-driven insights. <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet. <em>(Rating: 2)</em></li>
                <li>Training language models to critique with multi-agent feedback. <em>(Rating: 1)</em></li>
                <li>Generative verifiers: Reward modeling as next-token prediction. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8749",
    "paper_id": "paper-275458736",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "SCRIT",
            "name_full": "Self-evolving CRITic",
            "brief_description": "A self-training framework that enables an LLM to iteratively develop critique abilities by synthesizing contrastive critique data using reference solutions, self-validating corrections, and fine-tuning on validated critiques without external supervision at inference time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-72B-Instruct",
            "model_description": "Qwen2.5-72B-Instruct, a 72B-parameter instruction-tuned LLM used as the base and fine-tuning target for SCRIT; previously extensively pre-trained and post-trained per Qwen-Team (2024).",
            "reflection_method_name": "Self-evolving critique (SCRIT)",
            "reflection_method_description": "Generate high-quality critique-correction pairs by (1) contrastive critic generation conditioned on reference (correct) solutions, (2) self-validation that verifies the proposed correction yields a fully correct solution, and (3) self-training the model on validated (problem, student-solution) → (stepwise critique, correctness label, correction) pairs; reference solutions are used only during data synthesis and not at inference.",
            "task_name": "Critic & Correct across mathematical and scientific reasoning benchmarks (RealCritic) and error-identification benchmarks (PRM800K, ProcessBench)",
            "task_description": "Evaluation protocols that test step-wise critique, identification of the first erroneous step, and whether the critique leads to a correct corrected solution across datasets such as GSM8K, MATH, OlympiadBench, ARC-C, GPQA, MMLU-STEM, PRM800K, and ProcessBench.",
            "performance_with_reflection": "Average critique-correction accuracy (CC-Acc) improved to 50.0% on deliberately incorrect solutions (from 39.7%), 62.1% on balanced solutions (from 57.7%), and 62.9% on base-model self-solutions (from 61.7%); overall reported 10.0% relative gain in CC-Acc. Error-identification F1 (EI-F1) improved from 37.8% to 45.0% (19.0% relative improvement).",
            "performance_without_reflection": "Baseline Qwen2.5-72B-Instruct (no SCRIT self-evolution): 39.7% CC-Acc on deliberately incorrect solutions, 57.7% on balanced, 61.7% on base-model solutions; EI-F1 = 37.8%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted contrastive critique generation using reference solutions (prompt engineering), automatic self-validation via a direct critic checking that the generated correction is fully correct, and supervised fine-tuning (cross-entropy) on validated critique-correction pairs (no external memory or modules at inference).",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Quantitative gains reported across benchmarks (see performance_with_reflection). Additional evidence: gains scale with more training data and larger model size; multi-round iterations (up to 3 rounds) produced consistent further improvements; ablations (Table 4) show removal of self-validation reduces metrics.",
            "limitations_or_failure_cases": "Requires verifiable/reference solutions during data synthesis (not at inference); validation rates drop substantially with problem difficulty (e.g., 91.8% on GSM8K to 27.1% on Olympiad problems); residual gap to stronger external critic (o1-mini). Self-critic without validation can produce low-quality critiques that approve intermediate steps then reject final answer; framework is sensitive to domain diversity and problem complexity selection.",
            "comparison_to_other_methods": "Compared to Direct Critic and Bug-Injection Critic, SCRIT (with contrastive critic + self-validation) performed substantially better; approaches relying on external stronger models (e.g., o1-mini) still outperform SCRIT in some settings, but SCRIT approaches their level without external supervision.",
            "ablation_study_results": "Removing self-validation: CC-Acc -0.8, EI-F1 -3.0. Limiting domains to GSM8K+MATH: CC-Acc -1.4, EI-F1 -1.4. Training with more complex problems first hurts EI-F1; using a higher bad:good ratio (0.25:0.75) yields better EI-F1. Results summarized in Table 4.",
            "uuid": "e8749.0",
            "source_info": {
                "paper_title": "Self-Evolving Critique Abilities in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Contrastive Critic",
            "name_full": "Contrastive Critic (reference-conditioned critique)",
            "brief_description": "A critique-generation technique that conditions the critic on a correct reference solution prior to analyzing a target (student) solution, which mitigates blind approval and improves error identification and corrective guidance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-72B-Instruct (used to generate critiques during SCRIT data synthesis)",
            "model_description": "Same 72B Qwen2.5 instruction-tuned model used with prompts that provide a reference solution and ask for staged analysis (reference analysis, step-wise critique, conclusion, correction).",
            "reflection_method_name": "Contrastive critique (reference-conditioned self-critique)",
            "reflection_method_description": "Provide a correct reference solution first (Reference Analysis), then perform step-wise critique of the target solution using the derived conceptual understanding, emit a conclusion (correctness + first error step), and propose a correction; used to synthesize training data for self-training.",
            "task_name": "Synthesizing critique-correction training data for SCRIT; evaluated via Critic & Correct and error-identification benchmarks",
            "task_description": "Generates labeled critiques and corrections for problems where at least one correct and one incorrect solution exist, to be validated and used to fine-tune the critic model.",
            "performance_with_reflection": "When used for data synthesis in SCRIT, contributed to overall improvements: CC-Acc up to 58.3% (data/model scaling context) and EI-F1 up to 45.1% for the largest model; contrasted with Direct Critic and Bug-Injection, Contrastive Critic shows superior data quality and downstream gains (quantified within SCRIT results).",
            "performance_without_reflection": "Direct Critic (no reference) exhibits 'rubber-stamping' behavior and produces lower-quality critiques; when contrastive conditioning is removed, downstream performance degrades (qualitative and via controlled comparisons reported in the paper).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering to include a correct reference solution and a 4-stage self-critic prompt (Reference Analysis, Step-wise Critique, Conclusion, Correction); no extra modules beyond prompting and fine-tuning.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical comparisons (controlled experiments) showed contrastive critic avoids blind approvals and yields higher validation and better training data resulting in improved final critic performance; Figure 1 and Figure 5 illustrate qualitative differences and Figure 3 shows contrastive critic outperforms baselines in data-scaling plots.",
            "limitations_or_failure_cases": "Requires problems that have at least one correct reference solution available during data synthesis (filtering reduces dataset to problems with both correct and incorrect solutions). Effectiveness decreases on very difficult problems where validation rates are low.",
            "comparison_to_other_methods": "Outperforms Direct Critic (which tends to rubber-stamp) and Bug-Injection Critic (which injects simplistic, repetitive bugs) in both qualitative case studies and quantitative downstream metrics within SCRIT.",
            "ablation_study_results": null,
            "uuid": "e8749.1",
            "source_info": {
                "paper_title": "Self-Evolving Critique Abilities in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Self-Validation",
            "name_full": "Self-Validation (correction validation)",
            "brief_description": "A post-generation filtering step that accepts only those self-generated critiques whose proposed corrections are validated by a direct critic as fully correct solutions, improving training data quality for self-training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-72B-Instruct (used as validator in v_theta and g_l_theta functions)",
            "model_description": "Used in validation mode (direct critic prompt template) to evaluate whether a generated correction yields (y=1, j=-1), i.e., a fully correct solution with no errors.",
            "reflection_method_name": "Self-validation of generated corrections",
            "reflection_method_description": "After generating a critique and correction, run a direct critic function g_l_theta on the correction; accept the critique for training only if the validator returns (1, -1) meaning a fully correct solution; otherwise reject.",
            "task_name": "Filtering synthesized critique-correction pairs before self-training (applies to SCRIT data synthesis)",
            "task_description": "Improves precision of the generated dataset by removing critiques that lead to inconsistent or incorrect corrections.",
            "performance_with_reflection": "Inclusion of self-validation increased dataset quality (51.4% of 665K pairs passed critic+validation → 342K valid triplets) and final metrics: baseline SCRIT (with validation) CC-Acc 56.8%, EI-F1 40.2 (for 10K baseline in ablation).",
            "performance_without_reflection": "Removing self-validation yields CC-Acc drop of 0.8% and EI-F1 drop of 3.0 in controlled ablation (Table 4).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Automated validation via the same or a direct critic prompt: check that the proposed corrected solution is fully correct (binary label and first-error index); implemented through prompt-based model evaluation and filtering.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Ablation (Table 4) shows removing self-validation degrades performance (CC-Acc -0.8, EI-F1 -3.0). Case studies (Figures 6-7) demonstrate self-validation rejecting ineffective critiques and accepting effective ones.",
            "limitations_or_failure_cases": "Self-validation itself depends on the validator's accuracy and tends to have lower pass rates on complex problems; if validator is imperfect it may both falsely reject good critiques or accept flawed ones.",
            "comparison_to_other_methods": "Improves over naive acceptance of generated critiques; aligns with prior works that validate corrections (e.g., Critic-CoT) but here validation is performed without external stronger-model supervision.",
            "ablation_study_results": "Without self-validation: CC-Acc 56.0 (-0.8), EI-F1 37.2 (-3.0) relative to baseline in Table 4.",
            "uuid": "e8749.2",
            "source_info": {
                "paper_title": "Self-Evolving Critique Abilities in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Direct Critic",
            "name_full": "Direct Critic (baseline self-critique)",
            "brief_description": "A baseline approach where an LLM directly critiques a solution without access to a reference correct solution; observed to produce superficial critiques and 'rubber-stamping' behavior on complex reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-72B-Instruct (and compared baselines)",
            "model_description": "Used as prompt-based direct critic (no reference) following prior formulations (Zheng et al., 2024a) and implemented as a baseline critic generator in controlled experiments.",
            "reflection_method_name": "Direct critique (no reference)",
            "reflection_method_description": "Prompt the model to analyze a student solution and produce step-wise critiques, conclude on correctness and propose corrections without providing any separate correct reference solution.",
            "task_name": "Critique & Correct data synthesis and evaluation (compared within SCRIT experiments)",
            "task_description": "Used to directly generate critiques for student solutions across mathematical/scientific benchmarks; also used as a validator in SCRIT's self-validation step.",
            "performance_with_reflection": "Direct Critic used alone produced substantially lower-quality critiques and lower downstream training gains than contrastive critic; specific numeric drop not tabulated as a single number but reported qualitatively and shown in controlled comparisons/figures (e.g., rubber-stamping examples in Figures 1 and 5).",
            "performance_without_reflection": "N/A (this entry is the no-reference method itself).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering (direct critique prompts) without auxiliary reference or injected bugs.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper reports that Direct Critic often yields superficial critique and results in inferior downstream fine-tuning compared to contrastive critic; qualitative examples show blind approval and misled feedback.",
            "limitations_or_failure_cases": "Prone to 'rubber-stamping' where it uncritically approves incorrect solutions; generates low-quality training data if used naïvely; validation required to avoid training on such low-quality critiques.",
            "comparison_to_other_methods": "Worse than Contrastive Critic in generating useful training data; compared and illustrated in figures and case studies.",
            "ablation_study_results": null,
            "uuid": "e8749.3",
            "source_info": {
                "paper_title": "Self-Evolving Critique Abilities in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Bug-Injection Critic",
            "name_full": "Bug-Injection Critic (two-stage injected-error critique)",
            "brief_description": "A two-stage critique approach where synthetic bugs are injected into correct solutions to produce flawed solutions, which are then critiqued and corrected by the model; found to produce overly simplistic and repetitive errors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-72B-Instruct (used to critique bug-injected solutions in experiments)",
            "model_description": "Applied as a two-stage pipeline: (1) inject bugs into a correct solution, (2) prompt model to critique and correct the bug-injected solution; used as a baseline critic mechanism in comparisons.",
            "reflection_method_name": "Bug-injection critique",
            "reflection_method_description": "Generate synthetic errors (bugs) in otherwise correct solutions and then task the LLM to identify and correct these injected bugs, producing critique-correction pairs for training.",
            "task_name": "Critique & Correct data synthesis experiments (compared to SCRIT's contrastive critic)",
            "task_description": "Used to create synthetic erroneous solutions for critique/correction training across mathematical problems.",
            "performance_with_reflection": "Paper reports limited effectiveness: bug-injected data tends to be simplistic (basic arithmetic mistakes, variable confusions) and yields less effective downstream critique improvements compared to contrastive critic; no single aggregated numeric performance reported solely for this method.",
            "performance_without_reflection": "N/A (method itself is the injected-bug pipeline).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-based bug injection followed by direct critique prompts; relies on synthetic error generation rather than reference-conditioned understanding.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical comparison and case studies (Appendix F, Figure 8) show injected bugs are oversimplified and downstream benefits are limited relative to contrastive critic.",
            "limitations_or_failure_cases": "Injected bugs are often simplistic and repetitive, not reflecting the diversity and subtlety of real reasoning errors in complex math problems; limits its usefulness for producing realistic critique training data.",
            "comparison_to_other_methods": "Inferior to Contrastive Critic for SCRIT data synthesis; provides less diverse and less realistic error examples than those obtained by collecting multi-model solutions and using references.",
            "ablation_study_results": null,
            "uuid": "e8749.4",
            "source_info": {
                "paper_title": "Self-Evolving Critique Abilities in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Multi-round Self-evolving",
            "name_full": "Multi-round self-evolving iteration",
            "brief_description": "An iterative procedure where the SCRIT-enhanced model is used as the new base to synthesize higher-quality critique data in successive rounds, enabling progressive self-improvement without external supervision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-72B-Instruct (initial base; then SCRIT-refined models used for subsequent rounds)",
            "model_description": "Starting from Qwen2.5-72B-Instruct, SCRIT is applied to produce an enhanced model; that enhanced model is reused to generate improved critiques and training data in further rounds.",
            "reflection_method_name": "Iterative self-training (multi-round SCRIT)",
            "reflection_method_description": "Perform repeated cycles of (contrastive self-critic generation + self-validation + self-training), using the previously enhanced model to produce the next round of data, enabling continuous improvement.",
            "task_name": "Progressive improvement evaluated on CC-Acc and EI-F1 across rounds",
            "task_description": "Measure how critique-correction accuracy and error-identification F1 evolve when SCRIT is applied over multiple rounds (reported up to 3 rounds).",
            "performance_with_reflection": "Multi-round application produced consistent positive gains across both CC-Acc and EI-F1 over three iterations (qualitative and plotted in Figure 3). Exact per-round numeric sequence not tabulated in the main text but reported as steady improvements.",
            "performance_without_reflection": "Single-round (no iterative reuse) yields lower final performance compared to multi-round SCRIT; paper reports additional gains when using SCRIT-enhanced model for subsequent rounds.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Closed-loop self-training where the model's improved critique ability is leveraged to synthesize better training data in successive iterations; implemented via prompting, validation and fine-tuning (no external supervision).",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Figure 3 (right panel) shows consistent positive scaling across both CC-Acc and EI-F1 over 3 iterations; text reports further improvements when moving from Round 1 to Round 2 and Round 3.",
            "limitations_or_failure_cases": "Quality of iterative improvement depends on the quality of validation and the expressive capacity of the model; diminishing returns possible but not fully characterized; relies on availability of verifiable reference solutions for data synthesis each round.",
            "comparison_to_other_methods": "Unique to SCRIT among compared methods; supports continued improvement whereas Direct Critic and Bug-Injection were evaluated primarily in single-round data synthesis.",
            "ablation_study_results": null,
            "uuid": "e8749.5",
            "source_info": {
                "paper_title": "Self-Evolving Critique Abilities in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-critiquing models for assisting human evaluators.",
            "rating": 2,
            "sanitized_title": "selfcritiquing_models_for_assisting_human_evaluators"
        },
        {
            "paper_title": "Llm critics help catch llm bugs.",
            "rating": 2,
            "sanitized_title": "llm_critics_help_catch_llm_bugs"
        },
        {
            "paper_title": "Critic-cot: Boosting the reasoning abilities of large language model via chain-of-thoughts critic.",
            "rating": 2,
            "sanitized_title": "criticcot_boosting_the_reasoning_abilities_of_large_language_model_via_chainofthoughts_critic"
        },
        {
            "paper_title": "Supercorrect: Supervising and correcting language models with error-driven insights.",
            "rating": 2,
            "sanitized_title": "supercorrect_supervising_and_correcting_language_models_with_errordriven_insights"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet.",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Training language models to critique with multi-agent feedback.",
            "rating": 1,
            "sanitized_title": "training_language_models_to_critique_with_multiagent_feedback"
        },
        {
            "paper_title": "Generative verifiers: Reward modeling as next-token prediction.",
            "rating": 1,
            "sanitized_title": "generative_verifiers_reward_modeling_as_nexttoken_prediction"
        }
    ],
    "cost": 0.0157495,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Evolving Critique Abilities in Large Language Models
4 Aug 2025</p>
<p>Zhengyang Tang 
The Chinese University of Hong Kong
Shenzhen</p>
<p>Qwen Team
Alibaba Inc</p>
<p>Ziniu Li 
The Chinese University of Hong Kong
Shenzhen</p>
<p>Shenzhen Research Institute of Big Data</p>
<p>Zhenyang Xiao 
The Chinese University of Hong Kong
Shenzhen</p>
<p>Tian Ding dingtian@sribd.cn 
Shenzhen Research Institute of Big Data</p>
<p>Shenzhen International Center for Industrial and Applied Mathematics</p>
<p>Ruoyu Sun 
The Chinese University of Hong Kong
Shenzhen</p>
<p>Shenzhen Research Institute of Big Data</p>
<p>Shenzhen International Center for Industrial and Applied Mathematics</p>
<p>Benyou Wang wangbenyou@cuhk.edu.cn 
The Chinese University of Hong Kong
Shenzhen</p>
<p>Dayiheng Liu liudayiheng.ldyh@alibaba-inc.com 
Qwen Team
Alibaba Inc</p>
<p>Fei Huang 
Qwen Team
Alibaba Inc</p>
<p>Tianyu Liu 
Qwen Team
Alibaba Inc</p>
<p>Bowen Yu 
Qwen Team
Alibaba Inc</p>
<p>Junyang Lin 
Qwen Team
Alibaba Inc</p>
<p>Self-Evolving Critique Abilities in Large Language Models
4 Aug 20253156E36982582D08FF6E648237BC91F8arXiv:2501.05727v2[cs.CL]
Despite their remarkable performance, Large Language Models (LLMs) face a critical challenge: providing feedback for tasks where human evaluation is difficult or where LLMs potentially outperform humans.In such scenarios, leveraging the critique ability of LLMs themselves-identifying and correcting flaws-shows considerable promise.This paper explores enhancing critique abilities of LLMs, noting that current approaches rely on human annotations or more powerful models, leaving the challenge of improving critique abilities without external supervision unresolved.We introduce SCRIT (Self-evolving CRITic), a framework that trains LLMs with self-generated data to evolve their critique abilities.To address the low quality of naively generated data, we propose a contrastive-critic approach that uses reference solutions during data synthesis to enhance the model's understanding of key concepts, and incorporates a self-validation scheme to ensure data quality.The final trained model operates without any reference solutions at inference time.Implemented with Qwen2.5-72B-Instruct, a leading LLM, SCRIT demonstrates consistent improvements across a wide range of benchmarks spanning both mathematical and scientific reasoning: achieving a 10.0% relative gain in critique-correction accuracy and a 19.0%relative improvement in error identification F1-score.Our analysis reveals that SCRIT's performance scales positively with data and model size and enables continuous improvement through multi-round iterations.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) (Achiam et al., 2023;Anthropic, 2024;Qwen-Team, 2024) represent significant milestones in the development of artificial intelligence.They rely on human supervision through methods such as Supervised Fine-Tuning (SFT) (Wei et al., 2021;Li et al., 2025) and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022;Bai et al., 2022;Li et al., 2024).As a result, these models have evolved at an unprecedented pace, surpassing human capabilities in certain challenging domains.However, this framework encounters a fundamental challenge: how to provide effective and scalable feedback for LLMs in tasks that are not only difficult for humans to evaluate but where LLMs may outperform humans.This challenge, known as scalable oversight (Bowman et al., 2022), remains critical, yet progress in this area has been limited.</p>
<p>To address this challenge, leveraging LLMs for evaluation can help refine model outputs (Saunders et al., 2022;McAleese et al., 2024).Central to this approach is the critique ability-identifying and correcting flaws in responses.Accurate critique feedback enables LLMs to improve, advancing toward higher-order intelligence.Yet, studies show LLMs underperform in critique tasks (Zheng et al., 2024b;Yang et al., 2024;Tang et al., 2025).Thus, enhancing critique abilities is a key research problem, which this paper aims to tackle.</p>
<p>Figure 1: Direct critic (baseline) v.s.contrastive critic (ours).Left panel: input materials prepared for critique generation.Right panel: outputs from both approaches.The direct critic exhibits "rubber-stamping" behavior, incorrectly validating flawed solutions and providing misled feedback.The contrastive critic, however, utilizes reference solutions to grasp key concepts and strategies, enabling accurate error identification and correction.</p>
<p>Current approaches to improving the critique abilities of LLMs rely on two sources of supervision: human annotations (Saunders et al., 2022;McAleese et al., 2024) and stronger LLMs that serve as human proxy (e.g., GPT-4 and o1-mini) (Lan et al., 2024;Zhang et al., 2024;Zheng et al., 2024b;Yang et al., 2024)).While these methods have shown promise, they face three fundamental limitations.First, the quality of generated critiques is inherently bounded by the capabilities of the supervisors.Second, the dependence on human annotations or API calls to stronger models introduces significant costs, limiting the scalability of these approaches.Most critically, these approaches fail to address a fundamental question in scalable oversight: how can we enhance the critique abilities of our most capable models when stronger supervisors are no longer available?</p>
<p>In this work, we introduce SCRIT (Self-evolving CRITic), a framework that enables LLMs to develop self-evolving critique abilities in domains with verifiable solutions.We focus on mathematical and scientific reasoning as ideal testbeds for this approach.A key insight is that problems in these domains typically have well-defined reference solutions and corresponding final answers.These resources, leveraged only during the data synthesis phase, guide the critique of a student's solution and help verify the quality of the generated critique.</p>
<p>Our framework has two key steps to generate high-quality critique data for self-training.</p>
<p>• First, we develop a contrastive critique technique, where the model is provided with a reference solution to analyze and critique a student's solution.This step is grounded in our first philosophy: by conditioning on a correct reference solution first, the LLM develops a comprehensive understanding of the relevant concepts and problem-solving strategies, allowing it to accurately identify and address errors in student solutions.Our evidence shows that without this reference point, the model tends to exhibit "rubberstamping" behavior-uncritically approving incorrect solutions and offering misleading feedback (see Figure 1 for examples).</p>
<p>• Next, the LLM is tasked with self-validating the generated critique to improve the data quality.Specifically, the model checks whether the proposed corrections lead to valid solutions.This step is based on our second philosophy: critiques that result in internally consistent and correct correction are considered high-quality, which has also been widely adopted by recent works (Zheng et al., 2024b;Yang et al., 2024;Tang et al., 2025).These two steps together enable the generation of high-quality critique data without human supervisions in writing good critiques for student solutions.Finally, we leverage the generated data to enhance the model's critique abilities through self-training.We clarify that while our framework requires reference solutions as input, it does not depend on ground truth critiques themselves, thus remaining within the scalable oversight paradigm.</p>
<p>We use Qwen2.5-72B-Instruct(Qwen-Team, 2024), a leading 70B model, to implement SCRIT.Our goal is to test whether our framework can further improve its performance.It is important to note that this is a non-trivial task, as Qwen2.5-72B-Instructhas already undergone extensive pre-training and post-training.Experiments show that SCRIT enables substantial improvements across different evaluation protocols as shown in Tables 1 and 2. • On critic and correction tasks from (Tang et al., 2025), spanning 8 mathematical reasoning datasets across 3 scenarios, SCRIT consistently enhances the base Qwen2.5-72B-Instructmodel: improving from 39.7% to 50.0% on deliberately incorrect solutions, from 57.7% to 62.1% on balanced solutions, and from 61.7% to 62.9% on the base model's self-generated solutions, representing a 10.0% relative gain in critique-correction accuracy on average.• For error identification tasks on PRM800K (Lightman et al., 2023) and Process-Bench (Zheng et al., 2024a), two benchmarks with human-labeled error steps, SCRIT achieves consistent improvements across all datasets, raising the average F1 score from 37.8% to 45.0%, a 19.0%relative improvement.In addition to these advancements, we provide a systematic analysis, which will be elaborated on in the main text.Our framework and methodology are detailed in the subsequent sections.Due to space constraints, the related work is discussed in Appendix A.</p>
<p>SCRIT: Self-Evolving Critic</p>
<p>Problem Formulation and Overview</p>
<p>Let P denote a set of problems from a structured domain (e.g., mathematics, science), where each problem p ∈ P is paired with an answer a p .For each problem p, we collect a set of solutions S p = {s 1 , s 2 , ..., s n } from different models, where each solution s i consists of:
• A sequence of reasoning steps r i = [r 1 i , r 2 i , ..., r k i i ],
where k i is the number of steps • A final answer a s i A critique c is defined as a tuple c = (e, l, t), where:</p>
<p>• e = [e 1 , e 2 , ..., e k ] is a sequence of step-wise critiques, where each e i corresponds to the analysis of step r i • l = (y, j) is the conclusion, where y ∈ {0, 1} indicates solution correctness and j ∈ {−1} ∪ N denotes the first error step (j = −1 means no error) • t is the correction, consisting of a sequence of corrected steps and a final answer a t Our objective is to learn a critique function f θ : P × S → C that maps a problem p and a solution s to an effective critique c, where θ denotes the parameter to learn.</p>
<p>To achieve this objective, we propose SCRIT (Self-evolving CRITic), a framework that systematically leverages the shared mathematical understanding across different solutions to enable truly self-evolving critique abilities.SCRIT operates through a complete selfevolving cycle: it takes a problem and solutions as input, generates critiques through analyzing reference solutions, validates their quality, and uses the validated critiques for self-training.This forms a complete self-evolving cycle without any external supervision.</p>
<p>Solution Collection</p>
<p>Dataset The first step in our framework is to collect a diverse set of solutions.We build our collection process on the NuminaMath dataset (LI et al., 2024), a large-scale mathematical problem dataset covering various topics from elementary mathematics to competition-level problems.To ensure data quality, we develop a robust pipeline to compute reliable ground truth answers (detailed in Appendix B), resulting in 452K validated problem-answer pairs.Solution Generation Models To enhance the diversity of generated data, we gather solutions from seven models: deepseek-math-7b-rl (Shao et al., 2024), mathstral-7B-v0.1 (Mistral-AI, 2024a), Mistral-Large-Instruct-2411 (Mistral-AI, 2024b), DeepSeek-V2-Chat-0628 (DeepSeek-AI, 2024), Qwen2.5-Math-7B-Instruct(Qwen-Team, 2024), Qwen2.5-Math-1.5B-Instruct(Qwen-Team, 2024), and Qwen2-Math-1.5B-Instruct(Qwen-Team, 2024).It is important to note that the outputs from these models serve as inputs for the critic model, with no external supervision involved in the critic's learning process.</p>
<p>Data Filtering For each problem p ∈ P, we classify its collected solutions into correct solutions S + p and incorrect solutions S − p based on answer correctness.A crucial filtering criterion in our framework is that each problem must have at least one correct solution and one incorrect solution to enable later contrastive critic.Formally, we only retain problems that satisfy:
P valid = {p ∈ P ||S + p | &gt; 0 ∧ |S − p | &gt; 0}.</p>
<p>Self-Critic Generation</p>
<p>A key challenge in enabling effective critique generation is to ensure the model can identify and correct errors in complex mathematical reasoning, particularly when the problem difficulty approaches or exceeds the model's current capabilities.Our preliminary experiments reveal that the model often exhibits "rubber-stamping behavior" -blindly approving incorrect steps without genuine understanding of the mathematical concepts involved, as illustrated in Figures 1 and 5.This also aligns with findings in (Huang et al., 2023).</p>
<p>We initially explored two approaches from previous works: (1) Direct Critic (Zheng et al., 2024a), where a language model directly critiques a solution; and (2) Bug-Injection Critic (McAleese et al., 2024), a two-stage approach of first injecting errors into a correct solution and then ask the LLM to critic and correct it.However, both approaches showed limited effectiveness (detailed in Section 4.3).</p>
<p>To address these issues, we develop a new technique called Contrastive Critic.Our key insight stems from a fundamental property of mathematical reasoning: while problems may have multiple valid solutions, they share the same underlying mathematical concepts and key solving strategies.By explicitly providing a correct reference solution during critique generation, we enable the model to first understand these core mathematical concepts and solving strategies, then leverage this understanding to perform step-by-step critique of the target solution.This approach addresses the rubber-stamping issue by grounding the critique process in concrete mathematical understanding derived from correct references.</p>
<p>For each valid problem, we generate critiques using two solution pairing approaches:</p>
<p>• Correct-Incorrect Pairs.For each incorrect solution, we randomly select a correct reference solution and generate a critique by comparing the incorrect solution against the reference.</p>
<p>Self-Validation</p>
<p>With self-generated critique data, we apply post-validation techniques to further enhance the quality of generated outputs.This process specifically filters out low-quality cases where the model blindly approves all intermediate steps, only to suddenly reject the final answer upon detecting a discrepancy (see Appendix E).</p>
<p>To address these challenges, we employ direct validation on the correction part of the critique.Formally, we have that:
v θ (c) = 1 if g l θ (p, t) = (1, −1) 0 otherwise
where t is the correction part of critique c, and g l θ (prompt template in Appendix C) denotes direct critic's conclusion generation function that outputs a tuple (y, j) as defined in Section 2.1.Here g l θ (p, t) = (1, −1) indicates that Direct Critic validates the correction t as a fully correct solution with no errors (y = 1, j = −1).This validation mechanism ensures that only critiques leading to verifiably correct solutions are used for self-training.</p>
<p>Self-Training</p>
<p>Let V denote the set of validated solution-critique pairs across all problems: V = {(p, s, c)| p ∈ P valid , s ∈ S p , v θ (c) = 1}.For each validated triplet (p, s, c), we construct a training instance.The input to the model consists of the problem p and the student solution s.The target for fine-tuning consists of the critique components (e, l, t) from c. Crucially, the reference solution used during data generation is not provided as input during training.This ensures the model learns a general critique ability independent of reference solutions at inference time.We then fine-tune Qwen2.5-72B-Instruct to minimize the cross-entropy loss.</p>
<p>Experiments</p>
<p>Statistics of SCRIT</p>
<p>We present detailed statistics of data flow through each component of our framework.</p>
<p>Solution Collection</p>
<p>We start with 452K problem-answer pairs from our own NuminaMath dataset (see Appendix B).For solution generation, we employ 7 models of varying capabilities as described in Section 2.2.Each model generates one solution per problem, with solutions classified as correct or incorrect based on their final answers using Qwen2.5-72B-Instruct(detailed in Appendix I).Then we apply two filtering criteria: (a) Each problem must have at least one correct and one incorrect solution to enable contrastive learning; (b) Solutions from each model are capped at 50K for both correct and incorrect categories.After filtering, we obtain 665K problem-solution pairs, evenly split between good solutions (332K) and bad solutions (332K).</p>
<p>Self-Critic &amp; Self-Validation</p>
<p>To analyze the self-critic and self-validation step, we track the data flow from the initial 665K problem-solution pairs through these steps.Out of these pairs, 342K (51.4%) successfully pass the self-critic and self-validation step, yielding highquality problem-solution-critique triplets.Detailed analysis in Figure 2 reveals systematic patterns: validation rates decrease from elementary domains (GSM8K: 91.8%, ORCA Math: 77.6%) to competition-level problems (Olympiads: 27.1%), show strong negative correlation with problem complexity (91.7% for single-answer problems to 15.5% for seven-answer problems), while remaining relatively consistent across solution models (48.9% to 57.4%).This suggests our self-validation process is more sensitive to problem difficulty than to the source model.Analysis of error positions in critiqued solutions (see Figure 16) reveals that a majority of errors occur in earlier steps, aligning well with human-labeled error distributions in ProcessBench (Zheng et al., 2024a).This correlation suggests that our self-critic framework captures human-like error identification patterns.</p>
<p>Self-Training</p>
<p>We maintain a balanced 1:1 ratio between correct and incorrect solutions, resulting in 170K training examples.These balanced training data are used to fine-tune Qwen2.5-72B-Instructfollowing Section 2.5 (complete training details in Appendix J).</p>
<p>Evaluation</p>
<p>We present two complementary evaluation protocols to assess critique capabilities:</p>
<p>Critic and Correct</p>
<p>The first protocol evaluates a model's ability to critic and correct a given solution, following the assumption (Zheng et al., 2024b) that effective critiques should guide the correction of errors.We conduct experiments on RealCritic (Tang et al., 2025), which spans benchmarks from two key domains: Mathematical Reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), College Math (Tang et al., 2024), Minerva Math (Lewkowycz et al., 2022), OlympiadBench (He et al., 2024)) and Scientific Reasoning (ARC-C (Clark et al., 2018), GPQA (Rein et al., 2023), MMLU-STEM (Hendrycks et al., 2020)).</p>
<p>Evaluation is conducted across 3 scenarios: critic on incorrect solutions, balanced solutions, and the base model's self-generated solutions (i.e., Qwen2.5-72B-Instruct'sown solutions).</p>
<p>Critic and Correct with Error Identification</p>
<p>The second protocol requires models to provide accurate correction and identify the first error step.We evaluate on PRM800K (Lightman et al., 2023)1 and ProcessBench (Zheng et al., 2024a), which contain human-labeled error steps from advanced models across GSM8K, MATH, OlympiadBench, and Omni-Math.</p>
<p>Following ProcessBench, we use the F1 score of accuracies on incorrect and correct samples as our metric, with two adaptations to ensure critique effectiveness (See Appendix G).</p>
<p>Baselines Since our goal is to improve Qwen2.5-72B-Instruct'scritique ability through selfevolution, we use the original Qwen2.5-72B-Instruct as our primary baseline.Additionally, we compare against o1-mini (OpenAI, 2024), currently one of the most capable models in terms of critique ability (Zheng et al., 2024a).   1 presents results across three increasingly challenging scenarios.SCRIT substantially improves over the base Qwen2.5-72B-Instructmodel on deliberately incorrect solutions (50.0%vs 39.7%), maintains a 4.4% advantage on balanced solutions, and even improves when critiquing the base model's own solutions (62.9% vs 61.7%).These results represent a 10.0% relative gain in critique-correction accuracy across all scenarios, approaching the performance of o1-mini.
0 1 0 K 2 0 K 4 0 K 8 0 K 1 7 0 K Number of</p>
<p>Main Results</p>
<p>Critic and Correct Table</p>
<p>Critic and Correct with Error Identification</p>
<p>As shown in Table 2, SCRIT shows strong capabilities in error identification, achieving consistent improvements across all datasets in both PRM800K and ProcessBench.The average F1 score improves from 37.8% to 45.0% (a 19.0% relative improvement), with strong gains on mathematical reasoning tasks (GSM8K: +11.3%, MATH: +9.1%).While there remains a gap with o1-mini, SCRIT's improvements are notable given its self-evolving nature without reliance on external supervision.</p>
<p>Analysis</p>
<p>Throughout this section, we report two metrics: critique-correction accuracy (CC-Acc) from the Critic and Correct protocol, which is averaged across three scenarios, and error identification F1-score (EI-F1) from the Critic and Correct with Error Identification protocol.</p>
<p>Generalization to Scientific Reasoning</p>
<p>A key question is whether SCRIT's self-evolving mechanism, primarily trained on mathematical data, can generalize to other complex reasoning domains.Our evaluation already includes scientific reasoning benchmarks (ARC-C, GPQA, MMLU-STEM), where Table 1 shows consistent improvements, confirming cross-domain generalization.</p>
<p>To further investigate this, we conducted an additional experiment where we trained a separate SCRIT model exclusively on 10K critique examples synthesized from scientific reasoning problems (from AM-Thinking-v1 (Ji et al., 2025)).As shown in Table 3, SCRIT trained on scientific data yields even stronger performance on scientific benchmarks (+10.6% on balanced solutions) while remaining competitive on math tasks.This not only demonstrates the framework's effectiveness beyond mathematics but also highlights its ability to capture and leverage domain-specific error patterns.</p>
<p>Scaling Behavior of SCRIT</p>
<p>We investigate SCRIT's performance scaling with training data and model size (see Figure 3).</p>
<p>Data Size Scaling</p>
<p>Model Size Scaling</p>
<p>We evaluate SCRIT across three model sizes of Qwen2.5:1.5B, 7B, and 72B.Both metrics show strong positive correlation with model scale.The CC-Acc increases substantially from 41.7% (1.5B) to 51.2% (7B) and further to 58.3% (72B).The improvement is more pronounced for EI-F1, where metric rises from 12.5% to 29.9% and then to 45.1%, suggesting that larger models are particularly better at error identification.</p>
<p>To further verify that SCRIT is not only beneficial for large models, we applied the framework to a mid-sized model, Qwen2.5-32B-Instruct.SCRIT improved its performance meaningfully, with CC-Acc increasing from 53.9% to 56.5% and EI-F1 from 35.8% to 41.5%.This shows that the self-evolving mechanism is robust and effective across different model scales.</p>
<p>Which Critic Mechanism is Most Effective?</p>
<p>To identify the most effective critic mechanism for our self-evolving framework, we conduct strictly controlled experiments comparing three different critic approaches described in Section 2.3 using identical sets of problems and solutions.</p>
<p>Our experiments in Figure 3  Through case studies (detailed in Appendices D and F), we identify the key mechanisms behind these performance differences.Direct Critic often falls into superficial critiquing, tending to blindly agree with solutions without deep understanding.Contrastive Critic avoids this pitfall by first analyzing reference solutions, enabling the model to develop a deeper understanding of the underlying mathematical concepts and solution strategies before attempting critique.While Bug-Injection Critic has the theoretical advantage of known error descriptions, our analysis reveals that model-injected bugs tend to be simplistic and repetitive, predominantly focusing on basic arithmetic errors and variable confusions, limiting its effectiveness in real-world scenarios where errors are more diverse and subtle.</p>
<p>Does Multi-round Iteration Foster Improvement?</p>
<p>A unique advantage of SCRIT is its ability to support multi-round self-evolution.After collecting the initial solutions, we can iteratively apply the self-critic generation, self-validation, and self-training steps to continuously improve the model's critique abilities.Specifically, we conduct experiments with three rounds of iterations.Starting with Qwen2.5-72B-Instruct as the base model, we apply SCRIT to obtain an enhanced model with improved critique capabilities.As shown in Figure 3, using this enhanced model as the new base for Round 2, we observe further improvements in both metrics.Continuing with the Round 2 model for the third iteration, we achieve additional gains.</p>
<p>The performance demonstrates consistent positive scaling across both metrics through multiple rounds of iteration.This sustained improvement suggests that SCRIT can effectively leverage its own enhanced critique capabilities to generate increasingly higher-quality training data, enabling genuine self-evolution without external supervision.</p>
<p>How Important is Self-Validation?</p>
<p>To assess the necessity of self-validation in SCRIT, we conduct controlled experiments by removing the self-validation component while keeping all other settings identical.The results in Table 4 show clear performance degradation across both evaluation metrics: the CC-Acc drops by 0.8%, and more significantly, the EI-F1 decreases by 3.0%.Case analysis (see Appendix E) shows that the self-critic may still generate low-quality critiques, often blindly approving all intermediate steps only to suddenly claim "the final step is incorrect" when encountering answer discrepancies.By incorporating self-validation, we are able to further enhance the quality of data for self-training.</p>
<p>Does Problem Domain Diversity Matter?</p>
<p>To investigate the importance of problem domain diversity, we conduct controlled experiments by restricting the training data to only GSM8K and MATH, while keeping other settings unchanged.This represents a significant reduction in domain coverage compared to our full setting which spans 9 sources ranging from elementary to competition-level mathematics.The results in Table 4 show the value of domain diversity: when training with limited domains, the CC-Acc drops by 1.4% and the EI-F1 decreases by 1.4%.It suggests that exposure to diverse problem-solving patterns and error types is crucial for developing robust critique abilities.</p>
<p>How Does Problem Difficulty Impact Performance?</p>
<p>To understand the impact of problem difficulty, we conduct experiments by selecting training examples based on the number of unique answers generated across solution models -a proxy for problem complexity.We study two settings: training with problems that have more unique answers (indicating higher complexity) versus those with fewer unique answers (indicating lower complexity).Interestingly, training with less complex problems leads to better performance in EI-F1 in Table 4.This result suggests that SCRIT can generate more effective critiques on simpler problems, possibly because the mathematical concepts and solution strategies in these problems are more structured and well-defined, enabling the model to develop more precise and reliable critique patterns.</p>
<p>This finding leaves space for future work: how to optimally select training examples based on difficulty levels in a self-evolving framework.While our current approach uses all available data, a more sophisticated curriculum that gradually increases problem complexity might lead to more effective self-evolution.</p>
<p>Does the Choice of Solution Model Matter?</p>
<p>To study whether critiquing solutions from different models affects SCRIT's performance, we conduct controlled experiments by restricting the solutions being critiqued to those from a single model while keeping other settings identical.Our results in Table 4 show that the source model of solutions has limited impact on SCRIT's final performance.</p>
<p>Since solution generation models only provide the solutions for constructing contrastive critique pairs and do not directly participate in improving critique effectiveness, their individual capabilities have less influence on the final performance.What matters more is how to construct diverse and informative contrastive pairs that help the model learn effective critique strategies, regardless of the solution models.</p>
<p>Optimal Ratio between Good and Bad Solutions?</p>
<p>Finally, we investigate the impact of good-to-bad solution ratio in the training data.As shown in Table 4, training with a higher proportion of bad solutions (0.25:0.75) shows better performance than using more good solutions (0.75:0.25).This suggests that exposure to more bad solutions helps SCRIT develop stronger error identification capabilities, likely because it provides more diverse examples of mathematical mistakes and their corresponding corrections.More importantly, analyzing incorrect solutions forces the model to actively engage in error detection and correction, rather than simply validating correct steps.</p>
<p>Conclusion</p>
<p>In this work, we present SCRIT, a self-evolving critique framework that enhances critiquecorrection accuracy and error detection in domains with verifiable solutions.By leveraging a contrastive-critic mechanism during data synthesis, SCRIT improves its capabilities without external supervision.Our experiments, spanning both mathematical and scientific reasoning, demonstrate that SCRIT scales with data and model size, shows strong crossdomain generalization, and benefits from self-validation.Future work could consider using SCRIT's high-quality critiques to label reasoning steps and optimize student models via reinforcement learning (e.g., (Saunders et al., 2022;McAleese et al., 2024)), or extending the framework to other structured domains like coding and logic.</p>
<p>A Related Work</p>
<p>Scalable Oversight and Critic Models The challenge of providing effective feedback to language models on tasks difficult for humans to evaluate has attracted significant research attention.Early work by (Saunders et al., 2022) proposed fine-tuning LLMs to generate natural language critiques, introducing key components including critique generation, discrimination, and correction.Building on this direction, CriticGPT (McAleese et al., 2024) applied similar principles to code review tasks, incorporating RLHF and specialized human supervision through a "Tampering" step.These works established the importance of critique ability in enabling scalable oversight of language models.</p>
<p>Sources of Critique Supervision</p>
<p>Existing approaches to developing critique abilities primarily rely on two types of supervision sources.The first category uses human supervision, as demonstrated in (Saunders et al., 2022) through direct human annotation and in (McAleese et al., 2024) through human-injected errors.The second category employs strong model supervision, exemplified by MultiCritique (Lan et al., 2024), which utilizes feedback from advanced models like GPT-4 to generate critiques for fine-tuning smaller models.Recent work GenRM (Zhang et al., 2024) proposes Chain-of-Thought Verifiers that generate stepwise critiques for mathematical reasoning, though still relying on human or stronger model supervision.While these approaches have shown promise, they are fundamentally limited by either the capabilities of their supervisors or the substantial costs associated with obtaining supervision.</p>
<p>Critic and Correct</p>
<p>An important challenge in developing critique systems is how to evaluate the quality of critiques themselves, as directly measuring critique effectiveness is often as difficult as the original task.A key insight that has emerged in recent work is that truly effective critiques should be able to guide the correction of errors and lead to correct answers.</p>
<p>This assumption provides a validation mechanism for critique quality and has been widely adopted in the field.For instance, Critic-CoT (Zheng et al., 2024b) combines step-wise critique generation with correction validation using GPT4-Turbo.Similarly, SuperCorrect (Yang et al., 2024) collects critique and corrections from teacher models like o1-mini.These works show the value of using correction as an objective mechanism to verify critique quality, though they still rely on stronger models for supervision.</p>
<p>In contrast to existing approaches that rely on either human annotations or stronger models for supervision, our work introduces SCRIT, a framework that enables self-evolution of critique abilities.By analyzing correct reference solutions to understand key mathematical concepts and strategies, then validating critiques through correction outcomes, our approach creates a closed-loop learning system that can improve its critique capabilities without external supervision.</p>
<p>B Computing Ground Truth Answers for NuminaMath</p>
<p>A large-scale dataset with reliable ground truth answers is fundamental to our work.We choose NuminaMath (LI et al., 2024) for its diversity, difficulty distribution, and scale (860K problems).However, as the correctness of solutions in the original dataset is not guaranteed, we develop a robust pipeline to compute reliable ground truth answers.</p>
<p>B.1 Answer Generation and Validation Pipeline</p>
<p>We employ Qwen2.5-Math-72B-Instruct(Qwen-Team, 2024) under tool-integrated (Gou et al., 2023) settings to generate solutions, as it demonstrates state-of-the-art performance across multiple mathematical reasoning benchmarks.The solutions are then evaluated using Qwen2.5-Math-RM-72B(Qwen-Team, 2024), a specialized reward model for mathematical reasoning.We consider a solution correct if its reward score exceeds a predefined threshold, and use its final answer as the ground truth.</p>
<p>B.2 Threshold Selection and Validation</p>
<p>To determine an appropriate reward threshold, we conduct extensive experiments:</p>
<p>• Benchmark Validation: We evaluate the threshold's effectiveness across multiple standard benchmarks including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), GAOKAO2023-EN (Zhang et al., 2023), OlympiadBench (He et al., 2024), and College Math (Tang et al., 2024).With a threshold of 1.0, we achieve approximately 75% accuracy.</p>
<p>• Human Evaluation: We randomly sample 100 NuminaMath problems and conduct human evaluation of the answers selected using our threshold.The results show approximately 85% accuracy.</p>
<p>• Comparison with Alternative Methods: We explore majority voting among solutions from NuminaMath, Qwen2.5-Math-72B-Instruct, and Deepseek-V2-Chat-0628.However, this approach yields lower accuracy compared to our reward-based selection method.</p>
<p>After applying our pipeline with the validated threshold, we obtain a filtered dataset of 452K problem-answer pairs, which serves as the foundation for our work.</p>
<p>C Prompting Templates for Direct Critic, Bug-Injection Critic and Contrastive Critic</p>
<p>Here we present system prompts used for different critic mechanisms in Figure 4.By developing understanding of the underlying mathematical concepts, Contrastive Critic successfully generate an effective critique that guides the correction process to reach the correct final answer.</p>
<p>E Self-Validation Cases</p>
<p>We present two cases demonstrating the effectiveness of our Self-Validation mechanism in filtering critiques based on Self-Critic's correction in Figures 6 and 7.</p>
<p>F Bug-Injection Case Study</p>
<p>Here we show examples of oversimplified bugs injected by Bug-Injection Critic.These examples illustrate how Bug-Injection Critic tends to generate overly simplistic errors (e.g., misunderstanding basic math properties, variable confusion) rather than more sophisticated mathematical reasoning errors that typically occur in complex problem-solving.</p>
<p>Simple Conceptual Bug</p>
<p>G Adaptations to ProcessBench's Evaluation Protocol</p>
<p>In evaluating models' error identification capabilities, we make two adaptations to Pro-cessBench's original evaluation protocol.These modifications are designed to ensure that models demonstrate genuine understanding of mathematical errors rather than superficial critique.</p>
<p>G.1 Requiring Effective Correction</p>
<p>Our first adaptation stems from the core assumption behind critic and correct tasks: a truly effective critique should not only identify errors but also guide their correction towards an correct answer.Through extensive case studies, we found that models can sometimes correctly identify the error step (matching human annotations) without actually understanding the mathematical mistake.As shown in Figures 10 to 12, these cases highlight that merely</p>
<p>H Distribution of First Error</p>
<p>Step identified by Self-Critic</p>
<p>I Classify Solutions into Correct and Incorrect</p>
<p>Again we use Qwen2.5-72B-Instructitself to classify solutions into correct and incorrect ones.We present the system prompt in the following Figure 17: The key hyper-parameters for training are as follows:</p>
<p>• Batch size: 256</p>
<p>Figure 2 :
2
Figure 2: Data statistics before and after self-critic and self-validation filtering.</p>
<p>Figure 3 :
3
Figure 3: Scaling and multi-round performance analysis.Left panel: Data size scaling of Contrastive Critic, Direct Critic, and Bug-Injection Critic.Middle panel: Model size scaling from 1.5B to 72B parameters.Right panel: Multi-round self-evolving over 3 iterations.</p>
<p>Figure 4 :
4
Figure 4: System prompts used for different critic mechanisms.Top Left: Direct Critic directly analyzes solution correctness without any additional context.Bottom Left: Bug-Injection Critic first injects bugs (Step 1) then direct critic on bug-injected solution (Step 2).Right: Contrastive Critic first analyzes a reference solution to understand key mathematical concepts before conducting step-wise critique.</p>
<p>Figure 5 :
5
Figure 5: Comparison between Direct Critic and Contrastive Critic.Direct Critic shows blind approval of the student solution, failing to identify any errors and providing misleading approval.In contrast, Contrastive Critic first analyzes the reference solution to understand key mathematical concepts, enabling it to precisely locate the error in the student solution.By developing understanding of the underlying mathematical concepts, Contrastive Critic successfully generate an effective critique that guides the correction process to reach the correct final answer.</p>
<p>Critic, it fails to identify the first error The ineffective critic finally leads to a wrong and conflict final answer Self-Validation on the correction part of Self-Critic output successfully notices the wrong and conflict part in correction, and rejects the ineffective critic for Self-Training.</p>
<p>Figure 6 :
6
Figure 6: Case1: Self-Validation rejects an ineffective critic: Despite having access to a reference solution and using contrastive learning, the critic fails to identify Step 12 as the first error in solving a trigonometric equation.The subsequent correction leads to a conflicting final answer.The self-validation mechanism successfully detects this inconsistency and rejects this ineffective critique from the training data.</p>
<p>on the correction part of Self-Critic output also successfully accepts the effective critic for Self-Training.Self-Critic effectively identify the first errorAn effective critic successfully leads to a correct final answer, showing genuine understanding of reasoning process</p>
<p>Figure 7 :
7
Figure 7: Case2: Self-Validation accepts an effective critic: An example of effective critique that correctly identifies Step 3 as the error point where continuity requirements are mishandled.The correction follows logical mathematical reasoning and arrives at the correct final answer, which is then verified and accepted by the self-validation mechanism for training.</p>
<p>Figure 8 :Figure 9 :
89
Figure 8: An example of oversimplified bugs injected by Bug-Injection Critic: A conceptual bug involving basic misunderstanding of absolute value property.</p>
<p>Figure 16 :
16
Figure 16: Distribution of first error positions identified by our self-critic across different mathematical domains.</p>
<p>Figure 17 :
17
Figure 17: System Prompt to classify solutions into correct and incorrect ones.</p>
<p>• Correct-Correct Pairs. For</p>
<p>each correct solution, we randomly select a different correct solution as reference and generate a critique comparing the two.
Both pairing strategies promote diversity in the generated critiques, which we empiricallyvalidate for effectiveness in subsequent experiments. The self-critic function (prompttemplate in Appendix C) decomposes critique generation into four sequential stages. Stage1 (Reference Analysis
): Generate a reference analysis that captures key mathematical concepts, critical solution steps, and potential pitfalls.Stage 2 (Step-wise Critique): For each step in the solution, generate a critique by verifying mathematical and logical validity using the reference analysis, identifying error type and suggesting corrections if found, and stopping analysis upon first error detection.Stage 3 (Conclusion): Generate a conclusion indicating both solution correctness (binary) and the first error step (if any).Stage 4 (Correction): Generate a correction by following the original approach up to the error step (if any), then completing with proper correction.</p>
<p>Table 1 :
1
Performance comparison on Critic and Correct protocol.
ModelRealCriticAvg.ARC-CCollege MathGPQA GSM8K MATHMinerva MathMMLU STEMOlympiad BenchCritic on deliberately incorrect solutionsQwen2.5-72B-Instruct80.627.616.379.551.115.727.419.539.7+ SCRIT86.732.625.388.366.023.450.727.050.0o1-mini74.934.826.388.678.023.845.540.851.6Critic on balanced solutionsQwen2.5-72B-Instruct85.250.931.188.372.047.142.144.657.7+ SCRIT90.150.529.594.175.745.664.746.462.1o1-mini83.752.745.393.085.849.857.957.365.7Critic on Qwen2.5-72B-Instruct's own solutionQwen2.5-72B-Instruct93.545.932.696.783.638.359.643.461.7+ SCRIT91.345.935.396.782.538.767.545.362.9o1-mini93.947.036.896.789.940.268.553.665.8</p>
<p>Table 2 :
2
Performance comparison on Critic and Correct with Error Identification protocol.
ModelPRM800KProcessBenchAvg.GSM8K MATH Olympiad Bench OmniMathQwen2.5-72B-Instruct23.768.950.925.520.037.8+ SCRIT24.680.260.032.527.845.0o1-mini34.088.081.153.038.658.9</p>
<p>Table 3 :
3
Cross-domain generalization.SCRIT trained on domain-specific data shows strong in-domain performance and effective cross-domain transfer.CC-Acc is reported on balanced solutions.
ModelCC-Acc (Balanced)Overall AvgMath Reasoning Scientific ReasoningQwen2.5-72B-Instruct60.252.857.7+ SCRIT (Math Data)64.561.462.1+ SCRIT (Scientific Data)61.867.463.9</p>
<p>For data scaling experiments, we train SCRIT with different amounts of training examples, ranging from 10K to 170K.Both CC-Acc and EI-F1 show consistent improvements with increased training data.The CC-Acc improves from 53.0% to 58.3%, with the steepest gains in the early stage (0-20K examples) and continued but more gradual improvements afterwards.Similarly, EI-F1 increases from 37.8% to 45.1%, demonstrating that SCRIT can effectively leverage more training data to evolve its critique capabilities.</p>
<p>Table 4 :
4
Controlled ablation studies on SCRIT.Each experiment varies only the target component while keeping all other settings fixed at baseline: 10K training examples with contrastive critic and self-validation, diverse domains, all solution models, and balanced solution ratio.Red/green numbers indicate the relative performance decrease/increase.
SettingCC-AccEI-F1Baseline56.840.2Self-ValidationWithout Self-Validation56.0 (-0.8) 37.2 (-3.0)Problem DomainLimited to GSM8K + MATH 55.4 (-1.4) 38.8 (-1.4)Problem DifficultyMore Unique Answers First55.8 (-1.0) 38.1 (-2.1)Less Unique Answers First56.2 (-0.6) 42.3 (+2.1)Single Solution Modeldeepseek-math-7b-rl56.5 (-0.3) 39.8 (-0.4)mathstral-7B-v0.156.0 (-0.8) 39.2 (-1.0)Mistral-Large-Instruct56.3 (-0.5) 40.3 (+0.1)DeepSeek-V2-Chat56.3 (-0.5) 40.0 (-0.2)Qwen2.5-Math-7B56.2 (-0.6) 40.7 (+0.5)Qwen2.5-Math-1.5B56.2 (-0.6) 40.9 (+0.7)Qwen2-Math-1.5B55.9 (-0.9) 40.9 (+0.7)Good:Bad Solution Ratio0.75:0.2555.1 (-1.7) 38.1 (-2.1)0.25:0.7556.6 (-0.2) 41.0 (+0.8)</p>
<p>Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.How far can camels go? exploring the state of instruction tuning on open resources, 2023.Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu.Evaluating the performance of large language models on gaokao benchmark.arXiv preprint arXiv:2305.12474,2023.Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin.Processbench: Identifying process errors in mathematical reasoning.arXiv preprint arXiv:2412.06559,2024a.Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, and Le Sun.Critic-cot: Boosting the reasoning abilities of large language model via chain-of-thoughts critic.arXiv preprint arXiv:2408.16326,2024b.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, NanDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners.arXiv preprint arXiv:2109.01652, 2021.Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E Gonzalez, Bin Cui, andShuicheng Yan. Supercorrect: Supervising and correcting language models with error-driven insights. arXiv preprint arXiv:2410.09008, 2024.Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and RishabhAgarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprintarXiv:2408.15240, 2024.Xiaotian Zhang,
https://github.com/openai/prm800k/blob/main/prm800k/data/phase2 test.jsonl
The exact training time may vary depending on the specific hardware configuration and system load.
AcknowledgmentsThe work of Tian Ding is supported by Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone Project (No.HZQSWS-KCCYB-2024016).The work of Ruoyu Sun is supported by NSFC (No. 12326608); Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone Project (No.HZQSWS-KCCYB-2024016); University Development Fund UDF01001491, the Chinese University of Hong Kong, Shenzhen; Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (2023B1212010001).The work of Benyou Wang is supported by Shenzhen Doctoral Startup Funding (RCBS20221008093330065), Tianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC) (12326608), Shenzhen Science and Technology Program (Shenzhen Key Laboratory Grant No. ZDSYS20230626091302006), and Shenzhen Stability Science Program 2023, Shenzhen Key Lab of Multi-Modal Cognitive Computing.matching human-labeled error steps is insufficient for ensuring genuine understanding of mathematical errors.Therefore, we augment ProcessBench's protocol by requiring that models must not only identify the correct error step but also provide correction that leads to a mathematically valid solution.This stricter requirement helps ensure that models demonstrate genuine understanding of the mathematical concepts and errors involved.G.2 Allowing Step-Level FlexibilityOur second adaptation addresses an inherent ambiguity in error identification: in many cases, mathematical errors can reasonably be attributed to multiple consecutive steps.Through our analysis, we found numerous instances where the exact "error step" is debatable, with both the preceding and following steps being valid points of identification.As shown in Figures 13 to 15, these cases illustrate how mathematical errors often span multiple steps, making strict step-level matching overly rigid for meaningful evaluation..  To account for this ambiguity, we introduce a ±1 step tolerance in matching model predictions with human annotations.This modification better reflects the reality of mathematical error analysis while still maintaining rigor in evaluation.These adaptations result in a more meaningful evaluation protocol that better captures models' true understanding of mathematical errors and their ability to guide effective corrections.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>Measuring progress on scalable oversight for large language models. Jeeyoon Samuel R Bowman, Ethan Hyun, Edwin Perez, Craig Chen, Scott Pettit, Heiner, Amanda Kamil Ė Lukoši Ūt Ė, Andy Askell, Anna Jones, Chen, arXiv:2211.035402022arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. Deepseek-Ai , 2024</p>
<p>Tora: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, arXiv:2309.174522023arXiv preprint</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, arXiv:2402.140082024arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>Am-thinking-v1: Advancing the frontier of reasoning at 32b scale. Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, Xiangang Li, 2025</p>
<p>Training language models to critique with multi-agent feedback. Tian Lan, Wenwei Zhang, Chengqi Lyu, Shuaibin Li, Chen Xu, Heyan Huang, Dahua Lin, Xian-Ling Mao, Kai Chen, arXiv:2410.152872024arXiv preprint</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>. L I Jia, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Costa Shengyi, Kashif Huang, Longhui Rasul, Albert Yu, Ziju Jiang, Zihan Shen, Bin Qin, Li Dong, Yann Zhou, Guillaume Fleureau, Stanislas Lample, Polu, 2024Numinamath</p>
<p>Remax: A simple, effective, and efficient reinforcement learning method for aligning large language models. Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, Zhi-Quan Luo, Forty-first International Conference on Machine Learning. 2024</p>
<p>Preserving diversity in supervised fine-tuning of large language models. Ziniu Li, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Zhi-Quan, Ruoyu Luo, Sun, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>Nat Mcaleese, Rai Michael Pokorny, Juan Felipe Ceron, Evgenia Uribe, Maja Nitishinskaya, Jan Trebacz, Leike, arXiv:2407.00215Llm critics help catch llm bugs. -Ai Mistral, Mathstral, 2024. July 2024aarXiv preprint</p>
<p>Mistral-Ai , Mistral-large-2407. July 2024b</p>
<p>Learning to reason with LLMs. OpenAI Blog. Openai, Feb 2024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>5: A party of foundation models. Qwen-Team, Qwen2, September 2024</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.12022Gpqa: A graduate-level googleproof q&amp;a benchmark. 2023arXiv preprint</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, Jan Leike, arXiv:2206.058022022arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Li, Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Mathscale: Scaling instruction tuning for mathematical reasoning. Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei, arXiv:2403.028842024arXiv preprint</p>
<p>Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, arXiv:2501.14492Towards effectiveness-driven evaluation of language model critiques. 2025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
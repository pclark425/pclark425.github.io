<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2076 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2076</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2076</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-276725462</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.21151v1.pdf" target="_blank">A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images</a></p>
                <p><strong>Paper Abstract:</strong> This review surveys the state-of-the-art in text-to-image and image-to-image generation within the scope of generative AI. We provide a comparative analysis of three prominent architectures: Variational Autoencoders, Generative Adversarial Networks and Diffusion Models. For each, we elucidate core concepts, architectural innovations, and practical strengths and limitations, particularly for scientific image understanding. Finally, we discuss critical open challenges and potential future research directions in this rapidly evolving field.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2076.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2076.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Denoising Diffusion Probabilistic Models (and related diffusion-based generative models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of generative models that synthesize images by learning a reverse denoising process from Gaussian noise through many timesteps; widely reported here as state-of-the-art for image quality and diversity but prone to hallucination when extrapolating beyond training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Denoising diffusion probabilistic models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Diffusion Models (DDPM / score-based variants)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generative model (probabilistic diffusion / score-based)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>image generation / scientific imaging</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>synthetic images (text-to-image, image-to-image)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel to highly novel; capable of out-of-distribution generations but susceptible to hallucinations when extrapolating beyond training data</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>iterative denoising reverse process learned by a neural network (predict noise or score function) to transform Gaussian noise into data samples</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>comparison to ground truth when available (image similarity metrics such as SSIM and PSNR), benchmark datasets, and qualitative domain-expert review; conditioning alignment sometimes validated via models like CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Qualitative claim in paper: achieves state-of-the-art image quality and high diversity relative to GANs and VAEs; no numeric generation metrics (e.g., FID, IS) are reported in this review</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not quantitatively reported in this paper; validation typically uses SSIM/PSNR and expert assessment but no aggregate accuracy/precision/recall numbers provided here</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Paper states validation becomes substantially more difficult for novel / out-of-distribution scenarios; no quantitative curve reported, but reliability of validation decreases as novelty increases</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper explicitly highlights a gap: generation (high-fidelity, diverse outputs) can outpace validation capability for novel scientific scenarios, leading to hallucinations and unverified plausible-looking artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described in detail for diffusion models in this review; classifier guidance and guidance scalars are discussed but not framed as calibrated uncertainty estimates</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; review does not provide calibration metrics or analyses</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Reported qualitatively as weak: diffusion models can produce visually plausible but physically/biologically impossible images when extrapolating beyond training data; no numeric OOD metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — SSIM, PSNR, plausibility/coherence scores and domain-expert qualitative assessment are described as common proxies when ground truth is sparse</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended frequently for novel outputs; the paper states expert qualitative evaluation is essential when ground truth does not exist and that frequency should increase with output novelty</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical like scientific imaging (microscopy, MRI) — a less formalized domain where ground truth can be scarce, increasing the generation-validation gap</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Proposed strategies include rigorous V&V pipelines, use of domain-specific datasets, expert review, benchmark datasets where available, and conditioning/constraints informed by domain knowledge; no tested quantitative effectiveness reported</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Cited phenomena: hallucination, misrepresentation of scientific principles when models extrapolate beyond training data; lack of ground truth for novel scenarios; sparse/narrow curated scientific datasets</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No direct quantitative evidence contradicting the gap is presented in this review</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported; review does not provide concrete generation vs validation compute cost comparisons</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2076.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2076.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent Diffusion (Stable Diffusion / LDM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Diffusion Models (e.g., Stable Diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Diffusion models applied in a compressed latent space (using a VAE encoder/decoder) to reduce compute while preserving generation quality; commonly used with classifier-free guidance for conditioned generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>High-Resolution Image Synthesis with Latent Diffusion Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Latent Diffusion Models (Stable Diffusion variants)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generative model (latent-space diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>image generation / scientific imaging</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>synthetic images (text-to-image, image-to-image)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel; capable of creative recombinations and out-of-distribution samples but subject to same hallucination risks as pixel-space diffusion models</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>apply diffusion process to lower-dimensional latent encodings (VAE-learned latent space), then decode to pixel space; often conditioned with text/image embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>same validation approaches as other diffusion methods — image similarity metrics (SSIM, PSNR), benchmark comparisons, and expert review; alignment/semantic correctness often assessed via CLIP-like models</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Described qualitatively as high image quality with computational efficiency gains (latent-space reduces cost); no numeric metrics provided in this review</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not quantified here; relies on SSIM/PSNR and expert review when domain ground truth exists</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Validation difficulty increases with novelty; latent compression does not eliminate hallucination risk for OOD samples</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper indicates generation (high fidelity, diverse) often exceeds the ability to validate novel scientific images, particularly with scarce domain data</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not discussed for LDMs in this review</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Qualitatively can produce plausible but scientifically incorrect images when OOD; no numeric measures provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — SSIM/PSNR and plausibility/coherence proxies, plus use of CLIP embeddings for alignment checks</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Increases with novelty; recommended for scientific use-cases</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical/scientific imaging — limited formalization intensifies validation challenges</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Train on domain-specific data, apply domain constraints, incorporate expert V&V and curated benchmarks; effectiveness not quantified in review</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Same as diffusion models: hallucinations, scarcity of domain-specific benchmarks and datasets</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None presented</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Review notes latent-space methods reduce generation compute but does not quantify validation costs or ratio</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2076.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2076.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GANs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Adversarial Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adversarially trained generator/discriminator pairs that produce sharp, detailed synthetic images but are prone to training instability and mode collapse; historically important in image generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative adversarial nets</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GANs (Vanilla GAN, DCGAN, CGAN variants)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generative model (adversarial neural network)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>image generation / scientific imaging</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>synthetic images (image-to-image, conditional text-to-image via CGANs)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel; can produce diverse outputs but may collapse to limited modes (reduced novelty/diversity) under training issues</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>generator network maps noise (and optional conditioning) to images, trained adversarially against a discriminator network</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>benchmark metrics (commonly FID/IS in literature though not enumerated here), image similarity metrics (SSIM/PSNR), and domain expert qualitative assessment; conditional alignment checked through conditional inputs</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Qualitatively high image sharpness and detail; review states diffusion models often surpass GANs in overall detail even if GANs can be sharp; no numeric generation metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not quantitatively reported; validation suffers when GANs undergo mode collapse because generated diversity no longer represents data distribution</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Mode collapse and instability can reduce generation diversity especially for novel outputs; validation becomes unreliable if generator has collapsed modes</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Review highlights GANs' generation quality can be high but training instability and mode collapse complicate reliable validation and coverage of data distribution</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described for GANs in this review</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; GANs can fail to generalize and may produce unrealistic artifacts OOD</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — SSIM/PSNR and common generative model metrics in literature (FID/IS) are implied as proxy measures though not numerically reported here</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended, especially when using GAN outputs for sensitive scientific tasks or for novel-looking outputs</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical/scientific imaging — informal domain increases risk from GAN hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Architectural improvements (progressive GANs, attention), conditioning techniques, and rigorous V&V; review notes stability improvements are an area of active research but does not provide empirical validation of strategies</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Training instability and mode collapse reduce trust in generated outputs and make validation harder, supporting a generation-validation gap</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None provided in review</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2076.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2076.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAEs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Latent-variable generative models that encode data into a continuous latent space and decode samples by sampling the latent prior; typically produce blurrier images than GANs or diffusion models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Auto-encoding variational bayes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Variational Autoencoders (VAEs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generative model (variational latent-variable model)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>image generation / scientific imaging</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>synthetic images (image reconstruction, image-to-image generation)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>generally in-distribution / incremental novelty; less able to produce very high-fidelity novel outputs compared to diffusion/GANs</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>encode inputs into latent distribution; sample latents and decode to generate images, trained by optimizing ELBO (reconstruction + KL loss)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>image similarity metrics (SSIM, PSNR), reconstruction error, and domain-expert qualitative evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Qualitatively lower image fidelity (more blur) than diffusion models and GANs; diversity can be limited in practice; no numeric metrics provided here</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not numerically reported; validation via reconstruction metrics is straightforward when ground-truth exists but limited for novel outputs</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>VAEs perform better for in-distribution reconstruction tasks; generation and validation of highly novel outputs are limited and poorly supported</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>The paper notes VAEs' more stable training and tractable likelihoods help validation for in-distribution tasks, but their lower-fidelity generation limits usefulness for high-quality scientific image synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Implicit via probabilistic latent representation (KL term) but explicit calibrated uncertainty measures not discussed in review</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not evaluated in review</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Tends to be weak; VAEs are less capable of producing high-quality OOD novel images</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — reconstruction error and SSIM/PSNR are used as proxies</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for novel outputs; less critical for reconstruction tasks where ground truth exists</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical/scientific imaging</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Using VAEs as components of larger systems (e.g., latent spaces for diffusion) and combining with domain constraints; effectiveness not quantified</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>VAEs' limited fidelity implies generation is less capable for high-detail scientific images, but their validation is easier when ground truth is available — the gap exists in the other direction (generation weaker than validation) for VAEs</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>VAEs' tractable likelihoods and stable training provide cases where validation is stronger relative to generation quality</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2076.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2076.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Classifier Guidance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classifier-guided Diffusion (classifier guidance using noisy-image classifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditioning technique that steers diffusion sampling toward target classes by using gradients from a classifier trained to predict class labels from noisy inputs; can improve fidelity to conditional targets but adds dependence on classifier quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Classifier-guided diffusion (classifier guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>conditioning/guidance component for generative diffusion models</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>image generation / conditional generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>conditioned synthetic images (class-conditioned or attribute-conditioned)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Provides better adherence to conditioning but does not itself invent new scientific concepts beyond what model/classifier permit</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>use gradients of a classifier f_phi(y | x_t, t) on noisy inputs to perturb reverse diffusion mean toward the target class (adds term s * grad log f_phi)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Implicitly validated via classifier accuracy on noisy inputs and by downstream image similarity/semantic alignment checks (e.g., via CLIP or human review); review does not provide numeric classifier-validation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported qualitatively to improve conditional fidelity; no quantitative performance figures provided in review</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not numerically reported; depends on classifier performance which is not quantified here</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>If classifier is trained on in-distribution classes, guidance may fail for novel or out-of-distribution classes; validation degrades as target becomes more novel</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Guided generation can increase conditional correctness but ties validation to the classifier's generalization ability — a potential bottleneck for novel conditions</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not discussed; classifier gradients are used deterministically for steering, not as calibrated uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Guidance relies on classifier; if classifier poorly generalizes OOD, both generation and validation reliability fall</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Classifier accuracy on noisy inputs and semantic alignment scores (e.g., CLIP) are used as proxies</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended, especially for novel conditional targets</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical/scientific imaging</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Use classifier-free guidance to avoid dependence on separate classifiers, or improve classifier training on domain data; review describes classifier-free guidance as an improvement</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Dependency on classifier generalization is a validation bottleneck for novel conditions; review highlights this risk qualitatively</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No quantitative contradiction provided</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2076.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2076.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Classifier-free guidance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classifier-free Diffusion Guidance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A guidance technique that trains a single diffusion model to produce both conditional and unconditional outputs and mixes them at sampling time to steer generation, avoiding a separately trained classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Classifier-free diffusion guidance</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Classifier-free guidance</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>conditioning/guidance technique for diffusion models</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>image generation / multimodal generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>conditioned synthetic images steered by embeddings (text, image features)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Enables stronger conditional adherence without separate classifier bias; novelty of outputs follows underlying diffusion model capability</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>train one model with and without conditioning and combine conditional/unconditional predictions at inference with a guidance scale s: εθ(x_t|y) = εθ(x_t|0) + s*(εθ(x_t|y) - εθ(x_t|0))</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation uses standard image similarity metrics, CLIP alignment, and expert review; review notes classifier-free guidance is advantageous because it simplifies training and reduces some bias sources</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported qualitatively to improve conditional control and generation fidelity; no numerical metrics provided in the review</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not quantitatively reported</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>The method reduces bias from separate classifiers but does not inherently solve validation challenges for novel scientific outputs</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper argues classifier-free guidance streamlines generation and may mitigate some validation bias introduced by external classifiers, but generation-validation gap for novel outputs remains</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not discussed</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; still subject to limitations of base model generalization</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>CLIP alignment and SSIM/PSNR are mentioned as common validation proxies</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for novel outputs</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical/scientific imaging</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Described as preferable to classifier-guidance to reduce extra bias from separate classifiers; still requires domain-specific V&V</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>While reducing classifier bias, classifier-free guidance does not eliminate hallucinations when base model extrapolates beyond training data</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No quantitative contradicting evidence presented</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2076.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2076.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Language–Image Pretraining (CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal vision-language model trained contrastively on image-text pairs to produce embeddings that align text and images; used as a benchmark and alignment tool for text-to-image models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multimodal vision-language model (contrastive embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multimodal alignment for image generation / validation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>embeddings and alignment scores indicating semantic match between text and images</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Not an output generator of images per se; novelty pertains to alignment capability across modalities</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (used to score/alignment rather than generate images); used as conditioning or evaluation metric for generative models</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used as an automatic proxy to assess semantic alignment of generated images with prompts (e.g., ranking or score thresholds), and as conditioning during training to align generated images with text</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable as a generator; described as a benchmark for alignment and widely used to improve text-to-image fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Used as proxy validation metric for prompt-image alignment; review does not report numeric sensitivity/specificity or error rates when used as a validator for scientific images</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>As a learned alignment metric, CLIP's reliability can degrade for domain-specific or out-of-distribution scientific content not well represented in its training data; review notes general alignment usefulness but warns domain mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>CLIP can be used to validate semantic alignment, but review emphasizes that semantic alignment (via CLIP) is only a proxy and may not verify physical/biological correctness of scientific images</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not discussed in this review for CLIP when used as validator</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Performance likely degrades on specialized scientific imagery (paper notes domain mismatch and sparse curated scientific datasets), but no numeric OOD metrics are provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — CLIP similarity scores are used as proxies for prompt-image alignment and plausibility</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>High for domain-specific scientific outputs where CLIP may be unreliable</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical/scientific imaging</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Combine CLIP-based checks with domain-specific validation, expert review, and ground-truth comparisons where possible</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Review notes CLIP is a useful benchmark but that alignment scores are insufficient for scientific validity and can miss physically impossible content</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No quantitative evidence contradicting the limitation; CLIP presented as helpful but insufficient alone</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2076.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2076.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DALL-E family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DALL·E / DALL·E 2 / DALL·E 3 (OpenAI text-to-image models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A series of transformer-based and diffusion-backed text-to-image models that map textual prompts to images; later versions incorporate CLIP and other alignment methods to improve text-image fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Zero-shot text-to-image generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DALL·E family (DALL·E, DALL·E 2, DALL·E 3)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>text-to-image generative models (transformer/LLM-derived and diffusion-based pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>text-to-image generation / multimodal generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>synthetic images conditioned on text prompts</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Capable of producing creative, highly novel visual combinations of concepts (out-of-distribution recombinations), though scientific correctness is not guaranteed</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>transformer/LLM-based text understanding combined with image generation (DALL·E2 incorporates CLIP for alignment; later versions use improved diffusion pipelines and LLM-driven prompt understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Alignment via CLIP and human evaluation for semantic correctness; image-quality assessments via standard image metrics and qualitative inspection; review does not provide numeric validation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Described historically as groundbreaking for generating coherent novel images from text; review states successive versions improved text comprehension and image quality but provides no numeric metrics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not quantitatively reported in this review; alignment improvements attributed to CLIP and larger datasets but no explicit accuracy/precision metrics given</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>As prompts request more novel/out-of-distribution phenomena, models can hallucinate or produce scientifically incorrect images; validation becomes harder without ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper notes alignment improvements but also emphasizes persistent risk that visually plausible outputs may be scientifically invalid, indicating a generation-validation gap for scientific content</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not discussed</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Capable of creative OOD recombinations but can produce physically impossible or misleading scientific images when asked to extrapolate beyond trained concepts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>CLIP alignment scores, SSIM/PSNR, and human expert evaluation used as proxies</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended, particularly for scientific prompts and novel hypotheses/images</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical/scientific imaging when applied to science; review warns domain mismatch risks</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Improved prompt understanding via LLM components, CLIP alignment, and human-in-the-loop verification; domain-specific training datasets recommended</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Review describes hallucination and misrepresentation risks when models extrapolate past training data and notes scarcity of domain-specific benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No quantitative contradiction presented</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Denoising diffusion probabilistic models <em>(Rating: 2)</em></li>
                <li>High-Resolution Image Synthesis with Latent Diffusion Models <em>(Rating: 2)</em></li>
                <li>Generative adversarial nets <em>(Rating: 2)</em></li>
                <li>Auto-encoding variational bayes <em>(Rating: 2)</em></li>
                <li>Zero-shot text-to-image generation <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Classifier-free diffusion guidance <em>(Rating: 2)</em></li>
                <li>Score-based generative modeling through stochastic differential equations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2076",
    "paper_id": "paper-276725462",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "Diffusion Models",
            "name_full": "Denoising Diffusion Probabilistic Models (and related diffusion-based generative models)",
            "brief_description": "A class of generative models that synthesize images by learning a reverse denoising process from Gaussian noise through many timesteps; widely reported here as state-of-the-art for image quality and diversity but prone to hallucination when extrapolating beyond training data.",
            "citation_title": "Denoising diffusion probabilistic models",
            "mention_or_use": "mention",
            "system_name": "Diffusion Models (DDPM / score-based variants)",
            "system_type": "generative model (probabilistic diffusion / score-based)",
            "scientific_domain": "image generation / scientific imaging",
            "output_type": "synthetic images (text-to-image, image-to-image)",
            "novelty_level": "moderately novel to highly novel; capable of out-of-distribution generations but susceptible to hallucinations when extrapolating beyond training data",
            "generation_method": "iterative denoising reverse process learned by a neural network (predict noise or score function) to transform Gaussian noise into data samples",
            "validation_method": "comparison to ground truth when available (image similarity metrics such as SSIM and PSNR), benchmark datasets, and qualitative domain-expert review; conditioning alignment sometimes validated via models like CLIP",
            "generation_performance": "Qualitative claim in paper: achieves state-of-the-art image quality and high diversity relative to GANs and VAEs; no numeric generation metrics (e.g., FID, IS) are reported in this review",
            "validation_performance": "Not quantitatively reported in this paper; validation typically uses SSIM/PSNR and expert assessment but no aggregate accuracy/precision/recall numbers provided here",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Paper states validation becomes substantially more difficult for novel / out-of-distribution scenarios; no quantitative curve reported, but reliability of validation decreases as novelty increases",
            "generation_validation_comparison": "Paper explicitly highlights a gap: generation (high-fidelity, diverse outputs) can outpace validation capability for novel scientific scenarios, leading to hallucinations and unverified plausible-looking artifacts",
            "uncertainty_quantification": "Not described in detail for diffusion models in this review; classifier guidance and guidance scalars are discussed but not framed as calibrated uncertainty estimates",
            "calibration_quality": "Not reported; review does not provide calibration metrics or analyses",
            "out_of_distribution_performance": "Reported qualitatively as weak: diffusion models can produce visually plausible but physically/biologically impossible images when extrapolating beyond training data; no numeric OOD metrics provided",
            "validation_proxy_metrics": "Yes — SSIM, PSNR, plausibility/coherence scores and domain-expert qualitative assessment are described as common proxies when ground truth is sparse",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended frequently for novel outputs; the paper states expert qualitative evaluation is essential when ground truth does not exist and that frequency should increase with output novelty",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical like scientific imaging (microscopy, MRI) — a less formalized domain where ground truth can be scarce, increasing the generation-validation gap",
            "gap_mitigation_strategies": "Proposed strategies include rigorous V&V pipelines, use of domain-specific datasets, expert review, benchmark datasets where available, and conditioning/constraints informed by domain knowledge; no tested quantitative effectiveness reported",
            "evidence_supporting_gap": "Cited phenomena: hallucination, misrepresentation of scientific principles when models extrapolate beyond training data; lack of ground truth for novel scenarios; sparse/narrow curated scientific datasets",
            "evidence_contradicting_gap": "No direct quantitative evidence contradicting the gap is presented in this review",
            "computational_cost_ratio": "Not reported; review does not provide concrete generation vs validation compute cost comparisons",
            "uuid": "e2076.0"
        },
        {
            "name_short": "Latent Diffusion (Stable Diffusion / LDM)",
            "name_full": "Latent Diffusion Models (e.g., Stable Diffusion)",
            "brief_description": "Diffusion models applied in a compressed latent space (using a VAE encoder/decoder) to reduce compute while preserving generation quality; commonly used with classifier-free guidance for conditioned generation.",
            "citation_title": "High-Resolution Image Synthesis with Latent Diffusion Models",
            "mention_or_use": "mention",
            "system_name": "Latent Diffusion Models (Stable Diffusion variants)",
            "system_type": "generative model (latent-space diffusion)",
            "scientific_domain": "image generation / scientific imaging",
            "output_type": "synthetic images (text-to-image, image-to-image)",
            "novelty_level": "moderately novel; capable of creative recombinations and out-of-distribution samples but subject to same hallucination risks as pixel-space diffusion models",
            "generation_method": "apply diffusion process to lower-dimensional latent encodings (VAE-learned latent space), then decode to pixel space; often conditioned with text/image embeddings",
            "validation_method": "same validation approaches as other diffusion methods — image similarity metrics (SSIM, PSNR), benchmark comparisons, and expert review; alignment/semantic correctness often assessed via CLIP-like models",
            "generation_performance": "Described qualitatively as high image quality with computational efficiency gains (latent-space reduces cost); no numeric metrics provided in this review",
            "validation_performance": "Not quantified here; relies on SSIM/PSNR and expert review when domain ground truth exists",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Validation difficulty increases with novelty; latent compression does not eliminate hallucination risk for OOD samples",
            "generation_validation_comparison": "Paper indicates generation (high fidelity, diverse) often exceeds the ability to validate novel scientific images, particularly with scarce domain data",
            "uncertainty_quantification": "Not discussed for LDMs in this review",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Qualitatively can produce plausible but scientifically incorrect images when OOD; no numeric measures provided",
            "validation_proxy_metrics": "Yes — SSIM/PSNR and plausibility/coherence proxies, plus use of CLIP embeddings for alignment checks",
            "human_validation_required": true,
            "human_validation_frequency": "Increases with novelty; recommended for scientific use-cases",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical/scientific imaging — limited formalization intensifies validation challenges",
            "gap_mitigation_strategies": "Train on domain-specific data, apply domain constraints, incorporate expert V&V and curated benchmarks; effectiveness not quantified in review",
            "evidence_supporting_gap": "Same as diffusion models: hallucinations, scarcity of domain-specific benchmarks and datasets",
            "evidence_contradicting_gap": "None presented",
            "computational_cost_ratio": "Review notes latent-space methods reduce generation compute but does not quantify validation costs or ratio",
            "uuid": "e2076.1"
        },
        {
            "name_short": "GANs",
            "name_full": "Generative Adversarial Networks",
            "brief_description": "Adversarially trained generator/discriminator pairs that produce sharp, detailed synthetic images but are prone to training instability and mode collapse; historically important in image generation.",
            "citation_title": "Generative adversarial nets",
            "mention_or_use": "mention",
            "system_name": "GANs (Vanilla GAN, DCGAN, CGAN variants)",
            "system_type": "generative model (adversarial neural network)",
            "scientific_domain": "image generation / scientific imaging",
            "output_type": "synthetic images (image-to-image, conditional text-to-image via CGANs)",
            "novelty_level": "moderately novel; can produce diverse outputs but may collapse to limited modes (reduced novelty/diversity) under training issues",
            "generation_method": "generator network maps noise (and optional conditioning) to images, trained adversarially against a discriminator network",
            "validation_method": "benchmark metrics (commonly FID/IS in literature though not enumerated here), image similarity metrics (SSIM/PSNR), and domain expert qualitative assessment; conditional alignment checked through conditional inputs",
            "generation_performance": "Qualitatively high image sharpness and detail; review states diffusion models often surpass GANs in overall detail even if GANs can be sharp; no numeric generation metrics provided",
            "validation_performance": "Not quantitatively reported; validation suffers when GANs undergo mode collapse because generated diversity no longer represents data distribution",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Mode collapse and instability can reduce generation diversity especially for novel outputs; validation becomes unreliable if generator has collapsed modes",
            "generation_validation_comparison": "Review highlights GANs' generation quality can be high but training instability and mode collapse complicate reliable validation and coverage of data distribution",
            "uncertainty_quantification": "Not described for GANs in this review",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Not quantified; GANs can fail to generalize and may produce unrealistic artifacts OOD",
            "validation_proxy_metrics": "Yes — SSIM/PSNR and common generative model metrics in literature (FID/IS) are implied as proxy measures though not numerically reported here",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended, especially when using GAN outputs for sensitive scientific tasks or for novel-looking outputs",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical/scientific imaging — informal domain increases risk from GAN hallucinations",
            "gap_mitigation_strategies": "Architectural improvements (progressive GANs, attention), conditioning techniques, and rigorous V&V; review notes stability improvements are an area of active research but does not provide empirical validation of strategies",
            "evidence_supporting_gap": "Training instability and mode collapse reduce trust in generated outputs and make validation harder, supporting a generation-validation gap",
            "evidence_contradicting_gap": "None provided in review",
            "computational_cost_ratio": "Not reported",
            "uuid": "e2076.2"
        },
        {
            "name_short": "VAEs",
            "name_full": "Variational Autoencoders",
            "brief_description": "Latent-variable generative models that encode data into a continuous latent space and decode samples by sampling the latent prior; typically produce blurrier images than GANs or diffusion models.",
            "citation_title": "Auto-encoding variational bayes",
            "mention_or_use": "mention",
            "system_name": "Variational Autoencoders (VAEs)",
            "system_type": "generative model (variational latent-variable model)",
            "scientific_domain": "image generation / scientific imaging",
            "output_type": "synthetic images (image reconstruction, image-to-image generation)",
            "novelty_level": "generally in-distribution / incremental novelty; less able to produce very high-fidelity novel outputs compared to diffusion/GANs",
            "generation_method": "encode inputs into latent distribution; sample latents and decode to generate images, trained by optimizing ELBO (reconstruction + KL loss)",
            "validation_method": "image similarity metrics (SSIM, PSNR), reconstruction error, and domain-expert qualitative evaluation",
            "generation_performance": "Qualitatively lower image fidelity (more blur) than diffusion models and GANs; diversity can be limited in practice; no numeric metrics provided here",
            "validation_performance": "Not numerically reported; validation via reconstruction metrics is straightforward when ground-truth exists but limited for novel outputs",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "VAEs perform better for in-distribution reconstruction tasks; generation and validation of highly novel outputs are limited and poorly supported",
            "generation_validation_comparison": "The paper notes VAEs' more stable training and tractable likelihoods help validation for in-distribution tasks, but their lower-fidelity generation limits usefulness for high-quality scientific image synthesis",
            "uncertainty_quantification": "Implicit via probabilistic latent representation (KL term) but explicit calibrated uncertainty measures not discussed in review",
            "calibration_quality": "Not evaluated in review",
            "out_of_distribution_performance": "Tends to be weak; VAEs are less capable of producing high-quality OOD novel images",
            "validation_proxy_metrics": "Yes — reconstruction error and SSIM/PSNR are used as proxies",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for novel outputs; less critical for reconstruction tasks where ground truth exists",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical/scientific imaging",
            "gap_mitigation_strategies": "Using VAEs as components of larger systems (e.g., latent spaces for diffusion) and combining with domain constraints; effectiveness not quantified",
            "evidence_supporting_gap": "VAEs' limited fidelity implies generation is less capable for high-detail scientific images, but their validation is easier when ground truth is available — the gap exists in the other direction (generation weaker than validation) for VAEs",
            "evidence_contradicting_gap": "VAEs' tractable likelihoods and stable training provide cases where validation is stronger relative to generation quality",
            "computational_cost_ratio": "Not reported",
            "uuid": "e2076.3"
        },
        {
            "name_short": "Classifier Guidance",
            "name_full": "Classifier-guided Diffusion (classifier guidance using noisy-image classifiers)",
            "brief_description": "A conditioning technique that steers diffusion sampling toward target classes by using gradients from a classifier trained to predict class labels from noisy inputs; can improve fidelity to conditional targets but adds dependence on classifier quality.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Classifier-guided diffusion (classifier guidance)",
            "system_type": "conditioning/guidance component for generative diffusion models",
            "scientific_domain": "image generation / conditional generation",
            "output_type": "conditioned synthetic images (class-conditioned or attribute-conditioned)",
            "novelty_level": "Provides better adherence to conditioning but does not itself invent new scientific concepts beyond what model/classifier permit",
            "generation_method": "use gradients of a classifier f_phi(y | x_t, t) on noisy inputs to perturb reverse diffusion mean toward the target class (adds term s * grad log f_phi)",
            "validation_method": "Implicitly validated via classifier accuracy on noisy inputs and by downstream image similarity/semantic alignment checks (e.g., via CLIP or human review); review does not provide numeric classifier-validation metrics",
            "generation_performance": "Reported qualitatively to improve conditional fidelity; no quantitative performance figures provided in review",
            "validation_performance": "Not numerically reported; depends on classifier performance which is not quantified here",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "If classifier is trained on in-distribution classes, guidance may fail for novel or out-of-distribution classes; validation degrades as target becomes more novel",
            "generation_validation_comparison": "Guided generation can increase conditional correctness but ties validation to the classifier's generalization ability — a potential bottleneck for novel conditions",
            "uncertainty_quantification": "Not discussed; classifier gradients are used deterministically for steering, not as calibrated uncertainty",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Guidance relies on classifier; if classifier poorly generalizes OOD, both generation and validation reliability fall",
            "validation_proxy_metrics": "Classifier accuracy on noisy inputs and semantic alignment scores (e.g., CLIP) are used as proxies",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended, especially for novel conditional targets",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical/scientific imaging",
            "gap_mitigation_strategies": "Use classifier-free guidance to avoid dependence on separate classifiers, or improve classifier training on domain data; review describes classifier-free guidance as an improvement",
            "evidence_supporting_gap": "Dependency on classifier generalization is a validation bottleneck for novel conditions; review highlights this risk qualitatively",
            "evidence_contradicting_gap": "No quantitative contradiction provided",
            "computational_cost_ratio": "Not reported",
            "uuid": "e2076.4"
        },
        {
            "name_short": "Classifier-free guidance",
            "name_full": "Classifier-free Diffusion Guidance",
            "brief_description": "A guidance technique that trains a single diffusion model to produce both conditional and unconditional outputs and mixes them at sampling time to steer generation, avoiding a separately trained classifier.",
            "citation_title": "Classifier-free diffusion guidance",
            "mention_or_use": "mention",
            "system_name": "Classifier-free guidance",
            "system_type": "conditioning/guidance technique for diffusion models",
            "scientific_domain": "image generation / multimodal generation",
            "output_type": "conditioned synthetic images steered by embeddings (text, image features)",
            "novelty_level": "Enables stronger conditional adherence without separate classifier bias; novelty of outputs follows underlying diffusion model capability",
            "generation_method": "train one model with and without conditioning and combine conditional/unconditional predictions at inference with a guidance scale s: εθ(x_t|y) = εθ(x_t|0) + s*(εθ(x_t|y) - εθ(x_t|0))",
            "validation_method": "Validation uses standard image similarity metrics, CLIP alignment, and expert review; review notes classifier-free guidance is advantageous because it simplifies training and reduces some bias sources",
            "generation_performance": "Reported qualitatively to improve conditional control and generation fidelity; no numerical metrics provided in the review",
            "validation_performance": "Not quantitatively reported",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "The method reduces bias from separate classifiers but does not inherently solve validation challenges for novel scientific outputs",
            "generation_validation_comparison": "Paper argues classifier-free guidance streamlines generation and may mitigate some validation bias introduced by external classifiers, but generation-validation gap for novel outputs remains",
            "uncertainty_quantification": "Not discussed",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Not quantified; still subject to limitations of base model generalization",
            "validation_proxy_metrics": "CLIP alignment and SSIM/PSNR are mentioned as common validation proxies",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for novel outputs",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical/scientific imaging",
            "gap_mitigation_strategies": "Described as preferable to classifier-guidance to reduce extra bias from separate classifiers; still requires domain-specific V&V",
            "evidence_supporting_gap": "While reducing classifier bias, classifier-free guidance does not eliminate hallucinations when base model extrapolates beyond training data",
            "evidence_contradicting_gap": "No quantitative contradicting evidence presented",
            "computational_cost_ratio": "Not reported",
            "uuid": "e2076.5"
        },
        {
            "name_short": "CLIP",
            "name_full": "Contrastive Language–Image Pretraining (CLIP)",
            "brief_description": "A multimodal vision-language model trained contrastively on image-text pairs to produce embeddings that align text and images; used as a benchmark and alignment tool for text-to-image models.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "mention",
            "system_name": "CLIP",
            "system_type": "multimodal vision-language model (contrastive embedding)",
            "scientific_domain": "multimodal alignment for image generation / validation",
            "output_type": "embeddings and alignment scores indicating semantic match between text and images",
            "novelty_level": "Not an output generator of images per se; novelty pertains to alignment capability across modalities",
            "generation_method": "N/A (used to score/alignment rather than generate images); used as conditioning or evaluation metric for generative models",
            "validation_method": "Used as an automatic proxy to assess semantic alignment of generated images with prompts (e.g., ranking or score thresholds), and as conditioning during training to align generated images with text",
            "generation_performance": "Not applicable as a generator; described as a benchmark for alignment and widely used to improve text-to-image fidelity",
            "validation_performance": "Used as proxy validation metric for prompt-image alignment; review does not report numeric sensitivity/specificity or error rates when used as a validator for scientific images",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "As a learned alignment metric, CLIP's reliability can degrade for domain-specific or out-of-distribution scientific content not well represented in its training data; review notes general alignment usefulness but warns domain mismatch",
            "generation_validation_comparison": "CLIP can be used to validate semantic alignment, but review emphasizes that semantic alignment (via CLIP) is only a proxy and may not verify physical/biological correctness of scientific images",
            "uncertainty_quantification": "Not discussed in this review for CLIP when used as validator",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Performance likely degrades on specialized scientific imagery (paper notes domain mismatch and sparse curated scientific datasets), but no numeric OOD metrics are provided",
            "validation_proxy_metrics": "Yes — CLIP similarity scores are used as proxies for prompt-image alignment and plausibility",
            "human_validation_required": true,
            "human_validation_frequency": "High for domain-specific scientific outputs where CLIP may be unreliable",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical/scientific imaging",
            "gap_mitigation_strategies": "Combine CLIP-based checks with domain-specific validation, expert review, and ground-truth comparisons where possible",
            "evidence_supporting_gap": "Review notes CLIP is a useful benchmark but that alignment scores are insufficient for scientific validity and can miss physically impossible content",
            "evidence_contradicting_gap": "No quantitative evidence contradicting the limitation; CLIP presented as helpful but insufficient alone",
            "computational_cost_ratio": "Not reported",
            "uuid": "e2076.6"
        },
        {
            "name_short": "DALL-E family",
            "name_full": "DALL·E / DALL·E 2 / DALL·E 3 (OpenAI text-to-image models)",
            "brief_description": "A series of transformer-based and diffusion-backed text-to-image models that map textual prompts to images; later versions incorporate CLIP and other alignment methods to improve text-image fidelity.",
            "citation_title": "Zero-shot text-to-image generation",
            "mention_or_use": "mention",
            "system_name": "DALL·E family (DALL·E, DALL·E 2, DALL·E 3)",
            "system_type": "text-to-image generative models (transformer/LLM-derived and diffusion-based pipelines)",
            "scientific_domain": "text-to-image generation / multimodal generation",
            "output_type": "synthetic images conditioned on text prompts",
            "novelty_level": "Capable of producing creative, highly novel visual combinations of concepts (out-of-distribution recombinations), though scientific correctness is not guaranteed",
            "generation_method": "transformer/LLM-based text understanding combined with image generation (DALL·E2 incorporates CLIP for alignment; later versions use improved diffusion pipelines and LLM-driven prompt understanding)",
            "validation_method": "Alignment via CLIP and human evaluation for semantic correctness; image-quality assessments via standard image metrics and qualitative inspection; review does not provide numeric validation metrics",
            "generation_performance": "Described historically as groundbreaking for generating coherent novel images from text; review states successive versions improved text comprehension and image quality but provides no numeric metrics",
            "validation_performance": "Not quantitatively reported in this review; alignment improvements attributed to CLIP and larger datasets but no explicit accuracy/precision metrics given",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "As prompts request more novel/out-of-distribution phenomena, models can hallucinate or produce scientifically incorrect images; validation becomes harder without ground truth",
            "generation_validation_comparison": "Paper notes alignment improvements but also emphasizes persistent risk that visually plausible outputs may be scientifically invalid, indicating a generation-validation gap for scientific content",
            "uncertainty_quantification": "Not discussed",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Capable of creative OOD recombinations but can produce physically impossible or misleading scientific images when asked to extrapolate beyond trained concepts",
            "validation_proxy_metrics": "CLIP alignment scores, SSIM/PSNR, and human expert evaluation used as proxies",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended, particularly for scientific prompts and novel hypotheses/images",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical/scientific imaging when applied to science; review warns domain mismatch risks",
            "gap_mitigation_strategies": "Improved prompt understanding via LLM components, CLIP alignment, and human-in-the-loop verification; domain-specific training datasets recommended",
            "evidence_supporting_gap": "Review describes hallucination and misrepresentation risks when models extrapolate past training data and notes scarcity of domain-specific benchmarks",
            "evidence_contradicting_gap": "No quantitative contradiction presented",
            "uuid": "e2076.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Denoising diffusion probabilistic models",
            "rating": 2
        },
        {
            "paper_title": "High-Resolution Image Synthesis with Latent Diffusion Models",
            "rating": 2
        },
        {
            "paper_title": "Generative adversarial nets",
            "rating": 2
        },
        {
            "paper_title": "Auto-encoding variational bayes",
            "rating": 2
        },
        {
            "paper_title": "Zero-shot text-to-image generation",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Classifier-free diffusion guidance",
            "rating": 2
        },
        {
            "paper_title": "Score-based generative modeling through stochastic differential equations",
            "rating": 1
        }
    ],
    "cost": 0.0195755,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A REVIEW ON GENERATIVE AI FOR TEXT-TO-IMAGE AND IMAGE-TO-IMAGE GENERATION AND IMPLICATIONS TO SCIENTIFIC IMAGES
March 14, 2025</p>
<p>Zineb Sordo zsordo@lbl.gov 
Eric Chagnon echagnon@lbl.gov 
Daniela Ushizima dushizima@lbl.gov </p>
<p>Applied Math and Computational Research Lawrence Berkeley National Laboratory Berkeley
94720CA</p>
<p>Applied Math and Computational Research Lawrence Berkeley National Laboratory Berkeley
94720CA</p>
<p>Applied Math and Computational Research Lawrence Berkeley National Laboratory Berkeley
94720CA</p>
<p>A REVIEW ON GENERATIVE AI FOR TEXT-TO-IMAGE AND IMAGE-TO-IMAGE GENERATION AND IMPLICATIONS TO SCIENTIFIC IMAGES
March 14, 202593279B86D04D1E25EB8DE2DDC02FA5E2arXiv:2502.21151v2[cs.CV]
This review surveys the state-of-the-art in text-to-image and image-to-image generation within the scope of generative AI.We provide a comparative analysis of three prominent architectures: Variational Autoencoders, Generative Adversarial Networks and Diffusion Models.For each, we elucidate core concepts, architectural innovations, and practical strengths and limitations, particularly for scientific image understanding.Finally, we discuss critical open challenges and potential future research directions in this rapidly evolving field.</p>
<p>Introduction</p>
<p>Generative AI (genAI) has emerged as a powerful tool with the ability to create novel digital content, including images, text, and music [5].However, using generative AI to create scientific images of phenomena unseen by the model continues to be challenging, and prone to hallucination [43] and misrepresentation of scientific principles.If the model extrapolates beyond its training data, it can generate images that, while visually plausible, are physically or biologically impossible [37].This can lead to the propagation of inaccurate scientific concepts and hinder genuine discovery [19,20].This paper overviews the major milestones in the last few years, then describes how Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs) and Diffusion Models have revolutionized these areas.Finally, we delineate potential avenues for verification and validation.</p>
<p>Overall, this paper focuses on two key subdomains: text-to-image and image-to-image generation.</p>
<p>This review aims to:</p>
<p>• Analyze their applications in text-to-image and image-to-image generation.</p>
<p>• Compare and contrast the strengths and weaknesses of these approaches for scientific data generation.</p>
<p>• Discuss the challenges and future directions of research in this field.</p>
<p>Background</p>
<p>This section provides an overview of the key advancements in text-to-image and image-to-image generation technologies, particularly considering the technologies from major tech companies, such as Google, Meta, Microsoft and OpenAI.We highlight significant software releases and the underlying algorithms that have shaped the landscape of generative AI from 2021 to 2024.</p>
<p>In 2021, OpenAI introduced DALL-E, a groundbreaking model that utilized a variant of the GPT-3 architecture, which is a large language model (LLM), to generate images from textual descriptions.DALL-E employed a transformer-based architecture, leveraging the principles of attention mechanisms to understand and synthesize complex relationships between text and visual elements.The model was trained on a diverse dataset of text-image pairs, enabling it to create novel images that often combined disparate concepts in coherent and imaginative ways.This marked a significant advancement in generative models, setting the stage for future developments in text-to-image synthesis [30,25].</p>
<p>Figure 1: Major highlights of language and multimodal models, with less focus on text-to-image generation models [38].</p>
<p>The year 2022 saw significant advancements in text-to-image generation technologies.Google introduced Imagen, a diffusion-based model that generates high-quality images from textual prompts.Imagen utilized a two-step process involving a denoising diffusion probabilistic model (DDPM), which iteratively refined a random noise image into a coherent visual representation based on the input text.While Imagen did not directly employ Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs) [8], its diffusion approach presented an alternative to these traditional generative frameworks.Meanwhile, OpenAI released DALL-E 2, which improved upon its predecessor by incorporating CLIP (Contrastive Language-Image Pretraining) [25] to better align the generated images with textual descriptions [28].CLIP itself is a multi-modal vision and language model that understands and relates textual and visual information.CLIP has since become the benchmark for text-to-image and multi modal text+image-to-image generation.Meta's Make-A-Scene also emerged, allowing users to have more control over the composition of generated images through scene graphs, enhancing the interactivity of the image creation process [6].Additionally, Microsoft integrated OpenAI's models into its Azure platform, making these advanced capabilities more accessible to developers and businesses.</p>
<p>In 2023, the landscape of generative AI continued to evolve with notable releases and innovations.Google unveiled Imagen Video, extending the capabilities of its diffusion models to generate videos from text prompts, thus introducing temporal coherence and movement to the generative process [10].OpenAI launched DALL-E 3, which featured enhanced text comprehension and image generation capabilities, further refining the alignment between textual input and visual output through improved training techniques and larger datasets [1].This model continued to leverage the principles of LLMs to enhance its generative abilities.Anthropic's Claude AI also began to incorporate multimodal functionalities, allowing for a richer interaction between text and images, while not strictly focused on image generation, it illustrated the growing trend of integrating LLMs with visual understanding.Meta's Segment Anything Model (SAM), a zero-shot algorithm, has facilitated advanced image segmentation, providing tools for image-to-image manipulation by enabling users to specify regions of interest within images [16].Finally, Microsoft's Copilot integrated these generative AI technologies into design workflows, allowing users to seamlessly leverage AI-driven image generation and manipulation tools, thus democratizing access to sophisticated design capabilities [36].</p>
<p>In 2024, major tech players release improved technologies for text-to-image and image-to-image generation.Google, with its ongoing development of Imagen and related models, focused on enhanced photorealism and semantic understanding, leveraging diffusion models and potentially incorporating LLMs for improved prompt interpretation.Meta expanded its offerings, building upon its Emu architecture, emphasizing both speed and quality, exploring variations of diffusion models potentially incorporating VAEs for efficient latent space manipulation [17][7] [33].OpenAI continued refining DALL-E, focusing on higher resolution and more consistent image generation, further optimizing its diffusion-based pipeline and potentially integrating LLM-driven refinement steps.Anthropic, while primarily known for LLMs, began exploring visual generation in conjunction with its Claude model, potentially integrating diffusion models with their sophisticated contextual understanding capabilities.Microsoft further solidified its position with updates to its Designer and Image Creator powered by DALL-E 3, focusing on user accessibility and integration within its ecosystem.Underpinning these advancements are diffusion models, which iteratively denoise images from Gaussian noise, often operating within a latent space defined by VAEs for computational efficiency.Some models, though less prevalent in 2024 for high-fidelity image generation, may still utilize GANs for specific tasks like upscaling or style transfer, though diffusion models have become the dominant approach for general text-to-image and image-to-image tasks.The integration of LLMs, especially in prompt understanding and refinement, became a key trend, ensuring generated images more accurately reflect the intent of the user.</p>
<p>3 Key Generative Architectures</p>
<p>Variational Auto-Encoder (VAE)</p>
<p>First introduced in 2013 [15], the Variational Auto-Encoder (VAE) is a type of generative neural network capable of learning a probability distribution over a set of data points without labels.A VAE learns to encode input data into a lower-dimensional latent space and then decode it back to the original space by sampling latents, while ensuring the latent representations follow a known probability distribution.</p>
<p>VAE is categorized as a model with an explicit intractable density function (tractable models allow for explicit likelihood computation) that learns a probability distribution using variational inference and latent variables.Intuitively, latent variables (LV) "explain" the data in a "simpler" way, and more rigorously, LV result from a transformation of the data points into a continuous lower-dimensional space.Mathematically, we define x as a data point that follows a probability distribution p(x) and z, to be a latent variable that follows a probability distribution p(z), then:</p>
<p>• p(x) is the marginal distribution (and goal of the model) • p(z) is the prior distribution • p(x|z) is the likelihood mapping latents z to data points x • p(x, z) = p(x|z) * p(z) is the joint distribution of data points and latent variables • p(z|x) is the posterior distribution that describes z that can be produced by x x ∼ p(z|x) (see Figure 2).To find the parameters of the marginal distribution p(x), we can apply gradient descent which translates to compute the following:
∇logp θ (x) = p θ (z|x)∇ θ logp θ (x, z)dz(1)
The goal of variational inference is to approximate the posterior distribution p(z|x) with an explicit tractable probability distribution and allow its computation as an optimization problem.We can call this distribution the variational posterior q(z|x).During training, the goal is to minimize the Kullback-Leibler (KL) divergence, which expresses the difference between the true posterior and the variational posterior and is given by (with θ being the model parameters):
KL(P ||Q) = ∞ −∞ p(x)log( p(x) q(x) )dx = E x∼p(x) <a href="2">log( p(x) q(x, θ) </a>
To enable backpropagation in the decoder part of the VAE, this method considers a reparametrization trick where instead of just sampling from the latent distribution, they add a random noise to the mean and standard deviation to make gradient computation possible.The final loss function is given by the following equation:
L θ,ϕ (x) = E q ϕ(z|x) [logp ϕ (x|z)] − KL(q ϕ (z|x)||p θ (z))(3)
The first part of the previous equation (also called negative reconstruction error associated to the Decoder part of the VAE) controls how well the model reconstructs x given z of the variational posterior whereas the second part (corresponding the Encoder part of the VAE) controls how close the variational posterior q(z|x) is to the prior p(x) i.e. how well the dimensionality reduction of the Encoder captures the data features within the latent space.</p>
<p>Generative Adversarial Networks (GANs)</p>
<p>The first Generative Adversarial Network (GAN) was introduced in 2014 [9,8] and represents a major advancement in generative learning.GANs are a class of machine learning models that consists of two key components: a generator and a discriminator, where the generator aims to produce synthetic data, and the discriminator attempts to distinguish between the real data and synthetic data (see Figure 4. • The generator G(z) maps random noise z ∼ p(z) (also called latents) to the data distribution and outputs the synthetic image in the shape of a 1D-vector.The stochasticity given by this random sampling will provide a non-deterministic output, which is how the model creates diversity in the generation process.The goal here is to fool the discriminator and minimize log(1 − D(G(z))), which amounts to maximizing the discriminator's mistakes.• The discriminator D(x) takes as input a real and synthetic image (generated by the generator) and outputs the probability that the image corresponds to the real data distribution or not.The goal here is to maximize the loss function or the probability that it correctly classifies real and fake images.</p>
<p>This adversarial process drives both the generator and the discriminator to improve, resulting in high-quality synthetic data.In addition, the fact that the generator is only trained to fool the discriminator makes this Vanilla GAN model unsupervised.</p>
<p>The goal of the GAN is to solve the min-max game or adversarial game between the generator and the discriminator with the following objective function and optimization problem:
min G max D V (G, D) = E x∼p data (x) [logD(x)] + E z∼p f ake (z) <a href="4">log(1 − D(G(z)))</a>
where D(x) is the probability that x is real, G(z) is the generated sample, and thus D(G(z)) is the probability that the generated image given latent z is real.One of the most common limitations of GANs is the so-called mode collapse problem where the generator fails to represent accurately the pixel space of all possible outputs.This issue is common in high-resolution images, where too many fine-scale features must be captured.In that case, the generator gets stuck in a parameter setting with a similar level of noise that can consistently fool the discriminator and only captures a subset of the real data distribution.It then fails to produce diversity in its outputs and collapses to producing only a few types of synthetic samples.</p>
<p>Conditional GAN (CGAN)</p>
<p>As an extension of the Vanilla GAN, the Conditional GAN was introduced in 2014 [21] and uses conditional information (image or text) to guide the generation process.The CGAN performs conditioning generation by feeding information to both the generator and the discriminator.The generator G(z, y) takes as input random noise z, and the conditional embedding y and learns to generate data given this condition, whereas, the discriminator D(x, y) learns to classify real and fake images by checking that the condition y is met.The min-max optimization function becomes:
min G max D V (G, D) = E x∼p data (x) [logD(x|y)] + E z∼p f ake (z) <a href="5">log(1 − D(G(z|y), y))</a>
StackGAN [41] and Attentional GAN (AttnGAN) [40] are influential CGAN architectures that advanced text-to-image generation.StackGAN introduced a hierarchical approach, generating low-resolution images and iteratively refining them to high-resolution outputs.AttnGAN innovated with attention mechanisms, allowing the model to selectively attend to specific words or phrases in the text description when generating corresponding image regions.</p>
<p>Deep Convolutional GAN (DCGAN)</p>
<p>Following the initial development of GANs, various architectures emerged, notably Deep Convolutional Generative Adversarial Networks (DCGANs) introduced by Radford et al. in 2015 [29], which extended the foundational GAN framework.While the Vanilla GAN's architecture contains simple downsampling and upsampling layers with ReLU activations and a Sigmoid activation for the discriminator, this variant of the GAN is made of strided convolution layers, batch norm layers, and LeakyReLU activation functions.This architecture is adapted to small size images such as RGB inputs of shape (3,64,64).</p>
<p>Diffusion Models</p>
<p>Diffusion models, now producing state-of-the-art high-fidelity and diverse images, have evolved from the initial work of Sohl-Dickstein et al. in 2015 [34] to the significantly impactful Denoising Diffusion Probabilistic Models (DDPM) by Ho et al. in 2020 [11].These models differ from the previous generative models as they decompose the image generation process through small denoising steps.In fact, the idea behind diffusion models is that they take an input image x 0 and gradually add Gaussian noise in what is called the forward process.The second part of the network is the reverse process, or sampling process, which consists of removing the noise to obtain new data (see Figure 5).</p>
<p>The forward process in the diffusion network consists of a Markov chain of T steps.Given an input image x 0 sampled from the true data distribution x 0 ∼ q(x 0 ), at each step t &lt; T , Gaussian noise is added to x t−1 , according to a variance schedule β 1 , ..., β t to obtain x t ∼ q(x t |x t−1 ) where the x 1 , ..., x T are latents of the same dimensionality as x 0 :
q(x t |x t−1 ) = N (x t ; µ t = 1 − β t x t−1 , Σ t = β t I) (6)
Where I is the identity matrix and the variances β t can be learned or kept constant as hyperparameters.In the case of the DDPM paper, the authors used a linear schedule increasing from β 1 = 10 −4 to β T = 0.02.The scheduler, however, can be linear, quadratic, cosine [24] etc.The posterior probability can be defined as:
q(x 1:T |x 0 ) = T t=1 q(x t |x t−1 )(7)
And using the reparametrization trick to obtain a tractable closed-form sampling at any timestep, we define α t = 1 − β t , ᾱ = t s=0 α s where ϵ 0 , ..., ϵ t−1 ∼ N (0, I) and finally have:
x t ∼ q(x t |x 0 ) = N (x t ; ᾱt x 0 , (1 − ᾱ)I)(8)
Given that β t is a hyperparameter, it is possible to compute α t and ᾱt for all timesteps.We can therefore sample the noise at any timestep t and get the latent variables x t .</p>
<p>The second part of diffusion models is the reverse process which is also a Markov chain with learned Gaussian transitions starting at p(x T ) = N (x T ; 0, I).The goal of the reverse process is to learn the reverse distribution q(x t−1 |x t ) by approximating it with a parametrized model p θ (where p θ is Gaussian and the mean and variance will be parametrized and learned by a neural network):
p θ (x t−1 |x t ) = N (x t−1 ; µ θ (x t , t), Σ θ (x t , t))(9)p θ (x 0:T ) = p θ (x T ) T t=1 p θ (x t−1 |x t )(10)
To train diffusion models, we optimize the negative log-likelihood of the training data.This optimization, similar to the approach used in Variational Autoencoders (VAEs), is achieved by maximizing the Evidence Lower Bound (ELBO), a tractable approximation.The following expression represents the ELBO after a series of computational steps:
log p(x) ≥ E q(x1|x0) [log p θ (x 0 |x 1 )] − D KL (q(x T |x 0 )∥p(x T )) − T t=2 E q(xt|x0) <a href="11">D KL (q(x t−1 |x t , x 0 )∥p θ (x t−1 |x t ))</a>log p(x) ≥ L 0 − L T − T t=2 L t−1(12)
Where:</p>
<p>• E q(x1|x0) [log p θ (x 0 |x 1 )] is the reconstruction term • D KL (q(x T |x 0 )∥p(x T )) basically shows how close x T is to the Standard Gaussian distribution.</p>
<p>• T t=2 L t−1 = L t represents the difference between the denoising steps p θ (x t−1 |x t ) and the approximated ones q θ (x t−1 |x t , x 0 ).The KL divergence compares p θ (x t−1 |x t ) against forward process posteriors, which are tractable when conditioned on x 0 .</p>
<p>Therefore maximizing the likelihood amounts to learning the denoising steps L t .Based on further calculations, the DDPM paper shows that instead of predicting the mean of the distribution during the training of the reverse process, the model will predict the noise ϵ at each timestep t using the following simplified formula of the denoising term in the ELBO (also the loss function of the reverse process network):
L simple (θ) := E t,x0,ϵ ∥ϵ − ϵ θ ( √ ᾱt x 0 + √ 1 − ᾱt ϵ, t)∥ 2(13)
The model associated to the loss function of the reverse process is a U-Net architecture with residual blocks, group normalization as well as self-attention blocks.The timestep t is concatenated to the input image using a cosine positional embedding into each residual block.This denoising U-Net is trained to predict the noise at each timestep of the process.</p>
<p>Conditional Image Generation with Guided Diffusion Classifier Guidance</p>
<p>Similarly to the CGAN, an important extension of the diffusion model is the Guided diffusion model that includes conditional image generation in the network.In that scenario, the model adds conditioning information y at each diffusion step:
p θ (x 0:T |y) = p θ (x T ) T t=1 p θ (x t−1 |x t , y)(14)
Using Bayes rule with some computations and more importantly by adding the guidance scalar term s, we can show that guided diffusion models aim to learn ∇logp θ (x t |y) such that:
∇logp θ (x t |y) = ∇logp θ (x t ) + s.∇logp θ (y|x t )(15)
It was also shown in [34] and [3] that a classifier guidance model defined by f Φ (y|x t , t) can guide the diffusion towards the target class y by training f Φ (y|x t , t) on a noisy image x t to predict class y.To do so, we build a classconditional diffusion model with mean µ(x t |y) and variance Σ θ (x t |y) and perturb the mean by the gradients of logf Φ (y|x t , t) of class y, resulting in:
μ(x t |y) = µ θ (x t |y) + s • Σ θ (x t |y)∇logf Φ (y|x t , t)(16)
Figure 6: Algorithm of classifier guided diffusion.Source: [3] Classifier Free-Guidance</p>
<p>Classifier-free guidance, proposed by Ho et al. [12], allows for enhanced control in diffusion models by eliminating the need for separate classifiers.Instead of relying on a separate classifier, which increases training complexity and introduces bias potential, classifier-free guidance trains the diffusion model to directly learn and combine conditional and unconditional distributions during inference, streamlining the process.In other words, the authors train a conditional diffusion model ϵ θ (x t |y) and an unconditional model ϵ θ (x t |y = 0) as a single neural network as follows:
εθ (x t |y) = ϵ θ (x t |0) + s • (ϵ θ (x t |y) − ϵ θ (x t |0))(17)
This approach is advantageous compared to the previous one as it trains a single model to guide the diffusion process and can take different types of conditional data such as text embeddings.We will see that many models rely on classifier free-guidance especially when training on multi-modal data.</p>
<p>Score-based generative models</p>
<p>Score-Based Diffusion Models (SBDMs) are a class of diffusion models proposed by [35] that use score functions (gradient of the log probability density function) and Lagevin dynamics (iterative process where we draw samples from a distribution based only on its score function).Like GANs, SBDMs use adversarial training and try to generate images that are indistinguishable from real images.</p>
<p>Instead of learning a probability density p(x), the neural network s θ estimates the score function ∇ x logp(x) directly, and the training objective can be as follows:
E px ∥∇ x log p(x) − s θ (x) − ∥ 2 2 = p(x∥∇ x log p(x) − s θ (x)∥ 2 2 dx(18)
While Langevin dynamics can sample p(x) using the approximated score function, directly estimating ∇ x logp(x is difficult and imprecise.To address this, diffusion models learn score functions at various noise levels, achieved by perturbing the data with multiple scales of Gaussian noise.</p>
<p>So given the data distribution p(x), we perturb it with Gaussian noise N (0, σ 2 i I) where i = 1, 2, . . ., L to obtain a noise-perturbed distribution:
p σi (x) = p(y)N (x; y, σ 2 i I) dy(19)
Then we train a network s θ (x, i), known as the Noise Conditional Score-Based Network (NCSN), to estimate the score function ∇ x log p σi (x) .The training objective is a weighted sum of Fisher divergences for all noise scales:
L i=1 λ(i)E pσ i (x) ∥∇ x log p σi (x) − s θ (x, i)∥ 2 2 (20)
The authors of [35] combine components of NSCNs and DDPMs into one generative model, based on Stochastic Differential Equations (SDE) that does not depend on the data and no trainable parameters.Rather than perturbing data with a finite set of noise distributions, we utilize a continuous range of distributions that evolve over time through a diffusion process {x t } t∈[0,T ] .This process is governed by a predefined SDE.Then, it is possible to generate new samples by reversing this process.The forward process going from an input image x 0 to random noise x T is defined such that:
dx = f (x, t)dt + g(t)dw(21)
Where:</p>
<p>• dw is a Wiener process (random noise),</p>
<p>• f (x, t) is the drift term and a vector valued function</p>
<p>• g(t) is the diffusion term and a scalar function After adding noise to the original data distribution for enough time steps, the perturbed distribution becomes close to a tractable noise distribution.Then it is possible to generate new samples by reversing the diffusion process and computing the reverse SDE given that the SDE was chosen to have a corresponding reverse SDE in closed form (see Figure 7):
dx = f (x, t) − g 2 (t)∇ x log p t (x) dt + g(t)dw(22)
Figure 7: Score-based generative modeling through SDE</p>
<p>Stable Diffusion and Latent diffusion models</p>
<p>Latent diffusion models (LDMs) are yet another innovative extension of diffusion models [32].Instead of applying the diffusion on a high-dimensional input (pixel space), we project the input image into a smaller latent space and apply diffusion with latents as inputs.The authors of [32] propose to use an encoder network g to downsample the input into a latent representation z t = g(x t ) and apply the forward process to z t .Then the reverse process is the same as a standard diffusion process with a U-Net to generate new data that are then upsampled by a decoder network (see Figure 8).Therefore, given an encoder ε (Stable diffusion uses a pre-trained VAE encoder network), then the loss can be formulated as:
L LDM = E ε(x),t,ϵ <a href="23">∥ϵ − ϵ θ (z t , t) 2 ∥</a>
Figure 8: Latent diffusion architecture.Source: [18] Stable diffusion can also be conditioned, in particular, using classifier-free guidance by adding conditional embeddings such as image features or text descriptions using a text encoder (e.g.CLIP's text encoder) to steer the generation process.</p>
<p>Diffusion Transformers (DiT)</p>
<p>One of the most recent diffusion-based models is the Diffusion Transformer (DiT) proposed in [26] which is an architecture that combines the principles of diffusion models and transformer models and that generates high-quality synthetic images.It leverages the iterative denoising process inherent in diffusion models while utilizing the powerful representation learning capabilities of transformers for improved sample generation.The authors in [26] replace the U-Net backbone, in the LDM model, by a neural network called a Transformer [39].Transformers are a class of models based on self-attention mechanisms, and they have been proven to excel in tasks involving sequential data (like language processing).They work by attending to all input tokens at once and using multi-head self-attention to process the input efficiently.Mathematically, the attention mechanism can be formulated as follows:
Attention(Q, K, V ) = softmax QK T √ d k V(24)
Where:</p>
<p>• Q (query), K (key), and V (value) are input representations.</p>
<p>• d k is the dimensionality of the key vectors, and the softmax function normalizes the attention scores.</p>
<p>In the context of a Diffusion Transformer (see Figure 9), the input to the transformer is typically a set of tokens or features (e.g., image patches, sequence tokens), and self-attention helps the model attend to dependencies across all tokens to capture long-range relationships.In the reverse process of the diffusion model, the transformer network is responsible for predicting the noise at each step, conditioned on the noisy data.For example, given the noisy image at time step t, the transformer can model long-range spatial dependencies across the image patches (or sequence tokens) and generate a clean image at the next step:
x t−1 = Transformer(µ θ (x t , t), context)(25)
Where:</p>
<p>• µ θ (x t , t) is the predicted noise (as described in the reverse diffusion equation),</p>
<p>• context could be a conditioning input, such as a text prompt (in the case of text-to-image generation).</p>
<p>Comparative Analysis</p>
<p>We can define a set of metrics to evaluate a model family's general performance in image generation.Image Quality refers to the level of detail in the generated image.A model with high Image Quality strictly adheres to the imposed restrictions placed on it while maintaining a high level of detail, and absence of artifacts.A model with low Image Quality consistently generates images with large amounts of noise and/or artifacts and incoherent features [14].A model's Diversity refers to its range of potential outputs.A model with high diversity can produce a wide spectrum of images while maintaining a constant image quality.A model with low diversity can only generate images in a narrow range with a constant image quality [23].Leaving this narrow range can lead to significant and rapid decreases in image quality.Controllability refers to how easy it is to guide the image generation process with some additional input.For example, if you wanted to generate variations of an image you could condition the model with an input image to help shape the generated image.A highly controllable model can take into account additional user input, understand the underlying features, and apply those features to the generated image.Training stability refers to the model's ability to reliably and smoothly converge over the training process.</p>
<p>Within the scope of generative models for image synthesis, Diffusion Models stand out for their ability to produce the highest quality images, often surpassing GANs, which also generate sharp visuals but may not achieve the same level of detail as diffusion-based approaches.VAEs, on the other hand, tend to yield blurrier images, indicating a trade-off in image fidelity.When it comes to diversity, both GANs and Diffusion Models excel at generating a wide variety of outputs, while VAEs can struggle with high variability, limiting their effectiveness in certain applications.In terms of controllability, Diffusion Models offer the most significant level of control over the generation process, allowing for precise adjustments, whereas GANs provide moderate to high control that can vary based on specific architectural choices.VAEs, however, exhibit limited controllability, making them less suitable for applications requiring fine-tuned image generation.Lastly, in terms of training stability, VAEs and Diffusion Models are generally more stable during the training process, reducing the likelihood of issues, while GANs often face challenges related to instability and mode collapse, which can hinder their performance and diversity [44].Table 1 summarizes aspects about image quality, diversity, controllability and training stability.opportunity to create a more diverse datasets from a few "approved" images, which could be used by researchers to train models [4][27] [22].The challenge with scientific image generation lies in the Controllability or controlling their generation since pre-existent models are typically trained on data dissimilar to specialized imagery like microscopy data.So if you tried to just condition on a single cross-section on a standard GAN or Diffusion Model the results are likely lackluster.Alternatively, training a model from scratch would require a large dataset, which is actually the motivation for using image generation in the first place.Gathering sufficient amounts of data coming from experimental settings is often difficulty, and sometimes impossible, but without the sufficient quantity to reach convergence during training, the models can be useless.Considering the aforementioned strenghts, Diffusion Models are expected to exhibit optima performance in the synthesis of scientific imagery, as they address each of these criteria.</p>
<p>Verification &amp; Validation</p>
<p>Hallucinations and unexpected outcomes are some of the issues associated with GenAI.Other problems include inherent biases within training datasets can skew the generated images, reinforcing existing misconceptions [19] or overlooking important, yet underrepresented, scientific phenomena.Validation becomes exceptionally difficult when dealing with completely novel scenarios, as there may be no existing experimental or observational data for comparison.This lack of ground truth poses risks of generating misleading visualizations that could inadvertently guide research down unproductive paths, therefore only rigorous scrutiny and expert validation could potentially mitigate these risks [20] Verification and validation (V&amp;V) are essential for establishing the reliability and accuracy of AI generative models, and several efforts have focused on creating standardized benchmarks [13], however curated datasets using scientific imaging are either extremely narrow [31] or sparse [42].Verification assesses a model's adherence to specified requirements and its performance under defined conditions.This includes unit testing for component correctness and performance evaluation against benchmark datasets.Cross-validation further examines the predictive performance across data subsets, indicating robustness.Validation determines whether the model accurately reflects real-world phenomena.In scientific imaging, validation involves qualitative expert (domain scientist) evaluations of generated image realism and quantitative metrics such as structural similarity index (SSIM) or peak signal-to-noise ratio (PSNR) for image quality.Incorporating domain-specific knowledge strengthens reliability.For example, in biological or material sciences imaging, comparisons against existing scientific models and datasets ensure generated outputs are both visually and scientifically sound.Through rigorous V&amp;V, researchers can avoid major pitfalls of generative AI models, and potentially model utilization in critical scientific applications.</p>
<p>Conclusion &amp; Future Directions</p>
<p>The future of text-to-image and image-to-image technologies promises significant advancements, with profound implications across diverse fields, notably scientific data analysis.We can anticipate continuous refinements in diffusion models, leading to hyper-realistic image generation coupled with increasingly granular control over specific attributes and detail.AI models will develop a deeper comprehension of contextual relationships, enabling the production of more nuanced and precise visual representations.Additionally, ongoing optimization of algorithms and hardware will yield faster generation times and reduced computational costs, while cloud-based platforms and mobile applications could democratize access to these technologies.A significant trend is the rapid progression of light-weight multimodal models [13,2], with expectations of substantial improvements in quality and coherence, particularly taking advantage of high-performance computer systems.Finally, AI will increasingly personalize image generation, learning individual user preferences to produce highly tailored visual outputs.</p>
<p>The impact of these technologies on scientific data analysis, particularly with scarce image sets from specialized instruments, will be transformative.AI-driven data augmentation promises to enable the generation of synthetic data to supplement limited datasets, enhancing the training of machine learning models for critical tasks like image segmentation and object detection.Moreover, AI will translate abstract scientific data into intuitive visual representations, facilitating the identification of patterns and trends in fields like genomics and materials science.By generating visual representations of potential scenarios, AI will assist scientists in formulating hypotheses and designing experiments, such as simulating molecular interactions or astronomical phenomena.AI can also be employed to identify and rectify errors in scientific images, improving the accuracy and reliability of data analysis.Furthermore, AI will foster increased collaboration by creating easily understandable visual representations of data for diverse scientific audiences.</p>
<p>Despite the immense potential, challenges remain.AI models can inherit biases from training data, leading to inaccurate results, which requires careful attention to dataset representativeness.The "black box" nature of some AI models poses challenges to interpretability, requiring efforts to develop more transparent models for scientific applications.Crucially, validation of AI-generated results against experimental data and established scientific principles is essential, especially when dealing with scarce datasets, to ensure the responsible and effective application of these powerful tools.</p>
<p>Disclosure</p>
<p>This article incorporates text and table generation facilitated by generative artificial intelligence.Although these tools aided in the organization and presentation of information, the authors retain full responsibility for the accuracy and scientific validity of the content.This article is a working in progress and further version will be shared shortly.</p>
<p>Figure 2 :
2
Figure 2: Variational Inference</p>
<p>Figure 3 :
3
Figure 3: VAE Encode -Decoder architecture</p>
<p>Figure 4 :
4
Figure 4: Architecture of the Vanilla GAN</p>
<p>Figure 5 :
5
Figure5: Denoising diffusion probabilistic models (DDPMs).Source:[18]</p>
<p>Figure 9 :
9
Figure 9: Architecture of the Diffusion Transformer</p>
<p>Table 1 :
1
Comparison of VAEs, GANs, and Diffusion Models for Text-to-Image Generation
Model TypeImage QualityDiversityControllabilityTraining StabilityVariationalModerate to High:Moderate: Capable ofModerate: Can con-High: More stableAutoencodersGenerally producesgenerating diverse im-dition on text embed-during training com-(VAEs)images with goodages but may struggledings but lacks fine-pared to GANs, butquality but can bewith high variabilitygrained control overcan suffer from is-blurry due to the lossin complex datasets.image features.sues like posteriorfunction used.collapse.GenerativeHigh: Known forHigh: Capable of pro-Moderate to High:Moderate: TrainingAdversarialgenerating sharp andducing a wide varietyCanimplementcan be unstable andNetworksdetailed images, es-of images, especiallyvariouscondition-sensitive to hyperpa-(GANs)pecially with tech-with diverse traininging methods (e.g.,rameters; mode col-niques like Progres-data.text-to-image) butlapse can occur, lead-sive Growing GANs.may require complexing to reduced diver-architecturesforsity.precise control.DiffusionVery High: AchievesHigh: Generates di-High: Allows forHigh:GenerallyModelsstate-of-the-art imageverse images effec-more explicit controlmore stable thanquality, often surpass-tively, with the poten-over the generationGANs during train-ing GANs and VAEstial for high variabil-process through iter-ing, with well-definedin realism and detail.ity.ative denoising stepstraining objectivesand conditioning.that reduce issueslike mode collapse.
Scientific images can be detailed and high-resolution as many of them are acquired using advanced instruments, e.g., microscopes. In order to generate valuable synthetic images to augment scientific datasets, image quality is expected to be higher than in other domains, such as art. For example, MRI scans of human brains must both be detailed and expressly go through the HIPAA guidelines. Being able to generate synthetic MRI brains scans represent an invaluable
AcknowledgmentsThis work was supported by the US Department of Energy (DOE) Office of Science Advanced Scientific Computing Research (ASCR) and Basic Energy Sciences (BES) under Contract No. DE-AC02-05CH11231 to the Center for Advanced Mathematics for Energy Research Applications (CAMERA) program.It also included partial support from the DOE ASCR-funded project Autonomous Solutions for Computational Research with Immersive Browsing &amp; Exploration (ASCRIBE) and Laboratory Directed Research &amp; Development (LDRD) Program and the project Analytics through Diffusion Transformer Models (ADTM) for Scientific Image and Text.Competing interestsThe authors declare that they have no competing interests.
Improving image generation with better captions. James Betker, Gabriel Goh, Li Jing, Jianfeng Timbrooks, Linjie Wang, Li, Longouyang, Juntangzhuang, Joycelee, Yufeiguo, Wesammanassra, Prafulladhariwal, Caseychu, Aditya Yunxinjiao, Ramesh, 2023</p>
<p>Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping, arXiv:2401.03382Nvlm: Open frontier-class multimodal llms. 2024arXiv preprint</p>
<p>Diffusion models beat gans on image synthesis. Prafulla Dhariwal, Alexander Nichol, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, J Wortman Vaughan, Curran Associates, Inc202134</p>
<p>Conditional diffusion models for semantic 3d brain mri synthesis. Zolnamar Dorjsembe, Hsing-Kuo Pao, Sodtavilan Odonchimed, Furen Xiao, IEEE Journal of Biomedical and Health Informatics. 2872024</p>
<p>David Foster, Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play. O'Reilly Media. 20232nd edition</p>
<p>Greater creative control for ai image generation. Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, Yaniv Taigman, Jul 2022</p>
<p>Emu video: Factorizing text-to-video generation by explicit image conditioning. Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, Ishan Misra, 2024</p>
<p>Deep Learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016MIT Press</p>
<p>Generative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Z Ghahramani, M Welling, C Cortes, N Lawrence, K Q Weinberger, Curran Associates, Inc201427</p>
<p>Imagen video: High definition video generation with diffusion models. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, P Diederik, Ben Kingma, Mohammad Poole, David J Norouzi, Tim Fleet, Salimans, 2022</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>. Jonathan Ho, Tim Salimans, arXiv:2207.125982022Classifier-free diffusion guidance. arXiv preprint</p>
<p>Granite vision: a lightweight, open-source multimodal model for enterprise intelligence. Leonid Karlinsky, Assaf Arbelle, Abraham Daniels, Ahmed Nassar, Amit Alfassi, Bo Wu, Eli Schwartz, Dhiraj Joshi, Jovana Kondic, Nimrod Shabtay, Pengyuan Li, Roei Herzig, Shafiq Abedin, Shaked Perek, Sivan Harary, Udi Barzelay, Adi Raz Goldfarb, Aude Oliva, Ben Wieles, Bishwaranjan Bhattacharjee, Brandon Huang, Christoph Auer, Dan Gutfreund, David Beymer, David Wood, Hilde Kuehne, Jacob Hansen, Joseph Shtok, Ken Wong, Luis Angel Bathen, Mayank Mishra, Maksym Lysak, Michele Dolfi, Mikhail Yurochkin, Shila Ofekkoifman, Sriram Raghavan, Tanveer Syeda-Mahmood. Nimrod Harel, Ophir Azulai, Oshri Naparstek, Rafael Teixeira De Lima, Rameswar Panda, Sivan Doveh, Shubham Gupta, Subhro Das, Syed Zawad, Yusik Kim, Zexue He, Alexander Brooks, Gabe Goodhart, Anita Govindjee, Derek Leist, Ibrahim Ibrahim, Aya Soffer, David Cox, Kate Soule, Luis Lastras, Nirmit Desai, Peter Staar, Tal Drory, and Rogerio FerisPreprint, 2024. Available at</p>
<p>Analyzing and improving the image quality of stylegan. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2020</p>
<p>Auto-encoding variational bayes. P Diederik, Max Kingma, Welling, 2022</p>
<p>. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick, arXiv:2304.026432023Segment anything</p>
<p>Emu: Enhancing image generation models using photogenic needles in a haystack. Narine Kokhlikyan, Bargav Jayaraman, Florian Bordes, Chuan Guo, Kamalika Chaudhuri, Sep 2023</p>
<p>Akshay Kulkarni, Adarsha Shivananda, Anoosh Kulkarni, Dilip Gudivada, Diffusion Model and Generative AI for Images. Apress2023</p>
<p>The longtail impact of generative ai on disinformation: Harmonizing dichotomous perspectives. Jason S Lucas, Maung Barani, Maryam Maung, Keegan Tabar, Dongwon Mcbride, Lee, IEEE Intelligent Systems. 3952024</p>
<p>Ai hallucinations: A misnomer worth clarifying. Negar Maleki, Balaji Padmanabhan, Kaushik Dutta, 2024 IEEE Conference on Artificial Intelligence (CAI). 2024</p>
<p>Conditional generative adversarial nets. Mehdi Mirza, Simon Osindero, 2014</p>
<p>A morphology focused diffusion probabilistic model for synthesis of histopathology images. Puria Azadi Moghadam, Sanne Van Dalen, Karina C Martin, Jochen Lennerz, Stephen Yip, Hossein Farahani, Ali Bashashati, 2022</p>
<p>Reliable fidelity and diversity metrics for generative models. Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, Jaejun Yoo, Proceedings of the 37th International Conference on Machine Learning. Hal Daumé, Iii , Aarti Singh, the 37th International Conference on Machine LearningPMLRJul 2020119of Proceedings of Machine Learning Research</p>
<p>Improved denoising diffusion probabilistic models. Alex Nichol, Prafulla Dhariwal, Proceedings of the 38th International Conference on Machine Learning (ICML). the 38th International Conference on Machine Learning (ICML)PMLR2021139of Proceedings of Machine Learning Research</p>
<p>Openai, Clip, 02/27/2025Connecting text and images. </p>
<p>Scalable diffusion models with transformers. William Peebles, Saining Xie, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)October 2023</p>
<p>Brain imaging generation with latent diffusion models. H L Walter, Petru-Daniel Pinaya, Jessica Tudosiu, Pedro F Da Dafflon, Virginia Costa, Parashkev Fernandez, Sebastien Nachev, M Jorge Ourselin, Cardoso, Anirban Mukhopadhyay, Ilkay Oksuz, Sandy Engelhardt, Dajiang Zhu, and Yixuan Yuan. ChamSpringer Nature Switzerland2022Deep Generative Models</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, 2021</p>
<p>Unsupervised representation learning with deep convolutional generative adversarial networks. Alec Radford, Luke Metz, Soumith Chintala, 2016</p>
<p>Zero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, 2021</p>
<p>Cric searchable image database as a public platform for conventional pap smear cytology data. Mariana T Rezende, Raniere Silva, Fagner De, O Bernardo, Alessandra H G Tobias, Paulo H C Oliveira, M Tales, Caio S Machado, Fatima N S Costa, Daniela M Medeiros, Claudia M Ushizima, Andrea G C Carneiro, Bianchi, Nature Scientific Data. 811512021</p>
<p>High-Resolution Image Synthesis with Latent Diffusion Models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bjorn Ommer, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Los Alamitos, CA, USAIEEE Computer SocietyJune 2022</p>
<p>Emu edit: Precise image editing via recognition and generation tasks. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, Yaniv Taigman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2024</p>
<p>Deep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, Proceedings of the 32nd International Conference on Machine Learning. Francis Bach, David Blei, the 32nd International Conference on Machine LearningLille, FrancePMLRJul 201537</p>
<p>Score-based generative modeling through stochastic differential equations. Yang Song, Stefano Ermon, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Introducing microsoft 365 copilot -your copilot for work. Jared Spataro, Mar 2023</p>
<p>Ai hallucination: towards a comprehensive classification of distorted information in artificial intelligence-generated content. Yujie Sun, Dongfang Sheng, Zihan Zhou, Yifei Wu, Humanities and Social Sciences Communications. 11112782024</p>
<p>. Alan D Thompson, Life.architect. 2024</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. I Guyon, U Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc201730</p>
<p>Attngan: Fine-grained text to image generation with attentional generative adversarial networks. Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2018</p>
<p>Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas, 2017 IEEE International Conference on Computer Vision (ICCV). 2017</p>
<p>Sdrbench: Scientific data reduction benchmark for lossy compressors. Kai Zhao, Sheng Di, Xin Lian, Sihuan Li, Dingwen Tao, Julie Bessac, Zizhong Chen, Franck Cappello, 2020 IEEE International Conference on Big Data (Big Data). 2020</p>
<p>Larger and more instructable language models become less reliable. Lexin Zhou, Wout Schellaert, Fernando Martínez-Plumed, Yael Moros-Daval, Cèsar Ferri, José Hernández-Orallo, Nature. 63480322024</p>
<p>High-quality and diverse few-shot image generation via masked discrimination. Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan, IEEE Transactions on Image Processing. 332024</p>            </div>
        </div>

    </div>
</body>
</html>
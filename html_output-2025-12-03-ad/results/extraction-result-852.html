<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-852 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-852</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-852</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-271923781</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.12022v2.pdf" target="_blank">Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind</a></p>
                <p><strong>Paper Abstract:</strong> How do people understand and evaluate claims about others' beliefs, even though these beliefs cannot be directly observed? In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents' goals, beliefs, and intentions: a language-augmented Bayesian theory-of-mind (LaBToM). By translating natural language into an epistemic ``language-of-thought'' with grammar-constrained LLM decoding, then evaluating these translations against the inferences produced by inverting a generative model of rational action and perception, LaBToM captures graded plausibility judgments of epistemic claims. We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e852.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e852.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LaBToM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-augmented Bayesian Theory-of-Mind</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational model that combines a compositional epistemic language-of-thought (ELoT) with a Bayesian theory-of-mind (BToM) that performs probabilistic inference over belief states and goals to evaluate natural-language epistemic claims.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LaBToM (Language-augmented Bayesian Theory-of-Mind)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LaBToM translates natural-language epistemic sentences into structured ELoT formulas via grammar-constrained LLM decoding (SMC sampling), then evaluates those formulas against probabilities produced by a Bayesian Theory-of-Mind module. The BToM module models an approximately rational agent in a POMDP-like generative process (goal prior, state prior, belief prior, state transitions, belief updates by perception, Boltzmann action selection over cost-to-go estimates). Observers perform exact Bayesian inference (enumerating initial states and discrete initial belief distributions) to get posterior distributions over goals, state trajectories, and belief histories; these posterior belief distributions are then used to compute Pr(A, φ) for ELoT expressions and yield normalized likelihood ratings comparable to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL-derived first-order symbolic states + discrete belief states (particle/distribution over states); POMDP-style belief-space representation</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Base (non-epistemic) world model is a first-order logic language derived from PDDL: states are conjunctions of predicates/functions over objects. An agent belief b_t is a probability-weighted collection of environment state hypotheses {(s_i, w_i)} (particle-like discrete distributions). State transitions follow P(s_t | s_{t-1}, a_{t-1}) and belief updates filter hypotheses inconsistent with the agent's observations. Action selection is modeled via a Boltzmann policy over cost-to-go Q_g(b_t,a) estimated with a Q_MDP approximation (averaging Q* over belief hypotheses); Q*(s,a) is estimated by shortest-path cost in a deterministic state s. All representations are probabilistic (distributions over states, goals, and beliefs).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Semantic parsing: map natural language epistemic sentences to ELoT formulas; produces a distribution over translations via grammar-constrained sequential Monte Carlo (SMC) decoding, capturing parser uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LLaMa 3.1 8B (used for grammar-constrained SMC decoding); GPT-4o and Gemini used as multimodal baselines (not as core components)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>State/belief (agent epistemic) uncertainty; LLM parsing uncertainty (distribution over ELoT translations); uncertainty in action selection via Boltzmann randomness</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Exact Bayesian inference by enumerating hypotheses (G × S0 × B0) for the experimental scenarios; belief represented as weighted hypotheses (k-particle discrete distributions); Q_MDP averaging over hypothesis Q* values; SMC sampling to represent distribution over LLM translations; Boltzmann policy with fitted inverse temperature β.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Belief-space planning via Q_MDP approximation: Q_g(b,a) = Σ_{(s,w)∈b} w · Q*(s,a), where Q*(s,a) estimated by shortest-path search; action selection via Boltzmann distribution over Q_g.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Doors, Keys, & Gems (gridworld)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>A 2D gridworld where an agent must pick up single-use colored keys possibly hidden in boxes to unlock color-matched doors and reach one of four goal gems; environment state is fully symbolic (objects, boxes, keys, doors, gems); used here as the grounding environment for human experiments and BToM inference (not a text-only benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Correlation (Pearson r) with human normalized likelihood ratings for epistemic sentences; also translation accuracy for NL → ELoT (semantic equivalence), and in-vs-out-of-context classification accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Model-human Pearson r = 0.76 (full LaBToM across current+initial); with gold ELoT translations r = 0.81; ELoT grammar-constrained SMC translation equivalence accuracy up to 91%; LaBToM classifies in-context vs out-of-context statements correctly ~70% of the time.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Ablations and LLM baselines: Non-Planning ablation r = 0.40 overall (Current r = 0.58, Initial r = 0.07); True Belief ablation r = 0.10; GPT-4o best baseline (images + narratives + few-shot) r = 0.52 overall (Current 0.59, Initial 0.41); Gemini 1.5 Pro variants r ≈ 0.22-0.23.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations show modeling beliefs and planning under uncertainty is crucial: True-Belief (assume agent knows true state) performs very poorly (r ≈ 0.09), showing that ignoring agent epistemic uncertainty harms correspondence with human judgments. Non-Planning (heuristic Manhattan-distance cost) fails especially at inferring initial beliefs (Initial r ≈ 0.07), showing that assuming goal-directed epistemic planning is important. Ablations and parameter fits confirm thresholds and Boltzmann β matter but full probabilistic belief modeling yields best fit.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A structured probabilistic symbolic world model (PDDL-like first-order states) combined with discrete belief-state representations and an explicit BToM that integrates perception, belief updates, and belief-aware planning (via Q_MDP) explains human judgments about epistemic language substantially better than multimodal LLM baselines. Grammar-constrained SMC parsing into an epistemic language-of-thought (ELoT) captures the compositional structure of epistemic language and helps propagate LLM parsing uncertainty into downstream probabilistic evaluation. Exact (enumerative) Bayesian inference over symbolic hypotheses is feasible at the scenario scale and is key to integrating world-model uncertainty into sentence evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e852.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e852.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BSIPS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Belief-Space Sequential Inverse Plan Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference algorithm (belief-space variant of SIPS) that enumerates hypotheses over goals, initial states, and initial beliefs, simulates agent belief updates and actions, and computes exact posterior weights for hypotheses to infer agent goals and beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Belief-Space Sequential Inverse Plan Search (BSIPS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BSIPS enumerates hypothesis space H = G × S0 × B0 and maintains weights for each hypothesis. At each time step, it simulates state transitions and belief updates for each hypothesis, computes belief-space Q-values using a Q_MDP approximation with memoized shortest-path costs, computes action likelihoods via a Boltzmann policy, and reweights hypotheses by observation likelihood. For the paper's scenarios, BSIPS performs exact Bayesian posterior inference over beliefs/goals/states by enumeration (no Monte Carlo approximations required at this scale).</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Belief-space POMDP-style enumerated hypotheses over PDDL-like symbolic states and discrete belief distributions (particle counts define B0 size)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Hypotheses combine a goal, an initial symbolic state s0 (from S0 consistent with initial observation), and an initial belief distribution b0 (discrete distributions formed by distributing k particles over ns states). Beliefs are updated deterministically by filtering inconsistent state hypotheses given agent observations; Q-values are computed by averaging per-state Q* values weighted by the belief distribution. All probabilities are maintained exactly over the finite hypothesis set.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>State and belief uncertainty (epistemic uncertainty over which symbolic state is true and what the agent believes)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Exact Bayesian inference by enumerating all discrete hypotheses (G × S0 × B0), particle-based discrete belief distributions (k particles), reweighting by action/observation likelihoods; memoization to reuse shortest-path computations.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Uses Q_MDP approximation for belief-space Q-values where Q*(s,a) computed by shortest-path cost; action likelihoods via Boltzmann policy.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Doors, Keys, & Gems (gridworld)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Same gridworld used for grounding; BSIPS simulated agent actions and belief updates in this symbolic environment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Computational runtime per action and correctness of posterior inference as evidenced by LaBToM downstream correlation with human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Enumerative inference over 120 to 5,490 hypotheses runs from ~0.1 s/action (120 hypotheses) up to ~20 s/action (5,490 hypotheses) on an i7-1370P CPU with 64 GB RAM; enables exact posterior computation in the experimental scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not directly compared as an algorithmic baseline; compared indirectly via ablations that modify planning or belief assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>NA as BSIPS is the implemented inference engine; ablations of the overall model that alter belief/planning assumptions demonstrate the importance of accurate belief-space inference (see LaBToM ablation results).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BSIPS demonstrates that exact Bayesian inference over enumerated symbolic hypotheses is tractable for small discrete scenarios and provides a principled way to integrate symbolic world models, belief-state uncertainty, and planning models into inference about others' beliefs and into the evaluation of epistemic language.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e852.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e852.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Epistemic Language of Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compositional formal language for expressing epistemic claims that mirrors natural-language structure (operators like believes, knows, might, likely) and grounds modal/epistemic operators in Pr(A, φ) comparisons with fitted thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ELoT (Epistemic Language of Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ELoT is a formal, grammar-constrained representation layer sitting on top of a PDDL-derived first-order base language. Epistemic operators (believes, knows, might, could, likely, comparatives) are typed constructs whose semantics are defined by comparisons on Pr(A, φ) against operator-specific thresholds θ (learned/fitted). ELoT expressions can be lowered to probability-comparison formulas (Pr(A, φ) ≥ θ) for evaluation against BToM-inferred belief distributions. ELoT improves semantic parsing accuracy and better aligns with natural-language epistemic constructions than the lowered-only representation.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL-derived first-order logic base language plus epistemic operators grounded in probabilistic semantics (Pr comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Base formulas Φ are first-order predicate formulas (e.g., exists k. key(k) ∧ inside(k, box2)). ELoT wraps these with epistemic operators producing expressions whose evaluation depends on agent-assigned probabilities Pr(A, φ) computed from the belief distribution over symbolic states; thresholds θ define semantics of modals/epistemic adjectives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Target representation for grammar-constrained LLM semantic parsing; LLM produces ELoT formulas from English sentences (SMC samples a distribution over ELoT parses)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LLaMa 3.1 8B (used with grammar-constrained SMC); unconstrained baselines used Gemini Flash 8B</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>Parser uncertainty (distribution over ELoT translations); epistemic probability (Pr(A, φ)) used inside ELoT operators</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Grammar-constrained SMC sampling produces weighted ELoT parse samples; BToM supplies probability distributions Pr over base formulas; thresholds Θ map probability values to modal wording.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>ELoT itself is a representational layer; planning is handled by BToM/BSIPS (Q_MDP approximation) which produces the Pr(A, φ) values ELoT requires.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Doors, Keys, & Gems (gridworld)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>ELoT was evaluated on human-collected sentences about this gridworld; translation accuracy and downstream model fit evaluated in that setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Translation accuracy (exact equality and semantic equivalence) from English sentences to ELoT; downstream correlation with human judgments when using parsed ELoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Grammar-constrained SMC decoding into ELoT with LLaMa 3.1 8B achieved ~91% semantic-equivalence with gold ELoT translations; translating into ELoT was up to 2.4× more accurate than translating into the lowered form; LaBToM with gold ELoT translations achieved r = 0.81 with human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Lowered-form target (probability-comparison-only) produced substantially lower semantic parsing accuracy and downstream performance; unconstrained LLaMa sampling and Gemini Flash 8B also underperformed.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Comparing ELoT vs. lowered representation: ELoT yields much higher parsing accuracy and downstream human-model correlation; grammar-constrained decoding provides additional benefits over unconstrained sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Designing a compositional, language-aligned epistemic representation (ELoT) is crucial for reliable semantic parsing of epistemic language and for propagating uncertainty from symbolic belief inference into natural-language evaluation; ELoT makes it practical to connect symbolic probabilistic world models with LLM-based semantic parsing while preserving and quantifying uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e852.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e852.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grammar-constrained SMC parsing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grammar-constrained Sequential Monte Carlo LLM Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that constrains LLM decoding to a target grammar and uses Sequential Monte Carlo to sample weighted completions, providing a posterior distribution over structured outputs and avoiding beam-search failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Syntactic and semantic control of large language models via sequential monte carlo</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Grammar-constrained SMC decoding (LLM semantic parser)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompt an LLM with example translations and constrain its outputs to a formal grammar G_E (ELoT). Use Sequential Monte Carlo sampling to produce n_sigma weighted samples {(φ_i, w_i)} from the constrained completion distribution P_LLM(φ | σ, D, φ ∈ G_E). Use the top-weighted sample (or full weighted set) as the parsed ELoT output; SMC produces a posterior over parses that captures parser uncertainty and avoids greedy/beam failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Not itself a world model; it produces structured symbolic ELoT parses that are consumed by the symbolic probabilistic world model (BToM).</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>N/A (parsing method that outputs grammar-constrained symbolic formulas).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Generative backbone for producing constrained structured outputs (ELoT formulas) and for sampling a posterior over translations</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LLaMa 3.1 8B (primary); compared against unconstrained LLaMa 3.1 8B sampling and Gemini Flash 8B</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>LLM prediction / parsing uncertainty (distribution over possible ELoT parses)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Sequential Monte Carlo posterior sampling over constrained grammar completions; weighted sample set represents uncertainty over parses.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Used to translate human-written English epistemic sentences (about Doors, Keys & Gems) into ELoT; not itself an environment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Translation exactness and semantic-equivalence versus gold ELoT translations; downstream human-model correlation impact</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Grammar-constrained SMC decoding into ELoT improved equivalence accuracy by ~0.20 compared to unconstrained generation for LLaMa 3.1 8B and achieved ~91% semantic equivalence with gold translations on the evaluation subset.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Unconstrained self-consistency sampling with LLaMa 3.1 8B and Gemini Flash 8B; SMC outperformed both, avoiding syntax errors and mode collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Constrained SMC parsing into ELoT yields substantially better translation quality and downstream model fit than (a) translating to lowered representations and (b) unconstrained sampling; SMC's posterior samples can be used to model sentence ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Grammar-constrained SMC effectively channels LLM generation into a structured symbolic representation while producing a calibrated posterior over parses, enabling the propagation of LLM parsing uncertainty into symbolic probabilistic evaluation (BToM + ELoT).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e852.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e852.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q_MDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q_MDP approximation (value-function approximation for POMDPs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approximate method for planning under partial observability that estimates belief-space Q-values by averaging single-state optimal Q* values across belief hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Value-function approximations for partially observable markov decision processes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Q_MDP approximation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Q_MDP approximates the optimal belief-space Q* by computing Q_g(b,a) = Σ_{(s,w)∈b} w · Q*(s,a), where Q*(s,a) is the optimal cost-to-go from fully-observed state s. In LaBToM this is used to efficiently estimate Q_g for belief-aware instrumental planning by averaging shortest-path based Q*(s,a) values across belief hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>POMDP approximation operating over symbolic states (PDDL-like) and discrete belief distributions</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Treats each hypothesized fully-observed state s as if fully known, computes its Q*(s,a) (shortest-path cost-to-go), then averages these per-state Q*s weighted by belief weights to obtain belief-space Q_g(b,a). The method is an approximation (deterministic per-state Q* but probabilistic via averaging).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>State/belief uncertainty (approximated by averaging over hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Weighted averaging over discrete belief hypotheses (Q_MDP); belief uncertainty maintained in BToM is integrated via these weights.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Q_MDP approximation combined with shortest-path computation for Q*(s,a), then Boltzmann action selection</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Doors, Keys, & Gems (gridworld)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Used to compute instrumental cost-to-go estimates for the agent in the gridworld; Q*(s,a) estimated via shortest-path to goal in deterministic fully-known state s.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not reported as stand-alone numbers; used inside overall model where BToM + Q_MDP supports high model-human fit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Non-Planning ablation replaced Q_MDP with Manhattan-distance heuristic and performed substantially worse (especially at inferring initial beliefs).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Replacing belief-aware Q_MDP with a non-planning heuristic degrades model fit (Non-Planning ablation: overall r drops to 0.40 and initial belief r to 0.07), indicating that integrating uncertainty into planning matters.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Q_MDP is an efficient and practical approximation to integrate belief uncertainty into planning for symbolic domains where exact belief-space planning would be costly; its use here enables tractable computation of belief-aware action likelihoods for inverse planning and BToM inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e852.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e852.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gen probabilistic programming system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic programming platform used to implement the enumerative Bayesian inference and BSIPS algorithm in the paper, supporting programmable inference strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gen: a general-purpose probabilistic programming system with programmable inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gen (probabilistic programming system)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Gen is used to implement the BToM generative model and the BSIPS inference algorithm, enabling exact (enumerative) posterior computation over goals, initial states, and beliefs and providing a platform for future extensions (SMC approximations for larger state spaces).</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>Probabilistic-programming-backed representation of symbolic PDDL-like states and belief distributions</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Expresses the generative POMDP-like model (goal prior, state prior, belief prior, transitions, observation model, action selection) as a probabilistic program; inference used programmable strategies (enumeration for current experiments), making uncertainty explicit as probabilistic program latent variables.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>State/belief posterior uncertainty via probabilistic program latents</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Exact enumeration over hypotheses implemented in Gen; Gen can also support SMC/approximate inference (not required for the small scenario scale used).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td>Doors, Keys, & Gems (gridworld)</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td>Gen encodes the generative model and runs BSIPS simulations over the gridworld scenarios to produce posterior belief distributions used by ELoT evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Computational performance of inference (runtime per action) and correctness of posterior inferences (downstream human correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Enumerative inference runtimes: ~0.1s/action to ~20s/action depending on hypothesis count; supports exact posterior computation in the evaluated scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Probabilistic programming (Gen) provides a convenient, extensible implementation substrate for integrating symbolic world models, belief-state uncertainty, and structured LLM outputs into coherent inference pipelines; it facilitates exact inference at the scenario scale and can be extended with SMC for larger domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PDDL -the Planning Domain Definition Language <em>(Rating: 2)</em></li>
                <li>Planning and acting in partially observable stochastic domains <em>(Rating: 2)</em></li>
                <li>Value-function approximations for partially observable markov decision processes <em>(Rating: 2)</em></li>
                <li>A gentle introduction to epistemic planning: The del approach <em>(Rating: 2)</em></li>
                <li>Gen: a general-purpose probabilistic programming system with programmable inference <em>(Rating: 2)</em></li>
                <li>Syntactic and semantic control of large language models via sequential monte carlo <em>(Rating: 2)</em></li>
                <li>From word models to world models: Translating from natural language to the probabilistic language of thought <em>(Rating: 2)</em></li>
                <li>Belief-space Sequential Inverse Plan Search (SIPS) / Sequential Inverse Plan Search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-852",
    "paper_id": "paper-271923781",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [
        {
            "name_short": "LaBToM",
            "name_full": "Language-augmented Bayesian Theory-of-Mind",
            "brief_description": "A computational model that combines a compositional epistemic language-of-thought (ELoT) with a Bayesian theory-of-mind (BToM) that performs probabilistic inference over belief states and goals to evaluate natural-language epistemic claims.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LaBToM (Language-augmented Bayesian Theory-of-Mind)",
            "system_description": "LaBToM translates natural-language epistemic sentences into structured ELoT formulas via grammar-constrained LLM decoding (SMC sampling), then evaluates those formulas against probabilities produced by a Bayesian Theory-of-Mind module. The BToM module models an approximately rational agent in a POMDP-like generative process (goal prior, state prior, belief prior, state transitions, belief updates by perception, Boltzmann action selection over cost-to-go estimates). Observers perform exact Bayesian inference (enumerating initial states and discrete initial belief distributions) to get posterior distributions over goals, state trajectories, and belief histories; these posterior belief distributions are then used to compute Pr(A, φ) for ELoT expressions and yield normalized likelihood ratings comparable to human judgments.",
            "world_model_type": "PDDL-derived first-order symbolic states + discrete belief states (particle/distribution over states); POMDP-style belief-space representation",
            "world_model_description": "Base (non-epistemic) world model is a first-order logic language derived from PDDL: states are conjunctions of predicates/functions over objects. An agent belief b_t is a probability-weighted collection of environment state hypotheses {(s_i, w_i)} (particle-like discrete distributions). State transitions follow P(s_t | s_{t-1}, a_{t-1}) and belief updates filter hypotheses inconsistent with the agent's observations. Action selection is modeled via a Boltzmann policy over cost-to-go Q_g(b_t,a) estimated with a Q_MDP approximation (averaging Q* over belief hypotheses); Q*(s,a) is estimated by shortest-path cost in a deterministic state s. All representations are probabilistic (distributions over states, goals, and beliefs).",
            "uses_llm": true,
            "llm_role": "Semantic parsing: map natural language epistemic sentences to ELoT formulas; produces a distribution over translations via grammar-constrained sequential Monte Carlo (SMC) decoding, capturing parser uncertainty.",
            "llm_model_name": "LLaMa 3.1 8B (used for grammar-constrained SMC decoding); GPT-4o and Gemini used as multimodal baselines (not as core components)",
            "uncertainty_modeling": true,
            "uncertainty_type": "State/belief (agent epistemic) uncertainty; LLM parsing uncertainty (distribution over ELoT translations); uncertainty in action selection via Boltzmann randomness",
            "uncertainty_method": "Exact Bayesian inference by enumerating hypotheses (G × S0 × B0) for the experimental scenarios; belief represented as weighted hypotheses (k-particle discrete distributions); Q_MDP averaging over hypothesis Q* values; SMC sampling to represent distribution over LLM translations; Boltzmann policy with fitted inverse temperature β.",
            "planning_algorithm": "Belief-space planning via Q_MDP approximation: Q_g(b,a) = Σ_{(s,w)∈b} w · Q*(s,a), where Q*(s,a) estimated by shortest-path search; action selection via Boltzmann distribution over Q_g.",
            "planning_integrates_uncertainty": true,
            "text_environment_name": "Doors, Keys, & Gems (gridworld)",
            "text_environment_description": "A 2D gridworld where an agent must pick up single-use colored keys possibly hidden in boxes to unlock color-matched doors and reach one of four goal gems; environment state is fully symbolic (objects, boxes, keys, doors, gems); used here as the grounding environment for human experiments and BToM inference (not a text-only benchmark).",
            "performance_metric": "Correlation (Pearson r) with human normalized likelihood ratings for epistemic sentences; also translation accuracy for NL → ELoT (semantic equivalence), and in-vs-out-of-context classification accuracy",
            "performance_value": "Model-human Pearson r = 0.76 (full LaBToM across current+initial); with gold ELoT translations r = 0.81; ELoT grammar-constrained SMC translation equivalence accuracy up to 91%; LaBToM classifies in-context vs out-of-context statements correctly ~70% of the time.",
            "baseline_comparison": "Ablations and LLM baselines: Non-Planning ablation r = 0.40 overall (Current r = 0.58, Initial r = 0.07); True Belief ablation r = 0.10; GPT-4o best baseline (images + narratives + few-shot) r = 0.52 overall (Current 0.59, Initial 0.41); Gemini 1.5 Pro variants r ≈ 0.22-0.23.",
            "has_ablation_uncertainty": true,
            "ablation_results": "Ablations show modeling beliefs and planning under uncertainty is crucial: True-Belief (assume agent knows true state) performs very poorly (r ≈ 0.09), showing that ignoring agent epistemic uncertainty harms correspondence with human judgments. Non-Planning (heuristic Manhattan-distance cost) fails especially at inferring initial beliefs (Initial r ≈ 0.07), showing that assuming goal-directed epistemic planning is important. Ablations and parameter fits confirm thresholds and Boltzmann β matter but full probabilistic belief modeling yields best fit.",
            "key_findings": "A structured probabilistic symbolic world model (PDDL-like first-order states) combined with discrete belief-state representations and an explicit BToM that integrates perception, belief updates, and belief-aware planning (via Q_MDP) explains human judgments about epistemic language substantially better than multimodal LLM baselines. Grammar-constrained SMC parsing into an epistemic language-of-thought (ELoT) captures the compositional structure of epistemic language and helps propagate LLM parsing uncertainty into downstream probabilistic evaluation. Exact (enumerative) Bayesian inference over symbolic hypotheses is feasible at the scenario scale and is key to integrating world-model uncertainty into sentence evaluation.",
            "uuid": "e852.0",
            "source_info": {
                "paper_title": "Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "BSIPS",
            "name_full": "Belief-Space Sequential Inverse Plan Search",
            "brief_description": "An inference algorithm (belief-space variant of SIPS) that enumerates hypotheses over goals, initial states, and initial beliefs, simulates agent belief updates and actions, and computes exact posterior weights for hypotheses to infer agent goals and beliefs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Belief-Space Sequential Inverse Plan Search (BSIPS)",
            "system_description": "BSIPS enumerates hypothesis space H = G × S0 × B0 and maintains weights for each hypothesis. At each time step, it simulates state transitions and belief updates for each hypothesis, computes belief-space Q-values using a Q_MDP approximation with memoized shortest-path costs, computes action likelihoods via a Boltzmann policy, and reweights hypotheses by observation likelihood. For the paper's scenarios, BSIPS performs exact Bayesian posterior inference over beliefs/goals/states by enumeration (no Monte Carlo approximations required at this scale).",
            "world_model_type": "Belief-space POMDP-style enumerated hypotheses over PDDL-like symbolic states and discrete belief distributions (particle counts define B0 size)",
            "world_model_description": "Hypotheses combine a goal, an initial symbolic state s0 (from S0 consistent with initial observation), and an initial belief distribution b0 (discrete distributions formed by distributing k particles over ns states). Beliefs are updated deterministically by filtering inconsistent state hypotheses given agent observations; Q-values are computed by averaging per-state Q* values weighted by the belief distribution. All probabilities are maintained exactly over the finite hypothesis set.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "State and belief uncertainty (epistemic uncertainty over which symbolic state is true and what the agent believes)",
            "uncertainty_method": "Exact Bayesian inference by enumerating all discrete hypotheses (G × S0 × B0), particle-based discrete belief distributions (k particles), reweighting by action/observation likelihoods; memoization to reuse shortest-path computations.",
            "planning_algorithm": "Uses Q_MDP approximation for belief-space Q-values where Q*(s,a) computed by shortest-path cost; action likelihoods via Boltzmann policy.",
            "planning_integrates_uncertainty": true,
            "text_environment_name": "Doors, Keys, & Gems (gridworld)",
            "text_environment_description": "Same gridworld used for grounding; BSIPS simulated agent actions and belief updates in this symbolic environment.",
            "performance_metric": "Computational runtime per action and correctness of posterior inference as evidenced by LaBToM downstream correlation with human judgments",
            "performance_value": "Enumerative inference over 120 to 5,490 hypotheses runs from ~0.1 s/action (120 hypotheses) up to ~20 s/action (5,490 hypotheses) on an i7-1370P CPU with 64 GB RAM; enables exact posterior computation in the experimental scenarios.",
            "baseline_comparison": "Not directly compared as an algorithmic baseline; compared indirectly via ablations that modify planning or belief assumptions.",
            "has_ablation_uncertainty": null,
            "ablation_results": "NA as BSIPS is the implemented inference engine; ablations of the overall model that alter belief/planning assumptions demonstrate the importance of accurate belief-space inference (see LaBToM ablation results).",
            "key_findings": "BSIPS demonstrates that exact Bayesian inference over enumerated symbolic hypotheses is tractable for small discrete scenarios and provides a principled way to integrate symbolic world models, belief-state uncertainty, and planning models into inference about others' beliefs and into the evaluation of epistemic language.",
            "uuid": "e852.1",
            "source_info": {
                "paper_title": "Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "ELoT",
            "name_full": "Epistemic Language of Thought",
            "brief_description": "A compositional formal language for expressing epistemic claims that mirrors natural-language structure (operators like believes, knows, might, likely) and grounds modal/epistemic operators in Pr(A, φ) comparisons with fitted thresholds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ELoT (Epistemic Language of Thought)",
            "system_description": "ELoT is a formal, grammar-constrained representation layer sitting on top of a PDDL-derived first-order base language. Epistemic operators (believes, knows, might, could, likely, comparatives) are typed constructs whose semantics are defined by comparisons on Pr(A, φ) against operator-specific thresholds θ (learned/fitted). ELoT expressions can be lowered to probability-comparison formulas (Pr(A, φ) ≥ θ) for evaluation against BToM-inferred belief distributions. ELoT improves semantic parsing accuracy and better aligns with natural-language epistemic constructions than the lowered-only representation.",
            "world_model_type": "PDDL-derived first-order logic base language plus epistemic operators grounded in probabilistic semantics (Pr comparisons)",
            "world_model_description": "Base formulas Φ are first-order predicate formulas (e.g., exists k. key(k) ∧ inside(k, box2)). ELoT wraps these with epistemic operators producing expressions whose evaluation depends on agent-assigned probabilities Pr(A, φ) computed from the belief distribution over symbolic states; thresholds θ define semantics of modals/epistemic adjectives.",
            "uses_llm": true,
            "llm_role": "Target representation for grammar-constrained LLM semantic parsing; LLM produces ELoT formulas from English sentences (SMC samples a distribution over ELoT parses)",
            "llm_model_name": "LLaMa 3.1 8B (used with grammar-constrained SMC); unconstrained baselines used Gemini Flash 8B",
            "uncertainty_modeling": true,
            "uncertainty_type": "Parser uncertainty (distribution over ELoT translations); epistemic probability (Pr(A, φ)) used inside ELoT operators",
            "uncertainty_method": "Grammar-constrained SMC sampling produces weighted ELoT parse samples; BToM supplies probability distributions Pr over base formulas; thresholds Θ map probability values to modal wording.",
            "planning_algorithm": "ELoT itself is a representational layer; planning is handled by BToM/BSIPS (Q_MDP approximation) which produces the Pr(A, φ) values ELoT requires.",
            "planning_integrates_uncertainty": true,
            "text_environment_name": "Doors, Keys, & Gems (gridworld)",
            "text_environment_description": "ELoT was evaluated on human-collected sentences about this gridworld; translation accuracy and downstream model fit evaluated in that setting.",
            "performance_metric": "Translation accuracy (exact equality and semantic equivalence) from English sentences to ELoT; downstream correlation with human judgments when using parsed ELoT.",
            "performance_value": "Grammar-constrained SMC decoding into ELoT with LLaMa 3.1 8B achieved ~91% semantic-equivalence with gold ELoT translations; translating into ELoT was up to 2.4× more accurate than translating into the lowered form; LaBToM with gold ELoT translations achieved r = 0.81 with human ratings.",
            "baseline_comparison": "Lowered-form target (probability-comparison-only) produced substantially lower semantic parsing accuracy and downstream performance; unconstrained LLaMa sampling and Gemini Flash 8B also underperformed.",
            "has_ablation_uncertainty": false,
            "ablation_results": "Comparing ELoT vs. lowered representation: ELoT yields much higher parsing accuracy and downstream human-model correlation; grammar-constrained decoding provides additional benefits over unconstrained sampling.",
            "key_findings": "Designing a compositional, language-aligned epistemic representation (ELoT) is crucial for reliable semantic parsing of epistemic language and for propagating uncertainty from symbolic belief inference into natural-language evaluation; ELoT makes it practical to connect symbolic probabilistic world models with LLM-based semantic parsing while preserving and quantifying uncertainty.",
            "uuid": "e852.2",
            "source_info": {
                "paper_title": "Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Grammar-constrained SMC parsing",
            "name_full": "Grammar-constrained Sequential Monte Carlo LLM Decoding",
            "brief_description": "A method that constrains LLM decoding to a target grammar and uses Sequential Monte Carlo to sample weighted completions, providing a posterior distribution over structured outputs and avoiding beam-search failure modes.",
            "citation_title": "Syntactic and semantic control of large language models via sequential monte carlo",
            "mention_or_use": "use",
            "system_name": "Grammar-constrained SMC decoding (LLM semantic parser)",
            "system_description": "Prompt an LLM with example translations and constrain its outputs to a formal grammar G_E (ELoT). Use Sequential Monte Carlo sampling to produce n_sigma weighted samples {(φ_i, w_i)} from the constrained completion distribution P_LLM(φ | σ, D, φ ∈ G_E). Use the top-weighted sample (or full weighted set) as the parsed ELoT output; SMC produces a posterior over parses that captures parser uncertainty and avoids greedy/beam failure modes.",
            "world_model_type": "Not itself a world model; it produces structured symbolic ELoT parses that are consumed by the symbolic probabilistic world model (BToM).",
            "world_model_description": "N/A (parsing method that outputs grammar-constrained symbolic formulas).",
            "uses_llm": true,
            "llm_role": "Generative backbone for producing constrained structured outputs (ELoT formulas) and for sampling a posterior over translations",
            "llm_model_name": "LLaMa 3.1 8B (primary); compared against unconstrained LLaMa 3.1 8B sampling and Gemini Flash 8B",
            "uncertainty_modeling": true,
            "uncertainty_type": "LLM prediction / parsing uncertainty (distribution over possible ELoT parses)",
            "uncertainty_method": "Sequential Monte Carlo posterior sampling over constrained grammar completions; weighted sample set represents uncertainty over parses.",
            "planning_algorithm": null,
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": "Used to translate human-written English epistemic sentences (about Doors, Keys & Gems) into ELoT; not itself an environment.",
            "performance_metric": "Translation exactness and semantic-equivalence versus gold ELoT translations; downstream human-model correlation impact",
            "performance_value": "Grammar-constrained SMC decoding into ELoT improved equivalence accuracy by ~0.20 compared to unconstrained generation for LLaMa 3.1 8B and achieved ~91% semantic equivalence with gold translations on the evaluation subset.",
            "baseline_comparison": "Unconstrained self-consistency sampling with LLaMa 3.1 8B and Gemini Flash 8B; SMC outperformed both, avoiding syntax errors and mode collapse.",
            "has_ablation_uncertainty": true,
            "ablation_results": "Constrained SMC parsing into ELoT yields substantially better translation quality and downstream model fit than (a) translating to lowered representations and (b) unconstrained sampling; SMC's posterior samples can be used to model sentence ambiguity.",
            "key_findings": "Grammar-constrained SMC effectively channels LLM generation into a structured symbolic representation while producing a calibrated posterior over parses, enabling the propagation of LLM parsing uncertainty into symbolic probabilistic evaluation (BToM + ELoT).",
            "uuid": "e852.3",
            "source_info": {
                "paper_title": "Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Q_MDP",
            "name_full": "Q_MDP approximation (value-function approximation for POMDPs)",
            "brief_description": "An approximate method for planning under partial observability that estimates belief-space Q-values by averaging single-state optimal Q* values across belief hypotheses.",
            "citation_title": "Value-function approximations for partially observable markov decision processes",
            "mention_or_use": "mention",
            "system_name": "Q_MDP approximation",
            "system_description": "Q_MDP approximates the optimal belief-space Q* by computing Q_g(b,a) = Σ_{(s,w)∈b} w · Q*(s,a), where Q*(s,a) is the optimal cost-to-go from fully-observed state s. In LaBToM this is used to efficiently estimate Q_g for belief-aware instrumental planning by averaging shortest-path based Q*(s,a) values across belief hypotheses.",
            "world_model_type": "POMDP approximation operating over symbolic states (PDDL-like) and discrete belief distributions",
            "world_model_description": "Treats each hypothesized fully-observed state s as if fully known, computes its Q*(s,a) (shortest-path cost-to-go), then averages these per-state Q*s weighted by belief weights to obtain belief-space Q_g(b,a). The method is an approximation (deterministic per-state Q* but probabilistic via averaging).",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "State/belief uncertainty (approximated by averaging over hypotheses)",
            "uncertainty_method": "Weighted averaging over discrete belief hypotheses (Q_MDP); belief uncertainty maintained in BToM is integrated via these weights.",
            "planning_algorithm": "Q_MDP approximation combined with shortest-path computation for Q*(s,a), then Boltzmann action selection",
            "planning_integrates_uncertainty": true,
            "text_environment_name": "Doors, Keys, & Gems (gridworld)",
            "text_environment_description": "Used to compute instrumental cost-to-go estimates for the agent in the gridworld; Q*(s,a) estimated via shortest-path to goal in deterministic fully-known state s.",
            "performance_metric": "Not reported as stand-alone numbers; used inside overall model where BToM + Q_MDP supports high model-human fit.",
            "performance_value": null,
            "baseline_comparison": "Non-Planning ablation replaced Q_MDP with Manhattan-distance heuristic and performed substantially worse (especially at inferring initial beliefs).",
            "has_ablation_uncertainty": null,
            "ablation_results": "Replacing belief-aware Q_MDP with a non-planning heuristic degrades model fit (Non-Planning ablation: overall r drops to 0.40 and initial belief r to 0.07), indicating that integrating uncertainty into planning matters.",
            "key_findings": "Q_MDP is an efficient and practical approximation to integrate belief uncertainty into planning for symbolic domains where exact belief-space planning would be costly; its use here enables tractable computation of belief-aware action likelihoods for inverse planning and BToM inference.",
            "uuid": "e852.4",
            "source_info": {
                "paper_title": "Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Gen",
            "name_full": "Gen probabilistic programming system",
            "brief_description": "A probabilistic programming platform used to implement the enumerative Bayesian inference and BSIPS algorithm in the paper, supporting programmable inference strategies.",
            "citation_title": "Gen: a general-purpose probabilistic programming system with programmable inference",
            "mention_or_use": "use",
            "system_name": "Gen (probabilistic programming system)",
            "system_description": "Gen is used to implement the BToM generative model and the BSIPS inference algorithm, enabling exact (enumerative) posterior computation over goals, initial states, and beliefs and providing a platform for future extensions (SMC approximations for larger state spaces).",
            "world_model_type": "Probabilistic-programming-backed representation of symbolic PDDL-like states and belief distributions",
            "world_model_description": "Expresses the generative POMDP-like model (goal prior, state prior, belief prior, transitions, observation model, action selection) as a probabilistic program; inference used programmable strategies (enumeration for current experiments), making uncertainty explicit as probabilistic program latent variables.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "State/belief posterior uncertainty via probabilistic program latents",
            "uncertainty_method": "Exact enumeration over hypotheses implemented in Gen; Gen can also support SMC/approximate inference (not required for the small scenario scale used).",
            "planning_algorithm": null,
            "planning_integrates_uncertainty": null,
            "text_environment_name": "Doors, Keys, & Gems (gridworld)",
            "text_environment_description": "Gen encodes the generative model and runs BSIPS simulations over the gridworld scenarios to produce posterior belief distributions used by ELoT evaluations.",
            "performance_metric": "Computational performance of inference (runtime per action) and correctness of posterior inferences (downstream human correlation)",
            "performance_value": "Enumerative inference runtimes: ~0.1s/action to ~20s/action depending on hypothesis count; supports exact posterior computation in the evaluated scenarios.",
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Probabilistic programming (Gen) provides a convenient, extensible implementation substrate for integrating symbolic world models, belief-state uncertainty, and structured LLM outputs into coherent inference pipelines; it facilitates exact inference at the scenario scale and can be extended with SMC for larger domains.",
            "uuid": "e852.5",
            "source_info": {
                "paper_title": "Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PDDL -the Planning Domain Definition Language",
            "rating": 2,
            "sanitized_title": "pddl_the_planning_domain_definition_language"
        },
        {
            "paper_title": "Planning and acting in partially observable stochastic domains",
            "rating": 2,
            "sanitized_title": "planning_and_acting_in_partially_observable_stochastic_domains"
        },
        {
            "paper_title": "Value-function approximations for partially observable markov decision processes",
            "rating": 2,
            "sanitized_title": "valuefunction_approximations_for_partially_observable_markov_decision_processes"
        },
        {
            "paper_title": "A gentle introduction to epistemic planning: The del approach",
            "rating": 2,
            "sanitized_title": "a_gentle_introduction_to_epistemic_planning_the_del_approach"
        },
        {
            "paper_title": "Gen: a general-purpose probabilistic programming system with programmable inference",
            "rating": 2,
            "sanitized_title": "gen_a_generalpurpose_probabilistic_programming_system_with_programmable_inference"
        },
        {
            "paper_title": "Syntactic and semantic control of large language models via sequential monte carlo",
            "rating": 2,
            "sanitized_title": "syntactic_and_semantic_control_of_large_language_models_via_sequential_monte_carlo"
        },
        {
            "paper_title": "From word models to world models: Translating from natural language to the probabilistic language of thought",
            "rating": 2,
            "sanitized_title": "from_word_models_to_world_models_translating_from_natural_language_to_the_probabilistic_language_of_thought"
        },
        {
            "paper_title": "Belief-space Sequential Inverse Plan Search (SIPS) / Sequential Inverse Plan Search",
            "rating": 1,
            "sanitized_title": "beliefspace_sequential_inverse_plan_search_sips_sequential_inverse_plan_search"
        }
    ],
    "cost": 0.0220655,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind
18 Apr 2025</p>
<p>Lance Ying 
Massachusetts Institute of Technology
CambridgeMA</p>
<p>Harvard University
CambridgeMA</p>
<p>Tan Zhi-Xuan 
Massachusetts Institute of Technology
CambridgeMA</p>
<p>Lionel Wong 
Massachusetts Institute of Technology
CambridgeMA</p>
<p>Vikash Mansinghka 
Massachusetts Institute of Technology
CambridgeMA</p>
<p>Joshua B Tenenbaum 
Massachusetts Institute of Technology
CambridgeMA</p>
<p>Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind
18 Apr 2025D71D58B299BED3A41CB568B82148E1D9arXiv:2408.12022v2[cs.CL]
How do people understand and evaluate claims about others' beliefs, even though these beliefs cannot be directly observed?In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents' goals, beliefs, and intentions: a language-augmented Bayesian theory-ofmind (LaBToM).By translating natural language into an epistemic "language-ofthought" with grammar-constrained LLM decoding, then evaluating these translations against the inferences produced by inverting a generative model of rational action and perception, LaBToM captures graded plausibility judgments of epistemic claims.We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent's beliefs.In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.</p>
<p>Introduction</p>
<p>People regularly use and interpret language about other agents' beliefs, evaluating rich linguistic constructions that may involve claims about what others' consider possible ("Grace thinks Katie might have eaten the cookie"), what they find more probable ("Tom thinks Sam is the most likely to win the election"), or the relationship of their beliefs to the world ("John didn't know that today was a holiday").But since these beliefs are not directly observable, how do people judge the truth * Equal Contribution.</p>
<p>or acceptability of epistemic claims?Philosophers and linguists have long investigated the semantics of epistemic language (Hintikka, 1962;Partee, 1973;Loar, 1981), offering compositional theories of how epistemic claims relate to the sets of worlds deemed possible by an agent (Von Fintel and Heim, 2011).However, these theories do not explain how people ground epistemic language in its context of utterance.If someone says "Alice believes it might rain", how does her behavior (e.g.bringing an umbrella) render that statement more or less plausible or acceptable?</p>
<p>In this paper, we present a computational model of how people interpret epistemic language in context, grounded in inferences about what other agents believe (Figure 1) -a Languageaugmented Bayesian theory-of-mind (LaBToM).We build upon Bayesian theory-of-mind (BToM), a framework which casts human mentalizing as probabilistic inference over a generative model of rational action and perception (Baker et al., 2017;Jara-Ettinger et al., 2020).We combine this framework with the compositionality afforded by probabilistic extensions of the language-ofthought hypothesis (Piantadosi, 2011;Goodman and Lassiter, 2015), developing an epistemic language of thought (ELoT) to represent how others represent the world.We parse natural language to this representation using grammarconstrained sequential Monte Carlo decoding (Loula et al., 2025) of large language models (LLMs), defining a flexible but structured probabilistic mapping from epistemic language to epistemic concepts (Wong et al., 2023).This allows us to quantify the plausibility of epistemic sentences against BToM inferences.</p>
<p>To evaluate our model, we run an experiment where participants watch animations of a player solving a gridworld puzzle called Doors, Keys, &amp; Gems (Zhi-Xuan et al., 2020).In these puzzles, the player has to pick up (single-use) keys that Here, our model evaluates the plausibility of epistemic language about a player character trying to find keys in boxes so as to reach one of four goals (gems with different shapes/colors, g 1 -g 4 ) which may be locked behind doors.(Top) We translate natural language statements about the player's initial (σ 1 ) and current beliefs (σ 2 ) into an unambiguous epistemic language of thought (ELoT) via grammar-constrained LLM parsing.ELoT statements (i.e.epistemic formulas φ) are interpreted with a probability-based semantics (lowered formulas φ ′ ).(Bottom) We use our Bayesian theory-of-mind (BToM) module to produce inferences (bar charts) about the environment state (top left), the player's goal (bottom left), and the player's belief state (right) at each step t, given observations across time.Each possible belief state b i is itself a distribution over environment states s j , where state s j corresponds to a blue key being in box j. (Middle) We evaluate the posterior probability * of each ELoT statement φ from the BToM inferences at each step t ( * under a 50-50 prior over statement truth).Statement σ 1 ("The player thought that the blue key must be in box 3.") increases in probability from t=6 to t=9, then stays high, since it becomes clear from the player looking in box 3 at t=9 that they initially thought a key must be in box 3. Statement σ 2 ("The player believes that a blue key is more likely to be in box 1 than box 2.") decreases in probability as the player walks away from box 1 at t=6 through t=9.However, when the player finds box 3 empty, then moves past box 2 up to box 1 at t=15, it becomes much more probable that the player currently thinks a key is more likely to be in box 1 rather than box 2. may be hidden in boxes, using them to unlock doors of the same color to reach one of four valuable gems.The player's beliefs and goals are unknown to our participants, so they must infer these mental states.We ask one set of participants to write sentences describing the past and present beliefs of the player, collecting a rich dataset of epistemic language that includes modal verbs, uncertainty expressions, knowledge claims, and descriptions of false beliefs.We task another set of participants with evaluating these statements, asking them to rate how likely a statement is given the context.We find that the inferences produced by our LaBToM model correlate highly with these human ratings.In contrast, using a lowerlevel formalization of epistemic claims leads to substantially more parsing errors than our ELoT representation, and ablated BToM models and multimodal LLM baselines fail to explain human sentence judgments.These findings illustrate the importance of a conceptual vocabulary that is aligned with the structure of epistemic language, and the need for a coherent theory-of-mind to systematically interpret such language.</p>
<p>Model-theoretic formal semantics of epistemic language.Sentences about what agents believe or know have been studied extensively across philosophy and linguistics.</p>
<p>Both epistemic modals (Yalcin, 2007;Coates, 1987;Egan and Weatherson, 2011;MacFarlane, 2011) and propositional attitudes (Frege, 1948;Russell, 1903;Schiffer, 2003) complicate compositional and model-theoretic approaches to assigning truth conditions.Standard treatments associate belief sentences with sets of compatible worlds (Hintikka, 1962;Von Fintel and Heim, 2011), though the role of an agent's mental states in a truth-conditional semantics remains debated (Egan and Weatherson, 2011).More recent work grounds the gradedness of belief claims in probability (Lassiter, 2017;Moss, 2015), which we follow in this paper.Unlike purely modeltheoretic approaches, however, our model relates epistemic sentences to a functional theory of agents' mental states (Loar, 1981), explaining how people ground these sentences in agent behavior.</p>
<p>Cognitive approaches to (epistemic) language interpretation.Our model draws on accounts of propositional attitudes (A.Fodor, 1981) and word meaning that emphasize the role of mental states (Loar, 1981;Block, 1987;Lake and Murphy, 2023) and the interface between language and conceptual content, as in cognitive or psychosemantics (Fodor, 1987;Lakoff, 1988;Jackendoff, 2003) and functional role semantics (Harman, 1982).We build most closely upon theories that ground linguistic meaning in a (probabilistic) language of thought (Fodor et al., 1975;Goodman and Lassiter, 2015;Wong et al., 2023;Zhang et al., 2023), and models that map sentences to grounded symbolic representations of agents' behavior (Artzi and Zettlemoyer, 2013).</p>
<p>Bayesian Theory-of-Mind.To relate epistemic sentences to (inferred) mental states, we build on the Bayesian Theory-of-Mind framework (Baker et al., 2017;Zhi-Xuan et al., 2022), and related work on epistemic action understanding (Croom et al., 2023;Shvo et al., 2020).BToM provides a functional theory of propositional attitudes like knowledge and belief, leveraging the connection between symbolic representations of the world (used in (inverse) planning (McDermott et al., 1998)) and our representations of others' minds.</p>
<p>Theory-of-Mind in Language Models.With recent advances in LLM capabilities, some researchers have suggested that LLMs might serve as cognitive models (Binz and Schulz, 2023), including as models of theory-of-mind (Strachan et al., 2024), thereby implicitly capturing the semantics of belief sentences (Piantadosi and Hill, 2022).However, while LLMs perform well on some ToM tasks, they do not reliably generalize (Shapira et al., 2024) to richer multi-step (Kim et al., 2023) or multi-modal contexts (Jin et al., 2023;Ying et al., 2024a).Our model instead uses (grammar-constrained) LLMs as a flexible mappings between language and formal meaning representations (Wong et al., 2023;Ying et al., 2023a,b;Zhi-Xuan et al., 2024b), tying language to an explicit semantics of epistemic concepts.</p>
<p>Computational Model</p>
<p>Our LaBToM model comprises two interlinked modules.The first module (Figure 1, Top), an epistemic language of thought (ELoT), models our capacity to compositionally represent the world (including the contents of others' minds) by combining more basic concepts into richer thoughts and expressions (Goodman and Lassiter, 2015), and how we flexibly translate such thoughts from natural language (Wong et al., 2023).The second module (Figure 1, Bottom), a Bayesian theory-of-mind (Baker et al., 2017), captures our intuitive inferences about others' minds via Bayesian inference over a generative model of how agents update their beliefs and act towards their goals.Epistemic language understanding can thus be modeled by mapping language into ELoT formulas, which we evaluate against the inferences produced by rational mentalizing.</p>
<p>Interpreting Belief Sentences with an Epistemic Language of Thought</p>
<p>To represent epistemic concepts in a way that mirrors the structure of natural language, we introduce a formal language (Table 1) as our epistemic language of thought.Drawing upon the formal semantics of epistemic language (Hintikka, 1962;Lassiter, 2010), we adopt a compositional degree-based semantics for epistemic concepts grounded in probability comparisons (Moss, 2015;Lassiter, 2017).This allows us to quantitatively evaluate an epistemic expression using probabilities inferred by our BToM module.
(A, ϕ) E A, Φ Pr(A, ϕ) ≥ θ believes believes modal (A, M ) E A, E/A M (A) Knowledge Operators knows that (A, ϕ) E A, Φ believes(A, ϕ) ∧ ϕ knows if (A, ϕ) E A, Φ knows that (A, ϕ) ∨ knows that (A, ¬ϕ) knows about (A, C, ϕ) E A, Φ/O, Φ/O ∃x.C(x) ∧ knows that (A, ϕ(x)) not-knows that (A, ϕ) E A, Φ ¬believes(A, ϕ) ∧ ϕ Certainty Operators certain that (A, ϕ) E A, Φ Pr(A, ϕ) ≥ θ certain certain about (A, C, ϕ) E A, Φ/O, Φ/O ∃x.C(x) ∧ (Pr(A, ϕ(x)) ≥ θ certain ) uncertain if (A, ϕ, ψ) E A, Φ, Φ (Pr(A, ϕ) &lt; θ uncertain ) ∧ (Pr(A, ψ) &lt; θ uncertain ) uncertain about (A, C, ϕ) E A, Φ/O, Φ/O ∀x.C(x) → (Pr(A, ϕ(x)) &lt; θ uncertain ) Modal Verbs could(ϕ) E/A Φ λA.Pr(A, ϕ) ≥ θ could might(ϕ) E/A Φ λA</p>
<p>Representing Epistemic Formulas</p>
<p>We first define our non-epistemic base language -a first order logic derived from the Planning Domain Definition Language (McDermott et al., 1998).Our language assumes a set of predicates P and functions F used to describe a set of objects O. Predicates can be combined into formulas ϕ ∈ Φ via logical operators or quantification.For example, "A key is in box 2" can be represented as ∃k.key(k) ∧ inside(k, box2).Conceptually, a state s of the environment (or an agent's mental representation of state s) is just a large formula: A conjunction of predicates which fully describe the state.We denote the truth value of ϕ in s as ϕ s .On top of this base language Φ, we introduce epistemic expressions φ ∈ E to model assertions of belief, knowledge, or modal qualifications (Table 1a).</p>
<p>Following Lassiter (2017), the semantics of these expressions are grounded in the probability function Pr(A, ϕ) -the probability assigned by agent A to formula ϕ -and comparisons with term-specific thresholds (Table 1b).For example, the operator might(ϕ) takes a first-order formula ϕ ∈ Φ, returning a function λA.Pr(A, ϕ) ≥ θ might .Combined with the operator believes modal (A, F ) = F (A), we can express the claim that "A believes it might be that ϕ" as φ = believes modal (A, might(ϕ)).This formula φ can be lowered to the probability comparison Pr(A, ϕ) ≥ θ might , which uses θ might as the threshold instead of θ believes due to how our composition rules.We call this latter representation a lowered formula.</p>
<p>A key aspect of ELoT expressions is that they represent epistemic concepts at a similar level of granularity as our tested language (English), simplifying the mapping to ELoT propositions.In contrast, our lowered representation is just as expressive, but fails to match the structure of natural epistemic language.In our experiments, we show that translating into ELoT significantly improves the accuracy of semantic parsing.</p>
<p>Translating Epistemic Language</p>
<p>Natural language is varied and imprecise, with possibly many sentences σ mapping to the same ELoT formula φ (and vice versa).To handle this diversity, we perform semantic parsing via grammar-constrained LLM decoding (Shin et al., 2021;Scholak et al., 2021;Willard and Louf, 2023), producing a flexible context-sensitive mapping between natural language and ELoT expressions (Wong et al., 2023).Specifically, we use sequential Monte Carlo (SMC)-based grammar-constrained sampling (Loula et al., 2025), since it avoids the failure modes of beam search and greedy token-masking (Lew et al., 2023b).We prompt an LLM (LLaMa 3.1 8B) with example translations D from English to ELoT and a sentence σ to translate, using SMC to approximate the distribution over completions constrained to the ELoT grammar G E :
φ ∼ P LLM (φ|σ, D, φ ∈ G E )(1)
We run SMC with n σ samples.This produces weighted samples {(φ i , w i )} nσ i=1 , and we use the top-weighted sample as our ELoT translation φ.1</p>
<p>Inferring and Evaluating Beliefs with a</p>
<p>Bayesian Theory-of-Mind</p>
<p>Our ELoT semantics reduces belief expressions to the probability Pr(A, ϕ) an agent A assigns to a sentence ϕ.But where does Pr(A, ϕ) come from?This is the function of our BToM module: By modeling the functional role that belief plays in guiding actions, along with the influence of perceptions on beliefs, an observer can infer what the agent thinks based on what the agent sees and does.Following the structure of Partially Observable Markov Decision Processes (POMDPs) (Kaelbling et al., 1998), this theory of approximately rational agency can be formalized as a probabilistic generative model:
Goal Prior: g ∼ P (g) (2) State Prior: s 0 ∼ P (s 0 ) (3) Belief Prior: b 0 ∼ P (b 0 |s 0 ) (4) State Transition: s t ∼ P (s t |s t−1 , a t−1 ) (5) Belief Update: b t ∼ P (b t |s t , b t−1 ) (6) Action Selection: a t ∼ P (a t |b t , g) (7) Observations: o t ∼ P (o t |s t ) (8)</p>
<p>Modeling Perception and Action</p>
<p>Two crucial aspects of our BToM module are how it models belief updating (Eq.6) as the result of perception, and how it models goal-directed action given the agent's uncertain beliefs (Eq.7).To model perception, we represent an agent's belief b t as a probability-weighted collection {(s i , w i )} ns i=1 of possible environment states si (which are represented in turn as collections of ELoT predicates).Given an observation of the environment s t (e.g.observing that a box is empty), the agent updates its belief by filtering out inconsistent hypotheses si , setting w i = 0.</p>
<p>As for goal-directed action, our model builds upon methods for epistemic planning (Bolander, 2017) and belief-space planning in POMDPs (Littman et al., 1995).Given a belief b t , the agent engages in instrumental planning to achieve their goal g, which requires achieving instrumental subgoals (e.g.picking up keys), but also gathering goal-relevant information (e.g.finding out if a key is in a certain box).We model this by assuming that the agent acts by approximately minimizing a cost-to-go estimate Qg (b t , a): An estimate of the optimal cost Q * g (b t , a) of reaching g after action a starting from one's (uncertain) belief b t .Action selection can thus be modeled by a Boltzmann distribution over these Qg estimates:
P (a t |b t , g) ∝ exp −β Qg (b t , a)(9)
To estimate Q * g efficiently, we follow recent advances in inverse planning (Zhi-Xuan et al., 2024b) by computing the Q MDP approximation (Hauskrecht, 2000) of Q * g , averaging over the Qvalues for each hypothesis (s i , w i ) in the belief b t :
Qg (b t , a) = (s i ,w i )∈bt w i • Q * g (s i , a)(10)
The cost-to-go Q * g (s, a) from a known state s can itself be efficiently estimated by searching for a shortest path to g from s (Monfort et al., 2015).</p>
<p>Joint Inference of Goals and Beliefs</p>
<p>With this generative model, observers can jointly infer the agent's goal g, belief history b 0:T , and environment trajectory s 0:T given observations of the agent's actions a 1:T and partial observations o 0:T of the environment:
P (g, b 0:T , s 0:T |a 1:T , o 0:T ) ∝ (11) P (g, s 0 , b 0 ) T t=1 P (b t , a t , s t , o t |b t−1 , s t−1 )
To ensure tractable posterior inference, we considered only the set of initial states S 0 consistent with the initial observation o 0 , and a discrete set B 0 of possible beliefs b 0 sufficient to model comparative likelihood claims (e.g."The key is more likely in box 1 than 2.").Specifically, we consider all beliefs formed by distributing k particles across
n s := |S 0 | states, resulting in n b := |B 0 | = ns+k−1 k distributions.
We then perform exact Bayesian inference over all combinations of goals g, initial beliefs b 0 , and states s 0 , which we implement as a variant of Sequential Inverse Plan Search (Zhi-Xuan et al., 2020) using the Gen probabilistic programming system (Cusumano-Towner et al., 2019).More algorithmic details are provided in the Appendix.</p>
<p>Evaluating Epistemic Sentences</p>
<p>By inferring the agent's belief history b 0:T , we can compute the probability Pr(A, ϕ) of a formula ϕ at time t as the expected truth value under b t :
Pr(A, ϕ) = (s i ,w i )∈bt w i • ϕ s(12)
We can thus evaluate a epistemic formula φ given a belief b t and environment state s t by replacing Pr terms with their values, then determining the truth of the resulting expression in state s.We denote this operation by φ (st,bt) .However, observers do not have access to the true s t or b t , only inferences about them.As such, we model human evaluation of a sentence φ as a probabilistic judgment given their inferences:
P ( φ (st,bt) |a 1:T , o 0:T ) = E st,bt∼P (st,bt|a 1:T ,o 0:T ) φ (st,bt)
While these judgements are made after observing actions up to time T , φ may be retrospectively evaluated as a description of the agent's beliefs at any t ∈ [0, T ], with t=0 and t=T corresponding to initial and current beliefs respectively.</p>
<p>Following Ying et al. (2024b), we also assume that people provide ratings as if they have a uniform prior U φ about the truth of φ.Under this prior, ratings of φ can be interpreted as a normalized likelihood L( φ (st,bt) |a 1:T , o 0:T ), which measures the likelihood of statement φ relative to its negation ¬φ.In other words, we assume that humans rate an epistemic claim more highly when they have evidence for it.With no evidence, L( φ (st,bt) |a 1:T , o 0:T ) = 0.5.Results investigating the importance of this assumption can be found in the Appendix.</p>
<p>Experiments</p>
<p>To evaluate our model on a diverse dataset of epistemic language (Table 2), we conducted a two-part human experiment.We first recruited participants to write English sentences describing the current and initial beliefs of a player character as it navigated a gridworld puzzle that required finding keys hidden in boxes.Next, we asked two groups of participants to rate how likely these sentences were to be true given the player's behavior, with one group rating sentences about the player's current beliefs, and the other rating sentences about initial beliefs.</p>
<p>With this data, we evaluated our model by: (i) assessing the translation accuracy of our ELoT module, investigating the importance of our ELoT representation (vs. the lowered form) and its impact on grammar-constrained LLM decoding; (ii) testing our full LaBToM model in its ability to capture human interpretation of epistemic language, comparing human ratings against LaBToM inferences.Model and experiment code is available at https://osf.io/xq9c4/.</p>
<p>Scenario Construction</p>
<p>We constructed 20 scenarios in the Doors, Keys, &amp; Gems environment with varied maze designs and item locations (Figure 3).In each scenario, there were 4 goal gems with different shapes (triangle, square, hexagon, circle), some of which were locked behind doors.Scenarios also had 2 to 3 boxes with up to 2 colored keys among them.The player's actions were varied across scenarios to elicit inferences about a diversity of epistemic states, such as ignorance about key locations or false confidence about the location of a key.</p>
<p>Collecting Epistemic Language</p>
<p>In the first part of the experiment, we recruited 42 US participants via Prolific (mean age: 36.02,SD: 10.1; 16 women, 26 men).Following a tutorial, participants watched 10 scenario animations, with each stopping before the player reached their goal and all relevant keys were revealed.Participants were then asked to write at least two sentences about the player's likely beliefs at the end of the scenario (current beliefs), and another two sentences about the player's beliefs at the start of the scenario (initial beliefs).</p>
<p>To ensure that these sentences focused on beliefs about the environment, we instructed participants to The player believes box 1 may contain a blue key or a red key.The player believes if the red key is not in box 2 then it must be in box 3.</p>
<p>Probability</p>
<p>Sentences with probability expressions such as likely, uncertain, etc. 28 / 241 28 / 228</p>
<p>The player thought that box 1 was most likely to contain a red key.The player is unsure what color the key in box 2 will be.</p>
<p>Compositionality Sentences that embed compound propositions (conjunctions, disjunctions, etc.).49 / 241 69 / 228</p>
<p>The player thinks that there's more likely to be a red key in box 1 or 3 than box 2. The player believes that if box 1 does not have a blue key, then box 3 has a blue key.</p>
<p>Knowledge</p>
<p>Sentences that make knowledge or ignorance claims.17 / 241 20 / 228</p>
<p>The player already knows for sure there is no key in box 1 or box 2. The player did not know if box 2 contained a red key.</p>
<p>Table 2: Overview of our dataset of 464 human-written epistemic sentences, broken down by factors.</p>
<p>describe the player's beliefs about the contents of the boxes.We excluded 8 participants for failing to follow these instructions, and about one third of remaining sentences (see Appendix).This process left us with 241 (228) statements about current (initial) beliefs, which were then annotated with factors by two experimenters.Table 2 illustrates the diversity of language we collected.</p>
<p>Evaluating Epistemic Language</p>
<p>For the next part of our experiment, we recruited 94 US participants via Prolific to evaluate current belief statements (mean age = 35.7,SD = 12.4,67 women, 27 men), and another 104 US participants to evaluate initial belief statements (mean age = 35.27,SD = 11.7,69 women, 33 men, 2 nonbinary).Each participant was shown 10 out of 20 scenario animations, and was asked to rate the goals and beliefs of the player at several judgment points during each animation.For goals, participants were shown a checkbox for each gem, and asked to select all gems likely to be the agent's goal.This served as both an attention check and an additional data source for model validation.For beliefs, participants were shown 2 belief statements selected from our dataset of human-written statements, and asked to rate how likely each statement was on a scale from 1 to 7.These ratings were normalized between 0 and 1 for our analysis.We excluded 7 participants from the current belief condition and 5 from the initial belief condition for low outlying scores on the goal inference subtask.</p>
<p>Statement Selection</p>
<p>To ensure that the belief statements evaluated by our participants were (i) diverse and (ii) rated enough times to ensure sufficient statistical power (88% power at Cohen's d = 0.8), we selected 5 statements per scenario (3 plausible, 2 implausible) from our full dataset of statements.</p>
<p>The plausible statements were chosen by sampling many sets of 3 statements out of all those written for a scenario, then selecting the set that scored highest on a diversity metric derived from the factors in Table 2 (see Appendix for details).We then manually added 2 more statements that were originally written for other scenarios, and which we evaluated to be implausible descriptions of the target scenario.Participants were shown 2 of these 5 statements at random in each scenario.</p>
<p>ELoT Parser Evaluation</p>
<p>We evaluated the suitability of ELoT formulas as representations of epistemic sentences by investigating (i) the impact of using ELoT as the translation target (instead of the lowered form, which lacks epistemic operators besides Pr) and (ii) the value of the associated ELoT grammar G E in constraining LLM outputs.</p>
<p>To do so, we created gold translations of the subset of statements selected for human evaluation (125 statements) into ELoT formulas and their lowered forms.We then compared the translations produced by grammar-constrained SMC decoding (n σ =10 samples) of LLaMa 3.1 8B against these gold translations in terms of both strict equality and approximate semantic equivalence (i.e.cases of reasonable alternative translations of the original English statement).As baselines, we used unconstrained self-consistency sampling (Wang et al., 2023) with LLaMa 3.1 8B and the instruction-tuned Gemini Flash 8B, sampling n σ =10 times and taking the majority answer.All methods used the same prompt format (see Appendix), with 36 example translations.</p>
<p>LaBToM Fitting and Evaluation</p>
<p>We evaluated our LaBToM model on all 20 scenarios, producing normalized likelihood scores for the 5 current and 5 initial belief statements per scenario.We then computed Pearson's r between these scores and average human ratings.We fit our model parameters to maximize r, fitting the belief thresholds Θ := (θ believes , θ could ...) via coordinate ascent, and the inverse temperature β via grid search (see Appendix for robustness analyses).This produced the fitted values for Θ in Table 1, and β = 2 3/2 .For the set of possible initial agent beliefs B 0 , we fixed the number of belief particles to k = 3 to ensure tractable exact inference.</p>
<p>Alongside this direct comparison with humanprovided ratings, we evaluated our model on the full dataset of 469 human-written statements by pairing each statement with either the scenario it was written for (in-context) or 1-2 other scenarios with the same map layout but distinct agent trajectories (out-of-context).This allowed us to compare each statement's in-context normalized likelihood L with its (average) out-of-context likelihood score.Reasonable models of epistemic language interpretation should assign higher likelihood scores to most statements when they are evaluated in-context vs. out-of-context.</p>
<p>Baselines</p>
<p>To assess the import of a sufficiently rich theory of mind for epistemic language understanding, we evaluated several ablations of our model under simplified assumptions about the agent's beliefs or planning abilities.We also evaluated two stateof-the-art multi-modal LLMs, thereby testing the degree to which grounded evaluation of epistemic language can be achieved with sufficient scale:</p>
<p>True Belief.The True Belief ablation assumes that the observed agent has fully accurate beliefs about the environment (i.e. they already know where all the keys are located), equivalent to the full model from Ying et al. (2024b).The observer starts with a uniform prior over these true beliefs.</p>
<p>Non-Planning.The Non-Planning ablation assumes that the agent is incapable of planning towards instrumental subgoals (such as keys), and instead optimizes the heuristic of moving physically closer to the goal.This is implemented by using the Manhattan distance to the goal as the agent's cost-to-go estimate Qg .Multi-modal LLMs.We use GPT-4o (text &amp; image input, gpt-4o-2024-05-13) and Gemini 1.5 Pro (text &amp; image or video input) as multi-modal LLM baselines, providing them the same instructions as human participants (see Appendix for a unimodal baseline).Each baseline was run 3 times with a temperature of 1.0.In addition to providing an image or video of the scenario showing the actions up to each judgment point, we used the following prompting methods: Plans: The prompt plainly describes the agent's actions over time (e.g. the player moves right five times), and also describes the agent's observations (e.g. the player opens box 1 and finds a red key).</p>
<p>Narratives: A rich narrative of agent behavior is included in the prompt, providing key information about the scene (e.g. which gems are locked behind doors, which keys are visible), while also describing the agent's movements in relation to relevant objects (e.g. the player moves right five times, going past box 1 towards box 2).</p>
<p>Few-Shot Prompting: After describing the plan or narrative, we provide the LLM with human ratings for the 4 other statements tied that scenario, before querying its rating for the target statement.</p>
<p>Results</p>
<p>ELoT is a superior translation target and guide for (grammar-constrained) LLM parsing.We report translation accuracy in Table 3.</p>
<p>In confirmation of the idea that our ELoT formalism captures the semantics of belief sentences at the right level of granularity, we find that translating English into ELoT is up to 2.4 times as accurate as translating into the lowered form, even though they have the same expressive capacity.ELoT also increases the benefit of grammar-constrained   SMC decoding, leading to an improvement of 0.20 in equivalence accuracy for LLaMa 3.1 8B.In contrast, using the lowered grammar leads to only an 0.06 improvement over unconstrained generation.We show example translations and errors for each representation in Table 4.These findings highlight the importance of "language-ofthought" representations that are aligned with the structure of natural language, in line with work tying language use to the structure and acquisition of compositional concepts (Wong et al., 2022).Overall, grammar-constrained SMC decoding into ELoT with LLaMa 3.1 8B performs the best, achieving 91% equivalence with our gold translations.Upon inspection (see Appendix), we find that unconstrained sampling often leads to syntax errors not corrected for by majority voting.Gemini Flash 8B also exhibits low sample diversity typical of instruction tuning, such that majority voting gives no improvement.In comparison, grammar-constrained SMC produces not just accurate translations, but a distribution over reasonable translations, since SMC performs posterior sampling rather than optimization (Loula et al., 2025;Lew et al., 2023b).In future work, this could be used to more precisely model the interpretation of ambiguous epistemic language.LaBToM correlates highly with human ratings of epistemic statements.As Table 5 shows, our full model produces statement scores that correlate highly with human ratings (r = 0.76) across both current and initial belief conditions.</p>
<p>In the Appendix, we show results using the gold ELoT translations (r = 0.81), and a per-factor breakdown.Plotting average human judgments against LaBToM inferences (Figure 2, Row 1), we also find a strong qualitative fit: Factoring out incorrect ELoT translations (black dots), LaBToM generally assigns high or low ratings to sentences when humans do, using the ends of the scale as appropriate.In contrast, our ablated models do poorly, either because they fail to track how the player updates their beliefs (True Belief, Current r = 0.09), or fail to infer the player's initial beliefs (Non-Planning, Initial r = 0.07).These findings hold even when the parameters of our full model are adversarially optimized (see Appendix).</p>
<p>SotA multimodal LLMs struggle at grounded evaluation of epistemic language.The multimodal LLM baselines also perform less well than LaBToM despite extensive prompting, with the strongest LLM baseline (GPT-4o with images, narratives, and few-shot prompting) achieving human correlations of only 0.59 and 0.41 respectively.Increasing information in the prompt improves performance, although the Gemini models do poorly regardless, even with full scenario videos.We find that LLM performance is lower on initial belief sentences, suggesting that they are better at tracking how agents' current beliefs change, but worse at inferring past beliefs.</p>
<p>Figure 2 illustrates the difference between our model and the best performing GPT-4o baseline (I+Na+FS) in greater detail.Despite few-shot prompting, GPT-4o tends to assign ratings of 0.8 or more to many statements that humans find quite unlikely, suggesting that LLMs struggle to account for evidence against a belief claim.</p>
<p>LaBToM captures how human evaluations of epistemic language change with agent behavior.Our model predicts that human evaluations of epistemic sentences should change systematically as they gain more information about an agent's percepts and actions.As Figure 3 illustrates, this is what we find.When the player sees that a box is empty, both humans and our model sharply decrease their ratings for statements claiming that the player believes a key is in that box (e.g.Fig. 3b, S4 Current).When the player approaches one box instead of another, both humans and LaBToM gain confidence in statements about the relative likelihood of key locations (Fig. 3a, S5 Current).</p>
<p>We analyze just one scenario (Figure 3a) in detail.Here, the player first walks upwards away from box 3 (Judgment Point 1), leading both humans and LaBToM to assign scores of greater than 0.5 to "The player believes that box 3 is empty" and "The player initially believed that box 3 was empty".In contrast, the modal sentence "The player believes that box 3 may contain a red key" is rated lower.This is because if the player did believe that box 3 might have the (red) key, then it is more likely that they would have looked in box 3.However, this does not occur.</p>
<p>After the player opens box 2 and finds it empty, then walks back down towards box 3 (Judgment Point 3), both humans and our model decrease their confidence in the statement "The player believes that box 3 is empty".They also decrease confidence in "The player initially believed that box 3 was empty", but less sharply.This is because there are at least two possibilities consistent with the player's actions: They could have firmly believed that box 3 was empty, or they could have just believed that box 3 was less likely to contain the relevant key than box 2, without all-out believing that it was empty.This ability to understand beliefs about relative likelihood is made explicit by how ratings change over time for "The player believes that box 3 is more likely to contain a red key than box 1." Consistent with the principle of rational action, both humans and LaBToM assign a high score to this sentence once the player goes back to box 3, forgoing box 1.</p>
<p>Across a range of other settings (Figure 3b-c), our model largely captures fine-grained changes in how people evaluate a variety of epistemic expressions, including modal sentences, ignorance claims, and expressions of uncertainty.Unlike prior BToM models that lack language-like belief representations (Baker et al., 2017), LaBToM also distinguishes observer and agent uncertainty, assigning high ratings to claims that the agent is uncertain (Figure 3c, S5 Cur.), and vice versa.In contrast, the best LLM baseline (GPT-4o, I+Na+FS) often fails to adjust its ratings in the same direction as humans, while rating implausible statements highly.We discuss these results more in the Appendix, alongside cases where our model comes apart from humans.</p>
<p>LaBToM distinguishes in-context and out-ofcontext epistemic language.To investigate how our model generalizes to a larger set of epistemic expressions, we performed the in-context vs. outof-context likelihood comparison described in Section 4.5 for our full dataset of 469 sentences.We tested the full LaBToM model, ablations and the best applicable LLM baseline from Table 5 (GPT-4o, I+Na).Results are shown in Table 6.For both current and initial belief statements, we find that LaBToM assigns significantly higher scores when a statement is evaluated in-context vs. out-of-context, correctly classifying the context about 70% of the time by assigning a strictly higher in-context score.On closer inspection,  LaBToM most accurately distinguishes when epistemic language is evaluated in vs. out-of-context, assigning significantly higher scores in-context (s.e. in brackets).</p>
<p>many statements that are incorrectly classified turn out to be plausible in either context, resulting in equal or close-to-equal scores.The ablated models and GPT-4o perform significantly worse than LaBToM, especially for initial beliefs.</p>
<p>Discussion</p>
<p>Our experiments show that, similar to humans, our LaBToM model is able to coherently interpret and adjust its evaluations of natural language statements about agents' beliefs, whereas stateof-the-art multimodal LLMs struggle with this task.This ability is mediated by our ELoT representation and semantic parser: Without a compositional representation of epistemic concepts that is aligned with the structure of natural epistemic language, we find that translation accuracy drops significantly, even when using a grammar-constrained LLM parser.</p>
<p>We also find that LaBToM largely distinguishes in vs. out-of-context sentence usage on a large and diverse set of crowd-sourced epistemic language, indicating the generalizability of our approach.That said, our model is not without limitations.As we discuss at greater length in the Appendix, LaBToM's outputs depart from human judgments in several interesting ways, suggesting the need to account for (i) contextual adaptation of probability thresholds via pragmatic reasoning (Rudin, 2016;Schuster and Degen, 2020), (ii) the role of justification in human's intuitive evaluation of knowledge claims (Alston, 1989), and (iii) bounded human reasoning about logical implications (Smets and Solaki, 2018).</p>
<p>LaBToM is also an ideal observer model that does not scale readily to large belief spaces, leaving open how humans tractably infer and evaluate claims about others' beliefs (Van Rooij, 2008), perhaps by focusing on occurent beliefs (Bartlett, 2018) that are relevant to others' goals.Finally, LaBToM is a model of how people interpret epistemic language, but full understanding also includes the ability to produce such language.This could potentially be achieved by inverting the ELoT module of our model, using it to translate salient or conversationally-relevant inferences about an agent's beliefs into natural language.By extending our model in this way, we stand to gain an even richer account of what it means to understand epistemic language.</p>
<p>A Dataset Collection</p>
<p>A.1 Experimental Procedure</p>
<p>The interface used for collecting the statements and evaluating statements are shown in Figure A1.Participants first completed a tutorial that explained the task and experimental interface, then answered 5 comprehension questions before proceeding to the main experiment.In the main experiment, they were shown 10 out of the 20 stimuli in a randomized order.</p>
<p>To incentivize accurate but calibrated responses, participants were rewarded for accurately guessing the true goal.Specifically, they earned 1/N bonus points if they selected N goals out of which one was the true goal, but 0 points if none of their selected goals was the true goal.Participants were paid US$1 for every 40 bonus points they earned, on top of a base pay of US$15/hr.</p>
<p>A.2 Statement Post-Processing and Annotation</p>
<p>Once statements were collected, two experimenters independently annotated whether each statement was valid for inclusion.We excluded invalid sentences based on two criteria: (i) whether the statement had the right tense (present for current beliefs, past for initial beliefs) and (ii) whether the statement referred to beliefs about the boxes.We also corrected minor grammatical errors and normalized statements to the form "The player + [believes/knows/thinks/expects/is sure/is uncertain, etc.]..." for current beliefs and "The player initially + [believed/knew/thought/expected/was sure/uncertain, etc.]..." for initial beliefs.</p>
<p>After filtering and normalization, two experimenters independently annotated the statements based on four factors: possibility, probability, compositionality, and knowledge.These factors are not mutually exclusive, so a statement could be annotated with any combination of factors.The codes for possibility were [may, might, can, could, should, must, none] and the codes for probability were [certain, uncertain, likely, unlikely, none].The codes for compositionality and knowledge were binary [0, 1].The annotators agreed on 95% of the codes and discussed to resolve their differences.</p>
<p>A.3 Selecting Diverse Statements for Human Evaluation</p>
<p>After annotation, we selected a set of 3 plausible and 2 implausible statements per scenario for evaluation by human raters.Implausible statements were manually selected from other scenarios to be implausible in their scenario of evaluation.To select the 3 plausible statements, we sampled 100 subsets of 3 statements out of all statements written for that scenario, then computed a diversity score for each set S:
Score(S) = 1 4 |S possibility | + 1 4 |S probability | + 1 4 |S compositionality | + 1 4 |S knowledge |(13)
where |S possibility | indicates the number of unique possibility codes in set S and vice versa.We then chose the set with the highest diversity score among the 100 sampled sets.</p>
<p>B Model Configuration</p>
<p>B.1 Belief-Space Sequential Inverse Plan Search (BSIPS)</p>
<p>Our BToM inference algorithm is a belief-space policy variant of Sequential Inverse Plan Search (SIPS, Zhi-Xuan et al. ( 2020)), which uses policies to evaluate action likelihoods as in recent extensions of SIPS (Zhi-Xuan et al., 2024b,a).Perhaps surprisingly, Belief-Space SIPS (BSIPS) is able to exactly compute the posterior over beliefs, goals, and states (Equation 11) without any Monte Carlo approximation: For the scenarios we considered, there were between 120 and 5940 possible combinations of goals, initial states, and beliefs.Enumerative inference over these hypotheses could run as fast as 0.1s/action (120 hypotheses), going up to 20s/action (5490 hypotheses).Experiments were conducted with a i7-1370P 1.90 GHz CPU and 64 GB RAM.Code can be found at https://osf.io/xq9c4/.</p>
<p>Algorithm 1 provides the pseudocode for BSIPS.At each step t, we simulate how the environment changes, and how the agent updates their beliefs based on what they see (L6-7).Next, we efficiently compute belief-space Q-values by leveraging the Q MDP approximation described in Section 3.2.1.This involves averaging over Q-values for each state hypothesis s in the agent's belief b i t , which can be done cheaply by memoizing and reusing shortest-path computations across all belief hypotheses (L9-13).The Q-values allow us to compute the likelihood of the observed action a t , allowing us to reweight each hypothesis by how well it explains the observations (L14-16)</p>
<p>For epistemic language evaluation at this scale, the technical challenges are mostly representational, not algorithmic.However, scaling to larger spaces of goals (Zhi-Xuan et al., 2024a) and (belief) states will require additional layers of Monte Carlo approximation.Our implementation in Gen (Cusumano-Towner et al., 2019) can naturally be extended to these cases, e.g. by leveraging Sequential Monte Carlo for approximate inference of initial states (Del Moral et al., 2006;Lew et al., 2023a).</p>
<p>Algorithm 1 Belief-Space Sequential Inverse Plan Search (BSIPS) 1: procedure BSIPS(G, S0, B0, a1:T , o0:T ) 2:</p>
<p>H ← G × S0 × B0 ▷ Enumerate all hypotheses (goal, belief &amp; state combinations).3:
W ← {w i := P (o0|s i 0 )} |H| i=1
▷ Initialize (unnormalized) weights for all hypotheses.4:</p>
<p>for t ∈ [1, T ] do 5:</p>
<p>for
h i := (g i , s i 0:t−1 , b i 0:t−1 ) ∈ H do 6: s i t ← STATE-TRANSITION(s i t−1 , at−1) ▷ Simulate next environment state. 7: b i t ← BELIEF-UPDATE(b i t−1 , s i t , at−1) ▷ Simulate agent's belief update. 8: h i ← (g i , s i 0:t , b i 0:t ) 9: QBel(g i , b i t , ã) ← 0 for ã ∈ VALID-ACTIONS(b i t ) ▷ Initialize belief-space Q-values. 10:
for (s, w) ∈ b i t and ã ∈ VALID-ACTIONS(s) do ▷ Iterate over environment states in agent's belief.11:</p>
<p>Q * (g i , s, ã) ← MEMOIZED(PATH-COST(s, ã, g i ))</p>
<p>▷ Compute shortest path cost to goal (memoized).12:  In our experiment, we used raw visual inputs, such as images or videos, for the multimodal LLM baselines.As the LaBToM model operates over symbolic representations of agent and environment states encoded in the Planning Domain Definition Language (PDDL), we also experimented with prompting LLMs with symbolic PDDL inputs.We report the results on the best performing LLM baseline (GPT4o I+Na+FS) in Table C3, which shows that replacing raw visual input with PDDL (as text) hinders performance.
QBel(g i , b i t , ã) ← Q * (g i , b i t , ã) + w • Q(g i , s, ã) ▷ Update belief-space Q-values</p>
<p>C.3 Impact of a Normalized Statement Prior</p>
<p>In the main text, we report results under the assumption that human observers respond as if they have a normalized 50-50 prior U φ over whether each statement φ is true.Under this assumption, the posterior truth-value of a statement P ( φ (st,bt) |a 1:T , o 0:T ) can be interpreted as a normalized likelihood:
L( φ (st,bt) |a 1:T , o 0:T ) = P (a 1:T , o 0:T | φ (st,bt) ) P (a 1:T , o 0:T | φ (st,bt) ) + P (a 1:T , o 0:T | ¬φ (st,bt) )(14)
An alternative assumption is to use a uniform prior U S 0 ×B 0 over all possible initial states S 0 and belief distributions B 0 .This has the effect of up-weighting statements which are true in more possible worlds, e.g."The player believes that a red key might be in box 1, 2, or 3.".</p>
<p>Table C4 shows the impact of making either assumption, in terms of the Pearson's correlation coefficient (PCC) r and mean absolute error (MAE) with human judgments, assuming the model has the gold translations.Consistent with Ying et al. (2024b), using a normalized statement prior largely improves the correlation while reducing the mean absolute error for the full LaBToM model.In other words, people appear more willing to say that a statement φ is true only if they have evidence for φ, and otherwise default to a 50-50 rating.We see sharper differences for initial belief statements, which is  likely because priors have a stronger effect on initial beliefs, whereas an agent's current beliefs are more strongly determined by their percepts: If an agent sees that a box is empty, an observer's judgment about whether the agent believes that the box is empty should not depend on the observer's prior.</p>
<p>C.4 Per-Factor Model Performance using Gold Translations</p>
<p>Table C5 shows the correlation between human judgments and model outputs when using the gold ELoT translations.Performance is broken down by the annotated factors described in Table 2. Using gold ELoT translations improves LaBToM's correlation with human judgments from r=0.76 (Table 5) to r=0.81.</p>
<p>Additionally, LaBToM robustly outperforms the baselines on almost all of these data splits, achieving a correlation around r=0.8 in each case.The one exception is the set of statements about what the agent initially knows (Init.Know., r = 0.31), such as "The player initially knew that the red key was in box 3".As we discuss in the next section, this is likely because human participants assume that direct perception or justification is necessary for other agents to know some proposition ϕ.In contrast, our model treats knowledge claims as equivalent to claims of true belief.</p>
<p>C.5 Differences in Human Ratings vs. Model Inferences</p>
<p>While LaBToM largely matches human evaluations of epistemic language both in aggregate and at the individual scenario level, there a number of interesting ways in which they differ.</p>
<p>People are less certain than LaBToM.One difference is simply that our model tends to be more certain than people, using the extremes of the 0-1 scale in ways that our participants tended to avoid.This effect did not appear to be driven by the choice of the Boltzmann inverse temperature β, since lower values of β (which increase model uncertainty) led to poorer fits with human data.Instead, humans may be evaluating the truth a statement ϕ less strictly than our model does, perhaps by maintaining uncertainty over the probability thresholds θ associated with each statement.</p>
<p>People appear to adapt probability thresholds.Threshold uncertainty is closely related to another potential driver of difference: Unlike our model, participants appear to contextually adapt probability thresholds associated with modal words, in line with work on the pragmatics of epistemic modals (Schuster and Degen, 2020;Rudin, 2016;Lassiter, 2017).This is evinced by human responses for current belief statement S2 in Fig. 3b.People rate "The player is unsure which box has a key" highly despite the apparent confidence that the player exhibits in looking for a key in box 3 (at the expense of looking in box 1 or box 2).This is consistent with an upwards adjustment of θ uncertain from 0.55, such that the player is judged as uncertain even when they seem to think it is quite likely for a key to be in box 3.In contrast, our model thinks it is unlikely that the player is uncertain.Similar effects can be seen for current statement S1 and S5 in the same scenario, except that people appear to adjust their threshold for may and might downwards from 0.3 and 0.2 to accommodate even highly unlikely possibilities.</p>
<p>People respond as if knowledge requires justification.Unlike our model, which reduces knowledge statements to true belief claims (Table 1), people appear to assume that some form of justification (e.g. via direct perception) is necessary for agents to know that some proposition ϕ is true.This is clearly illustrated in Figure C2: In this scenario, the player very confidently walks past boxes 1 and 2 towards box 3, suggesting a strong belief about its contents.Since our model just treats knowledge as true belief (and it is quite possible that the player is correct), LaBToM assigns less than 50% probability to "The player does not know which box contains a red key" and "The player initially did not know the color of the key in box 3" at Judgment Point 1 (which occurs before box 3 is opened).Human judgments differ significantly, assigning more than 50% to these ignorance claims.This is consistent with people understanding knowledge to require justification or direct perception: Since the player has not seen what is in box 3 by Judgment Point 1, they cannot know what is in that box.</p>
<p>People reason boundedly about logical implications.A final difference between humans and our model is that people appear to exhibit bounded reasoning when evaluating statements with non-obvious implications.In Figure C2, for example, "The player initially believed a red key was most likely in box 2" is rated lowly by humans initially, then more highly, whereas our model assigns a zero rating to that statement by Judgment Point 2. This is because by then, it is clear that the player did not need a blue key, and was instead looking for a red key in box 1.The fact that they looked in box 1 first also implies that they initially thought box 1 was most likely to contain a red key, not box 2. Our model captures this reasoning, but people do not seem to independently grasp these multi-step implications, consistent with studies on the boundedness of human reasoning (Smets and Solaki, 2018;Mercier and Sperber, 2017).</p>
<p>Figure 1 :
1
Figure1: Overview of our model, a Language-augmented Bayesian Theory of Mind (LaBToM).Here, our model evaluates the plausibility of epistemic language about a player character trying to find keys in boxes so as to reach one of four goals (gems with different shapes/colors, g 1 -g 4 ) which may be locked behind doors.(Top) We translate natural language statements about the player's initial (σ 1 ) and current beliefs (σ 2 ) into an unambiguous epistemic language of thought (ELoT) via grammar-constrained LLM parsing.ELoT statements (i.e.epistemic formulas φ) are interpreted with a probability-based semantics (lowered formulas φ ′ ).(Bottom) We use our Bayesian theory-of-mind (BToM) module to produce inferences (bar charts) about the environment state (top left), the player's goal (bottom left), and the player's belief state (right) at each step t, given observations across time.Each possible belief state b i is itself a distribution over environment states s j , where state s j corresponds to a blue key being in box j. (Middle) We evaluate the posterior probability * of each ELoT statement φ from the BToM inferences at each step t ( * under a 50-50 prior over statement truth).Statement σ 1 ("The player thought that the blue key must be in box 3.") increases in probability from t=6 to t=9, then stays high, since it becomes clear from the player looking in box 3 at t=9 that they initially thought a key must be in box 3. Statement σ 2 ("The player believes that a blue key is more likely to be in box 1 than box 2.") decreases in probability as the player walks away from box 1 at t=6 through t=9.However, when the player finds box 3 empty, then moves past box 2 up to box 1 at t=15, it becomes much more probable that the player currently thinks a key is more likely to be in box 1 rather than box 2.</p>
<p>Figure 2 :
2
Figure2: Human correlation plots for LaBToM and GPT-4o.Black dots are from statements not accurately translated to ELoT.LaBToM provides a much stronger qualitative fit with human ratings compared to the best GPT-4o baseline, which fails to use the full scale.</p>
<p>Figure 3 :
3
Figure 3: Step-by-step ratings by humans and models across three scenarios.Judgement points are annotated on each map, and show the player's location before opening the nearest box.Keys picked up along the way are shown in light colors.Our model largely matches human responses qualitatively and quantitatively, unlike GPT-4o.</p>
<p>Figure A1 :
A1
Figure A1: Interfaces used for collecting (top) and evaluating (bottom) epistemic language.</p>
<p>Figure C2 :
C2
Figure C2: Scenario illustrating differences in human and model ratings of knowledge claims.</p>
<p>Epistemic terms and definitions (unlikely, less and least are omitted due to space limits).
.Pr(A, ϕ) ≥ θ mightmay(ϕ)E/AΦλA.Pr(A, ϕ) ≥ θmayshould(ϕ)E/AΦλA.Pr(A, ϕ) ≥ θ shouldmust(ϕ)E/AΦλA.Pr(A, ϕ) ≥ θ mustModal Adjectiveslikely(ϕ)E/AΦλA.Pr(A, ϕ) ≥ θ likelydegree(likely, A, ϕ)Φ FA, ΦPr(A, ϕ)Comparativesmore(P, ϕ, ψ)E/AP, Φ, ΦλA.degree(P, A, ϕ) &gt; degree(P, A, ψ)mostsup(P, O, C, ϕ)E/AP, O, Φ/O, Φ/OλA.degree(P, A, ϕ(O)) ≥ max x:C(x) degree(P, A, ϕ(x))most str (P, ϕ)E/AP, ΦλA.degree(P, A, ϕ) ≥ α most • θ P(a) Thresholds θMultipliers αbelieves certain uncertain likely unlikely could might may should must most0.750.950.700.700.400.200.200.300.800.951.5(b) Probability thresholds and multipliers (fitted against human data).</p>
<p>Table 1
1: Expressions in our epistemic language of thought (ELoT), including (a) epistemic terms and (b)probability thresholds Θ. ELoT terms may have the following types: E: epistemic formula, Φ: base formula, Φ F :function term, P: predicate symbol, A: agent, O: object, X /Y: function from X → Y.</p>
<p>Table 3 :
3
Translation accuracy for ELoT vs. lowered formulas as the translation target, compared across sampling methods and LLMs in terms of exact equality (Exact) or semantic equivalence (Equiv.).ELoT serves as a better target and constraint for translation.</p>
<p>Table 4 :
4
Example translations from English to ELoT and lowered formulas using LLaMa 3.1 8B.
ModelHuman Correlation r (s.e.)AllCurrentInitialLaBToMFull (ours)0.76 (0.01) 0.78 (0.01) 0.72 (0.01)Non-Planning 0.40 (0.01) 0.58 (0.01) 0.07 (0.01)True Belief0.10 (0.01) 0.09 (0.01) 0.09 (0.02)GPT-4o (OpenAI et al., 2023)I+Na+FS0.52 (0.01) 0.59 (0.01) 0.41 (0.01)I+Na0.48 (0.01) 0.52 (0.01) 0.41 (0.01)I+Pl0.28 (0.01) 0.32 (0.01) 0.18 (0.01)Gemini 1.5 Pro (Gemini Team et al., 2024)V+Na+FS0.23 (0.01) 0.28 (0.01) 0.14 (0.02)I+Na+FS0.22 (0.01) 0.29 (0.01) 0.11 (0.02)
I -Image, V -Video, Pl -Plans, Na -Narratives, FS -Few-Shot</p>
<p>Table 5 :
5
Model correlations with human ratings of epistemic language.LaBToM correlates strongly with humans, whereas multimodal LLMs struggle to do so.</p>
<p>Table 6 :
6
In vs. out-of-context statement evaluation.</p>
<p>Table C3 :
C3
correlation with human ratings across different input representations.</p>
<p>Table C4 :
C4
Similarity of human and model ratings for a normalized (U φ ) vs. unnormalized (U S0×B0 ) prior.</p>
<p>Table C5 :
C5
Human vs. model correlations broken down by linguistic factors.LaBToM results use gold translations.</p>
<p>The full set of SMC samples can instead be used to handle ambiguous sentences with multiple plausible ELoT translations. We leave the study of this to future work.
AcknowledgmentsWe thank our colleagues Cedegao Zhang, Brian Leahy, and Megan Wei for helpful discussions in the development of this project, and Ben LeBrun for his help in applying grammar-constrained SMC decoding to our setting.We also thank Jacob Andreas, Cedegao Zhang, Liam Bright, Daniel Lassiter, our reviewers, and our action editor for their valuable feedback on this paper.This work was funded in part by the DARPA Machine Common Sense, AFOSR, and ONR Science of AI programs, along with the MIT-IBM Watson AI Lab, the Siegel Family Foundation, and an anonymous donor.Tan Zhi-Xuan is supported by an Open Philanthropy AI Fellowship.B.2 Parameter Fitting and Robustness AnalysesWe performed a grid search over parameters for the LaBToM model.The range of inverse temperatures β for the Boltzmann policy went from 0.5 to 4 in multiplicative increments of √ 2. This produced human correlations between r = 0.75 (at β = 0.5) and r = 0.81 (at β = 4) for current beliefs, and between r = 0.64 (at β = 0.5) and r = 0.80 (at β = 2 3/2 ), with β = 2 3/2 producing the best fit overall.Across all values of β, we found that the full model outperformed the ablated baselines.We also fitted the threshold parameters Θ used in our ELoT representation.We performed grid-based coordinate ascent with a step size of 0.05, starting from values derived from the literature mapping modal words to probabilities(Wesson and Pulford, 2009;Hahn and Engelmann, 2014;Meder et al., 2022), and limiting the search to a range of 0.2 above and below this starting point.Our initial and final threshold parameters are shown in TableB1.To evaluate threshold sensitivity, we also ran the same procedure to minimize correlation with humans.This produced a value of r = 0.71, which was still much higher than the next best model (GPT-4o I+Na+FS, r = 0.52).Optimizing these thresholds for the True Belief and Non-Planning ablations led to maximal correlations of r = 0.12 and r = 0.47 respectively.Output: knows_that(player,formula(and(empty(box2), empty(box3)))) Input: The player knows the color of the keys in all of the boxes.Output: forall(box(B), knows_about(player, color(C), exists(and(key(K), inside(K, B)), iscolor (K, C)))) Input: The player doesn't know that there is a blue key in box 2. Output: not_knows_that(player, formula(exists(and(key(K), iscolor(K, red)), inside(K, box2)))) Input: The player is sure of the color of the key in box 4. Output: certain_about(player, color(C), exists(and(key(K), inside(K, box4)), iscolor(K, C))) Input: The player is uncertain about what's in box 2. Output: uncertain_about(player, color(C), exists(and(key(K), inside(K, box2)), iscolor(K, C))) Input: The player believes that there is a key in box 4. Output: believes(player, formula(exists(key(K), inside(K, box4)))) Input: The player thinks that there is a red key in either box 1 or box 3. Output: believes(player, formula(exists(and(key(K), iscolor(K, red)), or(inside(K, box1), inside(K, box3))))) Input: The player thinks there might be a key in box 1 or box 2. Output: believes(player, might(exists(key(K), or(inside(K, box1), inside(K, box2))))) Input: The player thinks there is likely a key in box 2. Output: believes(player, likely(exists(key(K), inside(K, box2))))B.4 LLM Baseline PromptsBelow we show the prompts we used for our multimodal LLM baselines (GPT-4o and Gemini 1.5 Pro).Associated images and videos can be found in our code and dataset release.[IMAGE OR VIDEO]You're watching someone play the treasure game shown above.The player controls a character, and their goal is collect one of the four gems (triangle, square, hexagon, or circle).The rules of the game are as follows:-The player can move on the white squares.-The player has a full view of the map at all time.-The player's goal is to collect exactly one target gem.-Keys unlock doors of the same color (e.g.red keys unlock red doors).-Each key can only be used once.Keys disappear after use.-Each box may be empty or contain exactly one key.- Given this information, which gem(s) are most likely to be the human agent's goal?And how would you rate the following statement about the player's current belief from 1 (definitely false) to 7 (definitely true)?Rate 4 if you think there is an equal chance of the statement being true and false.Please rate the following statement:Please respond in the following JSON format, indicating all gems that you think are likely to be the human's goal, and your rating as a number from 1 to 7.{ goal: [gems...], rating: x , }The gems should be any of[triangle, square, hexagon, circle]and you can indicate all the likely goal gems in your response.The rating should be an integer from 1 to 7. Please provide an explanation to your response.An example of plan-based prompting:The player moves right three times, then down twice, and finally left.An example of narrative-based prompting, which provides more contextual information:The square gem is locked behind a red door.The triangle, circle and hexagon gems are not locked behind any doors.There are three boxes.No keys are visible in the scene.The player moves right three times, then down twice, and finally left towards box 2 and away from box 3.For few shot prompting, the following text is added before showing the statement to be evaluated.We provide 4 examples of human ratings:StatementThe player initially expected to find a key in box 3. Gold Trans.believes(player, formula(exists(key(K), inside(K, box3))))Translation (67%) believes(player, likely(exists(key(K), inside(K, box3)))) (32%) believes(player, formula(exists(key(K), inside(K, box3)))) Error "Expected" is interpreted as "likely" instead of straightforward belief.LLaMa 3.1 8B Unconstrained + Majority VoteStatementThe player initially thought that box 2 contained a red key.Gold Trans.believes(player, formula(exists(and(key(K), iscolor(K, red)), inside(K, box2))))  When evaluating translations, we performed both an exact equality check against our gold translations, and also a manual check for approximate semantic equivalence (i.e.cases where the parser gave a reasonable alternative translation of the original English statement).For replicability, we include these manual equivalence annotations in our code release.TableC2shows characteristic ELoT translation errors produced by each parsing method.Many of the the strict errors from grammarconstrained SMC pass the approximate equivalence check, and SMC often produces a distribution over reasonable alternative translations (Row 1).In contrast, unconstrained sampling from the same LLaMa 3.18B base model leads to many syntax errors that majority voting cannot correct for (Row 2).With Gemini Flash 8B, we observe the mode collapse issue that frequently afflicts finetuned LLMs (O'Mahony et al., 2024), often leading all n σ =10 samples to produce the same incorrect translation (Row 3).C.2 Unimodal
Propositional attitudes. Jerry A Fodor, The Language and Thought Series. Harvard University Press1981</p>
<p>Epistemic justification: Essays in the theory of knowledge. Alston William, 1989Cornell University Press</p>
<p>Weakly supervised learning of semantic parsers for mapping instructions to actions. Yoav Artzi, Luke Zettlemoyer, Transactions of the association for computational linguistics. 12013</p>
<p>Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. Chris L Baker, Julian Jara-Ettinger, Rebecca Saxe, Joshua B Tenenbaum, Nature Human Behaviour. 142017</p>
<p>Occurrent states. Gary Bartlett, Canadian Journal of Philosophy. 4812018</p>
<p>Marcel Binz, Eric Schulz, arXiv:2306.03917Turning large language models into cognitive models. 2023arXiv preprint</p>
<p>Advertisement for a semantics for psychology. Ned Block, Midwest studies in philosophy. 198710</p>
<p>A gentle introduction to epistemic planning: The del approach. Thomas Bolander, arXiv:1703.021922017arXiv preprint</p>
<p>Epistemic modality and spoken discourse. Jennifer Coates, Transactions of the Philological society. 8511987</p>
<p>Seeing and understanding epistemic actions. Sholei Croom, Hanbei Zhou, Chaz Firestone, Proceedings of the National Academy of Sciences. 12049e23031621202023</p>
<p>Gen: a general-purpose probabilistic programming system with programmable inference. Feras A Marco F Cusumano-Towner, Alexander K Saad, Lew, Vikash, Mansinghka, Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation. the 40th ACM SIGPLAN Conference on Programming Language Design and ImplementationACM2019</p>
<p>Sequential Monte Carlo samplers. Pierre Del Moral, Arnaud Doucet, Ajay Jasra, Journal of the Royal Statistical Society Series B: Statistical Methodology. 6832006</p>
<p>Andy Egan, Brian Weatherson, Epistemic modality. Oxford University Press2011</p>
<p>Psychosemantics: The Problem of Meaning in the Philosophy of Mind. Fodor, 1987MIT Press</p>
<p>The language of thought. Jerry A Fodor, 1975Harvard university press5Cambridge, MA</p>
<p>Sense and reference. Gottlob Frege, The Philosophical Review. 5731948</p>
<p>. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2024. Gemini: A family of highly capable multimodal models</p>
<p>Probabilistic semantics and pragmatics uncertainty in language and thought. The handbook of contemporary semantic theory. D Noah, Daniel Goodman, Lassiter, 2015</p>
<p>Grounding epistemic modality in speakers' judgments. Udo Hahn, Christine Engelmann, Pacific Rim International Conference on Artificial Intelligence. Springer2014</p>
<p>Conceptual role semantics. Gilbert Harman, Notre Dame Journal of Formal Logic. 2321982</p>
<p>Value-function approximations for partially observable markov decision processes. Milos Hauskrecht, Journal of Artificial Intelligence Research. 132000</p>
<p>Knowledge and Belief: An Introduction to the Logic of the Two Notions. Kaarlo Jaakko, Juhani Hintikka, 1962Cornell University PressIthaca, NY, USA</p>
<p>Précis of foundations of language: Brain, meaning, grammar, evolution. Ray Jackendoff, Behavioral and brain sciences. 2662003</p>
<p>The naive utility calculus as a unified, quantitative framework for action understanding. Julian Jara-Ettinger, Laura E Schulz, Joshua B Tenenbaum, Cognitive Psychology. 1231013342020</p>
<p>MMToM-QA: Multimodal theory of mind question answering. Chuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo, Zhiting Hu, Tomer Ullman, Antonio Torralba, Joshua Tenenbaum, Tianmin Shu, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack, Kaelbling Michael L Littman, Anthony R Cassandra, Artificial intelligence. 1011-21998</p>
<p>Fantom: A benchmark for stress-testing machine theory of mind in interactions. Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Bras, Gunhee Kim, Yejin Choi, Maarten Sap, Proceedings of the 2023. the 20232023</p>
<p>Conference on Empirical Methods in Natural Language Processing. </p>
<p>Word meaning in minds and machines. M Brenden, Gregory L Lake, Murphy, Psychological review. 13024012023</p>
<p>Cognitive semantics. Meaning and Mental Representation. George Lakoff, 1988</p>
<p>Gradable epistemic modals, probability, and scale structure. Daniel Lassiter, Semantics and Linguistic Theory. 2010</p>
<p>Graded modality: Qualitative and quantitative perspectives. Daniel Lassiter, 2017Oxford University Press</p>
<p>SMCP3: Sequential Monte Carlo with probabilistic program proposals. George Alexander K Lew, Tan Matheos, Matin Zhi-Xuan, Nishad Ghavamizadeh, Stuart Gothoskar, Russell, Vikash, Mansinghka, International conference on artificial intelligence and statistics. PMLR2023a</p>
<p>Sequential monte carlo steering of large language models using probabilistic programs. Tan Alexander K Lew, Gabriel Zhi-Xuan, Vikash Grand, Mansinghka, ICML 2023 Workshop: Sampling and Optimization in Discrete Space. 2023b</p>
<p>Learning policies for partially observable environments: Scaling up. Anthony R Michael L Littman, Leslie Cassandra, Kaelbling Pack, Machine Learning Proceedings. Elsevier1995. 1995</p>
<p>Mind and Meaning. CUP Archive. Brian Loar, 1981</p>
<p>Syntactic and semantic control of large language models via sequential monte carlo. João Loula, Benjamin Lebrun, Li Du, Ben Lipkin, Clemente Pasti, Gabriel Grand, Tianyu Liu, Yahya Emara, Marjorie Freedman, Jason Eisner, Ryan Cotterell, Vikash Mansinghka, Alexander K Lew, Tim Vieira, Timothy J O'donnell, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>John Macfarlane, Are assessmentsensitive. Epistemic modality. 2011</p>
<p>PDDL -the Planning Domain Definition Language. Drew Mcdermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso, Daniel Weld, David Wilkins, 1998</p>
<p>Developmental trajectories in the understanding of everyday uncertainty terms. Björn Meder, Ralf Mayrhofer, Azzurra Ruggeri, Topics in Cognitive Science. 1422022</p>
<p>The Enigma of Reason. Hugo Mercier, Dan Sperber, 2017Harvard University Press</p>
<p>Softstar: Heuristic-guided probabilistic inference. Mathew Monfort, M Brenden, Brian Lake, Patrick Ziebart, Josh Lucey, Tenenbaum, Advances in Neural Information Processing Systems. 201528</p>
<p>On the semantics and pragmatics of epistemic vocabulary. Sarah Moss, 2015Semantics and Pragmatics8</p>
<p>Attributing mode collapse in the fine-tuning of large language models. Laura O' Mahony, Leo Grinsztajn, Hailey Schoelkopf, Stella Biderman, ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2024</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Altman, Shyamal Anadkat. et al. 2023. GPT-4 technical report</p>
<p>The semantics of belief-sentences. Barbara Hall, Partee , Approaches to natural language: Proceedings of the 1970 Stanford workshop on grammar and semantics. Springer1973</p>
<p>Meaning without reference in large language models. Steven Piantadosi, Felix Hill, NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI). 2022</p>
<p>Learning and the Language of Thought. Steven Thomas, Piantadosi , 2011Massachusetts Institute of TechnologyPh.D. thesis</p>
<p>Deriving a variable-strength might. Deniz Rudin, Proceedings of Sinn und Bedeutung. Sinn und Bedeutung201620</p>
<p>Bertrand Russell, Principles of mathematics. Routledge1903</p>
<p>The Things We Mean. Stephen Schiffer, 2003Clarendon Press</p>
<p>PICARD: Parsing incrementally for constrained autoregressive decoding from language models. Torsten Scholak, Nathan Schucher, Dzmitry Bahdanau, 10.18653/v1/2021.emnlp-main.779Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<ol>
<li>I know what you're probably going to say: Listener adaptation to variable use of uncertainty expressions. Sebastian Schuster, Judith Degen, Cognition. 203104285</li>
</ol>
<p>Clever Hans or Neural Theory of Mind? Stress testing social reasoning in large language models. Natalie Shapira, Mosh Levy, Hossein Seyed, Xuhui Alavi, Yejin Zhou, Yoav Choi, Maarten Goldberg, Vered Sap, Shwartz, Proceedings of the 18th Conference of the European Chapter. Long Papers. the 18th Conference of the European Chapterthe Association for Computational Linguistics20241</p>
<p>Constrained language models yield few-shot semantic parsers. Richard Shin, Christopher Lin, Sam Thomson, Charles ChenJr, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, Benjamin Van Durme, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Epistemic plan recognition. Maayan Shvo, Shirin Toryn Q Klassen, Sheila A Sohrabi, Mcilraith, Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems. the 19th International Conference on Autonomous Agents and MultiAgent Systems2020</p>
<p>The effort of reasoning: Modelling the inference steps of boundedly rational agents. Sonja Smets, Anthia Solaki, Logic, Language, Information, and Computation: 25th International Workshop. Bogota, ColombiaSpringer2018. 2018. July 24-27. 201825</p>
<p>Testing theory of mind in large language models and humans. Dalila James Wa Strachan, Giulia Albergo, Oriana Borghini, Eugenio Pansardi, Saurabh Scaliti, Krati Gupta, Alessandro Saxena, Stefano Rufo, Guido Panzeri, Manzi, Nature Human Behaviour. 2024</p>
<p>The tractable cognition thesis. Iris Van Rooij, Cognitive Science. 3262008</p>
<p>Intensional semantics. Kai Von, Fintel , Irene Heim, 2011Unpublished lecture notes</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Verbal expressions of confidence and doubt. Caroline J Wesson, Briony, Pulford, Psychological Reports. 10512009</p>
<p>Efficient guided generation for large language models. T Brandon, Rémi Willard, Louf, arXiv:2307.097022023arXiv preprint</p>
<p>Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, K Vikash, Jacob Mansinghka, Joshua B Andreas, Tenenbaum, arXiv:2306.12672From word models to world models: Translating from natural language to the probabilistic language of thought. 2023arXiv preprint</p>
<p>Identifying concept libraries from language about object structure. Lionel Wong, William P Mccarthy, Gabriel Grand, Yoni Friedman, Josh Tenenbaum, Jacob Andreas, Robert Hawkins, Judith E Fan, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science SocietyEpistemic modals. Mind2022. 200744</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7386 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7386</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7386</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-12b073694ad8cf460d1e74a34b68d07536e0db14</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/12b073694ad8cf460d1e74a34b68d07536e0db14" target="_blank">Can LLMs Serve As Time Series Anomaly Detectors?</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates the capabilities of LLMs, specifically GPT-4 and LLaMA3, in detecting and explaining anomalies in time series and proposes a synthesized dataset to automatically generate time series anomalies with corresponding explanations.</p>
                <p><strong>Paper Abstract:</strong> An emerging topic in large language models (LLMs) is their application to time series forecasting, characterizing mainstream and patternable characteristics of time series. A relevant but rarely explored and more challenging question is whether LLMs can detect and explain time series anomalies, a critical task across various real-world applications. In this paper, we investigate the capabilities of LLMs, specifically GPT-4 and LLaMA3, in detecting and explaining anomalies in time series. Our studies reveal that: 1) LLMs cannot be directly used for time series anomaly detection. 2) By designing prompt strategies such as in-context learning and chain-of-thought prompting, GPT-4 can detect time series anomalies with results competitive to baseline methods. 3) We propose a synthesized dataset to automatically generate time series anomalies with corresponding explanations. By applying instruction fine-tuning on this dataset, LLaMA3 demonstrates improved performance in time series anomaly detection tasks. In summary, our exploration shows the promising potential of LLMs as time series anomaly detectors.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7386.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7386.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary, large decoder-only language model from OpenAI used in this paper via API to perform time-series anomaly detection by prompting (including multi-modal, in‑context, and chain‑of‑thought prompts) and producing JSON outputs with indices and textual explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (GPT-4-0125-preview as used)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large decoder-only transformer language model, instruction-capable via API (used with prompt templates and JSON output parsing).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary / not specified</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Prompt-based zero-shot / few-shot detection using multi-modal prompts, in‑context learning, and chain‑of‑thought; LLM outputs indices + textual explanations (interpreting time series as token lists).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Instruction: Given a time series with values {time series values}, identify any anomalies and return JSON {"anomaly": [indices], "reason":"short explanation"}. Variants: Multi-modal instruction (consider visual representation), n‑shot in‑context examples (1–5), chain‑of‑thought stepwise reasoning. (See paper Appendix A.3/A.4 for full templates.)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>numeric time-series (1D sequences interpreted as lists/tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>YAHOO, ECG, SVDB, IOPS (selected 100 segments each, length 1080), plus trial examples</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F-score and Range-F (Range-F considers detection correct if within a window; window size = 5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per Table 1: YAHOO: F-score=0.0204, Range-F=0.0936; ECG: F-score=0.2911, Range-F=0.3258; SVDB: F-score=0.2681, Range-F=0.2945; IOPS: F-score=0.1020, Range-F=0.1414. Average rank reported 3 (rank columns in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to classic baselines (Table 1) including Isolation Forest, Matrix Profile, Autoencoder, LSTM, Prophet, Informer, DLinear, TimesNet. GPT-4 placed competitively (avg-rank=3) and was among top performers on ECG/SVDB/IOPS but performed poorly on YAHOO (low F-score). TimesNet and Prophet were strong baselines on some datasets (TimesNet avg-rank 1).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot and few-shot (in‑context learning 1–5 examples) variants evaluated; chain‑of‑thought prompting also assessed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Hallucination of indices (model sometimes outputs indices outside the segment; reported hallucination occurrence ≈21%–29% of segments across datasets), struggles on long time series and context‑aware anomalies (seasonality/shape/trend) relative to point anomalies, occasional misinterpretation of visualizations, and hallucinated index/value mentions in explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Serve As Time Series Anomaly Detectors?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7386.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7386.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3 (original)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 3 (8B) - original instruction model used for inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA3-8B (instruct variant) used off-the-shelf with prompting to detect anomalies in time series; showed limited improvement from prompt engineering compared to GPT-4 and tended to produce varied outputs and occasional hallucinated calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3-8B (Meta-Llama-3-8B-Instruct used for base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLM (8B parameters) instruction-tuned variant (inference via Groq API in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Prompt-based detection (direct, multi-modal, in‑context, chain‑of‑thought prompts); interpret time series as token lists and output JSON indices + explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Same instruction/requirements templates as used for GPT-4: provide time series, ask for anomaly indices and a short explanation; multi-modal instruction and n‑shot chain‑of‑thought variants tested (see Appendix A.3).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>numeric time-series</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Trial examples (various anomaly types) and synthesized datasets used later for fine-tuning (see separate entry); original base evaluations primarily on trial cases in paper (Appendix examples).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F-score and Range-F on synthesized evaluation sets (reported in Table 4 for original vs fine-tuned; original values included there).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On synthesized test sets (Table 4, 'Original' rows, overall averaged by length): For lengths 100/200/400 overall F-score = 0.1323 / 0.1392 / 0.1165; Range-F = 0.3702 / 0.2820 / 0.2144. Per-type examples: e.g., Seasonality F-score 0.2706/0.2890/0.2119, Shape F-score 0.2193/0.2171/0.2460.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared mainly to fine-tuned LLaMA3 (this paper). Also qualitatively compared to GPT-4 and classical benchmarks earlier in paper (GPT-4 outperforms LLaMA3 under prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot / prompt-based (in-context examples and chain-of-thought tested)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited benefit from prompt engineering relative to GPT-4; more variable responses (less consistent); produces general explanations (less detailed reasoning); prone to hallucinated calculations and indices; performance degrades as series length increases.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Serve As Time Series Anomaly Detectors?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7386.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7386.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3 (fine-tuned, LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA3-8B fine-tuned via instruction tuning using LoRA on synthetic TTGenerator dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper fine-tuned LLaMA3-8B using parameter-efficient LoRA on a synthetic time-series+explanation dataset (TTGenerator) to improve anomaly-detection and explanation capabilities; fine-tuning yielded measurable improvements (especially for seasonality anomalies) though hallucination and limitations remain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3-8B (Meta-Llama-3-8B-Instruct fine-tuned with LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLaMA3-8B base model further instruction-fine-tuned via LoRA (low-rank adaptation) applied to linear layers (rank=16, alpha=64, dropout=0.1) to learn to output JSON {anomaly,reasons}.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Supervised instruction fine-tuning using synthetic time series + textual explanations (TTGenerator) to produce detection indices and textual explanations; inference uses same JSON-output prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Training prompt: Instruction + Time Series Values + Requirements + Response, where Response is JSON with keys 'anomaly' and 'reason'. Inference prompt mirrors same JSON requirements (see Appendix A.3/A.5).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Synthetic TTGenerator instruction dataset: generated time series with injected anomalies and textual explanations; datasets constructed with sizes 100 / 500 / 1000 / 2000 samples and base lengths 180/360/720 for training; evaluation on synthesized held-out sets (evaluation generation separate lengths 100/200/400). Explanations were also paraphrased/refined using GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>numeric time-series (synthetic sequences) with textual labels/explanations</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TTGenerator-synthesized instruction datasets (training) and synthesized evaluation datasets (lengths 100, 200, 400 with 20 examples per anomaly type per length as evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F-score and Range-F reported per anomaly type and overall (see Table 4). Also manual counts of explanation quality (Table 6) and hallucination counts (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 4 (selected highlights): Overall F-score (Original vs Fine-tuned) by length: length100 0.1323 -> 0.1679, length200 0.1392 -> 0.1447, length400 0.1165 -> 0.1342. Overall Range-F: 0.3702 -> 0.3888 (100), 0.2820 -> 0.2899 (200), 0.2144 -> 0.2454 (400). Seasonality F-score improved markedly (e.g., 0.2706 -> 0.4189 at length100). Explanation quality counts (Table 6): good explanations by anomaly type (e.g., Global: 27 good explanations out of 60 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared pre- vs post-fine-tuning LLaMA3; also qualitatively compared to GPT-4 and classical baselines earlier in the paper (GPT-4 generally stronger under prompting). Fine-tuning improved several metrics vs original LLaMA3 but did not reach GPT-4 levels on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Fully supervised instruction fine-tuning (LoRA) using synthetic labeled data; inference uses prompted JSON requests.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still hallucinates indices (Table 5 shows sizeable hallucination statistics; LLaMA3 tends to hallucinate more indices than GPT-4), improved on some context-aware anomalies (seasonality) but local/global point detection gains limited; explanations are often more generic; constrained by LLM context window (training used lengths to fit token limits).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Fine-tuning settings reported (LoRA rank=16, alpha=64, dropout=0.1, learning rate=2e-4, gradient accumulation=4, paged_adamw_8bit optimizer) but no explicit GPU-hours or latency numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Serve As Time Series Anomaly Detectors?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7386.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7386.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TTGenerator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Time Series and Text Explanation Generator (TTGenerator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic data generator introduced in this paper that creates time series (combining trend, seasonality via sine/IFFT/square wave, noise) with injected anomalies (global/local point, seasonality, trend, shape) and automatically produced textual explanations; used to create instruction datasets for LLaMA3 fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TTGenerator (synthetic dataset generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Procedural generator that composes base time-series from trend + seasonality (single/multi-sine, IFFT) + noise, injects point-aware and context-aware anomalies (with configurable amplitude/period changes and breaks), and produces templated textual explanations which are optionally paraphrased by GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Not a detector — used to synthesize labeled anomaly examples and paired textual explanations for supervised instruction fine-tuning of an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Generates training datasets of sizes 100/500/1000/2000 samples; training lengths 180/360/720; anomaly types: global point, local point, seasonality (amplitude/period changes), trend (change/break), shape (pattern change/break); explanations templated and refined with GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>numeric time-series + textual explanations</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TTGenerator-produced synthetic datasets (training) and separate synthesized evaluation datasets (lengths 100/200/400 used for evaluation in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used to train/evaluate LLM detectors with F-score and Range-F on held-out synthesized evaluation sets; TTGenerator itself not evaluated with detection metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Used to create supervised instruction fine-tuning data (not zero/few-shot alone).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Synthetic samples may not capture the full diversity/noise of real-world data; explanations are template-based and further paraphrased by GPT-4 (potentially introducing biases); generation limited to lengths compatible with LLM context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Serve As Time Series Anomaly Detectors?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>One fits all: Power general time series analysis by pretrained lm <em>(Rating: 2)</em></li>
                <li>Large language model guided knowledge distillation for time series anomaly detection <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot time series forecasters <em>(Rating: 2)</em></li>
                <li>TSB-UAD: an end-to-end benchmark suite for univariate time-series anomaly detection <em>(Rating: 2)</em></li>
                <li>Large language models for spatial trajectory patterns mining <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7386",
    "paper_id": "paper-12b073694ad8cf460d1e74a34b68d07536e0db14",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A proprietary, large decoder-only language model from OpenAI used in this paper via API to perform time-series anomaly detection by prompting (including multi-modal, in‑context, and chain‑of‑thought prompts) and producing JSON outputs with indices and textual explanations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (GPT-4-0125-preview as used)",
            "model_description": "Large decoder-only transformer language model, instruction-capable via API (used with prompt templates and JSON output parsing).",
            "model_size": "proprietary / not specified",
            "anomaly_detection_approach": "Prompt-based zero-shot / few-shot detection using multi-modal prompts, in‑context learning, and chain‑of‑thought; LLM outputs indices + textual explanations (interpreting time series as token lists).",
            "prompt_template": "Instruction: Given a time series with values {time series values}, identify any anomalies and return JSON {\"anomaly\": [indices], \"reason\":\"short explanation\"}. Variants: Multi-modal instruction (consider visual representation), n‑shot in‑context examples (1–5), chain‑of‑thought stepwise reasoning. (See paper Appendix A.3/A.4 for full templates.)",
            "training_data": null,
            "data_type": "numeric time-series (1D sequences interpreted as lists/tokens)",
            "dataset_name": "YAHOO, ECG, SVDB, IOPS (selected 100 segments each, length 1080), plus trial examples",
            "evaluation_metric": "F-score and Range-F (Range-F considers detection correct if within a window; window size = 5)",
            "performance": "Per Table 1: YAHOO: F-score=0.0204, Range-F=0.0936; ECG: F-score=0.2911, Range-F=0.3258; SVDB: F-score=0.2681, Range-F=0.2945; IOPS: F-score=0.1020, Range-F=0.1414. Average rank reported 3 (rank columns in Table 1).",
            "baseline_comparison": "Compared to classic baselines (Table 1) including Isolation Forest, Matrix Profile, Autoencoder, LSTM, Prophet, Informer, DLinear, TimesNet. GPT-4 placed competitively (avg-rank=3) and was among top performers on ECG/SVDB/IOPS but performed poorly on YAHOO (low F-score). TimesNet and Prophet were strong baselines on some datasets (TimesNet avg-rank 1).",
            "zero_shot_or_few_shot": "Zero-shot and few-shot (in‑context learning 1–5 examples) variants evaluated; chain‑of‑thought prompting also assessed.",
            "limitations_or_failure_cases": "Hallucination of indices (model sometimes outputs indices outside the segment; reported hallucination occurrence ≈21%–29% of segments across datasets), struggles on long time series and context‑aware anomalies (seasonality/shape/trend) relative to point anomalies, occasional misinterpretation of visualizations, and hallucinated index/value mentions in explanations.",
            "computational_cost": null,
            "uuid": "e7386.0",
            "source_info": {
                "paper_title": "Can LLMs Serve As Time Series Anomaly Detectors?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "LLaMA3 (original)",
            "name_full": "Meta Llama 3 (8B) - original instruction model used for inference",
            "brief_description": "LLaMA3-8B (instruct variant) used off-the-shelf with prompting to detect anomalies in time series; showed limited improvement from prompt engineering compared to GPT-4 and tended to produce varied outputs and occasional hallucinated calculations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA3-8B (Meta-Llama-3-8B-Instruct used for base)",
            "model_description": "Decoder-only LLM (8B parameters) instruction-tuned variant (inference via Groq API in experiments).",
            "model_size": "8B",
            "anomaly_detection_approach": "Prompt-based detection (direct, multi-modal, in‑context, chain‑of‑thought prompts); interpret time series as token lists and output JSON indices + explanation.",
            "prompt_template": "Same instruction/requirements templates as used for GPT-4: provide time series, ask for anomaly indices and a short explanation; multi-modal instruction and n‑shot chain‑of‑thought variants tested (see Appendix A.3).",
            "training_data": null,
            "data_type": "numeric time-series",
            "dataset_name": "Trial examples (various anomaly types) and synthesized datasets used later for fine-tuning (see separate entry); original base evaluations primarily on trial cases in paper (Appendix examples).",
            "evaluation_metric": "F-score and Range-F on synthesized evaluation sets (reported in Table 4 for original vs fine-tuned; original values included there).",
            "performance": "On synthesized test sets (Table 4, 'Original' rows, overall averaged by length): For lengths 100/200/400 overall F-score = 0.1323 / 0.1392 / 0.1165; Range-F = 0.3702 / 0.2820 / 0.2144. Per-type examples: e.g., Seasonality F-score 0.2706/0.2890/0.2119, Shape F-score 0.2193/0.2171/0.2460.",
            "baseline_comparison": "Compared mainly to fine-tuned LLaMA3 (this paper). Also qualitatively compared to GPT-4 and classical benchmarks earlier in paper (GPT-4 outperforms LLaMA3 under prompting).",
            "zero_shot_or_few_shot": "Zero-shot / prompt-based (in-context examples and chain-of-thought tested)",
            "limitations_or_failure_cases": "Limited benefit from prompt engineering relative to GPT-4; more variable responses (less consistent); produces general explanations (less detailed reasoning); prone to hallucinated calculations and indices; performance degrades as series length increases.",
            "computational_cost": null,
            "uuid": "e7386.1",
            "source_info": {
                "paper_title": "Can LLMs Serve As Time Series Anomaly Detectors?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "LLaMA3 (fine-tuned, LoRA)",
            "name_full": "LLaMA3-8B fine-tuned via instruction tuning using LoRA on synthetic TTGenerator dataset",
            "brief_description": "The paper fine-tuned LLaMA3-8B using parameter-efficient LoRA on a synthetic time-series+explanation dataset (TTGenerator) to improve anomaly-detection and explanation capabilities; fine-tuning yielded measurable improvements (especially for seasonality anomalies) though hallucination and limitations remain.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA3-8B (Meta-Llama-3-8B-Instruct fine-tuned with LoRA)",
            "model_description": "Decoder-only LLaMA3-8B base model further instruction-fine-tuned via LoRA (low-rank adaptation) applied to linear layers (rank=16, alpha=64, dropout=0.1) to learn to output JSON {anomaly,reasons}.",
            "model_size": "8B",
            "anomaly_detection_approach": "Supervised instruction fine-tuning using synthetic time series + textual explanations (TTGenerator) to produce detection indices and textual explanations; inference uses same JSON-output prompt format.",
            "prompt_template": "Training prompt: Instruction + Time Series Values + Requirements + Response, where Response is JSON with keys 'anomaly' and 'reason'. Inference prompt mirrors same JSON requirements (see Appendix A.3/A.5).",
            "training_data": "Synthetic TTGenerator instruction dataset: generated time series with injected anomalies and textual explanations; datasets constructed with sizes 100 / 500 / 1000 / 2000 samples and base lengths 180/360/720 for training; evaluation on synthesized held-out sets (evaluation generation separate lengths 100/200/400). Explanations were also paraphrased/refined using GPT-4.",
            "data_type": "numeric time-series (synthetic sequences) with textual labels/explanations",
            "dataset_name": "TTGenerator-synthesized instruction datasets (training) and synthesized evaluation datasets (lengths 100, 200, 400 with 20 examples per anomaly type per length as evaluation)",
            "evaluation_metric": "F-score and Range-F reported per anomaly type and overall (see Table 4). Also manual counts of explanation quality (Table 6) and hallucination counts (Table 5).",
            "performance": "Table 4 (selected highlights): Overall F-score (Original vs Fine-tuned) by length: length100 0.1323 -&gt; 0.1679, length200 0.1392 -&gt; 0.1447, length400 0.1165 -&gt; 0.1342. Overall Range-F: 0.3702 -&gt; 0.3888 (100), 0.2820 -&gt; 0.2899 (200), 0.2144 -&gt; 0.2454 (400). Seasonality F-score improved markedly (e.g., 0.2706 -&gt; 0.4189 at length100). Explanation quality counts (Table 6): good explanations by anomaly type (e.g., Global: 27 good explanations out of 60 samples).",
            "baseline_comparison": "Compared pre- vs post-fine-tuning LLaMA3; also qualitatively compared to GPT-4 and classical baselines earlier in the paper (GPT-4 generally stronger under prompting). Fine-tuning improved several metrics vs original LLaMA3 but did not reach GPT-4 levels on many tasks.",
            "zero_shot_or_few_shot": "Fully supervised instruction fine-tuning (LoRA) using synthetic labeled data; inference uses prompted JSON requests.",
            "limitations_or_failure_cases": "Still hallucinates indices (Table 5 shows sizeable hallucination statistics; LLaMA3 tends to hallucinate more indices than GPT-4), improved on some context-aware anomalies (seasonality) but local/global point detection gains limited; explanations are often more generic; constrained by LLM context window (training used lengths to fit token limits).",
            "computational_cost": "Fine-tuning settings reported (LoRA rank=16, alpha=64, dropout=0.1, learning rate=2e-4, gradient accumulation=4, paged_adamw_8bit optimizer) but no explicit GPU-hours or latency numbers reported.",
            "uuid": "e7386.2",
            "source_info": {
                "paper_title": "Can LLMs Serve As Time Series Anomaly Detectors?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "TTGenerator",
            "name_full": "Time Series and Text Explanation Generator (TTGenerator)",
            "brief_description": "A synthetic data generator introduced in this paper that creates time series (combining trend, seasonality via sine/IFFT/square wave, noise) with injected anomalies (global/local point, seasonality, trend, shape) and automatically produced textual explanations; used to create instruction datasets for LLaMA3 fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "TTGenerator (synthetic dataset generator)",
            "model_description": "Procedural generator that composes base time-series from trend + seasonality (single/multi-sine, IFFT) + noise, injects point-aware and context-aware anomalies (with configurable amplitude/period changes and breaks), and produces templated textual explanations which are optionally paraphrased by GPT-4.",
            "model_size": "N/A",
            "anomaly_detection_approach": "Not a detector — used to synthesize labeled anomaly examples and paired textual explanations for supervised instruction fine-tuning of an LLM.",
            "prompt_template": null,
            "training_data": "Generates training datasets of sizes 100/500/1000/2000 samples; training lengths 180/360/720; anomaly types: global point, local point, seasonality (amplitude/period changes), trend (change/break), shape (pattern change/break); explanations templated and refined with GPT-4.",
            "data_type": "numeric time-series + textual explanations",
            "dataset_name": "TTGenerator-produced synthetic datasets (training) and separate synthesized evaluation datasets (lengths 100/200/400 used for evaluation in paper)",
            "evaluation_metric": "Used to train/evaluate LLM detectors with F-score and Range-F on held-out synthesized evaluation sets; TTGenerator itself not evaluated with detection metrics.",
            "performance": null,
            "baseline_comparison": null,
            "zero_shot_or_few_shot": "Used to create supervised instruction fine-tuning data (not zero/few-shot alone).",
            "limitations_or_failure_cases": "Synthetic samples may not capture the full diversity/noise of real-world data; explanations are template-based and further paraphrased by GPT-4 (potentially introducing biases); generation limited to lengths compatible with LLM context windows.",
            "computational_cost": null,
            "uuid": "e7386.3",
            "source_info": {
                "paper_title": "Can LLMs Serve As Time Series Anomaly Detectors?",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "One fits all: Power general time series analysis by pretrained lm",
            "rating": 2,
            "sanitized_title": "one_fits_all_power_general_time_series_analysis_by_pretrained_lm"
        },
        {
            "paper_title": "Large language model guided knowledge distillation for time series anomaly detection",
            "rating": 2,
            "sanitized_title": "large_language_model_guided_knowledge_distillation_for_time_series_anomaly_detection"
        },
        {
            "paper_title": "Large language models are zero-shot time series forecasters",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_time_series_forecasters"
        },
        {
            "paper_title": "TSB-UAD: an end-to-end benchmark suite for univariate time-series anomaly detection",
            "rating": 2,
            "sanitized_title": "tsbuad_an_endtoend_benchmark_suite_for_univariate_timeseries_anomaly_detection"
        },
        {
            "paper_title": "Large language models for spatial trajectory patterns mining",
            "rating": 2,
            "sanitized_title": "large_language_models_for_spatial_trajectory_patterns_mining"
        }
    ],
    "cost": 0.01642475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can LLMs Serve As Time Series Anomaly Detectors?</h1>
<p>Manqing Dong ${ }^{\dagger}$, Hao Huang ${ }^{\ddagger}$, and Longbing Cao ${ }^{\dagger}$<br>${ }^{\dagger}$ School of Computing, Macquarie University<br>${ }^{\ddagger}$ School of Computer Science, University of Technology Sydney</p>
<h4>Abstract</h4>
<p>An emerging topic in large language models (LLMs) is their application to time series forecasting, characterizing mainstream and patternable characteristics of time series. A relevant but rarely explored and more challenging question is whether LLMs can detect and explain time series anomalies, a critical task across various real-world applications. In this paper, we investigate the capabilities of LLMs, specifically GPT-4 and LLaMA3, in detecting and explaining anomalies in time series. Our studies reveal that: 1) LLMs cannot be directly used for time series anomaly detection. 2) By designing prompt strategies such as in-context learning and chain-of-thought prompting, GPT-4 can detect time series anomalies with results competitive to baseline methods. 3) We propose a synthesized dataset to automatically generate time series anomalies with corresponding explanations. By applying instruction fine-tuning on this dataset, LLaMA3 demonstrates improved performance in time series anomaly detection tasks. In summary, our exploration shows the promising potential of LLMs as time series anomaly detectors.</p>
<h2>1 Introduction</h2>
<p>With the capabilities of Large Language Models (LLMs) demonstrated in handling various tasks, particularly for natural language processing (NLP) [Achiam et al., 2023] and computer vision (CV) [Liu et al., 2024b], LLMs-based time series analysis emerges as a promising topic [Zhang et al., 2024]. Their primary focus is on time series forecasting, which is an increasingly concerned topic for its broad and lasting roles in wide applications. Their studies can be broadly classified into two groups: (1) prompt engineering approaches, where time series are treated as a series of tokens, either directly fed into the LLMs [Gruver et al., 2023] or combined with instruction prompts [Xue and Salim, 2023], to conduct time series forecasting in a sentence-to-sentence fashion; and (2) aligning approaches, which use LLMs as backbones to train encoders transforming time series into embeddings and decoders translating the LLM outputs into the required output, or even utilizing the middle layers of the LLMs via strategies like pretraining [Ansari et al., 2024] or parameter-efficient fine-tuning (PEFT) [He et al., 2022, Zhou et al., 2023, Jin et al., 2024]. In contrast, time series anomaly detection, while increasingly studied in deep anomaly detection [Pang et al., 2021], has been rarely explored in the realm of LLMs.
LLMs-based time series anomaly detection exhibits significant challenges differing from LLMs-based time series forecasting. The latter captures mainstream and patternable characteristics in time series, while the former needs to handle anomaly complexities including point and contextual exceptions. The limited work available on LLMs for time series anomaly detection [Zhou et al., 2023, Zhang et al., 2023, Liu et al., 2024a] does not explicitly verify or address these issues. They also overlook the textual reasoning ability of LLMs, treating both the input and output of LLMs as time series, without the explanation of how LLMs make their decisions. This motivates us to investigate an important capability area of LLMs in this paper: can LLMs serve as explainable time series anomaly detectors?</p>
<p>First, inspired by the work on treating LLMs as zero-shot learners for time series forecasting through prompt engineering [Gruver et al., 2023, Liu et al., 2024c], we investigate whether LLMs can understand general anomaly-sensitive patterns in time series and explain their decisions, which are essential for time series anomaly detection tasks. Second, time series anomalies can present in different forms, such as point anomalies and contextual anomalies. Therefore, we explore not only whether LLMs can detect time series anomalies but also whether they can identify specific types of anomalies. This approach goes beyond treating time series anomaly detection as a binary classification task [Zhou et al., 2023, Zhang et al., 2023].</p>
<p>Specifically, we investigate the ability of two representative LLMs, GPT-4 [Achiam et al., 2023] and LLaMA3 ${ }^{1}$, for time series anomaly detection and their explainability by addressing three questions: (1) Can LLMs be directly applied for explainable time series anomaly detection? Unfortunately, the answer is no, leading us to the next question: (2) How can LLMs detect and explain time series anomalies via designing appropriate prompt strategies? Through various tests, we find that GPT4 often excels as an explainable time series anomaly detector with minimal prompt instructions. However, our study also reveals gaps in the performance of smaller LLMs, i.e., LLaMA3, in promptbased time series anomaly detection. This brings us to our final question: (3) Can we improve LLMs' detection performance by designing proper instruction fine-tuning? Since there is no available time series data with both anomalies and explanations for instruction fine-tuning, we propose a Time Series and Text Explanation Generator (TTGenerator) to automatically generate time series with anomalies and corresponding descriptions for base and anomaly patterns. We demonstrate the augmentation and benchmarking roles of this dataset in enhancing LLM-based time series anomaly detection.</p>
<p>In a nutshell, our contributions include:</p>
<ul>
<li>Comprehensively investigating the zero-shot learning performance of LLMs in time series anomaly detection tasks and their explanatory capabilities.</li>
<li>Proposing strategic prompt engineering enabling advanced LLMs to achieve competent performance in anomaly detection, compared to baseline methods.</li>
<li>Introducing a synthesized dataset for fine-tuning LLMs, enhancing their performance in time series anomaly detection tasks post fine-tuning.</li>
</ul>
<p>To the best of our knowledge, this work represents a very first to comprehensively investigate and enhance the performance of LLMs in time series anomaly detection, with specific strategies designed to expand LLMs to the broad time series and anomaly detection domains.</p>
<h1>2 Related Work</h1>
<p>Transformers have demonstrated remarkable success in natural language processing (NLP) and, given their proficiency in handling sequential data, a significant number of transformer-based models have been proposed for time series forecasting. Early works focused on modifications to transformer modules, ranging from position embeddings [Nie et al., 2022] to attention mechanisms [Zhou et al., 2021], to better fit time series analysis [Zhou et al., 2023]. Most approaches can be regarded as aligning approaches, as they start training from transformer backbones (such as BERT [Kenton and Toutanova, 2019], GPT-2 [Radford et al., 2019], and T5 [Raffel et al., 2020]), and train the encoder, decoder, or middle layers of the transformers via fine-tuning [Zhou et al., 2023, Chang et al., 2024, Cao et al., 2024] or full parameter pretraining [Ansari et al., 2024]. However, these models often overlook the rich textual information within the pretrained models, where the fine-tuned model is still primarily used to process only time series data.</p>
<p>Starting with ChatGPT [Ouyang et al., 2022], we have witnessed the power of large language models (LLMs) such as GPTs [Achiam et al., 2023] and LLaMAs [Touvron et al., 2023a,b]. These models, with larger parameters and trained on more extensive datasets [Hoffmann et al., 2022], exhibit powerful reasoning capabilities for handling complex tasks. This has triggered initial research in time series analysis, where some studies directly treat the time series as tokens and feed them into LLMs for forecasting [Gruver et al., 2023], or incorporate time series data with instruction prompts [Xue and Salim, 2023] or chain-of-thought prompts [Liu et al., 2024c]. Some even further fine-tune the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of responses from LLaMA-3 and GPT-4 to time series with shape anomalies using direct and multi-modal instructions. The bottom panel shows the overall performance across all trial examples for different anomaly types: global point anomalies, local point anomalies, seasonal anomalies, trend anomalies, and shape anomalies; and prompting strategies: directly use LLMs, multimodal instruction, in-context and chain-of-thought strategies anomaly detection. For each anomaly type and prompting strategy, we conduct five trials and evaluate the correctness of both identified indices and explanations. A correctness rate of 100% means the model provided correct results in all five trials.</p>
<p>LLMs [Jin et al., 2024]. However, these outputs remain within the time series domain, limiting their applicability to tasks such as generating descriptions for time series data.</p>
<p>Few works have investigated utilizing LLMs for time series anomaly detection, a critical task in various real-world applications. Zhou et al. [2023] is likely the first to fine-tune language models for time series anomaly detection. They fine-tuned a general model for diverse time series tasks, such as classification, anomaly detection, forecasting, and few-shot or zero-shot learning, treating anomaly detection as a binary classification problem and adding layers on top of transformer modules for classification. Liu et al. [2024a] used LLMs as the teacher model and trained a student network to mimic the LLM outputs, identifying anomalies as points with distinct values between the teacher and student networks. However, these works did not utilize the reasoning ability of LLMs to provide textual explanations for detections. Zhang et al. [2023] evaluated GPT-4 and Claude-2 ${ }^{2}$ with prompt engineering for human mobility trajectory behavior anomaly detection, asking the models to provide explanations for the detections. However, they treated anomaly detection as a binary classification task (i.e., whether a given human mobility trajectory behavior sequence contains anomalies) and did not delve into the explanations provided by the LLMs. In this work, we focus on detecting specific anomaly points or segments within a time series and investigate whether LLMs can accurately capture the indices of anomalies and explain their detection results. To the best of our knowledge, this is one of the first comprehensive studies on time series anomaly detection using LLMs.</p>
<h1>3 Can LLMs Be Directly Applied for Time Series Anomaly Detection?</h1>
<p>We begin with an empirical study on evaluating two representative large language models (LLMs), GPT-4 and LLaMA-3, in identifying and explaining anomalies in time series data. Our approach involves interpreting time series data as text tokens and tasking the LLMs with: i) determining the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Templates for different prompt strategies, where the 'requirements' include the tasks for the LLMs to do, e.g., providing the indices for the anomalies, and explaining the reason if anomalies are detected, with examples in Figure 1. More details can be found in Appendix A.3
presence of anomalies, and ii) if anomalies are identified, providing the indices for the anomalies and explaining the reasons. We assess the LLMs in terms of their capabilities in detecting five representative types of time series anomalies: global point anomaly, local point anomaly, seasonality anomaly, trend anomaly, and shape anomaly. Detailed descriptions of these anomalies can be found in Section 5.1, and examples of each anomaly type are provided in Appendix A.4.
In Figure 1, the top part illustrates the performance of these LLMs on a short time series with shape anomalies at indices 17, 18, and 19. Unfortunately, after five trials, neither model achieves accurate results. Similar outcomes are observed for other anomaly types, including local point anomalies, seasonality anomalies, and trend anomalies. However, both LLaMA-3 and GPT-4 perform well in detecting global point anomalies. This indicates that LLMs cannot be directly applied to detect most typical time series anomalies.
Upon examining the intermediate reasoning steps of these LLMs, it appears that they involve simplistic methodologies, such as Isolation Forest [Liu et al., 2008] and the z-score technique, as shown in the figure. These approaches make it easier to identify global point anomalies. Unlike GPT-4, which may leverage external tools including Python, LLaMA-3's responses are solely derived from its textual reasoning capabilities. This occasionally results in hallucinated calculations and indices in its responses. Despite this, LLaMA-3 seems to intuitively understand the indices and corresponding values in the time series, particularly in the example shown in the figure, recognizing the significance of indices 4 and 12 for the value 5 , even though these are not the actual anomalies.
In summary, the operational logic of these LLMs for time series anomaly detection can be characterized as follows: they first select a suitable anomaly detection strategy, identify the time series sequences within the input, and then construct their responses based on this strategy. However, based on our exploration, we cannot directly apply LLMs for time series anomaly detection, in particular, comprehensive anomaly types in time series.</p>
<h1>4 How to Make LLMs An Explainable Time Series Anomaly Detector via Prompt Engineering?</h1>
<h3>4.1 Prompting Strategies</h3>
<p>Multi-modal Instruction Since LLMs seem to grasp the overall shape of time series, we add prompts to guide the LLMs to also consider the visual representation of the time series for anomaly detection. Refer to the example prompt template in Figure 2.
In-context Learning [Dong et al., 2023] In-context learning is a common prompting approach that includes $n$-shot examples in the prompts to help LLMs with target tasks. For time series anomaly detection, we include examples of five anomaly types: global point anomalies, local point anomalies, seasonality anomalies, trend anomalies, and shape anomalies, respectively. More details about these types of anomalies can be found in Section 5.1. Figure 2 shows an example of 1-shot in-context learning with an example of global point anomalies. This includes a brief description of the characteristics of global point anomalies, an example time series containing global point anomalies, and an example of the desired explanation for this time series anomaly detection.
Chain-of-thought Prompting [Wei et al., 2022b] Chain-of-thought prompting further guides LLMs to decompose complex questions into detailed intermediate reasoning steps. For time series anomaly detection, humans typically first look at the whole time series to detect whether there are anomalies.</p>
<p>If anomalies are detected, they then examine each anomaly in detail and explain the reasons for being anomalies. We formalize this process into a prompt, as illustrated in Figure 2. The simplest approach is to directly request the model to follow these reasoning steps. Alternatively, we can also add $n$-shot examples for the reasoning process.</p>
<h1>4.2 Performance on Trial Examples</h1>
<p>We apply the above prompting strategies to design the trial examples for the five types of anomalies and evaluate the ability of the LLMs in identifying and explaining such anomalies in time series. The bottom part in Figure 1 shows the overall performance of LLaMA-3 and GPT-4 on these trial examples. Additional examples of the responses from LLaMA-3 and GPT-4 can be found in Appendix A.4.</p>
<p>Generally, we observe that LLaMA-3 does not show significant improvement with different prompt designs and example cases, while GPT-4 demonstrates impressive results with any kind of prompts. For each case, we conduct five trials, and a detailed analysis of the responses from GPT-4 and LLaMA-3 reveals the following: GPT-4's responses are more consistent, suggesting that GPT-4 genuinely understands the prompts and examples. These instructions "activate" GPT-4 to consider different perspectives and provide correct results. In contrast, LLaMA-3 more likely provides varied responses to the same prompt. For more obvious anomalies, such as global point anomalies and trend anomalies (see Figure 7 in Appendix), LLaMA-3 provides more stable results with correctly identified anomalies and explanations. In summary, we observe more emergent abilities [Wei et al., 2022a] in GPT-4, where simple instructions can activate its capability of time series anomaly detection, leading to more accurate identification and explanation of time series anomalies. Although LLaMA-3 does not exhibit these abilities to the same extent (potentially due to its smaller parameter size compared to GPT-4), it still shows some capabilities in grasping the overall shape of time series.</p>
<h3>4.3 LLM against Anomaly Detection Baselines</h3>
<p>Given the impressive performance of GPT-4 on all trial examples with different prompts, we now evaluate how GPT-4 performs time series anomaly detection compared to classic anomaly detection baseline methods.</p>
<p>Datasets \&amp; Evaluation Metrics We evaluate the performance on four common time series anomaly detection datasets [Paparrizos et al., 2022]: YAHOO, ECG, SVDB, and IOPS, which include anomalies in monitoring services and ECG recordings. In our study, we carefully curate the datasets to encompass a broad spectrum of patterns. In each dataset, we select 100 distinct time series segments with length 1,080 that demonstrate maximum variability. We utilize the initial $50 \%$ of each time series as training data. More details can be found in Appendix A.1. We use F-score and Range-F [Paparrizos et al., 2022] to evaluate the performance. Range-F is an extension of the F-score, where a detection is considered accurate if the identified anomaly falls within the same window as the actual anomaly; in this case, we set the window size to 5 .
Baseline Methods In our comparison, we evaluate a range of time series anomaly detection methods. This includes traditional approaches such as Isolation Forest (IForest) [Liu et al., 2008], Matrix Profile (MP) [Yeh et al., 2016], and Autoencoder [Sakurada and Yairi, 2014]. Additionally, we explore forecasting-based methods, namely LSTM [Malhotra et al., 2015], Prophet [Taylor and Letham, 2018], Informer [Zhou et al., 2021], DLinear [Zeng et al., 2023] and TimesNet [Wu et al., 2022]. For these forecasting methods, anomalies are defined as observations deviating from the forecasted values by more than a 3- $\sigma$ (three standard deviations) threshold. More implementation details can be found in Appendix A.2.
LLM Settings We structure the time series segments using multi-modal prompts analogous to the example depicted in Figure 1, then feed the prompts to GPT-4 through the OpenAI API services ${ }^{3}$. Additionally, we craft a specific prompt designed to parse the output into a desired JSON format. This format encompasses two key components: a list of indices identifying the anomalous points, and a textual explanation that elucidates the rationale behind the identification of these anomalies. More details can be found in Appendix A.6.
Comparison Results Table 1 provides a comparative analysis of GPT-4 against baseline methods in anomaly detection tasks. Generally, we observe that most baseline methods perform well on</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Comparison between GPT-4 and Classic Time Series Anomaly Detectors with rank.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>YAHOO</th>
<th></th>
<th>ECG</th>
<th></th>
<th>SVDB</th>
<th></th>
<th>IOPS</th>
<th></th>
<th>avg-Rank</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>F-score</td>
<td>Range-F</td>
<td>F-score</td>
<td>Range-F</td>
<td>F-score</td>
<td>Range-F</td>
<td>F-score</td>
<td>Range-F</td>
<td></td>
</tr>
<tr>
<td>IForest</td>
<td>0.0271(7)</td>
<td>0.1066(8)</td>
<td>0.1602(9)</td>
<td>0.1780(9)</td>
<td>0.0839(5)</td>
<td>0.0886(9)</td>
<td>0.0754(4)</td>
<td>0.1125(5)</td>
<td>9(7,9)</td>
</tr>
<tr>
<td>MP</td>
<td>0.0630(4)</td>
<td>0.1719(4)</td>
<td>0.1654(8)</td>
<td>0.1808(8)</td>
<td>0.1170(4)</td>
<td>0.1303(4)</td>
<td>0.0064(9)</td>
<td>0.0160(9)</td>
<td>7(7,7)</td>
</tr>
<tr>
<td>AE</td>
<td>0.0212(8)</td>
<td>0.1078(7)</td>
<td>0.3199(2)</td>
<td>0.3580(2)</td>
<td>0.4352(1)</td>
<td>0.4626(1)</td>
<td>0.0667(6)</td>
<td>0.0985(7)</td>
<td>4(2,4)</td>
</tr>
<tr>
<td>LSTM</td>
<td>0.1466(1)</td>
<td>0.2144(1)</td>
<td>0.1737(7)</td>
<td>0.2012(7)</td>
<td>0.0773(9)</td>
<td>0.0900(8)</td>
<td>0.1062(2)</td>
<td>0.1395(3)</td>
<td>6(5,6)</td>
</tr>
<tr>
<td>Prophet</td>
<td>0.0455(5)</td>
<td>0.1324(5)</td>
<td>0.3852(1)</td>
<td>0.4556(1)</td>
<td>0.1745(3)</td>
<td>0.2107(3)</td>
<td>0.0456(8)</td>
<td>0.1324(4)</td>
<td>2(2,1)</td>
</tr>
<tr>
<td>Informer</td>
<td>0.0304(6)</td>
<td>0.1161(6)</td>
<td>0.1842(6)</td>
<td>0.2141(6)</td>
<td>0.0784(8)</td>
<td>0.0916(7)</td>
<td>0.0474(7)</td>
<td>0.0707(8)</td>
<td>8(9,8)</td>
</tr>
<tr>
<td>DLinear</td>
<td>0.1051(3)</td>
<td>0.1760(3)</td>
<td>0.1867(4)</td>
<td>0.2208(4)</td>
<td>0.0801(7)</td>
<td>0.0954(5)</td>
<td>0.0676(5)</td>
<td>0.0995(6)</td>
<td>5(5,5)</td>
</tr>
<tr>
<td>TimesNet</td>
<td>0.1457(2)</td>
<td>0.2112(2)</td>
<td>0.1867(4)</td>
<td>0.2178(5)</td>
<td>0.0809(6)</td>
<td>0.0943(6)</td>
<td>0.1443(1)</td>
<td>0.1889(1)</td>
<td>1(1,2)</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.0204(9)</td>
<td>0.0936(9)</td>
<td>0.2911(3)</td>
<td>0.3258(3)</td>
<td>0.2681(2)</td>
<td>0.2945(2)</td>
<td>0.1020(3)</td>
<td>0.1414(2)</td>
<td>3(2,3)</td>
</tr>
</tbody>
</table>
<p>specific datasets. For example, LSTM performs well on the YAHOO and IOPS datasets but poorly on the ECG and SVDB datasets. Conversely, the Autoencoder performs well on the ECG and SVDB datasets but poorly on YAHOO and IOPS. Compared to these baseline methods, GPT-4 shows notable achievements in F-score and Range-F metrics across the ECG, SVDB, and IOPS datasets, with an average rank of 3. Despite its performance on YAHOO, GPT-4 is the most stable model across the ECG, SVDB, and IOPS datasets, even when compared to TimesNet and Prophet. This demonstrates the potential of GPT-4 as a generalized time series anomaly detector.</p>
<p>Hallucination in Indices Although the F-score and Range-F results from GPT-4 appear promising, our analysis reveals that the model occasionally generates indices outside the expected time series segments, such as 1,200 for a series of length 1,000. As shown in Table 2, these hallucinations occur in approximately $21 \%$ to $29 \%$ of time series segments across different datasets. In datasets like YAHOO and IOPS, with a lower anomaly proportion (about $1 \%$ to $4 \%$ of segment points), the median number of hallucinated points is relatively low ( 2 or 3 ). However, the average number of hallucinations is significantly higher, indicating that some segments experience many spurious predictions. This issue is more pronounced in datasets such as ECG and SVDB, which have a higher anomaly ratio (about $20 \%$ to $30 \%$ of segment points), leading to an increase in hallucinated positions.</p>
<p>Explanation Analysis For the explanation provided by GPT-4, we manually analyze the results and classify the explanations into three categories: i) good explanation, where the model provides reasonable reasons for the detection results and correctly identifies the index of the anomalies; ii) bad explanation, where the model fails to explain the detected anomalies well or cannot detect them; and iii) good explanation with hallucination in values, where the model reasonably explains the detected anomalies but incorrectly mentions the index or value of the anomalies. Figure 3 shows examples for the three conditions, and more examples for other datasets can be found in Appendix B.2. Table 3 shows the counts for explanation performance under different conditions. For the YAHOO and IOPS datasets, which exhibit more local and global point anomalies like spikes and dips, GPT-4 generally provides accurate explanations. Poor explanation on the datasets typically occurs when GPT-4 fails to detect anomalies precisely, such as identifying only certain local point anomalies while overlooking significant global ones or missing pattern change anomalies. Occasionally, GPT-4 misinterprets figures, mistaking a dip for a spike, as shown in Figure 3. In contrast, the ECG and SVDB datasets, which contain more context-aware anomalies such as pattern changes, pose greater challenges for GPT-4 in providing accurate explanations. The model often continues to search for local and global point anomalies, sometimes mistakenly identifying periodic spikes in ECG signals as anomalies. However, when a pattern change is pronounced, GPT-4 can detect the shift and provide a coherent explanation. In general, hallucinations in explanation typically occur when GPT-4 describes the index or values of anomalies. Such errors are more frequent in</p>
<p>Table 3: Summary of the explanation capability of GPT-4 on different datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Count</th>
<th style="text-align: center;">YAHOO</th>
<th style="text-align: center;">ECG</th>
<th style="text-align: center;">SVDB</th>
<th style="text-align: center;">IOPS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Good Explanation</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">Good Explanation w Hallucination</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">35</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Examples for a) good, b) bad, and c) hallucinated explanation by GPT-4 on IOPS dataset.
the YAHOO and IOPS datasets, characterized by a higher incidence of point-based anomalies, but have hallucination when mentioning the indices of the anomalies or the values of the anomalies.</p>
<p>Summary With minimal instructions, GPT-4 often presents as a great explainable time series anomaly detector, sometimes ranking in the top three among various baseline methods. However, detecting longer time series poses significant challenges to GPT-4. While it excels in identifying local and global anomalies, it struggles with more nuanced, context-dependent scenarios and tends to hallucinate about both the anomaly indices and the explanations provided. Conversely, prompt strategies do not benefit LLaMA3, likely due to its smaller size compared to GPT-4. In the next section, we will explore whether LLaMA3's performance can be improved through instruction fine-tuning.</p>
<h1>5 Can LLMs be Improved via Instruction Fine-tuning?</h1>
<p>While GPT-4 can be "activated" as an effective explainable time series anomaly detector, particularly for shorter time series, LLaMA-3 does not benefit as much from prompt engineering, primarily due to its smaller parameter size. Therefore, we aim to investigate whether LLaMA-3's performance can be improved via fine-tuning. Given the scarcity of time series with anomalies and corresponding textual explanation datasets, we propose a time series and text explanation generator TTGenerator to create the instruction datasets for fine-tuning LLaMA-3.</p>
<h3>5.1 Time Series and Text Explanation Generator: TTGenerator</h3>
<p>Base Time Series Generation Formally, a time series dataset $X$ with $T$ timestamps can be represented as an ordered sequence of data points: $X=\left(x_{1}, x_{2}, \cdots, x_{T}\right)$, where $x_{i}$ is the data point at timestamp $i(i \in T)$. Generally, a time series is viewed as a combination of trend, seasonality, and noise components:</p>
<p>$$
X=s(T)+\tau(T)+\epsilon
$$</p>
<p>where $s(\cdot)$ represents the base shapelet function approximating the detrended series, which could be a combination of sine and square wave functions, i.e., $\sum_{n}\left(A_{n} \sin \left(2 \pi \omega_{n} T\right)\right)$, where $A$ is the amplitude and $\omega_{n}$ as the frequency. Alternatively, time series can be generated via Inverse Fast Fourier Transform (IFFT), i.e., $\sum_{n}\left(A_{n} \exp \frac{2 \pi \omega_{n} n}{T}\right) ; \tau(\cdot)$ models the overall trend of the series, which could be linear or exponential; and $\epsilon$ represents the noises which could be just white noises.
Anomaly Points Generation Following Lai et al. [2021], we examine various types of time series anomalies, including point-wise and pattern-wise anomalies. Point-wise anomalies are defined as unexpected incidents at individual time points:</p>
<p>$$
\left|x_{t}-\hat{x}_{t}\right|&gt;\delta
$$</p>
<p>This includes local point anomalies, where $\delta=\lambda \cdot \sigma\left(X_{[x-C \leq x \leq x+C]}\right)$ with $C$ as the context window size, and global point anomalies, where $\delta=\lambda \cdot \sigma(X)$, representing significant spikes or dips in the time series. Here, $\sigma$ denotes the standard deviation and $\lambda$ sets the threshold level. Pattern-wise anomalies represent anomalous subsequences characterized by changes in seasonality, trend, or shape. Specifically, within a time series data $X$, an underlying subsequence $X_{i, j}$ from timestamp $i$ to $j$ can be considered anomalous if:</p>
<p>$$
\operatorname{sim}\left(X_{i, j}, \hat{X}_{i, j}\right)&gt;\delta
$$</p>
<p>This indicates significant deviation from the expected values $\hat{X}<em n="n">{i, j}$. A seasonality anomaly may occur with an amplitude change (i.e., a modified $\bar{A}</em>}$ in $s\left(T_{i, j}\right)$ ) or a period change (i.e., a modified $\bar{\omega<em i_="i," j="j">{n}$ in $s\left(T</em>\right)$ ). Trend anomalies may involve a change point (where trends differ before and after point $i$, with $1&lt;i&lt;N$ ), or a trend break (where the trend changes at $i$ and then reverts at $j$, with $1&lt;i&lt;j&lt;N$ ). Shape change anomalies may manifest as a pattern change (where the base pattern shifts starting at $i$ and continues to $j$, with $1&lt;i&lt;N$ ), or a pattern break (where the base pattern changes at $i$ but returns to normal by $j$, with $1&lt;i&lt;j&lt;N$ ).</p>
<p>Explanation Generation After generating the base time series and the anomalies, we utilize a template to produce a description of the time series. This description includes: (i) details about the base time series such as seasonality, trend, and noise; and (ii) specifics about the anomalies, including the types of anomalies and their starting and ending indices. For time series that do not contain anomalies, the description will state: "There is no obvious anomaly in this time series". To enhance the diversity of the dataset, we employ GPT-4 to rewrite the description for each sample.</p>
<p>In summary, TTGenerator synthesizes time series with outliers by (i) selecting random seasonality and trend patterns, (ii) inserting various types of outliers, and (iii) generating descriptions for the time series and the anomalies. More details are provided in Appendix A.5.</p>
<h1>5.2 Instruction Fine-tuning on LLaMA3</h1>
<p>With TTGenerator, we generate the instruction dataset as follows: 1) Random Selection of Length: We randomly select the length of the generated time series from various time series lengths. We do not consider longer time series due to the context window length limitation for the LLMs, such as the 8 k token limit for LLaMA3. 2) Sample Generation: We generate a single sample that includes the time series values, labels for the anomalies, and explanations for both the base time series and the anomalies. 3) Text Prompt Formation: We concatenate the information of a time series to form the text prompt to train the model as:</p>
<p>Instruction: [instruction]
Time Series Values: [time series values]
Requirements: [requirements]
Response: [JSON format with keys anomaly as the labels for the anomalies and reason as the explanation for the anomalies]
where [instruction] refers to the general instruction and [requirements] specify that the output should be formatted in JSON with anomaly and reason as the two keys. 4) Repetition: Finally, we repeat the procedures 1)-3) $n$ times to create the final dataset, where $n$ is the dataset size. To fine-tune the instruction dataset on LLaMA3, we use a parameter-efficient fine-tuning (PEFT) approach, specifically LoRA [Hu et al., 2021], to obtain the fine-tuned model. More details on the fine-tuning can be found in Appendix A.6.</p>
<h3>5.3 Results</h3>
<p>We evaluate the performance on three synthesized datasets generated by TTGenerator for five types of time series anomalies: global point anomaly, local point anomaly, seasonality anomaly, trend anomaly, and shape anomaly. The datasets have different time series lengths of 100, 200, and 400. More details about the synthesized datasets can be found in Appendix A. 5 and more experimental results in Appendix B.3.</p>
<p>Overall Performance We compare the performance of the original LLaMA3 with our fine-tuned version. The results are shown in Table 4. Generally, for both models, the performance decreases as the length of the time series increases. Despite the relatively low F-score for point-aware anomalies, the Range-F score is relatively high, indicating that the model is able to capture the correct anomalies but may hallucinate in the surrounding indices. Comparing point-aware and context-aware anomalies, both models provide more stable performance on context-aware anomalies compared to point-aware anomalies across different time series lengths. Comparing the original and fine-tuned versions of LLaMA3, the ability to detect local point anomalies and shape anomalies does not seem to benefit much from the instruction fine-tuning. However, we observe general improvements in the average F-score and Range-F, with significant improvements in detecting seasonality anomalies.</p>
<p>Hallucination in Indices Similar to GPT-4, LLaMA3 exhibits hallucinations in its detection results. Table 5 presents these findings. Interestingly, we observe a decreasing trend in the number of</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Examples for a) good, b) bad, and c) hallucinated explanation by LLaMA3</p>
<p>Table 4: Overall performance of original and fine-tuned LLaMA3 on the synthesized dataset</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Anomaly Type</th>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fine-tuned</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: center;">Global Point</td>
<td style="text-align: center;">F-score</td>
<td style="text-align: center;">0.0528</td>
<td style="text-align: center;">0.0300</td>
<td style="text-align: center;">0.0104</td>
<td style="text-align: center;">0.0539</td>
<td style="text-align: center;">0.0264</td>
<td style="text-align: center;">0.0130</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Range-F</td>
<td style="text-align: center;">0.4091</td>
<td style="text-align: center;">0.2205</td>
<td style="text-align: center;">0.1207</td>
<td style="text-align: center;">0.4157</td>
<td style="text-align: center;">0.2443</td>
<td style="text-align: center;">0.1421</td>
</tr>
<tr>
<td style="text-align: center;">Local Point</td>
<td style="text-align: center;">F-score</td>
<td style="text-align: center;">0.0415</td>
<td style="text-align: center;">0.0286</td>
<td style="text-align: center;">0.0182</td>
<td style="text-align: center;">0.0426</td>
<td style="text-align: center;">0.0245</td>
<td style="text-align: center;">0.0174</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Range-F</td>
<td style="text-align: center;">0.3481</td>
<td style="text-align: center;">0.2595</td>
<td style="text-align: center;">0.1664</td>
<td style="text-align: center;">0.3611</td>
<td style="text-align: center;">0.2424</td>
<td style="text-align: center;">0.1624</td>
</tr>
<tr>
<td style="text-align: center;">Seasonality</td>
<td style="text-align: center;">F-score</td>
<td style="text-align: center;">0.2706</td>
<td style="text-align: center;">0.2890</td>
<td style="text-align: center;">0.2119</td>
<td style="text-align: center;">0.4189</td>
<td style="text-align: center;">0.3448</td>
<td style="text-align: center;">0.2121</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Range-F</td>
<td style="text-align: center;">0.4436</td>
<td style="text-align: center;">0.3904</td>
<td style="text-align: center;">0.3369</td>
<td style="text-align: center;">0.5479</td>
<td style="text-align: center;">0.4114</td>
<td style="text-align: center;">0.3477</td>
</tr>
<tr>
<td style="text-align: center;">Trend</td>
<td style="text-align: center;">F-score</td>
<td style="text-align: center;">0.0772</td>
<td style="text-align: center;">0.1315</td>
<td style="text-align: center;">0.0958</td>
<td style="text-align: center;">0.0750</td>
<td style="text-align: center;">0.1314</td>
<td style="text-align: center;">0.1871</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Range-F</td>
<td style="text-align: center;">0.2520</td>
<td style="text-align: center;">0.2195</td>
<td style="text-align: center;">0.1367</td>
<td style="text-align: center;">0.2321</td>
<td style="text-align: center;">0.2397</td>
<td style="text-align: center;">0.2354</td>
</tr>
<tr>
<td style="text-align: center;">Shape</td>
<td style="text-align: center;">F-score</td>
<td style="text-align: center;">0.2193</td>
<td style="text-align: center;">0.2171</td>
<td style="text-align: center;">0.2460</td>
<td style="text-align: center;">0.2489</td>
<td style="text-align: center;">0.1966</td>
<td style="text-align: center;">0.2414</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Range-F</td>
<td style="text-align: center;">0.3981</td>
<td style="text-align: center;">0.3199</td>
<td style="text-align: center;">0.3115</td>
<td style="text-align: center;">0.3871</td>
<td style="text-align: center;">0.3117</td>
<td style="text-align: center;">0.3393</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">F-score</td>
<td style="text-align: center;">0.1323</td>
<td style="text-align: center;">0.1392</td>
<td style="text-align: center;">0.1165</td>
<td style="text-align: center;">0.1679</td>
<td style="text-align: center;">0.1447</td>
<td style="text-align: center;">0.1342</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Range-F</td>
<td style="text-align: center;">0.3702</td>
<td style="text-align: center;">0.2820</td>
<td style="text-align: center;">0.2144</td>
<td style="text-align: center;">0.3888</td>
<td style="text-align: center;">0.2899</td>
<td style="text-align: center;">0.2454</td>
</tr>
</tbody>
</table>
<p>hallucinated time series segments with the fine-tuned LLaMA3 compared to GPT-4's performance (Appendix B.2), while LLaMA3 tends to hallucinate more indices than GPT-4.</p>
<p>Table 5: Details of hallucination of fine-tuned LLaMA3 on each dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Length</th>
<th style="text-align: center;"># Segments</th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">Median</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">197.5</td>
<td style="text-align: center;">166.0</td>
</tr>
<tr>
<td style="text-align: center;">200</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">213.2</td>
<td style="text-align: center;">203.5</td>
</tr>
<tr>
<td style="text-align: center;">400</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">148.9</td>
<td style="text-align: center;">141.0</td>
</tr>
</tbody>
</table>
<p>Table 6: Summary of the explanation capability of LLaMA3 for different anomaly types.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Count</th>
<th style="text-align: center;">Global</th>
<th style="text-align: center;">Local</th>
<th style="text-align: center;">Seasonal</th>
<th style="text-align: center;">Trend</th>
<th style="text-align: center;">Shape</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Good Explanation</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">Good Explanation w Hallucination</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Explanation Analysis We further manually check the explanation performance by the fine-tuned LLaMA3 on the five anomaly types over the three datasets, where there are 60 samples for each anomaly type. The results are shown in Table 6. Similar to GPT-4, we notice that the model explains well for point-aware anomalies than the context-aware anomalies. Different from GPT-4, the explanations provided by LLaMA3 are more general, such as "The anomalies are mostly due to the sudden changes in the value of the time series". Examples for the explanations provided by LLaMA3 can be found in Figure 4.</p>
<h1>6 Conclusion</h1>
<p>In this paper, we comprehensively investigate the capability of Large Language Models (LLMs) in time series anomaly detection by addressing three key questions: Can LLMs be directly applied for explainable time series anomaly detection? How can LLMs detect and explain time series anomalies via prompt engineering? Can we improve LLMs' detection performance through instruction finetuning? The answers to these questions are: No, Yes, and Yes, respectively, with evidence showing that GPT-4 demonstrates competent performance compared to baseline methods with minimal effort in prompt engineering, and LLaMA3 achieves better performance after instruction fine-tuning. In summary, LLMs show promising potential for time series anomaly detection, while customized prompts and instructions are essential.</p>
<h1>References</h1>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, and Yuyang Wang. Chronos: Learning the language of time series, 2024.</p>
<p>Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. TEMPO: Prompt-based generative pre-trained transformer for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=YH5w120UuU.</p>
<p>Ching Chang, Wei-Yao Wang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts: Aligning pre-trained llms as data-efficient time-series forecasters, 2024.</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning, 2023.</p>
<p>Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.</p>
<p>Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=ORDcd5Axok.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021.</p>
<p>Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. ICLR, 2024.</p>
<p>Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages $4171-4186,2019$.</p>
<p>Kwei-Herng Lai, Daochen Zha, Junjie Xu, Yue Zhao, Guanchu Wang, and Xia Hu. Revisiting time series outlier detection: Definitions and benchmarks. In Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 1), 2021.</p>
<p>Chen Liu, Shibo He, Qihang Zhou, Shizhong Li, and Wenchao Meng. Large language model guided knowledge distillation for time series anomaly detection. arXiv preprint arXiv:2401.15123, 2024a.</p>
<p>Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 eighth ieee international conference on data mining, pages 413-422. IEEE, 2008.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b.</p>
<p>Haoxin Liu, Zhiyuan Zhao, Jindong Wang, Harshavardhan Kamarthi, and B Aditya Prakash. Lstprompt: Large language models as zero-shot time series forecasters by long-short-term prompting. arXiv preprint arXiv:2402.16132, 2024c.</p>
<p>Pankaj Malhotra, Lovekesh Vig, Gautam Shroff, Puneet Agarwal, et al. Long short term memory networks for anomaly detection in time series. In Esann, volume 2015, page 89, 2015.</p>
<p>Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. ACM Computing Surveys, 54(2):1-38, 2021.</p>
<p>John Paparrizos, Yuhao Kang, Paul Boniol, Ruey S Tsay, Themis Palpanas, and Michael J Franklin. Tsb-uad: an end-to-end benchmark suite for univariate time-series anomaly detection. Proceedings of the VLDB Endowment, 15(8):1697-1711, 2022.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.</p>
<p>Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimensionality reduction. In Proceedings of the MLSDA 2014 2nd workshop on machine learning for sensory data analysis, pages 4-11, 2014.</p>
<p>Sean J Taylor and Benjamin Letham. Forecasting at scale. The American Statistician, 72(1):37-45, 2018.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022a. ISSN 2835-8856. URL https://openreview.net/forum?id=yzkSU5zdwD. Survey Certification.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022b.</p>
<p>Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The eleventh international conference on learning representations, 2022.</p>
<p>Hao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. IEEE Transactions on Knowledge and Data Engineering, 2023.</p>
<p>Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva, Abdullah Mueen, and Eamonn Keogh. Matrix profile i: all pairs similarity joins for time series: a unifying view that includes motifs, discords and shapelets. In 2016 IEEE 16th international conference on data mining (ICDM), pages 1317-1322. Ieee, 2016.</p>
<p>Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11121-11128, 2023.</p>
<p>Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, and Jingbo Shang. Large language models for time series: A survey, 2024.</p>
<p>Zheng Zhang, Hossein Amiri, Zhenke Liu, Andreas Züfle, and Liang Zhao. Large language models for spatial trajectory patterns mining. arXiv preprint arXiv:2310.04942, 2023.</p>
<p>Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106-11115, 2021.</p>
<p>Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained lm. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.</p>
<h1>A Experimental Settings</h1>
<h2>A. 1 Benchmark Dataset Settings</h2>
<p>We selected four widely used time series anomaly detection datasets: YAHOO, ECG, SVDB, and IOPS, as referenced in the paper by Paparrizos et al. (2022)[Paparrizos et al., 2022]. The original datasets can be downloaded from the repository ${ }^{4}$. We constructed the evaluation dataset by manually selecting segments from the time series data. First, we determined the window size for each time series using the Fast Fourier Transform (FFT) and then computed the median window size across the dataset. The segment length was set to four times the median window size, resulting in a segment length of 1080 based on the window sizes of the four datasets. Each time series was partitioned into multiple segments of this length. We manually inspected the segments with a length of 1080 for each dataset, selecting time series with diverse distributions. From these, we randomly extracted 100 segments for evaluation. Table 7 presents the specifics of segment lengths and the number of time series used, where '# Time Series' denotes the distinct time series in the original dataset, and '\% Anomalies' denotes the average proportion of anomalies in the selected segments. Examples for each dataset can be seen in Figure 3 and Figure 18.</p>
<p>Table 7: Dataset Details</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Segment Length</th>
<th style="text-align: center;"># Time Series</th>
<th style="text-align: center;">\% Anomalies</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">YAHOO</td>
<td style="text-align: center;">1080</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1.42</td>
</tr>
<tr>
<td style="text-align: center;">ECG</td>
<td style="text-align: center;">1080</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">19.99</td>
</tr>
<tr>
<td style="text-align: center;">SVDB</td>
<td style="text-align: center;">1080</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">27.94</td>
</tr>
<tr>
<td style="text-align: center;">IOPS</td>
<td style="text-align: center;">1080</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">3.78</td>
</tr>
</tbody>
</table>
<h2>A. 2 Baselines Settings</h2>
<p>IForest[Liu et al., 2008] We use the Scikit-learn implementation ${ }^{5}$ with n_estimators set to 100. Following the approach in Wu et al. [2022], we employ the Fast Fourier Transform to determine the optimal window size for each time series.
Matrix Profile (MP) [Yeh et al., 2016] We use the Stumpy implementation ${ }^{6}$ and set the window size for each time series based on the Fast Fourier Transform strategy.
Autoencoder (AE) [Sakurada and Yairi, 2014] Following the parameter settings suggested in Paparrizos et al. [2022], we use three encoder and three decoder layers with ReLU as the activation function. The window size is adjusted to match the length of the test data.
Prophet [Taylor and Letham, 2018] We use the official Facebook implementation ${ }^{7}$ and detect anomalies using the forecasted yhat_upper and yhat_lower bounds.</p>
<p>LSTM [Malhotra et al., 2015], Informer [Zhou et al., 2021], TimesNet [Wu et al., 2022], and DLinear [Zeng et al., 2023] Implementations are sourced from NeuralForecast ${ }^{8}$. Anomalies are detected by applying the 3- $\sigma$ rule, which flags any data point deviating more than three standard deviations from the mean.</p>
<h2>A. 3 Prompt Settings</h2>
<p>The prompt we used for inference contains two parts: the instruction part and the requirements part.
Instruction Prompt Figure 5 provides the full details of the instructions for each type of prompt strategy. The example time series used to generate the in-context learning and chain-of-thought</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Directly Use - General Instruction</h1>
<p>Given a time series with values ${$ time series values $}$, consider to identify any potential anomalies.</p>
<h2>Multi-modal Instruction</h2>
<p>Given a time series with values ${$ time series values $}$, please think about the visual representation of this series and evaluate it to identify any anomalies. This assessment should consider both the numerical data and the visual information.</p>
<h2>In-context Learning Instruction - n-shot</h2>
<p>{General Instruction or Multi-modal Instruction}
Generally, anomalies in time series can be either point-based anomalies or context-aware anomalies. For example,
for i in range $(\mathrm{n})$ :
Example ${\mathbf{i}}:$ {Anomaly Type $}$
Characteristics: {Characteristics of the Anomaly Type}
Time Series: {Example Time Series for the Anomaly Type}
Explanation: {Ideal Explanation for the Time Series Anomalies}</p>
<h2>Chain-of-thought Instruction - n-shot</h2>
<p>{General Instruction or Multi-modal Instruction}
Here are some basic knowledge about the time series anomalies: Generally, anomalies in time series can be either point-based anomalies or context-aware anomalies, where point-based anomalies are points have significant larger or lower values than other points, context-aware anomalies could be shifts in trend, or changing in base patterns.
Think to solve the problem step by step.
First, try identify whether there are anomalies in the input.
Second, if anomalies are identified, try to get it's index according to it's position in the list.
Third, explain why those points should be considered as anomalies.
for i in range $(\mathrm{n})$ :
Example ${\mathbf{i}}$ : For time series ${$ Example Time Series $}$
First, there are {Anomaly Type} in this time series.
Second, the values at positions {Example Time Series Anomaly Indices} are anomalies.
The reason is ${$ Ideal Explanation for the Time Series Anomalies $}$.
Figure 5: Full Instruction Prompt for Each Strategy. For in-context learning and chain-of-thought learning, either the general instruction or multi-modal instruction is added to the beginning of the prompt to guide LLMs in performing the anomaly detection task.
prompts are shown in Figure 7. Note that the range of n for in-context learning is 1-5, and for chain-of-thought learning is $0-5$.</p>
<p>Requirements Prompt The requirements are specified for either trial cases or general experiments, with full details shown in Figure 6. For general experiments, we request the LLMs to return results in JSON format to 1) facilitate easier extraction of detection results and 2) avoid generating lengthy responses that may exceed the context window length. After obtaining the JSON output, we use LangChain's ${ }^{9}$ JSON output parser for further analysis.</p>
<h2>A. 4 Trial Examples</h2>
<p>The details about the trial examples used in Sections 3 and 4 are shown in Figure 7, where the explanation part describes the ideal explanation for those anomalies. When constructing in-context learning and chain-of-thought prompts with n-shot examples, we use distinct anomaly types to formulate the prompt. For example, when inferring on a time series with shape anomalies, we will randomly choose examples of other types of anomalies, such as local point anomalies. Specifically, we set n to 1 to obtain the results shown in Figure 1.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Requirements for Trial Cases</h1>
<p>If anomalies are present, please indicate: 1) The presence of anomaly points in this time series. 2) The indices of these anomaly points, and 3) The reasoning behind these points being considered anomalies.</p>
<h2>Requirements for General Experiments</h2>
<p>Please consider answering the following questions according to your observation. First, please try to identify the potential anomalies, and provide the list of the indexes of anomalies, if no anomalies, please return []. Second, if there are anomalies in the time series, please provide a short explanation of the anomalies.
Summarize the answers into two keys:</p>
<ul>
<li>anomaly: a list of indexes</li>
<li>reason: a string of explanation</li>
</ul>
<p>And format the output as JSON with the two keys.
Required: return the JSON only without other information.</p>
<p>Figure 6: Details for the requirements for trial cases and general experiments.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Details for the trial examples used for the case studies for the time series.</p>
<h2>A. 5 TTGenerator Details</h2>
<p>Base Time Series Generation Generally, a time series is viewed as a combination of trend, seasonality, and noise, as described in equation 1. For the seasonality component, we use one of three methods: i) A single sine wave function, i.e., $A \sin (2 \pi \omega T+\beta)$, where $A$ is the amplitude (ranging from 1 to 1000), $\omega$ is the frequency (ranging from 1 to 10 ), and $\beta$ is a phase shift (ranging from 0 to $2 \pi$ ). ii) A combination of sine wave functions, i.e., $\sum_{n}\left(A_{n} \sin \left(2 \pi \omega_{n} T\right)\right)$, where $A_{n}=\frac{1}{2 n+1}$, following the settings in Lai et al. [2021], and $n$ is randomly sampled in the range of 3 to 10 . iii) An IFFT function, i.e., $\sum_{n}\left(A_{n} \exp \frac{2 \pi \omega_{n} n}{T}\right)$, where $n$ is randomly selected in the range of 0 to 10 . To determine the seasonality for a time series, we randomly sample from these three methods with probabilities $[0.25,0.25,0.5]$. For the trend component, we consider either a linear trend, polynomial trend, or no trend, with sampling probabilities $[0.3,0.1,0.6]$, assuming most time series have no trend and more linear trends than polynomial trends. For the linear trend, we randomly sample the slope in</p>
<h1>Trend</h1>
<h2>Linear Trend</h2>
<ul>
<li>Increasing: The time series is going with an increasing trend.</li>
<li>Decreasing: The time series is going with a decreasing trend.</li>
</ul>
<h2>Polynomial Trend</h2>
<ul>
<li>if min(prime_gradients) &gt; 0: This time series shows increasing polynomial trend.</li>
<li>if max(prime_gradients) &lt; 0: This time series shows decreasing polynomial trend.</li>
<li>else: This time series shows polynomial trend.</li>
</ul>
<h2>Seasonality</h2>
<p>Sine: This time series includes sine-wave like seasonal patterns, which include {number of periods $}$ periods with each last for approximately every ${$ number of points in a period $}$ points.</p>
<p>Square Sine: This time series includes sine-wave like seasonal patterns, which for $i$ in range( $n$ )
include {number of periods} periods with each last for approximately every {number of points in a period $}$ points</p>
<p>IFFT: The time series appears to contain signals that can be effectively analyzed using the Fourier Transform, likely featuring prominent frequencies at ${$ frequencies $}$.</p>
<p>No Seasonality: No seasonality observed in this time series.</p>
<h2>Noise</h2>
<p>The time series has normal distributed noises with mean as 0 and variance as 1 .</p>
<p>Figure 8: Template for time series base pattern explanation.
the range $(-1,1)$. For the polynomial trend, we randomly sample the degree in the range 2 to 5 , and the coefficients for each degree are sampled from the range $(-1,1)$, with a shift randomly sampled from the range $(-5,5)$. For the noise component, we use normally distributed noise with a mean of 0 and a standard deviation of 1 . We then combine these three components to generate the final base time series, where the amplitude for the trend is in the range $(1,200)$, and for the noise is in the range $(1,50)$.
Anomaly Points Generation The anomalies in time series can be roughly classified as point-aware anomalies and context-aware anomalies.
The point-aware anomalies can be either local anomalies, where $\delta=\lambda \cdot \sigma\left(X_{[x-C \leq x \leq x+C]}\right)$ with $C$ as the context window size, or global anomalies, where $\delta=\lambda \cdot \sigma(X)$. For global point anomalies, we set the anomaly value to be $\lambda \cdot \sigma(X)$ with $\lambda$ in the range $(3,20)$. For local point anomalies, we set the anomaly value to be $\lambda \cdot \sigma\left(X_{[x-C \leq x \leq x+C]}\right)$ with context window $C$ in the range $(10,50)$ and $\lambda$ in the range $(2,5)$. We randomly sample 1-6 points in a time series to be replaced by point-wise anomaly values.
The pattern-aware anomalies can be classified into seasonality anomalies, trend anomalies, and shape anomalies. Specifically, a seasonality anomaly may occur with an amplitude change, i.e., a modified $\widetilde{A}<em i_="i," j="j">{n}$ in $s\left(T</em>\right)$. We randomly sample whether the amplitude becomes larger or smaller, setting the</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: Template for time series anomaly pattern explanation - point-aware anomalies.
larger ratio to be 1.5 to 3 times the original values, and the smaller ratio to be 0.25 to 0.75 of the original values. A period change, i.e., a modified $\widetilde{\omega}<em i_="i," j="j">{n}$ in $s\left(T</em>\right)$, can also occur. We randomly sample whether the period becomes longer or shorter, with the longer ratio being 1.5 to 3 , and the shorter ratio being 0.25 to 0.75 .</p>
<p>Trend anomalies occur when there is a change point where trends differ before and after point $i$, with $1&lt;i&lt;N$. We set the change point between 0.2 to 0.8 of the time series and randomly select whether the trend is increasing or decreasing by 1.5 to 5 times the standard deviation of the base time series. A trend break occurs where the trend changes at $i$ and then reverts at $j$, with $1&lt;i&lt;j&lt;N$. We set $i$ in the 0.2 to 0.8 position of the time series and $j-i$ to be about 0.05 to 0.2 of the length of the time series. We randomly decide whether the trend break is increasing or decreasing, with the change being 1.5 to 5 times the standard deviation of the base time series.
Shape anomalies may occur as a pattern change, where the base pattern shifts starting at $i$, with $1&lt;i&lt;N$. We randomly select the start point in the 0.2 to 0.6 range of the original time series. A pattern break occurs where the base pattern changes at $i$ but returns to normal by $j$, with $1&lt;i&lt;j&lt;N$. We set the start point $i$ in the 0.2 to 0.6 range of the original time series, and the break length to be 0.2 to 0.4 of the original time series. Specifically, to generate shape anomalies, we use a different seasonality type from the original time series and reuse TTGenerator to create a new base time series with the targeted seasonality type. For example, if the original time series has sine wave seasonality, we generate a new time series with IFFT seasonality to insert as shape anomalies.
To generate anomalies in a time series, we first randomly select 1-3 types from the five anomaly categories: global point anomaly, local point anomaly, seasonality anomaly, shape anomaly, and trend anomaly. If a seasonality, shape, or trend anomaly is selected, we further specify the downstream anomaly type. For example, for a trend anomaly, we randomly select whether it is a shape change or a shape break.
Explanation Generation The templates used to generate descriptions for the base time series are shown in Figure 8. We concatenate the descriptions for the trend, seasonality, and noise components to form the overall description of the base time series. Note that if the time series lacks a trend, we omit any trend-related description. The templates used to generate explanations for the anomalies are shown in Figure 9 and Figure 10. We concatenate the descriptions of the anomalies in a time series to form the overall anomaly explanation for that time series.
Examples for the Generated Samples Figure 11 shows examples of the base time series generation results and their corresponding automatically generated explanations. Figure 12 illustrates examples of global and local point anomalies. Figure 13 presents examples of seasonality anomalies. Figure 14</p>
<h1>Context-aware Anomalies</h1>
<h2>Seasonality Anomaly</h2>
<h2>Amplitude Change</h2>
<ul>
<li>Larger Amplitude: We can observe the amplitude of the time series changes to larger values between indexes ${$ start index $}$ to ${$ end index $}$, where the values change to about ${$ ratio $}$ times about the original values.</li>
<li>Smaller Amplitude: We can observe the amplitude of the time series changes to smaller values between indexes ${$ start index $}$ to ${$ end index $}$, where the values change to about ${$ ratio $}$ of the original values.</li>
</ul>
<h2>Period Change</h2>
<ul>
<li>Longer Period: We can observe the seasonality period change between indexes ${$ start index $}$ and ${$ end index $}$, where the period changes to a longer period.</li>
<li>Shorter Period: We can observe the seasonality period change between indexes ${$ start index $}$ and ${$ end index $}$, where the period changes to a shorter period.</li>
</ul>
<h2>Trend Anomaly</h2>
<h2>Trend Change</h2>
<ul>
<li>If increase: We can observe a change point at index ${$ change point $}$ where the value increases by ${$ changed value $}$.</li>
<li>If decrease: We can observe a change point at index ${$ change point $}$ where the value decreases by ${$ changed value $}$.</li>
</ul>
<h2>Trend Break</h2>
<ul>
<li>If increase: There is a significant value increase since index ${$ start index $}$ and the values drop back to the original trend since index ${$ end index $}$.</li>
<li>If decrease: There is a significant value decrease since index ${$ start index $}$ and the values increase back to the original trend since index ${$ end index $}$.</li>
</ul>
<h2>Shape Anomaly</h2>
<p>Shape Change: There shows the base pattern of the time series change since the index {start index }, where the time series changed to ${$ changed time series base pattern description $}$.</p>
<p>Shape Break: There are base patterns changes between the index ${$ start index $}$ and the ${$ end index $}$, where during that time, we can observe the time series as ${$ changed time series base pattern description $}$.</p>
<p>Figure 10: Template for time series anomaly pattern explanation - context-aware anomalies.
displays examples of trend anomalies, and Figure 15 demonstrates examples of shape anomalies. Note that all explanations are automatically generated by our template and will be further refined by GPT-4.</p>
<p>Formalization to Instruction Dataset Specifically, a single data sample includes the time series values, labels for the anomalies, labels for the specific types of anomalies, an explanation for the base time series only, an explanation for the anomalies only, an explanation including both the base and anomaly explanations, and an explanation rewritten by the LLMs. We construct datasets with 100, 500, 1000 and 2000 samples, with time series lengths of 180, 360, and 720. Although in real applications the total length of the time series can be quite long, we do not consider this due to the context window limitations of the LLMs. During instruction fine-tuning, the prompt we feed into LLaMA3 is the concatenation of the ideal explanation with the instruction prompt and the requirements prompt as described in Appendix A.3.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 11: Examples for the base time series with automatically generated explanation. (a) Example for a time series with seasonality as IFFT and without trend. (b) Example for a time series with sine-wave like seasonality and linear increasing trend.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 12: Examples for the time series with point anomalies with automatically generated anomaly explanation.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 13: Examples for the time series with seasonality anomalies with automatically generated anomaly explanation.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 14: Examples for the time series with trend anomalies with automatically generated anomaly explanation.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 15: Examples for the time series with shape anomalies with automatically generated anomaly explanation.</p>
<h1>Global Point Anomaly - Correct in all instructions</h1>
<p>Yes, there is an anomaly in the time series. The anomaly occurs at index 11 ( 0 -based indexing). The average (mean) value of the series is approximately 1.55 with a standard deviation of about 0.92 . Typically, a point is considered anomalous if it lies more than 2 standard deviations from the mean. Here, the value at index 11 is 5 , which exceeds the mean by more than 2 standard deviations, hence it is considered anomalous.</p>
<h2>Local Point Anomaly</h2>
<p>Directly Use (Incorrect): No points in the series have a z-score greater than 3 or less than -3. Thus, based on this criterion, there are no anomalies in the data.</p>
<p>Chain-of-thought Instruction (Correct): Identify Anomalies: The pattern in the series typically increases from 1 to 5 and then decreases back to 1 . By visual inspection, one can observe a disruption in the expected decrease after the peak value (5). Indices of Anomaly Points: The expected pattern is disrupted at indices 9 and 10, where the values are [3, 3]. Reasoning for Anomalies: The values of 3 at indices 9 and 10 should be considered anomalies because they disrupt the expected descending sequence that should follow after reaching the peak value of 5 . In all other occurrences, after the value sequence reaches 5 , it decreases sequentially ( 5,4 , $3,2,1)$. However, in this instance, the sequence unexpectedly stabilizes at 3 for two consecutive points, thereby breaking the typical cyclical pattern.</p>
<h2>Seasonality Anomaly</h2>
<p>Directly Use (Incorrect): There are no anomalies in the time series as per the z-score threshold of 3.</p>
<p>Multi-modal Instruction (Correct): The numerical analysis confirms the visual observations. The anomalies in the time series are as follows: Index 17 (Value 5), Index 18 (Value 1), and Index 19 (Value 5). These points are considered anomalies because they show a significant and abrupt change from the established pattern of gradual increases and decreases.</p>
<h2>Trend Anomaly</h2>
<p>Directly Use (Incorrect): The mean of the time series is approximately 5.1, and the standard deviation is about 2.43. Using a threshold of 3 standard deviations from the mean to define anomalies, we did not detect any anomalous points in the series.</p>
<p>In-context Instruction (Correct): Presence of Anomaly Points: Yes, there are anomaly points in this time series. Indices of Anomaly Points: The anomaly points are located at indices 9,10 , and 11. Reasoning: The points at indices 9,10 , and $11([6,7,8])$ are considered anomalous because they mark a sudden and significant change from the previous cyclic pattern $[1,2,3,2,1,2,3,2,1]$. The magnitude of these points is much higher, indicating an unexpected shift in the time series' behavior.</p>
<p>Figure 16: More examples responses of GPT4 on the trial cases for other anomaly types.</p>
<p>Formalization to Evaluation Dataset We generated three datasets, each containing 100 samples, with time series lengths of 100, 200, and 400, respectively. Each dataset includes 20 samples for each of the following anomaly types: global point anomaly, local point anomaly, seasonality anomaly, trend anomaly, and shape anomaly. This distribution allows us to effectively evaluate model performance across different types of time series anomalies and various time series lengths. Additionally, the evaluation dataset lengths differ from those used to fine-tune LLaMA3 to prevent the model from learning any specific length-related patterns or tricks.
Difference from Lai et al. [2021] Although we have followed the definition of anomaly types from this work, TTGenerator considers more detailed types for the base pattern, including additional trend and seasonality types, as well as more detailed anomaly types. Most importantly, this work did not provide automatic textual descriptions for the time series, which are essential for constructing an instruction dataset.</p>
<h2>A. 6 LLM Settings</h2>
<p>For both models, we employ a rerun strategy: if the model fails to provide the required JSONformatted response, we automatically rerun the code until the response adheres to the specified format. If the model fails more than five trials, we return the default response as {anomaly: [], reason: ""].</p>
<h1>Global Point Anomaly</h1>
<p>Directly Use (Correct) Yes, there is at least one anomaly point in the time series. The index of the anomaly point is 11. Reasoning behind the anomaly: The value 5 at index 11 is considered an anomaly because it is significantly different from the rest of the values in the series. The majority of the values in the series are either 1 or 2 , with a few occurrences of 1 and 2 . The value 5 is significantly higher than the other values and does not follow the pattern of the series.
Multi-modal Instruction (Correct) Presence of anomaly points: Yes. Indices of anomaly points: 11 (index 11 corresponds to the value 5). Reasoning: The value 5 at index 11 is significantly higher than the rest of the values in the series, which are mostly in the range of 1 and 2 . This outlier value stands out as an anomaly due to its unusual magnitude and deviation from the overall pattern of the series.
Multi-modal Instruction (Incorrect) Yes, there are several anomaly points in this time series. The indices of the anomaly points are: Index 11 (value: 5): This value is significantly higher than the surrounding values, which are mostly in the range of 1-2. The sudden jump to 5 is unusual and may indicate an anomaly. Index 20 (value: 2): This value is higher than the surrounding values, which are mostly 1 . The sudden increase in value may indicate an anomaly.</p>
<h2>Local Point Anomaly</h2>
<p>Directly Use (Incorrect): Presence of anomaly points: Yes. Indices of anomaly points: 4, 12. Reasoning behind these points being considered anomalies: - Index 4: The value 5 has a Z-score of 1.29 , which is greater than 2 standard deviations away from the mean. This indicates that this data point is an anomaly. - Index 12: The value 5 has a Z-score of 1.29 , which is greater than 2 standard deviations away from the mean. This indicates that this data point is an anomaly.
In-context Instruction (Incorrect) Yes, there are several anomaly points in this time series. The indices of the anomaly points are: Index 9 (value: 3): This value is different from the surrounding values, which are mostly increasing. The sudden decrease in value may indicate an anomaly. Index 10 (value: 3): This value is the same as the previous value, which is unusual in a time series that is generally increasing. Index 17 (value: 2): This value is significantly lower than the surrounding values, which are mostly increasing. The sudden decrease in value may indicate an anomaly.</p>
<h2>Seasonality Anomaly</h2>
<p>Directly Use (Incorrect): Yes, there are anomalies in the time series. Indices of anomaly points: Index 0: The value 1 is an anomaly because it's significantly lower than the previous value ( 0 ) and the overall trend of the series. Index 14: The value 5 is an anomaly because it's significantly higher than the previous value (1) and the overall trend of the series. Index 17: The value 5 is an anomaly because it's significantly higher than the previous value (1) and the overall trend of the series.
Chain-of-thought Instruction (Incorrect) Yes, there are anomaly points in this time series. The anomaly points are at indices [12] and [17]. Reasoning behind these points being considered anomalies: The values at indices [12] and [17] are 5, which is significantly higher than the previous and subsequent values. The time series has a cyclical pattern with decreasing values, and the sudden increase at indices [12] and [17] breaks this pattern, making them anomalies.</p>
<h2>Trend Anomaly</h2>
<p>Directly Use (Partially Correct): Yes, there are anomalies in the time series. The indices of the anomaly points are $9,10,11$, and 12. Reasoning behind the anomaly points being considered anomalies: The points at indices 9,10 , and 11 (values 6,7 , and 8 ) are significantly higher than the rest of the time series, which consists mainly of values ranging from 1 to 3 . These higher values are likely anomalies because they deviate significantly from the overall pattern of the time series.
Multi-modal Instruction (Correct): Presence of anomaly points: Yes Indices of anomaly points: 9, 10, 11 Reasoning behind these points being considered anomalies: The time series can be visualized as a wave-like pattern with oscillations between 1, 2, and 3. The anomaly points at indices 9,10 , and 11 appear as a sudden spike in the wave pattern, making them stand out as anomalies.</p>
<p>Figure 17: More example responses of LLaMA3 on the trial cases for other anomaly types.</p>
<p>GPT-4 Settings The GPT-4 version we used is 'GPT-4-0125-preview ${ }^{10}$. Specifically, we employ LangChain ${ }^{11}$ to facilitate the integration of prompts with the OpenAI API and to parse the output into JSON format for easier evaluation. We use the default parameters in generating the responses.</p>
<p>LLaMA3 Settings To obtain the inference results for the original LLaMA3, we utilized Groq's API services ${ }^{12}$. All results were generated using LLaMA3-8B due to computational resource limitations. Our fine-tuning is based on Meta-Llama-3-8B-Instruct ${ }^{13}$. We used parameter-efficient fine-tuning approaches, specifically LoRA [Hu et al., 2021], with Hugging Face's PEFT packages ${ }^{14}$. The settings for LoRA are as follows: we fine-tuned all linear layers in the transformers, with the LoRA rank set to 16 , LoRA alpha set to 64 , and a dropout rate of 0.1 . The training arguments are: gradient accumulation for 4 steps, using the paged_adamw_8bit optimizer, a learning rate of $2 \mathrm{e}-4$ with a</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ https://openai.com/pricing
${ }^{11}$ https://www.langchain.com/
${ }^{12}$ https://groq.com/
${ }^{13}$ https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
${ }^{14}$ https://huggingface.co/docs/peft/index&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3142 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3142</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3142</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-932b6353204e56f20917edadda2fa636ace21090</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/932b6353204e56f20917edadda2fa636ace21090" target="_blank">Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is shown that when concatenating intermediate supervision to the input and training a sequence-to-sequence model on this modified input, unlearnable composite problems can become learnable.</p>
                <p><strong>Paper Abstract:</strong> The field of Natural Language Processing has experienced a dramatic leap in capabilities with the recent introduction of huge Language Models. Despite this success, natural language problems that involve several compounded steps are still practically unlearnable, even by the largest LMs. This complies with experimental failures for end-to-end learning of composite problems that were demonstrated in a variety of domains. An effective mitigation is to introduce intermediate supervision for solving sub-tasks of the compounded problem. Recently, several works have demonstrated high gains by taking a straightforward approach for incorporating intermediate supervision in compounded natural language problems: the sequence-to-sequence LM is fed with an augmented input, in which the decomposed tasks' labels are simply concatenated to the original input. In this paper, we prove a positive learning result that motivates these recent efforts. We show that when concatenating intermediate supervision to the input and training a sequence-to-sequence model on this modified input, unlearnable composite problems can become learnable. We show that this is true for any family of tasks which on the one hand, are unlearnable, and on the other hand, can be decomposed into a polynomial number of simple sub-tasks, each of which depends only on O(1) previous sub-task results. Beyond motivating contemporary empirical efforts for incorporating intermediate supervision in sequence-to-sequence language models, our positive theoretical result is the first of its kind in the landscape of results on the benefits of intermediate supervision for neural-network learning: Until now, all theoretical results on the subject are negative, i.e., show cases where learning is impossible without intermediate supervision, while our result is positive, showing that learning is facilitated in the presence of intermediate supervision.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3142.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3142.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Concatenated intermediate supervision (teacher forcing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-sequence concatenation of intermediate sub-task labels with teacher-forcing during training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention in which intermediate sub-task labels (single-hop results) are concatenated to the model input and the model is trained autoregressively with teacher forcing on those labels; at test time those labels are autoregressively predicted. This paper proves this enables efficient learning of compositional tasks that are otherwise unlearnable end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>sequence-to-sequence language models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive sequence-to-sequence models (analyzed theoretically with Elman RNN; evaluated empirically with Transformer/BERT-base sized models) that receive input x concatenated with a sequence S of intermediate labels forming z = Concat{x; S}; trained with teacher-forcing on S and final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>multi-hop/compositional tasks (bit-subset parity; cited: math word problems, multi-digit addition examples in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Provides intermediate ground-truth results that decompose the global computation into single-hop steps, increasing gradient signal and enabling the model to learn local computations and compose them; theoretically reduces complexity of learned hypothesis (expressing each sub-task as low-degree polynomial of O(1) inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Formal theorems (Theorem 2, Lemmas 1–3) showing polynomial sample/gradient complexity when intermediate labels belong to the defined hypothesis class; empirical Transformer experiments where concatenation enables learning of bit-subset parity tasks that fail without it.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct counter-evidence provided in this paper; limitations discussed include unknown minimal amount of supervision required (open question whether O(1) sub-tasks suffice).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>concatenating intermediate sub-task labels to input + teacher forcing during training; at test time labels are predicted autoregressively</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Transforms unlearnable end-to-end tasks into provably learnable ones with polynomial gradient steps and samples; empirically enables Transformer models to learn larger parity instances that fail without supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theoretical: polynomial number of gradient updates/sample complexity for tasks in hypothesis class; Empirical: BERT-base transformer learned parity tasks with supervision rapidly (learning defined as validation >60%, then typically grokking to >95%); without supervision, 32-bit parity showed no learning after >2M steps. (No per-bit accuracy table in main text.)</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>If intermediate supervision is removed, end-to-end learning fails (high zero-one loss close to random); error can accumulate at test time but Lemma 1 bounds test error by sum of per-step training errors (union bound) causing up-to-(T-d) factor increase.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors contrast with symbolic algorithms/complexity-theoretic hardness: with decomposition supervised, models can learn any P-time function; without it, there exist functions (e.g., parities) that are effectively intractable for polynomial-time learners, so decomposition makes model behavior closer to algorithmic/structured computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3142.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3142.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elman RNN analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theoretically analyzed Elman recurrent neural network with ReLU activations and frozen input/output embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simplified sequence-to-sequence RNN model (Elman RNN with ReLU) used for formal proofs: A, B, M0 (input mapping, output weights, init hidden) are fixed at initialization and only recurrent matrix W is trained; allows provable convergence analysis when intermediate supervision is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Elman RNN (ReLU)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Classical Elman recurrent network: h_t = ReLU(W h_{t-1} + A e_{z_t}), output s_t = B^T h_t; A, B, M0 initialized from N(0,2/m) or N(0,1/m) and kept frozen; only W is updated by SGD/GD (analyzed in Algorithm 1/2/3).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>theoretical analysis applied to compositional tasks (bit-subset parity) rather than decimal arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Shows that when intermediate labels are provided, each output can be represented as sign(ψ_t(⟨w^(t), selected input bits⟩)) with ψ_t low-degree polynomial and small input dependency N=O(1); training only recurrent W suffices to learn these local computations and compose them.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Formal lemmas/theorems (Theorem 2 and related lemmas) prove convergence/generalization bounds for RNNs trained with teacher-forced intermediate supervision; uses NTK/overparameterization style arguments (building on Wang et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The RNN analysis relies on strong assumptions (overparameterization, frozen A/B) and a particular hypothesis class whose complexity φ is exponential in degree and N; does not claim RNNs will learn arbitrary decompositions without these conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training only recurrent matrix W (freeze A, B, M0); teacher-forcing intermediate labels</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enables provable polynomial-time learnability for tasks whose per-step outputs depend on O(1) prior bits with low-degree polynomial activations; bounds test error via union bound on per-step errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theorem gives sample complexity/iteration bounds: need n = Õ(((φ + log(1/δ))/ε)^2) iterations and width m = poly(n, δ^{-1}, T) to get average zero-one loss < ε with high probability; concrete numeric rates depend on φ(T,ψ,N).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>If per-step polynomial degree or input dependency N grow with d, φ becomes exponential and theoretical guarantees vanish; end-to-end (no intermediate supervision) requires exponential updates for parity tasks (referenced negative results).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>The theoretical construction maps boolean circuits (polynomial-size) into sequence of intermediate labels, making the RNN capable of simulating P-time computations when supervised — i.e., matching symbolic algorithmic capability under supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3142.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3142.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bit-subset parity task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bit-subset parity (multi-hop parity over unknown subset of input bits)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A challenging compositional problem: given d-bit input and a hidden subset of d/2 indices, predict parity (odd/even) of bits in that subset; solved via binary-tree decomposition into pairwise parity sub-tasks and concatenating those intermediate results to input for training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>task (no single fixed LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Input: d-bit string x; target y = (-1)^{sum_{j in subset} x_j}. Sub-task decomposition: compute pairwise parities bottom-up in binary tree, convert tree to sequence S (inverse-BFS) and concatenate S after x for the sequence-to-sequence model to consume.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>bitwise parity computation (binary boolean task, multi-hop composition), not decimal arithmetic but representative of compositional arithmetic-like structure</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>When trained with concatenated intermediate parity labels, model learns local degree-2 polynomial functions for pairwise parity that compose; without supervision, end-to-end gradient methods fail (exponential difficulty).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Lemma 2 constructs an explicit degree-2 polynomial ψ and weight vector w that maps two input bits to their parity; Theorem 2 + Lemma 2 yields provable polynomial learnability with supervision; empirical Transformer experiments support a large empirical gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Without intermediate labels, theoretical negative results (Shalev-Shwartz et al. 2017) show end-to-end gradient descent fails to learn parity efficiently; empirical Transformer fails to learn 32-bit parity without supervision even after >2M steps.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>concatenate intermediate parity computations (tree nodes) to input; teacher forcing during training</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Exponential gap: with O(d) intermediate labels model learns in polynomial time; without them learning requires exponential gradient updates (theoretical and empirical evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theoretical: polynomial number of gradient updates/sample complexity when supervised. Empirical: Transformer (BERT-base sized) learned tasks with supervision quickly (learning defined as validation >60% then reaching >95%), while without supervision 32-bit parity showed no learning after >2M steps; specific step counts shown in Figure 3 (not numerically tabulated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>End-to-end training yields near-random final accuracy for most parities unless exponential computation/resources used; stochastic vs. full-GD distinctions noted in literature (some SGD variants or special inits can learn parities).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Parity is trivially solved by symbolic bitwise XOR algorithms; decomposition with supervision makes LMs emulate this algorithmic computation, whereas end-to-end LMs fail to discover such algorithmic structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3142.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3142.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-base Transformer experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical evaluation with a BERT-base sized Transformer on bit-subset parity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Practical experiments training a BERT-base sized Transformer on bit-subset parity tasks (input sizes 8–256 bits) with and without concatenated intermediate supervision; demonstrates empirical exponential gap consistent with theory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base sized Transformer (encoder-style architecture adapted for seq2seq experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT-base sized Transformer used as sequence model (details in appendix G); trained for typically 100k iterations (exceptions: some longer runs up to 300k or 2M for hard settings) on parity datasets of varying bit-lengths, with intermediate-label concatenation in supervised runs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>bit-subset parity (binary/compositional boolean task)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Empirical mechanism: concatenated intermediate scratchpads (intermediate calculations) act as supervision that guides the Transformer to learn local parity computations and compose them; without scratchpads Transformer struggles to obtain gradient signal to discover compositional algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Figure 3 demonstrates sharp divergence: models with intermediate supervision learn quickly across bit sizes; models without supervision fail beyond small sizes (e.g., no learning at 32 bits after >2M steps). Training dynamics show grokking: once validation surpasses ~60% it quickly goes to >95%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct contradictory empirical result in this paper; observed that even a strong Transformer (BERT-base) fails without supervision on modest-size parity tasks, indicating limitations of architecture without decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>concatenated intermediate labels (scratchpads); same model architecture otherwise; training hyperparameters varied (iterations increased for hard no-supervision runs)</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Dramatic improvement: with supervision, tasks up to tested sizes are learned in practical budgets; without, learning stalls or requires massively more steps (empirical evidence of practical infeasibility).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Learning criterion: validation accuracy >60% (then typically >95% after grokking). Report: 32-bit parity without supervision showed no learning after >2M steps; with supervision models reached learning much faster (exact step counts in Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Without intermediate supervision, Transformer shows no meaningful improvement on moderate parity sizes even with long training; grokking behavior indicates sudden generalization after a point rather than smooth improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Empirical behavior is far from symbolic/XOR algorithm performance: symbolic algorithm would solve parity trivially, whereas Transformer requires explicit intermediate supervision to emulate compositional computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3142.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3142.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Related prior empirical results (Nye et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Show your work: Scratchpads for intermediate computation with language models (Nye et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited empirical work showing that concatenating intermediate calculations (scratchpads) to model input dramatically improves performance on arithmetic tasks (reported >99% accuracy for 8-digit addition with scratchpads vs ~35% without).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show your work: Scratchpads for intermediate computation with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various language models evaluated in Nye et al. (2022)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Nye et al. train/modify language models to produce intermediate scratchpad computations for math tasks; details are in that paper (cited as motivating empirical evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>multi-digit addition (8-digit addition example) and math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Using scratchpads (concatenated intermediate calculations) provides explicit intermediate computation that language models can be trained to produce and thereby get the final answer correctly, i.e., models use explicit intermediate algorithmic steps when provided/supervised.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Reported empirical numbers: >99% accuracy for 8-digit addition when concatenating intermediate calculations vs ~35% without; cited in this paper as motivating evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Not detailed in this paper; mentioned as strong supportive empirical evidence for concatenated supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>concatenated scratchpads / intermediate calculations (same family of interventions studied here)</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Substantial accuracy improvement on arithmetic tasks (e.g., 8-digit addition), demonstrating intervention practical effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>>99% accuracy (with scratchpads) vs ~35% (without) on 8-digit addition as reported by Nye et al. (2022) and cited here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Without scratchpads, substantial drop in performance (example: ~35% accuracy), indicating inability of plain end-to-end models to reliably perform multi-digit addition.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Scratchpads bring model behavior closer to symbolic step-by-step calculation (humans often use multi-step scratch work); cited as analogous to explicitly showing intermediate computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Show your work: Scratchpads for intermediate computation with language models. <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>Program induction by rationale generation: Learning to solve and explain algebraic word problems <em>(Rating: 1)</em></li>
                <li>Measuring and improving BERT's mathematical abilities by predicting the order of reasoning <em>(Rating: 1)</em></li>
                <li>Failures of gradient-based deep learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3142",
    "paper_id": "paper-932b6353204e56f20917edadda2fa636ace21090",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Concatenated intermediate supervision (teacher forcing)",
            "name_full": "Sequence-to-sequence concatenation of intermediate sub-task labels with teacher-forcing during training",
            "brief_description": "An intervention in which intermediate sub-task labels (single-hop results) are concatenated to the model input and the model is trained autoregressively with teacher forcing on those labels; at test time those labels are autoregressively predicted. This paper proves this enables efficient learning of compositional tasks that are otherwise unlearnable end-to-end.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "sequence-to-sequence language models (general)",
            "model_description": "Autoregressive sequence-to-sequence models (analyzed theoretically with Elman RNN; evaluated empirically with Transformer/BERT-base sized models) that receive input x concatenated with a sequence S of intermediate labels forming z = Concat{x; S}; trained with teacher-forcing on S and final answer.",
            "arithmetic_task_type": "multi-hop/compositional tasks (bit-subset parity; cited: math word problems, multi-digit addition examples in related work)",
            "reported_mechanism": "Provides intermediate ground-truth results that decompose the global computation into single-hop steps, increasing gradient signal and enabling the model to learn local computations and compose them; theoretically reduces complexity of learned hypothesis (expressing each sub-task as low-degree polynomial of O(1) inputs).",
            "evidence_for_mechanism": "Formal theorems (Theorem 2, Lemmas 1–3) showing polynomial sample/gradient complexity when intermediate labels belong to the defined hypothesis class; empirical Transformer experiments where concatenation enables learning of bit-subset parity tasks that fail without it.",
            "evidence_against_mechanism": "No direct counter-evidence provided in this paper; limitations discussed include unknown minimal amount of supervision required (open question whether O(1) sub-tasks suffice).",
            "intervention_type": "concatenating intermediate sub-task labels to input + teacher forcing during training; at test time labels are predicted autoregressively",
            "effect_of_intervention": "Transforms unlearnable end-to-end tasks into provably learnable ones with polynomial gradient steps and samples; empirically enables Transformer models to learn larger parity instances that fail without supervision.",
            "performance_metrics": "Theoretical: polynomial number of gradient updates/sample complexity for tasks in hypothesis class; Empirical: BERT-base transformer learned parity tasks with supervision rapidly (learning defined as validation &gt;60%, then typically grokking to &gt;95%); without supervision, 32-bit parity showed no learning after &gt;2M steps. (No per-bit accuracy table in main text.)",
            "notable_failure_modes": "If intermediate supervision is removed, end-to-end learning fails (high zero-one loss close to random); error can accumulate at test time but Lemma 1 bounds test error by sum of per-step training errors (union bound) causing up-to-(T-d) factor increase.",
            "comparison_to_humans_or_symbolic": "Authors contrast with symbolic algorithms/complexity-theoretic hardness: with decomposition supervised, models can learn any P-time function; without it, there exist functions (e.g., parities) that are effectively intractable for polynomial-time learners, so decomposition makes model behavior closer to algorithmic/structured computation.",
            "uuid": "e3142.0",
            "source_info": {
                "paper_title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Elman RNN analysis",
            "name_full": "Theoretically analyzed Elman recurrent neural network with ReLU activations and frozen input/output embeddings",
            "brief_description": "A simplified sequence-to-sequence RNN model (Elman RNN with ReLU) used for formal proofs: A, B, M0 (input mapping, output weights, init hidden) are fixed at initialization and only recurrent matrix W is trained; allows provable convergence analysis when intermediate supervision is provided.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Elman RNN (ReLU)",
            "model_description": "Classical Elman recurrent network: h_t = ReLU(W h_{t-1} + A e_{z_t}), output s_t = B^T h_t; A, B, M0 initialized from N(0,2/m) or N(0,1/m) and kept frozen; only W is updated by SGD/GD (analyzed in Algorithm 1/2/3).",
            "arithmetic_task_type": "theoretical analysis applied to compositional tasks (bit-subset parity) rather than decimal arithmetic",
            "reported_mechanism": "Shows that when intermediate labels are provided, each output can be represented as sign(ψ_t(⟨w^(t), selected input bits⟩)) with ψ_t low-degree polynomial and small input dependency N=O(1); training only recurrent W suffices to learn these local computations and compose them.",
            "evidence_for_mechanism": "Formal lemmas/theorems (Theorem 2 and related lemmas) prove convergence/generalization bounds for RNNs trained with teacher-forced intermediate supervision; uses NTK/overparameterization style arguments (building on Wang et al., 2021).",
            "evidence_against_mechanism": "The RNN analysis relies on strong assumptions (overparameterization, frozen A/B) and a particular hypothesis class whose complexity φ is exponential in degree and N; does not claim RNNs will learn arbitrary decompositions without these conditions.",
            "intervention_type": "training only recurrent matrix W (freeze A, B, M0); teacher-forcing intermediate labels",
            "effect_of_intervention": "Enables provable polynomial-time learnability for tasks whose per-step outputs depend on O(1) prior bits with low-degree polynomial activations; bounds test error via union bound on per-step errors.",
            "performance_metrics": "Theorem gives sample complexity/iteration bounds: need n = Õ(((φ + log(1/δ))/ε)^2) iterations and width m = poly(n, δ^{-1}, T) to get average zero-one loss &lt; ε with high probability; concrete numeric rates depend on φ(T,ψ,N).",
            "notable_failure_modes": "If per-step polynomial degree or input dependency N grow with d, φ becomes exponential and theoretical guarantees vanish; end-to-end (no intermediate supervision) requires exponential updates for parity tasks (referenced negative results).",
            "comparison_to_humans_or_symbolic": "The theoretical construction maps boolean circuits (polynomial-size) into sequence of intermediate labels, making the RNN capable of simulating P-time computations when supervised — i.e., matching symbolic algorithmic capability under supervision.",
            "uuid": "e3142.1",
            "source_info": {
                "paper_title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Bit-subset parity task",
            "name_full": "Bit-subset parity (multi-hop parity over unknown subset of input bits)",
            "brief_description": "A challenging compositional problem: given d-bit input and a hidden subset of d/2 indices, predict parity (odd/even) of bits in that subset; solved via binary-tree decomposition into pairwise parity sub-tasks and concatenating those intermediate results to input for training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "task (no single fixed LM)",
            "model_description": "Input: d-bit string x; target y = (-1)^{sum_{j in subset} x_j}. Sub-task decomposition: compute pairwise parities bottom-up in binary tree, convert tree to sequence S (inverse-BFS) and concatenate S after x for the sequence-to-sequence model to consume.",
            "arithmetic_task_type": "bitwise parity computation (binary boolean task, multi-hop composition), not decimal arithmetic but representative of compositional arithmetic-like structure",
            "reported_mechanism": "When trained with concatenated intermediate parity labels, model learns local degree-2 polynomial functions for pairwise parity that compose; without supervision, end-to-end gradient methods fail (exponential difficulty).",
            "evidence_for_mechanism": "Lemma 2 constructs an explicit degree-2 polynomial ψ and weight vector w that maps two input bits to their parity; Theorem 2 + Lemma 2 yields provable polynomial learnability with supervision; empirical Transformer experiments support a large empirical gap.",
            "evidence_against_mechanism": "Without intermediate labels, theoretical negative results (Shalev-Shwartz et al. 2017) show end-to-end gradient descent fails to learn parity efficiently; empirical Transformer fails to learn 32-bit parity without supervision even after &gt;2M steps.",
            "intervention_type": "concatenate intermediate parity computations (tree nodes) to input; teacher forcing during training",
            "effect_of_intervention": "Exponential gap: with O(d) intermediate labels model learns in polynomial time; without them learning requires exponential gradient updates (theoretical and empirical evidence).",
            "performance_metrics": "Theoretical: polynomial number of gradient updates/sample complexity when supervised. Empirical: Transformer (BERT-base sized) learned tasks with supervision quickly (learning defined as validation &gt;60% then reaching &gt;95%), while without supervision 32-bit parity showed no learning after &gt;2M steps; specific step counts shown in Figure 3 (not numerically tabulated in main text).",
            "notable_failure_modes": "End-to-end training yields near-random final accuracy for most parities unless exponential computation/resources used; stochastic vs. full-GD distinctions noted in literature (some SGD variants or special inits can learn parities).",
            "comparison_to_humans_or_symbolic": "Parity is trivially solved by symbolic bitwise XOR algorithms; decomposition with supervision makes LMs emulate this algorithmic computation, whereas end-to-end LMs fail to discover such algorithmic structure.",
            "uuid": "e3142.2",
            "source_info": {
                "paper_title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "BERT-base Transformer experiments",
            "name_full": "Empirical evaluation with a BERT-base sized Transformer on bit-subset parity",
            "brief_description": "Practical experiments training a BERT-base sized Transformer on bit-subset parity tasks (input sizes 8–256 bits) with and without concatenated intermediate supervision; demonstrates empirical exponential gap consistent with theory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base sized Transformer (encoder-style architecture adapted for seq2seq experiments)",
            "model_description": "BERT-base sized Transformer used as sequence model (details in appendix G); trained for typically 100k iterations (exceptions: some longer runs up to 300k or 2M for hard settings) on parity datasets of varying bit-lengths, with intermediate-label concatenation in supervised runs.",
            "arithmetic_task_type": "bit-subset parity (binary/compositional boolean task)",
            "reported_mechanism": "Empirical mechanism: concatenated intermediate scratchpads (intermediate calculations) act as supervision that guides the Transformer to learn local parity computations and compose them; without scratchpads Transformer struggles to obtain gradient signal to discover compositional algorithm.",
            "evidence_for_mechanism": "Figure 3 demonstrates sharp divergence: models with intermediate supervision learn quickly across bit sizes; models without supervision fail beyond small sizes (e.g., no learning at 32 bits after &gt;2M steps). Training dynamics show grokking: once validation surpasses ~60% it quickly goes to &gt;95%.",
            "evidence_against_mechanism": "No direct contradictory empirical result in this paper; observed that even a strong Transformer (BERT-base) fails without supervision on modest-size parity tasks, indicating limitations of architecture without decomposition.",
            "intervention_type": "concatenated intermediate labels (scratchpads); same model architecture otherwise; training hyperparameters varied (iterations increased for hard no-supervision runs)",
            "effect_of_intervention": "Dramatic improvement: with supervision, tasks up to tested sizes are learned in practical budgets; without, learning stalls or requires massively more steps (empirical evidence of practical infeasibility).",
            "performance_metrics": "Learning criterion: validation accuracy &gt;60% (then typically &gt;95% after grokking). Report: 32-bit parity without supervision showed no learning after &gt;2M steps; with supervision models reached learning much faster (exact step counts in Figure 3).",
            "notable_failure_modes": "Without intermediate supervision, Transformer shows no meaningful improvement on moderate parity sizes even with long training; grokking behavior indicates sudden generalization after a point rather than smooth improvement.",
            "comparison_to_humans_or_symbolic": "Empirical behavior is far from symbolic/XOR algorithm performance: symbolic algorithm would solve parity trivially, whereas Transformer requires explicit intermediate supervision to emulate compositional computation.",
            "uuid": "e3142.3",
            "source_info": {
                "paper_title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Related prior empirical results (Nye et al. 2022)",
            "name_full": "Show your work: Scratchpads for intermediate computation with language models (Nye et al., 2022)",
            "brief_description": "Cited empirical work showing that concatenating intermediate calculations (scratchpads) to model input dramatically improves performance on arithmetic tasks (reported &gt;99% accuracy for 8-digit addition with scratchpads vs ~35% without).",
            "citation_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "mention_or_use": "mention",
            "model_name": "various language models evaluated in Nye et al. (2022)",
            "model_description": "Nye et al. train/modify language models to produce intermediate scratchpad computations for math tasks; details are in that paper (cited as motivating empirical evidence).",
            "arithmetic_task_type": "multi-digit addition (8-digit addition example) and math word problems",
            "reported_mechanism": "Using scratchpads (concatenated intermediate calculations) provides explicit intermediate computation that language models can be trained to produce and thereby get the final answer correctly, i.e., models use explicit intermediate algorithmic steps when provided/supervised.",
            "evidence_for_mechanism": "Reported empirical numbers: &gt;99% accuracy for 8-digit addition when concatenating intermediate calculations vs ~35% without; cited in this paper as motivating evidence.",
            "evidence_against_mechanism": "Not detailed in this paper; mentioned as strong supportive empirical evidence for concatenated supervision.",
            "intervention_type": "concatenated scratchpads / intermediate calculations (same family of interventions studied here)",
            "effect_of_intervention": "Substantial accuracy improvement on arithmetic tasks (e.g., 8-digit addition), demonstrating intervention practical effectiveness.",
            "performance_metrics": "&gt;99% accuracy (with scratchpads) vs ~35% (without) on 8-digit addition as reported by Nye et al. (2022) and cited here.",
            "notable_failure_modes": "Without scratchpads, substantial drop in performance (example: ~35% accuracy), indicating inability of plain end-to-end models to reliably perform multi-digit addition.",
            "comparison_to_humans_or_symbolic": "Scratchpads bring model behavior closer to symbolic step-by-step calculation (humans often use multi-step scratch work); cited as analogous to explicitly showing intermediate computation.",
            "uuid": "e3142.4",
            "source_info": {
                "paper_title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1
        },
        {
            "paper_title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
            "rating": 1
        },
        {
            "paper_title": "Measuring and improving BERT's mathematical abilities by predicting the order of reasoning",
            "rating": 1
        },
        {
            "paper_title": "Failures of gradient-based deep learning",
            "rating": 2
        }
    ],
    "cost": 0.017333750000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks</h1>
<p>Noam Wies, Yoav Levine \&amp; Amnon Shashua<br>The Hebrew University of Jerusalem<br>{noam.wies, yoav.levine, shashua}@cs.huji.ac.il</p>
<h4>Abstract</h4>
<p>The field of Natural Language Processing (NLP) has experienced a dramatic leap in capabilities with the recent introduction of huge Language Models (LMs). Despite this success, natural language problems that involve several compounded steps are still practically unlearnable, even by the largest LMs. This complies with experimental failures for end-to-end learning of composite problems that were demonstrated in a variety of domains. An effective mitigation is to introduce intermediate supervision for solving sub-tasks of the compounded problem. Recently, several works have demonstrated high gains by taking a straightforward approach for incorporating intermediate supervision in compounded natural language problems: the sequence-to-sequence LM is fed with an augmented input, in which the decomposed tasks' labels are simply concatenated to the original input (see figure 1). In this paper, we prove a positive learning result that motivates these recent efforts. We show that when concatenating intermediate supervision to the input and training a sequence-to-sequence model on this modified input, unlearnable composite problems can become learnable. We show that this is true for any family of tasks which on the one hand, are unlearnable, and on the other hand, can be decomposed into a polynomial number of simple sub-tasks, each of which depends only on $O(1)$ previous sub-task results. Beyond motivating contemporary empirical efforts for incorporating intermediate supervision in sequence-to-sequence language models, our positive theoretical result is the first of its kind in the landscape of results on the benefits of intermediate supervision for neural-network learning: Until now, all theoretical results on the subject are negative, i.e., show cases where learning is impossible without intermediate supervision, while our result is positive, showing that learning is facilitated in the presence of intermediate supervision.</p>
<h2>1 INTRODUCTION</h2>
<p>Large-scale language models such as BERT (Devlin et al., 2019), T5 (Raffel et al., 2020), and GPT3 (Brown et al., 2020) have recently pushed the envelope in many NLP tasks. Nevertheless, there are some problem-families that even the largest models do not seem to be capable of solving. One such family is that of "multi-hop" reasoning problems (see, e.g., Geva et al. (2021); Kalyan et al. (2021); Press et al. (2022)) that require compounding operations in order to produce an answer. For example, Gopher (Rae et al., 2021), one of the largest available language models, achieves $61 \%$ accuracy in the StrategyQA benchmark (Geva et al., 2021) that requires implicit decomposition into reasoning steps, while human level performance is around $87 \%$ accuracy.</p>
<p>The limitations of learning compounded tasks with neural networks in an end-to-end manner have been observed in a variety of non-linguistic domains. A leading experimental approach for addressing these is to first explicitly break the compounded operations into more basic "single-hop" operations and then combine the results. Gülçehre \&amp; Bengio (2016), one of the earliest works on this subject, propose that supervision for the single-hop intermediate steps is crucial for avoiding bad local minima in the optimization of neural networks. Afterward, Glasmachers (2017) demonstrated that gradient-based end-to-end multi-hop learning is inefficient for solving complex problems that are easily solved by a divide-and-conquer strategy. Beyond position papers, specific examples were</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustrative example of the prominent method for introducing sub-task decomposition and intermediate supervision for math word problems (Ling et al., 2017; Cobbe et al., 2021). Intermediate sub-tasks and their labels are concatenated to the original task's input to form a new input sequence. At training time, the likelihood of the entire sequence following the original input is maximized conditioned on the input, and at test time only the original input is given to the model.
shown, e.g., Chang et al. (2020) showed that SATNet (Wang et al., 2019) could not solve visual Sudoku without using intermediate labels to identify individual Sudoku digit images.</p>
<p>Similar limitations were observed in language related compounded tasks, including commonsense reasoning (Liu et al., 2022; Wei et al., 2022; Zelikman et al., 2022), math word problems (Piękos et al., 2021; Wei et al., 2022), and programs execution (Nye et al., 2022). The go-to architectures in this domain are powerful language models, which are trained as sequence-to-sequence models over text. In this setting, a particular form of introducing intermediate supervision for compounded tasks has emerged: intermediate sub-tasks and their labels are concatenated to the original task's input to form a new input sequence, on which the sequence-to-sequence LM is trained. This approach has recently been widely adopted, e.g., by Rajani et al. (2019); Cobbe et al. (2021); Piękos et al. (2021); Recchia (2021); Nye et al. (2022); Wei et al. (2022); Zelikman et al. (2022). Figure 1 illustrates this approach for math problems, as done in Ling et al. (2017); Cobbe et al. (2021). These works show that training sequence-to-sequence models with concatenated sub-task decomposition supervision significantly improves the results when compared to training the same model without the intermediate supervision. For example, Nye et al. (2022) show $&gt;99 \%$ accuracy for 8 digits addition when concatenating intermediate calculations to the input, while the vanilla accuracy without intermediate supervision is around $\sim 35 \%$.
While such decomposition based approaches are intuitive, we are not aware of theoretical results that motivate and formulate their benefits for learning composite problems with neural-networks. In this paper, we provide positive theoretical results in this domain, which are in fact the first of their kind (see related work in section 2). We show our results for sequential models, integrating the intermediate supervision in a manner that mimics the above cited successful empirical approaches in the language domain. In this formulation, a learner learns to predict a sequence composed of the task inputs $\mathbf{x}$, followed by the single-hop reasoning steps referred to as the evidence, and finally, the final answer $y$. We extend provable guarantees for the convergence of overparameterized recurrent neural networks (Wang et al., 2021) and prove that with intermediate sub-task supervision, even a simple sequence-to-sequence model provably learns any task that obeys an efficient decomposition into simpler subtasks that depend only on a small fraction of the input. Importantly, both the sample complexity and the required number of gradient updates are polynomial. In contrast, we rely on existing works (Valiant, 1984; Goldreich et al., 1986; Daniely \&amp; Shalev-Shwartz, 2016) to show that in the absence of intermediate supervision, there exist efficiently decomposable tasks that are unlearnable with polynomial time learning algorithms.</p>
<p>Our results apply to a broad family of tasks. As a first exemplifying step, we show a positive result for learning bit subset parity, a setting that is notoriously not amenable to gradient-based algorithms in an efficient way without intermediate supervision (Kearns, 1998; Shalev-Shwartz et al., 2017; Abbe \&amp; Sandon, 2020; Abbe et al., 2021). In this setting, the family of target functions consists of parities over subsets of unknown input bits. Specifically, the input is $d$ bits and the task is to predict whether the number of 1 's in certain unknown subset of $\forall / 2$ bits is odd or even. The corresponding sub-tasks we consider are the parities of subsets of the unknown input subset. We prove a theorem guaranteeing that, when intermediate supervision is available, efficient neural network learning is</p>
<p>made possible. As a result, we show an exponential gap between the end-to-end and decompositionbased neural network learnability of the bit subset parity problem.</p>
<p>Next, we generalize the above result, and show that when sufficient intermediate supervision is available, any family of functions with a polynomial time complexity, i.e., functions that belong to the $\mathbf{P}$ time complexity class, are efficiently learnable by neural networks. Accordingly, based on either standard cryptographic assumptions (Valiant, 1984; Goldreich et al., 1986) or computational complexity hardness assumptions (Daniely \&amp; Shalev-Shwartz, 2016) we prove that there exist tasks that, on the one hand, cannot be learned by any polynomial time algorithm and, on the other hand, can be efficiently learned by neural networks when intermediate supervision is present.</p>
<p>Our main result can be stated as follows:
Theorem 1. (Informal) There exists a binary classification problem parameterized by size d, such that the following holds:</p>
<ul>
<li>On one hand, when equipped with sub-task decomposition supervision, a simple sequence-to-sequence model can get arbitrarily low $\epsilon&gt;0$ zero-one loss with number of gradient updates that is polynomial in $d, \epsilon^{-1}$.</li>
<li>On the other hand, when supervision regarding sub-task is missing, then for any polynomial time learning algorithm and (constant) $\epsilon&gt;0$, the zero-one loss will be higher than $1 / 2-\epsilon$.</li>
</ul>
<p>To summarize, the main contributions of this paper are:</p>
<ol>
<li>We show the first positive result that guarantees neural networks learnability in the presence of intermediate supervision for a problem that is unlearnable without it.</li>
<li>We do so in the sequence-to-sequence setting that is currently used for applying state-of-the-art language models on complex multi-hop tasks in NLP.</li>
<li>We show that with sufficient intermediate supervision this sequence-to-sequence setting allows learning any function in the $\mathbf{P}$ time complexity class.</li>
</ol>
<p>The remainder of this paper is organized as follows. Section 3 presents the sequence to sequence model we analyzed. Section 4 presents a hypothesis class and proves that it can be learned with our sequence to sequence model. Section 5 presents a concrete example, and demonstrates that the task of learning bit-subset parity with sequence-to-sequence models can be learned with sub-task decomposition and the corresponding intermediate supervision. Finally, in section 6 we generalize the positive results to any function in the $\mathbf{P}$ time complexity class, thus establishing our main result.</p>
<h1>2 Related Work</h1>
<p>The concept of how learning can be enhanced by guiding a learner through intermediate tasks is an old one, dating back to animal training by shaping (Skinner, 1958; Peterson, 2004; Krueger \&amp; Dayan, 2009). Since then, a large body of work has shown its practical benefits for various machine learning tasks. For example, there exists a rich line of work on the importance of shaping rewards and adding sub-goals in reinforcement learning tasks. Karlsson (1994) introduced the methodology of using knowledge in the reward function, in order to decompose a holistic task into several subtasks. Ng et al. (1999) established necessary and sufficient conditions for reward shaping to reserved optimal policies. Marthi (2007) investigate the problem of automatically learning a decomposition of the reward function. All these work intuitively rely on benefits of adding intermediate supervision. Recently, Zhai et al. (2022) showed that adding sub-goal rewards provably reduces the complexity of the synchronous value iteration algorithm. However, this reduction is linear in the number of the sub-goals, unlike our work that proves exponential gap in the supervised learning setting. Moreover, several of the notions in their analysis are unique to the reinforcement leaning setup and cannot be easily translated into the supervised learning setting (e.g., One-Way Intermediate States).
Negative theoretical results exist, showing that end-to-end learning of multi-hop problems is unfeasible without decomposition. Shalev-Shwartz et al. (2017) explored the theoretical limitations of end-to-end gradients based learning, studying learnability of tasks that are composed of classification and parity tasks, proving that the end-to-end approach does not converge in a polynomial</p>
<p>number of gradient updates. They do show that when intermediate supervision is provided, the gradients have a much higher signal-to-noise ratio. However, they provide no guarantees that in this case learning is possible in a polynomial number of gradient updates. In addition, Shalev-Shwartz \&amp; Shashua (2016) proved an exponential gap between end-to-end-based verification sample complexity and the decomposition-based verification sample complexity. However, again, an explicit setting in which providing intermediate supervision for training actually improves the situation to a point that learning is feasible, is lacking. We provide the first theoretical result proving that neural networks also benefit from sub-task decomposition, while earlier theoretical works in this space only prove that end-to-end learning is unfeasible in some compounded cases.</p>
<h1>3 THE ANALYZED SEQUENCE-TO-SEQUENCE LEARNING ALGORITHM</h1>
<p>A recent successful empirical approach for solving compounded natural language problems (Ling et al., 2017; Rajani et al., 2019; Piękos et al., 2021; Recchia, 2021; Cobbe et al., 2021; Nye et al., 2022; Wei et al., 2022; Zelikman et al., 2022) is to concatenate intermediate supervision labels to the input. This way, the language model receives a sequence composed of the input followed by the labels of the intermediate tasks, before emitting the final compounded answer. For a compounded binary classification task which consists of a $d$-bit input string $\mathbf{x}$, with $\mathcal{S}$ denoting the string of intermediate step results, we denote the combined input sequence as $\mathbf{z}=\operatorname{Concat}{\mathbf{x} ; \mathcal{S}}$, and the combined output sequence as $\mathbf{y}$, defined in a standard autoregressive fashion by ${ }^{1} y_{t}=z_{t+1}$ (see figure 2 for a $d=8$ example). Training and testing follow conventional sequence-to-sequence model protocol: At training time, $z_{t}$ for $t&gt;d$ will be the ground-truth sub-task result $y_{t-1}$ (a practice sometimes referred to as "teacher forcing" (Williams \&amp; Zipser, 1989)), and at test time, $z_{t}$ for $t&gt;d$ will be the model's prediction at time $t-1$.</p>
<p>We analyze the classical Elman recurrent neural networks (Elman, 1990) with ReLU activations as our sequence-to-sequence model. Given an input sequence $\mathbf{z}$ of length $T=d+\operatorname{len}(\mathcal{S})$ as defined above, the architecture $f^{\mathrm{RNN}}$ computes:</p>
<p>$$
\begin{aligned}
&amp; \forall t \in[T] \quad h_{t}(\mathbf{z})=\operatorname{ReLU}\left(W h_{t-1}+A \mathbf{e}<em t="t">{z</em>\right) \
&amp; \forall t \in[T] \quad f_{t}^{\mathrm{RNN}}(\mathbf{z})=B^{T} h_{t}(\mathbf{z}) \
&amp; h_{0}(\mathbf{z})=\operatorname{ReLU}\left(M_{0}\right)
\end{aligned}
$$}</p>
<p>where $\mathbf{e}<em 1="1">{0}, \mathbf{e}</em>$ is the initialization of the hidden state.} \in \mathbb{R}^{2}$ are one-hot vectors, $A \in \mathbb{R}^{m \times 2}$ translates the input to the hidden dimension $m, W \in \mathbb{R}^{m \times m}$ is the learned hidden weights matrix, $B \in \mathbb{R}^{m}$ is the output weights vector and $M_{0} \in \mathbb{R}^{m</p>
<p>We will use the binary cross-entropy loss over output locations for $t \geq d$, i.e., our loss ignores the architecture's prediction of $\mathbf{x}$ and depends on its prediction of intermediate labels and final outcome:</p>
<p>$$
l(\mathbf{y}, \mathbf{s})=\left(\frac{1}{T-d}\right) \sum_{t=d}^{T} \log \left(1+e^{-y_{t} \cdot s_{t}}\right)
$$</p>
<p>Algorithm 1 below describes the analyzed training procedure of our sequence-to-sequence model. This algorithm describes a straightforward SGD training procedure where, for simplicity, we analyze a variant that updates only the hidden $W$ weights while keeping $A, B, M_{0}$ frozen at initialization. This amounts to keeping the input, output and the $t=0$ hidden state untrained, and training only the core recurrence operation to perform the task while complying with these frozen components.</p>
<h2>4 COMPOUNDED SEQUENCE TO SEQUENCE LEARNABILITY</h2>
<p>In this section, we present a hypothesis class and prove for it that the above described "teacher forcing" (Williams \&amp; Zipser, 1989) of intermediate supervision at training time with algorithm 1 provably leads to generalization in polynomial sample complexity and gradient updates. This guarantee will allow us to prove our positive results in the following sections, as we will show that interesting function families belong to this hypothesis class.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code>Algorithm 1: Training \(f^{\mathrm{RNN}}\) with SGD
Data: Data set \(\mathcal{D}\), learning rate \(\eta\).
Initialization: The entries of \(W^{(0)}, A, M_{0}\) are i.i.d. generated from \(N\left(0, \frac{2}{m}\right)\). The entries of \(B\)
    are i.i.d. generated from \(N\left(0, \frac{1}{m}\right)\).
for \(i=1,2,3 \ldots n\) do
    Randomly sample \(\left(\mathbf{x}_{i}, \mathbf{y}_{i}\right)\) from the data set \(\mathcal{D}\).
    \(W^{(i)}=W^{(i-1)}-\eta \nabla_{W^{(i-1)}} \ell\left(\mathbf{y}_{i}, f^{\mathrm{RNN}, W^{(i-1)}}\left(\mathbf{z}_{i}\right)\right)\).
end
</code></pre></div>

<p>In order to analyze the teacher forcing technique, we begin with an important observation. Essentially, we show that when the zero-one loss of all the single-hop sub-tasks is low, then it implies that also at test time, when the model does not have the ground truth results of the previous sub-tasks and the errors might accumulate, the zero-one loss on the final answer is still low:
Lemma 1. Denote by $z_{t}^{\text {train }}:=y_{t-1}$ the ground truth input at training time, and by $z_{t}^{\text {test }}:=$ $f_{t-1}^{R N N, W}\left(\mathbf{z}^{\text {test }}\right)$ the iteratively predicted input at test time. Then, for any $W$ the following holds:</p>
<p>$$
\mathbb{E}<em 0-1="0-1">{\mathbf{x}}\left[l</em>}\left(y, f_{T}^{R N N, W}\left(\mathbf{z}^{\text {test }}\right)\right)\right] \leq \mathbb{E<em t="d">{\mathbf{x}}\left[\sum</em>\right)\right)\right]
$$}^{T} l_{0-1}\left(y_{t}, f_{t}^{R N N, W}\left(\mathbf{z}^{\text {train }</p>
<p>Proof. Clearly, for any $\mathbf{x}$ when $f^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right)$ solves all of the sub-tasks correctly we have that $\mathbf{z}^{\text {test }}=\mathbf{z}^{\text {train }}$ and therefore they have the same zero zero-one loss. So it is enough to upper bound the probability that $f^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right)$ is erroneous in any sub-task. Now by definition, for any $t$ the zero-one loss at the $t$ 'th location is equal to the probability of wrong prediction at this location. Therefore, by the union bound, we get that the sum of the zero-one loss over all the locations is upper bounding the probability of $f^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right)$ make an error in any sub-task. See full details in section A at the appendix.</p>
<p>As expected, due to a union bound, when the model does not have the ground truth results of the previous sub-task the error can increase by a factor of $T-d$ but this increase is relatively modest as long as $T$ is polynomial in $d$.</p>
<p>Lemma 1 above assures us that it is enough to find an hypothesis class for which algorithm 1 converges and generalizes when we do have the ground truth results of the previous sub-tasks, in order to prove that the teacher forcing technique works. As a candidate for such a hypothesis class, we consider tasks for which the output at each location $d \leq t \leq T$ can be written as sign of composition of linear functions (represented by $\mathbf{w}$ below) of at most $N&lt;T$ input locations $j_{1}, \ldots, j_{N} \leq t$, with polynomials activations $\psi_{t}(x)=\sum_{i=0}^{\operatorname{deg}\left(\psi_{t}\right)} a_{t, i} x^{i}$ :</p>
<p>$$
\forall d \leq t \leq T \quad h_{t}(\mathbf{z})=\operatorname{sign}\left(\psi_{t}\left(\left\langle\frac{\mathbf{w}^{(t)}}{\left|\mathbf{w}^{(t)}\right|},\left(\begin{array}{c}
\mathbf{e}<em j__1="j_{1">{z</em> \
\vdots \
\mathbf{e}}}<em j__N="j_{N">{z</em>
\end{array}\right)\right\rangle\right)\right)
$$}}</p>
<p>In order to prove convergence and generalization results, we will measure the complexity of functions in the above hypothesis class by a function $\phi(T, \psi, N)$, described formally in appendix A. Importantly, $\phi(T, \psi, N)$ is polynomial in both $T$ and $\max <em i="i" t_="t,">{t, i}\left|a</em>\right|$, while exponential in both $\max <em t="t">{t} \operatorname{deg}\left(\psi</em>$ the hypothesis class described in eq 6.
Now, with this hypothesis class, we can combine lemma 1 with theorem 2 in Wang et al. (2021). They study the learnability of RNNs for binary classification tasks ${ }^{2}$ without intermediate supervision, and prove that algorithm 1 is capable of learning function where the final answer $y$ have low complexity $\phi(T, \psi, N)$.}\right)$ and $N$. We will denote by $\mathcal{H}_{\phi(T, \psi, N)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of the proposed input and output for learning the $d=8$ bit-subset parity problem with sequence-to-sequence models.</p>
<p>Theorem 2. Denote by $z_{t}^{\text {test }}:=\int_{t-1}^{R N N, W}\left(\mathbf{z}^{\text {test }}\right)$ the iteratively predicted input at test time, and let $\epsilon, \delta&gt;0$. Assume we run Algorithm 1 for $n&gt;\tilde{O}\left(\left(\frac{\phi(T, \psi, N)+\log \left(\frac{1}{\delta}\right)}{\epsilon}\right)^{2}\right)$ iterations with learning rate $\eta=\frac{1}{m \sqrt{n}}$, then there exists $m^{<em>}=$ poly $\left(n, \delta^{-1}, T\right)$ such that if $m&gt;m^{</em>}$ then for any $h \in$ $\mathcal{H}_{\phi(T, \psi, N)}$ with probability at least $1-\delta$ over the randomness in Algorithm 1, the following holds:</p>
<p>$$
\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}<em 0-1="0-1">{\mathbf{x}}\left[l</em>\right)\right)\right]&lt;\epsilon
$$}\left(y, f_{T}^{R N N, W^{(i)}}\left(\mathbf{z}^{t e x t</p>
<p>where $W^{(i)}$ denotes the output of Algorithm 1 at the $i^{\prime}$ th iteration and $l_{0-1}$ is the zero-one loss.
Note that sections C,D of the appendix extends theorem 2 for both SGD and GD with finite precision. In the next sections, we will prove our positive results by showing the intermediate single-hop subtasks of the analyzed tasks belong to $\mathcal{H}_{\phi(T, \psi, N)}$ with low complexity $\phi(T, \psi, N)$.</p>
<h1>5 Learning Bit-Subset Parity With Sequence to Sequence Models</h1>
<p>As a concrete demonstration, in this section we show that unlike the end-to-end case, bit-subset parity can be learned with neural networks when intermediate supervision is provided. We begin by defining the challenging task of learning parities over unknown subsets of input bits. Specifically, for a $d$-bit string with a subset of $d / 2$ randomly predefined unique indices $i_{1}, \ldots, i_{d / 2}$, our goal is to train a predictor mapping $\mathbf{x} \in{0,1}^{d}$ to $y=(-1)^{\sum_{j=1}^{d / 2} x_{i_{j}}}$ where $\mathbf{x}$ is uniformly distributed. In words, $y$ indicates whether the number of 1 's in the given subset of coordinates of $\mathbf{x}$ is odd or even.
We analyze this task as a "multi-hop" task by decomposing it into natural intermediate sub-tasks: parities of subsets of the predefined input subset $x_{i_{1}}, \ldots, x_{i_{d / 2}}$. Concretely, assuming for simplicity that $d / 2$ is a power of 2 , and beginning with only two adjacently indexed input bits at a time, we recursively treat the parity of every two adjacently indexed subgroups as an intermediate task. Figure 2(a) illustrates this binary tree sub-task decomposition of our parity problem. The leaves of the tree of intermediate labels $\mathcal{T}$ are $-1^{x_{i_{1}}+x_{i_{2}}}, \ldots,-1^{x_{i_{d / 2-1}}+x_{i_{d / 2}}}$ and each node in the tree represents the sub-task of calculating the parity function over its descendants.
In order to fit into the sequence-to-sequence setting of section 3, we translate our imaginary tree of intermediate labels $\mathcal{T}$ into a sequence of intermediate labels $\mathcal{S}$ by inverse-BFS like tree traversal, and then concatenate the sequence $\mathcal{S}$ after the input $\mathbf{x}$. An exact formulation of the mapping from tree $\mathcal{T}$ to sequence of intermediate labels $\mathcal{S}$ is given in appendix B.</p>
<h1>5.1 Learnability of Bit-Subset Parity With and Without Intermediate SUPERVISION</h1>
<p>In this section we show that the sub-tasks of learning bit-subset parity are simple enough to be covered by the result in theorem 2, i.e., we prove that our sequence-to-sequence formulation of the bit-subset parity target function, which includes the intermediate labels sequence $\mathcal{S}$ defined above, can be written as a multivariate function where each of its outputs is a simple low degree polynomial of at most $O(1)$ inputs bits. We show that this is indeed the case, i.e., that all the parity target functions comprising the intermediate supervision to our problem belong to $\mathcal{H}<em t="t">{\phi(T, \psi, N)}$ (see section 4) with $N, \max </em>\right), \max } \operatorname{deg}\left(\psi_{t<em i="i" t_="t,">{t, i}\left|a</em>\right)$. Thus, no efficient learning is guaranteed for the original compounded task.
We begin by showing that our single-hop tasks of parities over two bits (see illustration in figure 2(a)) are simple degree-2 polynomials:
Lemma 2. There exists degree two polynomial $\psi(x)=a_{2} x^{2}+a_{1} x+a_{0}$ with bounded coefficients $\forall i \quad\left|a_{i}\right|&lt;10$ as well as $\mathbf{w} \in \mathbb{R}^{4}$ such that:}\right|$ that do not grow with $d$. Importantly, when defining the input sequence to be only the original input, without the concatenated sub-task decomposition labels, then the function $h_{T}(\mathbf{x})$ clearly depends on $\nicefrac{{d}}{{2}}$ bits, and therefore will require $N=\nicefrac{{d}}{{2}}$, that leads to exponential complexity $\phi(T, \psi, N)=\Omega\left(e^{d</p>
<p>$$
\forall z_{1}, z_{2} \in{0,1} \quad \psi\left(\left\langle\frac{\mathbf{w}}{|\mathbf{w}|},\binom{\mathbf{e}<em 1="1">{z</em>}}}{\mathbf{e<em 2="2">{z</em>
1 &amp; z_{1}=z_{2} \
-1 &amp; z_{1} \neq z_{2}
\end{array}\right.
$$}}}\right\rangle\right\rangle=\left{\begin{array}{ll</p>
<p>Proof. We will use $\mathbf{w}$ to sum the first coordinates of $\mathbf{e}<em 1="1">{z</em>}}, \mathbf{e<em 2="2">{z</em>=1$ points, see full details in appendix B.}}$, and use polynomial interpolation to find a degree two polynomial $\psi$ that interpolates the $z_{1}=z_{2}=0, z_{1} \neq z_{2}, z_{1}=z_{2</p>
<p>The above lemma implies that all of the target functions in our defined intermediate supervision belong to $\mathcal{H}_{\phi(T, \psi, N)}$ for $\phi(T, \psi, N)=O(d)$. Therefore, together with theorem 2, it assures us that when intermediate supervision is available, Algorithm 1 can learn bit-subset parities with polynomial network size, sample complexity and number of gradient updates.
Now, after we showed that when incorporating intermediate supervision bit-subset parities can be learned by a neural network, we will use the results of Shalev-Shwartz et al. (2017) to establish an exponential gap between the end-to-end and decomposition-based neural network learnability ${ }^{3}$ :
Corollary 1. When learning bit-subset parities using neural networks, the following holds:</p>
<ul>
<li>On one hand, when equipped with sub-task decomposition supervision, a simple sequence-to-sequence model can get arbitrarily low $\epsilon&gt;0$ zero-one loss with number of gradient updates that is polynomial in $d, \epsilon^{-1}$.</li>
<li>On the other hand, when supervision regarding sub-task is missing, then for any (constant) $\epsilon&gt;0$ with high probability over the target parity, the zero-one loss will be higher than $\nicefrac{{1}}{{2}}-\epsilon$ unless the number of gradient updates is exponential in $d$.</li>
</ul>
<p>Proof. Follows directly by combining theorem 2 and lemma 2 with the the negative results in Shalev-Shwartz et al. (2017). See full details in section F at the appendix.</p>
<h3>5.2 Bit-Subset Parity Experiments</h3>
<p>In section 5.1 we proved an exponential gap when using Elman RNNs (Elman, 1990) to learn bitsubset parity with and without sub-task decomposition. This section empirically demonstrates that the same gap exists with the commonly used Transformer (Vaswani et al., 2017) architecture. We trained a series of models while varying the input sizes from 8 bits to 256 bits. For each input size,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>we trained a BERT-base sized Transformer model for $100 k$ iterations ${ }^{4}$ with and without intermediate supervision. The intermediate supervision was introduced exactly as described in the previous subsection, see Figure 2 for an illustration. See full technical details of the training apparatus in appendix G.</p>
<p>Figure 3 clearly shows that in a practical setting, using common Transformer networks, a very large gap quickly opens between the settings with and without intermediate supervision. The employed BERT base sized Transformer architecture is a strong network that pushed the envelope on very challenging NLP tasks, and is much stronger than the theoretically analyzed RNN. Still, learning even the 32 bit subset parity task without supervision proved to be too challenging even for this network (no learning after over 2 M steps), while it easily learned the task in the presence of intermediate supervision. Overall this experiment, performed on the same task on which we prove our theoretical results, reinforces their relevance to common Transformer architectures.</p>
<h1>6 UNIVERSALITY OF DECOMPOSITION BASED SEQUENCE-TO-SEQUENCE LEARNING</h1>
<p>In this section, we prove our main result (outlined in the introductory theorem 1). On the one hand, we generalize the positive results of section 5.1 by showing that when sufficient intermediate supervision is available, a neural network can efficiently learn any function in the $\mathbf{P}$ time complexity class. On the other hand, we rely on existing works (Valiant, 1984; Goldreich et al., 1986; Daniely \&amp; Shalev-Shwartz, 2016) to show that under either standard cryptographic assumptions or computational complexity hardness assumptions, there exist functions in the $\mathbf{P}$ time complexity class that cannot be learned by any polynomial time learning algorithm without intermediate supervision.</p>
<p>We begin by defining the decomposition of any function $f$ in the $\mathbf{P}$ time complexity class into sub tasks. For that, we will use the fact that any such $f$ has polynomial circuit complexity (see for example theorem 9.30 in Sipser (2013)), and therefore can be computed by a boolean circuit with polynomial size. We will denote by $G=(V, E)$ the directed acyclic graph associated with such a circuit, and by $l_{v}$ the logic gates of each vertex $v$. Furthermore, since both the"AND" and "OR" logical gates can be decomposed into a boolean circuit with binary-tree like structure, we may assume that the input degree of each vertex is $O(1)$.</p>
<p>Now, in order to fit into the sequence-tosequence setting of section 3, we define the intermediate labels sequence $\mathcal{S}$ for any $f$. Basically, each non-leaf vertex $v \in V$ will represent an intermediate task with its ground-truth label determined by $l_{v}$, and we will use a topological sorting of $G$ in order to translate $G$ into a sequence of intermediate labels $\mathcal{S}$ with length $T:=|V|$ (see figure 2 for a concrete example of this abstract construction strategy). Importantly, as in the bit-subsets parity task, $T$ is polynomial in $d$.</p>
<p>In order to show our generalized positive result, theorem 2 motivates us to prove that our sequence-to-sequence formulation of any function $f$ in the $\mathbf{P}$ time complexity class, which includes the intermediate labels sequence $\mathcal{S}$ defined above, can be written as a multivariate function where each of its outputs is a simple low degree polynomial of at most $O(1)$ input bits. Lemma 3 below shows that this is indeed the case, i.e., that all the target functions comprising the intermediate supervision to our problem belong to $\mathcal{H}<em t="t">{\psi(T, \psi, N)}$ (see section 3) with $N, \max </em>\right), \max } \operatorname{deg}\left(\psi_{t<em i="i" t_="t,">{t, i}\left|a</em>\right|$ that do not grow with $d$.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The number of steps until a BERTbase sized Transformer learns bit-subset parities with and without intermediate supervision. By learning we mean validation accuracy higher than $60 \%$. While this definition is somehow arbitrary, in practice we observed a grokking phenomenon (Power et al., 2021) where very soon after the accuracy became higher than random level it also became almost perfect (accuracy $&gt;95 \%$ ).</p>
<p>Lemma 3. For any logical gate $l_{v}:{0,1}^{N} \rightarrow{0,1}$ with $N=O(1)$, there exists $O(1)$ degree polynomial $\psi(x)=\sum_{i=0}^{\operatorname{deg}(\psi)} a_{i} x^{i}$ with bounded coefficients $\max <em i="i">{i}\left|a</em>$ such that:}\right|=O(1)$ as well as $\mathbf{w} \in \mathbb{R}^{2 N</p>
<p>$$
\forall z_{1}, \ldots, z_{N} \in{0,1} \quad \psi\left(\left\langle\frac{\mathbf{w}}{|\mathbf{w}|},\left(\begin{array}{c}
\mathbf{e}<em 1="1">{z</em> \
\vdots \
\mathbf{e}}<em N="N">{z</em>
\end{array}\right)\right\rangle\right)=l_{v}\left(z_{1}, \ldots, z_{N}\right)
$$}</p>
<p>Proof. We will use $\mathbf{w}$ to uniquely represent each possible combination of $z_{0}, \ldots, z_{N}$ as an $N$ bit real value number, and use polynomial interpolation to find a $2^{N}$-degree polynomial $\psi$ that interpolates the $z_{1}=\cdots=z_{N}=0, \ldots, z_{1}=\cdots=z_{N}=1$ points. Finally the $O(1)$ coefficients boundedness follow from taking maximum ${ }^{5}$ over all possible $l_{v}$ logical gates. see full details in appendix B.</p>
<p>The above lemma implies that all of the target functions in our defined intermediate supervision belong to $\mathcal{H}_{\phi(T, \psi, N)}$ for $\phi(T, \psi, N)=O(d)$. Therefore, together with theorem 2, it assures us that when intermediate supervision is available, Algorithm 1 can learn any function in the $\mathbf{P}$ time complexity class with polynomial network size, sample complexity and number of gradient updates.
Now, after we showed that when incorporating intermediate supervision any function in the $\mathbf{P}$ time complexity class can be learned by a neural network, our main results is a simple corollary of the above results:
Corollary 2. Under either standard cryptographic assumptions or computational complexity hardness assumptions, there exists a binary classification problem parameterized by size $d$, such that the following holds:</p>
<ul>
<li>On one hand, when equipped with sub-task decomposition supervision, a simple sequence-to-sequence model can get arbitrarily low $\epsilon&gt;0$ zero-one loss with number of gradient updates that is polynomial in $d, \epsilon^{-1}$.</li>
<li>On the other hand, when supervision regarding sub-task is missing, then for any polynomial time learning algorithm and (constant) $\epsilon&gt;0$, the zero-one loss will be higher than $1 / 2-\epsilon$.</li>
</ul>
<p>Proof. Follows directly by combining theorem 2 and lemma 3 with either the negative results in Valiant (1984); Goldreich et al. (1986) or in Daniely \&amp; Shalev-Shwartz (2016).</p>
<h1>7 DISCUSSION</h1>
<p>In this paper, we show for a broad family of functions an exponential gap between learning algorithms that rely on intermediate supervision and algorithms that do not rely on intermediate supervision. Across domains and architectures, there has been a wide range of proposed methods for introducing intermediate supervision. Some design specialized architectures, some add relevant loss terms, etc. The method that is taking over in the NLP domain is straightforward, and is particularly natural for this domain in which the core architectures are strong sequence-to-sequence Language Models: Concatenate the intermediate supervision to the input, and thus jointly train the model to maximize the likelihood of all the intermediate labels as well as the overall output. Our analysis is framed in this space, and motivates this intuitive incorporation of intermediate supervision in the framework of sequence-to-sequence models. We show that even with a simple sequence-to-sequence architecture it is feasible to expect such simultaneous compounded learning to be useful. In this regard, we view our work as providing timely theoretical feedback to the rapid empirical advances in this field.</p>
<p>Limitations: We proved universal learnability results when sufficient intermediate supervision was provided. A fundamental question is what happens when we limit the amount of sub-task supervision. For the task of bit-subset parity, we demonstrated that supervision regarding $O(d)$ sub-tasks can yield an exponential advantage. An interesting question that we leave open for future work is whether there exists a similar advantage with only $O(1)$ sub-tasks.
In addition, while our results show an exponential gain, it is still unclear which sub-tasks are solvable by end-to-end methods, and which tasks require decomposition? Interestingly, a recent study (Abbe et al., 2022) addressed exactly this question for one-layer hidden networks in the mean-field regime. However, our understanding of this question for practical architectures is still very limited.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>REPRODUCIbILITY STATEMENT</h1>
<p>A complete proof of all the theoretical claims was included in the appendix. We also provide the source code for the bit-subset parity experiment in https://github.com/HUJIDeep/sub_task_decomposition.</p>
<h2>ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING</h2>
<p>We thank Eran Malach and Shai Shalev-Shwartz for a helpful discussion on our stronger negative results, as well as Lifu Wang for clarifying Wang et al. (2021). This research was supported by the ERC (European Research Council) and the ISF (Israel Science Foundation). Yoav Levine was supported by the Israel Academy of Sciences Adams fellowship.</p>
<h2>REFERENCES</h2>
<p>Emmanuel Abbe and Colin Sandon. On the universality of deep learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 20061-20072. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ e7e8f8e5982b3298c8addedf6811d500-Paper.pdf.</p>
<p>Emmanuel Abbe, Pritish Kamath, eran malach, Colin Sandon, and Nathan Srebro. On the power of differentiable learning versus PAC and SQ learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=TZPidZS3r_z.</p>
<p>Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pp. 4782-4887. PMLR, 2022.</p>
<p>Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 0ee8b85a85a49346fdff9665312a5cc4-Paper.pdf.</p>
<p>Afonso S Bandeira and Ramon Van Handel. Sharp nonasymptotic bounds on the norm of random matrices with independent entries. The Annals of Probability, 44(4):2479-2506, 2016.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1876-1900. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Oscar Chang, Lampros Flokas, Hod Lipson, and Michael Spranger. Assessing satnet's ability to solve the symbol grounding problem. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1428-1439. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper/2020/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Amit Daniely and Shai Shalev-Shwartz. Complexity theoretic limitations on learning dnf's. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir (eds.), 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pp. 815-830, Columbia University, New York, New York, USA, 23-26 Jun 2016. PMLR. URL https://proceedings. mlr.press/v49/daniely16.html.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 41714186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.</p>
<p>Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL), 2021.</p>
<p>Tobias Glasmachers. Limits of end-to-end learning. In Asian Conference on Machine Learning, pp. 17-32. PMLR, 2017.</p>
<p>Oded Goldreich, Shafi Goldwasser, and Silvio Micali. How to construct random functions. Journal of the ACM (JACM), 33(4):792-807, 1986.</p>
<p>Çağlar Gülçehre and Yoshua Bengio. Knowledge matters: Importance of prior information for optimization. Journal of Machine Learning Research, 17(8):1-32, 2016. URL http://jmlr. org/papers/v17/gulchere16a.html.</p>
<p>Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.</p>
<p>Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HygegyrYwH.</p>
<p>Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, and Peter Clark. How much coffee was consumed during EMNLP 2019? fermi problems: A new reasoning challenge for AI. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7318-7328, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.582. URL https://aclanthology.org/2021.emnlp-main.582.</p>
<p>Jonas Karlsson. Task decomposition in reinforcement learning. In Proceedings of the AAAI Spring Symposium on Goal-Driven Learning, Stanford, CA, 1994.</p>
<p>Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM (JACM), 45(6):983-1006, 1998.</p>
<p>Kai A Krueger and Peter Dayan. Flexible shaping: How learning in small steps helps. Cognition, 110(3):380-394, 2009.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 158-167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.</p>
<p>Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3154-3169, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.225. URL https://aclanthology.org/2022. acl-long. 225 .</p>
<p>Bhaskara Marthi. Automatic shaping and decomposition of reward functions. In Proceedings of the 24th International Conference on Machine learning, pp. 601-608, 2007.</p>
<p>Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pp. 278-287, 1999.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop, 2022. URL https://openreview.net/forum? id=HBlx2idbkbq.</p>
<p>Gail B Peterson. A day of great illumination: Bf skinner's discovery of shaping. Journal of the experimental analysis of behavior, 82(3):317-328, 2004.</p>
<p>Piotr Piękos, Mateusz Malinowski, and Henryk Michalewski. Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 383-394, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.49. URL https://aclanthology.org/2021.acl-short.49.</p>
<p>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. In ICLR MATH-AI Workshop, 2021.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2022. URL https://arxiv. org/abs/2210.03350.</p>
<p>Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, James Bradbury, Matthew Johnson, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis \&amp; insights from training Gopher. arXiv submission, 2021.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4932-4942, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1487. URL https: //aclanthology.org/P19-1487.</p>
<p>Gabriel Recchia. Teaching autoregressive language models complex tasks by demonstration. arXiv preprint arXiv:2109.02102, 2021.</p>
<p>Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.</p>
<p>Shai Shalev-Shwartz and Amnon Shashua. On the sample complexity of end-to-end training vs. semantic abstraction training. arXiv preprint arXiv:1604.06915, 2016.</p>
<p>Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning. In International Conference on Machine Learning, pp. 3067-3075. PMLR, 2017.</p>
<p>Ohad Shamir. Distribution-specific hardness of learning neural networks. The Journal of Machine Learning Research, 19(1):1135-1163, 2018.</p>
<p>M Sipser. Introduction to the theory of computation. 3th. Cengage Learning, 2013.
Burrhus F Skinner. Reinforcement today. American Psychologist, 13(3):94, 1958.
Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Lifu Wang, Bo Shen, Bo Hu, and Xing Cao. On the provable generalization of recurrent neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 20258-20269. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ a928731e103dfc64c0027fa84709689e-Paper.pdf.</p>
<p>Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. In International Conference on Machine Learning, pp. 6545-6554. PMLR, 2019.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J.</p>
<p>Noam Wies, Yoav Levine, Daniel Jannai, and Amnon Shashua. Which transformer architecture fits my data? a vocabulary bottleneck in self-attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 11170-11181. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/wies21a.html.</p>
<p>Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270-280, 1989.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465.</p>
<p>Yuexiang Zhai, Christina Baek, Zhengyuan Zhou, Jiantao Jiao, and Yi Ma. Computational benefits of intermediate rewards for goal-reaching policy learning. Journal of Artificial Intelligence Research, 73:847-896, 2022.</p>
<h1>A COMPOUNDED SEQUENCE TO SEQUENCE LEARNABILITY DETAILS</h1>
<p>We start by formally defining our sequence-to-sequence functions complexity measure as:</p>
<p>$$
\phi(T, \psi, N):=\tilde{O}\left(T^{16+3 N+\max <em t="t">{t} \operatorname{deg}\left(\psi</em> \max }\right)} C^{2 N<em t="t">{t} \operatorname{deg}\left(\psi</em> \max }\right)^{3 N<em i="i" t_="t,">{t, i}\left|a</em>\right)
$$}\right|^{2</p>
<p>Where $C&gt;0$ is some constant.
Now we prove lemma 1 from the main text. Essentially this lemma applies the union bound to show that when the zero-one loss of all the single-hop sub-tasks is low, then also at test time - when the model does not have the ground truth results of the previous sub-task and errors may accumulate the zero-one loss on the final answer is still low.</p>
<p>Proof. Denote by $\epsilon$ the zero-one loss for $\mathbf{z}^{\text {train }}$, i.e., the right hand side in eq 5. Clearly, for any $\mathbf{x}$ when $f^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right)$ solves all the sub-tasks correctly we have that $\mathbf{z}^{\text {test }}=\mathbf{z}^{\text {train }}$ and therefore $l_{0-1}\left(y, f_{T}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {test }}\right)\right)=0$. So it is enough to upper bound the probability that $f^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right)$ makes an error in any sub-task by $\epsilon$. But by the zero-one loss definition, for any $t$ we have that:</p>
<p>$$
P_{\mathbf{x}}\left(f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right) \neq y_{t}\right)=\mathbb{E}<em 0-1="0-1">{\mathbf{x}}\left[l</em>\right)\right)\right]
$$}\left(y_{t}, f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }</p>
<p>And therefore $\sum_{t=d}^{T} P_{\mathbf{x}}\left(f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right) \neq y_{t}\right)=\epsilon$. Finally, by the union bound we got that</p>
<p>$$
P_{\mathbf{x}}\left(\exists d \leq t \leq T \quad f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right) \neq y_{t}\right) \leq \sum_{t=d}^{T} P_{\mathbf{x}}\left(f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z}^{\text {train }}\right) \neq y_{t}\right)=\epsilon
$$</p>
<h2>B Sub Tasks Learnability Proofs</h2>
<p>In this section we prove lemmas 2,3 from the main text, i.e., we prove that our intermediate steps are simple enough to be covered by theorem 2 .</p>
<p>We begin by formally describing the details of learning parities with sequence-to-sequence models. Since sequence-to-sequence models expect their inputs to be sequences, we will translate the tree described in section 5 into a sequence by inverse BFS like tree traversal, and concatenate the result sequence after $\mathbf{x}$. Therefore, our inputs sequence includes all the $d$ variables in $\mathbf{x}$, together with all the sub-tasks decomposition nodes in the binary tree except the root (that represent $y$ ). So we will have an input sequence length $T$ that is equal to:</p>
<p>$$
T:=d+\text { nodes in full binary tree with } \frac{d}{4} \text { leaves }-1=\frac{3}{2} d-2
$$</p>
<p>And the ground-truth sub-task results are recursively defined by:</p>
<p>$$
\forall t \geq d \quad y_{t}= \begin{cases}(-1)^{x_{t_{2(t-d)+1}}+x_{t_{2(t-d)+2}}} &amp; t&lt;\frac{5}{4} d \ (-1)^{y_{2}\left(\epsilon-\frac{5 d}{4}\right)+d}+y_{2\left(\epsilon-\frac{5 d}{4}\right)+d+1}} &amp; \text { else }\end{cases}
$$</p>
<p>For $t&gt;d$, at training time, $z_{t}$ will be the ground-truth sub-task result $y_{t-1}$. At test time $z_{t}$ will be the model prediction at time $t-1$ :</p>
<p>$$
\forall t \in[T] \quad z_{t}= \begin{cases}x_{t} &amp; t \leq d \ \frac{1}{2}+\frac{1}{2} \operatorname{sign}\left(f_{t-1}^{\mathrm{RNN}}\left(z_{1}, \cdots, z_{t-1}\right)\right) &amp; t&gt;d \wedge \text { test } \ \frac{1+y_{t-1}}{2} &amp; t&gt;d \wedge \text { training }\end{cases}
$$</p>
<p>Note that $f^{\mathrm{RNN}}$ is causal model, i.e. $f_{t_{1}}^{\mathrm{RNN}}$ does not depend on $\mathbf{z}<em 2="2">{t</em>$, and therefore eq 15 is well defined.}}$ for any $t_{2}&gt;t_{1</p>
<p>Now, we prove lemma 2 from the main text. Essentially this lemma shows that the intermediate steps of length 2 parities belong to the hypothesis class define in eq 6 .</p>
<p>Proof. Define $\mathbf{w}:=\frac{1}{\sqrt{2}}\left(\begin{array}{c}1 \ 0 \ 1 \ 0\end{array}\right)$, then</p>
<p>$$
\left\langle\frac{\mathbf{w}}{|\mathbf{w}|},\binom{\mathbf{e}<em 1="1">{z</em>}}}{\mathbf{e<em 2="2">{z</em>
$$}}}\right\rangle= \begin{cases}\sqrt{2} &amp; z_{1}=0 \wedge z_{2}=0 \ \frac{1}{\sqrt{2}} &amp; z_{1} \neq z_{2} \ 0 &amp; z_{1}=1 \wedge z_{2}=1\end{cases</p>
<p>Therefore, it is enough to find $\psi$ such that</p>
<p>$$
\psi(0)=\psi(\sqrt{2})=1 \wedge \psi\left(\frac{1}{\sqrt{2}}\right)=-1
$$</p>
<p>Finally, we will use Lagrange basis functions to find the required polynomial and get:</p>
<p>$$
\begin{aligned}
\psi(z) &amp; =\left(\frac{z-\frac{1}{\sqrt{2}}}{0-\frac{1}{\sqrt{2}}}\right)\left(\frac{z-\sqrt{2}}{0-\sqrt{2}}\right)-\left(\frac{z-0}{\frac{1}{\sqrt{2}}-0}\right)\left(\frac{z-\sqrt{2}}{\frac{1}{\sqrt{2}}-\sqrt{2}}\right)+\left(\frac{z-0}{\sqrt{2}-0}\right)\left(\frac{z-\frac{1}{\sqrt{2}}}{\sqrt{2}-\frac{1}{\sqrt{2}}}\right) \
&amp; =1 \cdot\left(z-\frac{1}{\sqrt{2}}\right)(z-\sqrt{2})+\frac{z}{2}(z-\sqrt{2})+3 z\left(z-\frac{1}{\sqrt{2}}\right) \
&amp; =\left(z^{2}-\frac{3}{\sqrt{2}} z+1\right)+\left(\frac{z^{2}}{2}-\frac{1}{\sqrt{2}} z\right)+\left(3 z^{2}-\frac{3 z}{\sqrt{2}}\right) \
&amp; =\frac{9}{2} z^{2}-\frac{7}{\sqrt{2}} z+1
\end{aligned}
$$</p>
<p>Now we prove lemma 3 from the main text. Essentially this lemma shows that also our intermediate steps for any functions in the $\mathbf{P}$ time complexity class belongs to the hypothesis class define in eq 6 .</p>
<p>Proof. Denote $\alpha\left(z_{1}, \ldots, z_{N}\right):=\sum_{i=0}^{N-1} 2^{i} \cdot 1_{z_{i}=0}$ the function that converts $N$ bits to their binary string, and define $\mathbf{w}:=\sqrt{\frac{3}{4^{N}-1}}\left(\begin{array}{c}2^{0} \ 0 \ 2^{1} \ 0 \ \vdots \ 2^{N-1} \ 0\end{array}\right)$, then $\mathbf{w}$ is a unit vector that represents $z_{1}, \ldots, z_{N}$ as $N$ bit numbers $\left\langle\frac{\mathbf{w}}{|\mathbf{w}|},\left(\begin{array}{c}\mathbf{e}<em 1="1">{z</em>}} \ \vdots \ \mathbf{e<em N="N">{z</em>\right)$. Now, we can use the Lagrange basis functions to find the required polynomial:}}\end{array}\right)\right\rangle=\alpha\left(z_{1}, \ldots, z_{N</p>
<p>$$
\psi(x)=\sum_{z_{1}, \ldots, z_{N}=0}^{1}\left(f_{v}\left(z_{1}, \ldots, z_{N}\right) \prod_{\left(\tilde{z}<em N="N">{1}, \ldots, \tilde{z}</em>}\right) \neq\left(z_{1}, \ldots, z_{N}\right)}\left(\frac{x-\alpha\left(\tilde{z<em N="N">{1}, \ldots, \tilde{z}</em>}\right)}{\alpha\left(z_{1}, \ldots, z_{N}\right)-\alpha\left(\tilde{z<em N="N">{1}, \ldots, \tilde{z}</em>\right)\right)
$$}\right)</p>
<p>Finally the $O(1)$ coefficients boundedness follows from taking the maximum ${ }^{6}$ over all possible $f_{v}$ functions.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C EXTENSION FOR SGD WITH FINITE PRECISION</h1>
<p>In this section, we prove theorem 2 from the main text holds also for algorithm 2 which is a finiteprecision variant of $\mathrm{SGD}^{2} \ldots$ We will follow the proof in Wang et al. (2021) while taking into account the finite precision gradients.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span>\<span class="p">(</span><span class="n">f</span><span class="o">^</span><span class="p">{</span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">RNN</span><span class="p">}}</span>\<span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">finite</span><span class="w"> </span><span class="n">precision</span><span class="w"> </span><span class="n">SGD</span><span class="w"> </span><span class="p">(</span><span class="n">an</span><span class="w"> </span><span class="n">finite</span><span class="w"> </span><span class="n">precisio</span><span class="w"> </span><span class="n">variant</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="n">Data</span><span class="p">:</span><span class="w"> </span><span class="n">Data</span><span class="w"> </span><span class="n">set</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">learning</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">eta</span>\<span class="p">),</span><span class="w"> </span><span class="n">finite</span><span class="w"> </span><span class="n">precision</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">sigma</span>\<span class="p">)</span><span class="o">.</span>
<span class="n">Initialization</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">entries</span><span class="w"> </span><span class="n">of</span><span class="w"> </span>\<span class="p">(</span><span class="n">W</span><span class="o">^</span><span class="p">{(</span><span class="mi">0</span><span class="p">)},</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">M_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">generated</span><span class="w"> </span><span class="n">i</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="w"> </span><span class="n">from</span><span class="w"> </span>\<span class="p">(</span><span class="n">N</span>\<span class="n">left</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span>\<span class="n">frac</span><span class="p">{</span><span class="mi">2</span><span class="p">}{</span><span class="n">m</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">entries</span><span class="w"> </span><span class="n">of</span><span class="w"> </span>\<span class="p">(</span><span class="n">B</span>\<span class="p">)</span>
<span class="n">are</span><span class="w"> </span><span class="n">generated</span><span class="w"> </span><span class="n">i</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="w"> </span><span class="n">from</span><span class="w"> </span>\<span class="p">(</span><span class="n">N</span>\<span class="n">left</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span>\<span class="n">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="n">m</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="o">.</span>
<span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="w"> </span>\<span class="n">ldots</span><span class="w"> </span><span class="n">n</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">    </span><span class="n">Randomly</span><span class="w"> </span><span class="n">sample</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span><span class="w"> </span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">y</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="n">set</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="n">Get</span><span class="w"> </span><span class="n">arbitrary</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">sigma</span>\<span class="p">)</span><span class="o">-</span><span class="n">approximation</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">gradient</span><span class="p">:</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">G</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="p">)}</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">B</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">infty</span><span class="p">}{</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">k</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">nabla_</span><span class="p">{</span><span class="n">W</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)}}</span><span class="w"> </span>\<span class="n">ell</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">y</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span><span class="w"> </span><span class="n">f</span><span class="o">^</span><span class="p">{</span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">RNN</span><span class="p">},</span><span class="w"> </span><span class="n">W</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)}}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">z</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span><span class="p">),</span><span class="w"> </span>\<span class="n">sigma</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="n">Update</span><span class="w"> </span><span class="n">weights</span><span class="p">:</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">W</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="p">)}</span><span class="o">=</span><span class="n">W</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)}</span><span class="o">-</span>\<span class="n">eta</span><span class="w"> </span><span class="n">G</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="p">)}</span>\<span class="p">)</span><span class="o">.</span>
<span class="n">end</span>
</code></pre></div>

<p>We begin by stating theorem $2^{9}$ in Wang et al. (2021) with our notations:
Theorem 3. Let $\delta&gt;0$, and assume we run algorithm 1 for $n$ iterations with learning rate $\eta=\frac{1}{m \sqrt{n}}$. Then there exists $m^{\star}=\operatorname{poly}\left(n, \delta^{-1}, T\right)$ such that if $m&gt;m^{\star}$ then for any $h \in \mathcal{H}_{\phi(T, \psi, N)}$ with probability at least $1-\delta$ over the randomness in algorithm 1, the following hold:</p>
<p>$$
\mathbb{E}<em t="d">{\mathbf{x}}\left[\left(\frac{1}{n(T-d)}\right) \sum</em>\right)
$$}^{T} \sum_{i=1}^{n} l_{0-1}\left(y_{t}, f_{t}^{R N N, W^{(i)}}(\mathbf{z})\right)\right]&lt;\tilde{O}\left(\frac{\phi(T, \psi, N)}{\sqrt{n}}\right)+O\left(\frac{\log \frac{1}{\delta}}{\sqrt{n}</p>
<p>where $W^{[i]}$ denote the output of algorithm 1 at the $i$ 'th iteration and $l_{0-1}$ is the zero-one loss.
Clearly when using infinite precision SGD theorem 2 follows from theorem 3 and lemma 1 by simple algebraic manipulations. Therefore, for proving theorem 2 with finite precision gradient based optimization, it is enough to modify theorem's 3 proof to analyze algorithm 2 that uses finite precision gradients, instead of algorithm 1 that uses full precision gradients.
While complicated, Wang et al. (2021)'s proof for theorem 3 can be divided into two high-level arguments. The first argument measure the complexity of the learned hypothesis class with respect to random initialization of $f^{\mathrm{RNN}}$ (lemma 6). And the second argument is a generalization bound for algorithm 1 with networks that are overparameterized enough. Since the first argument is independent of the gradients, the proof of the first argument still holds and we only need to prove a generalization bound for algorithm 2. More specifically we only need to prove a lemma that is equivalent to lemma 14 in Wang et al. (2021) and the rest of the second argument (lemma 7 in Wang et al. (2021)) remain unchanged.
Lemma 4. Let $n \in \mathbb{N}$, and denote by $L_{i}(W):=l\left(\mathbf{y}^{(i)}, f^{R N N, W}\left(\mathbf{z}^{(i)}\right)\right)$ the training loss. Suppose there exists $W^{\star} \in \mathcal{B}^{10}\left(W^{(0)}, \frac{R}{\sqrt{m}}\right)$ such that $R=O(\operatorname{poly}(T))=\Omega\left(T^{16}\right)$ and $L_{i}\left(W^{\star}\right) \leq \frac{1+R^{2}}{n}$. Then for any $\delta&gt;0$ there exists $m^{\star}=\operatorname{poly}\left(n, R, T, \delta^{-1}\right)$ such that if $m&gt;m^{\star}$ then with probability at least $1-\delta$ algorithm 2 with $\eta=\frac{1}{m \sqrt{n}}$ and finite precision $\sigma=O\left(\frac{1}{m}\right)$ will output:</p>
<p>$$
\mathbb{E}<em t="d">{\mathbf{x}}\left[\left(\frac{1}{n(T-d)}\right) \sum</em>\right)
$$}^{T} \sum_{i=1}^{n} l_{0-1}\left(y_{t}, f_{t}^{R N N, W^{(i)}}(\mathbf{z})\right)\right]&lt;O\left(\frac{R^{2}+\log \frac{1}{\delta}}{\sqrt{n}</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Proof. We begin by showing that with high probability over the initialization of $f^{\mathrm{RNN}}$, at any iteration $i \leq n$ of algorithm 2 , the distance of the learned hidden weights matrix $W^{(i)}$ from its initialization point $W^{(0)}$ is not too large. As a results we will get that the assumption of lemma 8 uphold, and therefore its upper bound of the deviation from linearization is valid.
By the triangle inequality for any $0 \leq i&lt;n$ we have that:</p>
<p>$$
\left|W^{(i+1)}-W^{(0)}\right|<em k="0">{F} \leq \sum</em>
$$}^{i}\left|W^{(k+1)}-W^{(k)}\right|_{F</p>
<p>Substituting algorithm 2 update rule for $W^{(k+1)}$, we get that there exist $\left|\sigma_{i}\right|_{\infty}&lt;\sigma$ such that:</p>
<p>$$
W^{(k+1)}-W^{(k)}=-\eta\left(\nabla_{W^{(k)}} \ell\left(\mathbf{y}<em k="k">{k}, f^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}</em>\right)
$$}\right)\right)+\sigma_{k</p>
<p>Now, explicitly writing $\nabla_{W^{(k)}} \ell\left(\mathbf{y}<em k="k">{k}, f^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}</em>\right)\right)$ with the chain rule we have that:</p>
<p>$$
\nabla_{W^{(k)}} \ell\left(\mathbf{y}<em k="k">{k}, f^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}</em>}\right)\right)=\frac{1}{T-d}\left(\sum_{t=d}^{T}\left(\frac{-y_{t}^{(k)} \cdot e^{-y_{t}^{(k)} f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z<em t="t">{k}\right)}}{1+e^{-y</em>}^{(k)} f_{t}^{\mathrm{RNN}, W}\left(\mathbf{z<em W_k_="W^{(k)">{k}\right)}}\right) \cdot\left(\nabla</em>\right)\right)\right)
$$}} f_{t}^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}_{k</p>
<p>and since $0 \leq \frac{x}{1+x} \leq 1$ for any $x \geq 0$, we conclude that:</p>
<p>$$
\left|\nabla_{W^{(k)}} \ell\left(\mathbf{y}<em k="k">{k}, f^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}</em>\right)\right)\right|<em T="T" _leq="\leq" d="d" t="t">{F} \leq \max </em>}\left|\nabla_{W^{(k)}} f_{t}^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z<em F="F">{k}\right)\right|</em>
$$</p>
<p>Now we will use an induction over $i$, to show that $\left|W^{(i+1)}-W^{(0)}\right|<em T="T" _leq="\leq" d="d" t="t">{F}=O\left(\frac{(i+1) T^{8}}{m \cdot \sqrt{n}}\right)$ for any $0 \leq i&lt;n$. By the induction hypothesis, lemma 10 assure us that for wide enough networks $m^{\star}=\Omega\left(\max \left{T^{6} \log ^{3}\left(\frac{n \cdot T}{2}\right), \sqrt{n} T^{8}\right}\right)$, with probability of at least $1-\delta$ over the initialization of $f^{\mathrm{RNN}}, \max </em>}\left|\nabla_{W^{(k)}} f_{t}^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z<em F="F">{k}\right)\right|</em>\right)$. Therefore, in this case}=O\left(T^{8</p>
<p>$$
\left|W^{(k+1)}-W^{(k)}\right|_{F}=\eta O\left(T^{8}+\frac{1}{m}\right)=O\left(\frac{T^{8}}{m \cdot \sqrt{n}}+\frac{1}{m^{2} \sqrt{n}}\right)
$$</p>
<p>and hence $\left|W^{(i+1)}-W^{(0)}\right|_{F}=O\left(\frac{(i+1) T^{8}}{m \cdot \sqrt{n}}\right)$ as required.
Now after we showed the assumptions of lemma 8 upholds, we can use it to obtain first order Taylor approximation of the training loss:</p>
<p>$$
\begin{aligned}
L_{i}\left(W^{(i)}\right)-L_{i}\left(W^{\star}\right) &amp; \leq\left\langle\nabla_{W^{(i)}} \ell\left(\mathbf{y}<em i="i">{i}, f^{\mathrm{RNN}, W^{(i)}}\left(\mathbf{z}</em>\right\rangle \
&amp; +\max }\right)\right), W^{(i)}-W^{\star<em t="t">{d \leq t \leq T} \underbrace{\left|\frac{y</em>} \cdot e^{-y_{t} f^{\mathrm{RNN}, W}\left(\mathbf{z<em t="t">{i}\right)}}{1+e^{-y</em>} f^{\mathrm{RNN}, W}\left(\mathbf{z<em 1="1" _leq="\leq">{i}\right)}}\right|}</em>
\end{aligned}
$$} \cdot O\left(\left(\frac{R}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} \sqrt{m}(\log m)\right)\left|W^{(i)}-W^{\star}\right|_{F</p>
<p>Where we assumed that $m^{\star}&gt;n$ and therefore $\left|W^{(i)}-W^{(0)}\right|_{F}&lt;O\left(\frac{R}{\sqrt{m}}\right)$.
Using algorithm 2 update rule for $W^{(i+1)}$ again (see eq 26), we can use an inequality from in ShalevShwartz \&amp; Ben-David (2014)'s lemma 14.1 to get that:</p>
<p>$$
\begin{array}{r}
\sum_{i=1}^{n}\left\langle\nabla_{W^{(i)}} \ell\left(\mathbf{y}<em i="i">{i}, f^{\mathrm{RNN}, W^{(i)}}\left(\mathbf{z}</em>\right\rangle \
\leq \frac{\left|W^{(1)}-W^{\star}\right|}\right)\right)+\sigma_{i}, W^{(i)}-W^{\star<em i="1">{F}^{2}}{2 \eta}+\frac{\eta}{2} \sum</em>}^{n}\left|\nabla_{W^{(i)}} \ell\left(\mathbf{y<em i="i">{i}, f^{\mathrm{RNN}, W^{(i)}}\left(\mathbf{z}</em>
\end{array}
$$}\right)\right)+\sigma_{i}\right|_{F}^{2</p>
<p>Now combing with Cauchy-Schwarz inequality we have that:</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{n}\left(L_{i}\left(W^{(i)}\right)-L_{i}\left(W^{\star}\right)\right) &amp; \leq \frac{\left|W^{(1)}-W^{\star}\right|<em i="1">{F}^{2}}{2 \eta}+\frac{\eta}{2} \sum</em>}^{n}\left|\nabla_{W^{(k)}} \ell\left(\mathbf{y<em k="k">{k}, f^{\mathrm{RNN}, W^{(k)}}\left(\mathbf{z}</em>\right|}\right)\right)+\sigma_{k<em i="1">{F}^{2} \
&amp; +O\left(\left(\frac{R}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} \sqrt{m}(\log m)\right)\left(\sum</em>\right|}^{n}\left|W^{(i)}-W^{\star<em i="1">{F}\right) \
&amp; -\sum</em>\right\rangle
\end{aligned}
$$}^{n}\left\langle\sigma_{i}, W^{(i)}-W^{\star</p>
<p>Substituting the upper bounds from eqs 28,29 and using the assumption that $R=\Omega\left(T^{16}\right)$ we get that:</p>
<p>$$
\begin{gathered}
\sum_{i=1}^{n}\left(L_{i}\left(W^{(i)}\right)-L_{i}\left(W^{\star}\right)\right) \leq \frac{O\left(R^{2}+T^{16}\right)}{2 \eta m}+\frac{\eta \cdot n}{2} O\left(T^{8}+1\right)^{2} \
+O\left(\left(\frac{R}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} n(\log m)\right)\left(R+n T^{8}\right)+O\left(m^{-\frac{3}{2}}\left(n^{2} T^{8}+n R\right)\right) \
\leq O\left(R^{2} \sqrt{n}\right)+O\left(\left(\frac{R}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} n(\log m)\right)\left(R+n T^{8}\right)+O\left(m^{-\frac{3}{2}} n^{2} R\right)
\end{gathered}
$$</p>
<p>To ensure the left hand side is upper bounded by $O\left(R^{2} \sqrt{n}\right)$ we will chose $m^{\star}$ such that $\frac{m^{\star}}{\left(\log ^{3} m^{\star}\right)}&gt;n^{\frac{3}{2}}$, note that, as required, $m^{\star}$ is polynomial in $n, T$. Then for $m&gt;m^{\star}$ we have that $\sum_{i=1}^{n} L_{i}\left(W^{(i)}\right) \leq O\left(R^{2} \sqrt{n}\right)$. Therefore,</p>
<p>$$
\frac{1}{n} \sum_{i=1}^{n} L_{i}\left(W^{(i)}\right) \leq O\left(\frac{R^{2}}{\sqrt{n}}\right)
$$</p>
<p>Now, to prove generalization bound we will follow lemma 4.3 in Ji \&amp; Telgarsky (2020) and use a martingale Bernstein bound argument. We begin by showing that during the whole training process, our binary cross entropy loss is bounded. Indeed lemma 9 assure us that :</p>
<p>$$
\max <em T="T" _leq="\leq" d_t="d&lt;t">{\mathbf{x} \in{0,1}^{d}} \max </em>\right)
$$}\left|f_{t}^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right|=O\left(T^{14} \cdot \sqrt{\frac{n}{m}}+T\right) \leq O\left(T^{14</p>
<p>Therefore, their exist a constant $C&gt;0$ such that the binary cross entropy loss is bounded by $\log \left(1+e^{O\left(T^{14}\right)}\right) \leq C \cdot T^{14}$.
Now, we will define a bounded martingle. For any $i \geq 0$, let $s_{i}$ denote $\left(\mathbf{x}<em i="i">{i}, \mathbf{y}</em>\right)$. Importantly, the quantity}\right)$ and $s_{0, i}$ denote $\left(s_{0}, \ldots, s_{i</p>
<p>$$
\frac{1}{C \cdot T^{14}}\left(\sum_{t&lt;i}\left(\mathbb{E}<em t="t">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(t)}}(\mathbf{z})\right)\right]-l\left(\mathbf{y}</em>\right)\right)\right)\right)
$$}, f^{\mathrm{RNN}, W^{(t)}}\left(\mathbf{z}_{t</p>
<p>is a martingal w.r.t the filration $\sigma\left(s_{0, i-1}\right)$. This martingal difference sequence is given by</p>
<p>$$
\frac{1}{C \cdot T^{14}}\left(\mathbb{E}<em t="t">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(t)}}(\mathbf{z})\right)\right]-l\left(\mathbf{y}</em>\right)\right)\right) \leq 1
$$}, f^{\mathrm{RNN}, W^{(t)}}\left(\mathbf{z}_{t</p>
<p>Moreover, we have</p>
<p>$$
\begin{gathered}
\mathbb{E}<em 0_="0," t="t">{s</em>}}\left[\frac{1}{C^{2} \cdot T^{28}}\left(\mathbb{E<em t="t">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(t)}}(\mathbf{z})\right)\right]-l\left(\mathbf{y}</em>}, f^{\mathrm{RNN}, W^{(t)}}\left(\mathbf{z<em 0_="0," i-1="i-1">{t}\right)\right)\right)^{2} \mid \sigma\left(s</em>\right)\right] \
=\frac{1}{C^{2} \cdot T^{28}}\left(\mathbb{E}<em 0_="0," t="t">{s</em>}}\left[l\left(\mathbf{y<em t="t">{t}, f^{\mathrm{RNN}, W^{(t)}}\left(\mathbf{z}</em>}\right)\right)^{2} \mid \sigma\left(s_{0, i-1}\right)\right]-\mathbb{E<em s__0_="s_{0," t="t">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(t)}}(\mathbf{z})\right)\right]^{2}\right) \
\leq \mathbb{E}</em>}}\left[\frac{1}{C \cdot T^{14}} l\left(\mathbf{y<em t="t">{t}, f^{\mathrm{RNN}, W^{(t)}}\left(\mathbf{z}</em>\right)\right] \
=\frac{1}{C \cdot T^{14}} \cdot \mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(t)}}(\mathbf{z})\right)\right]
\end{gathered}
$$}\right)\right) \mid \sigma\left(s_{0, i-1</p>
<p>Therefore, by lemma C. 2 in Ji \&amp; Telgarsky (2020) we have that with probability $1-\delta$</p>
<p>$$
\begin{gathered}
\frac{1}{C \cdot T^{14}} \sum_{i=1}^{n}\left(\mathbb{E}<em i="i">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right]-l\left(\mathbf{y}</em>}, f^{\mathrm{RNN}, W^{(i)}}\left(\mathbf{z<em i="1">{i}\right)\right)\right) \
\leq \frac{1}{C \cdot T^{14}}(e-2) \cdot \sum</em>\right)
\end{gathered}
$$}^{n} \mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right]+\ln \left(\frac{1}{\delta</p>
<p>And hence</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{n} \mathbb{E}<em i="1">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right] &amp; \leq \frac{1}{(3-e)} \sum</em>}^{n} l\left(\mathbf{y<em i="i">{i}, f^{\mathrm{RNN}, W^{(i)}}\left(\mathbf{z}</em>\right) \
&amp; =O\left(R^{2} \sqrt{n}\right)+O\left(R \ln \left(\frac{1}{\delta}\right)\right)
\end{aligned}
$$}\right)\right)+O\left(T^{14}\right) \ln \left(\frac{1}{\delta</p>
<p>Finally, for $y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})&lt;0$ we have that $\log \left(1+e^{-y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}\right)&gt;\log 2$. In addition, clearly $\log \left(1+e^{-y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}\right)&gt;0$. Therefore, we conclude that $\frac{1}{T-d} \sum_{t=d}^{T} l_{o-1}\left(y_{t}, f_{t}^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)&lt;$ $\frac{1}{\log 2} l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)$ and thus eq 24 uphold.</p>
<h1>D EXTENSION FOR GD WITH FINITE PRECISION</h1>
<p>In this section, we prove theorem 2 from the main text still holds when using the gradient descent based algorithm 3, instead of the stochastic gradient descent based algorithm 2. . Establishing our positive results that the parities task is efficiently learnable with sub-task decomposition supervision in the exact same setting of the negative results that show that learning is impossible without intermediate supervision, presented in section F. We will follow the proof in section C while taking into account the full non-stochastic gradients.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 3: Training \(f^{\mathrm{RNN}}\) with finite precision GD
Data: Data set \(\mathcal{D}\), learning rate \(\eta\), finite precision \(\sigma\).
Initialization: The entries of \(W^{(0)}, A, M_{0}\) are generated i.i.d. from \(N\left(0, \frac{2}{m}\right)\). The entries of \(B\)
    are generated i.i.d. from \(N\left(0, \frac{1}{m}\right)\).
for \(i=1,2,3 \ldots n\) do
    Get arbitrary \(\sigma\)-approximation of the gradient:
    \(G^{(i)} \in \mathcal{B}_{\infty}{ }^{11}\left(\mathbb{E}_{\mathbf{x}}\left[\nabla_{W^{(i-1)}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i-1)}}(\mathbf{z})\right)\right], \sigma\right)\).
    Update weights:
    \(W^{(i)}=W^{(i-1)}-\eta G^{(i)}\).
end
</code></pre></div>

<p>We begin by sampling a fake training set denoted by $\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}, \mathbf{y}^{(1)}, \ldots, \mathbf{y}^{(n)}$. Essentially, we will apply the same reasoning as in section $C$ with this fake training points. As in the finite precision SGD case it is enough to prove a lemma that is equivalent to lemma 4 in section C.
Lemma 5. Let $n \in \mathbb{N}$, and denote by $L_{i}(W):=l\left(\mathbf{y}^{(i)}, f^{\text {RNN }, W}\left(\mathbf{z}^{(i)}\right)\right)$ the training loss. Suppose there exists $W^{\star} \in \mathcal{B}^{12}\left(W^{(0)}, \frac{R}{\sqrt{m}}\right)$ such that $R=O(\operatorname{poly}(T))=\Omega\left(T^{16}\right)$ and $L_{i}\left(W^{\star}\right) \leq \frac{1+R^{2}}{n}$. Then for any $\delta&gt;0$ there exists $m^{\star}=\operatorname{poly}\left(n, R, T, \delta^{-1}\right)$ such that if $m&gt;m^{\star}$ then with probability at least $1-\delta$ algorithm 3 with $\eta=\frac{1}{m \sqrt{n}}$ and finite precision $\sigma=O\left(\frac{1}{m}\right)$ will output:</p>
<p>$$
\mathbb{E}<em _ell="d">{\mathbf{x}}\left[\left(\frac{1}{n(T-d)}\right) \sum</em>\right)
$$}^{T} \sum_{i=1}^{n} l_{0-1}\left(y_{t}, f_{t}^{\text {RNN }, W^{(i)}}(\mathbf{z})\right)\right]&lt;O\left(\frac{R^{2}+\log \frac{1}{\delta}}{\sqrt{n}</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Proof. We begin by showing that there exists $\tilde{W} \in \mathcal{B}\left(W^{(0)}, \frac{T^{8}}{\sqrt{m}}\right)$ such that:</p>
<p>$$
\mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, \tilde{W}}(\mathbf{z})\right)\right] \leq O\left(\frac{R^{2}}{\sqrt{n}}\right)
$$</p>
<p>Indeed, since the minimum can not be larger than the mean, eq'50 in the proof of lemma 4 assure us that under the assumptions of lemma 5 , for $m&gt;\max \left{n^{2}, \ln ^{4}\left(\frac{1}{\delta}\right)\right}$, with probability of at least $1-\delta$ algorithm 2 will reach such $\tilde{W}$ during the first $\max \left{n, \ln ^{2}\left(\frac{1}{\delta}\right)\right}$ SGD iteration.
Now we shows that that with high probability over the initialization of $f^{\mathrm{RNN}}$, at any iteration $i \leq n$ of algorithm 3, the distance of the learned hidden weights matrix $W^{(i)}$ from its initialization point $W^{(0)}$ is not too large. As a results we will get that the assumption of lemma 8 uphold, and therefore its upper bound of the deviation from linearization is valid.
By the triangle inequality for any $0 \leq i&lt;n$ we have that:</p>
<p>$$
\left|W^{(i+1)}-W^{(0)}\right|<em k="0">{F} \leq \sum</em>
$$}^{i}\left|W^{(k+1)}-W^{(k)}\right|_{F</p>
<p>Substituting algorithm 3 update rule for $W^{(k+1)}$, we get that there exist $\left|\sigma_{i}\right|_{\infty}&lt;\sigma$ such that:</p>
<p>$$
W^{(k+1)}-W^{(k)}=-\eta\left(\mathbb{E}<em W_k_="W^{(k)">{\mathbf{x}}\left[\nabla</em>\right)
$$}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right)\right]+\sigma_{k</p>
<p>Now, explicitly writing $\nabla_{W^{(k)}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right)$ with the chain rule we have that:</p>
<p>$$
\nabla_{W^{(k)}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right)=\frac{1}{T-d}\left(\sum_{t=d}^{T}\left(\frac{-y_{t} \cdot e^{-y_{t} f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}}{1+e^{-y_{t} f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}}\right) \cdot\left(\nabla_{W^{(k)}} f_{t}^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right)\right)
$$</p>
<p>and since $0 \leq \frac{x}{1+x} \leq 1$ for any $x \geq 0$, we conclude by Jensen's inequality that:</p>
<p>$$
\left|\mathbb{E}<em W_k_="W^{(k)">{\mathbf{x}}\left[\nabla</em>)\right)\right]\right|}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(k)}}(\mathbf{z<em t="d">{F} \leq \frac{1}{T-d} \sum</em>}^{T} \mathbb{E<em W_k_="W^{(k)">{\mathbf{x}}\left[\left|\nabla</em>\right]
$$}} f_{t}^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right|_{F</p>
<p>Now we will use an induction over $i$, to show that $\left|W^{(i+1)}-W^{(0)}\right|<em _mathbf_x="\mathbf{x">{F}=O\left(\frac{(i+1) T^{8}}{m \cdot \sqrt{n}}\right)$ for any $0 \leq i&lt;n$. By the induction hypothesis, lemma 10 assure us that for wide enough networks $m^{\star}=\Omega\left(\max \left{T^{7} \log ^{3}\left(\frac{n \cdot T}{\delta}\right), \sqrt{n} T^{8}\right}\right)$, with probability of at least $1-\delta$ over the initialization of $f^{\mathrm{RNN}}, \mathbb{E}</em>\right)$ for any $d \leq t \leq T$. Therefore, in this case}}\left[\left|\nabla_{W^{(k)}} f_{t}^{\mathrm{RNN}, W^{(k)}}(\mathbf{z})\right|_{F}\right]=O\left(T^{8</p>
<p>$$
\left|W^{(k+1)}-W^{(k)}\right|_{F}=\eta O\left(T^{8}+1\right)=O\left(\frac{T^{8}}{m \cdot \sqrt{n}}+\frac{1}{m^{2} \sqrt{n}}\right)
$$</p>
<p>and hence $\left|W^{(i+1)}-W^{(0)}\right|_{F}=O\left(\frac{(i+1) T^{8}}{m \cdot \sqrt{n}}\right)$ as required.
Now after we showed the assumptions of lemma 11 upholds, we can use it to obtain first order Taylor approximation of the training loss for any $\mathbf{x}$ :</p>
<p>$$
\begin{aligned}
&amp; l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)-l\left(\mathbf{y}, f^{\mathrm{RNN}, \tilde{W}}(\mathbf{z})\right) \leq\left\langle\nabla_{W^{(i)}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right), W^{(i)}-\tilde{W}\right\rangle \
&amp; \quad+\max <em t="t">{d \leq t \leq T} \underbrace{\left|\frac{y</em>} \cdot e^{-y_{t} f^{\mathrm{RNN}, W}(\mathbf{z})}}{1+e^{-y_{t} f^{\mathrm{RNN}, W}(\mathbf{z})}}\right|<em F="F">{\leq 1} \cdot O\left(\left(\frac{T^{8}}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} \sqrt{m}(\log m)\right)\left|W^{(i)}-\tilde{W}\right|</em>
\end{aligned}
$$</p>
<p>Where we assumed that $m^{\star}&gt;n$ and therefore $\left|W^{(i)}-W^{(0)}\right|_{F}&lt;O\left(\frac{T^{8}}{\sqrt{m}}\right)$.</p>
<p>Using algorithm 3 update rule for $W^{(k+1)}$ again (see eq 55), we can use an inequality from ShalevShwartz \&amp; Ben-David (2014)'s section 14.1.1 to get that</p>
<p>$$
\begin{array}{r}
\sum_{i=1}^{n}\left\langle\mathbb{E}<em W_i_="W^{(i)">{\mathbf{x}}\left[\nabla</em>\right\rangle \
\frac{\left|W^{(1)}-\tilde{W}\right|}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right]+\sigma_{i}, W^{(i)}-\tilde{W<em i="1">{F}^{2}}{2 \eta}+\frac{\eta}{2} \sum</em>}^{n}\left|\mathbb{E<em W_i_="W^{(i)">{\mathbf{x}}\left[\nabla</em>
\end{array}
$$}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right]+\sigma_{i}\right|_{F}^{2</p>
<p>Now, we can take expectation over eq 59, and combine the Cauchy-Schwarz inequality with the above bound and get that:</p>
<p>$$
\begin{aligned}
&amp; \sum_{i=1}^{n} \mathbb{E}<em F="F">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)-l\left(\mathbf{y}, f^{\mathrm{RNN}, \tilde{W}}(\mathbf{z})\right)\right] \leq \frac{\left|W^{(1)}-\tilde{W}\right|</em> \
&amp; \quad+\frac{\eta}{2} \sum_{i=1}^{n}\left|\mathbb{E}}^{2}}{2 \eta<em W_i_="W^{(i)">{\mathbf{x}}\left[\nabla</em>\right|}} \ell\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right]+\sigma_{i<em i="1">{F}^{2}+O\left(\left(\frac{T^{8}}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} \sqrt{m}(\log m)\right)\left(\sum</em>\right)
\end{aligned}
$$}^{n}\left|W^{(i)}-\tilde{W}\right|_{F</p>
<p>Substituting the upper bounds from eqs 57, 58 we get that:</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{n} \mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right. &amp; \left.-l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{*}}(\mathbf{z})\right)\right] \leq \frac{O\left(T^{16}\right)}{2 \eta m}+\frac{\eta \cdot n}{2} O\left(T^{8}+1\right)^{2} \
&amp; +O\left(\left(\frac{T^{8}}{\sqrt{m}}\right)^{\frac{1}{3}} T^{10} n(\log m)\right)\left(2 n T^{8}\right)+O\left(m^{-\frac{3}{2}}\left(n^{2} T^{8}\right)\right) \
&amp; \leq O\left(T^{16} \sqrt{n}\right)+O\left(m^{-\frac{1}{6}} T^{\frac{n 2}{3}} n^{2}(\log m)\right)+O\left(m^{-\frac{3}{2}} n^{2} T^{8}\right)
\end{aligned}
$$</p>
<p>To ensure the left hand side is upper bounded by $O\left(R^{2} \sqrt{n}\right)$ we will chose $m^{\star}$ such that $\frac{m^{\star}}{\left(\log ^{2} m^{\star}\right)}&gt;n^{\frac{6}{2}}$, note that, as required, $m^{\star}$ is polynomial in $n, T$. Then since eq 53 assure us that $\mathbb{E}<em i="1">{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, \tilde{W}}(\mathbf{z})\right)\right] \leq O\left(\frac{R^{2}}{\sqrt{n}}\right)$, we have that $\sum</em>$. Therefore,}^{n} \mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right] \leq O\left(R^{2} \sqrt{n}\right)$ for $m&gt;m^{\star</p>
<p>$$
\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}_{\mathbf{x}}\left[l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)\right] \leq O\left(\frac{R^{2}}{\sqrt{n}}\right)
$$</p>
<p>Finally, for $y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})&lt;0$ we have that $\log \left(1+e^{-y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}\right)&gt;\log 2$. In addition, clearly $\log \left(1+e^{-y_{t} \cdot f_{t}^{\mathrm{RNN}, W}(\mathbf{z})}\right)&gt;0$. Therefore, we conclude that $\frac{1}{T-d} \sum_{t=d}^{T} l_{o-1}\left(y_{t}, f_{t}^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)&lt;$ $\frac{1}{\log 2} l\left(\mathbf{y}, f^{\mathrm{RNN}, W^{(i)}}(\mathbf{z})\right)$ and thus eq 52 uphold.</p>
<h1>E Learnability Of RNNs</h1>
<p>In this section we state and extend several lemmas from Wang et al. (2021) and Allen-Zhu et al. (2019). We will use this lemmas in sections C,D for extending theorem 2 in the main text for finite precision SGD and GD.</p>
<p>Following Wang et al. (2021)'s notations, for any target function $h \in \mathcal{H}<em i="1">{\phi(T, \psi, N)}$ and $n$ samples $\left(\mathbf{z}^{(i)}\right)</em>$ we will denote:}^{n</p>
<p>$$
\mathbf{H}<em W_0_="W^{(0)">{i, j}^{t}:=\frac{1}{m}\left\langle\nabla</em>\right)\right\rangle
$$}} f_{t}^{\mathrm{RNN}, W^{(0)}}\left(\mathbf{z}^{(i)}\right), \nabla_{W^{(0)}} f_{t}^{\mathrm{RNN}, W^{(0)}}\left(\mathbf{z}^{(j)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ The ball is with respect to the distance that defined by the max matrix norm, i.e. the elementwise distances are at most $\sigma$.
${ }^{12}$ The ball is with respect to the distance that defined by the Frobenius matrix norm.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
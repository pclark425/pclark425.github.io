<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4360 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4360</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4360</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-267627721</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.06913v1.pdf" target="_blank">TL;DR Progress: Multi-faceted Literature Exploration in Text Summarization</a></p>
                <p><strong>Paper Abstract:</strong> This paper presents TL;DR Progress, a new tool for exploring the literature on neural text summarization. It organizes 514~papers based on a comprehensive annotation scheme for text summarization approaches and enables fine-grained, faceted search. Each paper was manually annotated to capture aspects such as evaluation metrics, quality dimensions, learning paradigms, challenges addressed, datasets, and document domains. In addition, a succinct indicative summary is provided for each paper, describing contextual factors, issues, and proposed solutions. The tool is available at {url{https://www.tldr-progress.de}}, a demo video at {url{https://youtu.be/uCVRGFvXUj8}}</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4360.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4360.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TL;DR PROGRESS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TL;DR PROGRESS: Multi-faceted Literature Exploration in Text Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive literature-explorer for neural text summarization that combines a manual fine-grained annotation scheme over 514 papers with automated contextual information extraction (indicative summaries, glossary, acronyms, and challenge clustering) using LLMs and clustering methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TL;DR PROGRESS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A web-based literature exploration system that (1) manually annotates 514 summarization papers with a multi-faceted annotation scheme (document representation, model training, summary generation, evaluation, and metadata), (2) generates indicative summaries per paper by combining these manual facets with automatically extracted contextual information (purpose, audience, downstream use, problems & solutions), (3) performs automatic terminology acquisition (glossary and acronym-expansion pairs), (4) clusters problem statements into challenge labels, and (5) exposes an interactive dashboard and figure browser. The automated extraction components ingest the introduction (and other text extracted from PDFs via Science Parse) and call an LLM with structured prompts to extract context factors, problem-solution pairs, glossary definitions, and acronym expansions; embeddings (Sentence-BERT) + UMAP + HDBSCAN are used to cluster problem statements across papers to synthesize common challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5 (OpenAI) - explicitly used for contextual extraction and terminology acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based extraction using structured prompts (Context Factors Prompt, Problems-and-Solutions Prompt, Glossary Prompt, Acronyms Prompt) applied to paper introductions; textual inputs are preprocessed by Science Parse; outputs are parsed into prefixed token formats.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Combine manual facet annotations with LLM-extracted contextual fields to form indicative summaries; embed extracted problem statements (Sentence-BERT), reduce dimensionality with UMAP, and perform soft clustering using HDBSCAN to aggregate and label common challenges across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>514 papers</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Neural text summarization (single-document, English)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Indicative abstractive summaries (contextualized by facets), glossary (concept:definition pairs), acronym-expansion pairs, list of clustered challenges, dashboard visualizations, figure/table browser entries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>User study relevance ratings (1-5 scale), usefulness scores per feature (1-5 scale); corpus-level statistics (e.g., ROUGE usage frequency) for meta-analysis; qualitative manual checks for extracted terms</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>User study (5 participants): both TL;DR PROGRESS and Semantic Scholar received mean relevance score of 4; 3/5 participants preferred TL;DR PROGRESS for literature review; feature usefulness: advanced search mean = 4.5, indicative summaries mean = 3.6. No comprehensive quantitative fidelity evaluation of LLM outputs was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Semantic Scholar (used as a comparative retrieval baseline in the user study)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Relevance: both tools scored 4; 3 out of 5 participants preferred TL;DR PROGRESS for literature review tasks; users rated advanced/faceted search in TL;DR PROGRESS as highly useful compared to typical keyword search workflows in Semantic Scholar.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompted LLMs (GPT-3.5) can effectively extract contextual factors and terminology from paper introductions to produce compact indicative summaries and glossaries; combining high-quality manual facet annotations with LLM outputs yields useful, skimmable overviews; embedding + clustering of extracted problem statements yields meaningful challenge groupings; facet-based retrieval significantly aids targeted literature discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Automated LLM extractions were only spot-checked for accuracy; no comprehensive faithfulness or hallucination evaluation was performed; reliance on introduction text may miss definitions/claims located elsewhere; current automation limited to summarization domain though scheme is extensible; potential hallucination and structure/quality issues with LLM outputs acknowledged.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors anticipate improved quality and structure when using larger/stronger models (e.g., GPT-4) but provide no empirical scaling study; system currently built on 514 papers and planned to be extended with automated annotation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TL;DR Progress: Multi-faceted Literature Exploration in Text Summarization', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4360.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4360.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PersLEARN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PersLEARN: Research training through the lens of perspective cultivation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A perspective-based literature exploration approach that lets researchers interact with a prompt-based model to develop viewpoints by identifying evidence from relevant papers and summarizing it to form new connections.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Perslearn: Research training through the lens of perspective cultivation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PersLEARN (perspective-based prompt model)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as a perspective-based exploration tool where users interact with a prompt-based model that identifies evidence from relevant papers related to user-provided seed perspectives and summarizes that evidence to help users form or expand viewpoints. The paper (cited in TL;DR PROGRESS) frames PersLEARN as empowering early-career researchers to develop perspectives via model-mediated evidence identification and summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based evidence identification from relevant papers (exact extraction architecture not specified in TL;DR PROGRESS)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Summarization of identified evidence to make new connections and support perspective development (aggregation of evidence summaries); details not provided in TL;DR PROGRESS.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature / research training (presented as a tool for literature exploration and perspective cultivation)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries of evidence tied to user perspectives; perspective-building assistance (viewpoint-oriented summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Framed as a tool that helps researchers develop viewpoints by surfacing and summarizing evidentiary material from relevant papers via interaction with a prompt-based model; highlighted as complementary to TL;DR PROGRESS's facet-based summarization specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Details on model choice, hallucination handling, scaling, and rigorous evaluation are not provided in the TL;DR PROGRESS text; thus limitations are unspecified here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TL;DR Progress: Multi-faceted Literature Exploration in Text Summarization', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Perslearn: Research training through the lens of perspective cultivation <em>(Rating: 2)</em></li>
                <li>Scilit: A platform for joint scientific literature discovery, summarization and citation generation <em>(Rating: 2)</em></li>
                <li>News summarization and evaluation in the era of GPT-3 <em>(Rating: 1)</em></li>
                <li>Autodive: An integrated onsite scientific literature annotation tool <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4360",
    "paper_id": "paper-267627721",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "TL;DR PROGRESS",
            "name_full": "TL;DR PROGRESS: Multi-faceted Literature Exploration in Text Summarization",
            "brief_description": "An interactive literature-explorer for neural text summarization that combines a manual fine-grained annotation scheme over 514 papers with automated contextual information extraction (indicative summaries, glossary, acronyms, and challenge clustering) using LLMs and clustering methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TL;DR PROGRESS",
            "system_description": "A web-based literature exploration system that (1) manually annotates 514 summarization papers with a multi-faceted annotation scheme (document representation, model training, summary generation, evaluation, and metadata), (2) generates indicative summaries per paper by combining these manual facets with automatically extracted contextual information (purpose, audience, downstream use, problems & solutions), (3) performs automatic terminology acquisition (glossary and acronym-expansion pairs), (4) clusters problem statements into challenge labels, and (5) exposes an interactive dashboard and figure browser. The automated extraction components ingest the introduction (and other text extracted from PDFs via Science Parse) and call an LLM with structured prompts to extract context factors, problem-solution pairs, glossary definitions, and acronym expansions; embeddings (Sentence-BERT) + UMAP + HDBSCAN are used to cluster problem statements across papers to synthesize common challenges.",
            "llm_model_used": "GPT-3.5 (OpenAI) - explicitly used for contextual extraction and terminology acquisition",
            "extraction_technique": "Prompt-based extraction using structured prompts (Context Factors Prompt, Problems-and-Solutions Prompt, Glossary Prompt, Acronyms Prompt) applied to paper introductions; textual inputs are preprocessed by Science Parse; outputs are parsed into prefixed token formats.",
            "synthesis_technique": "Combine manual facet annotations with LLM-extracted contextual fields to form indicative summaries; embed extracted problem statements (Sentence-BERT), reduce dimensionality with UMAP, and perform soft clustering using HDBSCAN to aggregate and label common challenges across papers.",
            "number_of_papers": "514 papers",
            "domain_or_topic": "Neural text summarization (single-document, English)",
            "output_type": "Indicative abstractive summaries (contextualized by facets), glossary (concept:definition pairs), acronym-expansion pairs, list of clustered challenges, dashboard visualizations, figure/table browser entries",
            "evaluation_metrics": "User study relevance ratings (1-5 scale), usefulness scores per feature (1-5 scale); corpus-level statistics (e.g., ROUGE usage frequency) for meta-analysis; qualitative manual checks for extracted terms",
            "performance_results": "User study (5 participants): both TL;DR PROGRESS and Semantic Scholar received mean relevance score of 4; 3/5 participants preferred TL;DR PROGRESS for literature review; feature usefulness: advanced search mean = 4.5, indicative summaries mean = 3.6. No comprehensive quantitative fidelity evaluation of LLM outputs was reported.",
            "comparison_baseline": "Semantic Scholar (used as a comparative retrieval baseline in the user study)",
            "performance_vs_baseline": "Relevance: both tools scored 4; 3 out of 5 participants preferred TL;DR PROGRESS for literature review tasks; users rated advanced/faceted search in TL;DR PROGRESS as highly useful compared to typical keyword search workflows in Semantic Scholar.",
            "key_findings": "Prompted LLMs (GPT-3.5) can effectively extract contextual factors and terminology from paper introductions to produce compact indicative summaries and glossaries; combining high-quality manual facet annotations with LLM outputs yields useful, skimmable overviews; embedding + clustering of extracted problem statements yields meaningful challenge groupings; facet-based retrieval significantly aids targeted literature discovery.",
            "limitations_challenges": "Automated LLM extractions were only spot-checked for accuracy; no comprehensive faithfulness or hallucination evaluation was performed; reliance on introduction text may miss definitions/claims located elsewhere; current automation limited to summarization domain though scheme is extensible; potential hallucination and structure/quality issues with LLM outputs acknowledged.",
            "scaling_behavior": "Authors anticipate improved quality and structure when using larger/stronger models (e.g., GPT-4) but provide no empirical scaling study; system currently built on 514 papers and planned to be extended with automated annotation pipelines.",
            "uuid": "e4360.0",
            "source_info": {
                "paper_title": "TL;DR Progress: Multi-faceted Literature Exploration in Text Summarization",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PersLEARN",
            "name_full": "PersLEARN: Research training through the lens of perspective cultivation",
            "brief_description": "A perspective-based literature exploration approach that lets researchers interact with a prompt-based model to develop viewpoints by identifying evidence from relevant papers and summarizing it to form new connections.",
            "citation_title": "Perslearn: Research training through the lens of perspective cultivation",
            "mention_or_use": "mention",
            "system_name": "PersLEARN (perspective-based prompt model)",
            "system_description": "Described as a perspective-based exploration tool where users interact with a prompt-based model that identifies evidence from relevant papers related to user-provided seed perspectives and summarizes that evidence to help users form or expand viewpoints. The paper (cited in TL;DR PROGRESS) frames PersLEARN as empowering early-career researchers to develop perspectives via model-mediated evidence identification and summarization.",
            "llm_model_used": null,
            "extraction_technique": "Prompt-based evidence identification from relevant papers (exact extraction architecture not specified in TL;DR PROGRESS)",
            "synthesis_technique": "Summarization of identified evidence to make new connections and support perspective development (aggregation of evidence summaries); details not provided in TL;DR PROGRESS.",
            "number_of_papers": null,
            "domain_or_topic": "General scientific literature / research training (presented as a tool for literature exploration and perspective cultivation)",
            "output_type": "Summaries of evidence tied to user perspectives; perspective-building assistance (viewpoint-oriented summaries)",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Framed as a tool that helps researchers develop viewpoints by surfacing and summarizing evidentiary material from relevant papers via interaction with a prompt-based model; highlighted as complementary to TL;DR PROGRESS's facet-based summarization specialization.",
            "limitations_challenges": "Details on model choice, hallucination handling, scaling, and rigorous evaluation are not provided in the TL;DR PROGRESS text; thus limitations are unspecified here.",
            "scaling_behavior": null,
            "uuid": "e4360.1",
            "source_info": {
                "paper_title": "TL;DR Progress: Multi-faceted Literature Exploration in Text Summarization",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Perslearn: Research training through the lens of perspective cultivation",
            "rating": 2,
            "sanitized_title": "perslearn_research_training_through_the_lens_of_perspective_cultivation"
        },
        {
            "paper_title": "Scilit: A platform for joint scientific literature discovery, summarization and citation generation",
            "rating": 2,
            "sanitized_title": "scilit_a_platform_for_joint_scientific_literature_discovery_summarization_and_citation_generation"
        },
        {
            "paper_title": "News summarization and evaluation in the era of GPT-3",
            "rating": 1,
            "sanitized_title": "news_summarization_and_evaluation_in_the_era_of_gpt3"
        },
        {
            "paper_title": "Autodive: An integrated onsite scientific literature annotation tool",
            "rating": 1,
            "sanitized_title": "autodive_an_integrated_onsite_scientific_literature_annotation_tool"
        }
    ],
    "cost": 0.010121249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TL;DR PROGRESS: Multi-faceted Literature Exploration in Text Summarization</p>
<p>Shahbaz Syed shahbaz.syed@uni-leipzig.de 
Leipzig University ‡ University of Groningen § ScaDS
AI</p>
<p>Khalid Al-Khatib 
Leipzig University ‡ University of Groningen § ScaDS
AI</p>
<p>Martin Potthast 
Leipzig University ‡ University of Groningen § ScaDS
AI</p>
<p>TL;DR PROGRESS: Multi-faceted Literature Exploration in Text Summarization
02626EC15C932B839F656C500CE67A9B
This paper presents TL;DR PROGRESS, a new tool for exploring the literature on neural text summarization.It organizes 514 papers based on a comprehensive annotation scheme for text summarization approaches and enables fine-grained, faceted search.Each paper was manually annotated to capture aspects such as evaluation metrics, quality dimensions, learning paradigms, challenges addressed, datasets, and document domains.In addition, a succinct indicative summary is provided for each paper, consisting of automatically extracted contextual factors, issues, and proposed solutions.The tool is available online at https://www.tldr-progress.de,a demo video at https://youtu.be/uCVRGFvXUj8.</p>
<p>Introduction</p>
<p>Research in the field of neural text summarization has evolved rapidly from the introduction of sequence-to-sequence (Sutskever et al., 2014;Rush et al., 2015) models to the era of transformers (Vaswani et al., 2017), greatly improving our ability to produce high-quality summaries in line with human preferences (Huang et al., 2020a;Goyal et al., 2022).As a result, the original focus of summarization research on the news domain has broadened to various other domains such as meetings, scientific papers, scripts and opinions.</p>
<p>To keep abreast of current advances, especially researchers new to the field must perform various tasks, including assimilating, organizing, annotating, and reviewing papers across multiple venues.Although search engines tailored to scholarly documents, such as Google Scholar, Semantic Scholar, DBLP, and the ACL anthology, provide access to a vast collection of articles, they merely support the discovery of relevant papers from within a multidomain collection and do not (strongly) support an in-depth comparative paper analysis.This paper introduces TL;DR PROGRESS, a literature explorer designed specifically for the text summarization literature.It contributes an intuitive annotation scheme designed to streamline finegrained, facet-based systematic reviews (Figure 1 and Section 3).To demonstrate the capabilities of our tool, we manually analyze a collection of 514 summarization papers and cover various important aspects for an efficient literature search (Section 4).</p>
<p>As part of its goal to organize summarization research, TL;DR PROGRESS provides an indicative summary for each paper, seamlessly integrating automatically extracted contextual information with manually annotated facets (Section 5).This includes identifying for what practical purpose a summarization approach is intended, the current challenges associated with summary generation, and a paper's contributions.Our tool also demonstrates the practical application of large language models (LLMs) in automatic terminology acquisition, involving the extraction of technical terms from papers, including glossary definitions for general concepts and acronym-expansion pairs to improve researchers' recall of specific papers (Section 6). 1 TL;DR PROGRESS has a dual function: it provides insights into current research and serves as a basis for future automation.In particular, our literature explorer shows a way forward for future research on large-scale systematic reviews of the NLP literature by extensively leveraging LLMs.</p>
<p>1 https://github.com/webis-de/eacl24-tldr-progress/arXiv:2402.06913v1[cs.CL] 10 Feb 2024</p>
<p>Paper aggregators such as Google Scholar, Semantic Scholar, DBLP, and the ACL Anthology provide access to a large number of papers from different disciplines and, most importantly, facilitate their discovery.However, these platforms lack solid support for in-depth comparative analysis.Per-sLEARN (Shi et al., 2023) introduces a perspectivebased approach to exploring scientific literature.It empowers early career researchers to develop their viewpoints by interacting with a prompt-based model.The tool identifies evidence from relevant papers that relate to the given seed perspectives and summarizes them to make new connections.In contrast, TL;DR PROGRESS focuses on summarization and provides fine-grained facets for each paper to enhance understanding of their contributions and content, along with indicative summaries, a feature not present in PersLEARN.</p>
<p>As for annotating papers, Autodive (Du et al., 2023) automates the in-place annotation of entities and relationships in PDFs, using external domainspecific NER models.In contrast, our approach includes a domain-specific annotation scheme and manual annotation for quality assurance.In addition, our tool facilitates unsupervised automatic terminology acquisition using LLMs.SciLit (Gu and Hahnloser, 2023) recommends relevant articles based on keywords entered by the user and generates citation sets with extracted highlights.While our tool supports keyword-based lexical search, it is less reliant on user-defined keywords due to its facet-based retrieval system.</p>
<p>Paperswithcode 2 is a platform that links papers with their code implementations.It provides an overview of the state-of-the-art in various NLP tasks.TL;DR PROGRESS complements Paperswithcode (for the summarization task) by providing an interactive dashboard presenting relevant statistics (Section 7) for a comprehensive understanding of the state-of-the-art.</p>
<p>Annotation Scheme</p>
<p>To create a comprehensive annotation scheme for summarization papers, we performed an in-depth analysis of the recent relevant literature.As shown in Figure 1, this scheme encapsulates the basic components of a neural summarization architecture, laying the foundation for a fine-grained annotation 2 https://paperswithcode.com/ tailored to individual contributions.Its underlying principle is to categorize contributions according to their main focus, as papers often address one or more components within the summarization pipeline.The annotation scheme distinguishes four components:</p>
<ol>
<li>Document representation.Conversion of a source document into a vector representation to model relationships between document units (words, sentences, paragraphs).This may include input data enrichment with user or style-specific information and model augmentation using external knowledge bases.</li>
</ol>
<p>Model training.</p>
<p>Training of a model with suitable data under a user-defined objective (or reward) function.This may include using pre-trained models for tasks, such as missing text prediction, paraphrasing, and detecting textual entailment.</p>
<ol>
<li>
<p>Summary generation.Generation of a summary based on the representation of its source document using a trained model.This may include selecting explicit units for inclusion, restricting the summary to a particular style, conditioning the generation process on certain aspects of the source document, and postprocessing steps such as length normalization and redundancy removal.</p>
</li>
<li>
<p>Evaluation.Evaluation setup such as the document domains and datasets used for testing, automatic evaluation metrics reported, and the human evaluation criteria for qualitative assessment of the generated summaries.</p>
</li>
</ol>
<p>These components encompass different facets.For example, document representation includes "input encoding", "unit relationship", "data augmentation", and "external knowledge".Definitions for each facet are given in Table 1.These facets are not mutually exclusive, i.e., a paper can contribute to several facets simultaneously.For example, a paper may present a novel input encoding scheme that explicitly models unit relationships in the source document.In such cases, we annotate the paper with multiple facets.The annotation scheme also includes metadata for each paper.Overall, our scheme enables a fine-grained retrieval of relevant summarization papers, a feature that is currently not available in other paper aggregators.</p>
<p>Facet Description / Examples</p>
<p>Document representation</p>
<p>Input encoding</p>
<p>The paper presents methods to improve the encoding of source documents (e.g., hierarchical/graphical attention, inclusion of discourse structure, etc.)</p>
<p>Unit relationship</p>
<p>The paper investigates methods that explicitly model the relationship between units in the source document, such as words, sentences, or passages.</p>
<p>Data augmentation</p>
<p>The paper introduces methods that use data augmentation techniques, e.g., to extract aspects, to create contrasting examples of robustness, or to overcome data scarcity in low-resource domains.</p>
<p>External knowledge</p>
<p>The paper investigates methods for integrating external knowledge using resources such as knowledge graphs, domain-specific vocabularies, or information from pre-trained language models.</p>
<p>Model training Learning Paradigm</p>
<p>Supervised, unsupervised, or reinforcement learning.</p>
<p>Objective Function</p>
<p>The paper introduces methods that incorporate new objective functions that emphasize diversity, faithfulness, or custom objectives appropriate to the task of summarization.</p>
<p>Auxiliary Tasks</p>
<p>The paper explores methods such as multi-task learning or pre-training on related tasks (e.g., textual entailment, paraphrasing, gap sentence prediction) to improve the summarization task.</p>
<p>Summary generation Unit Selection</p>
<p>The paper presents methods that explicitly select relevant units, such as words, sentences, or passages, for summarization, addressing the information loss associated with generating fixedlength summaries through techniques such as copying or pointing.</p>
<p>Controlled Generation</p>
<p>The paper presents methods that encourage the model to generate summaries with certain attributes (e.g., style, length, tone), for example, by providing additional textual guidance or limiting the model's vocabulary to a specific domain.</p>
<p>Post Processing</p>
<p>The paper explores methods for post-processing generated summaries to improve their quality.This includes re-ranking, re-writing or swapping certain text spans to achieve the desired goals.</p>
<p>Evaluation Domain</p>
<p>The domain of the source documents (e.g., opinions, screenplays, papers, etc.)</p>
<p>Dataset</p>
<p>The datasets used for training/evaluation (e.g., CNN/DailyMail, XSum, etc.)</p>
<p>Evaluation metric</p>
<p>The metrics used for automatic evaluation (e.g., ROUGE, BLEU, etc.)</p>
<p>Human evaluation</p>
<p>The summary quality criteria that were evaluated manually (e.g., informativeness, fluency, etc.) Metadata Paper type</p>
<p>A new method, analysis (evaluation), metric, dataset, or theory.</p>
<p>Venue / Year</p>
<p>Venue and year in which the work was published.</p>
<p>Table 1: Description of the annotation scheme shown in Figure 1.Pipeline components correspond to the three major components of the scheme, Document Representation, Model Training, and Summary Generation.</p>
<p>Webis Summarization Papers Corpus</p>
<p>To create TL;DR PROGRESS, we compiled a corpus of research papers on neural text summarization, annotated it according to our scheme, and analyzed the distribution of the papers across different dimensions.</p>
<p>Corpus Construction</p>
<p>To collect summarization papers, we conducted a keyword search ("summ") in the proceedings of the most important venues, including AAAI, AACL, ACL, CHIIR, CIKM, COLING, CONLL, EACL, ECIR, EMNLP, ICLR, IJCAI, IJCNLP, NAACL, NEURIPS, SIGIR, and TACL.The initial collection of 801 papers was refined through a careful review of titles and abstracts to identify papers that were directly relevant to single-document summarization of English texts.These included papers that evaluated or analyzed existing approaches and proposed new metrics, human assessment methodologies, meta-evaluations, datasets, and new model architectures.To extract textual content from the PDFs, we used Science Parse.3Papers that could not be automatically extracted or were duplicates were excluded, so that we ended up processing 514 papers.For each of the 514 papers, we performed a thorough manual annotation, focusing on the facets of our annotation scheme.The annotation was performed by one of the authors of this paper.The annotation process was iterative, with the annotator revisiting the previously annotated sections to ensure consistency.Another author reviewed the annotations to ensure their quality.</p>
<p>Corpus Statistics</p>
<p>Table 2 shows the distribution of papers across venues, with EMNLP and ACL emerging as the top venues for summarization research.Among the 514 papers, we observed the following distribution of paper types: 353 dealt with methods, 79 with analysis (including meta-evaluation and quality/model analysis), 73 were corpus-related, 61 focused on metrics and one on theory.The majority of the proposed models were trained using supervised learning (73%), compared to unsupervised (17%) and reinforcement learning (10%).</p>
<p>The different paper types were not mutually exclusive, so there were cases where a paper proposed a new dataset and applied methods to it at the same time.In terms of automatic evaluation, the ROUGE metric was used in 71.6% of papers, highlighting its widespread use for evaluating the quality of generated summaries in the field of single-document summarization of English texts.Only 39.5% of the papers included some form of manual evaluation.</p>
<p>In terms of reproducibility, we found that 58% of the papers published their code, indicating a slow but growing trend of code availability in this area compared to previous years.</p>
<p>Indicative Summaries of Papers</p>
<p>In contrast to informative summaries that aim to replace the entire paper, our tool provides indicative summaries that help users quickly decide if a paper is relevant to their information need.Our indicative summaries are unique in that they encompass an abstractive summary of the paper as well as multiple facets such as datasets, domains, evaluation metrics alongside other information.</p>
<p>Beyond Abstract as a Summary</p>
<p>Traditionally, the paper abstract serves the purpose of an informative summary (Luhn, 1958) or an ultra-short abstractive summary (Cachola et al., 2020) that outlines the major contributions.Yet, when dealing with a large collection of documents, these summaries fall short, as they do not enable fine-grained retrieval of relevant papers.Moreover, studies have shown that abstracts can introduce bias and may not offer a comprehensive representation of the paper's contents (Elkiss et al., 2008).</p>
<p>In contrast to informative summaries, which essentially substitute the source, indicative summaries serve as a roadmap for the contents of the source document (Mani, 2001).They aid readers in deciding whether they want to explore the source document in greater detail.Particularly in the context of literature reviews, indicative summaries provide an exploratory overview of papers, allowing researchers to quickly navigate and comprehend their contributions.TL;DR PROGRESS introduces a novel indicative summary that integrates manually annotated facets with automatically extracted contextual information.Motivated by the significance of considering contextual factors in summarization (Jones et al., 1999), we extract information related to: (1) the purpose of the generated summaries, (2) the target audience for the summaries, (3) the downstream application of the generated summaries, and (4) the problems and corresponding solutions presented in the paper.Figure 2 (Appendix) exemplifies an indicative summary generated by our tool.This summary distinctly outlines all the pertinent information that a reader would need to determine whether they wish to delve into the paper in more detail.</p>
<p>Contextual Information Extraction</p>
<p>We demonstrate the utilization of LLMs for the task of indicative summarization by extracting the contextual information described above through Context Factors Prompt (GPT3.5)You are a helpful assistant that can read and analyze scientific papers.You are given the following paper: {Introduction} Answer the following three questions: (1) Why are the authors generating the summaries of the documents?(2) Who are they for?(3) How will they be used?You must not include the proposed approach by the authors for generating the summaries.You will output a list of the question-answer pairs where each question is prefixed by the token QUESTION: and each answer is prefixed by the ANSWER: token.Each pair is separated by two lines.</p>
<p>Problems and Solutions Prompt (GPT3.5)You are a helpful assistant that can read and analyze scientific papers.You are given the following paper: {Introduction} Can you give me a list of the main problems tackled by the authors and their proposed solutions?In this list, each problem is described followed by a solution proposed by the authors.Each problem starts with the token PROBLEM and each solution starts with the token SOLUTION.</p>
<p>Here is the list: Table 4: Prompts for extracting contextual information from the introduction of a paper.This information is used to compose indicative summaries of papers.The specific instructions for controlling output format may not be required with newer models.</p>
<p>generative question-answering.To extract this information, we input the introduction section of the paper into the prompt.We devised two prompts corresponding to the context factors and problems and solutions (see Table 4).Each prompt poses specific questions related to the context, necessitating the generation of answers in a specific format using the relevant content from the paper.We employed GPT-3.5 for our experiments. 4 We conducted additional analysis of this contextual information to identify the frequently addressed challenges in text summarization.In particular, we employed a soft clustering approach (HDBSCAN (Campello et al., 2013)) on the set of problem statements. 5This process yielded 9 clusters, which we manually labeled with their respective challenges, as illustrated in Table 3.</p>
<p>Automatic Terminology Acquisition</p>
<p>Scientific terminology plays a vital role in research, requiring researchers to recall papers related to specific concepts or acronyms representing models/metrics.Moreover, previously defined terminology might be directly referenced in subsequent papers without detailed explanation (Ball et al., 2002) 4 https://platform.openai.com/docs/models/gpt-3-5 5 We clustered the contextual embeddings (Reimers and Gurevych, 2019) combined with dimensionality reduction using UMAP (McInnes and Healy, 2018).</p>
<p>Glossary Prompt (GPT3.5)</p>
<p>You are a scientist who can read and summarize scientific papers.You are given the following paper: {Introduction}.Your task is to extract a list of key concepts along with correct definitions like a glossary of the paper.Follow the format [Concept: Definition].</p>
<p>Acronyms Prompt (GPT3.5)</p>
<p>You are a scientist who can read and summarize scientific papers.You are given the following paper: {Introduction}.Your task is to extract a list of acronyms that the authors use along with correct expansions from the paper.For example (1) EDU: Elementary Discourse Unit, (2) SEHY: Simple Yet Effective Hybrid Model, (3) PLM: Pretrained Language Model.Exclude acronyms for which no expansion is explicitly provided by the authors.Follow the format [Acronym: Expansion].</p>
<p>Table 5: Prompts for automatic terminology acquisition from the introduction of a paper.We extract glossary as well as acronym-expansion pairs.For the latter, we provide examples of the expected output format.</p>
<p>or even inaccurately paraphrased, compelling researchers to trace back through multiple papers to find the original definitions.The task of automatic terminology acquisition (Judea et al., 2014) aims to tackle this issue by extracting various concepts defined in a paper along with their definitions.In our exploration of this task, we opted for LLMs instead of supervised methods that necessitate labeled data.</p>
<p>We utilized prompt engineering, leveraging GPT-3.5, with the introduction section of the paper as input for automatic terminology acquisition.We formulated two prompts specifically for extracting glossary definitions and acronym-expansion pairs.Examples of extracted glossary terms and acronym-expansion pairs are provided in Table 6.The prompts are shown in Table 5.</p>
<p>Dashboard and Figure Browser</p>
<p>TL;DR PROGRESS includes an interactive dashboard that provides real-time visualizations of key statistics gathered from the annotated documents.The dashboard displays: (1) the number of papers annotated per year, (2) the distribution of publicly released code and resources per year, (3) the popular datasets and document domains for training/evaluation, (4) the commonly emphasized quality criteria of summary (5) the dominant components targeted from the annotation scheme, and (6) the distribution of addressed challenges.</p>
<p>This extensive dashboard delivers a quantitative overview of the text summarization landscape, in line with the detailed facets and additional metadata in our annotation scheme.Key findings from the dashboard include:</p>
<ol>
<li>
<p>Authors consistently practice releasing code for reproducibility and adoption.</p>
</li>
<li>
<p>News (54.2%) and scholarly documents (13.3%) dominate as the most studied domains, calling for more diverse investigations.</p>
</li>
<li>
<p>The top three evaluated dimensions for summary quality are informativeness (17%), fluency (10%), and coherence (8.1%).</p>
</li>
<li>
<p>The majority of papers propose new objective functions and input encoding approaches.</p>
</li>
<li>
<p>Predominant challenges include controlled summarization, comprehensive evaluation, insufficient datasets, and risks of hallucinations.</p>
</li>
</ol>
<p>The tool also incorporates a dedicated figure browser (Appendix, Figure 3) hosting 1524 figures and tables (with captions) linked to their sources.This resource streamlines navigation and serves as a handy reference for researchers exploring standard illustrations depicting model architectures or layouts for presenting evaluation results.6</p>
<p>Evaluation</p>
<p>We conducted an empirical evaluation of the tool's efficacy in supporting systematic literature reviews for text summarization.The study involved presenting targeted inquiries relevant to beginners in the field and instructing participants to leverage both TL;DR PROGRESS and Semantic Scholar for retrieving relevant papers.Additionally, we systematically gathered feedback on the tool's usability and utility for understanding the effectiveness of its features.</p>
<p>Purpose-driven User Study</p>
<p>We conducted a study with five participants (3 PhDs, 2 PostDocs) specializing in natural language processing or information retrieval, but unfamiliar with text summarization research.Their task was to find up to five relevant papers for each of the ten research questions, covering various aspects of summarization research using both TL;DR PROGRESS and Semantic Scholar. 7The following research questions were crafted in reference to the New-Summ Workshop's Call for Papers.8</p>
<p>Term Definition / Expansion</p>
<p>Glossary Co-Decoding An algorithm that takes two review sets as input to compare and contrast the token probability distributions of the models to generate more distinctive summaries (Iso et al., 2021).</p>
<p>Concept-Pruning</p>
<p>An approach to reduce the number of concepts in a model to find optimal solutions efficiently (Boudin et al., 2015).Drop-Prompt Mechanism An approach to drop out hallucinated entities from a predicted content plan and to prompt the decoder with the modified plan to generate faithful summaries (Narayan et al., 2021).</p>
<p>Facet Bias Problem</p>
<p>The problem of centrality-based models tending to select sentences from one facet of a document, rather than important sentences from different facets (Liang et al., 2021).Indegree Centrality A measure of centrality that assumes a word receiving more relevance score from others is more likely to be important (Xu et al., 2020).</p>
<p>Acronyms</p>
<p>ADAQSUM Adapter-based query-focused abstractive summarization (Brazinskas et al., 2022).COLO Contrastive learning based re-ranking framework for one-stage summarization (An et al., 2022).PLATE Pseudo-labeling with larger attention temperature (Zhang et al., 2022).ASGARD Abstractive summarization with graph-augmentation and semantic-driven reward (Huang et al., 2020b).</p>
<p>ASAS</p>
<p>Answer selection and abstractive summarization (Deng et al., 2020).The participants were instructed to evaluate paper relevance using the summaries from the tools, rating them on a scale from 1 (least relevant) to 5 (most relevant).Alongside this, they were requested to share feedback on the usability, the utility of certain features of TL;DR PROGRESS, and its strengths and limitations.This evaluation provides both a comparative analysis and a qualitative understanding of the tool's practicality.</p>
<p>Results</p>
<p>Our tool effectively narrowed down the large collection of papers to a set of relevant results.The multifaceted search, in particular, facilitated quick paper filtering without keyword use.Three out of five participants favored our tool for literature review.However, Semantic Scholar offers a more "familiar" search experience and more recent results, albeit requiring extra effort for relevance filtering.Both tools received a score of 4 for the relevance of results.Users also rated the usefulness of features on a scale of 1 (least useful) to 5 (most useful).The advanced search (combining facets) was highly useful (mean score of 4.5), allowing users to easily adapt searches to the research question at hand.This underscores the utility of our annotation scheme (Table 1).Indicative summaries and the list of challenges were sufficiently useful (mean score of 3.6) for quickly skimming paper contents and finding papers addressing specific problems, respectively.Results are visualized in the Appendix, Figure 4.</p>
<p>Feedback</p>
<p>Users found the tool intuitive and easy to use, appreciating the multi-faceted search and indicative summaries.The dashboard was viewed as a useful resource for obtaining a quantitative overview of the text summarization field.Users offered constructive feedback, suggesting incorporating a more sophisticated search mechanism and integrating it with facet-based filtering.They pointed out that searching only by conceptual components was insufficient, as the resulting set of papers was still large and required further filtering.These insights will be considered for future improvements to the tool.</p>
<p>Conclusion</p>
<p>In summary, TL;DR PROGRESS offers an interactive platform for nuanced exploration of over 500 neural text summarization papers from top venues.Utilizing a tailored annotation scheme, the tool guides users through multifaceted retrieval, provides insightful indicative summaries, outlines challenges, and presents a quantitative overview, easing the entry for newcomers into the field.</p>
<p>Limitations</p>
<p>The tool leverages LLMs for automated summarization, extracting contextual factors like summary purpose, issues, solutions, and scientific terminology from papers.While we conducted a random accuracy check, a comprehensive assessment of hallucinations or faithfulness in the extracted information was not performed.We anticipate that with more advanced models, such as GPT-4, we can enhance the assurance of quality and structure the extracted content more effectively.Currently confined to summarization, the tool's annotation scheme can be readily extended to other domains, bootstrapped by experts accordingly.However, existing facets like datasets, domains, metrics, qualitative evaluation, and learning paradigms can be directly annotated for new domains.Additionally, a forthcoming feature is the tool's capability to incorporate new papers, automating the annotation process-a feature we plan to implement in future updates to the tool.</p>
<p>Figure 1 :
1
Figure1: Our annotation scheme is based on a summarization literature analysis.Its four components and their respective facets enable a fine-grained unified analysis of relevant papers.The indicative summary is automatically generated.</p>
<p>Figure 3 :
3
Figure 3: An overview of the figure browser which contains all the tables and figures pulled from the papers, accompanied by their captions.</p>
<p>(a) Retrieval effectiveness of TL;DR PROGRESS.(b) Retrieval effectiveness of Semantic Scholar.(c) Preference of TL;DR PROGRESS over Semantic Scholar for literature review.</p>
<p>(d) Usefulness of features in TL;DR PROGRESS.Advanced search which allows for combining multiple facets for filtering papers is the most useful feature, followed by the enumerated list of challenges.</p>
<p>Figure 4 :
4
Figure 4: Evaluation results of the effectiveness and usefulness of TL;DR PROGRESS compared to Semantic Scholar.</p>
<p>Table 2 :
2
Number of papers published per venue.Unsurprisingly, EMNLP and ACL are the most popular venues for summarization research.
Venue CountVenue CountVenueCountEMNLP184EACL13IJCNLP4ACL115TACL12ICLR2NAACL60CIKM12ECIR2COLING34AACL11NEURIPS2AAAI29IJCAI9SIGIR17CONLL8Challenges in Text SummarizationControlled and Tailored SummarizationEfficient Encoding of Long DocumentsExploiting the Structure of Long DocumentsHallucinations in the Generated SummariesIdentifying Important Contents from the DocumentInformation Loss / Incoherence in Extractive SummarizationLack of Suitable Training DataPretraining and Sample EfficiencyRobust Evaluation Methods</p>
<p>Table 3 :
3
Manually annotated labels for problem statement clusters extracted from all papers, highlighting the prevalent challenges in text summarization.</p>
<p>Table 6 :
6
Examples of automatically extracted glossary and acronym-expansion pairs from the papers.
1. How do neural text summarization modelsaddress hallucination challenges in abstractivesummarization?2. What are the efficient encoding strategies forhandling long documents in neural text sum-marization?3. How do neural text summarization modelscontrol/tailor the generated summaries to userpreferences/aspects/facets?4. How can pretrained language models be lever-aged for improving text summarization?5. How can additional sources of external knowl-edge be integrated into the text summarizationpipeline?6. What are the annotation strategies for evaluat-ing hallucination, faithfulness, and factualityin summarization?
https://github.com/allenai/science-parse
We used PDFFigures 2.0 (Clark and Divvala, 2016).
https://www.semanticscholar.org/
https://newsumm.github.io/2023/
Code / ResourcesArtifacts relevant to reproduce the paper's contribution.A IllustrationsThis section shows the indicative summaries of a paper and the figure browser.For indicative summary, we combined automatically extracted contextual information using prompts.
Colo: A contrastive learning based re-ranking framework for onestage summarization. Chenxin An, Ming Zhong, Zhiyong Wu, Qin Zhu, Xuanjing Huang, Xipeng Qiu, Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022. the 29th International Conference on Computational Linguistics, COLING 2022Gyeongju, Republic of Korea2022. October 12-17, 2022International Committee on Computational Linguistics</p>
<p>Paper trail reveals references go unread by citing authors. Philip Ball, Nature. 42069162002</p>
<p>Concept-based summarization using integer linear programming: From concept pruning to multiple optimal solutions. Florian Boudin, Hugo Mougard, Benoît Favre, 10.18653/V1/D15-1220Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalThe Association for Computational Linguistics2015. 2015. September 17-21, 2015</p>
<p>Efficient few-shot finetuning for opinion summarization. Arthur Brazinskas, Ramesh Nallapati, Mohit Bansal, Markus Dreyer, 10.18653/V1/2022.FINDINGS-NAACL.113Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, WA, United StatesAssociation for Computational Linguistics2022. July 10-15, 2022</p>
<p>TLDR: extreme summarization of scientific documents. Isabel Cachola, Kyle Lo, Arman Cohan, Daniel S Weld, 10.18653/V1/2020.FINDINGS-EMNLP.428Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics2020. 16-20 November 2020EMNLP 2020 of Findings of ACL</p>
<p>Density-based clustering based on hierarchical density estimates. J G B Ricardo, Davoud Campello, Jörg Moulavi, Sander, 10.1007/978-3-642-37456-2_14Advances in Knowledge Discovery and Data Mining, 17th Pacific-Asia Conference, PAKDD 2013. Lecture Notes in Computer Science. Gold Coast, AustraliaSpringer2013. April 14-17, 20137819Proceedings, Part II</p>
<p>Joint learning of answer selection and answer summary generation in community question answering. Andreas Christopher, Santosh Clark, ; Acm Yang Kumar Divvala, Wai Deng, Yuexiang Lam, Daoyuan Xie, Yaliang Chen, Min Li, Ying Yang, Shen, 10.1609/AAAI.V34I05.6266The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. Newark, NJ, USA; New York, NY, USAAAAI Press2016. June 19 -23, 2016. 2020. February 7-12, 20202020The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</p>
<p>Autodive: An integrated onsite scientific literature annotation tool. Yi Du, Ludi Wang, Mengyi Huang, Dongze Song, Wenjuan Cui, Yuanchun Zhou, 10.18653/V1/2023.ACL-DEMO.7Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2023. the 61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2023Toronto, CanadaAssociation for Computational Linguistics2023. July 10-12, 2023</p>
<p>Blind men and elephants: What do citation summaries tell us about a research article?. Aaron Elkiss, Siwei Shen, Anthony Fader, Günes Erkan, David J States, Dragomir R Radev, 10.1002/asi.20707J. Assoc. Inf. Sci. Technol. 5912008</p>
<p>News summarization and evaluation in the era of GPT-3. Tanya Goyal, Junyi , Jessy Li, Greg Durrett, 10.48550/arXiv.2209.12356CoRR, abs/2209.123562022</p>
<p>Scilit: A platform for joint scientific literature discovery, summarization and citation generation. Nianlong Gu, Richard H R Hahnloser, 10.18653/V1/2023.ACL-DEMO.22Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2023. the 61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2023Toronto, CanadaAssociation for Computational Linguistics2023. July 10-12, 2023</p>
<p>What have we achieved on text summarization?. Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, Yue Zhang, 10.18653/v1/2020.emnlp-main.33Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020a. 2020. November 16-20, 2020</p>
<p>Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward. Luyang Huang, Lingfei Wu, Lu Wang, 10.18653/V1/2020.ACL-MAIN.457Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020OnlineAssociation for Computational Linguistics2020b. July 5-10, 2020</p>
<p>Convex aggregation for opinion summarization. Hayate Iso, Xiaolan Wang, Yoshihiko Suhara, Stefanos Angelidis, Wang-Chiew Tan, 10.18653/v1/2021.findings-emnlp.328Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana. Association for Computational Linguistics2021. 16-20 November, 2021</p>
<p>Automatic summarizing: factors and directions. Jones Sparck, Advances in automatic text summarization. 1999</p>
<p>Unsupervised training set generation for automatic acquisition of technical terminology in patents. Alex Judea, Hinrich Schütze, Soeren Bruegmann, COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers. Dublin, IrelandACL2014. August 23-29, 2014</p>
<p>Improving unsupervised extractive summarization with facet-aware modeling. Xinnian Liang, Shuangzhi Wu, Mu Li, Zhoujun Li, 10.18653/V1/2021.FINDINGS-ACL.147Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Association for Computational Linguistics2021. August 1-6, 2021ACL/IJCNLP 2021 of Findings of ACL</p>
<p>The automatic creation of literature abstracts. Hans Peter, Luhn , IBM Journal of Reseach and Devopment. 221958</p>
<p>UMAP: uniform manifold approximation and projection for dimension reduction. Inderjeet Mani, CoRR, abs/1802.03426Summarization evaluation: An overview. Leland McInnes and John Healy. 2001. 2018</p>
<p>Planning with learned entity prompts for abstractive summarization. Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo Simões, Vitaly Nikolaev, Ryan T Mcdonald, 10.1162/TACL_A_00438Trans. Assoc. Comput. Linguistics. 92021</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019. November 3-7, 2019</p>
<p>A neural attention model for abstractive sentence summarization. Alexander M Rush, Sumit Chopra, Jason Weston, 10.18653/v1/d15-1044Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalThe Association for Computational Linguistics2015. 2015. September 2015</p>
<p>Perslearn: Research training through the lens of perspective cultivation. Yu-Zhe Shi, Shiqian Li, Xinyi Niu, Qiao Xu, Jiawen Liu, Yifan Xu, Shiyu Gu, Bingru He, Xinyang Li, Xinyu Zhao, Zijian Zhao, Yidong Lyu, Zhen Li, Sijia Liu, Lin Qiu, Jinhao Ji, Lecheng Ruan, Yuxi Ma, Wenjuan Han, Yixin Zhu, 10.18653/V1/2023.ACL-DEMO.2Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2023. the 61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2023Toronto, CanadaAssociation for Computational Linguistics2023. July 10-12, 2023</p>
<p>Sequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, V Quoc, Le, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Quebec, Canada2014. 2014. December 8-13 2014</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Long Beach, CA, USA2017. December 4-9, 2017</p>
<p>Self-attention guided copy mechanism for abstractive summarization. Song Xu, Haoran Li, Peng Yuan, Youzheng Wu, Xiaodong He, Bowen Zhou, 10.18653/V1/2020.ACL-MAIN.125Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020OnlineAssociation for Computational Linguistics2020. July 5-10, 2020</p>
<p>Attention temperature matters in abstractive summarization distillation. Shengqiang Zhang, Xingxing Zhang, Hangbo Bao, Furu Wei, 10.18653/V1/2022.ACL-LONG.11Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 20221ACL 2022</p>
<p>Indicative summary of a paper containing (1) an abstractive summary of the introduction, (2) manually annotated metadata attributes (details). 23) purpose of the summary encompassing the target audience, the downstream use, and the purpose, (4) claims and contributions of the paper</p>            </div>
        </div>

    </div>
</body>
</html>
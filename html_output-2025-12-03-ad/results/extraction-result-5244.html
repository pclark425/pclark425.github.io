<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5244 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5244</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5244</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-267636663</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.08170v3.pdf" target="_blank">LLaGA: Large Language and Graph Assistant</a></p>
                <p><strong>Paper Abstract:</strong> Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the Large Language and Graph Assistant (LLaGA), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for graphs. Our extensive experiments across popular graph benchmarks show that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-of-the-art graph models in both supervised and zero-shot scenarios. Our code is available at \url{https://github.com/VITA-Group/LLaGA}.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5244.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5244.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaGA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language and Graph Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that converts graph-centered substructures into fixed-length node embedding sequences via templates, encodes node text features + structural Laplacian/Hop embeddings, and maps those node embeddings into an LLM token embedding space via a trainable MLP projector; trains only the projector (LLM frozen) on multi-task QA-style prompts for node classification, link prediction, and node description.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Template-based node-sequence translation + projector mapping</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph -> for each central node build a fixed-shape representation: (1) Neighborhood Detail Template: sample a fixed-size k-hop computational tree (n1, n2, ... per hop), fill with [pad], level-order traverse to a fixed-length node sequence; (2) Hop-Field Overview Template: compute parameter-free hop embeddings via iterative neighbor averaging h_i = mean_{v' in N(v)} h_{i-1}^{v'}, producing a hop-sequence h0..hk. Text attributes of nodes are encoded (SimTeG / SBERT / RoBERTa), placeholders are zero vectors; append template Laplacian eigenvector positional embedding (computed once for fixed template shape); concatenate text encoding and Laplacian embedding to form node embeddings h_i, then map each h_i into the LLM token embedding space via a small MLP projector f_theta, and insert the resulting token embeddings in the LLM prompt positions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (citation networks: ogbn-Arxiv, Pubmed, Cora; e-commerce: ogbn-Products)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Parameter-free structural encoding step (sampling, averaging) that preserves original graph structure in a fixed-length sequence; fixed-shape computational tree enables precomputed Laplacian positional encodings; two template tradeoffs: Neighborhood Detail (high local fidelity, longer sequences) vs Hop-Field Overview (compact, larger receptive field, summarizes neighbors); projector-only tuning preserves frozen LLM generality and reduces compute; representation is interpretable (can generate node descriptions); sampling and padding introduce stochasticity/approximation for high-degree nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification, link prediction, node description (semantic generation), and zero-shot transfer across datasets/tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Metrics used: Accuracy (%) for node classification and link prediction; SBERT similarity score (semantic similarity) and Description Label Accuracy (%) for node description. Reported highlights: Node description (Sbert score / Label Accuracy) — ArXiv: ND 0.6023 / 74.64%, HO 0.6228 / 75.49%; Products: ND 0.4952 / 83.18%, HO 0.5193 / 84.60%; Pubmed: ND 0.6847 / 92.27%, HO 0.6934 / 94.27%; Cora: ND 0.6465 / 86.72%, HO 0.6545 / 86.90%. Zero-shot link prediction: Train ArXiv+Pubmed -> Test Cora: LLaGA-ND-7B 86.47% , LLaGA-HO-7B 87.35% ; Train ArXiv+Pubmed+Cora -> Test Products: LLaGA-ND-7B 92.65%, LLaGA-HO-7B 92.99%. (All accuracies in percent.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to prior graph->text approaches (plain natural-language descriptions, Instruct-GLM) and hybrid methods (GraphGPT), LLaGA demonstrates superior performance across supervised and zero-shot settings and across multiple datasets/tasks; ablation shows both ND and HO templates significantly outperform 'no-template' (center-node-only) baselines; ND typically better when fine-grained neighbor identity matters (e.g., product category), HO better when broader neighbor-category distributions matter (e.g., citation graphs). LLaGA outperforms GraphGPT in the compared tasks (GraphGPT reported much lower zero-shot LP accuracy ~50.74% in the paper's comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires choosing a template (ND vs HO) and sampling hyperparameters (n1,n2,..., hop count) which affect fidelity and compute; ND can produce long sequences and relies on stochastic neighbor sampling and padding which may introduce variance; HO sacrifices per-neighbor detail and may mislead when specific neighbor identities are critical; method assumes or benefits from textual node attributes and a text encoder; only the projector is trained — although this preserves LLM generality it may limit representational adaptivity compared to fine-tuning the LLM; fixed-template Laplacian embeddings assume fixed sample shape.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5244.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5244.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ND Template</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neighborhood Detail Template (LLaGA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A node-level linearization that builds a fixed-shape sampled computational tree around the central node, pads to constant fanout per hop, performs level-order traversal to produce a fixed-length sequence of neighbor node identities/features, and augments each position with a Laplacian eigenvector embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Neighborhood Detail Template (node-sequence linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct a k-hop sampled computational tree centered at node v with fixed per-hop sample sizes n1,n2,...; if insufficient neighbors, fill with [pad]; perform level-order traversal to obtain sequence (root, 1-hop children, 2-hop children, ...). Encode textual attributes for each listed node with an off-the-shelf encoder (SimTeG/SBERT/Roberta), represent [pad] as zero vector, append precomputed Laplacian eigenvector U_i for that fixed-position, concatenate to form node embedding sequence, then project to token embeddings via MLP.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (citation, product co-purchase graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>High local structural fidelity; position-specific Laplacian provides deterministic positional cues; fixed-length output simplifies batching; parameter-free sampling and padding preserves original structure in sampled subtree; longer sequences and higher per-sample compute and memory; stochastic sampling may affect reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification, link prediction, node description, and zero-shot transfer (e.g., product category classification where neighbor identity matters).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Improvements vs no-template: Node classification (examples from ablation Table 6): ArXiv NC NONE 73.92% -> ND 75.85%; Products NC NONE 80.45% -> ND 83.58%; Link prediction (Table 6) NONE ArXiv 89.98% -> ND 90.81%; Zero-shot link prediction ArXiv+Pubmed->Cora: ND 86.47%; ArXiv+Pubmed+Cora->Products: ND 92.65%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperforms no-template (center-node-only) and plain textual graph descriptions in experiments; compared with Hop-Field Overview, ND better on tasks needing fine-grained neighbor identity (Products), while HO can be better on some citation datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Longer sequences and sampling/padding overhead; sensitive to sample-size hyperparameters (n_i); padding placeholders may dilute signal when many pads are used; not as compact as hop-summarization for large receptive fields.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5244.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5244.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HO Template</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hop-Field Overview Template (LLaGA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compact node-level representation that summarizes the i-th hop neighborhood via parameter-free message-passing averaging to produce a sequence of hop embeddings (h0,h1,...,hk) representing increasing neighborhood radii around the central node.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Hop-Field Overview Template (hop-embedding sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute h0 = text-encoding(phi(x_v)) for center node; iteratively compute h_i = (1/|N(v)|) sum_{v' in N(v)} h_{i-1}^{v'} so that h_i aggregates information from i-hop neighborhood; produce the sequence [h0,h1,...,hk], append Laplacian or positional info as appropriate, then project via MLP to token embedding space for LLM input.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (citation networks, e-commerce graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact, captures broader receptive field with few tokens (one token per hop), parameter-free, better scaling for large-degree nodes, trades off per-neighbor identity for aggregate statistics, simpler and lower-latency representation than ND.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification, link prediction, node description, zero-shot transfer (shown to work well on citation graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: Node description Sbert/Label Accuracy (Table 4) ArXiv Sbert 0.6228 / 75.49% label accuracy; Pubmed Sbert 0.6934 / 94.27%; Zero-shot link prediction: ArXiv+Pubmed->Cora HO 87.35%; ArXiv+Pubmed+Cora->Products HO 92.99%. Ablation: HO outperforms no-template and often matches/exceeds ND on some datasets (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to ND: HO typically better on citation datasets where neighbor-category distribution matters; outperforms center-node-only (no-template) and GraphGPT baselines in experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Sacrifices per-neighbor detail and may fail when identity of specific neighbors (not just aggregate) is critical (e.g., fine-grained product categories); choice of hop count determines granularity and information captured.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5244.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5244.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plain-text descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural-language textual descriptions of graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Earlier approach of converting graphs into natural-language sentences describing nodes/edges to feed LLMs (e.g., list neighbors, describe relations); described in related work as verbose and often ineffective at encoding structural specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Plain natural-language descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Write out node/edge relations and attributes in human-readable text (e.g., 'Node A connects to B and C; A is about X'), then feed the textual description to an LLM as prompt input.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs/text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Human-readable, but verbose and repetitive; may lose explicit structural detail or be ambiguous about graph topology; can be inefficient in token usage and lead to degraded LLM performance on structural tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used in prior works for node classification and graph reasoning with LLMs (e.g., early attempts and Instruct-GLM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper states plain textual descriptions often cause poor LLM performance on basic graph tasks; LLaGA's template-based sequence encoding is proposed to avoid these issues and empirically outperforms such approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Verbosity, lack of faithful structural encoding, token budget inefficiency, often requires LLM fine-tuning or task-specific adaptations to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5244.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5244.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT (Tang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concurrent generalizable model that pairs text-encoded node features with a pretrained graph transformer to produce graph encodings for LLMs; used as a baseline in this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Hybrid text encoding + pretrained graph transformer</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode node textual attributes with a text encoder, then use a pretrained graph transformer to encode structural relationships; combine/align outputs with LLM inputs for downstream graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (citation and product graphs in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Uses a learned graph transformer pretraining stage to capture structure; may not distill task-specific structural signals for all downstream tasks and can be less general across diverse tasks/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification and link prediction (limited-task pretraining in GraphGPT vs LLaGA's multi-task training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported lower zero-shot link prediction performance in the paper's comparisons: GraphGPT-7B 50.74% (Table 5) versus LLaGA 86-93% depending on setting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>LLaGA (template + projector) outperforms GraphGPT on the compared tasks and demonstrates stronger multi-task and zero-shot generalization; paper argues GraphGPT's pretrained transformer may not capture all structural cues needed downstream.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on pretrained graph transformer which may not generalize to unseen tasks/datasets; the pretraining stage may not be sufficient to adapt to varied downstream task semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5244.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5244.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruct-GLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruct-GLM (Ye et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that describes graphs in (natural) language and fine-tunes a language model on graph tasks to improve LLM performance on graph-structured data, at the expense of general-purpose versatility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Textual-graph prompts with LLM fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert graph structure into natural language descriptions and fine-tune the LLM on these descriptions and downstream graph tasks to improve task-specific performance.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Can improve LLM performance on specific graph tasks via fine-tuning, but specialization reduces model versatility and may harm generalization to other tasks/domains.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used for improving graph-task performance via LLM fine-tuning (mentioned qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>LLaGA chooses to keep the LLM frozen and instead trains a projector to map graph representations into token space; authors argue this preserves LLM generality while giving competitive or superior performance compared to fine-tuning-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fine-tuning the LLM can reduce its general-purpose capabilities and may require task-specific data and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaGA: Large Language and Graph Assistant', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Graphgpt <em>(Rating: 2)</em></li>
                <li>Natural language is all a graph needs <em>(Rating: 2)</em></li>
                <li>Can large language models understand graph structured data? <em>(Rating: 1)</em></li>
                <li>A frustratingly simple approach improves textual graph learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5244",
    "paper_id": "paper-267636663",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "LLaGA",
            "name_full": "Large Language and Graph Assistant",
            "brief_description": "A framework that converts graph-centered substructures into fixed-length node embedding sequences via templates, encodes node text features + structural Laplacian/Hop embeddings, and maps those node embeddings into an LLM token embedding space via a trainable MLP projector; trains only the projector (LLM frozen) on multi-task QA-style prompts for node classification, link prediction, and node description.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Template-based node-sequence translation + projector mapping",
            "representation_description": "Graph -&gt; for each central node build a fixed-shape representation: (1) Neighborhood Detail Template: sample a fixed-size k-hop computational tree (n1, n2, ... per hop), fill with [pad], level-order traverse to a fixed-length node sequence; (2) Hop-Field Overview Template: compute parameter-free hop embeddings via iterative neighbor averaging h_i = mean_{v' in N(v)} h_{i-1}^{v'}, producing a hop-sequence h0..hk. Text attributes of nodes are encoded (SimTeG / SBERT / RoBERTa), placeholders are zero vectors; append template Laplacian eigenvector positional embedding (computed once for fixed template shape); concatenate text encoding and Laplacian embedding to form node embeddings h_i, then map each h_i into the LLM token embedding space via a small MLP projector f_theta, and insert the resulting token embeddings in the LLM prompt positions.",
            "graph_type": "Text-attributed graphs (citation networks: ogbn-Arxiv, Pubmed, Cora; e-commerce: ogbn-Products)",
            "representation_properties": "Parameter-free structural encoding step (sampling, averaging) that preserves original graph structure in a fixed-length sequence; fixed-shape computational tree enables precomputed Laplacian positional encodings; two template tradeoffs: Neighborhood Detail (high local fidelity, longer sequences) vs Hop-Field Overview (compact, larger receptive field, summarizes neighbors); projector-only tuning preserves frozen LLM generality and reduces compute; representation is interpretable (can generate node descriptions); sampling and padding introduce stochasticity/approximation for high-degree nodes.",
            "evaluation_task": "Node classification, link prediction, node description (semantic generation), and zero-shot transfer across datasets/tasks.",
            "performance_metrics": "Metrics used: Accuracy (%) for node classification and link prediction; SBERT similarity score (semantic similarity) and Description Label Accuracy (%) for node description. Reported highlights: Node description (Sbert score / Label Accuracy) — ArXiv: ND 0.6023 / 74.64%, HO 0.6228 / 75.49%; Products: ND 0.4952 / 83.18%, HO 0.5193 / 84.60%; Pubmed: ND 0.6847 / 92.27%, HO 0.6934 / 94.27%; Cora: ND 0.6465 / 86.72%, HO 0.6545 / 86.90%. Zero-shot link prediction: Train ArXiv+Pubmed -&gt; Test Cora: LLaGA-ND-7B 86.47% , LLaGA-HO-7B 87.35% ; Train ArXiv+Pubmed+Cora -&gt; Test Products: LLaGA-ND-7B 92.65%, LLaGA-HO-7B 92.99%. (All accuracies in percent.)",
            "comparison_to_other_representations": "Compared to prior graph-&gt;text approaches (plain natural-language descriptions, Instruct-GLM) and hybrid methods (GraphGPT), LLaGA demonstrates superior performance across supervised and zero-shot settings and across multiple datasets/tasks; ablation shows both ND and HO templates significantly outperform 'no-template' (center-node-only) baselines; ND typically better when fine-grained neighbor identity matters (e.g., product category), HO better when broader neighbor-category distributions matter (e.g., citation graphs). LLaGA outperforms GraphGPT in the compared tasks (GraphGPT reported much lower zero-shot LP accuracy ~50.74% in the paper's comparison).",
            "limitations_or_challenges": "Requires choosing a template (ND vs HO) and sampling hyperparameters (n1,n2,..., hop count) which affect fidelity and compute; ND can produce long sequences and relies on stochastic neighbor sampling and padding which may introduce variance; HO sacrifices per-neighbor detail and may mislead when specific neighbor identities are critical; method assumes or benefits from textual node attributes and a text encoder; only the projector is trained — although this preserves LLM generality it may limit representational adaptivity compared to fine-tuning the LLM; fixed-template Laplacian embeddings assume fixed sample shape.",
            "uuid": "e5244.0",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ND Template",
            "name_full": "Neighborhood Detail Template (LLaGA)",
            "brief_description": "A node-level linearization that builds a fixed-shape sampled computational tree around the central node, pads to constant fanout per hop, performs level-order traversal to produce a fixed-length sequence of neighbor node identities/features, and augments each position with a Laplacian eigenvector embedding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Neighborhood Detail Template (node-sequence linearization)",
            "representation_description": "Construct a k-hop sampled computational tree centered at node v with fixed per-hop sample sizes n1,n2,...; if insufficient neighbors, fill with [pad]; perform level-order traversal to obtain sequence (root, 1-hop children, 2-hop children, ...). Encode textual attributes for each listed node with an off-the-shelf encoder (SimTeG/SBERT/Roberta), represent [pad] as zero vector, append precomputed Laplacian eigenvector U_i for that fixed-position, concatenate to form node embedding sequence, then project to token embeddings via MLP.",
            "graph_type": "Text-attributed graphs (citation, product co-purchase graphs)",
            "representation_properties": "High local structural fidelity; position-specific Laplacian provides deterministic positional cues; fixed-length output simplifies batching; parameter-free sampling and padding preserves original structure in sampled subtree; longer sequences and higher per-sample compute and memory; stochastic sampling may affect reproducibility.",
            "evaluation_task": "Node classification, link prediction, node description, and zero-shot transfer (e.g., product category classification where neighbor identity matters).",
            "performance_metrics": "Improvements vs no-template: Node classification (examples from ablation Table 6): ArXiv NC NONE 73.92% -&gt; ND 75.85%; Products NC NONE 80.45% -&gt; ND 83.58%; Link prediction (Table 6) NONE ArXiv 89.98% -&gt; ND 90.81%; Zero-shot link prediction ArXiv+Pubmed-&gt;Cora: ND 86.47%; ArXiv+Pubmed+Cora-&gt;Products: ND 92.65%.",
            "comparison_to_other_representations": "Outperforms no-template (center-node-only) and plain textual graph descriptions in experiments; compared with Hop-Field Overview, ND better on tasks needing fine-grained neighbor identity (Products), while HO can be better on some citation datasets.",
            "limitations_or_challenges": "Longer sequences and sampling/padding overhead; sensitive to sample-size hyperparameters (n_i); padding placeholders may dilute signal when many pads are used; not as compact as hop-summarization for large receptive fields.",
            "uuid": "e5244.1",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "HO Template",
            "name_full": "Hop-Field Overview Template (LLaGA)",
            "brief_description": "A compact node-level representation that summarizes the i-th hop neighborhood via parameter-free message-passing averaging to produce a sequence of hop embeddings (h0,h1,...,hk) representing increasing neighborhood radii around the central node.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Hop-Field Overview Template (hop-embedding sequence)",
            "representation_description": "Compute h0 = text-encoding(phi(x_v)) for center node; iteratively compute h_i = (1/|N(v)|) sum_{v' in N(v)} h_{i-1}^{v'} so that h_i aggregates information from i-hop neighborhood; produce the sequence [h0,h1,...,hk], append Laplacian or positional info as appropriate, then project via MLP to token embedding space for LLM input.",
            "graph_type": "Text-attributed graphs (citation networks, e-commerce graphs)",
            "representation_properties": "Compact, captures broader receptive field with few tokens (one token per hop), parameter-free, better scaling for large-degree nodes, trades off per-neighbor identity for aggregate statistics, simpler and lower-latency representation than ND.",
            "evaluation_task": "Node classification, link prediction, node description, zero-shot transfer (shown to work well on citation graphs).",
            "performance_metrics": "Examples: Node description Sbert/Label Accuracy (Table 4) ArXiv Sbert 0.6228 / 75.49% label accuracy; Pubmed Sbert 0.6934 / 94.27%; Zero-shot link prediction: ArXiv+Pubmed-&gt;Cora HO 87.35%; ArXiv+Pubmed+Cora-&gt;Products HO 92.99%. Ablation: HO outperforms no-template and often matches/exceeds ND on some datasets (Table 6).",
            "comparison_to_other_representations": "Compared to ND: HO typically better on citation datasets where neighbor-category distribution matters; outperforms center-node-only (no-template) and GraphGPT baselines in experiments reported.",
            "limitations_or_challenges": "Sacrifices per-neighbor detail and may fail when identity of specific neighbors (not just aggregate) is critical (e.g., fine-grained product categories); choice of hop count determines granularity and information captured.",
            "uuid": "e5244.2",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Plain-text descriptions",
            "name_full": "Natural-language textual descriptions of graphs",
            "brief_description": "Earlier approach of converting graphs into natural-language sentences describing nodes/edges to feed LLMs (e.g., list neighbors, describe relations); described in related work as verbose and often ineffective at encoding structural specifics.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Plain natural-language descriptions",
            "representation_description": "Write out node/edge relations and attributes in human-readable text (e.g., 'Node A connects to B and C; A is about X'), then feed the textual description to an LLM as prompt input.",
            "graph_type": "General graphs/text-attributed graphs",
            "representation_properties": "Human-readable, but verbose and repetitive; may lose explicit structural detail or be ambiguous about graph topology; can be inefficient in token usage and lead to degraded LLM performance on structural tasks.",
            "evaluation_task": "Used in prior works for node classification and graph reasoning with LLMs (e.g., early attempts and Instruct-GLM).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Paper states plain textual descriptions often cause poor LLM performance on basic graph tasks; LLaGA's template-based sequence encoding is proposed to avoid these issues and empirically outperforms such approaches.",
            "limitations_or_challenges": "Verbosity, lack of faithful structural encoding, token budget inefficiency, often requires LLM fine-tuning or task-specific adaptations to improve performance.",
            "uuid": "e5244.3",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GraphGPT",
            "name_full": "GraphGPT (Tang et al., 2023)",
            "brief_description": "A concurrent generalizable model that pairs text-encoded node features with a pretrained graph transformer to produce graph encodings for LLMs; used as a baseline in this paper's comparisons.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Hybrid text encoding + pretrained graph transformer",
            "representation_description": "Encode node textual attributes with a text encoder, then use a pretrained graph transformer to encode structural relationships; combine/align outputs with LLM inputs for downstream graph tasks.",
            "graph_type": "Text-attributed graphs (citation and product graphs in comparisons)",
            "representation_properties": "Uses a learned graph transformer pretraining stage to capture structure; may not distill task-specific structural signals for all downstream tasks and can be less general across diverse tasks/datasets.",
            "evaluation_task": "Node classification and link prediction (limited-task pretraining in GraphGPT vs LLaGA's multi-task training).",
            "performance_metrics": "Reported lower zero-shot link prediction performance in the paper's comparisons: GraphGPT-7B 50.74% (Table 5) versus LLaGA 86-93% depending on setting.",
            "comparison_to_other_representations": "LLaGA (template + projector) outperforms GraphGPT on the compared tasks and demonstrates stronger multi-task and zero-shot generalization; paper argues GraphGPT's pretrained transformer may not capture all structural cues needed downstream.",
            "limitations_or_challenges": "Relies on pretrained graph transformer which may not generalize to unseen tasks/datasets; the pretraining stage may not be sufficient to adapt to varied downstream task semantics.",
            "uuid": "e5244.4",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Instruct-GLM",
            "name_full": "Instruct-GLM (Ye et al., 2023)",
            "brief_description": "An approach that describes graphs in (natural) language and fine-tunes a language model on graph tasks to improve LLM performance on graph-structured data, at the expense of general-purpose versatility.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Textual-graph prompts with LLM fine-tuning",
            "representation_description": "Convert graph structure into natural language descriptions and fine-tune the LLM on these descriptions and downstream graph tasks to improve task-specific performance.",
            "graph_type": "Text-attributed graphs",
            "representation_properties": "Can improve LLM performance on specific graph tasks via fine-tuning, but specialization reduces model versatility and may harm generalization to other tasks/domains.",
            "evaluation_task": "Used for improving graph-task performance via LLM fine-tuning (mentioned qualitatively).",
            "performance_metrics": null,
            "comparison_to_other_representations": "LLaGA chooses to keep the LLM frozen and instead trains a projector to map graph representations into token space; authors argue this preserves LLM generality while giving competitive or superior performance compared to fine-tuning-based approaches.",
            "limitations_or_challenges": "Fine-tuning the LLM can reduce its general-purpose capabilities and may require task-specific data and compute.",
            "uuid": "e5244.5",
            "source_info": {
                "paper_title": "LLaGA: Large Language and Graph Assistant",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Graphgpt",
            "rating": 2
        },
        {
            "paper_title": "Natural language is all a graph needs",
            "rating": 2,
            "sanitized_title": "natural_language_is_all_a_graph_needs"
        },
        {
            "paper_title": "Can large language models understand graph structured data?",
            "rating": 1,
            "sanitized_title": "can_large_language_models_understand_graph_structured_data"
        },
        {
            "paper_title": "A frustratingly simple approach improves textual graph learning",
            "rating": 1,
            "sanitized_title": "a_frustratingly_simple_approach_improves_textual_graph_learning"
        }
    ],
    "cost": 0.0179735,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLaGA: Large Language and Graph Assistant
11 Apr 2024</p>
<p>Runjin Chen 
Tong Zhao 
Ajay Jaiswal 
Neil Shah 
Zhangyang Wang 
LLaGA: Large Language and Graph Assistant
11 Apr 2024BC6FB28057380B1465F6B72D35557E05arXiv:2402.08170v3[cs.LG]
Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis.Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning.However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language.To this end, we introduce the Large Language and Graph Assistant (LLaGA), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data.LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input.LLaGA achieves this by reorganizing graph nodes to structureaware sequences and then mapping these into the token embedding space through a versatile projector.LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for graphs.Our extensive experiments across popular graph benchmarks show that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-ofthe-art graph models in both supervised and zeroshot scenarios.Our code is available at https: //github.com/VITA-Group/LLaGA</p>
<p>Introduction</p>
<p>Graphs are omnipresent, representing a myriad of real-world data from social networks, biological networks and recommendation systems, etc. Graph neural networks (GNNs) (Kipf &amp; Welling, 2017;Defferrard et al., 2016;Veličković et al., 2017), embedded with message passing and aggregation techniques, are powerful algorithmic tools on handling 1 The University of Texas at Austin 2 Snap Inc. Correspondence to: Zhangyang Wang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#116;&#108;&#97;&#115;&#119;&#97;&#110;&#103;&#64;&#117;&#116;&#101;&#120;&#97;&#115;&#46;&#101;&#100;&#117;">&#97;&#116;&#108;&#97;&#115;&#119;&#97;&#110;&#103;&#64;&#117;&#116;&#101;&#120;&#97;&#115;&#46;&#101;&#100;&#117;</a>,Runjin Chen <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#99;&#104;&#101;&#110;&#114;&#117;&#110;&#106;&#105;&#110;&#64;&#117;&#116;&#101;&#120;&#97;&#115;&#46;&#101;&#100;&#117;">&#99;&#104;&#101;&#110;&#114;&#117;&#110;&#106;&#105;&#110;&#64;&#117;&#116;&#101;&#120;&#97;&#115;&#46;&#101;&#100;&#117;</a>.</p>
<p>Preprint.</p>
<p>complex graph structures.Nonetheless, a critical limitation of GNNs is their weak multi-task handling capability.Typically trained on a single task, GNNs struggle to maintain performance when applied to multiple tasks.Self-supervised learning (Jin et al., 2021;Ju et al., 2023) may offer some improvement, but they still require task-specific heads or tuning for downstream tasks.</p>
<p>Recently, the advent of LLMs having massive context-aware knowledge and semantic comprehension capabilities (e.g., LLaMa (Touvron et al., 2023), GPTs (Achiam et al., 2023), Claude (Perez et al., 2022)) marks a significant advancement in AI research.A key advantage of LLMs is their ability to solve various tasks with a single model, showcasing strong language skills and the capacity to explain provided answers.These models have demonstrated remarkable proficiency not only in language-related tasks but also in understanding and generating visual content (Liu et al., 2023;Wang et al., 2023).However, direct application of such models presents challenges when it comes to graph-structured data, which inherently contains rich relational and structural information.Hence, researchers (Fatemi et al., 2023;Chen et al., 2023a) explored ways to translate graph structures into natural language suitable for consumption by language models.Yet, describing graphs in plain texts tends to be verbose and fails to directly represent the intrinsic characteristics of graphs, often leading to repetitive and unintuitive descriptions of nodes and edge relationships.Consequently, LLMs would perform poorly on basic graph tasks without specific adaptations (Chen et al., 2023a).Subsequently, Instruct-GLM (Ye et al., 2023) describes graphs in language and attempts to enhance LLMs' graph-task performance by taskspecific fine-tuning.However, this specialization constrains the model's versatility, potentially limiting its effectiveness in other graph tasks or non-graph-related domains.More recently, GraphGPT (Tang et al., 2023) has combined text descriptions with a self-supervised graph transformer to incorporate graph data into large language models (LLMs).However, the pre-trained graph transformer might not distill all relevant structural information for specific downstream tasks, leading to less satisfactory performances.Motivated by these issues, this work poses an important question: How to develop a framework that effectively encodes structural information for graphs across various tasks and domains, enabling its comprehension by LLMs, while maintaining LLMs' general-purpose ?</p>
<p>To this end, we introduce the Large Language and Graph Assistant (LLaGA), a novel framework that seamlessly integrates rich graph-structured data with the massive contextawareness skills and comprehension capabilities of Large Language Models.LLaGA has three impressive characteristics that distinguish LLaGA with prior works as follows:</p>
<p>• Versatility: LLaGA adopts a simple but universally applicable method for encoding structural details in graphs, and achieves a general alignment between graph and token spaces using a single, versatile projector.This projector efficiently handles various graph tasks across multiple datasets, eliminating the need for task-specific adjustments.Significantly, the performance of our versatile LLaGA framework can even exceed that of specialized task-focused graph models.• Generalizability: Given the comprehensive alignment between graph and token spaces, LLaGA not only excels in those datasets and tasks encountered during training but also demonstrates robust generalization to previously unseen datasets and tasks without additional tuning.• Interpretability: A key feature of LLaGA is its ability to provide detailed interpretations of node embeddings, greatly enhancing the understanding of its decisionmaking processes.</p>
<p>To achieve this, LLaGA uniquely reorganizes graph data into node sequences, without converting structural information into potentially ambiguous natural language descriptions.These sequences are formatted with the help of novel node-level templates, to reflect the structural information surrounding each central node while preserving the graph's node features.Note that this transformation is parameter-free, ensuring the preservation of the original structural integrity without necessitating further distillation.Subsequently, LLaGA translates node representations into LLMs' comprehensible token embedding space through a versatile projector, which can help in mitigating the expensive computational cost of fine-tuning LLMs as well as keeping LLMs' general purpose.The projector is generally trained on multiple graph datasets across various tasks, such as node classification, link prediction, and node description.This ensures it can interpret graph data from diverse perspectives and ingest an inherent ability to handle multiple tasks (all at once), bolstering its practical utility, and potentially augmenting LLaGA's generalization capabilities across various unseen datasets and tasks.Notably, unlike traditional multi-task learning methodologies used in GNNs, LLaGA trained all tasks in a uniform Question-Answer format, eschewing the need for task-specific loss functions or heads.</p>
<p>Our extensive experiments illustrate that LLaGA achieves a robust alignment between the graphs and token space of LLMs, facilitating the model's application to multiple tasks, unseen test set, and interestingly out-of-distribution datasets.</p>
<p>To our best knowledge, LLaGA is the first single model to preform consistently well across various graph datasets and tasks.It matches the effectiveness of specialized GNNs tailored for specific data and tasks, while also showing strong generalizability to unseen datasets or tasks.</p>
<p>Methodology</p>
<p>In this section, we introduce the details of LLaGA framework.We start with the notation setup, followed by a detailed explanation of the method employed for translating graphs into token embedding space.Subsequently, we delve into the training process, encompassing both the design of prompts and tasks as well as the training objectives.</p>
<p>Notation</p>
<p>A graph is a structure that encapsulates a set of entities and the interrelationships among them.Formally, a graph is denoted as G = (V, E, X ).Here, V denotes the set of nodes (entities).The set of edges, E, represents the connections between the nodes in V. X is the attribute information corresponding to the nodes.Each node v i ∈ V is associated with an attribute feature x i ∈ X .In this paper, our primary focus is on text-attributed graphs, implying that the attributes x i ∈ X of each node are expressed in a textual format.Additionally, we introduce N k v to denote the k th hop neighborhood set surrounding the node v.</p>
<p>Structure-Aware Graph Translation</p>
<p>The primary objective of LLaGA (Large Language and Graph Assistant) is to translate graph inputs into a token embedding space that is comprehensible to Large Language Models.This translation enables the utilization of LLMs' inherent reasoning capabilities for graph-related tasks, without necessitating any modifications to the LLM parameters.LLaGA accomplishes this by initially reorganizing nodes in graphs into node embedding sequences.These sequences are structured according to our proposed templates and are then converted into a sequence of token embeddings using a projector.</p>
<p>The first step involves converting graphs into node embedding sequences.Recognizing that the fundamental unit for graph analysis is the node, we developed two node-level templates for analysis on graphs.These templates are versatile, applicable not only to node-level tasks but also to other tasks like link prediction.Both templates are designed to encode structural information surrounding a node, offering different perspectives for analysis.The first, the Neighborhood Detail Template, provides an in-depth view of the central node and its immediate surroundings.The second, the Hop-Field Overview Template, offers a summarized view of a node's neighborhood, extendable to larger fields.</p>
<p>Neighborhood Detail Template is designed to elaborate on the detailed information of a node and its surrounding neighborhood.Given a node v, we first construct a fixedshape, sampled computational tree centered around v. For every hop of neighbors, we define a neighbor sample size, denoted as n 1 , n 2 , ..., where n i indicates the sample size for the i th hop.The computational tree is built with the root node being the central node v. From the 1-hop neighbor set of v, denoted as N 1 v , we randomly select n 1 nodes to form a new neighbor set N 1 v .If the size of N 1 v is smaller than n 1 , i.e., |N 1 v | &lt; n 1 , we supplement the set with placeholder nodes to reach a size of n 1 .Therefore, the size of N 1 v is consistently n 1 , i.e., | N 1 v | = n 1 .The nodes in N 1 v are treated as children of the root node.Subsequently, for each node in N 1 v , we recursively sample n 2 neighbors as its children.Any sets with insufficient nodes are filled with placeholder nodes.For any placeholder node, its children are exclusively placeholder nodes.As illustrated in upper-left of Figure 1, with the root node being A, we display a 2-hop neighbor structure of A, with the sample size of 3 for both hops.The first-order neighbors of A are {B, C, D}, so they are shown in the second layer of the computational graph.Since B has 2 neighbors {A, G}, we expand this set to {A, G, [pad]}, where [pad] represents the placeholder node.And similarly for nodes C and D. Ultimately, this process yields a perfect 3-ary computational tree centered around node A. We then perform a level-order traversal on the computational tree, transforming the comprehensive details of the central node and its neighborhood into a fixed-length node sequence.For instance, in Figure 1, the sequence representing node A and its neighborhood is
A B C D A G [pad] A [pad] [pad] A E F
, where each sequence position uniquely corresponds to a relative structural position within the original graph.</p>
<p>Post conversion of the center node and its structural information into a node sequence, we shift to mapping them into the node embedding space.In the context of text-attributed graphs, we can utilize various off-the-shelf text encoding models ϕ, such as SBERT (Reimers &amp; Gurevych, 2019), RoBERTa (Liu et al., 2019), and SimTeG (Duan et al., 2023), to encode text features.Placeholder nodes are represented by a zero vectors of the same size.We further integrate a Laplacian Embedding (Dwivedi &amp; Bresson, 2020) at each sequence position, enhancing the representation of structural information.Denoting the adjacency matrix of the computational tree by A tree , the Laplacian Embedding is defined as the eigenvectors of the Laplacian matrix of A tree :
L = I − D − 1 2 A tree D − 1 2 = U T ΛU (1)
where D represents the degree matrix of A tree and U symbolizes the Laplacian Embedding of the template.Notably, with a fixed sample size, the computational tree's shape remains unchanged, so the Laplacian Embedding is computed only once for all graphs using this template.This Embedding is then appended to the encoded node feature to form the final node embedding.The process is outlined as follows: Let v 1 , v 2 , ..., v n represent the encoded node sequence.The final node embedding h vi for v i is given by
h vi = 0 || U i , if v i = [pad]; ϕ(x vi ) || U i , otherwise,(2)
where || denotes concatenation.Subsequently, the central node and its structural information are transformed into the node embedding sequence h v1 , h v2 , ..., h vn .</p>
<p>Hop-Field Overview Template provides a summarized view of the central node and its neighborhood.This template employs hop embeddings to characterize the node features across various neighborhood hops.These hop embeddings are obtained through parameter-free message passing on encoded text features.For each central node v, the i th -hop embedding h i v is calculated as follows:
h i v = 1 |N 1 v | v ′ ∈N 1 v h i−1 v ′ ,(3)
where h 0 x = ϕ(x v ).Through this calculation, h i v potentially contains information from all neighbors in the i th -hop neighborhood set
N i v . A sequence of hop embeddings h 0 v , h 1 v , h 2 v , . .
. can represent the central node and its structural information.Unlike the Neighborhood Detail Template, which utilizes individual embeddings for each neighbor, the Hop-Field Overview Template summarizes each hop's neighbors with a single embedding.This approach may sacrifice some detail for the sake of a broader respective field.The choice between these templates should be based on the nature of the input data and the required level of detail.</p>
<p>To enhance the natural comprehension of graph inputs by Large Language Models (LLMs), it is essential to align the node embedding space with the input token space.This alignment is realized by mapping each node embedding into the token embedding space, utilizing a specifically calibrated projector, denoted as f θ .Mathematically, this process can be represented for a given node embedding h i as:
e i = f θ (h i ).(4)
Consequently, a sequence of node embeddings, h 1 , h 2 , ..., h n , is transformed into a corresponding sequence of token embeddings, e 1 , e 2 , ..., e n .In our framework, this transformation is facilitated by a simple MLP serving as the projector.It is important to note that the parameters θ of the projector are the only parameters subject to tuning during the training process of LLaGA.</p>
<p>Alignment Tuning</p>
<p>In LLaGA, we employ three key tasks on graphs -node classification, link prediction, and node description -to meticulously tune the projector.The first two tasks, node classification and link prediction, are well-established and widely recognized in the field of graph ML.Contrastingly, the node description task, which is somewhat less common in conventional graph analysis, is designed to align node embeddings with specific descriptive texts.This innovative task enables the provision of rich semantic interpretations of the graphs, providing a deeper insight of the logic behind graphbased predictions.The questions and answers to this task can be articulated as follows: Questions: Please describe the center node: <node sequence>.Answers: The center node represents a [paper / products /...], it's about [node description].For textual-attributed graphs, the node description can be obtained from node features.By integrating these three diverse tasks into the training process, our projector develops a comprehensive and nuanced understanding of graphs and can serve as a versatile translator between node embedding and token embedding space for all those tasks.Moreover, it can explicitly generate explanations for node embeddings, enhancing interpretability.</p>
<p>During training, we organize our questions and answers in a chat format.In our experiments, Vicuna (Chiang et al., 2023) serves as the primary foundational Large Language Model (LLM), so we follow the implementation strategy of Vicuna and set the system message accordingly.For details regarding the question-answer template and the training or inference input sequences, please refer to the illustrations in Figure 1.In the input processing phase, we tokenize all words in the prompt and convert them into their respective token embeddings.For the <node sequence>, we substitute this part with the projected node embeddings e 1 , e 2 , ..., e n , maintaining their original positions.The training objective is to maximize the probability of generating the correct answer, formulated as maximize θ</p>
<p>p(X answer |X graph , X question , X system ).(5)</p>
<p>Experimental Results</p>
<p>We conduct comprehensive experiments to validate the effectiveness of our framework across various settings, aiming to address several key research questions:</p>
<p>• RQ1: How does LLaGA perform in comparison to baseline models in standard graph tasks, such as node classification and link prediction?</p>
<p>• RQ2: How good are the interpretations generated by LLaGA for node embeddings?</p>
<p>• RQ3: How effectively does the model transfer knowledge when adapting to new datasets or tasks in zero-shot?</p>
<p>• RQ4: What is the contribution of our encoding templates to the overall performance?</p>
<p>Setup</p>
<p>Datasets.We train and evaluate our model on four widelyrecognized graph datasets: ogbn-Arxiv (Hu et al., 2020), ogbn-Products (Hu et al., 2020), Pubmed, and Cora (Yang et al., 2016).These datasets span domains of citation networks and e-commerce, varying in terms of sparsity and size, ranging from small to large scales.Detailed statistics and data splitting methods are presented in Appendix A.</p>
<p>Tasks.Baselines.In our comparative analysis, we benchmark our framework against three categories of state-of-the-art models to ensure a thorough evaluation.The first category comprises Graph Neural Networks, including GCN (Kipf &amp; Welling, 2016), GraphSage (Hamilton et al., 2017), GAT (Veličković et al., 2018), SGC (Wu et al., 2019), and SAGN (Sun et al., 2021).The second category encompasses transformer-based graph models, NodeFormer (Wu et al., 2022).The final category is represented by GPT-3.5, a leading general LLM.For the first two categories, identical text-encoding methods are employed to encode text features, ensuring a fair comparison.For GPT-3.5, we utilized node classification results from the survey by Chen et al. (Chen et al., 2023a) and extended this approach to the link prediction task by employing a consistent graph-description prompt format.In addition, we also compare with the concurrent work, GraphGPT (Tang et al., 2023).</p>
<p>Overall Performance Comparison (RQ1)</p>
<p>We compare our LLaGA model with various baselines across four distinct settings: Single Focus, Task Expert, Classification Expert, and General Model.The Single Focus setting involves models trained on a single dataset for a specific task, thereby concentrating exclusively on that task.Task Expert refers to models trained across all datasets but focused on a single task, enabling them to perform as specialists in that area.In the Classification Expert setting, models are trained on all datasets for both node classification and link prediction tasks.The General Model is trained for node classification, link prediction, and node description across all datasets, equipping the model to handle not just classification tasks but also semantic tasks like node description.The comparative results are presented in Table 1.Notably, when implementing the GNN-based or Transformer-based baselines in the Task Expert or Classification Expert settings, they were trained using a multi-task learning approach, which incorporates a shared backbone with task-specific classification heads for different datasets or tasks.In contrast, our LLaGA framework employs a single projector to handle all tasks.</p>
<p>Comparision with Baselines: Our analysis reveals three key observations.Observation 1: LLaGA framework demonstrates superior performance compared to baseline models across all settings, particularly in multi-task learning scenarios.This highlights LLaGA's versatility and robust capability in addressing various graph tasks.</p>
<p>Observation 2: While many baseline models experience significant performance degradation in multi-task learning scenarios, LLaGA stands out by exhibiting minimal decline or even improvements in performance.This reflects LLaGA's proficiency in extracting common patterns across different datasets and tasks.Such a trait hints at the potential for developing a powerful multi-model LLM equipped with simple projectors.Observation 3: Both the Neighborhood Detail Template and the Hop-Field Overview Template exhibit distinct advantages.The Neighborhood Detail Template excels in tasks requiring detailed neighbor information, whereas the Hop-Field Overview Template is more effective in tasks that depend on a broader overview of neighbor information with a larger receptive field.For instance, in identifying product categories, it is illogical to classify a product as 'Video Games' based solely on many of its neighbors being 'Electronics'.A more detailed analysis, revealing numerous 'Nintendo Switch' neighbors, makes classification more accurate, as seen in the case of the ogbn-Products dataset.Conversely, for some citation graphs, an overview of a paper's neighboring categories can be more informative, making the Hop-Field Overview Template the preferable choice.</p>
<p>Comparison with Concurrent Work: We conduct</p>
<p>Interpretation Ability Investigation (RQ2)</p>
<p>As previously stated, our LLaGA framework excels in providing comprehensive interpretations of node embeddings.We initially assess LLaGA's performance in the node description task using several quantitative metrics, with results presented in Table 4.The Sbert Score indicates the semantic similarity between the ground truth and LLaGA-generated text, measured using Sbert.We also include a Base value for your reference, representing the average similarity across two randomly chosen samples.Notably, LLaGA's Sbert score significantly exceeds this base value, demonstrating its effectiveness in generating meaningful and relevant descrip- tions for node embeddings.Furthermore, the high accuracy in extracting labels from these descriptions corroborates the precision of the generated content.</p>
<p>To further illustrate this, Table 3 showcases sample descriptions.These examples indicate the high quality of text produced by LLaGA.Even in some instances where LLaGA's label predictions diverge from the ground truth, its results are found to be reasonable and LLaGA effectively utilizes its generated text to substantiate these plausible interpretations.</p>
<p>Zero-Shot Ability Investigation (RQ3)</p>
<p>In this section, we illustrate the generalization capabilities of LLaGA, concentrating on the task of link prediction within a Zero-shot learning entails training a model on certain datasets and subsequently evaluating it on unseen datasets or tasks.This approach is instrumental in assessing a model's proficiency in transferring knowledge.In our study, we examine LLaGA's zero-shot performance in both in-domain and out-of-domain transfer scenarios.For in-domain transfer, the model is trained on the Arxiv and Pubmed datasets and evaluated on the Cora dataset.All three datasets comprise citation graphs.Conversely, for out-of-domain transfer, training is conducted on the Arxiv, Pubmed, and Cora datasets, with the evaluation on the Products dataset.Here, while the training datasets are citation graphs, the test set consists of e-commerce graphs.The results, as presented in Table 5, reveal that our model exhibits robust zero-shot capabilities in both scenarios.This indicates that LLaGA can effectively discern and leverage similar patterns across datasets, adeptly transferring knowledge not only to analogous data but also to datasets that markedly differ in domain.</p>
<p>Templates Ablation Study (RQ4)</p>
<p>We conduct an ablation study to investigate the individual contributions of our encoding templates.For this, we train a new model in a classification expert setting, but without using a template.This model solely relies on the embedding of the center node for prediction, rather than a node embedding sequence that encapsulates structural information surrounding the center node.The results are presented in Table 6.It is evident that both the Neighborhood Detail Template and the Hop-Field Overview Template significantly enhance performance compared to the model without a template.This is particularly noticeable in the link prediction task, a task that heavily relies on structural information.All these findings underscore the effectiveness of our templates in encoding the structural information of nodes.</p>
<p>Related Work</p>
<p>Graph Neural Networks</p>
<p>GNNs have long been at the forefront of graph machine learning.They are designed to transform input nodes into compact vector representations, suitable for subsequent classification tasks when paired with a classification head.A common strategy among many GNNs (Kipf &amp; Welling, 2016;Veličković et al., 2018;Xu et al., 2018;Gao et al., 2018;Chiang et al., 2019;You et al., 2020;Chen et al., 2018;Thekumparampil et al., 2018), involves a layer-wise message-passing mechanism.This approach enables nodes to progressively aggregate and process information from their immediate neighbors, thereby embedding the nodes into lower-dimensional spaces.Concurrently, a growing body of research (Yun et al., 2019;Ying et al., 2021;Wu et al., 2022;Chen et al., 2022), has been exploring the integration of transformer-based encoders for graph data analysis, opening new avenues for enhancing GNN capabilities.However, a significant limitation of traditional graph models is their poor task generalization capability.GNNs are usually trained on a single classification task.When applied to a variety of datasets or downstream tasks, these models often fail to perform consistently well across all tasks with one single model (Ju et al., 2023).</p>
<p>Self-Supervised Learning for GNNs</p>
<p>Recent advancements have employed self-supervised learning strategies on GNNs to bolster their generalization performance.These methods encompass developing specialized pretext tasks for graph structures, such as mutual information maximization (Veličković et al., 2019;Hassani &amp; Khasahmadi, 2020), whitening decorrelation (Zhang et al., 2021), and generative reconstruction (Hou et al., 2022).Moreover, investigations into integrating multi-task learning with self-supervised learning paradigms have been conducted, offering novel insights into enhancing model generalization ability (Ju et al., 2023).However, these methods still require task-specific classification heads and tuning for every downstream task, after obtaining a general embedding from the graph encoder.</p>
<p>Large Language Models for Graphs</p>
<p>Recent studies have explored integrating Large Language Models (LLMs) with GNNs, leveraging LLMs' extensive knowledge for graph data enhancement.Research has focused on augmenting GNNs with LLMs to enrich graph textual attributes (Ye et al., 2023;Chen et al., 2023b;Tang et al., 2023;Guo et al., 2023;He et al., 2023;Huang et al., 2023), though these approaches largely depend on GNNs for predictions, potentially limiting their scope.Alternatively, efforts to linguistically represent graphs for direct LLM processing encountered difficulties in effectively translating structures into natural language, often yielding suboptimal results (Huang et al., 2023;Guo et al., 2023).While fine-tuning LLMs for graphs can improve performance on specific tasks, it may also limit the LLMs' versatility.GraphGPT (Tang et al., 2023) sought to address these challenges by using a pretrained graph transformer for encoding graph structures for LLMs, though finding a universally applicable graph model proved difficult.Our contribution diverges by introducing a novel encoding method that translates graph data into sequences directly compatible with LLMs, avoiding the need for intermediary models.This method shows superior versatility and generalizability across a range of tasks, even in zero-shot scenarios, outperforming traditional graph models.</p>
<p>Conclusion</p>
<p>This paper introduces LLaGA, an innovative framework that effectively integrates Large Language Models (LLMs) into the graph domain while preserving their proficiency in other tasks.Instead of using complex language for describing structure information, LLaGA employs templates to transform graph structure into sequences, and then maps node embeddings to token embedding spaces using a tuned projector.This projector establishes a comprehensive alignment between texts and graphs, enabling the use of LLMs for fundamental graph tasks like node classification and link prediction across various datasets.And it can be further generalized to unseen datasets or tasks without any adaption.Additionally, it facilitates the generation of textual explanations for node embeddings.Through extensive evaluations in different settings, our method has demonstrated its effectiveness in both supervised and zero-shot graph learning scenarios.</p>
<p>Impact Statements</p>
<p>Our research introduces LLaGA, a novel framework that seamlessly blends the capabilities of Large Language Models (LLMs) with graph structures, enhancing the versatility of LLMs to perform fundamental graph tasks.The broader impact of LLaGA extends to numerous fields where graph data is pivotal, including but not limited to, bioinformatics, social network analysis, and knowledge graphs.As we push the boundaries of Machine Learning and AI, we recognize the importance of monitoring for unintended consequences, such as the perpetuation of biases or misuse of predictive insights.this end, we encourage continued ethical evaluation and the development of guidelines to ensure that the applications of LLaGA contribute constructively to society.This work aspires to be a stepping stone towards more sophisticated, equitable, and transparent AI systems that respect the intricate structure of data across various domains.</p>
<p>A. Dataset Statistics  To explore the generalization capabilities of LLaGA, we also employed zero-shot learning for node classification tasks.Unlike link prediction tasks, applying zero-shot learning to node classification presents greater challenges due to the distinct label sets and the varied knowledge requirements across tasks.However, a universal aspect potentially transferable across all node classification tasks is the alignment between the graph and the semantic token space.To this end, we trained models on node description tasks from certain datasets to establish a generalized alignment between the graph structure and the token space, subsequently testing this alignment on node classification tasks using different datasets.Furthermore, we assessed LLaGA's zero-shot performance in both in-domain and out-of-domain transfer scenarios.In the in-domain scenario, training was performed on citation graphs (Arxiv + Pubmed), with testing conducted also on citation graphs (Cora).However, the out-of-domain scenario involved training on citation graphs (Arxiv + Pubmed + Cora), with testing on the e-commerce graphs (Products).Since traditional GNNs depend on task-specific classification heads and new classification tasks may vary in label sets, they are unable to conduct zero-shot learning on node classification tasks.Our comparison was limited to llm-based baselines, specifically GraphGPT.</p>
<p>Our evaluation encompasses two kinds of prompts.In the first prompt, the model is only supplied with node embedding sequences, containing both attribute and structural information of the central node.The second prompt enhances this by also incorporating the textual attributes of the central node to assist the model.As detailed in Table 8, our findings reveal that LLaGA consistently outperforms GraphGPT across all settings.This superiority is attributed to LLaGA's comprehensive alignment between the graph space and the token space.Moreover, the inclusion of the central node's textual attributes appears to offer some advantages in zero-shot scenarios.However, prompts based solely on node sequence embeddings show potential for application to graphs whose node attributes are challenging to describe textually, such as non-textual graphs.LLaGA demonstrates flexibility in its text encoding methods for node attributes.In our initial experiments, we employed SimTeG (Duan et al., 2023) as the primary encoding model.This section also explores the use of SBERT (Reimers &amp; Gurevych, 2019) and RoBERTa (Liu et al., 2019) as alternative encoding methods.The outcomes of these trials are shown in Table 9.All models, including baselines, underwent training in a classification expert setting.For LLaGA, we utilized the Hop-Field Overview Template for structure encoding.Notably, LLaGA consistently surpassed other leading GNNs in performance, regardless of the chosen encoding model.LLaGA also demonstrates flexibility with various Base Large Language Models (LLMs).In our primary experiments, Vicuna-7B served as the foundational model.This section details the substitution of LLaGA's base LLM with alternative models, including LLaMA2-7B and OPT-2.7B.The outcomes of these replacements are presented in Table 10.For structural encoding, we employ the Hop-Field Overview Template.And models are trained in classification setting.It is evident that LLaGA consistently yields favorable results irrespective of the base LLM, showcasing its effectiveness even with comparatively lighter models such as OPT-2.7B.We perform training and inference five times on relatively small datasets, with the variance information detailed in Table 11.</p>
<p>C. Flexibility with Text Encoding Methods</p>
<p>D. Integration with Various LLMs</p>
<p>E. Experiment Variance</p>
<p>Figure 1 .
1
Figure 1.Illustration of LLaGA framework and its prompt design paradigm.</p>
<p>(Duan et al., 2023)3)n task, we predict the existence of edges between node pairs.The node description task involves generating node descriptions based on encoded node embeddings.The training ground truth is derived from classification labels and text features, structured as: The center node represents a paper/product in the [label] domain, it's about [text feature].Evaluation Metrics.For evaluation metrics, we employ Accuracy for both node classification and link prediction tasks, Sbert score and Description Label Accuracy for the node description task.The Sbert score measures the similarity between embeddings of the generated descriptions and the ground truth descriptions encoded by Sbert.Description Label Accuracy represents the Accuracy of labels inferred from node descriptions.For LLaGA framework, a sample is considered accurate only if it precisely identifies the category's full name in its response.Implementation Details.In our model's implementation, we primarily employ Vicuna-7B-v1.5-16K(Chiangetal., 2023)as the foundational base models, and SimTeg(Duan et al., 2023)as default text-encoding model.Additionally, we conduct a comparative analysis of various base LLMs and embeddings in Appendix C and D. The learning rate is consistently set to 2e-5, and the batch size is maintained at 16 for all models.We train our model for one epoch.However, to compensate for the limited data size, we replicate the training samples from the smallest dataset, Cora, three times.For the Neighborhood Detail Template, we sample two-hop neighbors around each node, setting the sample size to 10 for each hop.In the Hop-Field Overview Template, 4 hop embeddings are employed to encapsulate the structural information surrounding the central node.We denote LLaGA implementations with the Neighborhood Detail Template and Hop-Field Overview Template as LLaGA-ND-7B and LLaGA-HO-7B, respectively.</p>
<p>Our model utilizes LLaGA for 3 tasks: node classification, link prediction, and graph-based node description.The targets of node classification are to categorize nodes based on research topics or product characteristics.</p>
<p>Table 1 .
1
Performance comparison with baseline models on both node classification and link prediction under 4 settings.Single Focus denotes models trained on a single task and dataset.Task Expert refers to models trained exclusively on one task across all datasets, specializing in that task.Classification Expert indicates models trained in both node classification and link prediction on all datasets, becoming proficient in classification tasks.General Model are capable of handling classification tasks across datasets and excel in semantic tasks, such as generating interpretable descriptions for node embeddings.(bold signifies the best result across all methods, while underline highlights the best baseline result under this setting)
MODEL TYPEMODELNODE CLASSIFICATION ACCURACY(%) ARXIV PRODUCTS PUBMED CORA ARXIV PRODUCTS PUBMED CORA LINK PREDICTION ACCURACY(%)GCN73.7280.7592.9688.9391.4393.9590.9181.59GRAPHSAGE76.2982.8794.8788.8991.6494.9690.6479.15GAT74.0683.0692.3388.9785.9993.8583.9680.06SINGLESGC71.7775.4787.3587.9787.9988.5183.6080.94FOCUSSAGN75.7082.5895.1789.1990.6294.8590.4879.88NODEFORMER74.8583.7294.9088.2391.8490.9377.6977.26LLAGA-ND-7B75.9884.6095.0388.8691.2497.3691.4183.79LLAGA-HO-7B76.6684.6795.0389.2294.1595.5689.1886.82GCN71.4580.8889.2581.6288.5193.5481.0178.88GRAPHSAGE72.5682.5094.1581.9987.7693.4976.1480.74TASKGAT72.1982.6187.9783.5882.5892.0376.8579.76EXPERTNODEFORMER72.3582.9994.4183.2784.1193.4280.4081.03LLAGA-ND-7B76.4184.6094.7888.1991.2097.3893.2789.41LLAGA-HO-7B76.4084.1895.0689.8594.3695.8588.8887.50GCN70.9580.0289.0082.7787.6992.8872.2878.35GRAPHSAGE71.9181.6291.8182.4489.2392.2275.3682.09CLASSIFICATIONGAT70.9081.8387.7282.0785.1892.1175.0080.35EXPERTNODEFORMER63.2075.5589.5069.1982.3375.4278.2281.47LLAGA-ND-7B75.8583.5895.0687.6490.8196.5692.3687.35LLAGA-HO-7B75.9983.3294.8089.3094.3096.0588.6488.53GENERAL MODELGPT3.5-TURBO LLAGA-ND-7B55.00 74.2975.25 82.2188.00 92.4271.75 87.8263.80 90.5360.30 96.8268.70 86.3165.74 81.91LLAGA-HO-7B75.0182.0794.4587.8292.0486.8089.8184.41</p>
<p>Table 2 .
2
Compare with Concurrent Work.
MODELARXIV PUBMED PUBMED NC NC LPGRAPHGPT-MIX-7B64.7674.1658.86GRAPHGPT-STD-7B63.90-80.26LLAGA-ND-7B(GENERAL)74.2992.4286.31LLAGA-HO-7B(GENERAL)75.0194.4589.81a comparative analysis with our concurrent work,GraphGPT (Tang et al., 2023). GraphGPT is a generaliz-able model designed for solving graph tasks using LLM. Itemploys a text-encoding model to extract node features andutilizes a pre-trained graph transformer for encoding struc-tural information. In our comparison, we focus on our mostrobust and generalizable models, with the results detailedin</p>
<p>Table 2
2
, GraphGPT's results are referenced directly from its original paper.'Mix' and 'Std' represent two categories of prompts used in GraphGPT's training process.LLaGA's most general model is trained across 12 tasks, including node classification, link prediction, and node description on datasets such as Arxiv, Products, Pubmed, and Cora.In contrast, GraphGPT's most general model is trained on just three tasks: node classification on Arxiv and Pubmed, and link prediction on Pubmed.But our model still demonstrates superior performance on these three tasks, highlighting the efficacy of our LLaGA framework.</p>
<p>Table 3 .
3
Examples Demonstrating the Interpretability of the LLaGA Framework.Title: Real time hand gesture detection and classification using convolutional neural networks.Abstract: Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget.In this work, ... Label: cs.CV(Computer Vision and Pattern Recognition) USER: Please describe the center node: <node embedding sequence>.LLaGA:This node represents a paper in cs.CV(Computer Vision and Pattern Recognition) domain, it's about learning to detect and recognize hand gestures using convolutional neural networks.Title:An efficient graph convolutional network technique for the travelling salesman problem.Abstract: This paper introduces a new learning-based approach for approximately solving the Travelling Salesman Problem on 2D Euclidean graphs.We use deep Graph Convolutional Networks to TSP graph representations and output tours in a non-autoregressive manner via highly parallelized beam search.Our approach ... Label: cs.LG(Machine Learning) USER: Please describe the center node: <node embedding sequence>.LLaGA: This node represents a paper in cs.LG(Machine Learning) domain, it's about deep graph convolutional networks for solving the traveling salesman problem.Inverse boosting pruning trees for depression detection on twitter Abstract: Depression is one of the most common mental health disorders, and a large number of depression people commit suicide each year... We argue that it is feasible to identify depression at an early stage by mining online social behaviours.Our approach, ... Label: cs.LG(Machine Learning) USER: Please describe the center node: <node embedding sequence>.LLaGA: This node represents a paper in cs.SI(Social and Information Networks) domain, it's about predicting suicide risk using social media data.(Label is different from ground truth, but also reasonable)</p>
<p>Title:</p>
<p>Table 4 .
4
Quantitative evaluation of the node description task using Sbert Score and Description Label Accuracy.The term Base value refers to the mean Sbert similarity calculated between the ground truth descriptions of two randomly selected samples.
DATASETMODELBASE VALUESBERT SCOREACCARXIVLLAGA-ND-7B LLAGA-HO-7B0.22310.6023 74.64 0.6228 75.49PRODUCTSLLAGA-ND-7B LLAGA-HO-7B0.15130.4952 83.18 0.5193 84.60PUBMEDLLAGA-ND-7B LLAGA-HO-7B0.48690.6847 92.27 0.6934 94.27CORALLAGA-ND-7B LLAGA-HO-7B0.32210.6465 86.72 0.6545 86.90</p>
<p>Table 5 .
5
Zero-Shot on Link Prediction
TRAIN → TESTMODELACCURACYGCN58.97ARXIV+PUBMEDGRAPHSAGE67.68↓GRAPHGPT-7B50.74CORALLAGA-ND-7B86.47LLAGA-HO-7B87.35GCN56.73ARXIV+PUBMED+CORAGRAPHSAGE58.92↓GRAPHGPT-7B50.74PRODUCTSLLAGA-ND-7B92.65LLAGA-HO-7B92.99zero-shot setting. For analysis of generalization capabilitiesin node classification tasks, please refer to Appendix B.</p>
<p>Table 6 .
6
Templates Ablation Study.
TASK TEMPLATE ARXIV PRODUCTS PUBMED CORANONE73.9280.4594.6084.50NCND75.8583.5895.0687.64HO75.9983.3294.8089.30NONE89.9891.7378.1983.97LPND90.8196.5692.3687.35HO94.3096.0588.6488.53</p>
<p>Table 7 .
7
(Hu et al., 2020)s Arxiv, Pubmed, Cora), each node represents a paper, where the title and abstract serve as node features, and edges denote co-citations.For ogbn-Products, nodes represent Amazon products, featuring item descriptions as node features, with edges indicating co-purchases.Data Split.For node-level tasks, we adhere to the standard train/validation/test splits(Hu et al., 2020)for each dataset: 6:2:3 for Arxiv, 8:2:90 for Products, and 6:2:2 for both Pubmed and Cora.For link prediction, we randomly select node pairs from the node-level training set for training and from the node-level test set for testing, ensuring the edge-level training sets are equal in size to the node-level training sets.
DatasetDomain#Node#EdgeSparsity(‱)Coracitation2708542914.8065Pubmedcitation19717443382.2810Arxivcitation16934311662430.8134Products e-commerce 2449029 618591400.2063In citation graphs (ogbn-
B. Zero-Shot Ability on Node Classification</p>
<p>Table 8 .
8
Zero-Shot on Node Classification
TRAIN → TESTPROMPT TYPEMODELACCURACY(%)ARXIV+PUBMED → CORAONLY NODE EMBEDDINGGRAPHGPT-7B LLAGA-7B8.30 34.69(TEST TASK: 7 CATEGORIES)NODE EMBEDDING+TEXT ATTRIBUTESGRAPHGPT-7B LLAGA-7B44.65 59.59ARXIV+PUBMED+CORA → PRODUCTSONLY NODE EMBEDDINGGRAPHGPT-7B LLAGA-7B1.40 13.89(TEST TASK: 47 CATEGORIES)NODE EMBEDDING+TEXT ATTRIBUTESGRAPHGPT-7B LLAGA-7B18.84 43.79</p>
<p>Table 9 .
9
LLaGA Trained with SBert and Roberta Embedding.
EMBEDDINGMODELNODE CLASSIFICATION ACCURACY ARXIV PRODUCTS PUBMED CORALINK PREDICTION ACCURACY ARXIV PRODUCTS PUBMED CORAGCN66.0077.4182.0479.7091.3894.9184.3183.15SBERTGRAPHSAGE66.7976.0082.7480.6688.1894.2378.3883.62LLAGA74.4680.7090.0488.5693.6896.8491.3987.79GCN66.5177.7480.0479.3091.0194.6680.9481.03ROBERTAGRAPHSAGE68.1476.7381.2782.2988.8094.1174.3182.88LLAGA74.1981.1389.7888.1993.5296.7989.9685.15</p>
<p>Table 10 .
10
Integration with Various LLMs
BASE MODELNODE CLASSIFICATION ACCURACY ARXIV PRODUCTS PUBMED CORAARXIVLINK PREDICTION ACCURACY PRODUCTS PUBMEDCORAVICUNA-7B75.9983.3294.8089.3094.3096.0588.6488.53LLAMA2-7B76.2684.2194.8386.5394.1596.0389.3985.44OPT-2.7B75.6683.0195.0188.3893.3692.8386.9289.41</p>
<p>Table 11 .
11
Variance Information on Cora and Pubmed Dataset
SETTINGDATASETMODELNC(%)LP(%)SINGLE FOCUSCORA PUBMEDLLAGA-ND-7B 88.86±0.78 83.79±1.26 LLAGA-HO-7B 89.22±0.46 86.82±0.88 LLAGA-ND-7B 95.03±0.12 91.41±0.21 LLAGA-HO-7B 95.03±0.07 89.18±0.34</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Fastgcn: fast learning with graph convolutional networks via importance sampling. J Chen, T Ma, Xiao , C , arXiv:1801.102472018arXiv preprint</p>
<p>Nagphormer: A tokenized graph transformer for node classification in large graphs. J Chen, K Gao, G Li, K He, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, arXiv:2307.033932023aarXiv preprint</p>
<p>Label-free node classification on graphs with large language models (llms). Z Chen, H Mao, H Wen, H Han, W Jin, H Zhang, H Liu, J Tang, arXiv:2310.046682023barXiv preprint</p>
<p>Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. W.-L Chiang, X Liu, S Si, Y Li, S Bengio, C.-J Hsieh, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019</p>
<p>An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna, March 2023</p>
<p>Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems. M Defferrard, X Bresson, P Vandergheynst, 201629</p>
<p>K Duan, Q Liu, T.-S Chua, S Yan, W T Ooi, Q Xie, J He, Simteg, arXiv:2308.02565A frustratingly simple approach improves textual graph learning. 2023arXiv preprint</p>
<p>A generalization of transformer networks to graphs. V P Dwivedi, X Bresson, arXiv:2012.096992020arXiv preprint</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J Halcrow, B Perozzi, arXiv:2310.045602023arXiv preprint</p>
<p>Large-scale learnable graph convolutional networks. H Gao, Z Wang, Ji , S , Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2018</p>
<p>Can large language models understand graph structured data? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, Gpt4graph, arXiv:2305.150662023arXiv preprint</p>
<p>Inductive representation learning on large graphs. W Hamilton, Z Ying, J Leskovec, Advances in neural information processing systems. 201730</p>
<p>Contrastive multiview representation learning on graphs. K Hassani, A H Khasahmadi, International conference on machine learning. PMLR2020</p>
<p>Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. X He, X Bresson, T Laurent, A Perold, Y Lecun, B Hooi, arXiv:2305.195232023arXiv preprint</p>
<p>Self-supervised masked graph autoencoders. Z Hou, X Liu, Y Cen, Y Dong, H Yang, C Wang, J Tang, Graphmae, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. W Hu, M Fey, M Zitnik, Y Dong, H Ren, B Liu, M Catasta, J Leskovec, Advances in neural information processing systems. 202033</p>
<p>Can llms effectively leverage graph structural information: when and why. J Huang, X Zhang, Q Mei, J Ma, arXiv:2309.165952023arXiv preprint</p>
<p>Automated self-supervised learning for graphs. W Jin, X Liu, X Zhao, Y Ma, N Shah, J Tang, arXiv:2106.054702021arXiv preprint</p>
<p>Multi-task self-supervised graph neural networks enable stronger task generalization. M Ju, T Zhao, Q Wen, W Yu, N Shah, Y Ye, C Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Semi-supervised classification with graph convolutional networks. T Kipf, M Welling, ArXiv, abs/1609.029072017</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, International Conference on Learning Representations. 2016</p>
<p>Crafting papers on machine learning. P Langley, Proceedings of the 17th International Conference on Machine Learning (ICML 2000). P Langley, the 17th International Conference on Machine Learning (ICML 2000)Stanford, CAMorgan Kaufmann2000</p>
<p>H Liu, C Li, Q Wu, Y J Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Discovering language model behaviors with modelwritten evaluations. E Perez, S Ringer, K Lukošiūtė, K Nguyen, 2022</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. N Reimers, I Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Scalable and adaptive graph neural networks with self-label-enhanced training. C Sun, H Gu, J Hu, arXiv:2104.093762021arXiv preprint</p>
<p>J Tang, Y Yang, W Wei, L Shi, L Su, S Cheng, D Yin, C Huang, Graphgpt, arXiv:2310.13023Graph instruction tuning for large language models. 2023arXiv preprint</p>
<p>Attention-based graph neural network for semisupervised learning. K K Thekumparampil, C Wang, S Oh, L.-J Li, arXiv:1803.037352018arXiv preprint</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Graph attention networks. P Veličković, G Cucurull, A Casanova, A Romero, P Lio, Y Bengio, arXiv:1710.109032017arXiv preprint</p>
<p>Deep graph infomax. P Veličković, W Fedus, W L Hamilton, P Liò, Y Bengio, R D Hjelm, 2019</p>
<p>Graph attention networks. P Veličković, G Cucurull, A Casanova, A Romero, P Liò, Y Bengio, International Conference on Learning Representations. 2018</p>
<p>Large language model is also an open-ended decoder for visioncentric tasks. W Wang, Z Chen, X Chen, J Wu, X Zhu, G Zeng, P Luo, T Lu, J Zhou, Y Qiao, arXiv:2305.111752023arXiv preprint</p>
<p>Simplifying graph convolutional networks. F Wu, A Souza, T Zhang, C Fifty, T Yu, K Weinberger, International conference on machine learning. PMLR2019</p>
<p>Nodeformer: A scalable graph structure learning transformer for node classification. Q Wu, W Zhao, Z Li, D P Wipf, J Yan, Advances in Neural Information Processing Systems. 202235</p>
<p>How powerful are graph neural networks?. K Xu, W Hu, J Leskovec, S Jegelka, International Conference on Learning Representations. 2018</p>
<p>Revisiting semi-supervised learning with graph embeddings. Z Yang, W Cohen, R Salakhudinov, International conference on machine learning. PMLR2016</p>
<p>R Ye, C Zhang, R Wang, S Xu, Y Zhang, arXiv:2308.07134Natural language is all a graph needs. 2023arXiv preprint</p>
<p>Do transformers really perform badly for graph representation?. C Ying, T Cai, S Luo, S Zheng, G Ke, D He, Y Shen, T.-Y Liu, Advances in Neural Information Processing Systems. 342021</p>
<p>L2-gcn: Layerwise and learned efficient training of graph convolutional networks. Y You, T Chen, Z Wang, Y Shen, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020. 2020</p>
<p>Graph transformer networks. S Yun, M Jeong, R Kim, J Kang, H J Kim, Advances in neural information processing systems. 201932</p>
<p>From canonical correlation analysis to self-supervised graph neural networks. H Zhang, Q Wu, J Yan, D Wipf, P S Yu, Advances in Neural Information Processing Systems. 202134</p>            </div>
        </div>

    </div>
</body>
</html>
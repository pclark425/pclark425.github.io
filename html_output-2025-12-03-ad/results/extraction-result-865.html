<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-865 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-865</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-865</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-9ca19acce35fd440cb9ffa504907f36a2e176bbc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9ca19acce35fd440cb9ffa504907f36a2e176bbc" target="_blank">LOA: Logical Optimal Actions for Text-based Interaction Games</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Logical Optimal Actions (LOA), an action decision architecture of reinforcement learning applications with a neuro-symbolic framework which is a combination of neural network and symbolic knowledge acquisition approach for natural language interaction games is presented.</p>
                <p><strong>Paper Abstract:</strong> We present Logical Optimal Actions (LOA), an action decision architecture of reinforcement learning applications with a neuro-symbolic framework which is a combination of neural network and symbolic knowledge acquisition approach for natural language interaction games. The demonstration for LOA experiments consists of a web-based interactive platform for text-based games and visualization for acquired knowledge for improving interpretability for trained rules. This demonstration also provides a comparison module with other neuro-symbolic approaches as well as non-symbolic state-of-the-art agent models on the same text-based games. Our LOA also provides open-sourced implementation in Python for the reinforcement learning environment to facilitate an experiment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e865.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e865.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Optimal Actions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic action-decision architecture that uses Logical Neural Networks (LNN) to map semantic-parser-derived logical facts to recommended text-game actions; designed for text-based, partially observable environments (TextWorld, Jericho) and intended to combine learned rules and symbolic reasoning for interpretable navigation decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Logical Optimal Actions (LOA)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LOA is a neuro-symbolic RL framework that receives natural language observations, passes them through a language-understanding component (a simple semantic parser) to produce logical facts/propositions, and inputs those facts into a Logical Neural Network (LNN). The LNN implements weighted logical functions (rules and constraints) whose proposition truth-values and contradiction losses determine activation of candidate action nodes (e.g., 'go north', 'go east'); the LNN can be trained with reinforcement signals (rewards) so the decision network learns which logical-action mappings are optimal. The demo includes pre-defined logical rules (external knowledge) encoded in LNN and visualizes proposition nodes, logical-function nodes, action candidate nodes, and contradiction losses for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (also demo mentions Jericho and commonsense cleanup TextWorld variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>TextWorld is a text-based interactive game environment where the agent receives natural language observations describing the current room, exits, objects, etc. The environment is partially observable because textual observations do not present the full global state (e.g., unseen rooms, hidden object locations, and past visited-room identity must be inferred or remembered). Challenges include language understanding, long-term memory (visited rooms), exploration, and commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>A simple semantic parser that converts raw natural language observations into symbolic logical propositions; pre-defined external knowledge encoded as logical rules inside the LNN (i.e., a symbolic knowledge base represented within the LNN).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Symbolic logical propositions / boolean-like truth values (proposition activations), LNN-internal scores including contradiction loss and proposition truth probabilities, and action recommendation activations (which actions are logically supported).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Belief is represented symbolically as a set of logical propositions derived from the semantic parser and held as input proposition nodes in the LNN; the LNN maintains proposition truth-values and uses logical-function nodes (rules and constraints) to combine these propositions into action activations and contradiction losses. This provides an interpretable, rule-driven belief representation rather than a distributed vector memory.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>On each time step the environment's textual observation is parsed by the semantic parser into propositions which become the current logical-state inputs to the LNN; the LNN recomputes proposition truth-values, logical inferences, action activations, and contradiction losses. After an action executes and a reward/next observation arrives, the new observation is parsed and fed into the LNN to update the logical belief and consequent action recommendations. The LNN can also encode constraints (e.g., visited-room avoidance) that produce contradiction loss signals used to suppress certain action activations.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Neuro-symbolic decision making via Logical Neural Networks: rule-based logical inference combined with RL training; not presented as explicit search-based planning but as a learned policy over LNN action activations informed by symbolic rules and constraints (i.e., learned policy with symbolic logical reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation is performed by selecting text actions (e.g., 'go north', 'go east') recommended by LNN rule activations; path or room-revisit avoidance is handled via logical constraints within LNN (contradiction loss) rather than explicit path-finding algorithms like A*. No explicit shortest-path planner is described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LOA demonstrates that feeding semantic-parser-produced symbolic facts into an LNN that encodes pre-defined knowledge and learned rules yields interpretable action recommendations in partially observable text games. The LNN's contradiction loss and logical constraints can be used to avoid revisiting rooms and to prune undesirable navigation actions; the approach provides human-interpretable reasons for action choices and (according to the authors) can accelerate RL convergence compared to non-symbolic baselines, though no quantitative results are reported in this demo paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LOA: Logical Optimal Actions for Text-based Interaction Games', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Logical neural networks <em>(Rating: 2)</em></li>
                <li>Reinforcement learning with external knowledge by using logical neural networks <em>(Rating: 2)</em></li>
                <li>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-865",
    "paper_id": "paper-9ca19acce35fd440cb9ffa504907f36a2e176bbc",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "LOA",
            "name_full": "Logical Optimal Actions",
            "brief_description": "A neuro-symbolic action-decision architecture that uses Logical Neural Networks (LNN) to map semantic-parser-derived logical facts to recommended text-game actions; designed for text-based, partially observable environments (TextWorld, Jericho) and intended to combine learned rules and symbolic reasoning for interpretable navigation decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Logical Optimal Actions (LOA)",
            "agent_description": "LOA is a neuro-symbolic RL framework that receives natural language observations, passes them through a language-understanding component (a simple semantic parser) to produce logical facts/propositions, and inputs those facts into a Logical Neural Network (LNN). The LNN implements weighted logical functions (rules and constraints) whose proposition truth-values and contradiction losses determine activation of candidate action nodes (e.g., 'go north', 'go east'); the LNN can be trained with reinforcement signals (rewards) so the decision network learns which logical-action mappings are optimal. The demo includes pre-defined logical rules (external knowledge) encoded in LNN and visualizes proposition nodes, logical-function nodes, action candidate nodes, and contradiction losses for interpretability.",
            "environment_name": "TextWorld (also demo mentions Jericho and commonsense cleanup TextWorld variants)",
            "environment_description": "TextWorld is a text-based interactive game environment where the agent receives natural language observations describing the current room, exits, objects, etc. The environment is partially observable because textual observations do not present the full global state (e.g., unseen rooms, hidden object locations, and past visited-room identity must be inferred or remembered). Challenges include language understanding, long-term memory (visited rooms), exploration, and commonsense reasoning.",
            "is_partially_observable": true,
            "external_tools_used": "A simple semantic parser that converts raw natural language observations into symbolic logical propositions; pre-defined external knowledge encoded as logical rules inside the LNN (i.e., a symbolic knowledge base represented within the LNN).",
            "tool_output_types": "Symbolic logical propositions / boolean-like truth values (proposition activations), LNN-internal scores including contradiction loss and proposition truth probabilities, and action recommendation activations (which actions are logically supported).",
            "belief_state_mechanism": "Belief is represented symbolically as a set of logical propositions derived from the semantic parser and held as input proposition nodes in the LNN; the LNN maintains proposition truth-values and uses logical-function nodes (rules and constraints) to combine these propositions into action activations and contradiction losses. This provides an interpretable, rule-driven belief representation rather than a distributed vector memory.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "On each time step the environment's textual observation is parsed by the semantic parser into propositions which become the current logical-state inputs to the LNN; the LNN recomputes proposition truth-values, logical inferences, action activations, and contradiction losses. After an action executes and a reward/next observation arrives, the new observation is parsed and fed into the LNN to update the logical belief and consequent action recommendations. The LNN can also encode constraints (e.g., visited-room avoidance) that produce contradiction loss signals used to suppress certain action activations.",
            "planning_approach": "Neuro-symbolic decision making via Logical Neural Networks: rule-based logical inference combined with RL training; not presented as explicit search-based planning but as a learned policy over LNN action activations informed by symbolic rules and constraints (i.e., learned policy with symbolic logical reasoning).",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation is performed by selecting text actions (e.g., 'go north', 'go east') recommended by LNN rule activations; path or room-revisit avoidance is handled via logical constraints within LNN (contradiction loss) rather than explicit path-finding algorithms like A*. No explicit shortest-path planner is described.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "LOA demonstrates that feeding semantic-parser-produced symbolic facts into an LNN that encodes pre-defined knowledge and learned rules yields interpretable action recommendations in partially observable text games. The LNN's contradiction loss and logical constraints can be used to avoid revisiting rooms and to prune undesirable navigation actions; the approach provides human-interpretable reasons for action choices and (according to the authors) can accelerate RL convergence compared to non-symbolic baselines, though no quantitative results are reported in this demo paper.",
            "uuid": "e865.0",
            "source_info": {
                "paper_title": "LOA: Logical Optimal Actions for Text-based Interaction Games",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Logical neural networks",
            "rating": 2
        },
        {
            "paper_title": "Reinforcement learning with external knowledge by using logical neural networks",
            "rating": 2
        },
        {
            "paper_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines",
            "rating": 2
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 1
        }
    ],
    "cost": 0.006717,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LOA: Logical Optimal Actions for Text-based Interaction Games</h1>
<p>Daiki Kimura<em> Subhajit Chaudhury</em> Masaki Ono Michiaki Tatsubori<br>Don Joven Agravante Asim Munawar Akifumi Wachi Ryosuke Kohita Alexander Gray<br>IBM Research AI<br>{daiki, subhajit, moono, mich}@jp.ibm.com,<br>{don.joven.r.agravante, asim, akifumi.wachi, kohi, alexander.gray}@ibm.com</p>
<h4>Abstract</h4>
<p>We present Logical Optimal Actions (LOA), an action decision architecture of reinforcement learning applications with a neurosymbolic framework which is a combination of neural network and symbolic knowledge acquisition approach for natural language interaction games. The demonstration for LOA experiments consists of a web-based interactive platform for text-based games and visualization for acquired knowledge for improving interpretability for trained rules. This demonstration also provides a comparison module with other neuro-symbolic approaches as well as non-symbolic state-of-the-art agent models on the same text-based games. Our LOA also provides open-sourced implementation in Python for the reinforcement learning environment to facilitate an experiment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa</p>
<h2>1 Introduction</h2>
<p>Neuro-symbolic (NS) hybrid approaches have been proposed for overcoming the weakness of deep reinforcement learning (Dong et al., 2019; Jiang and Luo, 2019; Kimura, 2018; Kimura et al., 2018), including less training data with generalization, external knowledge utilization, and direct explainability of what is learned. Study of reinforcement learning (RL) in non-symbolic environments, such as those with natural language and visionary observations, would be an important step towards the real-world application of the approaches beyond classic and symbolic environments.</p>
<p>Under certain controls necessary for studying RL, text-based games provide complex, interactive, and a variety of simulated environments where the environmental game state observation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An architecture overview for LOA.
is obtained through the text description, and the agent is expected to make progress by entering text commands. In addition to language understanding (Ammanabrolu and Riedl, 2019; Adhikari et al., 2020), successful play requires skills such as long-term memory (Narasimhan et al., 2015), exploration (Yuan et al., 2018), observation pruning (Chaudhury et al., 2020), and common sense reasoning (Keerthiram Murugesan and Campbell, 2021). However, these studies are not using the neuro-symbolic approach which is a combination of the neural network and the symbolic framework.</p>
<p>A recent neuro-symbolic framework called the Logical Neural Networks (LNN) (Riegel et al., 2020) simultaneously provides key properties of both neural networks (learning) and symbolic logic (reasoning). The LNN can train the constraints and rules with logical functions in the neural networks, and since every neuron in the network has a component for a formula of weighted realvalued logics, it can calculate the probability and contradiction loss for each of the propositions. At the same time, trained LNN follow symbolic rules, which means they yield a highly interpretable disentangled representation. Using this benefit of LNN, we proposed a neuro-symbolic RL method that uses pre-defined external knowledge in logical networks, and the method successfully plays on the text-based games (Kimura et al., 2021).</p>
<p>In this demonstration (demo site: https://ibm.biz/acl21-loa), we present a Logical Optimal Actions (LOA) architecture for neuro-symbolic RL applications with LNN (Riegel</p>
<p>et al., 2020) for text-based interaction games. While natural language-based interactive agents are the ambitious but attractive target as real-world applications of neuro-symbolic, it is not easy to provide an environment for the agent. The proposed demonstration uses text-based games learning environment, called TextWorld (Côté et al., 2018), as a miniature of a natural languagebased interactive environment. The demonstration provides a web-based user interface for visualizing the game interaction, which is including displaying the natural text observation from the environment, typing the action sentence, and showing the reward value from the taken action. The LOA in this demonstration also visualizes trained and pre-defined logical rules in LNN via the same interface, and this will help the human user understand the benefits of introducing the logical rules via neuro-symbolic frameworks. We also supply an open-sourced implementation for demo environment and some RL methods. This implementation contains our logical approaches and other state-of-the-art agents.</p>
<h2>2 Logical Optimal Action</h2>
<p>Our proposing LOA is an RL framework which is combining logical reasoning and neural network training. These training and reasoning are provided from functionalities of LNN (Riegel et al., 2020) that is simultaneously providing key properties of both neural networks and symbolic logic. Figure 1 shows the overview architecture for LOA. The LOA model receives the logical state value as logical fact from the language understanding component which receives raw natural language state value from the environment. The model forwards into LNN for the input to get the optimal action for it, the action goes into the environment to execute the action command, then reward is input to LOA agent. The LOA will be trained the action decision network in LNN by using the acquired reward value and chosen action from the network.</p>
<h2>3 LOA Demo</h2>
<p>The proposing web-based LOA demonstration supports two functionalities: 1) play the text-based game by human interactions, 2) visualize the trained and pre-defined LNN to increase interpretability for acquired rules.</p>
<p>For playing the games by web interface, Fig. 2 shows an initial view for the LOA demonstration.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Initial view for LOA demo.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: View for playing the game.</p>
<p>On the left-hand side, we can choose the game from some existing text-based interaction games ${ }^{1}$, such as TextWorld Coin-Collector game (Côté et al., 2018), TextWorld Cooking game (Côté et al., 2018), TextWorld Commonsense Cleanup game (Keerthiram Murugesan and Campbell, 2021), and Jericho game (Hausknecht et al., 2019). Figure 3 shows the view for playing the TextWorld game, and Fig. 4 shows the view for another game (cleanup task). The human player can input any action by natural language then the demonstration system displays the raw observation output from the environment.</p>
<p>For visualizing the trained and pre-defined neurosymbolic network in LNN, Fig. 5 and Fig. 6 show the example of the LNN output. In these</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: View for playing the cleanup game.
figures, the LNN contains simple rules for the TextWorld Coin-Collector game; for example, the rule is the agent takes 'go east' action, when the agent finds the east room ("found west" $\rightarrow$ "go west"). The round box explains the proposition from the given observation inputs, the circle with a logical function means a logical function node of LNN, and the rectangle box explains an action candidate for the agent. The highlighted nodes (red node) have 'true' value, and nonhighlighted nodes (white node) have 'false' value. In Fig. 5, the agent found the north exit from the given observation ("Observation ( $t=1$ )") by using semantic parser ${ }^{2}$, then the going north room action ("go north") are activated. In Fig. 6, if the user clicks the selectable box, the LOA recommends only one action which is 'go north'. In this demonstration, we show the benefit of introducing the LNN into an RL agent, we don't prepare to automatically choose the action by LOA framework. However, if we execute the RL with LOA framework, the RL agent can converge faster than other non-symbolic and neuro-symbolic methods.</p>
<p>After selecting "go north" action at $t=1$, next observation sentence and LNN output for next step are shown in Fig 7. In this step, the agent found two doors, which are east and south; however, the south door is connected to the previous room because the agent took going north action at the previous step. Since this LNN is simple LNN, the "go south" action is also recommended in</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Displaying the simple LNN with given state.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: User can choose the recommended action.</p>
<p>Fig 7. Figure 8 shows the output of the complicated LNN which has functionality for avoiding revisiting the visited room. By using such the LNN, LOA can output only "go east" action by having contradiction loss in LNN. This is a benefit of introducing the neuro-symbolic framework, and the human user can easily understand the reason for the taken action by the agent with this interpretability by LOA.</p>
<h2>4 Conclusion</h2>
<p>We propose a novel demonstration (URL: https://ibm.biz/acl21-loa) which provides to play the text-based games on the web interface and visualize the benefit of the neuro-symbolic algorithm. This application helps the human user understand</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Result for simple LNN.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Result for avoiding revisiting LNN.
the trained network and the reason for taken action by the agent. We also extend more complicated LNN for other difficult games on the demo site. At the same time, we open the source code for the demonstration (URL: https://github.com/ibm/loa).</p>
<h2>References</h2>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton. 2020. Learning dynamic belief graphs to generalize on text-based games. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 3557-3565.</p>
<p>Subhajit Chaudhury, Daiki Kimura, Kartik Talamadupula, Michiaki Tatsubori, Asim Munawar, and Ryuki Tachibana. 2020. Bootstrapped q-learning with context relevant observation pruning to generalize in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 3002-3008.</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for text-based games. In Computer Games - 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers, pages 41-75.</p>
<p>Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural logic machines. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, Côté Marc-Alexandre, and Yuan Xingdi. 2019. Interactive fiction games: A colossal adventure. CoRR, abs/1909.05398.</p>
<p>Zhengyao Jiang and Shan Luo. 2019. Neural logic reinforcement learning. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 3110-3119.</p>
<p>Pavan Kapanipathi Pushkar Shukla Sadhana Kumaravel Gerald Tesauro Kartik Talamadupula Mrinmaya Sachan Keerthiram Murugesan, Mattia Atzeni and Murray Campbell. 2021. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. In Thirty Fifth AAAI Conference on Artificial Intelligence.</p>
<p>Daiki Kimura. 2018. Daqn: Deep auto-encoder and q-network. arXiv preprint arXiv:1806.00630.</p>
<p>Daiki Kimura, Subhajit Chaudhury, Ryuki Tachibana, and Sakyasingha Dasgupta. 2018. Internal model from observations for reward shaping.</p>
<p>Daiki Kimura, Subhajit Chaudhury, Akifumi Wachi, Ryosuke Kohita, Asim Munawar, Michiaki Tatsubori, and Alexander Gray. 2021. Reinforcement learning with external knowledge by using logical neural networks. arXiv preprint arXiv:2103.02363.</p>
<p>Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. 2015. Language understanding for textbased games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages $1-11$.</p>
<p>Ryan Riegel, Alexander G. Gray, Francois P. S. Luus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus Akhalwaya, Haifeng Qian, Ronald Fagin, Francisco Barahona, Udit Sharma, Shajith Ikbal, Hima Karanam, Sumit Neelam, Ankita Likhyani, and Santosh K. Srivastava. 2020. Logical neural networks. CoRR, abs/2006.13155.</p>
<p>Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew J. Hausknecht, and Adam Trischler. 2018. Counting to explore and generalize in text-based games. CoRR, abs/1806.11525.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ This parser is out of our current research topic, we prepare a simple semantic parser.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
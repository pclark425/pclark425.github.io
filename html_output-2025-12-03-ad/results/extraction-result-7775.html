<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7775 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7775</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7775</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-281103490</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.16561v3.pdf" target="_blank">FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article</a></p>
                <p><strong>Paper Abstract:</strong> The Future Work section of a scientific article outlines potential research directions by identifying gaps and limitations of a current study. This section serves as a valuable resource for early-career researchers seeking unexplored areas and experienced researchers looking for new projects or collaborations. In this study, we generate future work suggestions from a scientific article. To enrich the generation process with broader insights and reduce the chance of missing important research directions, we use context from related papers using RAG. We experimented with various Large Language Models (LLMs) integrated into Retrieval-Augmented Generation (RAG). We incorporate an LLM feedback mechanism to enhance the quality of the generated content and introduce an LLM-as-a-judge framework for robust evaluation, assessing key aspects such as novelty, hallucination, and feasibility. Our results demonstrate that the RAG-based approach using GPT-4o mini, combined with an LLM feedback mechanism, outperforms other methods based on both qualitative and quantitative evaluations. Moreover, we conduct a human evaluation to assess the LLM as an extractor, generator, and feedback provider.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7775.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7775.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLP-overlap metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLP overlap and similarity metrics (ROUGE-L, BLEU, BERTScore, Jaccard Similarity, Cosine Similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard automated text-evaluation metrics used to measure lexical and semantic overlap between generated future-work text and reference ground truth. The paper uses ROUGE-L, BLEU, BERTScore (f1), Jaccard and Cosine similarity to quantify surface and embedding-level agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o mini (primary generator/evaluator in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>mini</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (NLP/ML)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>future-work suggestions / hypothesis proposals</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROUGE-L, BLEU, BERTScore, Jaccard, Cosine</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute token/sequence overlap (ROUGE-L, BLEU), contextual embedding similarity (BERTScore), set overlap (Jaccard), and vector cosine similarity between model-generated text and ground-truth future-work text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROUGE-L, BLEU, BERTScore (F1), Jaccard similarity, Cosine similarity</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>ROUGE-L/BLEU: overlap-based scores typically reported on a 0–100 scale; BERTScore: F1 semantic similarity (0–100 or 0–1 normalized); Jaccard: intersection-over-union of token sets (0–1); Cosine similarity: cosine between dense embeddings (−1 to 1, typically reported 0–1).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACL papers (2023–2024, ~4,562 papers) and NeurIPS papers (2021–2022, 1,000 papers) with author-extracted and OpenReview-derived ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Example: ROUGE-L values reported in experiments ranged ~14–21 depending on model/config; combining OpenReview with author FW increased ROUGE-L by up to ~+6 points in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM-generated outputs were compared against author-written future work and OpenReview reviewer goals; combining both ground-truth sources improved overlap/similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Overlap metrics fail to capture novelty and can penalize valid but novel generations; surface overlap biased toward ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7775.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7775.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based evaluation framework (LLM-as-a-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use a separate LLM to provide human-like evaluations (scores and justifications) across qualitative dimensions (coherence, relevance, readability, grammar, overall impression, novelty) and to supply feedback for iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o mini (primary evaluator); LLaMA 3 70B used as secondary evaluator in robustness checks</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o mini (mini); LLaMA 3 70B (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (NLP/ML)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>future-work suggestion evaluation / critique</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based multi-criteria scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt an evaluator LLM to rate generated text vs ground truth on discrete scales for coherence, relevance, readability, grammar, overall impression (1–5) and to provide justifications; novelty measured separately on a 0–10 scale by an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Coherence (1–5), Relevance (1–5), Readability (1–5), Grammar (1–5), Overall Impression (1–5), Novelty (0–10)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Discrete scoring: 1 (worst) to 5 (best) for most criteria; Novelty: 0 (no new ideas) to 10 (entirely new directions).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACL and NeurIPS datasets with author and OpenReview ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM-based evaluations showed improvements with one round of feedback (e.g., novelty rose from 7.28 → 8.00 with self-feedback; fluency/readability metrics improved by ~+1 in some RAG settings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM-judged scores were compared across settings (LLM-only vs LLM+RAG, with vs without feedback); authors report alignment between GPT-4o mini judgments and human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Subjectivity and evaluator bias (self-validation) possible; different evaluator models (LLaMA) gave inconsistent/overinflated scores; absolute scores should be interpreted comparatively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7775.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7775.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Novelty (LLM-judged)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-judged novelty scoring (0–10 scale)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operationalization of novelty where an evaluator LLM assigns a 0–10 score indicating the extent to which generated future-work ideas are absent from the ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o mini (evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>mini</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (NLP/ML)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>novel idea detection for future-work suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based novelty scoring (0–10)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt an LLM to compare generated text to ground truth and rate novelty 0–10 with a justification: 0 = complete overlap, 10 = entirely new directions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Novelty score (0–10)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer scale 0–10; values ~7 indicate some originality while still overlapping with ground truth; ≥8 indicates strong divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACL and NeurIPS datasets (FW, OR, and FW+OR ground truths)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Novelty increased with self-feedback (e.g., 7.28 → 8.00); adding RAG produced small novelty gains in some ground-truth settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Novelty scoring is inherently subjective when judged by an LLM; authors note need for human evaluation to corroborate LLM-judged novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7775.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7775.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLI-based hallucination detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural Language Inference (NLI) approach to detect hallucination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reframes hallucination detection as an NLI task: premise = source paper + ground-truth future work, hypothesis = LLM-generated future work; judge LLM labels entailment/neutral/contradiction and flags neutral/contradiction as hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o mini (primary NLI judge); LLaMA 3 8B used for comparison</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o mini (mini); LLaMA 3 8B (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (NLP/ML)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>factual consistency / hallucination assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>NLI entailment classification (entailment/neutral/contradiction)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use an LLM to perform NLI between the paper+ground-truth and the generated suggestion; mark neutral or contradiction as hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hallucination rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion of generated items judged as neutral or contradiction (interpreted as hallucinated content).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACL and NeurIPS generated future-work outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4o mini judged ~26.26% of LLM-only outputs as hallucinated; RAG reduced this to ~19.52%. LLaMA 3 8B judged only ~3.60% (deemed unreliable).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>NLI-judge performance varies by model; some judges produce overly permissive entailment labels (risk under-reporting hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7775.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7775.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feasibility Check (binary)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binary feasibility assessment using an LLM judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A binary (feasible / not feasible) evaluation where an LLM judge assesses whether a generated future-work suggestion is executable given the paper's methods, data, and scope.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o mini (feasibility judge)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>mini</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (NLP/ML)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>practical feasibility assessment of proposals</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based feasibility classifier (feasible / not feasible)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt an evaluator LLM to judge whether each generated suggestion can be realistically executed using the methodology and data described in the source paper; returns binary label.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Feasibility classification (feasible / not feasible) and proportion feasible (%)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Binary label per suggestion; aggregate reported as percentage feasible across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACL and NeurIPS generated suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM+RAG-generated future-work suggestions were judged feasible in 98.92% of cases by GPT-4o mini.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Feasibility judgment depends on the evaluator's interpretation and can be optimistic; binary label lacks granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7775.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7775.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Expert Rating</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human annotator evaluation (expert raters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human evaluation by PhD-level NLP researchers assessing extractor fidelity, generator alignment, and the effect of LLM feedback; used to validate LLM-based extraction and generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>human</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (NLP/ML)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human judgement of generated hypotheses/future-work</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert ratings and agreement metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Three annotators evaluate samples (e.g., 500 for extractor fidelity, 100 for generator alignment) using task-specific rating scales (binary or 3-point scales) and compute inter-annotator agreement (Cohen's Kappa, Kendall's Tau).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Task-specific ratings (binary / 3-point scales) and inter-rater agreement metrics (Cohen's Kappa, Kendall's Tau)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Example: generator alignment rated on 3-point scale (mean 2.12/3); feedback improvement rated on 3-point scale (mean 2.75/3); Cohen's Kappa = 0.30; Kendall's Tau = 0.36.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Random samples from ACL/NeurIPS dataset (500 samples for extractor test; 100 samples for generator test)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Extractor: 3 annotators on 500 random samples (graduate students with ML background). Generator: 3 annotators on 100 examples (PhD-level NLP researchers). Feedback evaluation: 3 annotators comparing initial and revised outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Annotator agreement: Cohen's Kappa = 0.30, Kendall's Tau = 0.36; human scores: generator alignment mean 2.12/3, feedback improved mean 2.75/3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Human annotations corroborated that LLM-extracted ground truth is reliable; one round of LLM feedback increased perceived originality/usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Limited sample sizes and annotator pool; moderate inter-rater agreement indicates subjectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7775.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7775.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative LLM feedback (self-refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative LLM feedback / self-refinement loop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A feedback loop where an evaluator LLM critiques generated future-work suggestions and those critiques are incorporated into a regenerated output, repeated up to two iterations to improve quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o mini (both generator and judge in pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>mini</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (NLP/ML)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>iterative improvement of generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-driven iterative critique and regeneration</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>If any evaluation criterion <= 3 (midpoint), the judge's justification is appended to the prompt and the generator re-generates the suggestion; process repeated up to two times.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Improvement in LLM-based scores and NLP-overlap metrics across iterations</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Discrete criterion thresholds (<=3 triggers regeneration); novelty threshold <=7 triggers regeneration; improvements reported as delta in scores.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACL and NeurIPS datasets</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>One round of self-feedback consistently improved both NLP-based and LLM-based metrics (e.g., novelty rose to 8.00). A second round often degraded performance or introduced verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Risk of stylistic convergence and bias when same LLM acts as generator and judge; diminishing returns after one iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7775.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7775.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (Retrieval-Augmented Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) with hybrid retriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that augments LLM generation with retrieved passages from an external corpus to ground outputs and reduce hallucination; implemented with a hybrid FAISS + BM25 retriever and LlamaIndex vector store.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o mini (generator used with RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>mini</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (NLP/ML)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evidence-grounded future-work generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>RAG-grounded generation with retrieval metrics and downstream evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Retrieve top-K relevant chunks from a vector store using a hybrid FAISS (dense) + BM25 (term-based) retriever; concatenate retrieved context with input paper and prompt LLM to generate future work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Impact assessed via NLP-overlap metrics, LLM-based scores, hallucination rate, and feasibility</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Retrieval hyperparameters: hybrid weight 50% BM25/50% FAISS, chunk size up to 512 tokens, K=3 retrieved chunks, context window 3,900 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Local vector database of 100 randomly selected papers (removed from evaluation set); ACL and NeurIPS corpora</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>RAG reduced hallucination (26.26% → 19.52%) and modestly improved some LLM-based metrics (e.g., readability, grammar), though sometimes decreased overlap metrics (ROUGE-L drops reported when RAG introduced noise in first iteration).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Retrieval corpus selection (100 random papers) may include temporally later works; retrieval can introduce distracting or noisy external context if not carefully filtered.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7775.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7775.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval stack & embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval stack: FAISS + BM25 hybrid, embeddings (all-MiniLM-L6-v2, text-embedding-3-small), LlamaIndex</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Components used to implement RAG: FAISS for vector similarity, BM25 for term-based retrieval; 'all-MiniLM-L6-v2' for section similarity; OpenAI 'text-embedding-3-small' for RAG embeddings; LlamaIndex used for in-memory vector store.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>embedding models: all-MiniLM-L6-v2, text-embedding-3-small; retrieval infra used with GPT-4o mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (NLP/ML)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>supporting infrastructure for evidence-grounding</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hybrid retrieval and embedding-based similarity computations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use sentence-transformer embeddings and cosine similarity to rank sections vs future work; use hybrid FAISS (dense) + BM25 (sparse) retriever with equal weighting; LlamaIndex manages in-memory vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Retrieval relevance (implicit via downstream impact on generation metrics), cosine similarity for section selection</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Section selection via cosine similarity between 'all-MiniLM-L6-v2' embeddings and target future-work text; top-3 sections chosen by average cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACL/NeurIPS papers; local 100-paper retrieval corpus for RAG</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Top-3 section selection using cosine similarity gave slightly lower performance than using all sections (small drops in ROUGE/BERTScore/Jaccard/Cosine similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Embedding and retrieval choices affect results; chunking and context window constraints limit retrieved context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7775.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7775.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Datasets (ACL / NeurIPS / OpenReview)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACL 2023–2024 and NeurIPS 2021–2022 papers with OpenReview peer reviews</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Corpora used to extract author-mentioned future work and peer-review long-term goals to build ground truth and to train/evaluate generation models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n/a (data source)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (NLP/ML)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>ground-truth corpus for future-work generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Ground-truth creation via tool extraction + LLM re-extraction (Fg) and OpenReview extraction (OFg)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Extract sections using ScienceParse and regex rules to get candidate future-work (Ft), then refine with LLM extractor (GPT-4o mini) to produce LLM-extracted ground truth (Fg); extract long-term reviewer goals from OpenReview and curate with LLM (OFg).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used as reference for NLP overlap and LLM-based evaluations; counts and sample sizes reported</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Datasets size: ACL ~4,562 papers (ACL 2023 + 2024), NeurIPS 1,000 papers (2021–22) with OpenReview reviews; average ~5 FW sentences per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ACL (2023–2024) and NeurIPS (2021–22) corpora with OpenReview comments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human validation of extractor on 500 random samples with 3 annotators; master agent merging performed with LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Constructed dataset: 4,562 ACL papers and 1,000 NeurIPS papers; LLM-extracted ground truth (Fg) aligned better with generated outputs than tool-extracted Ft on various metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset limited to ACL and NeurIPS domains and years specified; OpenReview available only for NeurIPS subset; extraction tool introduced noisy candidates prior to LLM filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers <em>(Rating: 2)</em></li>
                <li>Self-Refine: Iterative Refinement with Self-Feedback <em>(Rating: 2)</em></li>
                <li>Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback <em>(Rating: 1)</em></li>
                <li>Training Language Models to Follow Instructions with Human Feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7775",
    "paper_id": "paper-281103490",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "NLP-overlap metrics",
            "name_full": "NLP overlap and similarity metrics (ROUGE-L, BLEU, BERTScore, Jaccard Similarity, Cosine Similarity)",
            "brief_description": "Standard automated text-evaluation metrics used to measure lexical and semantic overlap between generated future-work text and reference ground truth. The paper uses ROUGE-L, BLEU, BERTScore (f1), Jaccard and Cosine similarity to quantify surface and embedding-level agreement.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o mini (primary generator/evaluator in experiments)",
            "model_size": "mini",
            "scientific_domain": "Computer Science (NLP/ML)",
            "theory_type": "future-work suggestions / hypothesis proposals",
            "evaluation_method_name": "ROUGE-L, BLEU, BERTScore, Jaccard, Cosine",
            "evaluation_method_description": "Compute token/sequence overlap (ROUGE-L, BLEU), contextual embedding similarity (BERTScore), set overlap (Jaccard), and vector cosine similarity between model-generated text and ground-truth future-work text.",
            "evaluation_metric": "ROUGE-L, BLEU, BERTScore (F1), Jaccard similarity, Cosine similarity",
            "metric_definition": "ROUGE-L/BLEU: overlap-based scores typically reported on a 0–100 scale; BERTScore: F1 semantic similarity (0–100 or 0–1 normalized); Jaccard: intersection-over-union of token sets (0–1); Cosine similarity: cosine between dense embeddings (−1 to 1, typically reported 0–1).",
            "dataset_or_benchmark": "ACL papers (2023–2024, ~4,562 papers) and NeurIPS papers (2021–2022, 1,000 papers) with author-extracted and OpenReview-derived ground truth",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Example: ROUGE-L values reported in experiments ranged ~14–21 depending on model/config; combining OpenReview with author FW increased ROUGE-L by up to ~+6 points in some settings.",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM-generated outputs were compared against author-written future work and OpenReview reviewer goals; combining both ground-truth sources improved overlap/similarity metrics.",
            "limitations_noted": "Overlap metrics fail to capture novelty and can penalize valid but novel generations; surface overlap biased toward ground truth.",
            "uuid": "e7775.0",
            "source_info": {
                "paper_title": "FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "LLM-based evaluation framework (LLM-as-a-judge)",
            "brief_description": "Use a separate LLM to provide human-like evaluations (scores and justifications) across qualitative dimensions (coherence, relevance, readability, grammar, overall impression, novelty) and to supply feedback for iterative refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o mini (primary evaluator); LLaMA 3 70B used as secondary evaluator in robustness checks",
            "model_size": "GPT-4o mini (mini); LLaMA 3 70B (70B)",
            "scientific_domain": "Computer Science (NLP/ML)",
            "theory_type": "future-work suggestion evaluation / critique",
            "evaluation_method_name": "LLM-based multi-criteria scoring",
            "evaluation_method_description": "Prompt an evaluator LLM to rate generated text vs ground truth on discrete scales for coherence, relevance, readability, grammar, overall impression (1–5) and to provide justifications; novelty measured separately on a 0–10 scale by an LLM.",
            "evaluation_metric": "Coherence (1–5), Relevance (1–5), Readability (1–5), Grammar (1–5), Overall Impression (1–5), Novelty (0–10)",
            "metric_definition": "Discrete scoring: 1 (worst) to 5 (best) for most criteria; Novelty: 0 (no new ideas) to 10 (entirely new directions).",
            "dataset_or_benchmark": "ACL and NeurIPS datasets with author and OpenReview ground truth",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "LLM-based evaluations showed improvements with one round of feedback (e.g., novelty rose from 7.28 → 8.00 with self-feedback; fluency/readability metrics improved by ~+1 in some RAG settings).",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM-judged scores were compared across settings (LLM-only vs LLM+RAG, with vs without feedback); authors report alignment between GPT-4o mini judgments and human annotations.",
            "limitations_noted": "Subjectivity and evaluator bias (self-validation) possible; different evaluator models (LLaMA) gave inconsistent/overinflated scores; absolute scores should be interpreted comparatively.",
            "uuid": "e7775.1",
            "source_info": {
                "paper_title": "FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Novelty (LLM-judged)",
            "name_full": "LLM-judged novelty scoring (0–10 scale)",
            "brief_description": "Operationalization of novelty where an evaluator LLM assigns a 0–10 score indicating the extent to which generated future-work ideas are absent from the ground truth.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o mini (evaluator)",
            "model_size": "mini",
            "scientific_domain": "Computer Science (NLP/ML)",
            "theory_type": "novel idea detection for future-work suggestions",
            "evaluation_method_name": "LLM-based novelty scoring (0–10)",
            "evaluation_method_description": "Prompt an LLM to compare generated text to ground truth and rate novelty 0–10 with a justification: 0 = complete overlap, 10 = entirely new directions.",
            "evaluation_metric": "Novelty score (0–10)",
            "metric_definition": "Integer scale 0–10; values ~7 indicate some originality while still overlapping with ground truth; ≥8 indicates strong divergence.",
            "dataset_or_benchmark": "ACL and NeurIPS datasets (FW, OR, and FW+OR ground truths)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Novelty increased with self-feedback (e.g., 7.28 → 8.00); adding RAG produced small novelty gains in some ground-truth settings.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Novelty scoring is inherently subjective when judged by an LLM; authors note need for human evaluation to corroborate LLM-judged novelty.",
            "uuid": "e7775.2",
            "source_info": {
                "paper_title": "FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "NLI-based hallucination detection",
            "name_full": "Natural Language Inference (NLI) approach to detect hallucination",
            "brief_description": "Reframes hallucination detection as an NLI task: premise = source paper + ground-truth future work, hypothesis = LLM-generated future work; judge LLM labels entailment/neutral/contradiction and flags neutral/contradiction as hallucination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o mini (primary NLI judge); LLaMA 3 8B used for comparison",
            "model_size": "GPT-4o mini (mini); LLaMA 3 8B (8B)",
            "scientific_domain": "Computer Science (NLP/ML)",
            "theory_type": "factual consistency / hallucination assessment",
            "evaluation_method_name": "NLI entailment classification (entailment/neutral/contradiction)",
            "evaluation_method_description": "Use an LLM to perform NLI between the paper+ground-truth and the generated suggestion; mark neutral or contradiction as hallucination.",
            "evaluation_metric": "Hallucination rate (%)",
            "metric_definition": "Proportion of generated items judged as neutral or contradiction (interpreted as hallucinated content).",
            "dataset_or_benchmark": "ACL and NeurIPS generated future-work outputs",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT-4o mini judged ~26.26% of LLM-only outputs as hallucinated; RAG reduced this to ~19.52%. LLaMA 3 8B judged only ~3.60% (deemed unreliable).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "NLI-judge performance varies by model; some judges produce overly permissive entailment labels (risk under-reporting hallucination).",
            "uuid": "e7775.3",
            "source_info": {
                "paper_title": "FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Feasibility Check (binary)",
            "name_full": "Binary feasibility assessment using an LLM judge",
            "brief_description": "A binary (feasible / not feasible) evaluation where an LLM judge assesses whether a generated future-work suggestion is executable given the paper's methods, data, and scope.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o mini (feasibility judge)",
            "model_size": "mini",
            "scientific_domain": "Computer Science (NLP/ML)",
            "theory_type": "practical feasibility assessment of proposals",
            "evaluation_method_name": "LLM-based feasibility classifier (feasible / not feasible)",
            "evaluation_method_description": "Prompt an evaluator LLM to judge whether each generated suggestion can be realistically executed using the methodology and data described in the source paper; returns binary label.",
            "evaluation_metric": "Feasibility classification (feasible / not feasible) and proportion feasible (%)",
            "metric_definition": "Binary label per suggestion; aggregate reported as percentage feasible across samples.",
            "dataset_or_benchmark": "ACL and NeurIPS generated suggestions",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "LLM+RAG-generated future-work suggestions were judged feasible in 98.92% of cases by GPT-4o mini.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Feasibility judgment depends on the evaluator's interpretation and can be optimistic; binary label lacks granularity.",
            "uuid": "e7775.4",
            "source_info": {
                "paper_title": "FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Human Expert Rating",
            "name_full": "Human annotator evaluation (expert raters)",
            "brief_description": "Human evaluation by PhD-level NLP researchers assessing extractor fidelity, generator alignment, and the effect of LLM feedback; used to validate LLM-based extraction and generation quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "human",
            "model_size": "n/a",
            "scientific_domain": "Computer Science (NLP/ML)",
            "theory_type": "human judgement of generated hypotheses/future-work",
            "evaluation_method_name": "Human expert ratings and agreement metrics",
            "evaluation_method_description": "Three annotators evaluate samples (e.g., 500 for extractor fidelity, 100 for generator alignment) using task-specific rating scales (binary or 3-point scales) and compute inter-annotator agreement (Cohen's Kappa, Kendall's Tau).",
            "evaluation_metric": "Task-specific ratings (binary / 3-point scales) and inter-rater agreement metrics (Cohen's Kappa, Kendall's Tau)",
            "metric_definition": "Example: generator alignment rated on 3-point scale (mean 2.12/3); feedback improvement rated on 3-point scale (mean 2.75/3); Cohen's Kappa = 0.30; Kendall's Tau = 0.36.",
            "dataset_or_benchmark": "Random samples from ACL/NeurIPS dataset (500 samples for extractor test; 100 samples for generator test)",
            "human_evaluation_details": "Extractor: 3 annotators on 500 random samples (graduate students with ML background). Generator: 3 annotators on 100 examples (PhD-level NLP researchers). Feedback evaluation: 3 annotators comparing initial and revised outputs.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Annotator agreement: Cohen's Kappa = 0.30, Kendall's Tau = 0.36; human scores: generator alignment mean 2.12/3, feedback improved mean 2.75/3.",
            "comparison_to_human_generated": true,
            "comparison_results": "Human annotations corroborated that LLM-extracted ground truth is reliable; one round of LLM feedback increased perceived originality/usefulness.",
            "limitations_noted": "Limited sample sizes and annotator pool; moderate inter-rater agreement indicates subjectivity.",
            "uuid": "e7775.5",
            "source_info": {
                "paper_title": "FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Iterative LLM feedback (self-refinement)",
            "name_full": "Iterative LLM feedback / self-refinement loop",
            "brief_description": "A feedback loop where an evaluator LLM critiques generated future-work suggestions and those critiques are incorporated into a regenerated output, repeated up to two iterations to improve quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o mini (both generator and judge in pipeline)",
            "model_size": "mini",
            "scientific_domain": "Computer Science (NLP/ML)",
            "theory_type": "iterative improvement of generated hypotheses",
            "evaluation_method_name": "LLM-driven iterative critique and regeneration",
            "evaluation_method_description": "If any evaluation criterion &lt;= 3 (midpoint), the judge's justification is appended to the prompt and the generator re-generates the suggestion; process repeated up to two times.",
            "evaluation_metric": "Improvement in LLM-based scores and NLP-overlap metrics across iterations",
            "metric_definition": "Discrete criterion thresholds (&lt;=3 triggers regeneration); novelty threshold &lt;=7 triggers regeneration; improvements reported as delta in scores.",
            "dataset_or_benchmark": "ACL and NeurIPS datasets",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "One round of self-feedback consistently improved both NLP-based and LLM-based metrics (e.g., novelty rose to 8.00). A second round often degraded performance or introduced verbosity.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Risk of stylistic convergence and bias when same LLM acts as generator and judge; diminishing returns after one iteration.",
            "uuid": "e7775.6",
            "source_info": {
                "paper_title": "FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RAG (Retrieval-Augmented Generation)",
            "name_full": "Retrieval-Augmented Generation (RAG) with hybrid retriever",
            "brief_description": "A pipeline that augments LLM generation with retrieved passages from an external corpus to ground outputs and reduce hallucination; implemented with a hybrid FAISS + BM25 retriever and LlamaIndex vector store.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o mini (generator used with RAG)",
            "model_size": "mini",
            "scientific_domain": "Computer Science (NLP/ML)",
            "theory_type": "evidence-grounded future-work generation",
            "evaluation_method_name": "RAG-grounded generation with retrieval metrics and downstream evaluation",
            "evaluation_method_description": "Retrieve top-K relevant chunks from a vector store using a hybrid FAISS (dense) + BM25 (term-based) retriever; concatenate retrieved context with input paper and prompt LLM to generate future work.",
            "evaluation_metric": "Impact assessed via NLP-overlap metrics, LLM-based scores, hallucination rate, and feasibility",
            "metric_definition": "Retrieval hyperparameters: hybrid weight 50% BM25/50% FAISS, chunk size up to 512 tokens, K=3 retrieved chunks, context window 3,900 tokens.",
            "dataset_or_benchmark": "Local vector database of 100 randomly selected papers (removed from evaluation set); ACL and NeurIPS corpora",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "RAG reduced hallucination (26.26% → 19.52%) and modestly improved some LLM-based metrics (e.g., readability, grammar), though sometimes decreased overlap metrics (ROUGE-L drops reported when RAG introduced noise in first iteration).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Retrieval corpus selection (100 random papers) may include temporally later works; retrieval can introduce distracting or noisy external context if not carefully filtered.",
            "uuid": "e7775.7",
            "source_info": {
                "paper_title": "FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Retrieval stack & embeddings",
            "name_full": "Retrieval stack: FAISS + BM25 hybrid, embeddings (all-MiniLM-L6-v2, text-embedding-3-small), LlamaIndex",
            "brief_description": "Components used to implement RAG: FAISS for vector similarity, BM25 for term-based retrieval; 'all-MiniLM-L6-v2' for section similarity; OpenAI 'text-embedding-3-small' for RAG embeddings; LlamaIndex used for in-memory vector store.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "embedding models: all-MiniLM-L6-v2, text-embedding-3-small; retrieval infra used with GPT-4o mini",
            "model_size": "n/a",
            "scientific_domain": "Computer Science (NLP/ML)",
            "theory_type": "supporting infrastructure for evidence-grounding",
            "evaluation_method_name": "Hybrid retrieval and embedding-based similarity computations",
            "evaluation_method_description": "Use sentence-transformer embeddings and cosine similarity to rank sections vs future work; use hybrid FAISS (dense) + BM25 (sparse) retriever with equal weighting; LlamaIndex manages in-memory vectors.",
            "evaluation_metric": "Retrieval relevance (implicit via downstream impact on generation metrics), cosine similarity for section selection",
            "metric_definition": "Section selection via cosine similarity between 'all-MiniLM-L6-v2' embeddings and target future-work text; top-3 sections chosen by average cosine similarity.",
            "dataset_or_benchmark": "ACL/NeurIPS papers; local 100-paper retrieval corpus for RAG",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Top-3 section selection using cosine similarity gave slightly lower performance than using all sections (small drops in ROUGE/BERTScore/Jaccard/Cosine similarity).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Embedding and retrieval choices affect results; chunking and context window constraints limit retrieved context.",
            "uuid": "e7775.8",
            "source_info": {
                "paper_title": "FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Datasets (ACL / NeurIPS / OpenReview)",
            "name_full": "ACL 2023–2024 and NeurIPS 2021–2022 papers with OpenReview peer reviews",
            "brief_description": "Corpora used to extract author-mentioned future work and peer-review long-term goals to build ground truth and to train/evaluate generation models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "n/a (data source)",
            "model_size": "n/a",
            "scientific_domain": "Computer Science (NLP/ML)",
            "theory_type": "ground-truth corpus for future-work generation",
            "evaluation_method_name": "Ground-truth creation via tool extraction + LLM re-extraction (Fg) and OpenReview extraction (OFg)",
            "evaluation_method_description": "Extract sections using ScienceParse and regex rules to get candidate future-work (Ft), then refine with LLM extractor (GPT-4o mini) to produce LLM-extracted ground truth (Fg); extract long-term reviewer goals from OpenReview and curate with LLM (OFg).",
            "evaluation_metric": "Used as reference for NLP overlap and LLM-based evaluations; counts and sample sizes reported",
            "metric_definition": "Datasets size: ACL ~4,562 papers (ACL 2023 + 2024), NeurIPS 1,000 papers (2021–22) with OpenReview reviews; average ~5 FW sentences per paper.",
            "dataset_or_benchmark": "ACL (2023–2024) and NeurIPS (2021–22) corpora with OpenReview comments",
            "human_evaluation_details": "Human validation of extractor on 500 random samples with 3 annotators; master agent merging performed with LLM.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Constructed dataset: 4,562 ACL papers and 1,000 NeurIPS papers; LLM-extracted ground truth (Fg) aligned better with generated outputs than tool-extracted Ft on various metrics.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Dataset limited to ACL and NeurIPS domains and years specified; OpenReview available only for NeurIPS subset; extraction tool introduced noisy candidates prior to LLM filtering.",
            "uuid": "e7775.9",
            "source_info": {
                "paper_title": "FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers",
            "rating": 2,
            "sanitized_title": "can_llms_generate_novel_research_ideas_a_largescale_human_study_with_100_nlp_researchers"
        },
        {
            "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
            "rating": 1,
            "sanitized_title": "check_your_facts_and_try_again_improving_large_language_models_with_external_knowledge_and_automated_feedback"
        },
        {
            "paper_title": "Training Language Models to Follow Instructions with Human Feedback",
            "rating": 1,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        }
    ],
    "cost": 0.020063499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article
4 Sep 2025</p>
<p>Ibrahim Al Azher 
Dept of Computer Science
Northern Illinois University Dekalb
ILUSA</p>
<p>Miftahul Jannat Mokarrama 
Dept of Computer Science
Northern Illinois University Dekalb
ILUSA</p>
<p>Zhishuai Guo zguo@niu.edu 
Dept of Computer Science
Northern Illinois University Dekalb
ILUSA</p>
<p>Sagnik Ray Choudhury sagnik.raychoudhury@unt.edu 
Dept of Computer Science
University of North Texas Denton
TXUSA</p>
<p>Hamed Alhoori alhoori@niu.edu 
Dept of Computer Science
Northern Illinois University Dekalb
ILUSA</p>
<p>FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article
4 Sep 2025DEB5D6B0E34F9C5CB487AA9BFC75AEE1arXiv:2503.16561v3[cs.CL]Future WorkText GenerationLLMRetrieval Augmented GenerationLLM as a Judge
The Future Work section of a scientific article outlines potential research directions by identifying gaps and limitations of a current study.This section serves as a valuable resource for early-career researchers seeking unexplored areas and experienced researchers looking for new projects or collaborations.In this study, we generate future work suggestions from a scientific article.To enrich the generation process with broader insights and reduce the chance of missing important research directions, we use context from related papers using RAG.We experimented with various Large Language Models (LLMs) integrated into Retrieval-Augmented Generation (RAG).We incorporate an LLM feedback mechanism to enhance the quality of the generated content and introduce an LLM-as-ajudge framework for robust evaluation, assessing key aspects such as novelty, hallucination, and feasibility.Our results demonstrate that the RAG-based approach using GPT-4o mini, combined with an LLM feedback mechanism, outperforms other methods based on both qualitative and quantitative evaluations.Moreover, we conduct a human evaluation to assess the LLM as an extractor, generator, and feedback provider.</p>
<p>I. INTRODUCTION</p>
<p>The Future Work section in a scientific article plays a crucial role in amplifying a study's impact by demonstrating foresight and highlighting the broader implications of research [1,2].It serves as a catalyst for further exploration, interdisciplinary collaboration, and new ideas, transforming a single study into a foundation for future advancements [3].Beyond academia, Future Work insights benefit policymakers and funding agencies by identifying emerging research directions and prioritizing areas for strategic resource allocation [4,5].A wellconstructed Future Work section serves both methodological and practical purposes.For example, it encourages researchers to critically reflect on their study's limitations, fostering higher-quality subsequent research while also streamlining the peer review process by clarifying the authors' awareness of challenges and next steps [6].Recognizing the limitations of a study provides clear signals for further exploration, bridging current findings with future advancements [7].Additionally, generating sugesstive Future Work helps researchers align with current priorities, uncover unexplored gaps, and avoid redundancy.Furthermore, understanding long-term research trajectories allows early-career researchers to identify highimpact topics and strategically position their contributions in advancing scientific progress [8].</p>
<p>Author-written Future Work sections are, by nature, often speculative and may be expressed in broad or ambiguous terms [9].While this exploratory character is valuable for signaling open directions, it can make such sections difficult to locate, interpret, and systematically compare across papers.The level of detail also varies across research communities and venues, where factors such as page limits, reviewer expectations, and time constraints shape how future directions are articulated.Moreover, even when such sections are included, they may receive limited attention post-publication, as readers often focus primarily on the main contributions [48].To address these challenges, we generate Future Work automatically from each paper and evaluate it against both the author-mentioned future work and long-term goals extracted from OpenReview peer reviews.By combining these peer-reviewed objectives with author-written statements, our approach aims to construct a more comprehensive and reliable ground truth for evaluating future work generation.</p>
<p>Advances in artificial intelligence (AI) offer transformative potential for addressing this gap.Unlike traditional methods, AI can systematically synthesize research trajectories, uncover latent connections, and propose novel directions that align with emerging trends [10].For example, Si et al. [11] demonstrate that LLM-generated ideas are more novel than those generated by human experts.However, current applications of AI like ChatGPT risk homogenizing outputs and reducing individual creativity [12,13].Standard LLMs may generate overgeneralized, irrelevant, or fabricated Future Work directions.To address these challenges, this work utilizes an LLM to suggest Future Work, enhances its output using LLM-based feedback, and incorporates RAG using cross-domain insights.</p>
<p>Evaluation of AI-generated Future Work sentences is challenging.Traditional Natural Language Processing (NLP) evaluation metrics that rely on n-gram text overlaps or semantic similarity, such as ROUGE [14], BLEU [15], and BERTScore [16] to compare the generated text with references, fail to fully capture the nuances of this generation process.To address this issue, we incorporate LLM-based evaluation alongside NLPbased evaluations, providing human-like assessments with explanations.If the generated Future Work does not meet quality thresholds, we refine it using an LLM-driven feedback loop, improving its alignment with the context.We mitigate issues such as vagueness and redundancy observed in prior AI-driven ideation tools by integrating iterative large language model (LLM) feedback.</p>
<p>The goal of this research is not to replace authors in writing their own Future Work sections, but to develop a framework that can automatically extract, generate, and evaluate Future Work statements from scientific papers.This serves two complementary purposes: (1) enabling authors and researchers to obtain higher-quality and more diverse suggestions for Future Work in their own studies, and (2) allowing the broader community to systematically identify and analyze research directions across large collections of papers.To this end, we propose an LLM-based framework that integrates Retrieval-Augmented Generation (RAG), iterative LLM feedback, and LLM-as-a-judge evaluation.</p>
<p>Moreover, in our study, we'll be addressing these research questions: RQ1: How reliable is an LLM in extracting future work statements compared to traditional tool-based methods?RQ2: How does input context selection (top-3 vs. all sections) impact the quality of LLM-generated future work, and how does RAG influence these results?RQ3: How does expanding ground truth with peerreview goals and incorporating LLM feedback impact the performance and quality of LLM-generated future work?</p>
<p>Our contributions can be summarized as follows: Dataset.We created a dataset of Future Work sentences from nearly 4,000 papers collected from the ACL conference from 2023 and 2024 and 1000 papers from NeurIPS.Papers often do not have exclusive Future Work sections (they are combined with general conclusions or limitations), therefore, we use LLMs to extract relevant sentences.We also validated this extraction process on a random sample of the dataset with human annotators, ensuring that the LLM accurately identifies future-work content.Future Work Generation.To address RQ2, we prompted LLMs to generate future work suggestions using two input configurations: the top-3 most relevant sections and the full content (excluding the ground truth future work).Also, we developed an RAG-based system that augments the input with relevant content from related papers, enriching the generation process and improving the depth and relevance of the suggested future directions.LLM Feedback and Judgment Framework.To address RQ3, we incorporated long-term research goals from OpenReview to enrich the ground truth, applied LLM feedback to iteratively refine the generated future work, and used an LLM as a judge to evaluate quality-moving beyond traditional NLP-based metrics to assess dimensions such as novelty, feasibility, and hallucination.</p>
<p>II. RELATED WORK</p>
<p>Recent advancements in NLP and LLMs have enabled the automatic extraction and generation of various sections of scientific articles, including abstracts [18], methodologies [19], charts and graphs [22], and limitations [20,21,51].Within the domain of Future Work, prior studies have focused on tasks such as extraction [23], classification [24], identifying creative topics [25], thematic categorization [26], and trend prediction [27].For example, a BERT-based model has been used to annotate "future research" sentences, enabling clustering [28] and impact analysis [29].Other efforts have integrated retrieval-augmented methods for idea generation [30] and trend forecasting [49].However, most of these approaches emphasize identifying or categorizing Future Work rather than generating actionable suggestions.Our work addresses this gap by leveraging LLMs to synthesize suggestive Future Work informed by both paper content and peer-review insights.</p>
<p>Beyond Future Work, the application of LLMs in scientific discovery has gained significant attention.In the biomedical domain, LLMs have been used to generate new discoveries [31], while social science studies employed LLM-based agents to automatically propose and test hypotheses [32].Similarly, probabilistic models have been explored for hypothesis generation using reward functions [33].Statistical tests further show that LLM-generated research ideas can exhibit greater novelty than human-generated ones when comparing outputs against topics from recent conferences [34,35].These findings underscore the potential of LLMs for ideation, though their application to automated Future Work generation remains underexplored.</p>
<p>To enhance factual grounding, Retrieval-Augmented Generation (RAG) has emerged as a promising strategy.RAG improves generation by retrieving relevant passages from external corpora and conditioning the model's output on this evidence, thereby reducing hallucination and improving relevance [49].In our framework, we employ RAG to generate Future Work suggestions that align more closely with both the source paper and peer-review feedback.</p>
<p>A further challenge lies in improving the quality of LLMgenerated content.Prior work has shown that human feedback can enhance outputs [36], while self-refinement methods allow LLMs to identify and correct their own mistakes without human intervention [37].For example, iterative fine-tuning approaches [38], in-context prompt criticism [39], and feedbackdriven hypothesis generation [40,41] all highlight the potential of feedback mechanisms.Inspired by these approaches, we incorporate LLM feedback loops to refine generated Future Work and use the LLM itself as a judge, thereby improving fluency, coherence, and originality without additional training.</p>
<p>Finally, the evaluation of LLM-generated text has been widely studied.Research comparing human and LLM judgments [43,44] shows that LLM-based evaluations align well with human annotators.Building on these insights, our framework combines NLP-based metrics (e.g., ROUGE, BLEU, BERTScore) with LLM-based evaluation dimensions such as novelty, feasibility, coherence, and hallucination.</p>
<p>In summary, while prior research has advanced extraction, classification, and trend prediction of Future Work, as well as LLM-driven scientific discovery, the suggestive generation of Future Work remains an open challenge.Our work addresses this by combining LLM generation with RAG for grounding, iterative feedback for refinement, and robust evaluation metrics to assess quality and originality.</p>
<p>III. DATASET COLLECTION</p>
<p>We created our dataset by extracting Future Work sentences from 2,354 papers in ACL 2023 and 2,208 papers in ACL 2024, for a total of 4,562 papers.We also collected 1, 000 papers from NeurIPS, along with their open-access peer reviews from OpenReview1 from 2021-22.</p>
<ol>
<li>Author mentioned future work: We extracted all sections from a paper using the ScienceParse tool2 and extracted the 'main review' from OpenReview using Selenium.Then, we constructed the ground truth for future work by extracting content from both the paper and its OpenReview page.Our ground truth includes two components: (1) authormentioned future work and (2) long-term goals suggested by peer reviewers.The extraction process consisted of two stages-an initial rule-based extraction using a tool, followed by refinement using GPT.</li>
</ol>
<p>A. Tool-Based Extraction: Author-mentioned future work appears in two forms: explicit and implicit.For explicit future work, we identified papers with a clearly labeled section such as "Limitations and Future Work" and extracted the entire section using the Science Parse tool.</p>
<p>For implicit Future Work, where future directions are discussed within other sections (e.g., Discussion, Conclusion), we searched for the presence of keywords such as "future" or "future work" (case-insensitive).We did not extract from the Abstract, Introduction, Related Work, or Methodology sections, as these typically do not contain future work statements.Once a matching sentence was found, we extracted that sentence and all subsequent ones until the start of the next section.To avoid including unrelated content, parsing was stopped if we encountered any sentence containing keywords like "grant", "discussion", or "acknowledgements".Python regular expressions were used to automate this extraction.We denote the extracted future work by the tool as F t .</p>
<p>B. Re-extracted future work by LLM: Tool with Regexbased string matching contains various noisy sentences which is not related to future work, showing a high recall, so we further filtered these sentences using LLMs to improve precision.Since most papers do not have a dedicated section, it is often scattered in any other section.Therefore, filtering out noisy sentences is crucial to ensure accurate extraction.Here we employed an LLM as an extractor role to isolate only Future Work sentences while removing irrelevant sentences.We sent the tool-extracted future work F t to LLM (GPT-4o mini) and prompted it to extract sentences only related to future work, without generating any sentences, and get refined future work F g .This produces Future Work paragraphs from 4562 papers from ACL and 1000 papers fron NeurIPS, averaging five sentences per paper with an average word length of 63.</p>
<ol>
<li>OpenReview: Since no ACL OpenReview were available during our data collection process, we collected OpenReview from the NeurIPS papers only.</li>
</ol>
<p>A. Tool-Based Extraction: At first, we used a parsing tool (Selenium) to extract reviews from OpenReview, denoted as
O F t .
B. Re-extracted by LLM: After parsing text from OpenReview, we gathered all peer feedback O F t , and used an LLM to extract potential future-work suggestions for the authors.But these can include short-term extensions, implementation notes, or minor improvements.To ensure that only meaningful longterm research goals are retained, we applied a second LLM to validate the initially extracted sentences from OpenReview.This step filtered out any content that did not reflect true longterm research directions, resulting in a curated set of goals denoted as O F g .This additional step helps distinguish strategic, forward-looking directions from routine or incremental feedback.</p>
<ol>
<li>Master Agent: We employed a master agent LLM to concatenate the author-mentioned future work (F g ) with the long-term goals extracted from OpenReview (O F g ), without generating any new content.The master agent was designed to identify and merge overlapping or duplicate future work suggestions between F g and O F g .The entire extraction and merging process was carried out using GPT-4o mini.</li>
</ol>
<p>A. Evaluation</p>
<p>We generated future work with LLM + RAG setting denoted as F ′ G where input is Abstract, top 3 sections, and related texts from RAG (Details in Section IV).To compare the quality of the tool-extracted future work (F t ) and the LLMextracted future work (F g ), we used the LLM-generated future work (F ′ G ) as a reference.Specifically, we conducted an NLP-based evaluation by comparing F t and
F g against F ′ G which is Eval NLP (F t , F ′ G ) and Eval NLP (F g , F ′ G )
As shown in Table I, the GPT-extracted ground truth (F g ) outperforms the tool-extracted ground truth (F t ) across nearly all models and metrics, with notable gains in BERTScore, Jaccard Similarity, and Cosine Similarity.The main reason is that F t often includes noisy or irrelevant sentences, while F g filters these out and retains only true Future Work content, resulting in stronger alignment with LLM-generated text.</p>
<p>B. Human Evaluation</p>
<p>Additionally, we conducted a human evaluation to assess the effectiveness of LLMs in extracting future work sentences without introducing hallucinated or fabricated content.A user study was carried out on 500 randomly selected samples, involving three annotators who were asked: "Do you think the LLM extracted future work only, without generating new content or hallucinating?"We provided annotators with both the tool-extracted future work and the LLM-extracted future work for comparison.All annotators agreed that the LLM faithfully extracted existing future work sentences, primarily filtering out noisy content without introducing new information.The annotators were graduate students with a background in machine learning and had no affiliation with this study.</p>
<p>IV. METHODOLOGY</p>
<p>We propose a multi-stage framework for generating, evaluating, and analyzing LLM-based future work suggestions.The workflow includes: (1) constructing a high-quality dataset, (2) generating future work using LLMs with and without retrieval augmentation (RAG), (3) evaluating the generated outputs against ground truth using both NLP-based and LLM-based metrics, (4) assessing hallucination, novelty, and feasibility, and (5) validating results through human evaluation.</p>
<p>Our work focuses on an LLM-based RAG pipeline that generates future-work sections directly from ACL and NeurIPS papers.At first, we extracted the author's mentioned future work from the paper, then we re-extracted it using GPT.We also did a similar process for OpenReview and made a ground truth containing the author's mentioned future work and longterm goals from OpenReview (Details in III).In the ACL dataset, the ground truth consists of author-stated future work statements extracted using an LLM, F g .For the NeurIPS dataset, the ground truth includes both the authors' future work statements and long-term suggestions from OpenReview peer reviewers, (F g + O F g ).In both cases, we constructed input data by collecting all texts from the full paper after removing 'author-mentioned future work'.The future work generated by the LLM with RAG from the input data is denoted as F ′ G . Figure 1 depicts our end-to-end pipeline: starting from a sample paper, we first extract candidate "future work" sentences F t via regex with tool and refine (re-extract) them with an LLM and get F g , then pull peer-review comments from OpenReview and use the same LLM to isolate long-term goals (step 2,3), merging both using master LLM to make a robust ground truth.We remove the author's mentioned future work, feed the paper to an LLM augmented by a vectorstore retriever that supplies related documents, and generate new future work suggestions (step 4,5,6).Each paper is automatically scored on NLP metrics and LLM-based criteria (coherence, relevance, novelty, grammar, overall impression), and any suggestion scoring below a threshold of 3 is critiqued and re-fed into the generator for a polished second iteration (step 7).A more detailed overview of the tasks in this stage is given below.</p>
<p>Section Selection: Processing a full paper has more computational and API costs; to reduce computational and API costs, we experimented with using the abstract and the top three most relevant sections as input to generate future work suggestions.To identify the most relevant sections, we used a cosine similarity-based approach.We calculated the cosine similarity between each section and the paper's Future Work text (Table II) across all papers, and we selected the top three sections with the highest average cosine similarity.These sections formed the basis for generating Future Work content.For all experiments, we removed the author's mentioned future work from the paper and made the input text.</p>
<p>Generating Future Work Using LLM and RAG: Gen-erating future work from a single paper can yield narrow or generic suggestions, often defaulting to boilerplate ideas like "use more data" or "apply to new domains."RAG mitigates this by grounding generation in paper-specific passages and related works, reducing hallucinations and improving relevance.</p>
<p>In our experiments, we generated future work using either the top three sections or all sections (excluding the ground-truth future work), supported by an RAG system built from 100 random research papers (Details V).The Retriever processes the input query, which includes the prompt and the content of the input paper.It then retrieves additional relevant information from the vector database using a hybrid approach, FAISS + BM25, and applies a ranking mechanism to prioritize the extracted text (Figure 1).Here, BM25 is a traditional termbased method that ranks documents based on keyword overlap and term frequency, and FAISS is a vector-based similarity search library that retrieves texts based on dense embeddings.The final augmented input is fed into the LLM Generator, producing Future Work content based on the provided context (Figure 1, step 6).LLM as a Judge: Relying solely on NLP metrics (e.g., ngram overlap, similarity) often miss coherence, accuracy, and fluency.Combining them with LLM-based metrics provides a more holistic evaluation, capturing semantics, logical flow, and human-like judgment while reducing bias toward surface similarity.To assess the quality of generated future work, we used LLM as evaluators.Specifically, we prompted LLMs to rate each suggestion based on six criteria: coherence, relevance, readability, grammar, overall impression, and novelty.Each evaluation was rated on a discrete scale of 1 (worst) to 5 (best), with justifications (Figure 2).Our primary evaluator was GPT-4o-mini.To reduce generator-evaluator bias, we also evaluated using LLaMA 3 70B.</p>
<p>Instructions: You are provided with two texts for each pair: one is generated by a machine (Machine-Generated Text), and the other is the original or ground truth text (Ground Truth).After reviewing each text, assign a score from 1 to 5 based on the criteria outlined below... Scoring Criteria: Coherence and Logic, Relevance and Accuracy.Readability and Style.Grammatical Correctness.Overall Impression.(5: The text is exceptionally coherent; the ideas flow logically and are well connected.3: The text is coherent but may have occasional lapses in logic or flow.1: The text is disjointed or frequently illogical.)Task: For each text pair: Rate the Machine-Generated Text on each criterion and provide a final overall score out of 5. Provide a brief justification for your scores, highlighting strengths and weaknesses observed in the machine-generated text relative to the ground truth.Iterative Refinement: For each evaluation criterion (coherence, relevance, readability, grammar, overall impression, and novelty), we set a threshold of 3 as an acceptable midpoint (7 for novelty).If an LLM-generated Future Work received a score less than or equal to the midpoint in any metric, the justification was incorporated into the prompt, and the Future Work was regenerated accordingly (Figure 3).This iterative refinement process was repeated up to two times to assess whether performance improved (Figure 1, step 9, 10, and 11).</p>
<p>Your task is to generate a refined "future work" section for a scientific article.Input: [Input Paper] Here I am providing the texts and have found these problems.At first, read the feedback and try to improve it when you generate future work.[LLM Feedback] Based on these details, please generate comprehensive and plausible future work suggestions that could extend the research findings, address limitations, and propose new avenues for exploration.Future work should be within 100 words.Incorporating OpenReview: Authors are often ambiguous or hesitant to disclose ambitious or speculative research plans in their own papers [9].Relying solely on author-mentioned Future Work is therefore not ideal, as authors may be reluctant to share their true research directions or may intentionally limit the scope of their suggestions.To address this limitation, we incorporate long-term future work suggestions from OpenReview reviewers and combine them with author-stated future work to construct a more comprehensive ground truth (see Section III).These reviewer insights can uncover overlooked or forward-looking directions that are not explicitly stated by the authors but are critical for advancing the field.We evaluated our approach using three types of ground-truth annotations on NeurIPS data: Author-Mentioned Future Work (FW), OpenReview feedback (OR), and their combination (FW + OR).These labels served as supervision signals to assess the quality of LLM-generated suggestions.We applied this evaluation framework across two model settings: GPT-4omini and GPT-4o-mini with RAG.</p>
<p>Measuring Novelty: Measuring novelty is important to assess whether LLM-generated Future Work introduces ideas beyond the ground truth.While alignment ensures relevance, novelty reflects the model's ability to propose fresh, forwardlooking directions.We define novelty as the extent to which the generated text contains ideas absent from the ground truth.Following prior work on LLM-based evaluation, we operationalize novelty as a 0-10 scale judged by an evaluator LLM (GPT-4o mini), where 0 indicates complete overlap (no new ideas) and 10 indicates entirely new research directions.In this scale, a score of around 7 suggests that the output introduces some original ideas while still overlapping with the ground truth, whereas a score of 8 or higher reflects stronger divergence and originality.To evaluate this, we prompted the evaluator LLM to assign a score with justification for each generated output (Figure 4).Furthermore, if a paper's novelty score was less than or equal to 7, the justification and input paper were sent back to the LLM to regenerate the Future Work, thereby encouraging more diverse and underexplored directions.</p>
<p>Measuring Hallucination: Measuring hallucination is crucial in future work generation to ensure that the suggestions are grounded in the original paper's content and not fabricated or misleading.Hallucinated outputs can misrepresent the study's scope, overstate limitations, or propose directions that are inconsistent with the paper's findings.By evaluating hallucination, we can assess the factual consistency and You are an expert in evaluating research content for novelty and innovation.I have two sets of text provided below: Ground Truth Future Work: LLM-Generated Future Work: Your task is to compare the LLM-generated future work to the Ground Truth future work and assess its novelty relative to the Ground Truth future work.Follow these steps: Evaluate Novelty: Identify unique ideas, approaches, or directions in the LLM-generated future work that are not present in the Ground Truth future work.Analyze how innovative or distinct these additions are in the context of the research field.Quantify Novelty: Provide a novelty score (0-10) for the LLMgenerated future work, where 0 indicates complete overlap with the ground truth future work (no new ideas) and 10 indicates entirely new and distinct ideas.Justify the score with a clear reason explaining the extent of novel contributions or lack thereof.Present your response in a JSON object with the keys score (integer from 0-10) and reason (a concise explanation of the score).trustworthiness of the generated future work.So we measured hallucination by reframing it as a natural language inference (NLI) task.For each generation, we treated the original paper text concatenated with the ground-truth future work as the premise and the LLM-generated future work as the hypothesis (Figure 5).A judge LLM was employed to ask to classify whether each pair as entailment, neutral, or contradiction, and if the hypothesis is neutral or contradicts the premise, it was marked as hallucination.</p>
<p>You are a natural language inference (NLI) classifier.Given a premise and hypothesis, respond with exactly one word: "entailment", "neutral", or "contradiction".Premise: Input paper Hypothesis: LLM generated future work Feasibility Check: LLM-generated future work may appear insightful, but it may often lack the grounding needed for actual implementation.If such suggestions are not feasible-given the paper's methods, data, or research scope-they risk promoting impractical or unrealistic research directions.To address this concern, we conducted feasibility checks on each future work suggestion generated by both the LLM and LLM+RAG settings.Using a separate LLM (GPT-4omini) as a judge, we conducted a feasibility assessment by prompting whether each LLM and RAG-generated future work suggestion was executable given the paper's methods, data, or research context (Figure 6).We provided an input paper, LLM generated future work to judge LLM, and the judge LLM was instructed to return a binary judgment: feasible or not feasible.This assessment is crucial to ensure that LLM and RAG generated future work is not only meaningful but also applicable and actionable, supporting more grounded and effective research planning.</p>
<p>V. EXPERIMENTAL SETUP Our work is future work generation, where we benchmarked a diverse mix of generative and retrieval-augmented methods to understand their relative strengths under realistic constraints.BART and T5 serve as strong, well-studied seq2seq  baselines with fixed token-limit trade-offs, while GPT-3.5 and GPT-4o illustrate how off-the-shelf LLMs perform zeroshot under a controlled prompt budget.Fine-tuning LLaMA-3.1 with LoRA/QLoRA and FlashAttention demonstrates that even large open-source models can be adapted efficiently on modest hardware.Finally, integrating RAG grounds generation in concrete evidence, and comparing one-shot versus zero-shot LLM evaluators (GPT vs. Llama) lets us quantify both generation quality and evaluator bias.This multi-axis evaluation ensures our conclusions generalize beyond any single model or configuration.We conducted experiments with LLaMA and its fine-tuning on an A100 GPU with 40 GB VRAM.For BART and T5 models, we utilized Google Colab with a T4 GPU (15 GB VRAM).GPT-4o mini was accessed via the OpenAI API.</p>
<p>Generating Similarity between Other Sections and Future Works: We used Sentence Transformers ('all-MiniLM-L6-v2') for embedding generation and scikit-learn's cosine similarity function for similarity computation.</p>
<p>Fine-Tuning Models: First, we fine-tuned BART (1,024token limit) and T5 (512-token limit) on a 70/30 train/test split, discarding any over-length inputs.For LLaMA 3.1 7B fine-tuning, it was trained using 'Abstract,' 'Introduction,' and 'Conclusion' as input, and the extracted ' Future Work' was used as output.Leveraging LLaMA's extensive pretraining, we   We ran GPT-3.5 and GPT-4o in zero-shot mode, leveraging their 16 K and 128 K context windows, respectively.For LLM as a judge and LLM as an extractor, we used a zero-shot approach with GPT-4o mini.</p>
<p>LLM with RAG Integration: We integrated RAG with GPT-4o mini for Future work generation, leveraging OpenAI's 'textembedding-3-small' model for relevant document retrieval.</p>
<p>Our vector database comprises 100 randomly selected papers from the dataset, and these papers were removed from the dataset.For semantic search using vector embeddings, we employed LlamaIndex 3 , utilizing its in-memory vector store rather than an external database.We used a hybrid retriever system to fetch the data from the vector database consisting of BM25 and FAISS with an equal weight of 50%.We segmented the data into chunks of up to 512 tokens to accommodate smaller context windows.The overall context window was set to 3,900 tokens-the maximum number of tokens the LLM can process at a time.Additionally, we set K = 3, meaning that the top three most relevant chunks are retrieved from the vector store to provide contextual support during the generation process.</p>
<p>VI. EXPERIMENTS AND RESULTS</p>
<p>A. Evaluation of Future Work Generation</p>
<p>We experimented with various LLMs (Table V), incorporating RAG to generate Future Work sections.Table IV presents qualitative examples of Future Work statements from the original paper (ground truth), LLM-generated output, and RAG-generated output.The RAG-generated examples tend to be longer and contain a greater number of ideas compared to both the ground truth and the LLM-only output Traditional NLP-based metrics primarily focused on lexical overlap and semantic quality, and LLM-based evaluation provides more contextual assessment and novelty.NLP-based metrics couldn't measure novelty and relied more on ground truth, missing the depth evaluation.Solely relying on an 3 https://www.llamaindex.ai/LLM-based evaluation system raises potential bias issues.To alleviate these problems, we incorporated both LLM-based and NLP-based evaluation systems to make a robust evaluation system for evaluation by focusing on coherence and logic, relevance and accuracy, readability and style, grammatical correctness, overall impression, and novelty.</p>
<p>Evaluation-among all models: We evaluated multiple models and found that GPT-4o mini with RAG performed best overall when LLM feedback was incorporated once (iteration 2), as shown in Table V.This configuration achieved the highest scores across both traditional NLP metrics and LLMbased evaluations.Interestingly, applying RAG with GPT-4o mini in the first iteration slightly reduced performance compared to using GPT-4o mini alone.For example, ROUGE-L dropped by 3.5 points and cosine similarity by 1.2 points, while coherence, relevance, readability, and grammar also saw small decreases.This suggests that introducing external knowledge too early may sometimes add noise or distract the model from the original paper content.For the NeurIPS dataset (Table VI), RAG did not improve NLP-based metrics when evaluated against author-mentioned future work (FW), OpenReview reviews (OR), or their combination (FW+OR).However, it did yield modest gains on LLM-based evaluations.For example, when FW was used as ground truth, readability improved by +1.05 and grammar by +0.19; and when FW+OR was used, readability improved by +1.04 and grammar by +0.33.These results highlight that while RAG may not always boost traditional overlap metrics, it can enhance the fluency and readability of generated text.</p>
<p>Evaluation-incorporating LLM feedback: After generating the Future Work text, we used a separate LLM as a judge, providing scores and short justifications.This feedback was then fed back into the generation process to refine the output.Our experiments show that adding feedback once significantly improved performance across multiple evaluation metrics (Table III).For instance, a single feedback round helped produce text that was more accurate, coherent, and aligned with the source paper.Interestingly, we observed that one round of feedback was optimal.While the first feedback loop (iteration 2) boosted performance for GPT-4o mini with RAG (Table V), a second round (iteration 3) actually reduced performance, likely because repeated feedback introduced bias.On the NeurIPS dataset (Table VI), the same pattern emerged: a second feedback round provided small improvements in readability and grammar but slightly lowered overlap-based metrics.Since GPT-4o mini + RAG already achieved strong scores after one iteration, additional feedback was unnecessary.Overall, these findings highlight that a single round of feedback integration balances improvement with stability, making it the most effective strategy.</p>
<p>Novelty Evaluation: We evaluated the novelty of LLM-and RAG-generated Future Work using three ground-truth settings: author-mentioned future work (FW), OpenReview reviews (OR), and their combination (FW + OR) (    and the chosen ground truth.We therefore interpret novelty comparatively across settings rather than as an absolute value.Across all ground truths, the relative trends were consistent: GPT-4o mini produced reasonably novel suggestions, and adding RAG slightly increased novelty when FW was used as the reference (+0.42), though it led to a small decrease when FW + OR was used (-0.17).The most notable improvement came from introducing self-feedback, which raised the novelty score from 7.28 to 8.00, the highest across all settings.This indicates that a single round of feedback helps the model generate more diverse and underexplored research directions, while the comparative analysis across FW, OR, and FW+OR provides a more robust view of originality.LLM as a Judge: All of our evaluation experiments-including future work extraction and the assessment of coherence, relevance, readability, grammar, novelty, hallucination, and feasibility-were conducted using GPT-4o mini.While LLM-based evaluation is inherently more subjective than standard metrics such as BLEU or ROUGE, prior work has shown that LLMs can approximate human judgments with reasonable reliability.In our case, GPT-4o mini produced evaluations that aligned well with human annotations (see Section VI, B).To check robustness, we also experimented with LLaMA 3 70B, which produced less reliable justifications and tended to inflate scores even for low-quality outputs.These differences highlight that absolute values should be interpreted cautiously, and our results are best understood in comparative terms across settings (e.g., LLM-only vs. RAG, with vs. without feedback) rather than as fixed absolute scores.</p>
<p>Hallucination Rate: To assess whether LLM-and RAGgenerated Future Work contained hallucinations, we employed a natural language inference (NLI)-based approach that treated the input paper as the reference.Hallucination rates were then evaluated using two judge models: GPT-4o mini and LLaMA 3 8B (Table VIII).When GPT-4o mini served as the judge, about one in four generated Future Work statements (26.26%) contained information not supported by the source paper or its ground-truth future work, but this rate dropped to 19.52% when RAG was incorporated, indicating that retrieval helps ground the generated text more closely in the source content.In contrast, LLaMA 3 8B judged the same outputs much more conservatively, reporting only 3.60% hallucinations.However, this very low rate appears to reflect the limitations of LLaMA 3 8B as a judge rather than stronger grounding: the model has a shorter context window (8K) and fewer parameters (8B), and in practice, it almost always classified the LLM-generated text as entailment.For this reason, we do not rely on LLaMA 3 8B as a reliable judge.Overall, hallucination rates should be interpreted comparatively rather than absolutely: the key finding is that RAG consistently reduces hallucination relative to LLM-only generation.</p>
<p>Feasibility Check: We measure how feasible LLM + RAGgenerated future work is in terms of data and methodology.We employed a new GPT-4o mini as evaluator, and the evaluation revealed that 98.92% of the LLM + RAG-generated future work was classified as feasible, indicating strong contextual alignment between the proposed future work and the methodology and data described in the source papers.</p>
<p>B. Human Evaluation</p>
<p>Using the same LLM (GPT-4o mini) as the extractor, generator, and evaluator may introduce potential biases, such as self-validation and confirmation bias.To mitigate this concern, we conducted a threefold human evaluation who are PhD-level NLP researchers (not affiliated with this study).1. Evaluating the Extractor (LLM as Extractor): We asked three independent annotators to assess whether GPT-4o mini accurately extracted future work statements without generating new content or hallucinating.All annotators confirmed that the LLM faithfully extracted relevant content without introducing noise or fabricated text (see Section III, B). 2. Evaluating the Generator (LLM as Generator): To assess the quality of the LLM-generated future work, we randomly sampled 100 examples and asked three annotators to rate the generated outputs.The evaluation focused on the question: Q1: How well does the LLM-generated future work align with the ground truth?We provided annotators with the LLM-extracted ground truth alongside the LLM-generated future work for evaluation.As shown in Table IX, the responses yielded a moderate average score of 2.12 out of 3, with a Cohen's Kappa of 0.30 and Kendall's Tau of 0.36, indicating consistent agreement among annotators.3. Evaluating Feedback (LLM as Feedback Provider): To evaluate the effect of LLM-based self-feedback, we asked annotators the following: Q2: Does the LLM feedback iteration improve the originality and quality of the generated future work?We provided annotators with the initial LLM-generated future work alongside its revised version after incorporating one round of LLM feedback.The one-round feedback approach achieved a high average score of 2.75 out of 3, suggesting that the feedback loop significantly enhanced the originality and overall usefulness of the generated content.</p>
<p>C. Ablation Study</p>
<p>Evaluation-considering all sections vs top-3 sections.We used cosine similarity to identify the three most relevant sections in the ACL dataset.Future work was then generated using two input settings: (1) the top three most relevant sections, and (2) all sections excluding the author-mentioned future work.We evaluated performance using NLP-based metrics by comparing the LLM-generated future work against the ground truth (author-mentioned future work combined with OpenReview suggestions).As shown in Table VII, using only the top three sections led to a slight performance drop across all metrics, including ROUGE-L (-0.78),BERTScore (-0.12),Jaccard Similarity (-0.84), and Cosine Similarity (-0.85).</p>
<p>Evaluation incorporating OpenReview We conducted an ablation study using three types of ground truth: FW (authormentioned future work), OR (long-term goals from OpenReview), and FW + OR (a combination of both).As shown in Table VI, the combined FW + OR setting consistently produced the best results.For example, when GPT-4o mini was used as the generator, FW + OR outperformed FW alone with clear gains in overlap and similarity metrics (e.g., ROUGE-L improved by +2.78 and cosine similarity by +6.10).Compared to OR alone, the combined setting performed even better, with ROUGE-L increasing by nearly +6 points and cosine similarity by almost +12 points.These improvements suggest that combining author-written and peer-reviewed goals gives the model a broader and more balanced reference, leading to richer and more diverse future work suggestions.</p>
<p>A similar pattern was observed when GPT-4o mini + RAG was used as the generator.The combined FW + OR ground truth again outperformed FW alone on most metrics, including a +4.01 gain in ROUGE-L and a +9.30 gain in cosine similarity.Compared to OR alone, the combined setting also showed consistent advantages, though some LLM-based evaluation metrics saw minor declines.Overall, the results indicate that merging author and reviewer perspectives creates a stronger ground truth, allowing LLMs to generate future work that better captures the diversity of research directions.</p>
<p>GPT-extracted Ground Truth aligns well with LLMgenerated Future Work text than the tool-extracted ground truth, indicating that the tool-extracted Ground Truth contains more noise and irrelevant sentences.</p>
<p>VII. DISCUSSIONS</p>
<p>Our experiments focused on NLP and ML papers (ACL and NeurIPS) because these venues provide large, open-access datasets with rich peer-review feedback, making them ideal for benchmarking.However, the framework itself is not domainspecific.Extending it to other areas such as biomedicine or social sciences would primarily require (1) constructing domain-specific retrieval corpora, (2) curating ground truths from available peer reviews or domain experts, and (3) adjusting evaluation criteria to reflect community-specific standards.While the core methodology-RAG for grounding, LLM-asa-judge evaluation, and iterative self-feedback-remains the same, the effort lies mainly in dataset preparation and defining appropriate evaluation references for each field.RQ1: How reliable is an LLM in extracting future work statements compared to traditional tool-based methods?Traditional tools often miss Future Work when it is not a distinct section, whereas our LLM-based extractor can identify relevant content in such cases.Using LLM-extracted text as reference improves performance across NLP-and LLM-based metrics compared to tool-based extraction, as it filters out noise and yields cleaner ground truth.Human evaluations further confirm its reliability, showing that the LLM extracts relevant content without hallucination.RQ2: How does input context selection (top-3 vs. all sections) impact the quality of LLMgenerated future work, and how does RAG influence these results?Using only the top three most relevant sections, instead of the full paper, resulted in a slight decline across most NLP-based evaluation metrics.However, incorporating RAG with GPT-4o mini improved overall performance, reduced hallucinations, and increased some LLM based metrics, but it decreased the performance in n-gram or semantic similarity metrics as new information comes from the vector database.RQ3: How does expanding ground truth with peer-review goals and incorporating LLM feedback impact the performance and quality of LLM-generated future work?Augmenting the ground truth with long-term goals extracted from OpenReview peer reviews led to consistent improvements across all evaluation metrics.Which depicts LLM-generated future work can be more effectively grounded when both the author-mentioned future work and OpenReview feedback are used together as reference, rather than relying on a single source alone.Furthermore, a single round of LLM selffeedback consistently improved performance across both NLPbased and LLM-based evaluations.However, adding a second round of feedback introduced verbosity and reduced overall performance, suggesting diminishing returns with excessive iteration.Throughout our study, GPT-4o mini served as both the extractor and evaluator, demonstrating strong alignment with human judgments.</p>
<p>VIII. CONCLUSIONS</p>
<p>The Future Work section is a forward-looking guide, helping the research community explore new directions.We utilized an LLM to extract Future Work, producing a more coherent ground truth that enhances model performance and incorporated a strong ground truth from OpenReview to provide more broader perspective.Additionally, we integrated an external vector database to further improve LLM's performance to generate Future Work from input text.For evaluation, we applied NLP-based metrics alongside an LLM-as-a-Judge approach, using explainable LLM metrics to assess performance, provide feedback, and iteratively refine text generation.Onetime feedback improves the performance in all NLP and LLM-based metrics.Moreover, we measure hallucination rate, feasibility, and novelty to ensure that the generated future work is grounded in the paper's content, realistically achievable given the methods and data, and offers original, valuable directions beyond what is already stated.</p>
<p>LIMITATIONS AND FUTURE WORK</p>
<p>Our analysis is limited to ACL papers (2012-2024) and NeurIPS (2021-22), ensuring domain relevance but restricting cross-disciplinary generality.Using a single model for iterative refinement also risks stylistic convergence.For baselines (T5, BART), we capped inputs at 512-1,024 tokens and did not explore advanced feedback strategies (e.g., chain-of-thought, self-consistency).Our random selection of 100 papers for RAG may include works published after the target paper; in future, we will restrict retrieval to prior publications.Novelty was assessed using an LLM judge, but we plan to incorporate human evaluation for greater reliability.Looking ahead, we aim to extend our pipeline to other domains, evaluate open-source LLMs to reduce costs, build domainspecific retrieval corpora, and create a gold-standard dataset with more human annotators.We also plan to integrate citedby literature, advanced reasoning techniques, and evaluator alignment methods (RLHF/RLAIF), while releasing an opensource tool that generates Future Work suggestions directly from uploaded PDFs.</p>
<p>ETHICS STATEMENT</p>
<p>We recognize ethical considerations in extracting and generating Future Work-including intellectual property, authorship, and responsible AI use.Our framework is strictly assistive: it acknowledges sources, avoids claims of ownership, and anchors suggestions in the original paper through RAG, iterative refinement, and human feedback.It does not replace human insight but helps researchers organize and clarify directions.Still, we acknowledge risks: misuse could foster overreliance on AI and weaken critical reflection.As generative models advance toward broader sections of scientific writing, safeguards, transparency, and human responsibility remain essential.Our aim is to support-not supplant-authorship while preserving integrity and enabling more robust development of future research directions.</p>
<p>Fig. 1 .
1
Fig. 1.Overview of our LLM + RAG future-work pipeline: we extract author-written future work from the paper, extract long-term goals from peer reviews via a tool + LLM, generate new future work suggestions with a RAG-augmented LLM from the paper, evaluate their quality, and apply selffeedback for refinement.</p>
<p>Fig. 2 .
2
Fig. 2. Prompt for LLM as a Judge</p>
<p>Fig. 3 .
3
Fig. 3. Prompt for Iterative Refinement</p>
<p>Fig. 4 .
4
Fig. 4. Prompt for Measuring Novelty</p>
<p>Fig. 5 .
5
Fig. 5. Prompt for Measuring Hallucination</p>
<p>28T 3.5  16.65 86.2813.70 44.84 3.18 GPT4om 14.22 85.91 11.55 40.53 1.29 GPT4om+RAG 16.41 86.40 13.27 40.36 2.84 GPT Extracted Future Work (Fg) GPT 3.5 20.64 87.88 18.49 55.36 5.61 GPT 4om 17.69 87.44 15.87 50.52 2.72 GPT4om+RAG 14.19 87.67 16.95 49.32 3.42
ModelR-LBScore JSCSBleuTool extracted Future Work (Ft)</p>
<p>TABLE I PERFORMANCE
I
COMPARISON OF DIFFERENT MODELS, EVALUATED AGAINST TOOL-AND GPT-EXTRACTED FUTURE WORK USING LLM-GENERATED FUTURE WORK AS THE REFERENCE, BASED ON ACL PAPERS.NOTE: GPT 4OM, R-L, BSCORE, JS, CS REFERS TO GPT 4O MINI, ROUGE-L, BERTSCORE, JACCARD SIMILARITY, AND COSINE SIMILARITY, RESPECTIVELY.</p>
<p>"</p>
<p>You are an expert reviewer.Below is the content of a research paper followed by a suggestion for future work.Evaluate whether the future work is executable in the context of the paper's methodology, dataset, or other components.Respond with exactly one word: 'feasible' or 'not feasible'."Paper Content: Input paper Future Work Suggestion: LLM generated future work
Fig. 6. Prompt for Measuring FeasabilityAbs. Intro. RW Data Meth. Exp. Con. Lim.0.25 0.25 0.21 0.08 0.15 0.22 0.22 0.21</p>
<p>TABLE III COMPARISON
III
OF PERFORMANCE WITHOUT AND WITH LLM FEEDBACK USING GPT-4O MINI IN ACL DATA.</p>
<p>Table VI).Unlike established metrics such as BLEU or precision, novelty scoring is inherently subjective, as it depends on the evaluator model
ModelIterationR-LBSJSCSBleu Coh Rel ReadGrGPT 3.5120.64 87.88 18.49 55.36 5.61 3.89 4.473.384.04Llama 3 Zero Shot110.38 85.61 14.52 52.27 2.27 3.83 4.474.043.44Llama 3 Fine Tuning1-85.54 12.28 45.37 1.14 3.37 3.692.903.65GPT 4o mini117.69 87.44 15.87 50.52 2.72 3.94 4.073.194.02GPT 4o mini + RAG114.19 87.67 16.95 49.32 3.42 3.93 3.963.184.01GPT 4o mini + RAG (Ours)220.87 88.15 18.67 58.33 5.67 3.97 4.503.504.06GPT 4o mini + RAG320.18 87.94 17.92 56.34 4.68 3.96 4.343.364.05</p>
<p>TABLE V PERFORMANCE
V
COMPARISON OF VARIOUS MODELS IN GENERATING FUTURE WORK IN ACL DATA.INPUT IS THE TOP 3 SECTIONS WITH THE ABSTRACT.
NOTE: METRICS INCLUDE ROUGE (R-L), BERTSCORE (BS), COSINE SIMILARITY (CS), JACCARD SIMILARITY (JS), BLEU (BL), COHERENCE(COH), RELEVANCE (REL), READABILITY (READ), GRAMMAR (GRAM).GTIter.R-LBSCSJSBleu Coh Rel Read Gram Novelty OverallGPT 4o miniFW115.54 85.02 68.23 11.21 2.29 4.31 4.633.824.817.284.29OR112.34 82.94 62.56 10.43 1.26 3.67 4.373.194.187.843.65FW+OR118.32 85.20 74.33 14.46 3.55 4.41 4.873.854.677.284.40FW+OR217.43 84.63 73.50 14.29 3.42 4.48 4.794.154.898.004.44GPT 4o mini + RAGFW111.89 83.00 63.278.721.08 4.26 4.044.875.007.704.15OR112.43 82.81 62.91 10.15 1.01 4.56 4.564.895.007.804.52FW+OR115.90 83.47 72.57 12.29 2.02 4.17 4.364.895.007.114.38</p>
<p>TABLE VI PERFORMANCE
VI
COMPARISON BETWEEN DIFFERENT METHODS IN NEURIPS DATA.HERE, GT, FW, OR, ITER MEANS GROUND TRUTH, AUTHOR MENTIONED FUTURE WORK, OPENREVIEW SUGGESTION, AND ITERATIONS, RESPECTIVELY.
MetricsAll sections3 sectionsROUGE-121.0820.3(-0.78)ROUGE-26.415.36(-1.05)ROUGE-L17.0316.65(-0.38)BScore(f1)86.4086.28(-0.12)Jaccard S14.5413.70(-0.84)Cosine S45.6944.84(-0.85)BLEU4.133.18(-0.95)</p>
<p>TABLE VII PERFORMANCE
VII
COMPARISON OF GPT-3.5 IN GENERATING FUTURE WORK USING THREE SELECTED SECTIONS VERSUS FULL-TEXT INPUT, EVALUATED AGAINST BOTH TOOL EXTRACTED GROUND TRUTH (GT)AND GPT-EXTRACTED GROUND TRUTH ON ACL DATA.</p>
<p>TABLE IX AVERAGE
IX
RATING AND ANNOTATORS USER AGREEMENT ON USER STUDY.</p>
<p>https://openreview.net
https://github.com/allenai/science-parse
ACKNOWLEDGMENTSWe acknowledge the use of ChatGPT to refine the writing of this paper, specifically to improve readability, clarity, and flow.The research ideas, experimental design, data collection, analyses, and substantive contributions are entirely the work of the authors.ChatGPT was used only as a language assistant to polish text expression, and not for generating content, research ideas, or results.
Do younger researchers assess trustworthiness differently when deciding what to read and cite and where to publish?. David Nicholas, International Journal of Knowledge Content Development &amp; Technology. 522015</p>
<p>What you see is what you get? Enhancing methodological transparency in management research. Aguinis, Ravi S Herman, Nawaf Ramani, Alabduljader, Academy of Management Annals. 122018</p>
<p>An emerging view of scientific collaboration: Scientists' perspectives on collaboration and factors that impact collaboration. Noriko Hara, Journal of the American Society for Information science and Technology. 542003</p>
<p>National policy-makers speak out: are researchers giving them what they need?. Adnan A Hyder, Health policy and planning. 262011</p>
<p>What is research funding, how does it influence research, and how is it recorded? Key dimensions of variation. Mike Thelwall, Scientometrics. 1282023</p>
<p>Peer review in scientific publications: benefits, critiques, &amp; a survival guide. Jacalyn Kelly, Tara Sadeghieh, Khosrow Adeli, Ejifcc. 252272014</p>
<p>What research do state education agencies really need? The promise and limitations of state longitudinal data systems. Carrie Conaway, Venessa Keesler, Nathaniel Schwartz, Educational Evaluation and Policy Analysis. 372015</p>
<p>Performance-based funding in American higher education: A systematic synthesis of the intended and unintended consequences. Justin C Ortagus, Educational Evaluation and Policy Analysis. 422020</p>
<p>How the Future Works at. Jacques Suray, arXiv:2405.20785SOUPS: Analyzing Future Work Statements and Their Impact on Usable Security and Privacy Research. 2024arXiv preprint</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Nature. 6202023</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence from a Large, Dynamic Experiment. Joshua Ashkinaze, arXiv:2401.134812024arXiv preprint</p>
<p>Homogenization Effects of Large Language Models on Human Creative Ideation. Barrett R Anderson, Proceedings of the 16th Conference on Creativity &amp; Cognition. the 16th Conference on Creativity &amp; Cognition2024</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. 2004</p>
<p>BLEU: A Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational Linguistics2002</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, arXiv:1904.096752019arXiv preprint</p>
<p>BERTopic: Neural Topic Modeling with a Class-Based TF-IDF Procedure. Maarten Grootendorst, arXiv:2203.057942022arXiv preprint</p>
<p>A Deep Learning Approach for Sentence Classification of Scientific Abstracts. Gonc ¸alves, Sérgio, Artificial Neural Networks and Machine Learning-ICANN 2018. Springer2018</p>
<p>Method Mention Extraction from Scientific Research Papers. Hospice Houngbo, Robert E Mercer, Proceedings of COLING 2012. COLING 20122012</p>
<p>LimTopic: LLM-Based Topic Modeling and Text Summarization for Analyzing Scientific Articles Limitations. Al Azher, Ibrahim, 2024 ACM/IEEE Joint Conference on Digital Libraries (JCDL). 2024</p>
<p>Generating Suggestive Limitations from Research Articles Using LLM and Graph-Based Approach. Ibrahim Azher, Al, Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries. the 24th ACM/IEEE Joint Conference on Digital Libraries2024</p>
<p>Mitigating Visual Limitations of Research Papers. Al Azher, Ibrahim , Hamed Alhoori, 2024 IEEE International Conference on Big Data (BigData). 2024</p>
<p>Mining and Analyzing the Future Works in Scientific Articles. Yue Hu, Xiaojun Wan, arXiv:1507.021402015arXiv preprint</p>
<p>Automatic Recognition and Classification of Future Work Sentences from Academic Articles in a Specific Domain. Chengzhi Zhang, Journal of Informetrics. 1711013732023</p>
<p>Identifying Academic Creative Concept Topics Based on Future Work of Scientific Papers. Ruoxuan Song, Data Analysis and Knowledge Discovery. 552021</p>
<p>The ACL FWS-RC: A Dataset for Recognition and Classification of Sentences about Future Works. Wenke Hao, Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020. the ACM/IEEE Joint Conference on Digital Libraries in 20202020</p>
<p>Using Future Work Sentences to Explore Research Trends of Different Tasks in a Special Domain. Yuchen Qian, Proceedings of the Association for Information Science and Technology. the Association for Information Science and Technology202158</p>
<p>Recognizing Sentences Concerning Future Research from the Full Text of JASIST. Zihe Zhu, Proceedings of the Association for Information Science and Technology. the Association for Information Science and Technology201956</p>
<p>How the Future Works at. Jacques Suray, arXiv:2405.20785SOUPS: Analyzing Future Work Statements and Their Impact on Usable Security and Privacy Research. 2024arXiv preprint</p>
<p>Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination. Marissa Radensky, arXiv:2409.146342024arXiv preprint</p>
<p>Large Language Models Are Zero Shot Hypothesis Proposers. Biqing Qi, arXiv:2311.059652023arXiv preprint</p>
<p>Automated Social Science: Language Models as Scientist and Subjects. Benjamin S Manning, 2024National Bureau of Economic Research</p>
<p>GFlowNets for AI-Driven Scientific Discovery. Moksh Jain, Digital Discovery. 232023</p>
<p>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. Chris Lu, arXiv:2408.062922024arXiv preprint</p>
<p>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. Chenglei Si, arXiv:2409.041092024arXiv preprint</p>
<p>Training Language Models to Follow Instructions with Human Feedback. Long Ouyang, Advances in Neural Information Processing Systems. 202235</p>
<p>Teaching Large Language Models to Self-Debug. Xinyun Chen, arXiv:2304.051282023arXiv preprint</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, arXiv:2203.111712022arXiv preprint</p>
<p>Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback. Yao Fu, arXiv:2305.101422023arXiv preprint</p>
<p>Large Language Models for Automated Open-Domain Scientific Hypotheses Discovery. Zonglin Yang, arXiv:2309.027262023arXiv preprint</p>
<p>Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. Baolin Peng, arXiv:2302.128132023arXiv preprint</p>
<p>Self-Refine: Iterative Refinement with Self-Feedback. Aman Madaan, Advances in Neural Information Processing Systems. 202336</p>
<p>Can Large Language Models Be an Alternative to Human Evaluations. Cheng- Chiang, Hung-Yi Han, Lee, arXiv:2305.019372023arXiv preprint</p>
<p>A Comparative Study of Quality Evaluation Methods for Text Summarization. Huyen Nguyen, arXiv:2407.007472024arXiv preprint</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. Edward J Hu, ICLR. 1232022</p>
<p>QLoRA: Efficient Finetuning of Quantized LLMs. Tim Dettmers, Advances in Neural Information Processing Systems. 202336</p>
<p>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Tri Dao, Advances in Neural Information Processing Systems. 202235</p>
<p>Do" Future Work" sections have a purpose? Citation links and entailment for global scientometric questions. Simone Teufel, BIRNDL@ SIGIR. 2017</p>
<p>Using Future Work sentences to explore research trends of different tasks in a special domain. Yuchen Qian, Proceedings of the Association for Information Science and Technology. 582021</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Advances in neural information processing systems. 332020</p>
<p>BAGELS: Benchmarking the Automated Generation and Extraction of Limitations from Scholarly Text. Ibrahim Azher, Al, arXiv:2505.182072025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
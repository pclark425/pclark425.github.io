<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5397 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5397</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5397</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-19e499d76a2e49574b499cd9ebd31304880d33d6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/19e499d76a2e49574b499cd9ebd31304880d33d6" target="_blank">Language Model Cascades: Token-level uncertainty and beyond</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is argued that naive predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties in generative LM tasks, and it is shown that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies.</p>
                <p><strong>Paper Abstract:</strong> Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs. Cascading offers a simple strategy to achieve more favorable cost-quality tradeoffs: here, a small model is invoked for most"easy"instances, while a few"hard"instances are deferred to the large model. While the principles underpinning cascading are well-studied for classification tasks - with deferral based on predicted class uncertainty favored theoretically and practically - a similar understanding is lacking for generative LM tasks. In this work, we initiate a systematic study of deferral rules for LM cascades. We begin by examining the natural extension of predicted class uncertainty to generative LM tasks, namely, the predicted sequence uncertainty. We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across examples. To mitigate this issue, we propose to exploit the richer token-level uncertainty information implicit in generative LMs. We argue that naive predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties. By contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models. We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5397.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5397.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative confidence probe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative confidence probe / self-probing (Kadavath et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that reprompts a language model to explicitly report its confidence in a generated answer or to output an auxiliary confidence score; also discussed is training an extra prediction head to estimate model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models (mostly) know what they know</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>generative confidence probe / self-probing</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model is prompted (or reprompted) to produce an explicit confidence estimate for its answer (e.g., natural-language confidence statement or numeric score) or an additional head is trained to output confidence; this is a form of querying the model about its own certainty rather than aggregating token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>The paper cites Kadavath et al. (2022) as exploring this approach but states it is not clear how these probes compare to standard softmax-based probabilities without additional finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes unclear comparative effectiveness; such probes may require prompt-engineering or additional training (an extra head) and the authors say it's not established that they outperform classical softmax-based confidence; potential calibration issues are implied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Cascades: Token-level uncertainty and beyond', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5397.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5397.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linguistic self-reported confidence (Shrivastava et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linguistic confidence reports / surrogate confidence estimation (Shrivastava et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods in which a model linguistically expresses its confidence (e.g., 'I am 80% sure') or where surrogate models are used to estimate confidence from model outputs; evaluated against classical softmax-derived uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llamas know what gpts don’t show: Surrogate models for confidence estimation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>linguistic confidence probe / surrogate confidence estimation</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model is asked to produce a linguistic confidence statement or a surrogate model is used to estimate confidence from outputs rather than relying solely on internally computed softmax probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>The paper reports (via citation) that Shrivastava et al. (2023) found linguistically generated confidence measures give worse uncertainty estimates than classical softmax-based measures (even when the softmax-based measures come from a weaker model), though they also found the two sources can be complementary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported worse estimates compared to softmax-based measures; suggests linguistic confidence alone may be unreliable but can add complementary information when combined with other uncertainty signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Cascades: Token-level uncertainty and beyond', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5397.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5397.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consensus / answer-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consensus-based uncertainty via multiple samples / answer consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measure confidence by sampling multiple outputs from a generative model and assessing consensus or agreement among samples; higher consensus implies higher confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>consensus-based / answer consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple output sequences (via sampling) from the model and quantify agreement (consensus) among them as a confidence measure; can distinguish peaked from diffuse output distributions and has been used to construct cascades.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>The paper cites prior work (e.g., Yue et al. 2023 and others) indicating that sampling and measuring consensus can produce useful confidence signals for cascades, but provides no direct experimental comparison in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Consensus methods require multiple decodings (higher computation) and thus are less attractive for cost-sensitive cascades; semantic equivalence across diverse outputs can complicate consensus assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Cascades: Token-level uncertainty and beyond', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5397.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5397.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selective clarification (CLAM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLAM: Selective clarification for ambiguous questions with generative language models (Kuhn et al., 2023b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative approach where the model first detects ambiguity or uncertainty in a prompt, then asks clarifying questions and conditions its final answer on the clarification, i.e., an interactive generate-then-reflect/clarify procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Clam: Selective clarification for ambiguous questions with generative language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>selective clarification / iterative clarification</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model identifies ambiguous or uncertain queries and generates clarifying questions; after receiving clarifications (or simulating them), it re-generates its answer, thereby using an iterative generate-then-reflect (clarify-then-answer) loop to improve final output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>The paper references Kuhn et al. (2023b) as proposing clarification/iterative questioning to handle ambiguity, implying benefits in some settings, but provides no quantitative results in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires extra interaction or simulated clarification steps; may add latency and cost; effectiveness depends on ability to detect true ambiguity and obtain useful clarifications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model Cascades: Token-level uncertainty and beyond', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models (mostly) know what they know <em>(Rating: 2)</em></li>
                <li>Llamas know what gpts don’t show: Surrogate models for confidence estimation <em>(Rating: 2)</em></li>
                <li>Clam: Selective clarification for ambiguous questions with generative language models <em>(Rating: 2)</em></li>
                <li>Answer consistency / consensus-based uncertainty (Yue et al., 2023) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5397",
    "paper_id": "paper-19e499d76a2e49574b499cd9ebd31304880d33d6",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "Generative confidence probe",
            "name_full": "Generative confidence probe / self-probing (Kadavath et al., 2022)",
            "brief_description": "A technique that reprompts a language model to explicitly report its confidence in a generated answer or to output an auxiliary confidence score; also discussed is training an extra prediction head to estimate model confidence.",
            "citation_title": "Language models (mostly) know what they know",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "generative confidence probe / self-probing",
            "reflection_method_description": "The model is prompted (or reprompted) to produce an explicit confidence estimate for its answer (e.g., natural-language confidence statement or numeric score) or an additional head is trained to output confidence; this is a form of querying the model about its own certainty rather than aggregating token probabilities.",
            "num_iterations": null,
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "The paper cites Kadavath et al. (2022) as exploring this approach but states it is not clear how these probes compare to standard softmax-based probabilities without additional finetuning.",
            "limitations_or_failure_cases": "Paper notes unclear comparative effectiveness; such probes may require prompt-engineering or additional training (an extra head) and the authors say it's not established that they outperform classical softmax-based confidence; potential calibration issues are implied.",
            "uuid": "e5397.0",
            "source_info": {
                "paper_title": "Language Model Cascades: Token-level uncertainty and beyond",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Linguistic self-reported confidence (Shrivastava et al.)",
            "name_full": "Linguistic confidence reports / surrogate confidence estimation (Shrivastava et al., 2023)",
            "brief_description": "Methods in which a model linguistically expresses its confidence (e.g., 'I am 80% sure') or where surrogate models are used to estimate confidence from model outputs; evaluated against classical softmax-derived uncertainty.",
            "citation_title": "Llamas know what gpts don’t show: Surrogate models for confidence estimation",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "linguistic confidence probe / surrogate confidence estimation",
            "reflection_method_description": "The model is asked to produce a linguistic confidence statement or a surrogate model is used to estimate confidence from outputs rather than relying solely on internally computed softmax probabilities.",
            "num_iterations": null,
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "The paper reports (via citation) that Shrivastava et al. (2023) found linguistically generated confidence measures give worse uncertainty estimates than classical softmax-based measures (even when the softmax-based measures come from a weaker model), though they also found the two sources can be complementary.",
            "limitations_or_failure_cases": "Reported worse estimates compared to softmax-based measures; suggests linguistic confidence alone may be unreliable but can add complementary information when combined with other uncertainty signals.",
            "uuid": "e5397.1",
            "source_info": {
                "paper_title": "Language Model Cascades: Token-level uncertainty and beyond",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Consensus / answer-consistency",
            "name_full": "Consensus-based uncertainty via multiple samples / answer consistency",
            "brief_description": "Measure confidence by sampling multiple outputs from a generative model and assessing consensus or agreement among samples; higher consensus implies higher confidence.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "consensus-based / answer consistency",
            "reflection_method_description": "Generate multiple output sequences (via sampling) from the model and quantify agreement (consensus) among them as a confidence measure; can distinguish peaked from diffuse output distributions and has been used to construct cascades.",
            "num_iterations": null,
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "The paper cites prior work (e.g., Yue et al. 2023 and others) indicating that sampling and measuring consensus can produce useful confidence signals for cascades, but provides no direct experimental comparison in this work.",
            "limitations_or_failure_cases": "Consensus methods require multiple decodings (higher computation) and thus are less attractive for cost-sensitive cascades; semantic equivalence across diverse outputs can complicate consensus assessment.",
            "uuid": "e5397.2",
            "source_info": {
                "paper_title": "Language Model Cascades: Token-level uncertainty and beyond",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Selective clarification (CLAM)",
            "name_full": "CLAM: Selective clarification for ambiguous questions with generative language models (Kuhn et al., 2023b)",
            "brief_description": "An iterative approach where the model first detects ambiguity or uncertainty in a prompt, then asks clarifying questions and conditions its final answer on the clarification, i.e., an interactive generate-then-reflect/clarify procedure.",
            "citation_title": "Clam: Selective clarification for ambiguous questions with generative language models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "selective clarification / iterative clarification",
            "reflection_method_description": "The model identifies ambiguous or uncertain queries and generates clarifying questions; after receiving clarifications (or simulating them), it re-generates its answer, thereby using an iterative generate-then-reflect (clarify-then-answer) loop to improve final output quality.",
            "num_iterations": null,
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "The paper references Kuhn et al. (2023b) as proposing clarification/iterative questioning to handle ambiguity, implying benefits in some settings, but provides no quantitative results in this work.",
            "limitations_or_failure_cases": "Requires extra interaction or simulated clarification steps; may add latency and cost; effectiveness depends on ability to detect true ambiguity and obtain useful clarifications.",
            "uuid": "e5397.3",
            "source_info": {
                "paper_title": "Language Model Cascades: Token-level uncertainty and beyond",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 2
        },
        {
            "paper_title": "Llamas know what gpts don’t show: Surrogate models for confidence estimation",
            "rating": 2
        },
        {
            "paper_title": "Clam: Selective clarification for ambiguous questions with generative language models",
            "rating": 2
        },
        {
            "paper_title": "Answer consistency / consensus-based uncertainty (Yue et al., 2023)",
            "rating": 1
        }
    ],
    "cost": 0.013004749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Model Cascades: Token-level uncertainty and beyond</h1>
<p>Neha Gupta* Harikrishna Narasimhan Wittawat Jitkrittum<br>Ankit Singh Rawat Aditya Krishna Menon Sanjiv Kumar<br>Google Research, New York</p>
<p>April 17, 2024</p>
<h4>Abstract</h4>
<p>Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs. Cascading offers a simple strategy to achieve more favorable cost-quality tradeoffs: here, a small model is invoked for most "easy" instances, while a few "hard" instances are deferred to the large model. While the principles underpinning cascading are wellstudied for classification tasks - with deferral based on predicted class uncertainty favored theoretically and practically - a similar understanding is lacking for generative LM tasks. In this work, we initiate a systematic study of deferral rules for LM cascades. We begin by examining the natural extension of predicted class uncertainty to generative LM tasks, namely, the predicted sequence uncertainty. We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across examples. To mitigate this issue, we propose to exploit the richer token-level uncertainty information implicit in generative LMs. We argue that naïve predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties. By contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models. We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff.</p>
<h2>1 Introduction</h2>
<p>Recent advances in generative language modeling have yielded a series of Transformer-based models with remarkably improved quality on complex NLP tasks (Radford et al., 2018; Raffel et al., 2020; Brown et al., 2020; Black et al., 2022; Hoffmann et al., 2022; Chowdhery et al., 2022; Wei et al., 2022; Chung et al., 2022; Tay et al., 2023; Anil et al., 2023; Touvron et al., 2023; Team et al., 2023). Unfortunately, such models also involve significantly increased inference costs, which has motivated a series of efforts at reducing the same. These span careful infrastructure optimization (Chowdhery et al., 2022; Pope et al., 2022; Sheng et al., 2023),</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) In cascades, small models are used for easy instances whereas hard instances are routed to larger models. For generative LMs, the key challenge is to design a deferral rule based on uncertainties from multiple tokens. (b) Standard baselines which take the product and geometric mean of the probabilities are affected by the length of the output and perform sub-optimally. (c) Our proposed solution captures nuanced per-token uncertainty and outperforms both baselines.
rethinking the autoregressive decoding that underpin Transformers (Stern et al., 2018; Leviathan et al., 2023; Chen et al., 2023a; Sun et al., 2023), modifications of the underlying model architecture (Dao et al., 2022), and model compression strategies (Frantar \&amp; Alistarh, 2023; Agarwal et al., 2023).</p>
<p>Cascading is one simple strategy to achieve more favorable cost-quality tradeoffs via adaptive inference. In a two-model cascade, a small model is invoked for most "easy" instances, while a few "hard" instances are deferred to a large model. Cascades have been widely explored in the vision domain (Viola \&amp; Jones, 2001; Trapeznikov \&amp; Saligrama, 2013; Bolukbasi et al., 2017; Huang et al., 2018; Rawat et al., 2021; Kag et al., 2023; Jitkrittum et al., 2023), and have seen increasing adoption within NLP (Mamou et al., 2022; Varshney \&amp; Baral, 2022; Khalili et al., 2022; Dohan et al., 2022; Chen et al., 2023b,a). Importantly, cascades can be implemented in a black-box fashion over existing models, and do not necessitate any additional training.</p>
<p>The key challenge in cascading is to design a deferral rule which decides whether to defer an input to the larger model. The principles underpinning optimal deferral rules are well known for the classification setting, where the standard recipe is to employ the small model's prediction confidence, as canonically measured by its softmax probability output (Chow's rule (Chow, 1970)). This simple deferral rule is remarkably hard to surpass in most natural settings (Jitkrittum et al., 2023).</p>
<p>However, the narrative is more complex for generative LMs. While one can naïvely translate Chow's rule for such models based on the softmax probability of the output sequence, this suffers from a length bias issue: one tends to defer longer predictions, regardless of quality. Further, simply normalizing the probability by sequence length tends to over-correct this bias, and defer shorter predictions. Intuitively, such naïve translations of Chow's rule ignore a key distinction between the classification and generative LM setting: the former involves a single probability distribution over labels, while the latter involves a sequence of distributions over multiple tokens of the LM output; moreover, the number of output tokens differs across examples. This variability complicates summarizing the sequence of uncertainty (or confidence) values into a single deferral score.</p>
<p>To mitigate the length bias and capture fine-grained information from the uncertainty vector over tokens, we propose to use quantiles over the vector. Via experiments on a range of NLP benchmarks and FLAN-T5 models, we show that these quantiles can capture rich and complementary sources of uncertainty informa-</p>
<p>tion from the uncertainty sequence vector and do better than the simple aggregation schemes like sum and average. However, we observe that there is no fixed quantile value which works across all datasets. This motivates us to learn a deferral rule based on these quantile values as features, which can combine the strengths of these different quantile scores. We show that our trained deferral rule is the most consistently performant method compared to all the natural baseline aggregation strategies. We further show that using embeddings from the smaller model and intermediate embeddings from the larger model can give further performance improvement.</p>
<p>To summarize, our contributions are:
(i) We show that simple sequence-level LM confidence measures for deferral can yield strongly sub-optimal cost-quality tradeoffs, owing to a length bias issue (§3.5).
(ii) We introduce token-level uncertainty in the form of distribution quantiles, and show that they can yield to consistently more effective cost-quality tradeoffs, owing to their finer-grained measure of uncertainty. However, there is no fixed quantile which works well across all settings (§3.5).
(iii) We propose a post-hoc deferral rule trained on quantile features, and show it can outperform all other strategies on a range of NLP benchmarks for FLAN-T5 models (§4.3). We further demonstrate that using the large model's intermediate embeddings can significantly boost performance.</p>
<h1>2 Background and Problem Setup</h1>
<p>In this section, we discuss the relevant background and set up the problem of LM cascades.
Language models (LMs). Given a finite vocabulary $\mathcal{V}$ (e.g., tokens derived from SentencePiece (Kudo \&amp; Richardson, 2018)), a language model ( $L M$ ) defines a distribution $p(\cdot \mid \boldsymbol{x}) \in \Delta(\mathcal{V})$ over all possible tokens given any context $\boldsymbol{x}=\left(x_{1}, \ldots, x_{m}\right) \in \mathcal{V}^{m}$. This in turn defines a distribution over sequences $\boldsymbol{y}=\left(y_{1}, \ldots, y_{n}\right) \in \mathcal{V}^{n}$ for any $n \in \mathbb{N}<em 1="1">{+}$, with $p\left(y</em>\right)$ via the chain rule of probability.}, \ldots, y_{n} \mid \boldsymbol{x}\right)=p\left(y_{1} \mid \boldsymbol{x}\right) \cdot \prod_{i=1}^{n-1} p\left(y_{i+1} \mid \boldsymbol{x}, y_{1}, \ldots, y_{i</p>
<p>LMs based on Transformers (Vaswani et al., 2017) have proven increasingly popular in recent years. Such models are typically pre-trained on large corpora based on self-supervised objectives (Radford et al., 2018; Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020; Tay et al., 2022; Anil et al., 2023). These objectives involve different (input, output) pair constructions $(\boldsymbol{x}, \boldsymbol{y})$ (e.g., masking out the next token), upon which one minimizes the cross-entropy or log-loss, i.e., $-\log p(\boldsymbol{y} \mid \boldsymbol{x})$.</p>
<p>At inference time, given a trained LM and any input context $\boldsymbol{x}$, it is common to perform either classification or generation. In the former, given a set of predefined choices $\mathcal{C}=\left{\boldsymbol{c}<em _in_L_="\in[L]" i="i">{i}\right}</em>)$ to produce a suitable output string response, e.g., by temperature (Ficler \&amp; Goldberg, 2017), top- $k$ (Fan et al., 2018), or nucleus sampling (Holtzman et al., 2020).}$ (e.g., { yes, no }), one scores each $p\left(\boldsymbol{c}_{i} \mid \boldsymbol{x}\right)$ and returns the highest scoring choice. In the latter, one performs sampling from $p(\cdot \mid \boldsymbol{x</p>
<p>Model cascades. Cascades are a simple, generic strategy to improve the inference cost-quality tradeoff (Wang et al., 2022). Given a collection of models of varying inference cost, the key idea is to perform adaptive inference: "easy" samples are afforded less computation compared to "hard" samples. Concretely, for any test input, one first executes the lowest cost model, and uses a deferral rule to determine whether to terminate with its prediction, or to invoke the next cheapest model. Cascades can reduce the average inference cost if only a small fraction of inputs are deferred.</p>
<p>Cascades have a long history of usage in vision (Viola \&amp; Jones, 2001; Huang et al., 2018; Wang et al., 2018), where they are often applied for classification problems. Given an instance space $\mathcal{X}$ and label space $\mathcal{Y}$, the classification problem seeks a classifier $h: \mathcal{X} \rightarrow \mathcal{Y}$ with good average quality under some distribution</p>
<p>$\mathbb{P}$, as measured by $\mathbb{E}<em _="+">{(x, y) \sim \mathbb{P}}[q(x, y, h(x))]$ for some $q(x, y, h(x)) \in \mathbb{R}</em>)$ measures the classifier accuracy.}$. In the simplest case, $q(x, y, \hat{y})=$ $\mathbf{1}(y=\hat{y</p>
<p>Now suppose we have two classifiers $h^{(1)}, h^{(2)}$, with inference costs (e.g., latencies) $c^{(1)} \ll c^{(2)}$. Operationally, a cascade first invokes the "small" model $h^{(1)}$, and then applies a deferral rule to decide whether to either defer prediction to the "large" model $h^{(2)}$, or terminate with $h^{(1)}$ 's prediction. More precisely, let $r: \mathcal{X} \rightarrow{0,1}$ denote the deferral rule, where $r(x)=1$ denotes that we wish to defer to the large model. Then, the cascaded classifier is (Kag et al., 2023; Jitkrittum et al., 2023):</p>
<p>$$
h^{\text {cas }}(x)=\mathbf{1}(r(x)=0) \cdot h^{(1)}(x)+\mathbf{1}(r(x)=1) \cdot h^{(2)}(x)
$$</p>
<p>Given an input $x$, the corresponding cascade quality and cost are:</p>
<p>$$
\begin{aligned}
Q\left(x, y, h^{\text {cas }}(x)\right) &amp; \doteq \mathbf{1}(r(x)=0) \cdot q\left(x, y, h^{(1)}(x)\right)+\mathbf{1}(r(x)=1) \cdot q\left(x, y, h^{(2)}(x)\right) \
C\left(x, h^{\text {cas }}(x)\right) &amp; \doteq \mathbf{1}(r(x)=0) \cdot c^{(1)}+\mathbf{1}(r(x)=1) \cdot\left(c^{(1)}+c^{(2)}\right)
\end{aligned}
$$</p>
<p>Ideally, one seeks to maximize quality given a budget $B$ on average inference cost:</p>
<p>$$
\max <em x_="x," y="y">{r: \mathcal{X} \rightarrow{0,1}} \mathbb{E}</em>(x)\right)\right] \leq B
$$}\left[Q\left(x, y, h^{\text {cas }}(x)\right)\right]: \quad \mathbb{E}_{x}\left[C\left(x, h^{\text {cas }</p>
<p>We note that the average cost $\mathbb{E}\left[C\left(x, h^{\text {cas }}(x)\right)\right]$ is related to the deferral rate $D(x)=\mathbb{P}(r(x)=1)$, via $\mathbb{E}\left[C\left(x, h^{\text {cas }}(x)\right)\right]=c^{(1)}+D(x) \cdot c^{(2)}$. In practice, one may set $r(x)=\mathbf{1}(s(x)&lt;t)$ for suitable $s: \mathcal{X} \rightarrow \mathbb{R}$ and threshold $t \in \mathbb{R}$. One may choose $t$ to satisfy the inference cost constraint.</p>
<p>Now, we discuss cascades for generative LMs. This largely follows the setup described above, except that we now consider probabilistic models over sequences. Concretely, suppose we have two language models $p^{(1)}, p^{(2)}$, with inference costs $c^{(1)}, c^{(2)}$. Similarly, suppose $q: \mathcal{V}^{m} \times \mathcal{V}^{m^{\prime}} \times \Delta\left(\mathcal{V}^{n}\right) \rightarrow \mathbb{R}_{+}$is a measure of the quality of a given distribution over responses for a given prompt. A cascade $p^{\text {cas }}$ of these models is parameterized by a deferral rule $r: \mathcal{V}^{m} \rightarrow{0,1}$, and is given by:</p>
<p>$$
p^{\text {cas }}(\cdot \mid \boldsymbol{x})=\mathbf{1}(r(\boldsymbol{x})=0) \cdot p^{(1)}(\cdot \mid \boldsymbol{x})+\mathbf{1}(r(\boldsymbol{x})=1) \cdot p^{(2)}(\cdot \mid \boldsymbol{x})
$$</p>
<p>Given an input sequence $\boldsymbol{x}$ and target sequence $\boldsymbol{y}$, an LM cascade results in quality and cost</p>
<p>$$
\begin{aligned}
Q\left(\boldsymbol{x}, \boldsymbol{y}, p^{\text {cas }}(\cdot \mid \boldsymbol{x})\right) &amp; \doteq \mathbf{1}(r(\boldsymbol{x})=0) \cdot q\left(\boldsymbol{x}, \boldsymbol{y}, p^{(1)}(\cdot \mid \boldsymbol{x})\right)+\mathbf{1}(r(\boldsymbol{x})=1) \cdot q\left(\boldsymbol{x}, \boldsymbol{y}, p^{(2)}(\cdot \mid \boldsymbol{x})\right) \
C\left(\boldsymbol{x}, p^{\text {cas }}(\cdot \mid \boldsymbol{x})\right) &amp; \doteq \mathbf{1}(r(\boldsymbol{x})=0) \cdot c^{(1)}+\mathbf{1}(r(\boldsymbol{x})=1) \cdot\left(c^{(1)}+c^{(2)}\right)
\end{aligned}
$$</p>
<p>With these, we may construct a similar constrained objective as in Equation 1. Similarly to the classification case, we may parameterize the deferral rule as $r(\boldsymbol{x})=\mathbf{1}(s(\boldsymbol{x})&lt;t))$ for a suitable deferral score function $s: \mathcal{V}^{m} \rightarrow \mathbb{R}$ (which may depend on the output of $p^{(1)}(\cdot \mid \boldsymbol{x})$ ). We will investigate and analyze different types of deferral score functions on different NLP tasks.</p>
<p>Recently, Chen et al. (2023b) introduced the FrugalGPT system to achieve efficient inference via multiple strategies, including LM cascades. They also learn a deferral score to determine whether or not to terminate prediction; however, this depends on the input prompt and the generated output text, and does not consider the model's token-level uncertainty as we shall explore subsequently. A few works have proposed to learn a router which can decide which model to use amongst a set of models depending upon the input prompt (Shnitzer et al., 2023; Hari \&amp; Thomson, 2023). However, their settings do not necessarily consider models of increasing capacity and hence, their routers depend only on the input prompt not on the model confidence.</p>
<h1>3 Confidence Measures for Language Model Cascades</h1>
<p>A key question in the design of cascades is the choice of deferral rule. In this work, we seek to understand the behaviors of different types of deferral functions on NLP tasks. We start by discussing a few natural extensions of commonly used deferral rules for classification.</p>
<h3>3.1 Chow-Sum and Chow-Average</h3>
<p>Chow-Sum. We start with the multi-class classification setting where the output space $\mathcal{Y}={1, \ldots, L}$ and $L \in \mathbb{N}_{+}$. In the simplest case, one may defer if the confidence in the prediction $h^{(1)}(x)$ of the small model is sufficiently low. There are several means of quantifying confidence in classification (Shafer \&amp; Vovk, 2008; Guo et al., 2017; Kendall \&amp; Gal, 2017; Jiang et al., 2018), but arguably the simplest is the predicted class probability (Huang et al., 2018; Wang et al., 2022; Jitkrittum et al., 2023), which aligns with Chow's rule from the closely related problem (see Mozannar \&amp; Sontag (2020); Narasimhan et al. (2022)) of learning to reject (Chow, 1970):</p>
<p>$$
s(x) \doteq p^{(1)}(\hat{y} \mid x)
$$</p>
<p>where $p^{(1)}(\cdot \mid x)$ denotes the predictive distribution over possible labels of the small model, and $\hat{y}=$ $\arg \max _{y \in \mathcal{Y}} p^{(1)}(y \mid x)$ denotes the predicted label.</p>
<p>To design a deferral rule for LM cascading, a natural starting point is to mimic the predicted class probability (Equation 2): we may compute the (log) probability of the model generated sequence $\hat{\boldsymbol{y}}$,</p>
<p>$$
\begin{aligned}
s_{\text {sum }}(\boldsymbol{x}) &amp; \doteq \log p^{(1)}(\hat{\boldsymbol{y}} \mid \boldsymbol{x}) \
&amp; =\sum_{i=0}^{|\hat{\boldsymbol{y}}|-1} \log p^{(1)}\left(\hat{y}<em 1="1">{i+1}^{\prime} \mid \boldsymbol{x}, \hat{y}</em>\right)
\end{aligned}
$$}^{\prime}, \ldots, \hat{y}_{i}^{\prime</p>
<p>We term this approach Chow-Sum, as it involves the sum of per-token log probabilities. Analogous to the prediction rule for classification, we may set $\hat{\boldsymbol{y}} \doteq \arg \max _{\boldsymbol{y} \in \mathcal{V}^{<em>}} p^{(1)}(\boldsymbol{y} \mid \boldsymbol{x})$, denoting by $\mathcal{V}^{</em>}$ the set of all sequences. This requires searching over an exponentially large set; however, efficient approximations via greedy or beam search are feasible.</p>
<p>Chow-Average. Chow-Sum computes the aggregate sequence-level log-probability. A natural variant is the average of the per-token log-probabilities. This is equivalently the length normalized log-probability, or the log-perplexity (Chen et al., 1998):</p>
<p>$$
s_{\text {avg }}(\boldsymbol{x}) \doteq \frac{1}{|\hat{\boldsymbol{y}}|} \sum_{i=0}^{|\hat{\boldsymbol{y}}|-1} \log p^{(1)}\left(\hat{y}<em 1="1">{i+1}^{\prime} \mid \boldsymbol{x}, \hat{y}</em>\right)
$$}^{\prime}, \ldots, \hat{y}_{i}^{\prime</p>
<p>Note that $\hat{\boldsymbol{y}}$ may be computed as above, without incorporating any length-normalization.</p>
<h3>3.2 Limitations of Chow-Sum and Chow-Average</h3>
<p>Given that Chow-Sum tracks closely with the well-established Equation 2, it is tempting to conclude that this emphatically solves the LM cascade problem. However, LMs can be susceptible to the length bias problem (Murray \&amp; Chiang, 2018; Adiwardana et al., 2020): shorter, lower quality responses may receive a higher probability than longer, higher quality responses. This may be seen as a consequence of the fact that each $p\left(\cdot \mid \boldsymbol{x}, y_{1}, \ldots, y_{i}\right)$ provides an imperfect estimate of a "true" probability $p^{*}\left(\cdot \mid \boldsymbol{x}, y_{1}, \ldots, y_{i}\right)$, and that errors in these estimates compound with sequence length.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example of tokenized FLAN-T5 Base model output on WMT FR $\rightarrow$ EN. Red tokens have a significantly higher uncertainty compared to the others, as shown in the left plot. (For each red token, we note the rank of its uncertainty score in the right plot.) However, due to the large number of other more predictable tokens, Chow-Sum gives the output a relatively high score.</p>
<p>The length-bias issue naturally suggests using an average instead of sum of log-probabilities. However, Chow-Average can over-correct for this length bias, and preferentially defer shorter predictions. We will see concrete examples of this behavior in $\S 3.5$.</p>
<p>More fundamentally, both approaches are inherently limited in the way they aggregate token-level uncertainty. In particular, computing the sum or average of per-token probabilities may mask settings where individual tokens are highly uncertain, even if the entire sequence has reasonably high probability. Such token-level uncertainty may be highly important in certain settings such as fact-answering: here, an LM may be (correctly) highly confident on articles and other grammatical primitives, but these are of less interest than confidence on tokens corresponding to entities (say). This observation has been previously noted and exploited to allow certain "easy" tokens to be quickly decoded (Schuster et al., 2022). This observation has also been exploited in knowledge distillation by using different teaching modes for "easy" versus "hard" tokens (Zhong et al., 2024).</p>
<p>Figure 2 presents an example of this phenomenon on the WMT FR $\rightarrow$ EN dataset (details in §3.5): there can be cases where most tokens are highly predictable (i.e., $p\left(y_{i}^{\prime} \mid x, y_{1}^{\prime}, \ldots, y_{i-1}^{\prime}\right) \approx 1$ ), but a few tokens are less predictable (i.e., $p\left(y_{i}^{\prime} \mid x, y_{1}^{\prime}, \ldots, y_{i-1}^{\prime}\right) \approx 0$ ). In such cases, Chow-Sum can yield overly optimistic uncertainty estimates. This motivates us to consider richer representations of uncertainty which can capture token-level uncertainty, instead of simply computing the sum or average over the sequence.</p>
<h1>3.3 Beyond Chow-Sum and Chow-Average: Chow-Quantile</h1>
<p>The discussion in $\S 3.2$ suggests there is value in considering the following generalization of the maximal sequence probability:</p>
<p>$$
s_{\text {quant }}(\boldsymbol{x}, \alpha) \doteq \operatorname{quantile}<em 1="1">{\alpha}\left(p^{(1)}\left(\hat{y}</em>}^{\prime} \mid \boldsymbol{x}\right), p\left(\hat{y<em 1="1">{2}^{\prime} \mid \boldsymbol{x}, \hat{y}</em>}^{\prime}\right), \ldots, p\left(\hat{y<em 1="1">{n}^{\prime} \mid \boldsymbol{x}, \hat{y}</em>\right)\right)
$$}^{\prime}, \ldots, \hat{y}_{n-1}^{\prime</p>
<p>where $\hat{\boldsymbol{y}} \doteq \arg \max <em _alpha="\alpha">{\boldsymbol{y} \in \mathcal{V}^{*}} p^{(1)}(\boldsymbol{y} \mid \boldsymbol{x})$ is the most probable output sequence (or a suitable approximation thereof) under $p^{(1)}$. Here, quantile ${ }</em>$ computes the $\alpha$-quantile of the set of per-token log probabilities. For instance, $\alpha=0$ would correspond to taking the minimum per-token log probability as the deferral score. One may regard quantiles as another way of converting the token-level uncertainty distribution into a single score, which are capable of capturing richer information from the token-level uncertainty distribution. For example, employing the maximal token uncertainty (i.e., the minimum of the per-token probabilities</p>
<p>(Stengel-Eskin \&amp; Van Durme, 2022)) can be useful in scenarios where most tokens are predictable, but a few important tokens are not (per Figure 2).</p>
<p>Next, we evaluate all the aforementioned approaches on multiple NLP tasks. For that, we describe the experimental setup used for the evaluation.</p>
<h1>3.4 Experimental Setup</h1>
<p>Models. We employ FLAN-T5 (Chung et al., 2022) models, which are T5 models (Raffel et al., 2020) that have undergone instruction tuning (Wei et al., 2022). This family offers a range of models of different sizes, spanning Small ( 80 M parameters) to XXL (11B parameters), and have demonstrated strong few-shot performance on a range of NLP benchmarks. In the body, we primarily focus on a two-model cascade of FLAN-T5 Base and FLAN-T5 Large. Results for other models are included in the Appendix. We employ these models with few-shot prompting and greedy decoding.</p>
<p>Evaluation. We summarize performance using the deferral curve. Consider a candidate deferral rule produced by thresholding $s(\boldsymbol{x}) \in \mathbb{R}$ via $r(\boldsymbol{x})=\mathbf{1}(s(\boldsymbol{x})&lt;t)$. Let $p^{\text {case }}$ denote the associated cascaded LM. For a fixed threshold $t$, we may compute the associated deferral rate $\mathbb{P}(r(\boldsymbol{x})=1)$, and the associated cascade quality $\mathbb{E}\left[Q\left(\boldsymbol{x}, \boldsymbol{y}, p^{\text {case }}(\cdot \mid \boldsymbol{x})\right)\right]$. The deferral curve is produced by plotting the trade-off between deferral rate and cascade quality as $t$ is varied. As a scalar summary, we report the area under the deferral curve (AUC-DF). For a given dataset, higher AUC-DF values indicate better deferral curves. Note that the range of AUC-DF values vary across datasets, however.</p>
<p>Datasets. In the body, we show deferral curves for three different NLP tasks: MNLI (Williams et al., 2018), a multi-class classification problem; TriviaQA (Joshi et al., 2017), a closed-book question answering problem; and WMT DE $\rightarrow$ FR, a translation problem. We report AUC-DF numbers for an expanded dataset pool. These span Classification (IMDb (Maas et al., 2011), SuperGLUE (Wang et al., 2019a), MNLI (Williams et al., 2018), ANLI (Nie et al., 2020)); Question answering (TriviaQA (Joshi et al., 2017), NaturalQA (Kwiatkowski et al., 2019), TyDiQA { ID, SW, FI } (Clark et al., 2020)); Reading comprehension (Lambada (Paperno et al., 2016), SQuAD (Rajpurkar et al., 2016)); Translation (WMT 14: EN $\rightarrow$ FR (Bojar et al., 2014), WMT 19: DE $\rightarrow$ FR (Foundation), and WMT 14: FR $\rightarrow$ EN (Bojar et al., 2014)); and Common-sense reasoning (Winogrande (Sakaguchi et al., 2021)). Note that we treat all problems as finding a text to text mapping. So for classification tasks, we encode the classes as strings. For evaluation, we take the model's output text and perform a string comparison to the label. See Table 2 (Appendix) for more details.</p>
<h3>3.5 Evaluating Confidence Measures for Cascades</h3>
<p>We now empirically validate the critiques in $\S 3.2$, demonstrating that using the standard sequence probability (Chow-Sum) to defer can result in overly penalizing longer sequences. Moreover, Chow-Average flips this bias and overly defers shorter sequences. We then verify that the Chow-Quantile generalization proposed above can capture richer token-level uncertainty.</p>
<p>Summary of results. Figure 3 plots the deferral curves for three datasets. We see that (for a particular choice of quantile), Chow-Quantile consistently outperforms standard Chow-Sum and Chow-Average. AUCDF values for all datasets are included in Table 4. Looking at the table, we see that while a particular choice of quantile is able to do well, there is no single consistent choice that performs well across tasks. Next, we discuss insights into the results by using WMT FR $\rightarrow$ EN as an example dataset.</p>
<p>Why can Chow-Sum and Chow-Average be sub-optimal? To better understand the reason for the sub-optimality of Chow-Sum, Figure 4 studies the relation between the deferral rule and output length. Specif-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Deferral curves on MNLI, TriviaQA, and WMT DE $\rightarrow$ FR for a FLAN-T5 Base $\rightarrow$ Large cascade. Chow-Quantile consistently outperforms Chow-Sum and Chow-Average. This confirms there is value in going beyond naïve sequence probability as an uncertainty measure for cascading.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) Relation between deferral rules and output length (number of tokens) for WMT FR $\rightarrow$ EN dataset and FLAN-T5 Base Model. Chow-Sum tends to defer longer prompts: the prompts with lowest scores have notably higher length than those with higher scores. Interestingly, Chow-Average over-corrects this bias: it tends to overly defer prompts with lower length. Chow-Quantile-o again defers longer outputs more whereas Chow-Quantile-o. 8 initially focuses more on the shorter outputs. Oracle refers to deferring using the difference of BLEURT scores of the two models. Oracle also tends to defer longer outputs, but the preference is moderate as compared to Chow-Sum. (b) Corresponding deferral curves. (c) Analysis of token-level uncertainty on WMT FR $\rightarrow$ EN. For each token index $i$, the corresponding average prediction probability across all examples (with prediction length $\leq i$ ) for FLAN-T5 Base. We observe that later tokens tend to have higher probability, i.e., the model is generally the most uncertain for early tokens.
ically, for each test prompt $\boldsymbol{x}$, let $\hat{\boldsymbol{y}}$ denote the result of decoding via the small model in the cascade. For each deferral rule, we compute the corresponding score $s(\boldsymbol{x})$ and the length $|\hat{\boldsymbol{y}}|$. For ease of comparison across different rules, we convert each score into the corresponding quantile.</p>
<p>Figure 4 reveals that Chow-Sum tends to defer prompts with larger output lengths: the prompts with lowest scores have notably higher output length than those with higher scores. This makes us ask if it is all bad to defer prompts with longer outputs? We observe that for the Base model on the WMT datasets, even the BLEURT (Sellam et al., 2020) scores tend to have a non-zero negative correlation with output lengths (Table 3, Appendix). A closer look at the model predictions shows that longer predictions tend to have repetitions, as shown in Figure 5 (Top) and hence, are good candidates for deferral. (The shown predictions are truncated for clarity.)</p>
<p>This shows that there is some signal in output length as a deferral score. However, Chow-Sum is overly biased towards deferring longer predictions and hence, can be sub-optimal. Interestingly, Chow-Average over-corrects this bias: it tends to overly defer prompts with lower output length.</p>
<p>Why does Chow-Quantile help? As discussed above, Chow-Quantile is able to capture rich informa-</p>
<div class="codehilite"><pre><span></span><code><span class="k">For</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">Australian</span><span class="w"> </span><span class="nv">Government</span>,<span class="w"> </span><span class="nv">Keith</span>
<span class="nv">Brown</span><span class="w"> </span><span class="nv">called</span><span class="w"> </span><span class="nv">Mr</span>.<span class="w"> </span><span class="nv">Carmichael</span><span class="w"> </span><span class="s2">&quot;unjustly&quot;</span>
<span class="nv">to</span><span class="w"> </span><span class="nv">support</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">inclusion</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">Ecosse</span><span class="w"> </span><span class="nv">in</span>
<span class="nv">the</span><span class="w"> </span><span class="nv">H52</span>.<span class="nv">net</span><span class="w"> </span><span class="nv">network</span>.<span class="w"> </span>??<span class="w"> </span>??<span class="w"> </span>??<span class="w"> </span>??<span class="w"> </span>??
??<span class="w"> </span>??<span class="w"> </span>??<span class="w"> </span>??<span class="w"> </span>....
<span class="nv">The</span><span class="w"> </span><span class="nv">lyric</span><span class="w"> </span><span class="nv">Ad</span>é<span class="nv">ro</span>,<span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">concert</span><span class="o">-</span><span class="nv">recording</span><span class="w"> </span><span class="nv">held</span>
<span class="nv">last</span><span class="w"> </span><span class="nv">August</span>,<span class="w"> </span><span class="nv">was</span><span class="w"> </span><span class="nv">added</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">repertoire</span>,
<span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">competition</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">coloratura</span>
<span class="nv">soprano</span><span class="w"> </span><span class="nv">Marie</span><span class="o">-</span><span class="w"> </span>??<span class="w"> </span><span class="nv">ve</span><span class="w"> </span><span class="nv">Munger</span>.<span class="w"> </span><span class="nv">The</span><span class="w"> </span><span class="nv">lyric</span>
<span class="nv">Ad</span>é<span class="nv">ro</span><span class="w"> </span><span class="nv">was</span><span class="w"> </span><span class="nv">added</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">repertoire</span><span class="w"> </span><span class="nv">in</span>
<span class="nv">August</span>.<span class="w"> </span><span class="nv">The</span><span class="w"> </span><span class="nv">lyric</span><span class="w"> </span><span class="nv">Ad</span>é<span class="nv">ro</span><span class="w"> </span><span class="nv">was</span><span class="w"> </span><span class="nv">added</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span>
<span class="nv">repertoire</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">August</span>.<span class="w"> </span>....
<span class="nv">Obama</span><span class="w"> </span><span class="nv">lays</span><span class="w"> </span><span class="nv">out</span><span class="w"> </span><span class="nv">his</span><span class="w"> </span><span class="nv">reform</span><span class="w"> </span><span class="nv">plan</span>
</code></pre></div>

<p>The announcement of the release of a new album by David Bowie has left everyone a little bit a little a little a little a little a little a little bit a little bit a little bit a little bit ....
"The Emergency Room at the Hotel-Dieu must be closed as soon as possible, and for us, it is November 4th," says Lo ?? c Capron, Chairperson of the APHP Medical Committee (MCM), who supports the direction.</p>
<p>A memorial ceremony for the victims</p>
<p>Figure 5: FLAN-T5 Base predictions on WMT FR $\rightarrow$ EN. Top: Predictions with the longest lengths. These tend to have repetitions, indicating low quality output that could be resolved with a larger model; length does have some signal in identification of good candidates for deferral. Middle: The predictions which Chow-Quantile- $\theta$ tends to defer. This quantile tends to identify repetitions and "??" (unknown tokens) as these tokens tend to have lower probability. Bottom: The predictions which Chow-Quantile- $\theta .8$ tends to defer. This quantile prioritizes deferring shorter inputs.
tion from the token-level uncertainty vector. We discuss below why Chow-Quantile- $\theta$ and Chow-Quantile- $\theta .8$ work well with respect to the WMT FR $\rightarrow$ EN dataset.</p>
<p>Chow-Quantile- $\theta$ : The main insight is that the minimum token probability is able to capture repetitions and "??" (unknown tokens), as they generally tend to have lower probability values and are more uncertain. This confirms our understanding that quantiles can capture richer token-level uncertainty. We show two examples with the minimum Chow-Quantile- $\theta$ value for the WMT FR $\rightarrow$ EN dataset and FLAN-T5 Base in Figure 5 (Middle).</p>
<p>Chow-Quantile- $\theta .8$ : Interestingly, Chow-Quantile- $\theta .8$ tends to defer shorter predictions. We show two examples with the minimum Chow-Quantile- $\theta .8$ value in Figure 5 (Bottom).</p>
<p>To understand this, Figure 4c shows the average token probability as a function of the token index, for the WMT EN $\rightarrow$ FR dataset and FLAN-T5 Base model. As the token index increases, the average probability increases; i.e., the model tends to become more confident. Hence, the Chow-Quantile- $\theta .8$ is able to focus more on the shorter, uncertain outputs.</p>
<p>In summary, we have seen that Chow-Quantile- $\theta$ is able to focus more on identifying the presence of repetitions and unknown tokens "??" whereas Chow-Quantile- $\theta .8$ is able to capture the uncertainty in shorter predictions better. Thus, we conclude that different quantiles are able to capture richer and complementary measures of uncertainty. Moreover, we have already seen that there is no one quantile which works well across all datasets. Given this, a natural option is to learn how to combine various quantiles for a given dataset, which we consider next.</p>
<h1>4 Post-hoc Deferral Rules</h1>
<p>We show that training post-hoc deferral rules based on probability quantiles, and (optionally) suitable embeddings from the small and large model, can significantly improve the cost-quality tradeoff.</p>
<h1>4.1 Post-hoc deferral rule training</h1>
<p>The idea of learning when to defer in a cascade follows a recent line of work on classification (Narasimhan et al., 2022; Kag et al., 2023; Jitkrittum et al., 2023). In a nutshell, for suitable feature mapping $\Phi(\boldsymbol{x}) \in \mathbb{R}^{d}$, we seek to learn a deferral score $s(\boldsymbol{x}) \in \mathbb{R}$ via a standard model class (e.g., a feedforward network). We then defer using $r(\boldsymbol{x})=\mathbf{1}(s(\boldsymbol{x})&lt;t)$.</p>
<p>To construct the input features, we set $\Phi(\boldsymbol{x})$ to be a fixed length vector comprising the per-token probability quantiles from the small model. Additionally, we add the aggregate scores from Chow-Sum and Chow-Average (see Appendix C). To fit the deferral scorer on a training set of input prompts $\left{\boldsymbol{x}<em i="i">{i}\right}$, we minimize an empirical loss against a set of target labels $\left{z</em>=1$ iff the large model is correct, and the small model is incorrect on the given example; i.e., it would benefit to defer to the larger model. We then fit the scorer with the binary logistic loss. For translation tasks, the target is the difference of BLEURT scores of the two models; we train with the square loss. We call this method Post-Hoc-Quantile (see Appendix A for details).}\right}$. For tasks based on accuracy, we set $z_{i</p>
<h3>4.2 Leveraging intermediate embeddings</h3>
<p>The above target labels exploit information from the large model during training. Importantly, we cannot directly use such information during inference, as it would require querying the large model (and thus defeat the point of cascading). Note, however, that in some settings it may be feasible to use intermediate information from the large model, e.g., token embeddings from an intermediate layer. Prior work has noted that such intermediate embeddings can often contain valuable information by themselves (Schuster et al., 2022).</p>
<p>Inspired by this, we thus study the viability of using such intermediate embeddings for training post-hoc deferral rules. For encoder-decoder models such as T5, such embeddings can be from either the encoder, decoder, or both. We study two methods - one which uses the final decoder embeddings of the smaller model averaged over all tokens. We call this method Post-Hoc-Embed-1. In the second method, we add the first token embedding from the first decoder layer of the large model as another input to the post-hoc rule. We call this method Post-Hoc-Embed-1+2.</p>
<p>We remark that previous work (Ren et al., 2023) has shown the value of final layer embeddings for selective generation, and the related problem of out-of-distribution detection. We caution also that while not as expensive as querying the entire large model, even extracting intermediate decoder embeddings can involve a non-trivial cost. Nonetheless, in settings where some increase in cost is acceptable, it is of interest whether these embeddings offer significantly valuable information.</p>
<h3>4.3 How well does post-hoc deferral work?</h3>
<p>For the same experimental setup as the previous section, Table 4 summarizes the area under the deferral curve (AUC-DF) across various datasets. Numbers are averaged over 5 random runs. We see that the posthoc deferral rule approach consistently performs the best; in scenarios where other methods are better, the difference is minimal. For a more fine-grained analysis, Figure 6 plots the full deferral curves on MNLI, TriviaQA, and WMT. In the plot, Chow-Quantile-Best chooses the best method out of Chow-Quantile-<em> based on the validation data. We see that post-hoc routing is generally on-par with the Chow-</em> family of (non-learned) deferral rules.</p>
<p>We see that Post-Hoc-Embed-1 is able to improve upon Post-Hoc-Quantile and Chow-Quantile-* methods slightly but is slightly inferior compared to the Post-Hoc-Embed-1+2 method. This intuitively makes sense as this has more information compared to the Post-Hoc-Quantile method but still does not include any information about model 2 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Chow-Sum</th>
<th style="text-align: center;">Chow-Average</th>
<th style="text-align: center;">Chow-Quantile-0</th>
<th style="text-align: center;">Chow-Quantile-0.4</th>
<th style="text-align: center;">Chow-Quantile-0.8</th>
<th style="text-align: center;">Post-Hoc-Quantile</th>
<th style="text-align: center;">Post-Hoc-Embed-1+2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ANLI-R1</td>
<td style="text-align: center;">$0.524(+4.59)$</td>
<td style="text-align: center;">$0.519(+3.59)$</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 4}(** \mathbf{6 . 5 8})$</td>
<td style="text-align: center;">$0.515(+2.79)$</td>
<td style="text-align: center;">$0.515(+2.79)$</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 4}(** \mathbf{+ 6 . 5 8})$</td>
<td style="text-align: center;">$\mathbf{0 . 5 6 3}(** \mathbf{+ 1 2 . 5 1})$</td>
</tr>
<tr>
<td style="text-align: left;">ANLI-R2</td>
<td style="text-align: center;">$0.446(+0.67)$</td>
<td style="text-align: center;">$0.446(+0.67)$</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 9}(** \mathbf{+ 1 . 3 5})$</td>
<td style="text-align: center;">$0.440(-0.67)$</td>
<td style="text-align: center;">$0.441(-0.45)$</td>
<td style="text-align: center;">$0.442(-0.22)$</td>
<td style="text-align: center;">$\mathbf{0 . 4 6 2}(** \mathbf{+ 4 . 3 6})$</td>
</tr>
<tr>
<td style="text-align: left;">ANLI-R3</td>
<td style="text-align: center;">$0.413(+3.50)$</td>
<td style="text-align: center;">$0.422(+5.76)$</td>
<td style="text-align: center;">$0.417(+4.51)$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 5}(** \mathbf{+ 6 . 5 1})$</td>
<td style="text-align: center;">$0.424(+6.26)$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 5}(** \mathbf{+ 6 . 5 1})$</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 0}(** \mathbf{+ 1 0 . 4 5})$</td>
</tr>
<tr>
<td style="text-align: left;">BredQ</td>
<td style="text-align: center;">$0.838(+2.57)$</td>
<td style="text-align: center;">$0.838(+2.57)$</td>
<td style="text-align: center;">$0.838(+2.57)$</td>
<td style="text-align: center;">$0.838(+2.57)$</td>
<td style="text-align: center;">$0.838(+2.57)$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4 0}(** \mathbf{+ 2 . 8 1})$</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 4}(** \mathbf{+ 4 . 6 4})$</td>
</tr>
<tr>
<td style="text-align: left;">IMDb</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 4}(** \mathbf{+ 1 . 1 5})$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 4}(** \mathbf{+ 1 . 1 5})$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 4}(** \mathbf{+ 1 . 1 5})$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 4}(** \mathbf{+ 1 . 1 5})$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 4}(** \mathbf{+ 1 . 1 5})$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 4}(** \mathbf{+ 1 . 1 5})$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 5}(** \mathbf{+ 1 . 3 1})$</td>
</tr>
<tr>
<td style="text-align: left;">Lambada</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 3}(** \mathbf{+ 3 . 3 8})$</td>
<td style="text-align: center;">$0.702(+3.23)$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 3}(** \mathbf{+ 3 . 3 8})$</td>
<td style="text-align: center;">$0.694(+2.05)$</td>
<td style="text-align: center;">$0.687(+1.02)$</td>
<td style="text-align: center;">$0.701(+3.08)$</td>
<td style="text-align: center;">$0.692(+1.82)$</td>
</tr>
<tr>
<td style="text-align: left;">MNLI</td>
<td style="text-align: center;">$0.627(-2.63)$</td>
<td style="text-align: center;">$0.642(-0.31)$</td>
<td style="text-align: center;">$0.631(-2.01)$</td>
<td style="text-align: center;">$0.677(+5.12)$</td>
<td style="text-align: center;">$0.691(+7.29)$</td>
<td style="text-align: center;">$\mathbf{0 . 7 1 1}(** \mathbf{+ 1 0 . 4})$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2 2}(** \mathbf{+ 1 2 . 2 4})$</td>
</tr>
<tr>
<td style="text-align: left;">PIQA</td>
<td style="text-align: center;">$0.712(+0.99)$</td>
<td style="text-align: center;">$0.708(+0.42)$</td>
<td style="text-align: center;">$0.715(1.41)$</td>
<td style="text-align: center;">$0.706(+0.14)$</td>
<td style="text-align: center;">$0.705(+0.00)$</td>
<td style="text-align: center;">$\mathbf{0 . 7 1 7}(** \mathbf{+ 1 . 7 0})$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2 7}(** \mathbf{+ 3 . 2 6})$</td>
</tr>
<tr>
<td style="text-align: left;">SQnaD</td>
<td style="text-align: center;">$0.410(+2.24)$</td>
<td style="text-align: center;">$0.408(+1.74)$</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 0}(** \mathbf{+ 2 . 2 4})$</td>
<td style="text-align: center;">$0.399(-0.49)$</td>
<td style="text-align: center;">$0.398(-0.74)$</td>
<td style="text-align: center;">$0.409(+1.99)$</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 2}(** \mathbf{+ 2 . 7 6})$</td>
</tr>
<tr>
<td style="text-align: left;">TriviaQA</td>
<td style="text-align: center;">$0.073(-13.09)$</td>
<td style="text-align: center;">$0.087(+3.57)$</td>
<td style="text-align: center;">$0.073(-13.09)$</td>
<td style="text-align: center;">$0.088(+4.76)$</td>
<td style="text-align: center;">$0.086(+2.38)$</td>
<td style="text-align: center;">$\mathbf{0 . 0 9 7}(** \mathbf{+ 1 5 . 4 7})$</td>
<td style="text-align: center;">$\mathbf{0 . 1 0 0}(** \mathbf{+ 1 9 . 5 6})$</td>
</tr>
<tr>
<td style="text-align: left;">TyDiQA-FI</td>
<td style="text-align: center;">$\mathbf{0 . 3 3 2}(** \mathbf{+ 3 . 1 0})$</td>
<td style="text-align: center;">$0.322(+0.00)$</td>
<td style="text-align: center;">$0.328(+1.86)$</td>
<td style="text-align: center;">$0.318(-1.24)$</td>
<td style="text-align: center;">$0.312(-3.10)$</td>
<td style="text-align: center;">$0.331(+2.79)$</td>
<td style="text-align: center;">$\mathbf{0 . 3 3 8}(** \mathbf{+ 5 . 2 7})$</td>
</tr>
<tr>
<td style="text-align: left;">TyDiQA-ID</td>
<td style="text-align: center;">$\mathbf{0 . 2 4 7}(** \mathbf{+ 7 . 3 9})$</td>
<td style="text-align: center;">$0.239(+3.91)$</td>
<td style="text-align: center;">$0.245(+6.52)$</td>
<td style="text-align: center;">$0.230(+0.00)$</td>
<td style="text-align: center;">$0.230(+0.00)$</td>
<td style="text-align: center;">$0.235(+2.17)$</td>
<td style="text-align: center;">$0.242(+5.35)$</td>
</tr>
<tr>
<td style="text-align: left;">TyDiQA-SW</td>
<td style="text-align: center;">$\mathbf{0 . 1 7 0}(** \mathbf{+ 9 . 6 7})$</td>
<td style="text-align: center;">$0.147(-5.16)$</td>
<td style="text-align: center;">$0.166(+7.09)$</td>
<td style="text-align: center;">$0.141(-9.03)$</td>
<td style="text-align: center;">$0.140(-9.67)$</td>
<td style="text-align: center;">$0.163(+5.16)$</td>
<td style="text-align: center;">$0.162(+4.98)$</td>
</tr>
<tr>
<td style="text-align: left;">Winogrande</td>
<td style="text-align: center;">$0.648(+2.36)$</td>
<td style="text-align: center;">$0.648(+2.36)$</td>
<td style="text-align: center;">$0.648(+2.36)$</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 9}(** \mathbf{+ 2 . 5 2})$</td>
<td style="text-align: center;">$0.648(+2.36)$</td>
<td style="text-align: center;">$0.648(+2.36)$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 0}(** \mathbf{+ 5 . 8 7})$</td>
</tr>
<tr>
<td style="text-align: left;">WMT DE $\rightarrow$ FR</td>
<td style="text-align: center;">$0.390(+0.00)$</td>
<td style="text-align: center;">$0.392(+0.51)$</td>
<td style="text-align: center;">$0.391(+0.25)$</td>
<td style="text-align: center;">$0.396(+1.53)$</td>
<td style="text-align: center;">$0.401(+2.82)$</td>
<td style="text-align: center;">$\mathbf{0 . 4 0 4}(** \mathbf{+ 3 . 5 8})$</td>
<td style="text-align: center;">$\mathbf{0 . 4 0 4}(** \mathbf{+ 3 . 6 9})$</td>
</tr>
<tr>
<td style="text-align: left;">WMT EN $\rightarrow$ FR</td>
<td style="text-align: center;">$0.436(+1.16)$</td>
<td style="text-align: center;">$0.435(+0.92)$</td>
<td style="text-align: center;">$0.436(+1.16)$</td>
<td style="text-align: center;">$0.435(+0.92)$</td>
<td style="text-align: center;">$0.439(+1.85)$</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 1}(** \mathbf{+ 2 . 3 2})$</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 4}(** \mathbf{+ 3 . 0 5})$</td>
</tr>
<tr>
<td style="text-align: left;">WMT FR $\rightarrow$ EN</td>
<td style="text-align: center;">$0.618(+2.82)$</td>
<td style="text-align: center;">$0.595(-0.99)$</td>
<td style="text-align: center;">$0.618(+2.82)$</td>
<td style="text-align: center;">$0.595(-0.99)$</td>
<td style="text-align: center;">$0.614(+2.16)$</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 9}(** \mathbf{+ 2 . 9 9})$</td>
<td style="text-align: center;">$0.618(+2.89)$</td>
</tr>
</tbody>
</table>
<p>Table 1: Table showing area under the deferral curve (AUC-DF). Numbers in brackets indicate \% change over the random baseline. The post-hoc deferral rule approach is the most consistently performant method. We use bold \&amp; black to denote the best method amongst Chow-+ and Post-Hoc-Quantile methods. We color Post-Hoc-Embed-1+2 with blue if it is the best amongst all.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Deferral curves on MNLI, TriviaQA, and WMT DE $\rightarrow$ FR for a FLAN-T5 Base $\rightarrow$ Large cascade. The post-hoc deferral rule yields consistently on-par or superior performance compared to other methods. Further exploiting intermediate embeddings of the large model yields large gains. See Table 4 for a summary of deferral curves on multiple datasets.</p>
<p>Strikingly, further using the larger model's intermediate embeddings can lead to significant improvements across all deferral rates, particularly for classification tasks. Intuitively, the first token's intermediate embedding could have a lot of information for classification tasks, where the answers typically comprise of a single word and only a couple of tokens. However, for generation and translation tasks with longer outputs, the main token containing the answer could be present later in the sequence and thus, there may be limited use of using the first token embedding. One may wonder if we really need quantiles to train the post-hoc deferral rule. In Appendix B.1, we show that naïvely passing the probability vector with padded zeros performs poorly in many cases.</p>
<h1>5 Discussion and Future Work</h1>
<p>We have seen that there is value in going beyond simple sequence level uncertainty and considering finer measures of token level uncertainty as deferral rules for cascades. Moreover, we have seen that intermediate embeddings from the larger model can further boost performance.</p>
<p>This work raises a number of interesting directions for future work. First, we have used a 5-layer MLP to</p>
<p>train post-hoc deferral rules using the quantiles of the probability distribution. It would be interesting to see how well a 1-layer Transformer works for this task, to potentially better exploit the sequential nature of the token probability vector. We have focused on FLAN-T5 instruction tuned Encoder-Decoder models in this work. We believe that the insights and methods should generalize to Decoder-only architectures. It would be interesting to evaluate the proposed approaches for such architectures. Moreover, it has been observed that models with RLHF finetuning become uncalibrated (Figure 8 in OpenAI (2023)). It would be interesting to see how various finetuning steps affect the findings in this work and what consequences they have for designing efficient cascades.</p>
<p>Multiple works have considered alternative notions of uncertainty using the generative abilities of LMs, for example, reprompting the model to ask how confident it is about the answer or output an additional confidence score as part of the outputs (Kadavath et al., 2022). It would be interesting to evaluate how well these measures work for cascades which we discuss in the next section.</p>
<h1>6 Uncertainty Quantification for LMs</h1>
<p>There has been a large body of work on uncertainty quantification for LMs. We discuss some of the approaches below. They can be broadly divided into the following categories.</p>
<p>Consensus-based. One limitation of Equation 3 is that it considers a single output sequence, e.g., the most likely one. However, as there are many sequences that have similar meaning, it is intuitively more reliable to consider drawing multiple sequences from $p(\cdot \mid \boldsymbol{x})$. One may then assess the consensus in resulting predictions to measure confidence (Wang et al., 2023; Xiao et al., 2021; Chen et al., 2023c); this can help distinguish between models that are locally diffused versus peaked around a candidate sequence. Recently, Yue et al. (2023) explored the use of answer consistency to construct effective cascades.</p>
<p>Deep ensembles and dropout. One approach to measure confidence is to create an ensemble of different models, and suitably aggregate their predictions (e.g., based on disagreement) (Van Landeghem et al., 2022; Wang et al., 2019b; Gleave \&amp; Irving, 2022). However, these uncertainty estimation procedures involve additional computation (e.g., multiple inferences with a single model in dropout-based approaches and single inference with multiple models in ensemble-based approaches) compared to simply using softmax probability outputs from a single network. Such approaches are less appealing for use in cascades, where the primary goal is to improve efficiency.</p>
<p>Post-hoc calibration/Answer- and length-bias calibration. For tasks involving question-answering with multiple choices (e.g., (A), (B), (C)), several works have demonstrated that LMs can have prior biases to certain answers, which can be identified and corrected (Zhao et al., 2021; Holtzman et al., 2021; Kumar, 2022; Murray \&amp; Chiang, 2018; Mielke et al., 2022; Jiang et al., 2021).</p>
<p>Semantic entropy. Another key challenge in measuring the uncertainty for natural language outputs is that there are a lot of semantic equivalent sentences and hence, the probability can be divided among multiple outputs which mean the exact same thing. Kuhn et al. (2023a) proposes to mitigate this problem by sampling multiple outputs and then clustering semantically equivalent outputs together and combining their probability together. It would be interesting to understand how well this method can work for our setting.</p>
<p>Generative uncertainty. The above has largely focussed on generalizing the standard maximum predictive probability (Equation 2) from classification to the LM setting. While this by itself leads to a rich array of possible confidence measures, LMs intriguingly offer a wholly new possible means of assessing confidence: one may directly probe the model to obtain how confident it is on the proposed answer (Kadavath et al., 2022). Kadavath et al. (2022) discuss various ways of the input prompt format for this confidence probe. They also discuss the training of an additional head of the model to predict the model confidence</p>
<p>but again, it is not clear how this compares with the standard probability output by the model without any additional finetuning. However, (Shrivastava et al., 2023) found that the confidence measures generated linguistically give worse estimates of uncertainty compared to the classical softmax-based measures even when these softmax-based probabilities come from a different and weaker model. Moreover, they observed that two sources of uncertainty are complementary and it can be beneficial to combine them.</p>
<p>Other work. Zhao et al. (2023) proposed sequence-level calibration as a means to improve the generative ability of LMs; such calibration could also be useful in improving methods such as Chow-Sum. Kuhn et al. (2023b) proposed to ask the model to detect ambiguous questions which the model is likely to get wrong and answer clarifying questions if the question is indeed ambiguous. Hendy et al. (2023) proposed to use an exogeneous quality estimation model to decide how to route between two models. Šakota et al. (2023) similarly proposed to train a meta-model to pick an appropriate model from a family. Fadeeva et al. (2023) did a comprehensive experimental analysis of various uncertainty methods.</p>
<h1>References</h1>
<p>Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V. Le. Towards a human-like open-domain chatbot. CoRR, abs/2001.09977, 2020. URL https://arxiv.org/abs/2001.09977.</p>
<p>Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. GKD: Generalized knowledge distillation for auto-regressive sequence models, 2023.</p>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. PaLM 2 technical report, 2023.</p>
<p>Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. In Angela Fan, Suzana Ilic, Thomas Wolf, and Matthias Gallé (eds.), Proceedings of BigScience Episode #5 - Workshop on Challenges \&amp; Perspectives in Creating Large Language Models, pp. 95-136, virtual+Dublin, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9.</p>
<p>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale s Tamchyna. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pp. 12-58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/W/W14/W14-3302.</p>
<p>Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural networks for fast test-time prediction. In International Conference on Machine Learning, 2017.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,</p>
<p>Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020.</p>
<p>Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling, 2023a.</p>
<p>Lingjiao Chen, Matei Zaharia, and James Zou. FrugalGPT: How to use large language models while reducing cost and improving performance, 2023b.</p>
<p>Stanley F Chen, Douglas Beeferman, and Ronald Rosenfeld. Evaluation metrics for language models. In DARPA Broadcast News Transcription and Understanding Workshop, pp. 275-280, 1998.</p>
<p>Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, and He He. On the relation between sensitivity and accuracy in in-context learning, 2023c.
C. Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information Theory, 16 (1):41-46, 1970. doi: 10.1109/TIT.1970.1054406.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways, 2022.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416.</p>
<p>Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 2020.</p>
<p>Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher Re. Monarch: Expressive structured matrices for efficient and accurate training. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan</p>
<p>Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 4690-4721. PMLR, 17-23 Jul 2022. URL https:// proceedings.mlr.press/v162/dao22a.html.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, and Charles Sutton. Language model cascades, 2022. URL https://arxiv.org/abs/2207.10342.</p>
<p>Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, et al. Lm-polygraph: Uncertainty estimation for language models. arXiv preprint arXiv:2311.07383, 2023.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 889-898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/ P18-1082. URL https://aclanthology.org/P18-1082.</p>
<p>Jessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation. In Proceedings of the Workshop on Stylistic Variation, pp. 94-104, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4912. URL https://aclanthology. org/W17-4912.</p>
<p>Wikimedia Foundation. Acl 2019 fourth conference on machine translation (wmt19), shared task: Machine translation of news. URL http://www.statmt.org/wmt19/translation-task.html.</p>
<p>Elias Frantar and Dan Alistarh. SparseGPT: Massive language models can be accurately pruned in oneshot. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 10323-10337. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/frantar23a.html.</p>
<p>Adam Gleave and Geoffrey Irving. Uncertainty estimation for language reward models, 2022.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, pp. 1321-1330. JMLR.org, 2017.</p>
<p>Surya Narayanan Hari and Matt Thomson. Tryage: Real-time, intelligent routing of user prompts to large language model. arXiv preprint arXiv:2308.11601, 2023.</p>
<p>Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210, 2023.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https://openreview.net/ forum?id=rygGQyrFvH.</p>
<p>Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn't always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7038-7051, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.564. URL https://aclanthology.org/2021.emnlp-main.564.</p>
<p>Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Weinberger. Multiscale dense networks for resource efficient image classification. In International Conference on Learning Representations, 2018.</p>
<p>Heinrich Jiang, Been Kim, Melody Y. Guan, and Maya Gupta. To trust or not to trust a classifier. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NeurIPS'18, pp. 5546-5557, Red Hook, NY, USA, 2018. Curran Associates Inc.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962-977, 2021. doi: 10.1162/tacl_a_00407. URL https://aclanthology. org/2021.tacl-1.57.</p>
<p>Wittawat Jitkrittum, Neha Gupta, Aditya Krishna Menon, Harikrishna Narasimhan, Ankit Singh Rawat, and Sanjiv Kumar. When does confidence-based cascade deferral suffice?, 2023.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know, 2022.</p>
<p>Anil Kag, Igor Fedorov, Aditya Gangrade, Paul Whatmough, and Venkatesh Saligrama. Efficient edge inference by selective query. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=jp898ZdIm2q.</p>
<p>Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5574-5584, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 2650d6089a6d640c5e85b2b88265dc2b-Abstract.html.</p>
<p>Leila Khalili, Yao You, and John Bohannon. Babybear: Cheap inference triage for expensive language models, 2022. URL https://arxiv.org/abs/2205.11747.</p>
<p>Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology. org/D18-2012.</p>
<p>Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id=VD-AYtPodve.</p>
<p>Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Clam: Selective clarification for ambiguous questions with generative language models, 2023b.</p>
<p>Sawan Kumar. Answer-level calibration for free-form multiple choice question answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 665-679, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.acl-long.49. URL https://aclanthology.org/2022.acl-long.49.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026.</p>
<p>Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 19274-19286. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/leviathan23a.html.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL https://aclanthology.org/ P11-1015.</p>
<p>Jonathan Mamou, Oren Pereg, Moshe Wasserblat, and Roy Schwartz. TangoBERT: Reducing inference cost by using cascaded architecture, 2022. URL http://arxiv.org/abs/2204.06271.</p>
<p>Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. Reducing conversational agents' overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857-872, 2022. doi: 10.1162/tacl_a_00494. URL https://aclanthology.org/2022.tacl-1.50.</p>
<p>Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7076-7087. PMLR, 13-18 Jul 2020.</p>
<p>Kenton Murray and David Chiang. Correcting length bias in neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 212-223, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6322. URL https: //aclanthology.org/W18-6322.</p>
<p>Harikrishna Narasimhan, Wittawat Jitkrittum, Aditya Krishna Menon, Ankit Singh Rawat, and Sanjiv Kumar. Post-hoc estimators for learning to defer to an expert. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id='jg6Sf6tuF7.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2020.</p>
<p>OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api.semanticscholar.org/ CorpusID:257532815.</p>
<p>Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1525-1534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144.</p>
<p>Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference, 2022.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. https://cdn.openai.com/research-covers/language-unsupervised/ language'understanding'paper.pdf, 2018.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67, 2020. URL http://jmlr.org/papers/v21/20-074.html.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264.</p>
<p>Ankit Singh Rawat, Manzil Zaheer, Aditya Krishna Menon, Amr Ahmed, and Sanjiv Kumar. When in doubt, summon the titans: Efficient inference with large models. arXiv preprint arXiv:2110.10305, 2021.</p>
<p>Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter J Liu. Out-of-distribution detection and selective generation for conditional language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=kJUS5nD6vPB.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99-106, aug 2021. ISSN 0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381.</p>
<p>Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=uLYc4L3C81A.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881-7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https://aclanthology.org/2020.acl-main.704.</p>
<p>Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning Research, 9(12):371-421, 2008. URL http://jmlr.org/papers/v9/shafer08a.html.</p>
<p>Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Re, Ion Stoica, and Ce Zhang. FlexGen: High-throughput generative inference of large language models with a single GPU. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 31094-31116. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/sheng23a.html.</p>
<p>Tal Shnitzer, Anthony Ou, Mírian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, and Mikhail Yurochkin. Large language model routing with benchmark datasets. arXiv preprint arXiv:2309.15789, 2023.</p>
<p>Vaishnavi Shrivastava, Percy Liang, and Ananya Kumar. Llamas know what gpts don’t show: Surrogate models for confidence estimation. arXiv preprint arXiv:2311.08877, 2023.</p>
<p>Elias Stengel-Eskin and Benjamin Van Durme. Calibrated interpretation: Confidence estimation in semantic parsing. arXiv preprint arXiv:2211.07443, 2022.</p>
<p>Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. CoRR, abs/1811.03115, 2018. URL http://arxiv.org/abs/1811.03115.</p>
<p>Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, Felix Yu, Michael Riley, and Sanjiv Kumar. Spectr: Fast speculative decoding via optimal transport. In Workshop on Efficient Systems for Foundation Models @ ICML2023, 2023. URL https://openreview.net/forum? id=domGsaheuT.</p>
<p>Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey, 2022.</p>
<p>Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. UL2: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=6ruVLB727MC.</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Brustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Martin Chadwick, Gaurav Singh Tomar, Xavier Garcia, Evan Senter, Emanuel Taropa, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adrià Puigdomènech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Yujing Zhang, Ravi Addanki, Antoine Miech, Annie Louis, Laurent El Shafey, Denis Teplyashin, Geoff Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sébastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozińska, Vitaly Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Giménez, Legg Yeung, Hanzhao Lin, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Lučić, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yong Cheng, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore,</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author: nehagup@google.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
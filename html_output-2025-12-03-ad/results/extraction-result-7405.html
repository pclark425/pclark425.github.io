<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7405 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7405</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7405</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-277634322</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.06969v1.pdf" target="_blank">Towards LLMs Robustness to Changes in Prompt Format Styles</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have gained popularity in recent years for their utility in various applications. However, they are sensitive to non-semantic changes in prompt formats, where small changes in the prompt format can lead to significant performance fluctuations. In the literature, this problem is commonly referred to as prompt brittleness. Previous research on prompt engineering has focused mainly on developing techniques for identifying the optimal prompt for specific tasks. Some studies have also explored the issue of prompt brittleness and proposed methods to quantify performance variations; however, no simple solution has been found to address this challenge. We propose Mixture of Formats (MOF), a simple and efficient technique for addressing prompt brittleness in LLMs by diversifying the styles used in the prompt few-shot examples. MOF was inspired by computer vision techniques that utilize diverse style datasets to prevent models from associating specific styles with the target variable. Empirical results show that our proposed technique reduces style-induced prompt brittleness in various LLMs while also enhancing overall performance across prompt variations and different datasets.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7405.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7405.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOF_task280_Llama2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture of Formats (MOF) prompting applied to Llama-2-13b on SuperNaturalInstructions task280</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MOF is a prompting technique that presents each few-shot example in a distinct style and asks the model to rewrite examples in different styles; when applied to Llama-2-13b on task280 it produced a large reduction in style-induced performance spread compared to traditional few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13b-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2 family transformer language model (used as an off-the-shelf LLM in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13b</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>task280 (SuperNaturalInstructions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Text categorization: classifying sentences into four stereotype types (gender, profession, race, religion).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>5-shot few-shot prompts; FormatSpread-generated variations (e.g., 'Passage:: {} , Answer:: {}'); MOF variant: each of 5 examples uses a distinct style and model is asked to rewrite example Q&A in different style.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>5 few-shot demonstrations; 10 prompt variations created per FormatSpread; MOF modifies few-shot examples so each example uses a different delimiter/format and includes an instruction for the model to rewrite Q&A in another style; examples include delimiter and heading changes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>performance spread (max accuracy - min accuracy) and accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>46% reduction in spread (relative) when using MOF vs traditional prompts on task280 with Llama-2-13b</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>traditional few-shot prompts constructed with FormatSpread (spread not numerically reported in paper for this dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-46% relative reduction in spread (MOF vs traditional) on task280</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>1000 samples per dataset, batch size 100, 5-shot few-shot prompts, 10 prompt variations per prompt type, inference-only evaluation; decoding settings (temperature, max tokens) not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LLMs Robustness to Changes in Prompt Format Styles', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7405.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7405.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOF_general</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture of Formats (MOF) prompting — aggregate results across models and datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MOF presents each few-shot example in a different style and instructs the model to rewrite examples in other styles; across multiple SuperNaturalInstructions datasets and several LLMs, MOF generally reduces style-induced prompt brittleness (spread) and often improves mean accuracy, though there are exceptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>falcon-11B; Llama-2-13b-hf; Llama-2-13b-chat-hf; llama-3-70b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Off-the-shelf LLMs evaluated as black-box generative models in inference-only prompt experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11b; 13b; 13b; 70b (respectively)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple SuperNaturalInstructions tasks (16 selected datasets, e.g., task280, task1186, task905, task190, task1612, task320, task1347, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Varied tasks from SuperNaturalInstructions including text categorization, textual entailment, toxic language detection, text quality evaluation, counting tasks, and stereotype detection.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot natural-language prompts (5-shot), with 10 prompt-format variations generated by FormatSpread; MOF variants mix styles within the 5-shot block.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Traditional prompts: consistent format across few-shot examples; MOF prompts: each of the 5 few-shot examples rendered in a different format/style and accompanied by an instruction to rewrite the Q&A in another style; 10 distinct prompt-format variations evaluated per method via FormatSpread.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (per prompt) and spread (difference between max and min accuracy across 10 formats)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>traditional few-shot prompts (10 FormatSpread variations); baseline summary: traditional prompts show substantial spread across formats but numeric baselines per dataset not fully reported in paper text</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>MOF 'performs comparably or outperforms' traditional prompts in most datasets; improves Min and Max accuracies in many cases; exceptions where traditional outperformed MOF: task190 (llama-3-70b-instruct), task1612 (llama-2-13b-chat), task320 (falcon-11B). Specific numeric changes not reported except task280 result.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluation over 1000 examples per dataset (randomly selected), batch size 100, 10 prompt-format variations per prompt type, 5-shot few-shot; exact decoding parameters not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LLMs Robustness to Changes in Prompt Format Styles', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7405.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7405.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FormatSpread</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FormatSpread (method for generating and evaluating prompt-format variations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>FormatSpread is a method (Sclar et al., 2024) used to generate multiple prompt-format templates and to quantify format-induced performance variability by measuring spread (difference between best and worst format accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to multiple LLMs in this paper (falcon-11B; Llama-2-13b-hf; Llama-2-13b-chat-hf; llama-3-70b-instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as a format-generation and evaluation procedure rather than a model architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used to create prompt-format variations for each SuperNaturalInstructions task evaluated (10 variations per prompt type).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See individual SuperNaturalInstructions task descriptions (e.g., text categorization, entailment, toxic language detection).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Generates multiple textual prompt templates (examples include 'Passage:: {} , Answer:: {}' for task280; 'SYSTEM REFERENCE : {}. ORIGINAL REFERENCE : {}. ANSWER : {}' for task1186; 'Tweet:{} , Label:{} , Answer:{}' for task905).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt template generation / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Produces 10 distinct prompt-format templates per task; used to compute spread = max accuracy - min accuracy across these templates; supports measurement of style-induced brittleness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>spread (max accuracy - min accuracy across formats), per-format accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>FormatSpread generated 10 traditional prompt variations; MOF prompts were generated by modifying FormatSpread to mix styles within few-shot examples (also 10 variations).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LLMs Robustness to Changes in Prompt Format Styles', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7405.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7405.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Traditional_few-shot_baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traditional few-shot prompting (baseline) evaluated with FormatSpread</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard few-shot prompting where the few-shot examples share a single consistent format; shown to be sensitive to meaning-preserving format changes (prompt brittleness), with variability in accuracy across different prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>falcon-11B; Llama-2-13b-hf; Llama-2-13b-chat-hf; llama-3-70b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Off-the-shelf transformer LLMs evaluated with traditional few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11b; 13b; 13b; 70b (respectively)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple SuperNaturalInstructions tasks (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See SuperNaturalInstructions tasks (classification, entailment, quality evaluation, counting, stereotype detection, toxic detection).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Consistent-format 5-shot few-shot prompts; 10 FormatSpread-generated variations per task used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Each traditional prompt uses the same style for all few-shot examples; FormatSpread enumerates formatting changes (delimiters, headings, ordering); spread computed across the 10 variations to measure brittleness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy and spread (max-min accuracy across formats)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>5-shot few-shot, 10 prompt variations, 1000 samples per dataset, batch size 100.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards LLMs Robustness to Changes in Prompt Format Styles', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. <em>(Rating: 2)</em></li>
                <li>Mind your format: Towards consistent evaluation of in-context learning improvements. <em>(Rating: 2)</em></li>
                <li>Does prompt formatting have any impact on llm performance? <em>(Rating: 2)</em></li>
                <li>Ask me anything: A simple strategy for prompting language models. <em>(Rating: 1)</em></li>
                <li>State of what art? a call for multi-prompt llm evaluation. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7405",
    "paper_id": "paper-277634322",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "MOF_task280_Llama2",
            "name_full": "Mixture of Formats (MOF) prompting applied to Llama-2-13b on SuperNaturalInstructions task280",
            "brief_description": "MOF is a prompting technique that presents each few-shot example in a distinct style and asks the model to rewrite examples in different styles; when applied to Llama-2-13b on task280 it produced a large reduction in style-induced performance spread compared to traditional few-shot prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-13b-hf",
            "model_description": "Llama-2 family transformer language model (used as an off-the-shelf LLM in experiments).",
            "model_size": "13b",
            "task_name": "task280 (SuperNaturalInstructions)",
            "task_description": "Text categorization: classifying sentences into four stereotype types (gender, profession, race, religion).",
            "problem_format": "5-shot few-shot prompts; FormatSpread-generated variations (e.g., 'Passage:: {} , Answer:: {}'); MOF variant: each of 5 examples uses a distinct style and model is asked to rewrite example Q&A in different style.",
            "format_category": "prompt style",
            "format_details": "5 few-shot demonstrations; 10 prompt variations created per FormatSpread; MOF modifies few-shot examples so each example uses a different delimiter/format and includes an instruction for the model to rewrite Q&A in another style; examples include delimiter and heading changes.",
            "performance_metric": "performance spread (max accuracy - min accuracy) and accuracy",
            "performance_value": "46% reduction in spread (relative) when using MOF vs traditional prompts on task280 with Llama-2-13b",
            "baseline_performance": "traditional few-shot prompts constructed with FormatSpread (spread not numerically reported in paper for this dataset)",
            "performance_change": "-46% relative reduction in spread (MOF vs traditional) on task280",
            "experimental_setting": "1000 samples per dataset, batch size 100, 5-shot few-shot prompts, 10 prompt variations per prompt type, inference-only evaluation; decoding settings (temperature, max tokens) not reported.",
            "statistical_significance": null,
            "uuid": "e7405.0",
            "source_info": {
                "paper_title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MOF_general",
            "name_full": "Mixture of Formats (MOF) prompting — aggregate results across models and datasets",
            "brief_description": "MOF presents each few-shot example in a different style and instructs the model to rewrite examples in other styles; across multiple SuperNaturalInstructions datasets and several LLMs, MOF generally reduces style-induced prompt brittleness (spread) and often improves mean accuracy, though there are exceptions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "falcon-11B; Llama-2-13b-hf; Llama-2-13b-chat-hf; llama-3-70b-instruct",
            "model_description": "Off-the-shelf LLMs evaluated as black-box generative models in inference-only prompt experiments.",
            "model_size": "11b; 13b; 13b; 70b (respectively)",
            "task_name": "Multiple SuperNaturalInstructions tasks (16 selected datasets, e.g., task280, task1186, task905, task190, task1612, task320, task1347, etc.)",
            "task_description": "Varied tasks from SuperNaturalInstructions including text categorization, textual entailment, toxic language detection, text quality evaluation, counting tasks, and stereotype detection.",
            "problem_format": "Few-shot natural-language prompts (5-shot), with 10 prompt-format variations generated by FormatSpread; MOF variants mix styles within the 5-shot block.",
            "format_category": "prompt style / prompt format",
            "format_details": "Traditional prompts: consistent format across few-shot examples; MOF prompts: each of the 5 few-shot examples rendered in a different format/style and accompanied by an instruction to rewrite the Q&A in another style; 10 distinct prompt-format variations evaluated per method via FormatSpread.",
            "performance_metric": "accuracy (per prompt) and spread (difference between max and min accuracy across 10 formats)",
            "performance_value": null,
            "baseline_performance": "traditional few-shot prompts (10 FormatSpread variations); baseline summary: traditional prompts show substantial spread across formats but numeric baselines per dataset not fully reported in paper text",
            "performance_change": "MOF 'performs comparably or outperforms' traditional prompts in most datasets; improves Min and Max accuracies in many cases; exceptions where traditional outperformed MOF: task190 (llama-3-70b-instruct), task1612 (llama-2-13b-chat), task320 (falcon-11B). Specific numeric changes not reported except task280 result.",
            "experimental_setting": "Evaluation over 1000 examples per dataset (randomly selected), batch size 100, 10 prompt-format variations per prompt type, 5-shot few-shot; exact decoding parameters not reported.",
            "statistical_significance": null,
            "uuid": "e7405.1",
            "source_info": {
                "paper_title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "FormatSpread",
            "name_full": "FormatSpread (method for generating and evaluating prompt-format variations)",
            "brief_description": "FormatSpread is a method (Sclar et al., 2024) used to generate multiple prompt-format templates and to quantify format-induced performance variability by measuring spread (difference between best and worst format accuracy).",
            "citation_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.",
            "mention_or_use": "use",
            "model_name": "applied to multiple LLMs in this paper (falcon-11B; Llama-2-13b-hf; Llama-2-13b-chat-hf; llama-3-70b-instruct)",
            "model_description": "Used as a format-generation and evaluation procedure rather than a model architecture.",
            "model_size": null,
            "task_name": "Used to create prompt-format variations for each SuperNaturalInstructions task evaluated (10 variations per prompt type).",
            "task_description": "See individual SuperNaturalInstructions task descriptions (e.g., text categorization, entailment, toxic language detection).",
            "problem_format": "Generates multiple textual prompt templates (examples include 'Passage:: {} , Answer:: {}' for task280; 'SYSTEM REFERENCE : {}. ORIGINAL REFERENCE : {}. ANSWER : {}' for task1186; 'Tweet:{} , Label:{} , Answer:{}' for task905).",
            "format_category": "prompt template generation / prompt style",
            "format_details": "Produces 10 distinct prompt-format templates per task; used to compute spread = max accuracy - min accuracy across these templates; supports measurement of style-induced brittleness.",
            "performance_metric": "spread (max accuracy - min accuracy across formats), per-format accuracy",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "FormatSpread generated 10 traditional prompt variations; MOF prompts were generated by modifying FormatSpread to mix styles within few-shot examples (also 10 variations).",
            "statistical_significance": null,
            "uuid": "e7405.2",
            "source_info": {
                "paper_title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Traditional_few-shot_baseline",
            "name_full": "Traditional few-shot prompting (baseline) evaluated with FormatSpread",
            "brief_description": "Standard few-shot prompting where the few-shot examples share a single consistent format; shown to be sensitive to meaning-preserving format changes (prompt brittleness), with variability in accuracy across different prompt templates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "falcon-11B; Llama-2-13b-hf; Llama-2-13b-chat-hf; llama-3-70b-instruct",
            "model_description": "Off-the-shelf transformer LLMs evaluated with traditional few-shot prompts.",
            "model_size": "11b; 13b; 13b; 70b (respectively)",
            "task_name": "Multiple SuperNaturalInstructions tasks (as above)",
            "task_description": "See SuperNaturalInstructions tasks (classification, entailment, quality evaluation, counting, stereotype detection, toxic detection).",
            "problem_format": "Consistent-format 5-shot few-shot prompts; 10 FormatSpread-generated variations per task used as baseline.",
            "format_category": "prompt style",
            "format_details": "Each traditional prompt uses the same style for all few-shot examples; FormatSpread enumerates formatting changes (delimiters, headings, ordering); spread computed across the 10 variations to measure brittleness.",
            "performance_metric": "accuracy and spread (max-min accuracy across formats)",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "5-shot few-shot, 10 prompt variations, 1000 samples per dataset, batch size 100.",
            "statistical_significance": null,
            "uuid": "e7405.3",
            "source_info": {
                "paper_title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.",
            "rating": 2,
            "sanitized_title": "quantifying_language_models_sensitivity_to_spurious_features_in_prompt_design_or_how_i_learned_to_start_worrying_about_prompt_formatting"
        },
        {
            "paper_title": "Mind your format: Towards consistent evaluation of in-context learning improvements.",
            "rating": 2,
            "sanitized_title": "mind_your_format_towards_consistent_evaluation_of_incontext_learning_improvements"
        },
        {
            "paper_title": "Does prompt formatting have any impact on llm performance?",
            "rating": 2,
            "sanitized_title": "does_prompt_formatting_have_any_impact_on_llm_performance"
        },
        {
            "paper_title": "Ask me anything: A simple strategy for prompting language models.",
            "rating": 1,
            "sanitized_title": "ask_me_anything_a_simple_strategy_for_prompting_language_models"
        },
        {
            "paper_title": "State of what art? a call for multi-prompt llm evaluation.",
            "rating": 2,
            "sanitized_title": "state_of_what_art_a_call_for_multiprompt_llm_evaluation"
        }
    ],
    "cost": 0.01128325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards LLMs Robustness to Changes in Prompt Format Styles
9 Apr 2025</p>
<p>Lilian Ngweta ngwetl@rpi.edu 
Rensselaer Polytechnic Institute</p>
<p>Kiran Kate 
IBM Research</p>
<p>Jason Tsay 
IBM Research</p>
<p>Yara Rizk 
IBM Research</p>
<p>Towards LLMs Robustness to Changes in Prompt Format Styles
9 Apr 202568AFF47AE1A2DD639148D7D5E969C65BarXiv:2504.06969v1[cs.CL]
Large language models (LLMs) have gained popularity in recent years for their utility in various applications.However, they are sensitive to non-semantic changes in prompt formats, where small changes in the prompt format can lead to significant performance fluctuations.In the literature, this problem is commonly referred to as prompt brittleness.Previous research on prompt engineering has focused mainly on developing techniques for identifying the optimal prompt for specific tasks.Some studies have also explored the issue of prompt brittleness and proposed methods to quantify performance variations; however, no simple solution has been found to address this challenge.We propose Mixture of Formats (MOF), a simple and efficient technique for addressing prompt brittleness in LLMs by diversifying the styles used in the prompt fewshot examples.MOF was inspired by computer vision techniques that utilize diverse style datasets to prevent models from associating specific styles with the target variable.Empirical results show that our proposed technique reduces style-induced prompt brittleness in various LLMs while also enhancing overall performance across prompt variations and different datasets.</p>
<p>Introduction</p>
<p>Large language models (LLMs) are useful for many applications and tasks i.e., content generation, translation, text analysis, etc.One of the popular techniques for adapting pre-trained LLMs to specific tasks that has emerged in recent years is prompt engineering (Liu et al., 2023;Tonmoy et al., 2024;Chen et al., 2023).Prompt engineering involves carefully crafting task-specific instructions and a few input-output demonstrations (prompts) to guide LLMs without changing their parameters (Sahoo et al., 2024).The popularity of prompt engineering can be attributed to the fact that it does not require labeled data and only needs a few demonstrations in prompts containing few-shot examples (Liu et al., 2023).Prompting is also generally computationally cheaper than supervised fine-tuning techniques since the model parameters are not modified (Sahoo et al., 2024).</p>
<p>Existing prompting techniques include zero-shot prompting (Radford et al., 2019), few-shot prompting (Brown et al., 2020), chain-of-thought (CoT) prompting (Wei et al., 2022), and automatic chainof-thought (Auto-CoT) prompting (Zhang et al., 2023).Most research on prompting techniques has focused on identifying or designing good prompts for specific tasks (Zhou et al., 2023b;Wan et al., 2023).However, a key problem often overlooked by these techniques is the sensitivity of LLMs to meaning-preserving changes in prompts.Examples of such changes include adding extra spaces, replacing two colons with one, changing the order of few-shot examples, or varying the choice of fewshot examples (He et al., 2024;Sclar et al., 2024;Lu et al., 2022;Wan et al., 2023).This problem is sometimes referred to as prompt brittleness (Zhou et al., 2023a).Prompt brittleness contributes to LLMs being unreliable and prevents their adoption in high-risk domains such as healthcare.</p>
<p>In this work, we focus on style-induced prompt brittleness as illustrated in Figure 1, and propose Mixture of Formats (MOF) to address it.MOF is a simple and computationally efficient prompting technique where each few-shot example in the prompt is presented in a distinct style.Furthermore, the model is instructed to rewrite each example using a different style, as shown in Figure 2. MOF was inspired by ideas from computer vision that involve learning from datasets with diverse styles to prevent models from associating styles with the target variable (Arjovsky et al., 2019;Kamath et al., 2021;Yin et al., 2021;Wald et al., 2021;Ngweta et al., 2023;Li et al., 2021).We evaluate the effectiveness of MOF prompting using datasets from var-ious tasks within SuperNaturalInstructions (Wang et al., 2022), comparing its performance against traditional prompts.Our experiments focus on fewshot prompting, where a traditional prompt refers to a regular few-shot prompt, and a MOF prompt is a few-shot prompt that has been converted into the MOF style, as demonstrated in Figure 2.</p>
<p>Related work</p>
<p>Traditional prompt engineering techniques.Several prompt engineering techniques have been proposed in recent years.Zero-shot prompting is a technique in which a prompt contains a description of the task and no training data is required (Radford et al., 2019).Unlike zero-shot prompting, few-shot prompting adds a few input-output demonstrations to the prompt to further help the model understand the task (Brown et al., 2020).Both zero-shot and few-shot prompting techniques enable the application of LLMs on new tasks without extensive training (Sahoo et al., 2024).For reasoning and logic tasks, prompting techniques that have been proposed include chain-of-thought (CoT) (Wei et al., 2022) and automatic chain-of-thought (Auto-CoT) (Zhang et al., 2023).CoT is a prompting technique that encourages LLMs to do step-by-step reasoning (Wei et al., 2022).Since manually creating CoT examples is time-consuming and not easily scalable, Zhang et al. (2023) proposed Auto-CoT to automatically guide LLMs to generate reasoning steps using a "Let's think step by step" statement in the prompt.</p>
<p>These traditional prompting techniques can be adapted to the MOF format by applying differ-ent formatting styles to each prompt example, as demonstrated in Figure 2. In this paper, we focus on the application of MOF to few-shot prompting.</p>
<p>Optimizing for the best prompt.This line of work focuses on optimizing and identifying the most effective prompt for a given task.Zhou et al. (2023b) propose the automatic prompt engineer (APE), an approach that enables the generation and selection of prompt instructions automatically.APE involves analyzing input queries, generating candidate prompt instructions, and then using reinforcement learning to select the best prompt (Zhou et al., 2023b).Similarly, Wan et al. (2023) propose a method where an LLM generates zero-shot outputs for given inputs, followed by selecting high-quality few-shot examples to construct an improved prompt, focusing on consistency, diversity, and repetition.Since automatic prompt optimization (APO) methods focus on optimizing instruction or optimizing few-shot examples, Wan et al. (2024) propose a technique to optimize for both, and compare its performance with the performance of techniques that only optimize instructions or examples.Yang et al. (2024) present Optimization by PROmpting (OPRO), a method that leverages LLMs as optimizers by describing the optimization task in natural language (Yang et al., 2024).Pryzant et al. (2023) propose Prompt Optimization with Textual Gradients (ProTeGi), which employs text gradients guided by beam search and bandit selection techniques for automatic prompt optimization (Pryzant et al., 2023).Additionally, Khattab et al. (2024) introduce DSPy, a framework that replaces hard-coded prompt templates with a systematic approach for building language model pipelines.Other methods for identifying optimal prompts include (Feffer et al., 2024;Sorensen et al., 2022;Yin et al., 2023).</p>
<p>Unlike existing methods in this area that repeatedly search for optimal prompts per task and model, our goal is to reduce style-induced prompt brittleness using an efficient and straightforward recipe illustrated in Figure 2.</p>
<p>Quantifying prompt brittleness in LLMs.Several works have shown that LLMs are sensitive to changes in prompt formats (Sclar et al., 2024;He et al., 2024;Voronov et al., 2024) and to the order of few-shot examples in the prompt (Lu et al., 2022).Sclar et al. (2024) propose FormatSpread, a method to efficiently measure performance variations in LLMs caused by prompt format changes, ANSWER : {} for dataset task1186, and Tweet:{} , Label:{} , Answer:{} for dataset task905.These formats are generated using FormatSpread (Sclar et al., 2024), as described in Section 3.1.The datasets used are described in Table 3.</p>
<p>by computing the performance difference (spread) between the best-performing format and the worstperforming format.Due to the sensitivity of LLMs to prompt format variations, Polo et al. (2024) propose PromptEval, an efficient method for evaluating LLMs on multiple prompts instead of a single prompt.Similarly, Mizrahi et al. (2024) propose metrics for multi-prompt evaluation of LLMs.</p>
<p>While these approaches are valuable tools for quantifying prompt brittleness, our proposed method focuses on mitigating it, particularly the brittleness arising from style variations in prompt formats.</p>
<p>Prompt ensembles.Arora et al. (2022) introduce Ask Me Anything (AMA), a prompting approach that transforms inputs into a question-answering format to encourage open-ended responses.AMA generates multiple imperfect prompts and combines the responses using a weak supervision strategy to produce the final output (Arora et al., 2022).Similarly, Voronov et al. (2024) propose Template Ensembles, an approach that aggregates model predictions across multiple prompt templates.However, both methods are computationally expensive, as they require aggregating predictions from multiple prompts.Furthermore, unlike our proposed method, they do not specifically address prompt brittleness caused by style variations in prompt formats.</p>
<p>Mixture of Formats</p>
<p>Style-induced prompt brittleness in LLMs is similar to problems observed in computer vision, where small changes to an image's style (eg.color or background) can affect the model's ability to make accurate predictions (Nagarajan et al., 2020).In computer vision, various approaches have been developed to address this issue, often involving learning from diverse datasets (Arjovsky et al., 2019;Ngweta et al., 2023;Kamath et al., 2021;Yin et al., 2021;Wald et al., 2021;Li et al., 2021).The underlying idea is that exposure to diverse data points helps the model disassociate styles from the target variable.Drawing inspiration from these techniques, we propose Mixture of Formats (MOF), a novel prompting strategy that deviates from traditional ways of crafting prompts by employing a distinct style format for each few-shot example in the prompt.To further reinforce model understanding, we have the model rewrite the question and answer of each example using a different format style, as illustrated in Figure 2. The effectiveness of this approach is evaluated in the subsequent subsections.</p>
<p>Experiments</p>
<p>Let X denote input queries for a task, and Y denote the target variable.Given N observations of inputs X and their corresponding targets Y as data D = {X n , Y n } N n=1 , we automatically build a traditional prompt and its MOF prompt version, each containing 5 few-shot examples, and use them for inference with an LLM.The traditional prompt is created using FormatSpread (Sclar et al., 2024), while the MOF prompt is generated by modifying FormatSpread to incorporate diverse formats within the few-shot examples, as illustrated in Figure 2.</p>
<p>Using FormatSpread, we create 10 traditional Spread is a metric for quantifying style-induced prompt brittleness and it is obtained by taking the difference between the best performing prompt (maximum accuracy) and the worst performing prompt (minimum accuracy).MOF prompts perform comparably or outperform traditional prompts in most datasets and in some datasets, traditional prompts have better performance.</p>
<p>prompt variations and 10 MOF prompt variations.</p>
<p>From the 10 prompt variations, for both traditional and MOF prompts, we compute performance accuracies for each prompt format across various tasks.</p>
<p>The goal is to compare the style-induced prompt brittleness between traditional prompts and MOF prompts.As in Sclar et al. (2024), we measure brittleness by calculating the performance spread, defined as the accuracy difference between the bestperforming and worst-performing prompt formats.The evaluation pipelines for traditional and MOF prompts are summarized in Algorithm 1 and Algorithm 2, respectively.</p>
<p>Datasets</p>
<p>We perform experiments on datasets covering various tasks from SuperNaturalInstructions (Mishra et al., 2022;Wang et al., 2022).Due to limited computational resources, we randomly selected 16 datasets and for each dataset we use 1000 samples and a batch size of 100.The datasets used are described in Table 3.</p>
<p>Baselines, metrics, and LLMs used In our experiments, we use traditional few-shot prompts as our baselines, where we compare the performance of LLMs when using traditional prompts versus MOF prompts.A primary focus of this work is to determine whether MOF prompting can minimize performance variations (spread) in LLMs when prompt format styles change.The performance spread is obtained by taking the difference between the highest performing prompt (denoted as "Max Accuracy" in the results tables) and the minimum performing prompt (denoted as "Min Accuracy").The spread value ranges from 0.0 to 1.0, where values closer to 0.0 indicate that the LLM is more robust and less sensitive to style changes, while values closer to 1.0 suggest that the LLM is highly sensitive to these changes.Additionally, for both traditional and MOF prompts, we compute the average accuracy across all 10 prompt variations to assess the overall performance of MOF prompts relative to traditional prompts.We use four LLMs in our experiments: falcon-11B, Llama-2-13b-hf, Llama-2-13b-chat-hf, and llama-3-70b-instruct.</p>
<p>We emphasize that while MOF prompting can be applied and compared with other existing traditional prompting techniques, such as automatic 1: Best performing format (Max Accuracy) and worst performing format (Min Accuracy) results for both traditional prompts and MOF prompts for llama-3-70b-instruct. MOF prompts improve the Min Accuracy and the Max Accuracy over traditional prompts in most cases.</p>
<p>Task</p>
<p>Traditional and the automatic prompt engineer (APE) (Zhou et al., 2023b), this paper focuses on applying MOF prompting to regular few-shot prompting and comparing their performances, due to limited computational resources.</p>
<p>Generating responses for evaluation To generate a response for a given question, a traditional or MOF prompt is combined with the question and then passed to an LLM to generate the response.The generated response is then compared to the ground-truth answer to calculate the model's accuracy.</p>
<p>Results</p>
<p>We perform experiments to evaluate whether MOF prompts reduce prompt brittleness in LLMs by comparing their spread with traditional prompts.We also assess improvements by analyzing the best (Max Accuracy) and worst (Min Accuracy) performing prompts.Finally, we evaluate overall performance by comparing the mean accuracies across all 10 prompt variations for both prompt types.</p>
<p>Minimizing prompt brittleness Figure 3 shows that MOF prompting effectively reduces style-induced prompt brittleness across several datasets and LLMs, with a notable 46% reduction in task280 using Llama-2-13b.While MOF prompts generally perform as well or better than traditional prompts, exceptions occur in task190 (llama-3-70b-instruct), task1612 (llama-2-13b-chat), and task320 (falcon-11B), where traditional prompts perform better.Investigating why MOF fails on these datasets is an important future direction.</p>
<p>Best and worst performing prompts Results for the best-performing prompt (Max Accuracy) and worst-performing prompt (Min Accuracy) for both traditional and MOF prompting are reported in Table 1.We observe that MOF prompting not only reduces spread but also improves both minimum and maximum accuracies.Average accuracy results across all 10 prompt variations for both traditional and MOF prompts are discussed in Appendix A.</p>
<p>Conclusion and future work</p>
<p>Addressing prompt brittleness remains a challenge, particularly when caused by changes in prompt format styles.In this work, we introduce a simple and efficient prompting technique, MOF, and evaluate its effectiveness in addressing style-induced prompt brittleness.The preliminary results are promising, with significant improvements over traditional prompting in many datasets, as shown in Figure 3. Future directions include integrating MOF with techniques like chain-of-thought (CoT) and automatic prompt engineer (APE), comparing its performance with methods that aggregate results from multiple prompts such as AMA (Arora et al., 2022) and Template Ensembles (Voronov et al., 2024), and conducting experiments with larger LLMs like GPT-4, Claude 3.5 Sonnet, Falcon 40B, and Llama 3.1 405B.Additionally, analyzing MOF's failures on certain datasets is a crucial area for further exploration.</p>
<p>We hope this work will inspire further research into addressing prompt brittleness in LLMs, and the code for this project is publicly available on GitHub. 1 1 Code: github.com/lilianngweta/mof.</p>
<p>2: Average accuracy results across 10 prompt variations for traditional prompts (denoted as Trad Mean Acc) and MOF prompts (denoted as MOF Mean Acc).For all LLMs, MOF prompts perform comparable and in most cases have a higher overall average accuracy than traditional prompts.task1347 A text matching dataset that involves classifying the semantic similarity of two sentences on a scale of 0 -5.</p>
<p>task1612</p>
<p>A textual entailment dataset derived from the SICK dataset, that involves accurately classifying labels to show the relationship between two sentences.task1502 A toxic language detection dataset that involves classifying the type of tweet in HateXplain.</p>
<p>task161</p>
<p>A dataset focused on counting the words in a sentence that contain a specified letter.</p>
<p>task158</p>
<p>A dataset that involves counting the number of times a word occurs in a sentence.</p>
<p>task1186 A text quality evaluation dataset that involves evaluating the naturalness of system generated reference.task190 A textual entailment dataset that involves choosing whether two given sentences agree, disagree, or neither with each other.</p>
<p>task1284 A text quality evaluation dataset that involves evaluating the informativeness of system generated reference.</p>
<p>task607</p>
<p>A toxic language detection that involves determining whether or not the post is intentionally offensive.</p>
<p>task163</p>
<p>A dataset that involves counting the number of words in the sentence that end with a specified letter.</p>
<p>task905</p>
<p>A toxic language detection dataset that involves determining whether the given category of a tweet is true or false.task320 A stereotype detection dataset that involves determining whether a given target pertaining to race in two sentences is a stereotype.</p>
<p>task316</p>
<p>A stereotype detection dataset that involves classifying whether a sentence is stereotype or anti-stereotype.</p>
<p>task162</p>
<p>A dataset that involves counting the words in a sentence that begin with a specified letter.</p>
<p>Figure 1 :
1
Figure 1: A demonstration of how small changes to the prompt format style can sometimes lead to incorrect predictions in LLMs.</p>
<p>Figure 2 :
2
Figure 2: An illustration of how to convert a traditional prompt into a MOF prompt.This example serves as a simple demonstration of the conversion process.In the actual experiments, datasets use various formats such as Passage:: {} , Answer:: {} for dataset task280, SYSTEM REFERENCE : {}.ORIGINAL REFERENCE : {}.ANSWER : {} for dataset task1186, and Tweet:{} , Label:{} , Answer:{} for dataset task905.These formats are generated using FormatSpread(Sclar et al., 2024), as described in Section 3.1.The datasets used are described in Table3.</p>
<p>Figure 3 :
3
Figure3: Comparing the performance spread of traditional prompts and MOF prompts.Spread is a metric for quantifying style-induced prompt brittleness and it is obtained by taking the difference between the best performing prompt (maximum accuracy) and the worst performing prompt (minimum accuracy).MOF prompts perform comparably or outperform traditional prompts in most datasets and in some datasets, traditional prompts have better performance.</p>
<p>Table 3 :
3
(Mishra et al., 2022;Wang et al., 2022)Mishra et al., 2022;Wang et al., 2022)that we used in our experiments.
Dataset IDDataset Descriptiontask280A text categorization dataset that involves classifying sentences into four types of stereotypes: gender, profession, race, and religion.task317A stereotype detection dataset that involves classifying sentences into various types of stereotypes.</p>
<p>Martin Arjovsky, Léon Bottou, Ishaan , David Lopez-Paz, arXiv:1907.02893Invariant risk minimization. 2019arXiv preprint</p>
<p>Ask me anything: A simple strategy for prompting language models. Simran Arora, Avanika Narayan, Laurel Mayee F Chen, Neel Orr, Kush Guha, Ines Bhatia, Christopher Chami, Re, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS '20. the 34th International Conference on Neural Information Processing Systems, NIPS '20Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; Red Hook, NY, USACurran Associates Inc2020Language models are few-shot learners</p>
<p>Unleashing the potential of prompt engineering in large language models: a comprehensive review. Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, Shengxin Zhu, arXiv:2310.147352023arXiv preprint</p>
<p>Prompt exploration with prompt regression. Michael Feffer, Ronald Xu, Yuekai Sun, Mikhail Yurochkin, arXiv:2405.110832024arXiv preprint</p>
<p>Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, Sadid Hasan, arXiv:2411.10541Does prompt formatting have any impact on llm performance? arXiv preprint. 2024</p>
<p>Does invariant risk minimization capture invariance?. Pritish Kamath, Akilesh Tangella, Danica Sutherland, Nathan Srebro, International Conference on Artificial Intelligence and Statistics. PMLR2021</p>
<p>DSPy: Compiling declarative language model calls into stateof-the-art pipelines. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, A , Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Emmanuel Candès, and Chiara Sabatti. Shuangning Li, Matteo Sesia, Yaniv Romano, arXiv:2106.04118Searching for consistent associations with a multi-environment knockoff filter. 2021arXiv preprint</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, 10.18653/v1/2022.acl-long.556Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, ACL. 2022</p>
<p>State of what art? a call for multi-prompt llm evaluation. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, Gabriel Stanovsky, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Understanding the failure modes of out-of-distribution generalization. Anders Vaishnavh Nagarajan, Behnam Andreassen, Neyshabur, arXiv:2010.157752020arXiv preprint</p>
<p>Simple disentanglement of style and content in visual representations. Lilian Ngweta, Subha Maity, Alex Gittens, Yuekai Sun, Mikhail Yurochkin, International Conference on Machine Learning. PMLR2023</p>
<p>Felipe Maia Polo, Ronald Xu, Lucas Weber, Mírian Silva, Onkar Bhardwaj, Leshem Choshen, arXiv:2405.17202Efficient multi-prompt evaluation of llms. Yuekai Oliveira, Mikhail Sun, Yurochkin, 2024arXiv preprint</p>
<p>Automatic prompt optimization with "gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, </p>
<p>arXiv:2402.07927A systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr, The Twelfth International Conference on Learning Representations. 2024</p>
<p>An information-theoretic approach to prompt without ground truth labels. Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, David Wingate, 10.18653/v1/2022.acl-long.60Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsPapers; Dublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Sm Tonmoy, Vinija Zaman, Anku Jain, Rani, Aman Vipula Rawte, Amitava Chadha, Das, arXiv:2401.01313A comprehensive survey of hallucination mitigation techniques in large language models. 2024arXiv preprint</p>
<p>Mind your format: Towards consistent evaluation of in-context learning improvements. Anton Voronov, Lena Wolf, Max Ryabinin, arXiv:2401.067662024arXiv preprint</p>
<p>On calibration and out-of-domain generalization. Yoav Wald, Amir Feder, Daniel Greenfeld, Uri Shalit, Advances in Neural Information Processing Systems. 202134</p>
<p>Better zero-shot reasoning with self-adaptive prompting. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Arik, Tomas Pfister, 10.18653/v1/2023.findings-acl.216Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Teach better or show smarter? on instructions and exemplars in automatic prompt optimization. Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Sercan O Arik, arXiv:2406.157082024arXiv preprint</p>
<p>Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, EMNLP. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Large language models as optimizers. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, Xinyun Chen, arXiv:2309.034092024Preprint</p>
<p>Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu, 10.18653/v1/2023.acl-long.172Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Optimization-based causal estimation from heterogenous environments. Mingzhang Yin, Yixin Wang, David M Blei, arXiv:2109.11990The Eleventh International Conference on Learning Representations. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, 2021. 2023arXiv preprintAutomatic chain of thought prompting in large language models</p>
<p>Batch calibration: Rethinking calibration for in-context learning and prompt engineering. Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, Subhrajit Roy, arXiv:2309.172492023aarXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>In this section, we focus on the performance of traditional and MOF prompts across all 10 prompt variations for each. The average accuracy across these 10 prompt variations for both traditional and MOF prompts is reported in Table 2. For all LLMs, we find that MOF prompts perform nearly as well as traditional prompts, with MOF prompts generally leading to significant overall mean accuracy improvements. Algorithm 1 Traditional prompts evaluation pipeline 1: Input: Data D 2: Create 10 variations of traditional prompts using FormatSpread. Sclar, 20247Output: Return accuracies for the best performing prompt (max accuracy), worst performing prompt (min accuracy), the spread, and the average accuracy across all 10 traditional prompt variations. Algorithm 2 MOF prompts evaluation pipeline 1: Input: Data D 2: Create 10 variations of MOF prompts using a modified FormatSpread. Sclar et al., 2024) that incorporates diverse styles in the few-shot examples as illustrated in Figure 2</p>
<p>Use the created MOF prompt variations to generate responses. 4: Evaluate each of the 10 MOF prompts and save results. 5: Compute the average accuracy across all 10 MOF prompt variations. 6: Identify the best performing prompt. worst performing prompt, and compute the spread. 7: Output: Return accuracies for the best performing prompt (max accuracy. worst performing prompt (min accuracy), the spread, and the average accuracy across all 10 MOF prompt variations</p>            </div>
        </div>

    </div>
</body>
</html>
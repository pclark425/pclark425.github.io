<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9055 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9055</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9055</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-264828854</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.conll-1.25.pdf" target="_blank">Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</a></p>
                <p><strong>Paper Abstract:</strong> To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs’ robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.</p>
                <p><strong>Cost:</strong> 0.034</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9055.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9055.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open base large language model (7B parameters) evaluated in its base form on ToM tests; trained with web-scale data and not instruction tuned in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base transformer LLM (decoder-style) trained by the Falcon authors on web data; here used as a base-LLM (no instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sally-Anne (first- and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Classic false-belief narratives assessing first-order (SA1) and second-order (SA2) Theory of Mind (belief attribution about another's belief).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed above child level on SA1 in some cases but at or below child level on SA2; struggled strongly when deviations increased (deviation 2 worsened performance). Exact numeric scores not reported in text for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225 (reported group means in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Falcon-7B is roughly comparable to children on first-order but below or at child level for second-order ToM; performance degrades with more novel deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Inference with near-zero temperature; base-LLM prompted as text-completion (e.g. 'Sally will look for the ball in the '), deviations levels 0-2 used to probe generalization. Each question run independently to avoid in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Base-LLM prompt-as-completion format differs from instruct-LLM QA format; prompt length/context window may limit performance; paper notes base models may be disadvantaged by prompt format and longer scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9055.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open base large language model (7B parameters) evaluated in its base form on ToM tests; trained with web-scale data and not instruction tuned in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base transformer LLM (decoder-style) trained by the Falcon authors on web data; here used as a base-LLM (no instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Seven short vignettes assessing comprehension of non-literal language (e.g., white lies, sarcasm, pretend) requiring inference of speakers' intentions (ToM-relevant).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed generally below child level across Strange Stories items, with less catastrophic decline on harder items than children but overall below many instruct-LLMs; deviation levels had little impact on most base-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): performance declines across items as difficulty increases; specific numeric group means not reported for Strange Stories in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below child baseline overall for Strange Stories; did not match instruct-LLMs that often reached child level or above on these items.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Base-LLM prompted as completion tasks rather than instruction-following; deviations 0-2 applied; answers typed/completed as continuations rather than QA form.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Prompt format differences may suppress latent knowledge; available training data likely includes many nonliteral examples but base-LLM output format made assessment harder.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9055.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open base large language model (7B parameters) evaluated in its base form on ToM tests; trained with web-scale data and not instruction tuned in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base transformer LLM (decoder-style) trained by the Falcon authors on web data; here used as a base-LLM (no instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories followed by true/false questions; includes 'intentionality' questions probing recursive levels of embedded beliefs (higher-order ToM) and matched memory questions assessing factual recall.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed below child level on both intentionality and memory questions across recursion levels; little variation except slightly better on some higher recursion levels for larger base models (not this one).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y): memory question scores > 0.85 across most levels; intentionality shows a significant drop after level 1→2 (paper reports β = −0.222, p < .05).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Falcon-7B is below human baseline on IM for both memory and intentionality questions.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>For IM true/false closed questions base-LLMs were given two example continuations to constrain output format (e.g. '[correct]'/'[incorrect]') to push toward desired answer forms; all runs deterministic and independent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>IM prompts are long; base models with small context windows likely disadvantaged; paper notes prompt length/context limits may explain base-LLM struggles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9055.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B-I</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-instruct (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned 7B Falcon variant optimized for following instructions and conversational prompts; evaluated here as an instruct-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B-I (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned version of Falcon trained to follow natural-language instructions and aligned with human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sally-Anne (first- and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>First- and second-order false-belief narratives assessing belief attribution capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>The chat-optimized Falcon-7B-I failed broadly on second-order questions and performed worst overall among instruct-LLMs on SA; often at or below child level even on first-order when deviations were present.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Falcon-7B-I performs below child baseline on many SA2 questions and is among the weakest instruct-LLMs on Sally-Anne tests.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Instruct-LLMs prompted in QA format mirroring children's questionnaires; deterministic decoding; deviations introduced to test generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Smaller model size and instruction tuning did not rescue second-order ToM; performance sensitive to deviation and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9055.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B-I</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-instruct (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned 7B Falcon variant optimized for following instructions and conversational prompts; evaluated here as an instruct-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B-I (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned version of Falcon trained to follow natural-language instructions and aligned with human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Seven vignettes assessing comprehension of non-literal language and inference of intentions (ToM-related).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed worst among instruct-LLMs on Strange Stories and generally below child level; smaller model size limited performance though deviation levels had limited additional effect.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): performance declines with item difficulty (no single mean provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below child baseline and below larger instruct-LLMs on Strange Stories.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompted in question-answer format close to children's test instructions; deterministic sampling; deviations 0-2 tested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Smaller instruct-LLM; may lack capacity to fully leverage instruction tuning for pragmatic/non-literal inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9055.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B-I</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-instruct (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned 7B Falcon variant optimized for following instructions and conversational prompts; evaluated here as an instruct-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B-I (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned version of Falcon trained to follow natural-language instructions and aligned with human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories plus true/false questions probing recursive intentionality and matched memory recall.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed below child level on both intentionality and memory questions across recursion levels; did not show the pattern of improvement seen in some larger instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y): memory > 0.85 across levels except a dip at level 4; intentionality showed a drop after level two.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Falcon-7B-I is below human baseline on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>IM prompts are long and included voice-over/visual support for children (not for models); base/instruct prompt formatting controlled and deterministic.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Small instruction-tuned model; prompt length and context-window constraints likely a major limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9055.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (30B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large base transformer LLM (approx. 30B parameters) used as a base-LLM in evaluations; not instruction tuned in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-30B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base LLaMA model (decoder-style transformer) trained on large text corpora; used here without instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sally-Anne (first- and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>First- and second-order false-belief tasks probing ToM abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed above child level on first-order ToM (SA1) but fell at or below child level on second-order ToM (SA2); performance decreased with deviations and recursion.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLaMA-30B can match or exceed children on SA1 but is below or similar to children for SA2.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Base-LLM prompted as text-completion; near-zero temperature; deviations 0-2 included; motivation questions formatted with correct answer for base-LLMs to encourage sensible continuation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Base-completion prompting may advantage or disadvantage the model relative to QA format; no instruction tuning possibly limits application of latent knowledge to explicit QA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9055.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (30B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large base transformer LLM (approx. 30B parameters) used as a base-LLM in evaluations; not instruction tuned in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-30B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base LLaMA model (decoder-style transformer) trained on large text corpora; used here without instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Vignettes assessing non-literal language understanding and intention inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Base LLaMA performed below child level on Strange Stories overall, with little sensitivity to deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): performance declines across items as difficulty increases; exact numerical mean not provided for Strange Stories in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below child baseline on Strange Stories.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompts given as text completions; deviation levels tested; answers extracted from completions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Base-completion prompt format may hide latent abilities for non-literal language present in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9055.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (30B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large base transformer LLM (approx. 30B parameters) used as a base-LLM in evaluations; not instruction tuned in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-30B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base LLaMA model (decoder-style transformer) trained on large text corpora; used here without instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories followed by true/false items probing recursive intentionality and memory recall.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed below child level across both intentionality and memory questions; larger base models showed slight improvement on higher recursion but not to child level.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y): memory questions ~>0.85; intentionality drops significantly after level 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Closed IM questions presented as completions for base-LLMs with helper examples to constrain output ('[correct]'/'[incorrect]').</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Long IM prompts may exceed effective context for some base models; method of forcing output format may bias responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9055.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-davinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-002/003; 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive transformer base-LLM (175B parameters) originating from the GPT-3 family; used in base and instruct variants across LLM research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-davinci (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 era large decoder-only transformer (175B) trained with self-supervised objectives; in this paper used in a base completion format (text-davinci variant).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sally-Anne (first- and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>First- and second-order false-belief stories testing ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported structurally low scores compared to expectations and inconsistent with Kosinski (2023); performed above child level on some SA1 items but low on SA2; prompting as open questions (not completion) likely impacted results.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Overall below top instruct-LLMs; sometimes above child on SA1 but at or below child level on SA2.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Base-LLM prompted as text-completion; for motivation prompts the correct experimental answer was inserted to encourage coherent continuation; deterministic decoding used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Different prompting choice (open QA vs. completion) compared to prior work likely explains discrepancies; paper explicitly notes this.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9055.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-davinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-002/003; 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive transformer base-LLM (175B parameters) originating from the GPT-3 family; used in base and instruct variants across LLM research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-davinci (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 era large decoder-only transformer (175B) trained with self-supervised objectives; in this paper used in a base completion format (text-davinci variant).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Vignettes assessing non-literal language comprehension and intention inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-davinci (when treated as base) performed below child level on Strange Stories generally; instruct-tuned descendants performed better (see GPT-3.5/GPT-4 entries).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): observed decline with item difficulty; numeric group mean not specified in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below child baseline on Strange Stories in this base completion prompting setup.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompted as completion tasks without QA instruction-following; outputs constrained via example continuations for IM but not for SS.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Prompt format differences relative to instruct models limit comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9055.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-davinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-002/003; 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive transformer base-LLM (175B parameters) originating from the GPT-3 family; used in base and instruct variants across LLM research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-davinci (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 era large decoder-only transformer (175B) trained with self-supervised objectives; in this paper used in a base completion format (text-davinci variant).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories plus true/false questions probing recursive intentionality and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed below child level on both intentionality and memory items; overall base-LLMs fared poorly on IM relative to instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y): memory questions generally > 0.85; intentionality drops after level 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Two example continuations were provided to base-LLMs to encourage '[correct]'/'[incorrect]' style outputs for closed IM questions; deterministic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>The example-continuation technique constrains outputs but may not fully reflect model reasoning; prompt length and format matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e9055.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLOOM-176B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLOOM (176B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large open-access base LLM (~176B parameters) trained on multilingual web data and evaluated as a base-LLM in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOM-176B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open multilingual transformer model (176B params) trained on large multilingual corpora; used here in base form (no instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>176B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sally-Anne (first- and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>First- and second-order false-belief narratives testing Theory of Mind.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>One of the base-LLMs performing above child level on first-order ToM (SA1) but at or below child level on second-order (SA2); struggled more with deviations and higher-order recursion.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>BLOOM can exceed child performance on SA1 but not on SA2; overall below best instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompts given as text-completion; motivation prompts for base models inserted correct experimental answer to elicit reasoned continuation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Base completion format and lack of instruction tuning may have limited performance on explicit QA-style ToM questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e9055.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLOOM-176B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLOOM (176B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large open-access base LLM (~176B parameters) trained on multilingual web data and evaluated as a base-LLM in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOM-176B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open multilingual transformer model (176B params) trained on large multilingual corpora; used here in base form (no instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>176B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Seven social vignettes probing non-literal language comprehension and intention inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed below child level overall on Strange Stories, similar to other base-LLMs; deviations had little effect.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): performance declines with item difficulty; no single mean provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below child baseline on Strange Stories in this experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Base completion prompting; deterministic decoding; deviations 0-2 applied.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Base format may hide potential comprehension capabilities present in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e9055.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLOOM-176B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLOOM (176B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large open-access base LLM (~176B parameters) trained on multilingual web data and evaluated as a base-LLM in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOM-176B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open multilingual transformer model (176B params) trained on large multilingual corpora; used here in base form (no instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>176B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories plus true/false questions probing recursive intentionality and matched memory recall.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed below child level on both intentionality and memory questions across recursion levels; did not approach GPT-4 level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y): high memory scores (>0.85) and drop in intentionality after level two.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Base-LLM outputs guided via example continuations for closed IM items; deterministic sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Long prompts and lack of instruction tuning likely constrained performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e9055.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (11B, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned encoder-decoder transformer (FLAN-T5 family) used as an instruct-LLM; relatively small in parameter count but instruction-tuned to follow prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5 (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>FLAN-T5 instruction-tuned variant (11B params) optimized to follow natural-language instructions and perform zero-shot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sally-Anne (first- and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>First- and second-order false-belief tasks assessing ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Smaller instruct-LLM with mixed SA performance: often below larger GPT-family instruct models, struggled on second-order ToM and with deviations; generally poorer than GPT-3.5/GPT-4 on SA2.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below leading instruct-LLMs and often below child level on SA2; sometimes near child level on SA1.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompted in QA format similar to children's instructions; deterministic decoding; deviations applied.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Though instruction-tuned, smaller parameter count likely limited recursive ToM performance; deviation-robustness weaker than large instruct models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e9055.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (11B, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned encoder-decoder transformer (FLAN-T5 family) used as an instruct-LLM; relatively small in parameter count but instruction-tuned to follow prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5 (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>FLAN-T5 instruction-tuned variant (11B params) optimized to follow natural-language instructions and perform zero-shot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Seven vignettes probing non-literal language understanding requiring inference of intentions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Outperformed some larger models (e.g., PaLM) on harder Strange Stories items and surpassed child level earlier than PaLM in some cases; overall performed well relative to its size.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): performance declines with item difficulty; no single mean provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>FLAN-T5 matched or surpassed child performance on several Strange Stories items, especially the more complex ones.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>QA-format prompts; deviations levels applied; deterministic sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Smaller model but instruction tuning yields better pragmatic/non-literal performance than some larger base models; results may not generalize to other ToM paradigms (e.g., recursive belief tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e9055.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (11B, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned encoder-decoder transformer (FLAN-T5 family) used as an instruct-LLM; relatively small in parameter count but instruction-tuned to follow prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5 (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>FLAN-T5 instruction-tuned variant (11B params) optimized to follow natural-language instructions and perform zero-shot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories plus true/false items probing recursive intentionality (higher-order ToM) and memory recall.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Surprisingly increased performance with higher recursion levels and ended at approximately child level for IM; performed better on IM than many other instruct-LLMs despite small size.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y): memory > 0.85; intentionality declines after level two.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>FLAN-T5 reached child-level performance on IM by the highest recursion levels but was not consistently above children across all levels.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Instruction-tuned QA prompts; IM closed questions used yes/no format for children and tailored output format for models; deterministic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Pattern of increasing performance with recursion is atypical and may reflect dataset or prompt interactions rather than genuine recursive reasoning; paper notes need for further exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e9055.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-003, instruct-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 175B-parameter GPT-3 family model (text-davinci) that has instruction-tuned variants; used here as an instruct-LLM in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003; instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 family model (175B) with instruction-tuning (text-davinci variants) trained to follow prompts and produce coherent responses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sally-Anne (first- and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>First- and second-order false-belief narratives probing ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed at or near child level on simpler items (SA1) but showed decline on SA2; overall better than many base-LLMs but worse than GPT-3.5 and GPT-4 on SA2 and on deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Comparable to children on some first-order items but below strongest instruct-LLMs for second-order ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Instruct-LLM prompted in QA format similar to children's tests; deterministic sampling; deviations introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Performance sensitive to deviation; instruction tuning helped relative to base GPT-3 but not sufficient for robust second-order ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e9055.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-003, instruct-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 175B-parameter GPT-3 family model (text-davinci) that has instruction-tuned variants; used here as an instruct-LLM in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003; instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 family model (175B) with instruction-tuning (text-davinci variants) trained to follow prompts and produce coherent responses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Vignettes testing non-literal language understanding and intention inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed at or close to child level on easier Strange Stories items (item 1), with some decline on harder items but overall stayed well above child level relative to base models; deviation levels had little effect for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): decreasing performance with item difficulty; exact numeric mean not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>At or above child level on several Strange Stories items, though behind GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompted in QA style; deviations 0-2 tested; deterministic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Not as robust as GPT-4 on the full set; performance drops with increasing item difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.20">
                <h3 class="extraction-instance">Extracted Data Instance 20 (e9055.20)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-003, instruct-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 175B-parameter GPT-3 family model (text-davinci) that has instruction-tuned variants; used here as an instruct-LLM in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003; instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 family model (175B) with instruction-tuning (text-davinci variants) trained to follow prompts and produce coherent responses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories plus true/false items probing recursive intentionality and memory recall.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed below child level on IM overall; not among the better-performing instruct-LLMs for recursive intentionality questions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y): memory >0.85; intentionality performance drops after level two.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>IM closed questions presented in yes/no form for children; model prompted similarly in QA format; deterministic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Instruction tuning improved some pragmatic capacities but not recursive ToM required by IM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.21">
                <h3 class="extraction-instance">Extracted Data Instance 21 (e9055.21)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (175B family instruct-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3.5 family instruct-tuned model optimized for chat/interactive tasks and instruction following; evaluated as an instruct-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruct-optimized GPT-3.5 family model (approx. 175B scale per paper sources) trained with human feedback to follow instructions and produce conversational outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (approx.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sally-Anne (first- and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>First- and second-order false-belief tasks assessing ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed well above child level on first-order ToM and remained above child level on second-order ToM in some conditions, though performance degraded substantially with deviation level 2 (novel scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GT P-3.5 generally outperforms children on SA1 and can stay above children on SA2 in original formulations but loses robustness under substantial deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Instruct-LLM QA prompts matching children's instructions; deterministic decoding; deviations 0-2 tested to measure generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Robustness to deviations is limited; performance falls with scenario rewrites and increased novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.22">
                <h3 class="extraction-instance">Extracted Data Instance 22 (e9055.22)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (175B family instruct-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3.5 family instruct-tuned model optimized for chat/interactive tasks and instruction following; evaluated as an instruct-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruct-optimized GPT-3.5 family model (approx. 175B scale per paper sources) trained with human feedback to follow instructions and produce conversational outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (approx.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Vignettes assessing non-literal language understanding and inference of intentions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed at or close to child level on early items and stayed well above child level on many items; overall strong performance though not as high as GPT-4 which approached perfect scores.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): performance declines as item difficulty increases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3.5 generally outperforms or matches child baseline on Strange Stories and is stronger than most base-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>QA-style prompts; deviations had little effect on larger instruct-LLMs including GPT-3.5; deterministic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Less robust than GPT-4 across all tasks; IM performance notably worse than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.23">
                <h3 class="extraction-instance">Extracted Data Instance 23 (e9055.23)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (175B family instruct-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3.5 family instruct-tuned model optimized for chat/interactive tasks and instruction following; evaluated as an instruct-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruct-optimized GPT-3.5 family model (approx. 175B scale per paper sources) trained with human feedback to follow instructions and produce conversational outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (approx.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories and true/false questions probing recursive intentionality and matched memory recall.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed worst of the instruct-LLMs on IM in this study, generally below child level on intentionality and memory items and did not display the robust recursive ToM capabilities of GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y): strong memory (>0.85) and drop in intentionality beyond level two.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline and below GPT-4 on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>QA-format prompts; deterministic decoding; IM presented as closed yes/no items for models too.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Shows that instruction tuning alone is not sufficient for robust recursive ToM; model-specific limitations observed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.24">
                <h3 class="extraction-instance">Extracted Data Instance 24 (e9055.24)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's PaLM 2 family of large language models (estimated 175-340B parameters) evaluated both in base and chat variants; here PaLM2 (non-chat) used as an instruct-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2 (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer LLM from Google (PaLM 2 family), instruction-tuned and evaluated in this study; exact parameter count not disclosed in paper (est. 175–340B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175-340B (est.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sally-Anne (first- and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>First- and second-order false-belief tasks assessing Theory of Mind.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed well on first-order SA1 (often above child level) but showed perturbation on second-order SA2; some PaLM2 variants (e.g., PaLM2-chat) were more stable than non-chat PaLM2 on SA2 but still degraded under deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PaLM2 can match or exceed child performance on SA1 but struggles with SA2 and novel deviations relative to top GPT-family instruct models.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Instruct-LLM QA prompts; deviations 0-2; deterministic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>PaLM2 non-chat variant less robust than PaLM2-chat and GPT-4; deviation sensitivity suggests limited generalization in certain SA2 contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.25">
                <h3 class="extraction-instance">Extracted Data Instance 25 (e9055.25)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's PaLM 2 family of large language models (estimated 175-340B parameters) evaluated both in base and chat variants; here PaLM2 (non-chat) used as an instruct-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2 (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer LLM from Google (PaLM 2 family), instruction-tuned and evaluated in this study; exact parameter count not disclosed in paper (est. 175–340B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175-340B (est.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Vignettes assessing non-literal language understanding and intention inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2 performed well on Strange Stories; larger PaLM2 models matched or exceeded child level on many items though PaLM2-chat and PaLM2 had different strengths (PaLM2-chat often stronger). Deviation levels had little effect on larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): decreasing performance across items with difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PaLM2 matched or outperformed child baseline on Strange Stories items, though GPT-4 remained superior.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>QA-format prompts; deviations 0-2; deterministic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Exact parameterization and training details undisclosed; PaLM2-chat variant generally more robust than non-chat.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.26">
                <h3 class="extraction-instance">Extracted Data Instance 26 (e9055.26)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's PaLM 2 family of large language models (estimated 175-340B parameters) evaluated both in base and chat variants; here PaLM2 (non-chat) used as an instruct-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2 (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer LLM from Google (PaLM 2 family), instruction-tuned and evaluated in this study; exact parameter count not disclosed in paper (est. 175–340B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175-340B (est.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories plus true/false items probing recursive intentionality and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2 performed below child level on IM overall, with instruct-LLMs generally failing IM except for GPT-4; PaLM2-chat performed better than non-chat PaLM2 on some tasks but did not match children on IM recursive intentionality.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y): strong memory performance and drop-off in intentionality after level two.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline on IM for recursive intentionality; PaLM2 variants did not reach child-level robustness on IM except marginal improvements in chat variant for some items.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>QA-format prompts; IM closed questions presented similarly for models and children; deterministic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>IM is a challenging robustness test; PaLM2's undisclosed training details limit interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.27">
                <h3 class="extraction-instance">Extracted Data Instance 27 (e9055.27)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 - chat variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Chat-optimized PaLM2 variant tuned for conversational instruction following; evaluated as an instruct-LLM and generally stronger than non-chat PaLM2 on conversational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2-chat (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM2 family chat-optimized variant (est. 175–340B) instruction-tuned for dialog and QA tasks; used in this study on ToM batteries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175-340B (est.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sally-Anne (first- and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>First- and second-order false-belief tasks assessing ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2-chat performed well on SA1 but showed perturbation on SA2; second-order questions and deviations reduced performance, though PaLM2-chat was more robust than PaLM2 non-chat in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PaLM2-chat often above child level on SA1 and sometimes on SA2 in original formulations, but loses ground under strong deviations compared to GPT-3.5/GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompted in QA conversational format; deterministic decoding; deviations 0-2 employed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Although chat-optimized, PaLM2-chat did not match GPT-4's robustness on SA2 with deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.28">
                <h3 class="extraction-instance">Extracted Data Instance 28 (e9055.28)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 - chat variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Chat-optimized PaLM2 variant tuned for conversational instruction following; evaluated as an instruct-LLM and generally stronger than non-chat PaLM2 on conversational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2-chat (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM2 family chat-optimized variant (est. 175–340B) instruction-tuned for dialog and QA tasks; used in this study on ToM batteries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175-340B (est.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Vignettes probing non-literal language comprehension and intention inference. </td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2-chat matched or exceeded child performance on Strange Stories and showed little effect from deviations for larger instruct-LLMs, indicating robust non-literal language handling.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): expected decline with increasing item difficulty (no single mean provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PaLM2-chat generally matched or surpassed child baseline on Strange Stories items, though GPT-4 remained the top performer.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>QA-format instruction prompts; deviations 0-2 included; deterministic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Performance likely aided by exposure to pragmatic and conversational examples in pretraining and instruction data; exact training details undisclosed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.29">
                <h3 class="extraction-instance">Extracted Data Instance 29 (e9055.29)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 - chat variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Chat-optimized PaLM2 variant tuned for conversational instruction following; evaluated as an instruct-LLM and generally stronger than non-chat PaLM2 on conversational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2-chat (instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM2 family chat-optimized variant (est. 175–340B) instruction-tuned for dialog and QA tasks; used in this study on ToM batteries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175-340B (est.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories plus true/false items probing recursive intentionality and memory recall.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2-chat performed better than PaLM2 non-chat on some aspects but did not reach child-level robustness on IM; GPT-4 remained the only model consistently above child level on IM after second-order intentionality.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y): memory >0.85; intentionality drops after level two.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline on IM for recursive intentionality despite being stronger than many other instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>QA conversational prompts; deterministic decoding; IM closed format testing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>IM's recursive intentionality places high demands that only GPT-4 met consistently in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.30">
                <h3 class="extraction-instance">Extracted Data Instance 30 (e9055.30)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4, a large instruction-tuned transformer model (estimated >340B parameters per paper) and the top-performing model in the study across ToM batteries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned transformer model from OpenAI; parameter count undisclosed in paper (reported >340B estimate); trained with reinforcement learning from human feedback and optimized for instruction following and dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>340B (est.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sally-Anne (first- and second-order)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>First- and second-order false-belief tasks assessing Theory of Mind abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 outperformed all other models and often outperformed children; remained above child level on SA2 in original scenarios but performance decreased substantially with extreme deviations (deviation level 2), though it still outperformed most other models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 typically outperforms the child baseline and other LLMs on Sally-Anne tests, though robustness to large deviations is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Instruct-LLM QA prompts close to children's instructions; deterministic decoding (temperature ≈ 0); deviations 0-2 probed generalization; each question run independently.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Passing ToM tests does not imply human-like ToM; updates to GPT-family models can change capabilities over time; instruction-tuning and dataset exposure likely crucial to its performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.31">
                <h3 class="extraction-instance">Extracted Data Instance 31 (e9055.31)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4, a large instruction-tuned transformer model (estimated >340B parameters per paper) and the top-performing model in the study across ToM batteries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned transformer model from OpenAI; parameter count undisclosed in paper (reported >340B estimate); trained with reinforcement learning from human feedback and optimized for instruction following and dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>340B (est.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Seven vignettes probing non-literal language comprehension, such as white lies, sarcasm, and pretence, requiring inference of speakers' intentions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 approached near-perfect scores across Strange Stories items and was robust to deviations; significantly outperformed most other models and children on many non-literal items.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y): performance declines with item difficulty; exact numeric mean not provided for Strange Stories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 outperforms child baseline and other LLMs on Strange Stories, approaching perfect understanding of non-literal intents in tested scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>QA-format prompts mirroring children's instructions; deterministic decoding; deviations 0-2 had little effect for GPT-4 on Strange Stories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>High performance likely influenced by large exposure to conversational/non-literal text in training and instruction tuning; note that high scores do not prove human-like ToM or social grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.32">
                <h3 class="extraction-instance">Extracted Data Instance 32 (e9055.32)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4, a large instruction-tuned transformer model (estimated >340B parameters per paper) and the top-performing model in the study across ToM batteries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned transformer model from OpenAI; parameter count undisclosed in paper (reported >340B estimate); trained with reinforcement learning from human feedback and optimized for instruction following and dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>340B (est.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories followed by a battery of true/false questions including recursively nested intentionality items probing higher-order ToM and matched memory questions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 was the only model that performed consistently well across IM levels and remained above child level after second-order intentionality; passed IM most robustly among all models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y): memory > 0.85 across most levels; intentionality shows a drop after level two (statistically significant difference between level 2 and 1: β = −0.222, p < .05).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 outperforms children on IM across many levels and is uniquely robust on recursive intentionality compared to other LLMs tested.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>IM closed questions administered; for models, deterministic decoding and instruction-style prompts used; deviations not relevant for IM (no deviations applied to this unpublished adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>IM is a newly adapted test in this study; while GPT-4 passes robustly here, authors caution that good performance may reflect instruction-tuning and exposure to relevant patterns rather than genuine social grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.33">
                <h3 class="extraction-instance">Extracted Data Instance 33 (e9055.33)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (comparison notes)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (overall findings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summary observation: GPT-4 is the top performer across ToM test batteries and typically outperforms other LLMs and the child baselines used here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (overall)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned model with superior performance on ToM-related standardized tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>340B (est.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Aggregate across Sally-Anne, Strange Stories, Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Collection of ToM-sensitive standardized tests probing first- and second-order false belief, non-literal language and intention inference, and recursive intentionality/memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Across tasks, GPT-4 often approaches or achieves near-perfect scores on Strange Stories, outperforms children across batteries, and uniquely passes the IM test robustly; its SA2 performance degrades under large deviations but still beats most models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children: SA1 = 0.45, SA2 = 0.225 (7-8y); IM memory >0.85 and intentionality declines after level 2 (9-10y groups).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 > other LLMs and > child baseline on most ToM measures in this study, though not invulnerable to test rewrites/novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>All inferences deterministic; prompts mirrored children's instructions for instruct-LLMs; deviations 0-2 applied to SA and SS; IM adapted and no deviations were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors emphasize that strong performance does not equate to human-like ToM; instruction-tuning and training data exposure likely explain much of the capability; model updates over time may change these results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.34">
                <h3 class="extraction-instance">Extracted Data Instance 34 (e9055.34)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grand (aggregate) - base-LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Base-LLMs (aggregate summary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate finding summarizing base-LLM behaviour (Falcon, LLaMA, GPT-davinci, BLOOM) across ToM tests: base models generally underperform compared to instruct-LLMs and children, particularly on recursive ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Base-LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models trained with self-supervised objectives without instruction tuning; evaluated using completion-style prompting in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–176B (varies by model)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Aggregate across Sally-Anne, Strange Stories, Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Set of ToM-relevant standardized tests probing false-belief, non-literal language, and recursive intentionality & memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall, base-LLMs mostly operate below child level across the three tests; some larger base models reached child-level on SA1 but not on SA2 or IM.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children: SA1 = 0.45, SA2 = 0.225 (7-8y); IM memory >0.85 with intentionality drop beyond level 2 (9-10y).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Aggregate: base-LLMs < instruct-LLMs and often < child baseline, especially for recursive ToM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Base-LLMs prompted as completion tasks; motivation prompts included the correct answer for some base-LLM trials to constrain output; IM closed questions used few-shot style examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Differences in prompt format vs instruct-LLMs and context-window limitations are confounds; authors caution against overinterpreting failure as absence of any underlying capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9055.35">
                <h3 class="extraction-instance">Extracted Data Instance 35 (e9055.35)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grand (aggregate) - instruct-LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction-tuned LLMs (aggregate summary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate finding summarizing instruct-LLM behaviour (GPT family, PaLM2, FLAN-T5, Falcon-instruct) across ToM tests: instruction-tuned variants outperform base models and often match or surpass child performance, with GPT-4 as best performer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruct-LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models that underwent instruction-tuning with human feedback, improving adherence to communicative formats and QA-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B–>340B (varies by model)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Aggregate across Sally-Anne, Strange Stories, Imposing Memory</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Battery of ToM-relevant standardized tests covering false-belief, non-literal language, and recursive intentionality/memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Instruction-tuned LLMs (notably GPT-4, GPT-3.5, PaLM2-chat, FLAN-T5) often match or exceed child baselines on Strange Stories and SA1; only GPT-4 consistently passes IM and remains above children on recursive intentionality.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children: SA1 = 0.45, SA2 = 0.225 (7-8y); IM memory >0.85 and intentionality drop beyond level 2 (9-10y).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Aggregate: instruct-LLMs > base-LLMs and often ≥ child baseline, with GPT-4 > all others; performance depends on model size/tuning and task type.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Instruct-LLMs prompted in QA/instruction-following format mirroring children's tasks; deterministic decoding; deviations used to probe generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Instruction-tuning appears crucial; authors propose analogy between instruction-tuning and cooperative communication shaping ToM-like behaviour, but caution that this is not evidence of human-like mental states.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theory of mind may have spontaneously emerged in large language models <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with GPT-4 <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks <em>(Rating: 2)</em></li>
                <li>Clever Hans or neural theory of mind? stress testing social reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Turning large language models into cognitive models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9055",
    "paper_id": "paper-264828854",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "Falcon-7B",
            "name_full": "Falcon (7B)",
            "brief_description": "An open base large language model (7B parameters) evaluated in its base form on ToM tests; trained with web-scale data and not instruction tuned in this paper's experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Falcon-7B (base)",
            "model_description": "Base transformer LLM (decoder-style) trained by the Falcon authors on web data; here used as a base-LLM (no instruction tuning).",
            "model_size": "7B",
            "test_battery_name": "Sally-Anne (first- and second-order)",
            "test_description": "Classic false-belief narratives assessing first-order (SA1) and second-order (SA2) Theory of Mind (belief attribution about another's belief).",
            "llm_performance": "Performed above child level on SA1 in some cases but at or below child level on SA2; struggled strongly when deviations increased (deviation 2 worsened performance). Exact numeric scores not reported in text for this model.",
            "human_baseline_performance": "Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225 (reported group means in paper).",
            "performance_comparison": "Falcon-7B is roughly comparable to children on first-order but below or at child level for second-order ToM; performance degrades with more novel deviations.",
            "experimental_details": "Inference with near-zero temperature; base-LLM prompted as text-completion (e.g. 'Sally will look for the ball in the '), deviations levels 0-2 used to probe generalization. Each question run independently to avoid in-context learning.",
            "limitations_or_caveats": "Base-LLM prompt-as-completion format differs from instruct-LLM QA format; prompt length/context window may limit performance; paper notes base models may be disadvantaged by prompt format and longer scenarios.",
            "uuid": "e9055.0",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B",
            "name_full": "Falcon (7B)",
            "brief_description": "An open base large language model (7B parameters) evaluated in its base form on ToM tests; trained with web-scale data and not instruction tuned in this paper's experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Falcon-7B (base)",
            "model_description": "Base transformer LLM (decoder-style) trained by the Falcon authors on web data; here used as a base-LLM (no instruction tuning).",
            "model_size": "7B",
            "test_battery_name": "Strange Stories",
            "test_description": "Seven short vignettes assessing comprehension of non-literal language (e.g., white lies, sarcasm, pretend) requiring inference of speakers' intentions (ToM-relevant).",
            "llm_performance": "Performed generally below child level across Strange Stories items, with less catastrophic decline on harder items than children but overall below many instruct-LLMs; deviation levels had little impact on most base-LLMs.",
            "human_baseline_performance": "Children (7-8y): performance declines across items as difficulty increases; specific numeric group means not reported for Strange Stories in text.",
            "performance_comparison": "Below child baseline overall for Strange Stories; did not match instruct-LLMs that often reached child level or above on these items.",
            "experimental_details": "Base-LLM prompted as completion tasks rather than instruction-following; deviations 0-2 applied; answers typed/completed as continuations rather than QA form.",
            "limitations_or_caveats": "Prompt format differences may suppress latent knowledge; available training data likely includes many nonliteral examples but base-LLM output format made assessment harder.",
            "uuid": "e9055.1",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B",
            "name_full": "Falcon (7B)",
            "brief_description": "An open base large language model (7B parameters) evaluated in its base form on ToM tests; trained with web-scale data and not instruction tuned in this paper's experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Falcon-7B (base)",
            "model_description": "Base transformer LLM (decoder-style) trained by the Falcon authors on web data; here used as a base-LLM (no instruction tuning).",
            "model_size": "7B",
            "test_battery_name": "Imposing Memory (IM)",
            "test_description": "Stories followed by true/false questions; includes 'intentionality' questions probing recursive levels of embedded beliefs (higher-order ToM) and matched memory questions assessing factual recall.",
            "llm_performance": "Performed below child level on both intentionality and memory questions across recursion levels; little variation except slightly better on some higher recursion levels for larger base models (not this one).",
            "human_baseline_performance": "Children (9-10y): memory question scores &gt; 0.85 across most levels; intentionality shows a significant drop after level 1→2 (paper reports β = −0.222, p &lt; .05).",
            "performance_comparison": "Falcon-7B is below human baseline on IM for both memory and intentionality questions.",
            "experimental_details": "For IM true/false closed questions base-LLMs were given two example continuations to constrain output format (e.g. '[correct]'/'[incorrect]') to push toward desired answer forms; all runs deterministic and independent.",
            "limitations_or_caveats": "IM prompts are long; base models with small context windows likely disadvantaged; paper notes prompt length/context limits may explain base-LLM struggles.",
            "uuid": "e9055.2",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B-I",
            "name_full": "Falcon-instruct (7B)",
            "brief_description": "An instruction-tuned 7B Falcon variant optimized for following instructions and conversational prompts; evaluated here as an instruct-LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Falcon-7B-I (instruct)",
            "model_description": "Instruction-tuned version of Falcon trained to follow natural-language instructions and aligned with human feedback.",
            "model_size": "7B",
            "test_battery_name": "Sally-Anne (first- and second-order)",
            "test_description": "First- and second-order false-belief narratives assessing belief attribution capacity.",
            "llm_performance": "The chat-optimized Falcon-7B-I failed broadly on second-order questions and performed worst overall among instruct-LLMs on SA; often at or below child level even on first-order when deviations were present.",
            "human_baseline_performance": "Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.",
            "performance_comparison": "Falcon-7B-I performs below child baseline on many SA2 questions and is among the weakest instruct-LLMs on Sally-Anne tests.",
            "experimental_details": "Instruct-LLMs prompted in QA format mirroring children's questionnaires; deterministic decoding; deviations introduced to test generalization.",
            "limitations_or_caveats": "Smaller model size and instruction tuning did not rescue second-order ToM; performance sensitive to deviation and complexity.",
            "uuid": "e9055.3",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B-I",
            "name_full": "Falcon-instruct (7B)",
            "brief_description": "An instruction-tuned 7B Falcon variant optimized for following instructions and conversational prompts; evaluated here as an instruct-LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Falcon-7B-I (instruct)",
            "model_description": "Instruction-tuned version of Falcon trained to follow natural-language instructions and aligned with human feedback.",
            "model_size": "7B",
            "test_battery_name": "Strange Stories",
            "test_description": "Seven vignettes assessing comprehension of non-literal language and inference of intentions (ToM-related).",
            "llm_performance": "Performed worst among instruct-LLMs on Strange Stories and generally below child level; smaller model size limited performance though deviation levels had limited additional effect.",
            "human_baseline_performance": "Children (7-8y): performance declines with item difficulty (no single mean provided in text).",
            "performance_comparison": "Below child baseline and below larger instruct-LLMs on Strange Stories.",
            "experimental_details": "Prompted in question-answer format close to children's test instructions; deterministic sampling; deviations 0-2 tested.",
            "limitations_or_caveats": "Smaller instruct-LLM; may lack capacity to fully leverage instruction tuning for pragmatic/non-literal inference.",
            "uuid": "e9055.4",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B-I",
            "name_full": "Falcon-instruct (7B)",
            "brief_description": "An instruction-tuned 7B Falcon variant optimized for following instructions and conversational prompts; evaluated here as an instruct-LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Falcon-7B-I (instruct)",
            "model_description": "Instruction-tuned version of Falcon trained to follow natural-language instructions and aligned with human feedback.",
            "model_size": "7B",
            "test_battery_name": "Imposing Memory (IM)",
            "test_description": "Stories plus true/false questions probing recursive intentionality and matched memory recall.",
            "llm_performance": "Performed below child level on both intentionality and memory questions across recursion levels; did not show the pattern of improvement seen in some larger instruct-LLMs.",
            "human_baseline_performance": "Children (9-10y): memory &gt; 0.85 across levels except a dip at level 4; intentionality showed a drop after level two.",
            "performance_comparison": "Falcon-7B-I is below human baseline on IM.",
            "experimental_details": "IM prompts are long and included voice-over/visual support for children (not for models); base/instruct prompt formatting controlled and deterministic.",
            "limitations_or_caveats": "Small instruction-tuned model; prompt length and context-window constraints likely a major limiting factor.",
            "uuid": "e9055.5",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-30B",
            "name_full": "LLaMA (30B)",
            "brief_description": "A large base transformer LLM (approx. 30B parameters) used as a base-LLM in evaluations; not instruction tuned in this paper's experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-30B (base)",
            "model_description": "Base LLaMA model (decoder-style transformer) trained on large text corpora; used here without instruction tuning.",
            "model_size": "30B",
            "test_battery_name": "Sally-Anne (first- and second-order)",
            "test_description": "First- and second-order false-belief tasks probing ToM abilities.",
            "llm_performance": "Performed above child level on first-order ToM (SA1) but fell at or below child level on second-order ToM (SA2); performance decreased with deviations and recursion.",
            "human_baseline_performance": "Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.",
            "performance_comparison": "LLaMA-30B can match or exceed children on SA1 but is below or similar to children for SA2.",
            "experimental_details": "Base-LLM prompted as text-completion; near-zero temperature; deviations 0-2 included; motivation questions formatted with correct answer for base-LLMs to encourage sensible continuation.",
            "limitations_or_caveats": "Base-completion prompting may advantage or disadvantage the model relative to QA format; no instruction tuning possibly limits application of latent knowledge to explicit QA.",
            "uuid": "e9055.6",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-30B",
            "name_full": "LLaMA (30B)",
            "brief_description": "A large base transformer LLM (approx. 30B parameters) used as a base-LLM in evaluations; not instruction tuned in this paper's experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-30B (base)",
            "model_description": "Base LLaMA model (decoder-style transformer) trained on large text corpora; used here without instruction tuning.",
            "model_size": "30B",
            "test_battery_name": "Strange Stories",
            "test_description": "Vignettes assessing non-literal language understanding and intention inference.",
            "llm_performance": "Base LLaMA performed below child level on Strange Stories overall, with little sensitivity to deviations.",
            "human_baseline_performance": "Children (7-8y): performance declines across items as difficulty increases; exact numerical mean not provided for Strange Stories in text.",
            "performance_comparison": "Below child baseline on Strange Stories.",
            "experimental_details": "Prompts given as text completions; deviation levels tested; answers extracted from completions.",
            "limitations_or_caveats": "Base-completion prompt format may hide latent abilities for non-literal language present in training data.",
            "uuid": "e9055.7",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-30B",
            "name_full": "LLaMA (30B)",
            "brief_description": "A large base transformer LLM (approx. 30B parameters) used as a base-LLM in evaluations; not instruction tuned in this paper's experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-30B (base)",
            "model_description": "Base LLaMA model (decoder-style transformer) trained on large text corpora; used here without instruction tuning.",
            "model_size": "30B",
            "test_battery_name": "Imposing Memory (IM)",
            "test_description": "Stories followed by true/false items probing recursive intentionality and memory recall.",
            "llm_performance": "Performed below child level across both intentionality and memory questions; larger base models showed slight improvement on higher recursion but not to child level.",
            "human_baseline_performance": "Children (9-10y): memory questions ~&gt;0.85; intentionality drops significantly after level 2.",
            "performance_comparison": "Below human baseline on IM.",
            "experimental_details": "Closed IM questions presented as completions for base-LLMs with helper examples to constrain output ('[correct]'/'[incorrect]').",
            "limitations_or_caveats": "Long IM prompts may exceed effective context for some base models; method of forcing output format may bias responses.",
            "uuid": "e9055.8",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-davinci",
            "name_full": "GPT-3 (text-davinci-002/003; 175B)",
            "brief_description": "A large autoregressive transformer base-LLM (175B parameters) originating from the GPT-3 family; used in base and instruct variants across LLM research.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-davinci (base)",
            "model_description": "GPT-3 era large decoder-only transformer (175B) trained with self-supervised objectives; in this paper used in a base completion format (text-davinci variant).",
            "model_size": "175B",
            "test_battery_name": "Sally-Anne (first- and second-order)",
            "test_description": "First- and second-order false-belief stories testing ToM.",
            "llm_performance": "Reported structurally low scores compared to expectations and inconsistent with Kosinski (2023); performed above child level on some SA1 items but low on SA2; prompting as open questions (not completion) likely impacted results.",
            "human_baseline_performance": "Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.",
            "performance_comparison": "Overall below top instruct-LLMs; sometimes above child on SA1 but at or below child level on SA2.",
            "experimental_details": "Base-LLM prompted as text-completion; for motivation prompts the correct experimental answer was inserted to encourage coherent continuation; deterministic decoding used.",
            "limitations_or_caveats": "Different prompting choice (open QA vs. completion) compared to prior work likely explains discrepancies; paper explicitly notes this.",
            "uuid": "e9055.9",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-davinci",
            "name_full": "GPT-3 (text-davinci-002/003; 175B)",
            "brief_description": "A large autoregressive transformer base-LLM (175B parameters) originating from the GPT-3 family; used in base and instruct variants across LLM research.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-davinci (base)",
            "model_description": "GPT-3 era large decoder-only transformer (175B) trained with self-supervised objectives; in this paper used in a base completion format (text-davinci variant).",
            "model_size": "175B",
            "test_battery_name": "Strange Stories",
            "test_description": "Vignettes assessing non-literal language comprehension and intention inference.",
            "llm_performance": "GPT-davinci (when treated as base) performed below child level on Strange Stories generally; instruct-tuned descendants performed better (see GPT-3.5/GPT-4 entries).",
            "human_baseline_performance": "Children (7-8y): observed decline with item difficulty; numeric group mean not specified in text.",
            "performance_comparison": "Below child baseline on Strange Stories in this base completion prompting setup.",
            "experimental_details": "Prompted as completion tasks without QA instruction-following; outputs constrained via example continuations for IM but not for SS.",
            "limitations_or_caveats": "Prompt format differences relative to instruct models limit comparability.",
            "uuid": "e9055.10",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-davinci",
            "name_full": "GPT-3 (text-davinci-002/003; 175B)",
            "brief_description": "A large autoregressive transformer base-LLM (175B parameters) originating from the GPT-3 family; used in base and instruct variants across LLM research.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-davinci (base)",
            "model_description": "GPT-3 era large decoder-only transformer (175B) trained with self-supervised objectives; in this paper used in a base completion format (text-davinci variant).",
            "model_size": "175B",
            "test_battery_name": "Imposing Memory (IM)",
            "test_description": "Stories plus true/false questions probing recursive intentionality and memory.",
            "llm_performance": "Performed below child level on both intentionality and memory items; overall base-LLMs fared poorly on IM relative to instruct-LLMs.",
            "human_baseline_performance": "Children (9-10y): memory questions generally &gt; 0.85; intentionality drops after level 2.",
            "performance_comparison": "Below human baseline on IM.",
            "experimental_details": "Two example continuations were provided to base-LLMs to encourage '[correct]'/'[incorrect]' style outputs for closed IM questions; deterministic decoding.",
            "limitations_or_caveats": "The example-continuation technique constrains outputs but may not fully reflect model reasoning; prompt length and format matter.",
            "uuid": "e9055.11",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "BLOOM-176B",
            "name_full": "BLOOM (176B)",
            "brief_description": "A very large open-access base LLM (~176B parameters) trained on multilingual web data and evaluated as a base-LLM in the study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BLOOM-176B (base)",
            "model_description": "Open multilingual transformer model (176B params) trained on large multilingual corpora; used here in base form (no instruction tuning).",
            "model_size": "176B",
            "test_battery_name": "Sally-Anne (first- and second-order)",
            "test_description": "First- and second-order false-belief narratives testing Theory of Mind.",
            "llm_performance": "One of the base-LLMs performing above child level on first-order ToM (SA1) but at or below child level on second-order (SA2); struggled more with deviations and higher-order recursion.",
            "human_baseline_performance": "Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.",
            "performance_comparison": "BLOOM can exceed child performance on SA1 but not on SA2; overall below best instruct-LLMs.",
            "experimental_details": "Prompts given as text-completion; motivation prompts for base models inserted correct experimental answer to elicit reasoned continuation.",
            "limitations_or_caveats": "Base completion format and lack of instruction tuning may have limited performance on explicit QA-style ToM questions.",
            "uuid": "e9055.12",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "BLOOM-176B",
            "name_full": "BLOOM (176B)",
            "brief_description": "A very large open-access base LLM (~176B parameters) trained on multilingual web data and evaluated as a base-LLM in the study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BLOOM-176B (base)",
            "model_description": "Open multilingual transformer model (176B params) trained on large multilingual corpora; used here in base form (no instruction tuning).",
            "model_size": "176B",
            "test_battery_name": "Strange Stories",
            "test_description": "Seven social vignettes probing non-literal language comprehension and intention inference.",
            "llm_performance": "Performed below child level overall on Strange Stories, similar to other base-LLMs; deviations had little effect.",
            "human_baseline_performance": "Children (7-8y): performance declines with item difficulty; no single mean provided in text.",
            "performance_comparison": "Below child baseline on Strange Stories in this experiment.",
            "experimental_details": "Base completion prompting; deterministic decoding; deviations 0-2 applied.",
            "limitations_or_caveats": "Base format may hide potential comprehension capabilities present in training data.",
            "uuid": "e9055.13",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "BLOOM-176B",
            "name_full": "BLOOM (176B)",
            "brief_description": "A very large open-access base LLM (~176B parameters) trained on multilingual web data and evaluated as a base-LLM in the study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BLOOM-176B (base)",
            "model_description": "Open multilingual transformer model (176B params) trained on large multilingual corpora; used here in base form (no instruction tuning).",
            "model_size": "176B",
            "test_battery_name": "Imposing Memory (IM)",
            "test_description": "Stories plus true/false questions probing recursive intentionality and matched memory recall.",
            "llm_performance": "Performed below child level on both intentionality and memory questions across recursion levels; did not approach GPT-4 level performance.",
            "human_baseline_performance": "Children (9-10y): high memory scores (&gt;0.85) and drop in intentionality after level two.",
            "performance_comparison": "Below human baseline on IM.",
            "experimental_details": "Base-LLM outputs guided via example continuations for closed IM items; deterministic sampling.",
            "limitations_or_caveats": "Long prompts and lack of instruction tuning likely constrained performance.",
            "uuid": "e9055.14",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Flan-T5-11B",
            "name_full": "FLAN-T5 (11B, instruction-tuned)",
            "brief_description": "An instruction-tuned encoder-decoder transformer (FLAN-T5 family) used as an instruct-LLM; relatively small in parameter count but instruction-tuned to follow prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5 (instruct)",
            "model_description": "FLAN-T5 instruction-tuned variant (11B params) optimized to follow natural-language instructions and perform zero-shot tasks.",
            "model_size": "11B",
            "test_battery_name": "Sally-Anne (first- and second-order)",
            "test_description": "First- and second-order false-belief tasks assessing ToM.",
            "llm_performance": "Smaller instruct-LLM with mixed SA performance: often below larger GPT-family instruct models, struggled on second-order ToM and with deviations; generally poorer than GPT-3.5/GPT-4 on SA2.",
            "human_baseline_performance": "Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.",
            "performance_comparison": "Below leading instruct-LLMs and often below child level on SA2; sometimes near child level on SA1.",
            "experimental_details": "Prompted in QA format similar to children's instructions; deterministic decoding; deviations applied.",
            "limitations_or_caveats": "Though instruction-tuned, smaller parameter count likely limited recursive ToM performance; deviation-robustness weaker than large instruct models.",
            "uuid": "e9055.15",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Flan-T5-11B",
            "name_full": "FLAN-T5 (11B, instruction-tuned)",
            "brief_description": "An instruction-tuned encoder-decoder transformer (FLAN-T5 family) used as an instruct-LLM; relatively small in parameter count but instruction-tuned to follow prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5 (instruct)",
            "model_description": "FLAN-T5 instruction-tuned variant (11B params) optimized to follow natural-language instructions and perform zero-shot tasks.",
            "model_size": "11B",
            "test_battery_name": "Strange Stories",
            "test_description": "Seven vignettes probing non-literal language understanding requiring inference of intentions.",
            "llm_performance": "Outperformed some larger models (e.g., PaLM) on harder Strange Stories items and surpassed child level earlier than PaLM in some cases; overall performed well relative to its size.",
            "human_baseline_performance": "Children (7-8y): performance declines with item difficulty; no single mean provided.",
            "performance_comparison": "FLAN-T5 matched or surpassed child performance on several Strange Stories items, especially the more complex ones.",
            "experimental_details": "QA-format prompts; deviations levels applied; deterministic sampling.",
            "limitations_or_caveats": "Smaller model but instruction tuning yields better pragmatic/non-literal performance than some larger base models; results may not generalize to other ToM paradigms (e.g., recursive belief tasks).",
            "uuid": "e9055.16",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Flan-T5-11B",
            "name_full": "FLAN-T5 (11B, instruction-tuned)",
            "brief_description": "An instruction-tuned encoder-decoder transformer (FLAN-T5 family) used as an instruct-LLM; relatively small in parameter count but instruction-tuned to follow prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5 (instruct)",
            "model_description": "FLAN-T5 instruction-tuned variant (11B params) optimized to follow natural-language instructions and perform zero-shot tasks.",
            "model_size": "11B",
            "test_battery_name": "Imposing Memory (IM)",
            "test_description": "Stories plus true/false items probing recursive intentionality (higher-order ToM) and memory recall.",
            "llm_performance": "Surprisingly increased performance with higher recursion levels and ended at approximately child level for IM; performed better on IM than many other instruct-LLMs despite small size.",
            "human_baseline_performance": "Children (9-10y): memory &gt; 0.85; intentionality declines after level two.",
            "performance_comparison": "FLAN-T5 reached child-level performance on IM by the highest recursion levels but was not consistently above children across all levels.",
            "experimental_details": "Instruction-tuned QA prompts; IM closed questions used yes/no format for children and tailored output format for models; deterministic inference.",
            "limitations_or_caveats": "Pattern of increasing performance with recursion is atypical and may reflect dataset or prompt interactions rather than genuine recursive reasoning; paper notes need for further exploration.",
            "uuid": "e9055.17",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3 (text-davinci-003)",
            "name_full": "GPT-3 (text-davinci-003, instruct-tuned variant)",
            "brief_description": "A 175B-parameter GPT-3 family model (text-davinci) that has instruction-tuned variants; used here as an instruct-LLM in the study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003; instruct)",
            "model_description": "GPT-3 family model (175B) with instruction-tuning (text-davinci variants) trained to follow prompts and produce coherent responses.",
            "model_size": "175B",
            "test_battery_name": "Sally-Anne (first- and second-order)",
            "test_description": "First- and second-order false-belief narratives probing ToM.",
            "llm_performance": "Performed at or near child level on simpler items (SA1) but showed decline on SA2; overall better than many base-LLMs but worse than GPT-3.5 and GPT-4 on SA2 and on deviations.",
            "human_baseline_performance": "Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.",
            "performance_comparison": "Comparable to children on some first-order items but below strongest instruct-LLMs for second-order ToM.",
            "experimental_details": "Instruct-LLM prompted in QA format similar to children's tests; deterministic sampling; deviations introduced.",
            "limitations_or_caveats": "Performance sensitive to deviation; instruction tuning helped relative to base GPT-3 but not sufficient for robust second-order ToM.",
            "uuid": "e9055.18",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3 (text-davinci-003)",
            "name_full": "GPT-3 (text-davinci-003, instruct-tuned variant)",
            "brief_description": "A 175B-parameter GPT-3 family model (text-davinci) that has instruction-tuned variants; used here as an instruct-LLM in the study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003; instruct)",
            "model_description": "GPT-3 family model (175B) with instruction-tuning (text-davinci variants) trained to follow prompts and produce coherent responses.",
            "model_size": "175B",
            "test_battery_name": "Strange Stories",
            "test_description": "Vignettes testing non-literal language understanding and intention inference.",
            "llm_performance": "Performed at or close to child level on easier Strange Stories items (item 1), with some decline on harder items but overall stayed well above child level relative to base models; deviation levels had little effect for this model.",
            "human_baseline_performance": "Children (7-8y): decreasing performance with item difficulty; exact numeric mean not provided.",
            "performance_comparison": "At or above child level on several Strange Stories items, though behind GPT-4.",
            "experimental_details": "Prompted in QA style; deviations 0-2 tested; deterministic decoding.",
            "limitations_or_caveats": "Not as robust as GPT-4 on the full set; performance drops with increasing item difficulty.",
            "uuid": "e9055.19",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3 (text-davinci-003)",
            "name_full": "GPT-3 (text-davinci-003, instruct-tuned variant)",
            "brief_description": "A 175B-parameter GPT-3 family model (text-davinci) that has instruction-tuned variants; used here as an instruct-LLM in the study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003; instruct)",
            "model_description": "GPT-3 family model (175B) with instruction-tuning (text-davinci variants) trained to follow prompts and produce coherent responses.",
            "model_size": "175B",
            "test_battery_name": "Imposing Memory (IM)",
            "test_description": "Stories plus true/false items probing recursive intentionality and memory recall.",
            "llm_performance": "Performed below child level on IM overall; not among the better-performing instruct-LLMs for recursive intentionality questions.",
            "human_baseline_performance": "Children (9-10y): memory &gt;0.85; intentionality performance drops after level two.",
            "performance_comparison": "Below human baseline on IM.",
            "experimental_details": "IM closed questions presented in yes/no form for children; model prompted similarly in QA format; deterministic decoding.",
            "limitations_or_caveats": "Instruction tuning improved some pragmatic capacities but not recursive ToM required by IM.",
            "uuid": "e9055.20",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (175B family instruct-tuned)",
            "brief_description": "A GPT-3.5 family instruct-tuned model optimized for chat/interactive tasks and instruction following; evaluated as an instruct-LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "An instruct-optimized GPT-3.5 family model (approx. 175B scale per paper sources) trained with human feedback to follow instructions and produce conversational outputs.",
            "model_size": "175B (approx.)",
            "test_battery_name": "Sally-Anne (first- and second-order)",
            "test_description": "First- and second-order false-belief tasks assessing ToM.",
            "llm_performance": "Performed well above child level on first-order ToM and remained above child level on second-order ToM in some conditions, though performance degraded substantially with deviation level 2 (novel scenarios).",
            "human_baseline_performance": "Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.",
            "performance_comparison": "GT P-3.5 generally outperforms children on SA1 and can stay above children on SA2 in original formulations but loses robustness under substantial deviations.",
            "experimental_details": "Instruct-LLM QA prompts matching children's instructions; deterministic decoding; deviations 0-2 tested to measure generalization.",
            "limitations_or_caveats": "Robustness to deviations is limited; performance falls with scenario rewrites and increased novelty.",
            "uuid": "e9055.21",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (175B family instruct-tuned)",
            "brief_description": "A GPT-3.5 family instruct-tuned model optimized for chat/interactive tasks and instruction following; evaluated as an instruct-LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "An instruct-optimized GPT-3.5 family model (approx. 175B scale per paper sources) trained with human feedback to follow instructions and produce conversational outputs.",
            "model_size": "175B (approx.)",
            "test_battery_name": "Strange Stories",
            "test_description": "Vignettes assessing non-literal language understanding and inference of intentions.",
            "llm_performance": "Performed at or close to child level on early items and stayed well above child level on many items; overall strong performance though not as high as GPT-4 which approached perfect scores.",
            "human_baseline_performance": "Children (7-8y): performance declines as item difficulty increases.",
            "performance_comparison": "GPT-3.5 generally outperforms or matches child baseline on Strange Stories and is stronger than most base-LLMs.",
            "experimental_details": "QA-style prompts; deviations had little effect on larger instruct-LLMs including GPT-3.5; deterministic decoding.",
            "limitations_or_caveats": "Less robust than GPT-4 across all tasks; IM performance notably worse than GPT-4.",
            "uuid": "e9055.22",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (175B family instruct-tuned)",
            "brief_description": "A GPT-3.5 family instruct-tuned model optimized for chat/interactive tasks and instruction following; evaluated as an instruct-LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "An instruct-optimized GPT-3.5 family model (approx. 175B scale per paper sources) trained with human feedback to follow instructions and produce conversational outputs.",
            "model_size": "175B (approx.)",
            "test_battery_name": "Imposing Memory (IM)",
            "test_description": "Stories and true/false questions probing recursive intentionality and matched memory recall.",
            "llm_performance": "Performed worst of the instruct-LLMs on IM in this study, generally below child level on intentionality and memory items and did not display the robust recursive ToM capabilities of GPT-4.",
            "human_baseline_performance": "Children (9-10y): strong memory (&gt;0.85) and drop in intentionality beyond level two.",
            "performance_comparison": "Below human baseline and below GPT-4 on IM.",
            "experimental_details": "QA-format prompts; deterministic decoding; IM presented as closed yes/no items for models too.",
            "limitations_or_caveats": "Shows that instruction tuning alone is not sufficient for robust recursive ToM; model-specific limitations observed.",
            "uuid": "e9055.23",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2",
            "name_full": "PaLM 2",
            "brief_description": "Google's PaLM 2 family of large language models (estimated 175-340B parameters) evaluated both in base and chat variants; here PaLM2 (non-chat) used as an instruct-LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM2 (instruct)",
            "model_description": "Large transformer LLM from Google (PaLM 2 family), instruction-tuned and evaluated in this study; exact parameter count not disclosed in paper (est. 175–340B).",
            "model_size": "175-340B (est.)",
            "test_battery_name": "Sally-Anne (first- and second-order)",
            "test_description": "First- and second-order false-belief tasks assessing Theory of Mind.",
            "llm_performance": "Performed well on first-order SA1 (often above child level) but showed perturbation on second-order SA2; some PaLM2 variants (e.g., PaLM2-chat) were more stable than non-chat PaLM2 on SA2 but still degraded under deviations.",
            "human_baseline_performance": "Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.",
            "performance_comparison": "PaLM2 can match or exceed child performance on SA1 but struggles with SA2 and novel deviations relative to top GPT-family instruct models.",
            "experimental_details": "Instruct-LLM QA prompts; deviations 0-2; deterministic decoding.",
            "limitations_or_caveats": "PaLM2 non-chat variant less robust than PaLM2-chat and GPT-4; deviation sensitivity suggests limited generalization in certain SA2 contexts.",
            "uuid": "e9055.24",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2",
            "name_full": "PaLM 2",
            "brief_description": "Google's PaLM 2 family of large language models (estimated 175-340B parameters) evaluated both in base and chat variants; here PaLM2 (non-chat) used as an instruct-LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM2 (instruct)",
            "model_description": "Large transformer LLM from Google (PaLM 2 family), instruction-tuned and evaluated in this study; exact parameter count not disclosed in paper (est. 175–340B).",
            "model_size": "175-340B (est.)",
            "test_battery_name": "Strange Stories",
            "test_description": "Vignettes assessing non-literal language understanding and intention inference.",
            "llm_performance": "PaLM2 performed well on Strange Stories; larger PaLM2 models matched or exceeded child level on many items though PaLM2-chat and PaLM2 had different strengths (PaLM2-chat often stronger). Deviation levels had little effect on larger models.",
            "human_baseline_performance": "Children (7-8y): decreasing performance across items with difficulty.",
            "performance_comparison": "PaLM2 matched or outperformed child baseline on Strange Stories items, though GPT-4 remained superior.",
            "experimental_details": "QA-format prompts; deviations 0-2; deterministic decoding.",
            "limitations_or_caveats": "Exact parameterization and training details undisclosed; PaLM2-chat variant generally more robust than non-chat.",
            "uuid": "e9055.25",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2",
            "name_full": "PaLM 2",
            "brief_description": "Google's PaLM 2 family of large language models (estimated 175-340B parameters) evaluated both in base and chat variants; here PaLM2 (non-chat) used as an instruct-LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM2 (instruct)",
            "model_description": "Large transformer LLM from Google (PaLM 2 family), instruction-tuned and evaluated in this study; exact parameter count not disclosed in paper (est. 175–340B).",
            "model_size": "175-340B (est.)",
            "test_battery_name": "Imposing Memory (IM)",
            "test_description": "Stories plus true/false items probing recursive intentionality and memory.",
            "llm_performance": "PaLM2 performed below child level on IM overall, with instruct-LLMs generally failing IM except for GPT-4; PaLM2-chat performed better than non-chat PaLM2 on some tasks but did not match children on IM recursive intentionality.",
            "human_baseline_performance": "Children (9-10y): strong memory performance and drop-off in intentionality after level two.",
            "performance_comparison": "Below human baseline on IM for recursive intentionality; PaLM2 variants did not reach child-level robustness on IM except marginal improvements in chat variant for some items.",
            "experimental_details": "QA-format prompts; IM closed questions presented similarly for models and children; deterministic decoding.",
            "limitations_or_caveats": "IM is a challenging robustness test; PaLM2's undisclosed training details limit interpretation.",
            "uuid": "e9055.26",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2-chat",
            "name_full": "PaLM 2 - chat variant",
            "brief_description": "Chat-optimized PaLM2 variant tuned for conversational instruction following; evaluated as an instruct-LLM and generally stronger than non-chat PaLM2 on conversational tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM2-chat (instruct)",
            "model_description": "PaLM2 family chat-optimized variant (est. 175–340B) instruction-tuned for dialog and QA tasks; used in this study on ToM batteries.",
            "model_size": "175-340B (est.)",
            "test_battery_name": "Sally-Anne (first- and second-order)",
            "test_description": "First- and second-order false-belief tasks assessing ToM.",
            "llm_performance": "PaLM2-chat performed well on SA1 but showed perturbation on SA2; second-order questions and deviations reduced performance, though PaLM2-chat was more robust than PaLM2 non-chat in some cases.",
            "human_baseline_performance": "Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.",
            "performance_comparison": "PaLM2-chat often above child level on SA1 and sometimes on SA2 in original formulations, but loses ground under strong deviations compared to GPT-3.5/GPT-4.",
            "experimental_details": "Prompted in QA conversational format; deterministic decoding; deviations 0-2 employed.",
            "limitations_or_caveats": "Although chat-optimized, PaLM2-chat did not match GPT-4's robustness on SA2 with deviations.",
            "uuid": "e9055.27",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2-chat",
            "name_full": "PaLM 2 - chat variant",
            "brief_description": "Chat-optimized PaLM2 variant tuned for conversational instruction following; evaluated as an instruct-LLM and generally stronger than non-chat PaLM2 on conversational tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM2-chat (instruct)",
            "model_description": "PaLM2 family chat-optimized variant (est. 175–340B) instruction-tuned for dialog and QA tasks; used in this study on ToM batteries.",
            "model_size": "175-340B (est.)",
            "test_battery_name": "Strange Stories",
            "test_description": "Vignettes probing non-literal language comprehension and intention inference. ",
            "llm_performance": "PaLM2-chat matched or exceeded child performance on Strange Stories and showed little effect from deviations for larger instruct-LLMs, indicating robust non-literal language handling.",
            "human_baseline_performance": "Children (7-8y): expected decline with increasing item difficulty (no single mean provided).",
            "performance_comparison": "PaLM2-chat generally matched or surpassed child baseline on Strange Stories items, though GPT-4 remained the top performer.",
            "experimental_details": "QA-format instruction prompts; deviations 0-2 included; deterministic decoding.",
            "limitations_or_caveats": "Performance likely aided by exposure to pragmatic and conversational examples in pretraining and instruction data; exact training details undisclosed.",
            "uuid": "e9055.28",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2-chat",
            "name_full": "PaLM 2 - chat variant",
            "brief_description": "Chat-optimized PaLM2 variant tuned for conversational instruction following; evaluated as an instruct-LLM and generally stronger than non-chat PaLM2 on conversational tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM2-chat (instruct)",
            "model_description": "PaLM2 family chat-optimized variant (est. 175–340B) instruction-tuned for dialog and QA tasks; used in this study on ToM batteries.",
            "model_size": "175-340B (est.)",
            "test_battery_name": "Imposing Memory (IM)",
            "test_description": "Stories plus true/false items probing recursive intentionality and memory recall.",
            "llm_performance": "PaLM2-chat performed better than PaLM2 non-chat on some aspects but did not reach child-level robustness on IM; GPT-4 remained the only model consistently above child level on IM after second-order intentionality.",
            "human_baseline_performance": "Children (9-10y): memory &gt;0.85; intentionality drops after level two.",
            "performance_comparison": "Below human baseline on IM for recursive intentionality despite being stronger than many other instruct-LLMs.",
            "experimental_details": "QA conversational prompts; deterministic decoding; IM closed format testing.",
            "limitations_or_caveats": "IM's recursive intentionality places high demands that only GPT-4 met consistently in this study.",
            "uuid": "e9055.29",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's GPT-4, a large instruction-tuned transformer model (estimated &gt;340B parameters per paper) and the top-performing model in the study across ToM batteries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large instruction-tuned transformer model from OpenAI; parameter count undisclosed in paper (reported &gt;340B estimate); trained with reinforcement learning from human feedback and optimized for instruction following and dialogue.",
            "model_size": "&gt;340B (est.)",
            "test_battery_name": "Sally-Anne (first- and second-order)",
            "test_description": "First- and second-order false-belief tasks assessing Theory of Mind abilities.",
            "llm_performance": "GPT-4 outperformed all other models and often outperformed children; remained above child level on SA2 in original scenarios but performance decreased substantially with extreme deviations (deviation level 2), though it still outperformed most other models.",
            "human_baseline_performance": "Children (7-8y): SA1 mean = 0.45, SA2 mean = 0.225.",
            "performance_comparison": "GPT-4 typically outperforms the child baseline and other LLMs on Sally-Anne tests, though robustness to large deviations is imperfect.",
            "experimental_details": "Instruct-LLM QA prompts close to children's instructions; deterministic decoding (temperature ≈ 0); deviations 0-2 probed generalization; each question run independently.",
            "limitations_or_caveats": "Passing ToM tests does not imply human-like ToM; updates to GPT-family models can change capabilities over time; instruction-tuning and dataset exposure likely crucial to its performance.",
            "uuid": "e9055.30",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's GPT-4, a large instruction-tuned transformer model (estimated &gt;340B parameters per paper) and the top-performing model in the study across ToM batteries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large instruction-tuned transformer model from OpenAI; parameter count undisclosed in paper (reported &gt;340B estimate); trained with reinforcement learning from human feedback and optimized for instruction following and dialogue.",
            "model_size": "&gt;340B (est.)",
            "test_battery_name": "Strange Stories",
            "test_description": "Seven vignettes probing non-literal language comprehension, such as white lies, sarcasm, and pretence, requiring inference of speakers' intentions.",
            "llm_performance": "GPT-4 approached near-perfect scores across Strange Stories items and was robust to deviations; significantly outperformed most other models and children on many non-literal items.",
            "human_baseline_performance": "Children (7-8y): performance declines with item difficulty; exact numeric mean not provided for Strange Stories.",
            "performance_comparison": "GPT-4 outperforms child baseline and other LLMs on Strange Stories, approaching perfect understanding of non-literal intents in tested scenarios.",
            "experimental_details": "QA-format prompts mirroring children's instructions; deterministic decoding; deviations 0-2 had little effect for GPT-4 on Strange Stories.",
            "limitations_or_caveats": "High performance likely influenced by large exposure to conversational/non-literal text in training and instruction tuning; note that high scores do not prove human-like ToM or social grounding.",
            "uuid": "e9055.31",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's GPT-4, a large instruction-tuned transformer model (estimated &gt;340B parameters per paper) and the top-performing model in the study across ToM batteries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large instruction-tuned transformer model from OpenAI; parameter count undisclosed in paper (reported &gt;340B estimate); trained with reinforcement learning from human feedback and optimized for instruction following and dialogue.",
            "model_size": "&gt;340B (est.)",
            "test_battery_name": "Imposing Memory (IM)",
            "test_description": "Stories followed by a battery of true/false questions including recursively nested intentionality items probing higher-order ToM and matched memory questions.",
            "llm_performance": "GPT-4 was the only model that performed consistently well across IM levels and remained above child level after second-order intentionality; passed IM most robustly among all models.",
            "human_baseline_performance": "Children (9-10y): memory &gt; 0.85 across most levels; intentionality shows a drop after level two (statistically significant difference between level 2 and 1: β = −0.222, p &lt; .05).",
            "performance_comparison": "GPT-4 outperforms children on IM across many levels and is uniquely robust on recursive intentionality compared to other LLMs tested.",
            "experimental_details": "IM closed questions administered; for models, deterministic decoding and instruction-style prompts used; deviations not relevant for IM (no deviations applied to this unpublished adaptation).",
            "limitations_or_caveats": "IM is a newly adapted test in this study; while GPT-4 passes robustly here, authors caution that good performance may reflect instruction-tuning and exposure to relevant patterns rather than genuine social grounding.",
            "uuid": "e9055.32",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 (comparison notes)",
            "name_full": "GPT-4 (overall findings)",
            "brief_description": "Summary observation: GPT-4 is the top performer across ToM test batteries and typically outperforms other LLMs and the child baselines used here.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (overall)",
            "model_description": "Large instruction-tuned model with superior performance on ToM-related standardized tasks in this paper.",
            "model_size": "&gt;340B (est.)",
            "test_battery_name": "Aggregate across Sally-Anne, Strange Stories, Imposing Memory",
            "test_description": "Collection of ToM-sensitive standardized tests probing first- and second-order false belief, non-literal language and intention inference, and recursive intentionality/memory.",
            "llm_performance": "Across tasks, GPT-4 often approaches or achieves near-perfect scores on Strange Stories, outperforms children across batteries, and uniquely passes the IM test robustly; its SA2 performance degrades under large deviations but still beats most models.",
            "human_baseline_performance": "Children: SA1 = 0.45, SA2 = 0.225 (7-8y); IM memory &gt;0.85 and intentionality declines after level 2 (9-10y groups).",
            "performance_comparison": "GPT-4 &gt; other LLMs and &gt; child baseline on most ToM measures in this study, though not invulnerable to test rewrites/novelty.",
            "experimental_details": "All inferences deterministic; prompts mirrored children's instructions for instruct-LLMs; deviations 0-2 applied to SA and SS; IM adapted and no deviations were applied.",
            "limitations_or_caveats": "Authors emphasize that strong performance does not equate to human-like ToM; instruction-tuning and training data exposure likely explain much of the capability; model updates over time may change these results.",
            "uuid": "e9055.33",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Grand (aggregate) - base-LLMs",
            "name_full": "Base-LLMs (aggregate summary)",
            "brief_description": "Aggregate finding summarizing base-LLM behaviour (Falcon, LLaMA, GPT-davinci, BLOOM) across ToM tests: base models generally underperform compared to instruct-LLMs and children, particularly on recursive ToM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Base-LLMs (aggregate)",
            "model_description": "Models trained with self-supervised objectives without instruction tuning; evaluated using completion-style prompting in this study.",
            "model_size": "7B–176B (varies by model)",
            "test_battery_name": "Aggregate across Sally-Anne, Strange Stories, Imposing Memory",
            "test_description": "Set of ToM-relevant standardized tests probing false-belief, non-literal language, and recursive intentionality & memory.",
            "llm_performance": "Overall, base-LLMs mostly operate below child level across the three tests; some larger base models reached child-level on SA1 but not on SA2 or IM.",
            "human_baseline_performance": "Children: SA1 = 0.45, SA2 = 0.225 (7-8y); IM memory &gt;0.85 with intentionality drop beyond level 2 (9-10y).",
            "performance_comparison": "Aggregate: base-LLMs &lt; instruct-LLMs and often &lt; child baseline, especially for recursive ToM tasks.",
            "experimental_details": "Base-LLMs prompted as completion tasks; motivation prompts included the correct answer for some base-LLM trials to constrain output; IM closed questions used few-shot style examples.",
            "limitations_or_caveats": "Differences in prompt format vs instruct-LLMs and context-window limitations are confounds; authors caution against overinterpreting failure as absence of any underlying capability.",
            "uuid": "e9055.34",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Grand (aggregate) - instruct-LLMs",
            "name_full": "Instruction-tuned LLMs (aggregate summary)",
            "brief_description": "Aggregate finding summarizing instruct-LLM behaviour (GPT family, PaLM2, FLAN-T5, Falcon-instruct) across ToM tests: instruction-tuned variants outperform base models and often match or surpass child performance, with GPT-4 as best performer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruct-LLMs (aggregate)",
            "model_description": "Models that underwent instruction-tuning with human feedback, improving adherence to communicative formats and QA-style tasks.",
            "model_size": "11B–&gt;340B (varies by model)",
            "test_battery_name": "Aggregate across Sally-Anne, Strange Stories, Imposing Memory",
            "test_description": "Battery of ToM-relevant standardized tests covering false-belief, non-literal language, and recursive intentionality/memory.",
            "llm_performance": "Instruction-tuned LLMs (notably GPT-4, GPT-3.5, PaLM2-chat, FLAN-T5) often match or exceed child baselines on Strange Stories and SA1; only GPT-4 consistently passes IM and remains above children on recursive intentionality.",
            "human_baseline_performance": "Children: SA1 = 0.45, SA2 = 0.225 (7-8y); IM memory &gt;0.85 and intentionality drop beyond level 2 (9-10y).",
            "performance_comparison": "Aggregate: instruct-LLMs &gt; base-LLMs and often ≥ child baseline, with GPT-4 &gt; all others; performance depends on model size/tuning and task type.",
            "experimental_details": "Instruct-LLMs prompted in QA/instruction-following format mirroring children's tasks; deterministic decoding; deviations used to probe generalization.",
            "limitations_or_caveats": "Instruction-tuning appears crucial; authors propose analogy between instruction-tuning and cooperative communication shaping ToM-like behaviour, but caution that this is not evidence of human-like mental states.",
            "uuid": "e9055.35",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theory of mind may have spontaneously emerged in large language models",
            "rating": 2,
            "sanitized_title": "theory_of_mind_may_have_spontaneously_emerged_in_large_language_models"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "rating": 2,
            "sanitized_title": "large_language_models_fail_on_trivial_alterations_to_theoryofmind_tasks"
        },
        {
            "paper_title": "Clever Hans or neural theory of mind? stress testing social reasoning in large language models",
            "rating": 2,
            "sanitized_title": "clever_hans_or_neural_theory_of_mind_stress_testing_social_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Turning large language models into cognitive models",
            "rating": 1,
            "sanitized_title": "turning_large_language_models_into_cognitive_models"
        }
    ],
    "cost": 0.0337895,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</p>
<p>Max Van Duijn m.j.van.duijn@liacs.leidenuniv.nl 
Leiden Institute of Advanced Computer Science</p>
<p>Bram Van Dijk 
Leiden Institute of Advanced Computer Science</p>
<p>Tom Kouwenhoven 
Leiden Institute of Advanced Computer Science</p>
<p>Werner De Valk 
Leiden Institute of Advanced Computer Science</p>
<p>Marco Spruit 
Leiden Institute of Advanced Computer Science</p>
<p>Leiden University Medical Centre</p>
<p>Peter Van Der Putten 
Leiden Institute of Advanced Computer Science</p>
<p>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests
1E65FAFC9595B1E8297BDE5B3DFE8E8D
To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)?Here we add to this emerging debate by (i) testing 11 base-and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including nonliteral language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks.We find that instructiontuned LLMs from the GPT family outperform other models, and often also children.Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting.We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context.We conclude by arguing for a nuanced perspective on ToM in LLMs.</p>
<p>Introduction</p>
<p>Machines that can think like us have always triggered our imagination.Contemplation of such machines can be traced as far back as antiquity (Liveley and Thomas, 2020), and peaked with the advent of all kinds of 'automata' in the early days of the Industrial Revolution (Voskuhl, 2013) before settling in computer science from the 1950s (Turing, 1950).Currently people around the world can interact with powerful chatbots driven by Large Language Models (LLMs), such as OpenAI's ChatGPT (OpenAI, 2023), and wonder to what degree such systems are capable of thought.</p>
<p>LLMs are large-scale deep neural networks, trained on massive amounts of text from the web.*Equal contribution.</p>
<p>They are vastly complex systems: even if all details about their architecture, training data, and optional fine-tuning procedures are known (which is currently not the case for the most competitive models), it is very difficult to oversee their capabilities and predict how they will perform on a variety of tasks.Researchers from linguistics (Manning et al., 2020), psychology (Binz and Schulz, 2023b;Kosinski, 2023;Webb et al., 2023), psychiatry (Kjell et al., 2023), epistemology (Sileo and Lernould, 2023), logic (Creswell et al., 2022), and other fields, have therefore started to study LLMs as new, 'alien' entities, with their own sort of intelligence, that needs to be probed with experiments, an endeavour recently described as 'machine psychology' (Hagendorff, 2023).This not only yields knowledge about what LLMs are capable of, but also provides a unique opportunity to shed new light on questions surrounding our own intelligence (Dillion et al., 2023;Binz and Schulz, 2023a).</p>
<p>Here we focus on attempts to determine to what degree LLMs demonstrate a capacity for Theory of Mind (ToM), defined as the ability to work with beliefs, intentions, desires, and other mental states, to anticipate and explain behaviour in social settings (Apperly, 2010).We first address the question how LLMs perform on standardized, language-based tasks used to assess ToM capabilities in humans.We extend existing work in this area, surveyed in Section 2, in four ways: by (i) testing 11 models (see Table 1) for a broader suite of capabilities relevant to ToM beyond just the dominant falsebelief paradigm, including non-literal language understanding and recursive intentionality (A wants B to believe that C intends...); (ii) using newly written versions of standardized tests with varying degrees of deviation from the originals; (iii) including open questions besides closed ones; and (iv) benchmarking LLM performance against that of children aged 7-8 (n=37) and 9-10 (n=36) on the same tasks.Section 3 contains details of our test procedures for both children and LLMs.After reporting the results in Section 4, we turn to the question how variation in performance of the LLMs we tested can be explained in Section 5. We conclude by placing our findings in the broader context of strong links between language and ToM in human development and evolution, and tentatively interpret what it means for an LLM to pass (or fail) ToM tests.</p>
<p>We are aware of issues regarding LLM training and deployment, for example regarding the biases they inherit (Lucy and Bamman, 2021;Bender et al., 2021), problems for educators (Sparrow, 2022), and ethical concerns in obtaining human feedback (Perrigo, 2023).Ongoing reflection on the use of LLMs is necessary, but outside the scope of this paper.</p>
<p>Background</p>
<p>Large Language Models</p>
<p>The field of Natural Language Processing (NLP) has been revolutionized by the advent of Transformer models (Vaswani et al., 2017;Devlin et al., 2019), deep neural networks that can induce language structures through self-supervised learning.</p>
<p>During training, such models iteratively predict masked words from context in large sets of natural language data.They improve at this task by building representations of the many morphological, lexical, and syntactic rules governing human language production and understanding (Manning et al., 2020;Rogers et al., 2021;Grand et al., 2022).Models exclusively trained through such self-supervision constitute what we refer to as 'base-LLMs' in this paper.</p>
<p>Base-LLMs can generate natural language when prompted with completion queries ('A mouse is an ...').They can also be leveraged successfully for an array of other challenges, such as questionanswering and translation, which often requires task-specific fine-tuning or prompting with specific examples, known as few-shot-learning (Brown et al., 2020).This makes them different from a new generation of LLMs that we refer to as 'instruct-LLMs' in this paper, and to which the currently most competitive models belong.In instruction-tuning, various forms of human feedback are collected, such as ranking most suitable responses, which then forms the reward-signal for further aligning these models to human preferences through reinforcement learning (Ouyang et al., 2022).The resulting LLMs can be prompted with natural language in the form of instructions to perform a wide variety of tasks directly, amounting to zero-shot learning (Wei et al., 2022).</p>
<p>A key realization is thus that LLMs are given either no explicitly labelled data at all, or, in the case of instruct-LLMs, data with human labels pertaining to relatively general aspects of communicative interaction.As such they are part of a completely different paradigm than earlier language models that were trained on, for example, data sets of human-annotated language structures (e.g.Nivre et al., 2016).This means that when LLMs are capable of such tasks as solving co-reference relationships or identifying word classes (Manning et al., 2020), this arises as an emergent property of the model's architecture and training on different objectives.Given that such emergent linguistic capabilities have been observed (Reif et al., 2019;Grand et al., 2022), it is a legitimate empirical question which other capacities LLMs may have acquired as 'by-catch'.</p>
<p>Theory of Mind in Humans and LLMs</p>
<p>ToM, also known as 'mindreading', is classically defined as the capacity to attribute mental states to others (and oneself), in order to explain and anticipate behaviour.The concept goes back to research in ethology in which Premack and Woodruff (1978) famously studied chimpanzees' abilities to anticipate behaviour of caretakers.When focus shifted to ToM in humans, tests were developed that present a scenario in which a character behaves according to its false beliefs about a situation, and not according to the reality of the situation itself--which a successful participant, having the benefit of spectatorsight, can work out (see Section 3.1).</p>
<p>Initial consensus that children could pass versions of this test from the age of 4 was followed by scepticism about additional abilities it presumed, including language skills and executive functioning, which led to the development of simplified false-belief tests based on eye-gaze that even 15 month-olds were found to 'pass' (Onishi and Baillargeon, 2005).While this line of research also met important criticism (for a review see Barone et al., 2019), it highlights two key distinctions in debate from the past decades: implicit-behavioural versus explicit-representational and innate versus learned components of ToM.Some researchers see results from eye-gaze paradigms as evidence for a native or very early developing capacity for beliefattribution in humans (Carruthers, 2013) and hold that performance on more complex tests is initially 'masked' by a lack of expressive skills (cf.also Fodor, 1992).Others have attempted to explain eyegaze results in terms of lower-level cognitive mechanisms (Heyes, 2014) and argued that the capacity for belief-attribution itself develops gradually in interaction with more general social, linguistic, and narrative competencies (Heyes and Frith, 2014;Milligan et al., 2007;Hutto, 2008).Two-systems approaches (Apperly, 2010) essentially reconcile both sides by positing that our mindreading capacity encompasses both a basic, fast, and early developing component and a more advanced and flexible component that develops later.</p>
<p>In computational cognitive research, a variety of approaches to modelling ToM have been proposed (e.g.Baker and Saxe, 2011;Arslan et al., 2017).More recently neural agents (Rabinowitz et al., 2018) have been implemented, along with an increasing number of deep-learning paradigms aimed at testing first-and second-order ToM via question-answering.Initially this was done with recurrent memory networks (Grant et al., 2017;Nematzadeh et al., 2018) using data sets of classic false-belief tests from psychology, but after issues surfaced with simple heuristics for solving such tasks, scenarios were made more varied and challenging (Le et al., 2019).From the inception of BERT as one of the first LLMs (Devlin et al., 2019), we have seen roughly two approaches for testing ToM in LLMs: many different ToM scenarios integrated in large benchmark suites (e.g.Sap et al., 2022;Srivastava et al., 2023;Sileo and Lernould, 2023;Ma et al., 2023;Shapira et al., 2023), and studies that modified standardized ToM tests as used in developmental and clinical research for prompting LLMs (e.g.Kosinski, 2023;Ullman, 2023;Bubeck et al., 2023;Brunet-Gouet et al., 2023;Chowdhery et al., 2022;Moghaddam and Honey, 2023;Marchetti et al., 2023).This paper adds to the latter tradition in four respects, as listed in the introduction.</p>
<p>Methodology</p>
<p>Here we describe our tasks and procedures for testing LLMs and children; all code, materials, and data are on OSF: https://shorturl.at/FQR34.</p>
<p>ToM Tests</p>
<p>Sally-Anne test, first-order (SA1) --The Sally-Anne test (Wimmer and Perner, 1983;Baron-Cohen et al., 1985) is a classic first-order false belief test.It relies on a narrative in which Sally and Anne stand behind a table with a box and a basket on it.When Anne is still present, Sally puts a ball in her box.When Sally leaves, Anne retrieves the ball from the box and puts it in her own basket.The story ends when Sally returns and the participant is asked the experimental question 'Where will Sally look for the ball?'The correct answer is that she will look in her box.We followed up by asking a motivation question, 'Why?', to prompt an explanation to the effect of 'she (falsely) believes the object is where she left it'.</p>
<p>Sally-Anne test, second-order (SA2) --While SA1 targets the participant's judgement of what a character believes about the location of an unexpectedly displaced object, in SA2 the participant needs to judge what a character believes that another character believes about the location of an ice-cream truck (Perner and Wimmer, 1985).Sally and Anne are in a park this time, where an icecream man is positioned next to the fountain.Anne runs home to get her wallet just while the ice-cream man decides to move his truck to the swings.He tells Sally about this, but unknown to her, he meets Anne on the way and tells her too.Sally then runs after Anne, and finds her mother at home, who says that Anne picked up the wallet and went to buy ice cream.The experimental question now is 'Where does Sally think Anne went to buy ice cream?', with as correct answer 'to the fountain', also followed up with 'Why?', to prompt an explanation to the effect of 'Sally doesn't know that the ice-cream man told Anne that he was moving to the swings'.</p>
<p>Strange Stories test (SS) --The Strange Stories test (Happé, 1994;Kaland et al., 2005) depicts seven social situations with non-literal language use that can easily be misinterpreted, but causes no problems to typically developed adults.To understand the situations, subjects must infer the characters' intentions, applying ToM.For example, in one of the items a girl wants a rabbit for Christmas.When she opens her present, wrapped in a big enough box, it turns out that she received a pile of books.She says that she is really happy with her gift, after which subjects are asked the experimental question 'Is what the girl says true?', with correct answer 'No'.They can motivate their answer after the question 'Why does she say this?', with as correct answer 'to avoid her parents' feelings being hurt'.Items increase in difficulty and cover a lie, pretend-play scenario, practical joke, white lie (example above), misunderstanding, sarcasm, and double bluff.</p>
<p>Imposing Memory test (IM) --The Imposing Memory test was originally developed by Kinderman et al. (1998), but the test has been revised several times; we rely on an unpublished version created by Anneke Haddad and Robin Dunbar (van Duijn, 2016), originally for adolescents, which we adapted thoroughly to make it suitable for children aged 7-10.Our version features two different stories, followed by true/false questions, 10 of which are 'intentionality' and 12 are 'memory' questions.</p>
<p>For instance, in one story Sam has just moved to a new town.He asks one of his new classmates, Helen, where he can buy post stamps for a birthday card for his granny.When Helen initially sends him to the wrong location, Sam wonders whether she was playing a prank on him or just got confused about the whereabouts of the shop herself.He goes and asks another classmate, Pete, for help.As in the original IM, the intentionality questions involve reasoning about different levels of recursively embedded mental states (e.g., at third-level: 'Helen thought Sam did not believe that she knew the location of the store that sells post stamps'), whereas the memory questions require just remembering facts presented in the story (e.g., to match third-level intentionality questions, three elements from the story are combined: 'Sam was looking for a store where they sell post stamps.He told Pete that he had asked Helen about this').</p>
<p>Scoring Test Answers</p>
<p>Test scores for both children and LLMs were determined in the following way.For each of the SA1 and SA2 items, as well as for the seven SS items, a correct answer to the experimental question yielded 1 point.These answers were discrete and thus easy to assess ('box', 'fountain', 'no', etc.).For the motivation question a consensus score was obtained from two expert raters, on a range from 0-2, with 0 meaning a missing, irrelevant, or wrong motivation, 1 meaning a partly appropriate motivation, and 2 meaning a completely appropriate motivation that fully explained why the character in each scenario did or said something, or had a mental or emotional mind state.Thus, the maximum score for the SA1, SA2, and SS was 3 points per item, which were averaged to obtain a score between 0 and 1.For each correct answer to a true/false question in the IM, 1 point was given.All scores and ratings can be found on OSF.</p>
<p>Deviations</p>
<p>We tested the LLMs on the original SA and SS scenarios, but also on manually created deviations that increasingly stray from their original formulations, to prevent LLMs from leveraging heuristics and memorizing relevant patterns from the training data.Thus, deviations probe the degree to which performance on ToM tests in LLMs generalizes.Deviation 0 was always the original test scenario (likely present in the training data); deviation 1 was a superficial variation on the original with only e.g., objects and names changed (similar to Kosinski (2023)), whereas deviation 2 was a completely new scenario where only the ToM-phenomenon at issue was kept constant (e.g., 'second-order false belief' or 'irony').Since our adaptation of the IM test has hitherto not been used or published, we did not include deviations for this test.</p>
<p>Test Procedures for LLMs</p>
<p>We leveraged 11 state-of-the-art LLMs: 4 base-LLMs and 7 instruct-LLMs (see Table 1).Inference parameters were set such that their output was as deterministic as possible (i.e. a temperature ≊ zero or zero where possible) improving reproducibility.Each inference was done independently to avoid in-context learning or memory leakage between questions.This means that for each question, the prompt repeated the following general structure:
[instruction] + [test scenario] + [question].
Instruct-LLMs were prompted in a questionanswering format that stayed as close as possible to the questionnaires given to children, without any further custom prompting or provision of examples.Instructions were also similar to those given to children (e.g.'You will be asked a question.Please respond to it as accurately as possible without using many words.').The 'Why'-questions in SA1 and SA2 were created by inserting the experimental question and answer the LLM gave into the prompt:
[instruction] + [test scenario] + [ex- perimental question] + [LLM answer] +['Why?'].
This was not necessary for SS, given that experimental and motivation questions could be answered independently.For base-LLMs, known to continue prompts rather than follow instructions, staying this close to the children's questionnaires was not feasible.For the SA and SS we therefore fed base-LLMs the scenario as described before, but formulated the questions as text-completion exercises (e.g.'Sally will look for the ball in the ').Additionally, when creating the motivation questions for SA1 and SA2, we inserted the correct answer to the experimental question, instead of the LLM's answer.This was because base-LLMs so often derailed in their output that the method described for instruct-LLMs did not yield sensible prompts.Base-LLMs thus had an advantage here over children and instruct-LLMs, who were potentially providing a motivation following up on an incorrect answer they gave to the experimental question.</p>
<p>For the closed questions in the IM we attempted to streamline the output of base-LLMs by including two example continuations in the desired answer format.These examples were based on trivial information we added to the scenarios, unrelated to the actual experimental questions.For example: 'Helen: I wear a blue jumper today.This is [incorrect]', where it was added in the story that Helen wears a green jumper.This pushed nearly all base-LLM responses towards starting with '[correct]' or '[incorrect]', which we then assessed as answers to the true/false questions.We considered a similar prompt structure for SA and SS, amounting to adopting few-shot learning for base-LLMs throughout (Brown et al., 2020), but given that reformulating questions as text-completion exercises was by itself effective to get the desired output format, we refrained from inserting further differences from how instruct-LLMs are prompted.It is important to note that our prompts were in general not optimized for maximal test performance, but rather designed to stay as uniform and close to the way children were tested as possible, enabling a fair comparison among LLMs and with child performance.</p>
<p>Test Procedures for Children</p>
<p>Children were recruited from one Dutch and one international school in the South-West of the Netherlands: 37 children in the younger group (7-8y) and 36 children in the older group (9-10y).Children were administered digital versions of the SA and SS for the younger group, and of the IM for the older group, which they completed individually on tablets or PCs equipped with a touch screen.Test scenarios and questions were presented in a self-paced text format and all SA and SS questions were followed by an open text field in which they had to type their answer.As the IM features long scenarios, voice-overs of the text were included to alleviate reading fatigue.Here children had to answer by pressing yes/no after each question.To reduce memory bottlenecks, accompanying drawings were inserted (see OSF) and navigating back and forth throughout the tests was enabled.Informed consent for each child was obtained from caretakers, and the study was approved by the Leiden University Science Ethics Committee (ref. no. 2021-18).Test answers were evaluated and scored parallel to the approach for LLMs (Section 3.2).</p>
<p>Results</p>
<p>Sally-Anne</p>
<p>Overall performance on SA1 versus SA2 is given in Figure 1, left column.Most base-LLMs perform above child level on first-order ToM (BLOOM, Davinci, LLaMA-30B) but fall at or or below child level on second-order ToM.A similar pattern is visible for instruct-LLMs: most models perform well above child level on first-order (GPT-4, GPT-3.5, PaLM2-chat, PaLM2), but not on second-order ToM.Exceptions are GPT-4 and GPT-3.5:while degrading on second-order, they remain above child level.For both base-and instruct-LLMs, smaller models tend to perform worse (Falcon-7B, Falcon-7B-I, FLAN-T5) with GPT-3's structurally low scores as striking exception.This is inconsistent with results reported by (Kosinski, 2023) for GPT-3, which is probably due to the fact that Kosinski applied a text-completion approach whereas we prompted GPT-3 with open questions.</p>
<p>When we consider the performance on SA1 and SA2 over deviations (middle and right columns in Figure 1), we see once more that almost all LLMs struggle with second-order ToM, since performance decreases already on deviation 0 (i.e. the original test scenario), except for GPT-3.5 and GPT-4.Yet, it is the combination of second-order ToM and deviation 2 that pushes also GPT-3.5 and GPT-4 substantially below child levels, except for Falcon-7B, although the chat-optimized version of this model (Falcon-7B-I) fails on all second-order questions.</p>
<p>Strange Stories</p>
<p>General performance on SS is given in Figure 2, left column.Whereas child performance declines as items become more complex (from 1 to 7; see Section 3.1), this is overall less the case for LLM performance.For instruct-LLMs, we see that GPT-4 approaches perfect scores throughout.GPT-3 and GPT-3.5 perform at or close to child level on item 1, after which their performance somewhat declines, while staying well above child level.Other instruct-LLMs show a mixed picture: PaLM2-chat and FLAN-T5 surpass child level earlier than PaLM2.Interestingly, smaller FLAN-T5 outperforms large PaLM and PaLM2-chat on more difficult items.Falcon-7B-I, as smallest instruct-LLM, performs overall worst.</p>
<p>If performance is plotted over deviations (right column in Figure 2) we see little impact on most base-LLMs.For instruct-LLMs, it is striking that deviation levels have almost no effect on the larger models (GPT-4, PaLM2, PaLM2-chat, GPT-3, GPT-3.5), but do more dramatically lower performance of smaller models (FLAN-T5, Falcon-7B-I).In sum, base-LLMs perform below child level, except for the most complex items.Several large instruct-LLMs match or surpass child level throughout, others only for more complex items.Unlike for SA, deviation levels seem to have little negative impact.</p>
<p>Imposing Memory</p>
<p>The classical finding for the IM test is that error rates go up significantly for questions involving higher levels of recursive intentionality, but not for memory questions on matched levels of complexity, suggesting a limit to the capacity for recursive ToM specifically (Stiller and Dunbar, 2007). 1 We verified this for our child data (n=36) with two mixed linear models for memory and intentional questions with random intercepts.We included five predictors that were contrast-coded such that each predictor indicated the difference in average performance with the previous level.For intentional questions, only the difference between level two and one was significant (β = −0.222,p &lt; .05),marking a cutoff point after which performance remained consistently low.For memory questions, performance 1 While there is consensus in the literature that higher levels of intentionality are significantly harder for participants than lower levels, by various measures, there is debate about the difference with memory questions; see e.g.Lewis et al. (2017).For a critical discussion of measuring recursive intentionality in general, see Wilson et al. (2023).remained high across all levels (&gt; .85),except for level four, where scores were significantly lower than at level three (β = −0.292,p &lt; .00),but went up again at level five (β = 0.208, p &lt; .00).Thus, in line with earlier work, we find a cut-off point after which scores on intentionality questions remained consistently low, compared to scores on matched memory questions.We have no clear explanation for the dip in performance on memory questions at level four, but observe that it is driven by low scores on only one specific question out of a total of four for this level, which children may have found confusing.</p>
<p>In Figure 3 we see that all base-LLMs perform below child level, in general and on both intentionality and memory questions, and there is little variation in performance, except that larger base-LLMs (BLOOM, GPT-davinci) improve on higher levels of recursion.Regarding instruct-LLMs, we see largely the same picture, as they almost all perform below child level, in general and on both types of questions.The exception is GPT-4, which performs consistently well on all levels and stays above child level after second-order intentionality.For the difference between memory and intentional questions, instruct-LLMs perform better on easier memory questions, and drop towards the end, while on intentional questions, they already start lower and stay relatively constant.Lastly, it is remarkable that FLAN-T5, as one of the smallest instruct-LLMs, overall increases performance as recursion levels go up, and ends at child level.For GPT-3.5, which performs worst of all instruct-LLMs on this task, we see the exact opposite.</p>
<p>Notes on Child Performance</p>
<p>It can be observed that performance for SA was overall low compared to what could be expected from children aged 7-8 years: x = 0.45 for SA1 and x = 0.225 for SA2.We have two complementary explanations for this.Firstly, as discussed in Section 3.5, children had to read the tests on a screen, after which they had to type answers in open text fields.This is a challenging task by itself that relies on additional skills including language proficiency, conscientiousness, digital literacy, and more.Secondly, whereas 'passing' originally only means that a child can work out where Sally will look (for the ball, or for Anne on her way to buy ice cream), we also asked for a motivation, which makes the test more demanding.For the SS, completed by the same group of children, we see the expected pattern that scores show a downward tendency as test items increase in difficulty.The older group, aged 9-10, completed the IM.As discussed in Section 4.3, scores resonate with earlier work.Given that we see child performance not as the central phenomenon under observation in this paper, but rather as a reference for LLM performance, further discussion is outside our scope.</p>
<p>Discussion</p>
<p>Summing up the results for the Sally-Anne tests, while it is less surprising that base-LLMs and smaller instruct-LLMs struggle with increasing test complexity and deviations, it is striking that second-order ToM immediately perturbs some large instruct-LLMs (e.g.PaLM2-chat), and that adding deviations from the original test formula- tions pushed performance of even the most competitive models down (e.g.GPT-4, GPT-3.5).This initially suggests that performance on ToM tasks does not generalize well beyond a few standard contexts in LLMs, in line with earlier work (Sap et al., 2022;Shapira et al., 2023;Ullman, 2023).</p>
<p>For the Strange Stories we saw that base-LLMs perform generally below child level.Most instruct-LLMs perform close to or above child level, particularly as items become more complex and child performance drops much more dramatically than LLM performance.Levels of deviation from the original test formulation seem to have made almost no impact for the SS, suggesting that the capacity to deal with non-literal language targeted by the Strange Stories test does generalize to novel contexts.We conclude that instruct-LLMs are quite capable at interpreting non-literal language, a skill that in humans involves ToM.Since the training data of LLMs includes numerous books and fora, which are typically rich in irony, misunderstanding, jokes, sarcasm, and similar figures of speech, we tentatively suggest that LLMs are in general wellequipped to handle the sort of scenarios covered in the Strange Stories.This should in theory include base-LLMs, but it could be that their knowledge does not surface due to the test format, even after specialized prompting.Going one step further, we hypothesize that Sally-Ann is generally harder for LLMs given that this test relies less on a very specific sort of advanced language ability, but more on a type of behaviourally-situated reasoning that LLMs have limited access to during training (see also Mahowald et al., 2023).</p>
<p>The Imposing Memory test was the most chal-lenging for both base-and instruct-LLMs.Since our version of it was never published before, it constitutes another robustness test, which only GPT-4 as largest instruct-LLM seems to pass well.The gap between base-and instruct-LLMs is best summarized in Figure 4.Here we see that no base-LLM achieves child level: all LLMs approaching or exceeding child performance are larger instruct-LLMs.Our adapted prompts and insertion of correct answers for motivation questions did not make a difference.We suggest that another issue for base-LLMs, besides the prompt format, was prompt length.This was highest for IM, which can explain why they struggled most with this test.Prompt length, in relation to the models' varying context window sizes and ability to engage in what Hagendorff et al. ( 2023) call chain-of-thought reasoning, merits further research (see also Liu et al., 2023).We tested whether there was a difference between model performance on closed versus open questions across all three tasks, but found no signal: the models that struggled with closed questions were also those that performed low on open questions (for more details and additional information on prompting, see Appendix A on OSF).</p>
<p>Evidence is emerging that most LLM capacities are learned during self-supervised pre-training (Gudibande et al., 2023;Ye et al., 2023), which suggests that base-LLMs are essentially 'complete' models.Yet instruction-tuning, even in small amounts (Zhou et al., 2023), adds adherence to the desired interaction format and teaches LLMs, as it were, to apply their knowledge appropriately.We see a parallel between instruction-tuning and the role for rewarding cooperative communication in human evolution and development.It has been argued extensively that human communication is fundamentally cooperative in that it relies on a basic ability and willingness to engage in mental coordination (e.g Verhagen, 2015;Grice, 1975).It is a key characteristic of the socio-cultural niche in which we evolved that, when growing up, we are constantly being rewarded for showing such willingness and cooperating with others to achieve successful communicative interactions (Tomasello, 2008).Reversely, if we do not, we are being punished, explicitly or implicitly via increasing social exclusion (David-Barrett and Dunbar, 2016).This brings us back to our context: instruction-tuning essentially rewards similar cooperative principles, but punishes the opposite, which may amount to an enhanced capacity for coordinating with an interaction partner's perspective, in humans and LLMs alike.This is reflected in performance on ToM tasks, which are banking on this capacity too.</p>
<p>Finally, we do not claim that LLMs that performed well also have ToM in the way that humans have it.Validity of cognitive tests such as those used in ToM research is a general issue (e.g.van Duijn, 2016).Yet for humans ToM tests are validated 'quick probes': decades of research have shown that proficiency on such tests correlates with an array of real-world social and cognitive abilities (Beaudoin et al., 2020).For LLMs we are in a very early stage of figuring out what is entailed by proficon ToM tests: on the one hand it is impressive that some models show a degree of robust performance, without explicit training on ToM.On the other hand it remains an open question whether this amounts to any actual capacities in the social-cognitive domain, in which they are clearly very differently grounded (if at all) compared to humans.</p>
<p>For future research we believe in the format of testing models that differ in other respects than just size, on a varied array of tasks, with multiple tests per test item, to gain further insight into the aspects that explain variability in performance.For this, more openness about architecture and training procedures of current and future LLMs is imperative.In addition, we believe to have contributed to the debate by benchmarking LLM results on child data, but more of this is needed.We had limited samples and age distributions, and tests were not presented in optimal ways (see Section 3.5).</p>
<p>We emphasize that our results need to be seen within the time frame of late Spring 2023.The fast pace with which LLMs are currently released and, in some cases, updated, makes them a moving target.There are indications that specific capacities of models from the GPT-family have declined over time, perhaps as a result of such updates (e.g., handling math problems and producing code; Chen et al., 2023).Future studies need to address how such developments impact the capacities assessed in this paper.</p>
<p>Conclusion</p>
<p>We have shown that a majority of recent Large Language Models operate below performance of children aged 7-10 on three standardized tests relevant to Theory of Mind.Yet those that are largest in terms of parameters, and most heavily instructiontuned, surpass children, with GPT-4 well above all other models, including more recent competitors like PaLM2-chat and PaLM2 (see Figure 4).We have interpreted these findings by drawing a parallel between instruction-tuning and rewarding cooperative interaction in human evolution.We concede that researching the degree to which LLMs are capable of anything like thought in the human sense has only just begun, which leaves the field with exciting challenges ahead.</p>
<p>Figure 1 :
1
Figure 1: Performance on Sally-Anne tests for base-LLMs (top row) and instruct-LLMs (bottom row).Left column depicts performance on first-and second-order ToM (i.e.SA1 vs. SA2), averaged over the original and rewritten test versions.Middle and left columns depict performance for SA1 and SA2 over levels of deviation from the original test (0, 1, and 2; see Section 3.3).Dashed lines indicate child performance (n=37, age 7-8 years).</p>
<p>Figure 2 :
2
Figure 2: Performance on Strange Stories for base-LLMs (top row) and instruct-LLMs (bottom row).Left column shows overall performance, averaged over levels of deviation from the original test.Right column shows performance over deviation levels, averaged over items.Dashed lines indicate child performance (n=37, 7-8y).</p>
<p>Figure 3 :
3
Figure 3: Performance on Imposing Memory test for base-LLMs (top row) and instruct-LLMs (bottom row).Left column depicts overall performance over five levels of recursion, averaged over deviations.Middle and left columns depict performance for Memory and Intentional questions.Dashed lines indicate child performance (n=36, 9-10y).</p>
<p>Figure 4 :
4
Figure 4: Grand mean performance (stars) of all mean test scores (dots) for children and LLMs.</p>
<p>Table 1 :
1
Elias (2023))this study.Model sizes are undisclosed for GPT-4 and for PaLM2 and PaLM2-chat, thus we base ourselves on secondary sources for estimations;Knight (2023)andElias (2023), respectively.
Base-LLMsSourceSizeFalconPenedo et al. (2023)7BLLaMATouvron et al. (2023)30BGPT-davinciBrown et al. (2020)175BBLOOMScao et al. (2022)176BInstruct-LLMs""Falcon-instructPenedo et al. (2023)7BFlan-T5Chung et al. (2022)11BGPT-3(text-davinci-003) Ouyang et al. (2022)175BGPT-3.5-turboOuyang et al. (2022)175BPaLM2Anil et al. (2023)175-340BPaLM2-chatAnil et al. (2023)175-340BGPT-4OpenAI (2023)&gt;340B
AcknowledgementsThis research took place in the context of the project A Telling Story, financed by the Dutch Research Council NWO (VI.Veni.191C.051).We are grateful to the children and their caregivers and teachers for participating in our research, and we thank Li Kloostra, Lola Vandame, and three anonymous reviewers for their help and constructive feedback.
. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, Yaguang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, ; Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, arXiv:2305.10403v3Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin XuSlav PetrovYunhan Xu, Linting Xue, Pengcheng Yin,arXiv preprintand Yonghui Wu. 2023. PaLM 2 Technical Report</p>
<p>Mindreaders: the Cognitive Basis of "Theory of Mind. Ian Apperly, 2010Psychology Press</p>
<p>Five-year-olds' systematic errors in secondorder false belief tasks are due to first-order theory of mind strategy selection: A computational modeling study. Niels A Burcu Arslan, Rineke Taatgen, Verbrugge, Frontiers in psychology. 82752017</p>
<p>Bayesian theory of mind: Modeling joint belief-desire attribution. Chris Baker, Rebecca Saxe, Proceedings of the Thirty-Third Annual Conference of the Cognitive Science Society. the Thirty-Third Annual Conference of the Cognitive Science Society2011</p>
<p>Does the autistic child have a "theory of mind. Simon Baron-Cohen, Alan M Leslie, Uta Frith, Cognition. 2111985</p>
<p>Infants' performance in spontaneous-response false belief tasks: A review and meta-analysis. Pamela Barone, Guido Corradi, Antoni Gomila, 10.1016/j.infbeh.2019.101350Infant Behavior and Development. 571013502019</p>
<p>Systematic review and inventory of theory of mind measures for young children. Cindy Beaudoin, Élizabel Leblanc, Charlotte Gagner, Miriam H Beauchamp, Frontiers in psychology. 1029052020</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and Transparency2021</p>
<p>Turning large language models into cognitive models. Marcel Binz, Eric Schulz, arXiv.2306.039172023aarXiv preprint</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023b</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>Do conversational agents have a theory of mind? a single case study of chatgpt with the hinting, false beliefs and false photographs, and strange stories paradigms. Eric Brunet-Gouet, Nathan Vidal, Paul Roux, 2023</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with GPT-4. 2023</p>
<p>Peter Carruthers, 10.1111/mila.12014Mindreading in infancy. Mind &amp; Language. 201328</p>
<p>Lingjiao Chen, Matei Zaharia, James Zou, arXiv:2307.09009How is ChatGPT's behavior changing over time?. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311PaLM: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, 10.48550/ARXIV.2210.114162022Scaling instruction-finetuned language models</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022arXiv preprint</p>
<p>Language as a coordination tool evolves slowly. Tamas David, - Barrett, Robin I M Dunbar, 10.1098/rsos.160259R. Soc. open sci. 31602592016</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2019</p>
<p>Can AI language models replace human participants?. Danica Dillion, Niket Tandon, Yuling Gu, Kurt Gray, 10.1016/j.tics.2023.04.008Trends in Cognitive Sciences. 2772023</p>
<p>Google's newest A.I. model uses nearly five times more text data for training than its predecessor. Jennifer Elias, 2023</p>
<p>A theory of the child's theory of mind. J A Fodor, 10.1016/0010-0277(92)90004-2Cognition. 4431992</p>
<p>Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Gabriel Grand, Idan Asher Blank, Francisco Pereira, Evelina Fedorenko, Nature human behaviour. 672022</p>
<p>How can memory-augmented neural networks pass a false-belief task?. Erin Grant, Aida Nematzadeh, Thomas L Griffiths, CogSci. 2017</p>
<p>Logic and conversation. Paul Grice, Syntax and semantics. Peter Cole, Jerry Morgan, New YorkAcademic Press19753Speech acts</p>
<p>The false promise of imitating proprietary llms. Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song, 2023</p>
<p>Humanlike intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. T Hagendorff, S Fabi, M Kosinski, 10.1038/s43588-023-00527-xNature Compututer Science. 2023</p>
<p>Thilo Hagendorff, arXiv:2303.13988Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods. 2023arXiv preprint</p>
<p>An advanced test of theory of mind: Understanding of story characters' thoughts and feelings by able autistic, mentally handicapped, and normal children and adults. G E Francesca, Happé, Journal of autism and Developmental disorders. 2421994</p>
<p>False belief in infancy: a fresh look. Cecilia Heyes, 10.1111/desc.12148Developmental Science. 1752014</p>
<p>The cultural evolution of mind reading. Cecilia M Heyes, Chris D Frith, 10.1126/science.1243091Science. 344619012430912014</p>
<p>Folk Psychological Narratives: The Sociocultural Basis of Understanding Reasons. D Daniel, Hutto, 2008The MIT Press</p>
<p>The Strange Stories test -a replication study of children and adolescents with Asperger syndrome. Nils Kaland, Annette Møller-Nielsen, Lars Smith, Erik Lykke Mortensen, Kirsten Callesen, Dorte Gottlieb, European child &amp; adolescent psychiatry. 1422005</p>
<p>Theory-of-mind deficits and causal attributions. P Kinderman, R Dunbar, R P Bentall, 10.1111/j.2044-8295.1998.tb02680British Journal of Psychology. 21998</p>
<p>Ai-based large language models are ready to transform psychological health assessment. Oscar Kjell, Katarina Kjell, Andrew Schwartz, 2023</p>
<p>A new chip cluster will make massive ai models possible. Will Knight, 2023</p>
<p>Michal Kosinski, arXiv:2302.02083Theory of mind may have spontaneously emerged in large language models. 2023arXiv preprint</p>
<p>Revisiting the evaluation of theory of mind through question answering. Matthew Le, Y-Lan Boureau, Maximilian Nickel, 10.18653/v1/D19-1598Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Higher order intentionality tasks are cognitively more demanding. Penelope A Lewis, Amy Birch, Alexander Hall, Robin I M Dunbar, 10.1093/scan/nsx034Social Cognitive and Affective Neuroscience. 1272017</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023arXiv preprint</p>
<p>Homer's intelligent machines: AI in antiquity. Genevieve Liveley, Sam Thomas, 10.1093/oso/9780198846666.003.00022020</p>
<p>Gender and Representation Bias in GPT-3 Generated Stories. Li Lucy, David Bamman, 10.18653/v1/2021.nuse-1.5Proceedings of the Third Workshop on Narrative Understanding. the Third Workshop on Narrative UnderstandingVirtual. Association for Computational Linguistics2021</p>
<p>Tomchallenges: A principle-guided dataset and diverse evaluation tasks for exploring theory of mind. Xiaomeng Ma, Lingyu Gao, Qihui Xu, arXiv:2305.150682023arXiv preprint</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, arXiv:2301.06627Dissociating language and thought in large language models: a cognitive perspective. 2023arXiv preprint</p>
<p>Emergent linguistic structure in artificial neural networks trained by self-supervision. D Christopher, Kevin Manning, John Clark, Urvashi Hewitt, Omer Khandelwal, Levy, 10.1073/pnas.1907367117Proceedings of the National Academy of Sciences. the National Academy of Sciences2020117</p>
<p>Developing chatgpt's theory of mind. Antonella Marchetti, Cinzia Di Dio, Angelo Cangelosi, Federico Manzi, Davide Massaro, 10.3389/frobt.2023.1189525Frontiers in Robotics and AI. 102023</p>
<p>Language and theory of mind: Metaanalysis of the relation between language ability and false-belief understanding. Karen Milligan, Janet Wilde Astington, Lisa Ain Dack, 10.1111/j.1467-8624.2007.01018.xChild development. 7822007</p>
<p>Boosting theory-of-mind performance in large language models via prompting. Rahimi Shima, Christopher J Moghaddam, Honey, 2023</p>
<p>Evaluating theory of mind in question answering. Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, Tom Griffiths, 10.18653/v1/D18-1261Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Universal dependencies v1: A multilingual treebank collection. Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christopher D Manning, Ryan Mcdonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). the Tenth International Conference on Language Resources and Evaluation (LREC'16)2016</p>
<p>Do 15-month-old infants understand false beliefs?. Kristine H Onishi, Renée Baillargeon, 10.1126/science.1107621Science. 30857192005</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>attribution of second-order beliefs by 5-to 10-year-old children. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. 2023. 198539arXiv preprintJohn thinks that Mary thinks that</p>
<p>Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic. Billy Perrigo, 2023</p>
<p>Does the chimpanzee have a theory of mind?. David Premack, Guy Woodruff, Behavioral and brain sciences. 141978</p>
<p>Machine theory of mind. Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, S M Ali Eslami, Matthew Botvinick, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLR201880of Proceedings of Machine Learning Research</p>
<p>Visualizing and measuring the geometry of BERT. Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, Been Kim, Advances in Neural Information Processing Systems. 201932</p>
<p>Anna Rogers, Olga Kovaleva, Anna Rumshisky, 10.1162/tacl_a_00349A primer in BERTology: What we know about how BERT works. 20218</p>
<p>Neural theory-of-mind? on the limits of social intelligence in large LMs. Maarten Sap, Le Ronan, Daniel Bras, Yejin Fried, Choi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100BLOOM: A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Natalie Shapira, Mosh Levy, Hossein Seyed, Xuhui Alavi, Yejin Zhou, Yoav Choi, Maarten Goldberg, Vered Sap, Shwartz, Clever Hans or neural theory of mind? stress testing social reasoning in large language models. 2023</p>
<p>MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. Damien Sileo, Antoine Lernould, arXiv.2305.033532023arXiv preprint</p>
<p>Full-on robot writing': the artificial intelligence challenge facing universities. Jeff Sparrow, 2022</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew M Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Christopher Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Damien Garbacea, Dan Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, C Roth, Daniel Daniel Freeman, Daniel Khashabi, Daniel Levy, Danielle Moseguí González, Danny Perszyk, Danqi Hernandez, Daphne Chen, Dar Ippolito, David Gilboa, David Dohan, David Drakard, Debajyoti Jurgens, Deep Datta, Denis Ganguli, Denis Emelin, Deniz Kleyko, Derek Yuret, Derek Chen, Dieuwke Tam, Diganta Hupkes, Dilyar Misra, Dimitri Coelho Buzan, Diyi Mollo, Dong-Ho Yang, Dylan Lee, Ekaterina Schrader, Ekin Shutova, Elad Dogus Cubuk, Eleanor Segal, Elizabeth Hagerman, Elizabeth Barnes, Ellie Donoway, Emanuele Pavlick, Emma Rodolà, Eric Lam, Eric Chu, Erkut Tang, Ernie Erdem, Ethan A Chang, Ethan Chi, Ethan Dyer, Ethan Jerzak, Eunice Engefu Kim, Evgenii Manyasi, Fanyue Zheltonozhskii, Fatemeh Xia, Fernando Siar, Francesca Martínez-Plumed, Francois Happé, Frieda Chollet, Gaurav Rong, ; Mishra, Germán Gerard De Melo, Giambattista Kruszewski, Giorgio Parascandolo, Gloria Xinyue Mariani, Gonzalo Wang, Gregor Jaimovitch-Lopez, Guy Betz, Hana Gur-Ari, Hannah Galijasevic, Hannah Kim, Hannaneh Rashkin, Harsh Hajishirzi, Hayden Mehta, Henry Bogar, Anthony Francis, Hinrich Shevlin, Hiromu Schuetze, Hongming Yakura, Hugh Mee Zhang, Ian Wong, Isaac Ng, Jaap Noble, Jack Jumelet, Jackson Geissinger, Jacob Kernion, Jaehoon Hilton, Jaime Fernández Lee, James B Fisac, James Simon, James Koppel, James Zheng, Jan Zou, Jana Kocon, Janelle Thompson, Jared Wingfield, Jarema Kaplan, Jascha Radom, Jason Sohl-Dickstein, Jason Phang, Jason Wei, Jekaterina Yosinski, Jelle Novikova, Jennifer Bosscher, Jeremy Marsh, Jeroen Kim, Jesse Taal, Jesujoba Engel, Jiacheng Alabi, Jiaming Xu, Jillian Song, Joan Tang, John Waweru, John Burden, John U Miller, Jonathan Balis, Jonathan Batchelder, Jörg Berant, Jos Frohberg, Jose Rozen, Joseph Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joshua B Jones, Joshua S Tenenbaum, Joyce Rule, Kamil Chua, Karen Kanclerz, Karl Livescu, Karthik Krauth, Katerina Gopalakrishnan, Katja Ignatyeva, Kaustubh Markert, Kevin Dhole, Kevin Gimpel, Kory Wallace Omondi, Kristen Mathewson, Ksenia Chiafullo, Kumar Shkaruta, Kyle Shridhar, Kyle Mc-Donell, Laria Richardson, Leo Reynolds, Li Gao, Liam Zhang, Lianhui Dugan, Lidia Qin, Louis-Philippe Contreras-Ochando, Luca Morency, Lucas Moschella, Lucy Lam, Ludwig Noble, Luheng Schmidt, Luis He, Luke Oliveros-Colón, Lütfi Kerem Metz, Maarten Senel, Maarten Bosma, Maartje Sap, Maheen Ter Hoeve, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Jose Maru, Marie Ramirez-Quintana, Mario Tolkiehn, Martha Giulianelli, Martin Lewis, Matthew L Potthast, Matthias Leavitt, Hagen ; Mimee, Mirac Xu, Mitch Suzgun, Mo Walker, Mohit Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T Varma, Nanyun Peng, Nathan Andrew Chi, Nayeon Lee, Neta Gur-, Ari Krakover, ; Niveditha, S Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans ; Peiyuan, Percy Liao, Peter W Liang, Sam Chang, Sam Shleifer, Samuel Wiseman, Gruetter, R Samuel, Samuel Bowman, Sanghyun Stern Schoenholz, Sanjeev Han, Sarah A Kwatra, Sarik Rous, Sayan Ghazarian, Sean Ghosh, Sebastian Casey, Sebastian Bischoff, Sebastian Gehrmann, Sepideh Schuster, Shadi Sadeghi, Sharon Hamdan, Shashank Zhou, Sherry Srivastava, Shikhar Shi, Shima Singh, Asaadi, Shane Shixiang, Shubh Gu, Shubham Pachchigar, Shyam Toshniwal, Upadhyay, Shammie Shyamolima, Siamak Debnath, Simon Shakeri, Simone Thormeyer, Siva Melzi, Reddy, Priscilla Sneha, Soo-Hwan Makini, Spencer Lee, Sriharsha Torene, Stanislas Hatwar, Stefan Dehaene, Stefano Divic, Stella Ermon, Stephanie Biderman, Stephen Lin, Steven Prasad, Piantadosi ; Tao, Tao Li, Tariq Yu, Tatsunori Ali, William Hashimoto, William Fedus, William Saunders, Zhang ; Yasaman, Yejin Bahri, Yichi Choi, Yiding Yang, Yifu Hao, Yonatan Chen, Yu Belinkov, Yufang Hou, Yuntao Hou, Zachary Bai, Zhuoye Seid, Zijian Zhao, Zijie J Wang, Zirui Wang, Wang, Vikas Raunak, Vinay Venkatesh Ramasesh, vinay uday prabhu, Vishakh Padmakumar, Vivek Srikumar. Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael Andrew Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain,; Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Stuart Shieber; Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick; Titus Tunduny, Tobias Gerstenberg, Trenton ChangTimofei Kornev29Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster,Pablo Antonio Moreno CasaresSocial Networks</p>
<p>Origins of Human Communication. Michael Tomasello, 2008MIT PressCambridge, MA</p>
<p>LLaMA: Open and Efficient Foundation Language Models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, 2023</p>
<p>Computing machinery and intelligence. Alan M Turing, 10.1093/mind/LIX.236.4331950Mind, LIX</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. Tomer Ullman, arXiv:2302.083992023arXiv preprint</p>
<p>The lazy mindreader: a humanities perspective on mindreading and multiple-order intentionality. Max J Van Duijn, 2016Leiden UniversityPh.D. thesis</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Grammar and cooperative communication. Arie Verhagen, 10.1515/9783110292022-012Handbook of Cognitive Linguistics. Ewa Dabrowska, Dagmar Divjak, Berlin, München, BostonDe Gruyter Mouton2015</p>
<p>Adelheid Voskuhl, 10.7208/chicago/9780226034331.003.0001One introduction: Androids, enlightenment, and the human-machine boundary. 2013</p>
<p>Emergent analogical reasoning in large language models. Taylor Webb, Keith J Holyoak, Hongjing Lu, Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M. Dai, and Quoc V. Le.2023. 2022Finetuned language models are zero-shot learners</p>
<p>Is recursive "mindreading" really an exception to limitations on recursive thinking. Robert Wilson, Alexander Hruby, Daniel Perez-Zapata, Sanne W Van Der Kleij, Ian A Apperly, 10.1037/xge0001322Journal of Experimental Psychology: General. 15252023</p>
<p>Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. Heinz Wimmer, Josef Perner, 10.1016/0010-0277(83)90004-5Cognition. 1311983</p>
<p>Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, arXiv:2303.10420A comprehensive capability analysis of GPT-3 and GPT-3.5 series models. 2023arXiv preprint</p>
<p>. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy, 2023Less is more for alignmentLima</p>            </div>
        </div>

    </div>
</body>
</html>
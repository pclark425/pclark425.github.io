<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8554 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8554</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8554</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-32afdb07021fda775ceaedd231c58bfed0aa980a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/32afdb07021fda775ceaedd231c58bfed0aa980a" target="_blank">Automated Crossword Solving</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The Berkeley Crossword Solver is presented, a state-of-the-art approach for automatically solving crossword puzzles that improves exact puzzle accuracy from 57% to 82% on crosswords from The New York Times and obtains 99.9% letter accuracy on themeless puzzles.</p>
                <p><strong>Paper Abstract:</strong> We present the Berkeley Crossword Solver, a state-of-the-art approach for automatically solving crossword puzzles. Our system works by generating answer candidates for each crossword clue using neural question answering models and then combines loopy belief propagation with local search to find full puzzle solutions. Compared to existing approaches, our system improves exact puzzle accuracy from 57% to 82% on crosswords from The New York Times and obtains 99.9% letter accuracy on themeless puzzles. Our system also won first place at the top human crossword tournament, which marks the first time that a computer program has surpassed human performance at this event. To facilitate research on question answering and crossword solving, we analyze our system’s remaining errors and release a dataset of over six million question-answer pairs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8554.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8554.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BCS Bi-Encoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-base bi-encoder QA model (clue and answer encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-style question-answering model using two BERT-base-uncased encoders (one for clues, one for answers) trained to embed clues and candidate answers into a shared space; used as the first-pass closed-book candidate generator for crossword solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated Crossword Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base bi-encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two encoders (E_C for clues, E_A for answers) initialized from BERT-base-uncased; trained like DPR with one distractor per example; maps clues and answers into a joint embedding space and scores answers by dot product similarity; precomputes answer embeddings and uses FAISS for fast retrieval from a closed answer set of 437.8K answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword (NYT, LA Times, Newsday, The New Yorker, The Atlantic)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based word puzzle with crossing-letter constraints (2D grid where each cell belongs to two words); requires reasoning about letter positions and consistency across intersecting answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>First-pass closed-book retrieval: for each clue compute embedding and retrieve top-k candidates (they focus on top-1000) from the training answer set; produce softmax probabilities over retrieved candidates; segmented answers (via a segmentation model) are used for training/inference.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Retrieval-style bi-encoder (closed-book) to produce candidate lists; answers restricted to training answer set; used downstream with loopy belief propagation (BP) on a bipartite clue-cell graph to enforce spatial (crossing-letter) constraints, greedy inference to produce a solution, and local search to refine errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Top-1000 recall on 2021 NYT test: 94.6% (Dr.Fill QA baseline: 84.4%). Oracle recall (accounting for unseen answers) ≈ 96%. When combined in full pipeline: BCS (with BP+LS) achieves 81.7% perfect-puzzle accuracy on NYT 2021 (Dr.Fill baseline 70.5%); themeless NYT puzzles: 89.5% perfect puzzles and 99.9% letter accuracy reported for BCS.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Explicit: BP constructs a bipartite graph with clue nodes and cell nodes and passes messages to enforce consistency of letters across intersecting words; results show that when the true answer is in the bi-encoder's top-1000, BP and subsequent steps almost always fill it correctly (Figure 11), indicating the model leverages crossing-letter (spatial) constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against Dr.Fill's QA module (TFIDF-like ensemble): bi-encoder improves top-1000 recall from 84.4% to 94.6%. Ablations: full pipeline (BCS QA + BP + LS) 81.7% perfect vs BCS QA + BP (no LS) 44.3% and BCS QA + Dr.Fill solver 73.7% showing superiority of the BCS components.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Closed-book constraint: ≈4% of test answers absent from answer set, limiting recall; QA errors concentrated on knowledge, wordplay, and cross-reference clues; bi-encoder can be overconfident and cause BP to converge to suboptimal solutions; temporal shift (e.g., FAUCI error) and themed puzzles (rebuses, multi-letter cells) are major failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Crossword Solving', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8554.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8554.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ByT5 Scorer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ByT5-small character-level generative QA model (second-pass scorer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A character/byte-level generative model finetuned to generate answers from clues and used to score candidate puzzle proposals in local search, chosen for robustness against nonsensical high-confidence candidates from the bi-encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated Crossword Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ByT5-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Character/byte-level encoder-decoder (ByT5-small) finetuned on the crossword QA training set to generate answers from clues; used to compute likelihoods P(answer | clue) and score whole-puzzle proposals by the product of likelihoods across clues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based word puzzle with crossing-letter constraints (2D grid).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>During local search, propose candidate puzzles (1–2 letter edits guided by BP character marginals or dictionary/segmentation rules), then score each candidate by computing the product over clues of ByT5's P(a_j | c_j); accept best-scoring edits iteratively until no improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Generative likelihood scoring at character/byte level (more robust to odd candidates than bi-encoder discriminative scores); used in an iterative local search that proposes small edits (within 2-letter edit distance) focused on uncertain letters from BP marginals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Local search (scored by ByT5) applied 243 edits that improved accuracy and 31 edits that hurt across 234 NYT test puzzles; including local search raised perfect-puzzle accuracy from 44.3% (BCS QA + BP) to 81.7% (full BCS).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>ByT5 is applied only to score candidate puzzles that respect grid constraints (edits are constrained by BP and segmentation/dictionary checks); improvements in puzzle/letter accuracy after ByT5-scored local search indicate that the scorer effectively helps correct spatially constrained letter errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Authors found ByT5 empirically more robust than scoring candidate puzzles with the bi-encoder (which sometimes assigned high confidence to nonsensical fills); ByT5-based local search is a key component in outperforming Dr.Fill and in ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Local search scoring failures: 9 puzzles where ByT5 rejected correct proposals or accepted incorrect ones (classified in error analysis as 'Local Search Scoring'); overall local search also produced some harmful edits (31 across 234 puzzles).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Crossword Solving', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8554.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8554.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Segmentation GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 small fine-tuned word segmentation model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-2 small model finetuned to map canonical unsegmented crossword fill strings (ALLCAPS, no spaces/punctuation) to segmented natural-language forms to improve downstream QA model tokenization and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated Crossword Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 small (segmentation model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 small finetuned to generate segmented n-grams from their unsegmented counterparts (trained on frequent n-grams from Wikipedia with spaces/punctuation removed); applied to all answers so QA models can be trained/evaluated on segmented text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based word puzzle with crossing-letter constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Preprocessing step: run the segmenter on canonical crossword answers (e.g., DAAABEARS -> 'daaa bears') to produce segmented forms for QA training and to map model outputs back to crossword fill format by removing spaces/punctuation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Sequence generation for segmentation to align crossword fills with natural-language tokenization of transformer models; improves QA model handling of multiword or unusual answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No explicit numeric evaluation of segmentation accuracy reported in the paper; segmentation model used across the 6.4M QA training set and in all downstream experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>None — segmentation is a preprocessing step and does not itself perform spatial reasoning for puzzle solving.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Authors note simpler rule-based segmentation (splitting into known English words) is insufficient for many crossword answers and motivated using a learned GPT-2 model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No quantitative failure cases reported, but authors state simpler approaches were insufficient for many nonstandard fills (e.g., DAAABEARS, EENYMEENYMINYMOE).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Crossword Solving', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8554.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8554.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BCS (end-to-end)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Berkeley Crossword Solver (BCS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end crossword solving pipeline combining a neural bi-encoder QA model (BERT-base), loopy belief propagation for constraint resolution, greedy decoding, and local search scored by a character-level ByT5; achieves state-of-the-art puzzle solving performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated Crossword Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BCS pipeline (GPT-2 segmenter + BERT bi-encoder + BP + Greedy + ByT5 local search)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-stage system: (1) segment answers with GPT-2 small; (2) generate candidate answers with a closed-book BERT-base bi-encoder over a 437.8K answer set and retrieve top-k (FAISS); (3) resolve letter constraints with loopy BP on a bipartite clue-cell graph producing marginals; (4) greedy decoding to produce initial fill; (5) local search proposing small letter edits scored by ByT5-small likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword (various publishers; primary eval on 2020/2021 NYT puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based word puzzle with crossing-letter constraints; includes themed puzzles with rebuses or multi-letter cells that complicate spatial mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Multi-stage decoding described above; evaluation on held-out set of complete NYT puzzles (408 total test puzzles from multiple publishers), with detailed ablation experiments (e.g., removing LS or swapping components with Dr.Fill).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Combines neural QA for candidate generation with probabilistic structured inference (loopy BP) to enforce spatial constraints and local search to repair remaining errors; BP constructs clue and cell nodes and passes messages to enforce letter consistency across crossings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Main results: on NYT test puzzles (234 puzzles) BCS achieves 81.7% perfect-puzzle accuracy (Dr.Fill baseline 70.5%), word accuracy 98.9% vs 97.9%, letter accuracy 99.7% vs 99.2%; themeless NYT puzzles: 89.5% perfect and 99.9% letter accuracy. In the 2021 ACPT live tournament the submitted system (BCS QA + Dr.Fill solver variant) outperformed all 1033 humans; retrospective evaluation of full BCS: total score 13,065 (6/7 puzzles perfect, 1 puzzle 1 letter wrong).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Strong: BP explicitly encodes spatial structure (cells and their intersection with clues); empirical analyses show BP and local search substantially increase accuracy by exploiting crossing-letter constraints (e.g., top-1000 recall almost always leads to correct fill after BP), and removal of local search drastically reduces perfect-puzzle accuracy (81.7% -> 44.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to prior systems Proverb, WebCrow, and Dr.Fill. Ablation table: BCS QA + BP + LS (81.7% perfect) vs BCS QA + BP (44.3%) vs BCS QA + Dr.Fill Solver (73.7%) vs Dr.Fill QA + Dr.Fill Solver (70.5%). Also compared QA top-k recall against Dr.Fill QA (94.6% vs 84.4% top-1000).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Major failures on themed puzzles (21 of 43 unsolved NYT 2021 puzzles were theme-related), local search proposal failures (9 puzzles), local search scoring failures (9 puzzles where ByT5 mis-scored proposals), connected errors where overconfident QA + BP fill multiple connected errors, and closed-book limitations (≈4% unseen answers). Temporal distribution shift also produced errors (example: FAUCI misfilled). The system does not explicitly handle many theme types (rebuses, circled letters), which Dr.Fill handles with heuristic modules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Crossword Solving', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A probabilistic approach to solving crossword puzzles <em>(Rating: 2)</em></li>
                <li>Dr. Fill: Crosswords and an implemented solver for singly weighted CSPs <em>(Rating: 2)</em></li>
                <li>WebCrow: a web-based system for crossword solving <em>(Rating: 2)</em></li>
                <li>Cryptonite: A cryptic crossword benchmark for extreme ambiguity in language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8554",
    "paper_id": "paper-32afdb07021fda775ceaedd231c58bfed0aa980a",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "BCS Bi-Encoder",
            "name_full": "BERT-base bi-encoder QA model (clue and answer encoders)",
            "brief_description": "A retrieval-style question-answering model using two BERT-base-uncased encoders (one for clues, one for answers) trained to embed clues and candidate answers into a shared space; used as the first-pass closed-book candidate generator for crossword solving.",
            "citation_title": "Automated Crossword Solving",
            "mention_or_use": "use",
            "model_name": "BERT-base bi-encoder",
            "model_description": "Two encoders (E_C for clues, E_A for answers) initialized from BERT-base-uncased; trained like DPR with one distractor per example; maps clues and answers into a joint embedding space and scores answers by dot product similarity; precomputes answer embeddings and uses FAISS for fast retrieval from a closed answer set of 437.8K answers.",
            "model_size": null,
            "puzzle_name": "Crossword (NYT, LA Times, Newsday, The New Yorker, The Atlantic)",
            "puzzle_type": "Grid-based word puzzle with crossing-letter constraints (2D grid where each cell belongs to two words); requires reasoning about letter positions and consistency across intersecting answers.",
            "task_setup": "First-pass closed-book retrieval: for each clue compute embedding and retrieve top-k candidates (they focus on top-1000) from the training answer set; produce softmax probabilities over retrieved candidates; segmented answers (via a segmentation model) are used for training/inference.",
            "mechanisms_or_strategies": "Retrieval-style bi-encoder (closed-book) to produce candidate lists; answers restricted to training answer set; used downstream with loopy belief propagation (BP) on a bipartite clue-cell graph to enforce spatial (crossing-letter) constraints, greedy inference to produce a solution, and local search to refine errors.",
            "performance_metrics": "Top-1000 recall on 2021 NYT test: 94.6% (Dr.Fill QA baseline: 84.4%). Oracle recall (accounting for unseen answers) ≈ 96%. When combined in full pipeline: BCS (with BP+LS) achieves 81.7% perfect-puzzle accuracy on NYT 2021 (Dr.Fill baseline 70.5%); themeless NYT puzzles: 89.5% perfect puzzles and 99.9% letter accuracy reported for BCS.",
            "evidence_of_spatial_reasoning": "Explicit: BP constructs a bipartite graph with clue nodes and cell nodes and passes messages to enforce consistency of letters across intersecting words; results show that when the true answer is in the bi-encoder's top-1000, BP and subsequent steps almost always fill it correctly (Figure 11), indicating the model leverages crossing-letter (spatial) constraints.",
            "comparisons": "Compared against Dr.Fill's QA module (TFIDF-like ensemble): bi-encoder improves top-1000 recall from 84.4% to 94.6%. Ablations: full pipeline (BCS QA + BP + LS) 81.7% perfect vs BCS QA + BP (no LS) 44.3% and BCS QA + Dr.Fill solver 73.7% showing superiority of the BCS components.",
            "limitations_or_failure_cases": "Closed-book constraint: ≈4% of test answers absent from answer set, limiting recall; QA errors concentrated on knowledge, wordplay, and cross-reference clues; bi-encoder can be overconfident and cause BP to converge to suboptimal solutions; temporal shift (e.g., FAUCI error) and themed puzzles (rebuses, multi-letter cells) are major failure modes.",
            "uuid": "e8554.0",
            "source_info": {
                "paper_title": "Automated Crossword Solving",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "ByT5 Scorer",
            "name_full": "ByT5-small character-level generative QA model (second-pass scorer)",
            "brief_description": "A character/byte-level generative model finetuned to generate answers from clues and used to score candidate puzzle proposals in local search, chosen for robustness against nonsensical high-confidence candidates from the bi-encoder.",
            "citation_title": "Automated Crossword Solving",
            "mention_or_use": "use",
            "model_name": "ByT5-small",
            "model_description": "Character/byte-level encoder-decoder (ByT5-small) finetuned on the crossword QA training set to generate answers from clues; used to compute likelihoods P(answer | clue) and score whole-puzzle proposals by the product of likelihoods across clues.",
            "model_size": null,
            "puzzle_name": "Crossword",
            "puzzle_type": "Grid-based word puzzle with crossing-letter constraints (2D grid).",
            "task_setup": "During local search, propose candidate puzzles (1–2 letter edits guided by BP character marginals or dictionary/segmentation rules), then score each candidate by computing the product over clues of ByT5's P(a_j | c_j); accept best-scoring edits iteratively until no improvement.",
            "mechanisms_or_strategies": "Generative likelihood scoring at character/byte level (more robust to odd candidates than bi-encoder discriminative scores); used in an iterative local search that proposes small edits (within 2-letter edit distance) focused on uncertain letters from BP marginals.",
            "performance_metrics": "Local search (scored by ByT5) applied 243 edits that improved accuracy and 31 edits that hurt across 234 NYT test puzzles; including local search raised perfect-puzzle accuracy from 44.3% (BCS QA + BP) to 81.7% (full BCS).",
            "evidence_of_spatial_reasoning": "ByT5 is applied only to score candidate puzzles that respect grid constraints (edits are constrained by BP and segmentation/dictionary checks); improvements in puzzle/letter accuracy after ByT5-scored local search indicate that the scorer effectively helps correct spatially constrained letter errors.",
            "comparisons": "Authors found ByT5 empirically more robust than scoring candidate puzzles with the bi-encoder (which sometimes assigned high confidence to nonsensical fills); ByT5-based local search is a key component in outperforming Dr.Fill and in ablation studies.",
            "limitations_or_failure_cases": "Local search scoring failures: 9 puzzles where ByT5 rejected correct proposals or accepted incorrect ones (classified in error analysis as 'Local Search Scoring'); overall local search also produced some harmful edits (31 across 234 puzzles).",
            "uuid": "e8554.1",
            "source_info": {
                "paper_title": "Automated Crossword Solving",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Segmentation GPT-2",
            "name_full": "GPT-2 small fine-tuned word segmentation model",
            "brief_description": "A GPT-2 small model finetuned to map canonical unsegmented crossword fill strings (ALLCAPS, no spaces/punctuation) to segmented natural-language forms to improve downstream QA model tokenization and performance.",
            "citation_title": "Automated Crossword Solving",
            "mention_or_use": "use",
            "model_name": "GPT-2 small (segmentation model)",
            "model_description": "GPT-2 small finetuned to generate segmented n-grams from their unsegmented counterparts (trained on frequent n-grams from Wikipedia with spaces/punctuation removed); applied to all answers so QA models can be trained/evaluated on segmented text.",
            "model_size": null,
            "puzzle_name": "Crossword",
            "puzzle_type": "Grid-based word puzzle with crossing-letter constraints.",
            "task_setup": "Preprocessing step: run the segmenter on canonical crossword answers (e.g., DAAABEARS -&gt; 'daaa bears') to produce segmented forms for QA training and to map model outputs back to crossword fill format by removing spaces/punctuation.",
            "mechanisms_or_strategies": "Sequence generation for segmentation to align crossword fills with natural-language tokenization of transformer models; improves QA model handling of multiword or unusual answers.",
            "performance_metrics": "No explicit numeric evaluation of segmentation accuracy reported in the paper; segmentation model used across the 6.4M QA training set and in all downstream experiments.",
            "evidence_of_spatial_reasoning": "None — segmentation is a preprocessing step and does not itself perform spatial reasoning for puzzle solving.",
            "comparisons": "Authors note simpler rule-based segmentation (splitting into known English words) is insufficient for many crossword answers and motivated using a learned GPT-2 model.",
            "limitations_or_failure_cases": "No quantitative failure cases reported, but authors state simpler approaches were insufficient for many nonstandard fills (e.g., DAAABEARS, EENYMEENYMINYMOE).",
            "uuid": "e8554.2",
            "source_info": {
                "paper_title": "Automated Crossword Solving",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "BCS (end-to-end)",
            "name_full": "Berkeley Crossword Solver (BCS)",
            "brief_description": "An end-to-end crossword solving pipeline combining a neural bi-encoder QA model (BERT-base), loopy belief propagation for constraint resolution, greedy decoding, and local search scored by a character-level ByT5; achieves state-of-the-art puzzle solving performance.",
            "citation_title": "Automated Crossword Solving",
            "mention_or_use": "use",
            "model_name": "BCS pipeline (GPT-2 segmenter + BERT bi-encoder + BP + Greedy + ByT5 local search)",
            "model_description": "Multi-stage system: (1) segment answers with GPT-2 small; (2) generate candidate answers with a closed-book BERT-base bi-encoder over a 437.8K answer set and retrieve top-k (FAISS); (3) resolve letter constraints with loopy BP on a bipartite clue-cell graph producing marginals; (4) greedy decoding to produce initial fill; (5) local search proposing small letter edits scored by ByT5-small likelihoods.",
            "model_size": null,
            "puzzle_name": "Crossword (various publishers; primary eval on 2020/2021 NYT puzzles)",
            "puzzle_type": "Grid-based word puzzle with crossing-letter constraints; includes themed puzzles with rebuses or multi-letter cells that complicate spatial mapping.",
            "task_setup": "Multi-stage decoding described above; evaluation on held-out set of complete NYT puzzles (408 total test puzzles from multiple publishers), with detailed ablation experiments (e.g., removing LS or swapping components with Dr.Fill).",
            "mechanisms_or_strategies": "Combines neural QA for candidate generation with probabilistic structured inference (loopy BP) to enforce spatial constraints and local search to repair remaining errors; BP constructs clue and cell nodes and passes messages to enforce letter consistency across crossings.",
            "performance_metrics": "Main results: on NYT test puzzles (234 puzzles) BCS achieves 81.7% perfect-puzzle accuracy (Dr.Fill baseline 70.5%), word accuracy 98.9% vs 97.9%, letter accuracy 99.7% vs 99.2%; themeless NYT puzzles: 89.5% perfect and 99.9% letter accuracy. In the 2021 ACPT live tournament the submitted system (BCS QA + Dr.Fill solver variant) outperformed all 1033 humans; retrospective evaluation of full BCS: total score 13,065 (6/7 puzzles perfect, 1 puzzle 1 letter wrong).",
            "evidence_of_spatial_reasoning": "Strong: BP explicitly encodes spatial structure (cells and their intersection with clues); empirical analyses show BP and local search substantially increase accuracy by exploiting crossing-letter constraints (e.g., top-1000 recall almost always leads to correct fill after BP), and removal of local search drastically reduces perfect-puzzle accuracy (81.7% -&gt; 44.3%).",
            "comparisons": "Directly compared to prior systems Proverb, WebCrow, and Dr.Fill. Ablation table: BCS QA + BP + LS (81.7% perfect) vs BCS QA + BP (44.3%) vs BCS QA + Dr.Fill Solver (73.7%) vs Dr.Fill QA + Dr.Fill Solver (70.5%). Also compared QA top-k recall against Dr.Fill QA (94.6% vs 84.4% top-1000).",
            "limitations_or_failure_cases": "Major failures on themed puzzles (21 of 43 unsolved NYT 2021 puzzles were theme-related), local search proposal failures (9 puzzles), local search scoring failures (9 puzzles where ByT5 mis-scored proposals), connected errors where overconfident QA + BP fill multiple connected errors, and closed-book limitations (≈4% unseen answers). Temporal distribution shift also produced errors (example: FAUCI misfilled). The system does not explicitly handle many theme types (rebuses, circled letters), which Dr.Fill handles with heuristic modules.",
            "uuid": "e8554.3",
            "source_info": {
                "paper_title": "Automated Crossword Solving",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A probabilistic approach to solving crossword puzzles",
            "rating": 2
        },
        {
            "paper_title": "Dr. Fill: Crosswords and an implemented solver for singly weighted CSPs",
            "rating": 2
        },
        {
            "paper_title": "WebCrow: a web-based system for crossword solving",
            "rating": 2
        },
        {
            "paper_title": "Cryptonite: A cryptic crossword benchmark for extreme ambiguity in language",
            "rating": 1
        }
    ],
    "cost": 0.01669225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automated Crossword Solving</h1>
<p>Eric Wallace*<br>UC Berkeley<br>Nicholas Tomlin ${ }^{+}$<br>Albert Xu ${ }^{+}$<br>UC Berkeley<br>Kevin Yang ${ }^{+}$<br>UC Berkeley<br>Eshaan Pathak ${ }^{+}$<br>Matthew L. Ginsberg<br>Dan Klein<br>UC Berkeley Matthew Ginsberg, LLC UC Berkeley<br>{ericwallace, nicholas_tomlin, albertxu3, klein}@berkeley.edu</p>
<h4>Abstract</h4>
<p>We present the Berkeley Crossword Solver, a state-of-the-art approach for automatically solving crossword puzzles. Our system works by generating answer candidates for each crossword clue using neural question answering models and then combines loopy belief propagation with local search to find full puzzle solutions. Compared to existing approaches, our system improves exact puzzle accuracy from $71 \%$ to $82 \%$ on crosswords from The New York Times and obtains $99.9 \%$ letter accuracy on themeless puzzles. Additionally, in 2021, a hybrid of our system and the existing Dr.Fill system outperformed all human competitors for the first time at the American Crossword Puzzle Tournament. To facilitate research on question answering and crossword solving, we analyze our system's remaining errors and release a dataset of over six million question-answer pairs.</p>
<h2>1 Introduction</h2>
<p>"The key to solving crosswords is mental flexibility. If one answer doesn't seem to be working out, try something else."</p>
<ul>
<li>Will Shortz, NYT Crossword Editor</li>
</ul>
<p>Crossword puzzles are perhaps the world's most popular language game, with millions of solvers in the United States alone (Ginsberg, 2011). Crosswords test knowledge of word meanings, trivia, commonsense, and wordplay, while also requiring one to simultaneously reason about multiple intersecting answers. Consequently, crossword puzzles provide a testbed to study open problems in AI and NLP, ranging from question answering to search and constraint satisfaction. In this paper, we describe an end-to-end system for solving crossword puzzles that tackles many of these challenges.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A partially-solved example crossword puzzle from the 2021 American Crossword Puzzle Tournament, where our system scored higher than all 1033 human solvers. The highlighted fill KUNGFU answers the wordplay clue: Something done for kicks?</p>
<h3>1.1 The Crossword Solving Problem</h3>
<p>Crossword puzzles are word games consisting of rectangular grids of squares that are to be filled in with letters based on given clues (e.g., Figure 1). Puzzles typically consist of 60-80 clues that vary in difficulty due to the presence of complex wordplay, intentionally ambiguous clues, or esoteric knowledge. Each grid cell belongs to two words, meaning that one must jointly reason about answers to multiple questions. Most players complete crosswords that are published daily in newspapers and magazines such as The New York Times (NYT), while other more expert enthusiasts also compete in live events such as the American Crossword Puzzle Tournament (ACPT). These events are intensely competitive: one previous winner reportedly solved twenty puzzles per day as practice (Grady, 2010), and top competitors can perfectly solve expert-level puzzles with over 100 clues in just 3 minutes.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Clue</th>
<th>Answer</th>
<th>QA Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>Knowledge (37\%)</td>
<td>Birds on Minnesota state quarters <br> Architect Frank</td>
<td>LOONS <br> GEHRY</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Definition (33\%)</td>
<td>First in a series <br> Tusked savanna dweller</td>
<td>PILOT <br> WARTHOG</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Commonsense (14\%)</td>
<td>Like games decided by buzzer beaters <br> Opposite of luego</td>
<td>CLOSE <br> AHORA</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Wordplay (8\%)</td>
<td>Frequent book setting <br> One followed by nothing?</td>
<td>SHELF <br> TEN</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Phrase (8\%)</td>
<td>"Is it still a date?" <br> "Post ___ analysis"</td>
<td>AREWEEN <br> HOC</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Cross-Reference (2\%)</td>
<td>See Capital of 52-Down <br> Oft-wished-upon sighting</td>
<td>GHANA <br> SHOOTINGMETEOR</td>
<td>$\boldsymbol{\Omega}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Types of reasoning used in The New York Times Crossword. We compute each type's frequency by manually analyzing 200 clues. See Appendix A for category definitions. We also indicate if our QA model correctly predicts each answer based on top-1000 recall. Cross-reference clues mention other clues or themes, e.g., SHOOTINGMETEOR replaces the clued phrase SHOOTINGSTAR based on the context from the puzzle.</p>
<p>Automated crossword solvers have been built in the past and can outperform most hobbyist humans. Two of the best such systems are Proverb (Littman et al., 2002) and Dr.Fill (Ginsberg, 2011). Despite their reasonable success, past systems struggle to solve the difficult linguistic phenomena present in crosswords, and they fail to outperform expert humans. At the time of its publication, Proverb would have ranked 213th out of 252 in the ACPT. Dr.Fill would have placed 43rd at publication and has since improved to place as high as 11th in the 2017 ACPT.</p>
<h3>1.2 A Testbed for Question Answering</h3>
<p>Answering crossword clues involves challenges not found in traditional question answering (QA) benchmarks. The clues are typically less literal; they span different reasoning types (c.f., Table 1); and they cover diverse linguistic phenomena such as polysemy, homophony, puns, and other types of wordplay. Many crossword clues are also intentionally underspecified, and to solve them, one must be able to "know what they don't know" and defer answering those clues until crossing letters are known. Crosswords are also useful from a practical perspective as the data is abundant, wellvalidated, diverse, and constantly evolving. In particular, there are millions of question-answer pairs online, and unlike crowdsourced datasets that are often rife with artifacts (Gururangan et al., 2018; Min et al., 2019), crossword clues are written and validated by experts. Finally, crossword data is diverse as it spans many years of pop culture, is written by thousands of different constructors, and contains various publisher-specific idiosyncrasies.</p>
<h3>1.3 A Testbed For Constraint Satisfaction</h3>
<p>Solving crosswords goes beyond just generating answers to each clue. Without guidance from a constraint solver, QA models cannot reconcile crossing letter and length constraints. Satisfying these constraints is challenging because the search space is enormous and many valid solutions exist, only one of which is correct. Moreover, due to miscalibration in the QA model predictions, exact inference may also lead to solutions that are high-likelihood but completely incorrect, similar to other types of structured decoding problems in NLP (Stahlberg and Byrne, 2019; Kumar and Sarawagi, 2019). Finally, the challenges in search are amplified by the unique long tail of crossword answers, e.g., "daaa bears" or "eeny meeny miny moe," which makes it highly insufficient to restrict the search space to solutions that contain only common English words.</p>
<h3>1.4 The Berkeley Crossword Solver</h3>
<p>We present the Berkeley Crossword Solver (BCS), which is summarized in Figure 2. The BCS is based on the principle that some clues are difficult to answer without any letter constraints, but other (easier) clues are more standalone. This naturally motivates a multi-stage solving approach, where we first generate answers for each question independently, fill in the puzzle using those answers, and then rescore uncertain answers while conditioning on the predicted letter constraints. We refer to</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of the Berkeley Crossword Solver. We use a neural question answering model to generate answer probabilities for each question, and then refine the probabilities with loopy belief propagation. Finally, we fill the grid with greedy search and iteratively improve uncertain areas of the puzzle using local search.</p>
<p>These stages as first-pass QA, constraint resolution, and local search, and we describe each component in Sections 3–5 after describing our dataset in Section 2. In Section 6, we show that the BCS substantially improves over the previous state-of-the-art Dr.Fill system, perfectly solving 82% of crosswords from <em>The New York Times</em>, compared to 71% for Dr.Fill. Nevertheless, room for additional improvement remains, especially on the QA front. To facilitate further exploration, we publicly release our code, models, and dataset: https://github.com/albertkx/berkeley-crossword-solver.</p>
<h2>2 Crossword Dataset</h2>
<p>This section describes the dataset that we built for training and evaluating crossword solving systems. Recall that a crossword puzzle contains both question-answer pairs and an arrangement of those pairs into a grid (e.g., Figure 1). Unfortunately, complete crossword puzzles are protected under copyright agreements; however, their individual question-answer pairs are free-to-use. Our dataset efforts thus focused on collecting numerous question-answer pairs (Section 2.1) and we collected a smaller set of complete puzzle grids to use for final evaluation (Section 2.2).</p>
<h3>2.1 Collecting Question-Answer Pairs</h3>
<p>We collected a dataset of over six million question-answer pairs from top online publishers such as <em>The New York Times</em>, <em>The LA Times</em>, and <em>USA Today</em>. We show qualitative examples in Table 1, summary statistics in Table 2, and additional breakdowns in Appendix B. Compared to existing QA datasets, our crossword dataset represents a unique and challenging testbed as it is large and carefully labeled, is varied in authorship, spans over 70 years of pop culture, and contains examples that are difficult for even expert humans. We built validation and test sets by splitting off every question-answer pair used in the 2020 and 2021 NYT puzzles. We use recent NYT puzzles for evaluation because the NYT is the most popular and well-validated crossword publisher, and because using newer puzzles helps to evaluate temporal distribution shift.</p>
<p>Word Segmentation of Answers Crossword answers are canonically filled in using all capital letters and without spaces or punctuation, e.g., "<em>whale that stinks</em>" becomes WHALETHATSTINKS. These unsegmented answers may confuse neural QA models that are pretrained on natural English text that is tokenized into wordpieces. To remedy this, we trained a word segmentation model that maps the clues to their natural language form.<sup>1</sup> We collected segmentation training data by retrieving common <em>n</em>-grams from Wikipedia and removing their spaces and punctuation. We then finetuned GPT-2 small (Radford et al., 2019) to generate the segmented <em>n</em>-gram given its unsegmented version. We ran the segmenter on all answers in our data. In all our experiments, we train our QA models using segmented answers and we post-hoc remove spaces and punctuation from their predictions.</p>
<h3>2.2 Collecting Complete Crossword Puzzles</h3>
<p>To evaluate our final crossword solver, we collected a validation and test set of complete 2020 and 2021</p>
<p><sup>1</sup>More simplistic algorithms that segment the answer into known English words are insufficient for many crossword answers, e.g., DAAABEARS and EENYMEENYMINYMOE.</p>
<table>
<thead>
<tr>
<th></th>
<th>Train</th>
<th>Validation</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>QA Pairs</td>
<td>6.4 M</td>
<td>30.4 K</td>
<td>21.3 K</td>
</tr>
<tr>
<td>Answer Set</td>
<td>437.8 K</td>
<td>17.2 K</td>
<td>13.4 K</td>
</tr>
<tr>
<td>Timeframe</td>
<td>1951-2019</td>
<td>2020</td>
<td>2021</td>
</tr>
</tbody>
</table>
<p>Table 2: Summary statistics of our QA dataset. We collect question-answer pairs from 26 sources (The LA Times, The New York Times, etc.) for training, and we hold out the latest data from NYT for validation and testing. Our dataset is large and contains a wide range of authors, answers, puzzle sources, and years.
puzzle grids. We use puzzles from The New York Times, The LA Times, Newsday, The New Yorker, and The Atlantic. Using multiple publishers for evaluation provides a unique challenge as each publisher contains different idiosyncrasies, answer distributions, and crossword styles. We use 2020 NYT as our validation set and hold out all other puzzles for testing. There are 408 total test puzzles.</p>
<h2>3 Bi-Encoder QA Model</h2>
<p>The initial step of the BCS is question answering: we generate a list of possible answer candidates and their associated probabilities for each clue. A key requirement for this QA model is that it does not output unreasonable or overly confident answers for hard clues. Instead, this model is designed to be used as a "first-pass" that generates reasonable candidates for every clue, in hope that harder clues can be reconciled later when predicted letter constraints are available. We achieve this by restricting our first-pass QA model to only output answers that are present in the training set. As discussed in Section 5, we later generate answers outside of this closed-book set with our second-pass QA model.</p>
<p>Model Architecture We build our QA model based on a bi-encoder architecture (Bromley et al., 1994; Karpukhin et al., 2020) due to its ability to score numerous answers efficiently and learn using few examples per answer. We have two neural network encoders: $\mathrm{E}<em _mathrm_A="\mathrm{A">{\mathrm{C}}(\cdot)$, the clue encoder, and $\mathrm{E}</em>}}(\cdot)$, the answer encoder. Both encoders are initialized with BERT-base-uncased (Devlin et al., 2019) and output the encoder's [CLS] representation as the final encoding. These two encoders are trained to map the questions and answers into the same feature space. Given a clue $c$, the model scores all possible answers $a_{i}$ using a dot product similarity function between feature vectors: $\operatorname{sim}\left(c, a_{i}\right)=\mathrm{E<em _mathrm_A="\mathrm{A">{\mathrm{C}}(c)^{\mathrm{T}} \mathrm{E}</em>$}}\left(a_{i}\right)$. Our answer set consists of the 437.8 K answers in the training data. ${ }^{2</p>
<p>Training We train the encoders in the same fashion as DPR (Karpukhin et al., 2020): batches consist of clues, answers, and "distractor" answers. The two encoders are trained jointly to assign a high similarity to the correct question-answer pairs and low similarity to all other pairs formed between the clue and distractor answers. We use one distractor answer per clue that we collect by searching each clue in the training set using TFIDF and returning the top incorrect answer. We tune hyperparameters of our bi-encoder model based on its top- $k$ accuracy on the NYT validation set.</p>
<p>Inference At test time, for each clue $c$, we compute the embedding $v_{c}=E_{C}(c)$ and retrieve the answers whose embeddings have the highest dot product similarity with $v_{c}$. We obtain probabilities for each answer by softmaxing the dot product scores. To speed up inference, we precompute the answer embeddings and use FAISS (Johnson et al., 2019) for similarity scoring.</p>
<h3>3.1 Top-k Recall of Our QA Model</h3>
<p>To evaluate our bi-encoder, we compute its top- $k$ recall on the question-answer pairs from the NYT test set. We are most interested in top-1000 recall, as we found it to be highly-correlated with downstream solving performance (discussed in Section 7). As a baseline, we compare against the QA portion of the previous state-of-the-art Dr.Fill crossword solver (Ginsberg, 2011). This QA model works by ensembling TFIDF-like scoring and numerous additional modules (e.g., synonym matching, POS matching). Our bi-encoder model considerably outperforms Dr.Fill, improving top-1000 recall from $84.4 \%$ to $94.6 \%$ (Figure 3). Also note that approximately $4 \%$ of test answers are not seen during training, and thus the oracle recall for our first-pass QA model is $\approx 96 \%$.</p>
<h2>4 Resolving Letter Constraints Using BP</h2>
<p>Given the list of answer candidates and their associated probabilities from the first-pass QA model, we next built a solver that produces a puzzle solution</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: <em>Top-k accuracy on the 2021 NYT test set</em>. Dr. Fill QA is an existing crossword QA system that ensembles TFIDF-like scoring with numerous additional scoring modules. Our neural bi-encoder model improves top-1000 accuracy from 84.4% to 94.6%.</p>
<p>That satisfies the letter constraints. Formally, crossword solving is a weighted constraint satisfaction problem, where the probability over solutions is given by the product of the confidence scores produced by the QA model (Ginsberg, 2011). There are numerous algorithms for solving such problems, including branch-and-bound, integer linear programming, and more.</p>
<p>We use belief propagation (Pearl, 1988), henceforth BP, for two reasons. First, BP directly searches for the solution with the highest <em>expected overlap</em> with the ground-truth solution, rather than the solution with the highest likelihood under the QA model (Littman et al., 2002). This is advantageous as it maximizes the total number of correct words and letters in the solution, and it also avoids strange solutions that may have spuriously high scores under the QA model. Second, BP also produces marginal distributions over words and characters, which is useful for generating an <em>n</em>-best list of solution candidates (used in Section 5).</p>
<p><strong>Loopy Belief Propagation</strong> We use loopy BP, inspired by the Proverb crossword solver (Littman et al., 2002). That is, we construct a bipartite graph with nodes for each of the crossword's clues and cells. For each clue node, we connect it via an edge to each of its associated cell nodes (e.g., a 5-letter clue will have degree 5 in the constructed graph). Each clue node maintains a belief state over answers for that clue, which is initialized using a mixture of the QA model's probabilities and a uniform letter LM.<sup>3</sup> Each cell node maintains a belief state over letters for that cell. We then iteratively apply BP with each iteration doing message passing for all clue nodes in parallel and then for all cell nodes in parallel. The algorithm empirically converges after 5–10 iterations and completes in just 10 seconds on a single-threaded Python process.</p>
<p><strong>Greedy Inference</strong> BP produces a marginal distribution over words for each clue. To generate an actual puzzle solution, we run greedy search where we first fill in the answer with the highest marginal likelihood, remove any crossing answers that do not share the same letter, and repeat.</p>
<h1>5 Iteratively Improving Puzzle Solutions</h1>
<p>Many of the puzzle solutions generated by BP are close to correct but have small letter mistakes, e.g., NAUCI instead of FAUCI or TAZOAMBASSADORS instead of JAZZAMBASSADORS, as shown in Figure 4.<sup>4</sup> We remedy this in the final stage of the BCS with local search (LS), where we take a "second-pass" through the puzzle and score alternate proposals that are a small edit distance away from the BP solution. In particular, we alternate between proposing new candidate solutions by flipping uncertain letters and scoring those proposals using a second-pass QA model.</p>
<p><strong>Proposing Alternate Solutions</strong> Similar to related problems in structured prediction (Stahlberg and Byrne, 2019) or model-based optimization (Fu and Levine, 2021), the key challenge in searching for alternate puzzle solutions is to avoid false positives and adversarial inputs. If we score <em>every</em> proposal within a small edit distance to the original, we are bound to find nonsensical character flips that nevertheless lead to higher model scores. We avoid this by only scoring proposals that are within a 2-letter edit distance and also have nontrivial likelihoods according to BP or a dictionary. Specifically, we score all proposals whose 1–2 modified letters each have probability 0.01 or greater under the char-</p>
<p><sup>3</sup>The unigram letter LM accounts for the probability that an answer is not in our answer set. We build the LM by counting the frequency of each letter in our QA training set.</p>
<p><sup>4</sup>These errors stem from multiple sources. First, 4% of the answers in a test crossword are not present in our bi-encoder's answer set. Those answers will be not be filled in correctly unless the solver can identify the correct answer for <em>all</em> of the crossing answers. Second, natural QA errors exist even on questions with non-novel answers. Finally, the BP algorithm may converge to a sub-optimal solution.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: We show the result of our solver on a NYT puzzle after running greedy search and three consecutive steps of local search. Local search considerably improves accuracy but fails to fix the answer regarding Dr. Fauci (an error due to temporal shift in our QA models). Red squares indicate errors from the output of greedy search, while green squares indicate corrections from the local search. See Figure 12 for the clues and associated answers in the puzzle.</p>
<p>acter marginal probabilities produced by BP.5 We also score all proposals whose 1–2 modified letters cause the corresponding answer to segment into valid English words.6</p>
<p><strong>Scoring Solutions With Second-Pass QA</strong> Given the alternate puzzle solutions, we could feed each of them into our bi-encoder model for scoring. However, we found that bi-encoders are not robust—they sometimes produce high-confidence predictions for the nonsensical answers present in some candidate solutions. We instead use generative QA models to score the proposed candidates as we found these models to be empirically more robust. We finetuned the character-level model ByT5-small (Xue et al., 2022) on our training set to generate the answer from a given clue. We then score each proposed candidate using the product of the model's likelihoods of the answers given the clues, $\prod_{j} P(a_j \mid c_j)$.</p>
<p>After scoring all candidate proposals, we apply the best-scoring edit and repeat the proposal and scoring process until no better edits exist. Figure 4 shows an example of the candidates accepted by LS. Quantitatively, we found that LS applied 243 edits that improved accuracy and 31 edits that hurt accuracy across 234 NYT test puzzles.</p>
<p>5 The character-level marginal distribution for most characters assigns all probability mass to a single letter after a few iterations of BP (e.g., probability 0.9999). We empirically chose 0.01 as it achieved the highest validation accuracy.</p>
<p>6 For instance, given a puzzle that contains a fill such as MUNNYANDCLYDE, we consider alternate solutions that contain answers such as BUNNYANDCLYDE and SUNNYANDCLYDE, as they segment to "<em>bunny and clyde</em>" and "<em>sunny and clyde</em>."</p>
<h2>6 End-to-End System Results</h2>
<p>We evaluate our final system on our set of test puzzles and compare the results to the state-of-the-art Dr.Fill system (Ginsberg, 2011). We compute three accuracy metrics: perfect puzzle, word, and letter. Perfect puzzle accuracy requires answering every clue in the puzzle correctly and serves as our primary—and most challenging—metric.</p>
<p>Table 3 shows our main results. We outperform Dr.Fill on perfect puzzle accuracy across crosswords from every publication source. For example, we obtain a 11.2% absolute improvement on perfect puzzle accuracy on crossword puzzles from <em>The New York Times</em>, which is a statistically significant improvement (p &lt; 0.01) according to a paired t-test. We also observe comparable or better word and letter accuracies than Dr.Fill across all sources. Our improvement on puzzles from <em>The New Yorker</em> is relatively small; this discrepancy is possibly due to the small amount of data from <em>The New Yorker</em> in our training set (see Figure 7).</p>
<p><strong>Themed vs. Themeless Puzzles</strong> Although the BCS achieves equivalent or worse letter accuracy on <em>Newsday</em> and <em>LA Times</em> puzzles, it obtains substantially higher puzzle accuracy on these splits. We attribute this behavior to errors concentrated in unique themed puzzles, e.g., ones that place multiple letters into a single cell. To test this, we break down NYT puzzles into those with and without special theme entries (see Appendix D for our definition of theme puzzles). On themeless NYT puzzles, we achieve 99.9% letter accuracy and 89.5% perfect puzzles, showing that themed puzzles are a major source of our errors. Note that the Dr.Fill system includes various methods to detect and resolve themes and is thus more competitive on such</p>
<p>|  |  | Perfect Puzzle (\%) | Word Acc. (\%) | Letter Acc. (\%) |  |
| Source | # Puzzles | Dr. Fill | BCS | Dr. Fill | BCS | Dr. Fill | BCS |
| --- | --- | --- | --- | --- | --- | --- | --- |
| The Atlantic | 46 | 82.6 | $\mathbf{8 9 . 1}$ | 98.5 | $\mathbf{9 9 . 1}$ | 99.7 | $\mathbf{9 9 . 8}$ |
| Newsday | 52 | 86.2 | $\mathbf{9 4 . 2}$ | 98.6 | $\mathbf{9 9 . 6}$ | 99.1 | $\mathbf{9 9 . 8}$ |
| The New Yorker | 22 | $\mathbf{8 6 . 4}$ | 77.2 | $\mathbf{9 9 . 5}$ | 98.9 | $\mathbf{9 9 . 9}$ | 99.8 |
| The LA Times | 54 | 81.5 | $\mathbf{9 2 . 6}$ | 99.4 | $\mathbf{9 9 . 7}$ | 99.9 | 99.9 |
| The New York Times | 234 | 70.5 | $\mathbf{8 1 . 7}$ | 97.9 | $\mathbf{9 8 . 9}$ | 99.2 | $\mathbf{9 9 . 7}$ |</p>
<p>Table 3: Final results of the Berkeley Crossword Solver. We compare the BCS to Dr. Fill, the previous state-of-the-art crossword solving system, on a range of puzzle sources. The BCS produces significantly more perfect puzzles and achieves better or comparable letter-level and word-level accuracies.
puzzles, although it still underperforms our system.
American Crossword Puzzle Tournament For our last evaluation, we submitted a system to participate live in the American Crossword Puzzle Tournament (ACPT), the longest-running and most prestigious human crossword tournament. Our team obtained special permission from the organizers to participate in the 2021 version of the tournament, along with 1033 human competitors. For the live tournament, we used an earlier system, which does not use belief propagation or local search but instead uses Dr.Fill's constraint-resolution system along with the BCS QA modules described above. The submitted system outperformed all of the human participants - we had a total score of 12,825 compared to the top human who had 12,810 (scoring details in Appendix C). Figure 5 shows our scores compared to the top and median human competitor on the 7 puzzles used in the competition. We also retrospectively evaluated the final BCS system as detailed in this paper (i.e., using our solver based on belief propagation and local search), and achieved a higher total score of 13,065. This corresponds to getting 6 out of the 7 puzzles perfect and 1 letter wrong on 1 puzzle.</p>
<p>System Ablations We also investigated the importance of our QA model, BP inference, and local search with an ablation study. Table 4 shows results for perfect puzzle accuracy on NYT 2021 puzzles under different settings. The first ablation shows that our local search step is crucial for our solver to achieve high accuracy. The second and third ablations show that the BCS's QA and solver are both superior to their counterparts from Dr.Fillswapping out either component hurts accuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: center;">Puzzle (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BCS QA + BP + LS</td>
<td style="text-align: center;">81.7</td>
</tr>
<tr>
<td style="text-align: left;">BCS QA + BP</td>
<td style="text-align: center;">44.3</td>
</tr>
<tr>
<td style="text-align: left;">BCS QA + Dr.Fill Solver</td>
<td style="text-align: center;">73.7</td>
</tr>
<tr>
<td style="text-align: left;">Dr.Fill QA + Dr.Fill Solver</td>
<td style="text-align: center;">70.5</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablations on NYT puzzles. Our full system consists of a bi-encoder QA model, loopy belief propagation (BP), and local search (LS). We find that our QA and solver are both superior to that of Dr.Fill and that our local search step is key to achieving high accuracy.</p>
<h2>7 Error Analysis</h2>
<p>Although our system obtains near-perfect accuracy on a wide variety of puzzles, we maintain that crosswords are not yet solved. In this section, we show that substantial headroom remains on QA accuracy and the handling of themed puzzles.</p>
<p>QA Error Analysis We first measured how well a QA model needs to perform on each clue in order for our solver to find the correct solution. We found that when our QA model ranks the true answer within the top 1,000 predictions, the answer is almost always filled in correctly (Figure 11). Despite top-1000 accuracy typically being sufficient, our QA model still makes numerous errors. We manually analyzed these mistakes by sampling 200 errors from the NYT 2021 puzzles and placing them in the same categories used in Table 1. Figure 6 shows the results and indicates that knowledge, wordplay, and cross-reference clues make up the majority of errors.</p>
<p>End-to-end Analysis We next analyzed the errors for our full system. There are 43 NYT 2021 puzzles that we did not solve perfectly. We manually separated these puzzles into four categories:</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: A breakdown of our 2021 ACPT performance. The 2021 ACPT consisted of 7 puzzles, for which our combined system achieves a perfect score and surpasses the top human competitor on 5 out of the 7 puzzles. We include the median competitor's performance to illustrate the difficulty of the puzzles.</p>
<ul>
<li>Themes (21 puzzles). Puzzles with unique themes, e.g., placing four characters in one cell.</li>
<li>Local Search Proposals (9 puzzles). Puzzles where we did not propose a puzzle edit in local search that would have improved accuracy.</li>
<li>Local Search Scoring (9 puzzles). Puzzles where the ByT5 scorer either rejected a correct proposal or accepted an incorrect proposal.</li>
<li>Connected Errors (4 puzzles). Puzzles with errors that cannot be fixed by local search, i.e., there are several connected errors.</li>
</ul>
<p>Overall, the largest source of remaining puzzle failures is special themed puzzles, which is unsurprising as the BCS system does not explicitly handle themes. The remaining errors are mostly split between proposal and scoring errors. Finally, connected errors typically arise when BP fills in an answer that is in our bi-encoder's answer set but is incorrect, i.e., the first-pass model was overconfident.</p>
<h2>8 Related Work</h2>
<p>Past Crossword Solvers Prior to our work, the three most successful automated crossword solvers were Proverb, WebCrow (Ernandes et al., 2005), and Dr.Fill. Dr.Fill uses a relatively straightforward TFIDF-like search for question answering, but Proverb and WebCrow combine a number of bespoke modules for QA; WebCrow also relies on a search engine to integrate external knowledge. On the solving side, Proverb and WebCrow both use loopy belief propagation, combined with A* search for inference. Meanwhile, Dr.Fill uses a modified depth-first search known as limited discrepancy search, as well as a post-hoc local search</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: We manually categorize our QA failures using the categories from Table 1. The rate at which each category occurs in random examples is shown in parentheses. A disproportionate fraction of QA errors are due to cross-reference and wordplay clues.</p>
<p>with heuristics to score alternate puzzles.</p>
<p><strong>Standalone QA Models for Crosswords</strong> Past work also evaluated QA techniques using crossword question-answer pairs. These include linear models (Barlacchi et al., 2014), WordNet suggestions (Thomas and S., 2019), and shallow neural networks (Severyn et al., 2015; Hill et al., 2016); we instead use state-of-the-art transformer models.</p>
<p><strong>Ambiguous QA</strong> Solving crossword puzzles requires answering ambiguous and underspecified clues while maintaining accurate estimates of model uncertainty. Other QA tasks share similar challenges (Ferrucci et al., 2010; Rodriguez et al.,</p>
<p>2021; Rajpurkar et al., 2018; Min et al., 2020). Crossword puzzles pose a novel challenge as they contain unique types of reasoning and linguistic phenomena such as wordplay.</p>
<p>Crossword Themes We have largely ignored the presence of themes in crossword puzzles. Themes range from simple topical similarities between answers to puzzles that must be filled in a circular pattern to be correct. While Dr.Fill (Ginsberg, 2011) has a variety of theme handling modules built into it, integrating themes into our probabilistic formulation remains as future work.</p>
<p>Cryptic Crosswords We solve American-style crosswords that differ from British-style "cryptic" crosswords (Efrat et al., 2021; Rozner et al., 2021). Cryptic crosswords involve a different set of conventions and challenges, e.g., more metalinguistic reasoning clues such as anagrams, and likely require different methods from those we propose.</p>
<h2>9 Conclusion</h2>
<p>We have presented new methods for crossword solving based on neural question answering, structured decoding, and local search. Our system outperforms even the best human solvers and can solve puzzles from a wide range of domains with perfect accuracy. Despite this progress, some challenges remain in crossword solving, especially on the QA side, and we hope to spur future research in this direction by releasing a large dataset of question-answer pairs. In future work, we hope to design new ways of evaluating automated crossword solvers, including testing on puzzles that are designed to be difficult for computers and tasking models with puzzle generation.</p>
<h2>Ethical Considerations</h2>
<p>Our data comes primarily from crosswords published in established American newspapers and journals, where a lack of diversity among puzzle constructors and editors may influence the types of clues that appear. For example, only $21 \%$ of crosswords published in The New York Times have at least one woman constructor (Chen, 2021) and a crossword from January 2019 was criticized for including a racial slur as an answer (Graham, 2019). We view the potential for real-world harm as limited since automated crossword solvers are unlikely to be deployed widely in the real world and have limited potential for dual use. However, we note
that these considerations may be important to researchers using our data for question answering research more broadly.</p>
<h2>Acknowledgements</h2>
<p>We thank Sewon Min, Sameer Singh, Shi Feng, Nikhil Kandpal, Michael Littman, and the members of the Berkeley NLP Group for their valuable feedback. We are also grateful to Will Shortz and the organizers of the American Crossword Puzzle Tournament for allowing us to participate in the event. This work was funded in part by the DARPA XAI and LwLL programs. Nicholas Tomlin is supported by the National Science Foundation Graduate Research Fellowship.</p>
<h2>References</h2>
<p>Gianni Barlacchi, Massimo Nicosia, and Alessandro Moschitti. 2014. Learning to rank answer candidates for automatic resolution of crossword puzzles. In CoNLL.</p>
<p>Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. 1994. Signature verification using a "Siamese" time delay neural network. In NeurIPS.</p>
<p>Jeff Chen. 2021. Women constructors in the Shortz era. XWord Info.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL.</p>
<p>Avia Efrat, Uri Shaham, Dan Kilman, and Omer Levy. 2021. Cryptonite: A cryptic crossword benchmark for extreme ambiguity in language. In EMNLP.</p>
<p>Marco Ernandes, Giovanni Angelini, and Marco Gori. 2005. WebCrow: a web-based system for crossword solving. In AAAI.</p>
<p>David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welty. 2010. Building Watson: An overview of the DeepQA project. AI Magazine.</p>
<p>Justin Fu and Sergey Levine. 2021. Offline model-based optimization via normalized maximum likelihood estimation. In ICLR.</p>
<p>Matthew L Ginsberg. 2011. Dr. Fill: Crosswords and an implemented solver for singly weighted CSPs. In JAIR.</p>
<p>Denise Grady. 2010. Across and down, the wizard who is fastest of all. New York Times.</p>
<p>Ruth Graham. 2019. The NYT crossword puzzle's use of an ethnic slur says a lot about the state of crossword puzzling. Slate.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In NAACL.</p>
<p>Felix Hill, Kyunghyun Cho, Anna Korhonen, and Yoshua Bengio. 2016. Learning to understand phrases by embedding the dictionary. In TACL.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs. In IEEE Transactions on Big Data.</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In EMNLP.</p>
<p>Aviral Kumar and Sunita Sarawagi. 2019. Calibration of encoder decoder models for neural machine translation. arXiv preprint arXiv:1903.00802.</p>
<p>Michael L. Littman, Greg A. Keim, and Noam Shazeer. 2002. A probabilistic approach to solving crossword puzzles. In Artificial Intelligence.</p>
<p>Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. In EMNLP.</p>
<p>Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. Compositional questions do not necessitate multi-hop reasoning. In $A C L$.</p>
<p>Judea Pearl. 1988. Probabilistic reasoning in intelligent systems.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In $A C L$.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In EMNLP.</p>
<p>Pedro Rodriguez, Shi Feng, Mohit Iyyer, He He, and Jordan Boyd-Graber. 2021. Quizbowl: The case for incremental question answering. In $J M L R$.</p>
<p>Josh Rozner, Christopher Potts, and Kyle Mahowald. 2021. Decrypting cryptic crosswords: Semantically complex wordplay puzzles as a target for NLP. In NeurIPS.</p>
<p>Aliaksei Severyn, Massimo Nicosia, Gianni Barlacchi, and Alessandro Moschitti. 2015. Distributional neural networks for automatic resolution of crossword puzzles. In $A C L$.</p>
<p>Felix Stahlberg and Bill Byrne. 2019. On NMT search errors and model errors: Cat got your tongue? In EMNLP.</p>
<p>Anu Thomas and Sangeetha S. 2019. Towards a semantic approach for candidate answer generation in solving crossword puzzles. In CoCoNet.</p>
<p>Linting Xue, Aditya Barua, Noah Constant, Rami AlRfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. ByT5: Towards a tokenfree future with pre-trained byte-to-byte models. In TACL.</p>
<h2>A Details of Qualitative Analysis</h2>
<p>In this section, we provide rough definitions for the categories used to construct Table 1 and conduct the manual QA error analysis in Figure 6:</p>
<p>Knowledge Clues that require knowledge of history, scientific terminology, pop culture, or other trivia topics. Answers to knowledge questions are frequently multi-word expressions or proper nouns that may fall outside of our closed-book answer set, and clues often involve additional relational reasoning, e.g., Book after Song of Solomon (ISAIAH).</p>
<p>Definition Clues that are either rough definitions or synonyms of the answer.</p>
<p>Commonsense Clues that rely on relational reasoning about well-known entities. These clues often involve subset-superset, part-whole, or causeeffect relations, e.g., Cause of a smudge (WETINK).</p>
<p>Wordplay Clues that involve reasoning about heteronyms, puns, anagrams, or other metalinguistic patterns. Such clues are usually (but not always) indicated by a question mark.</p>
<p>Phrase Clues or answers that involve common phrases or multi-word expressions. These clues are often written with quotation marks or blanks and their answers are frequently synonymous expressions, e.g., Hey man! (YODUDE).</p>
<p>Cross-Reference Clues that require knowledge of other elements in the puzzle, either through explicit reference (e.g., See 53-Down) or due to their usage of crossword themes.</p>
<h2>B Additional Dataset Statistics</h2>
<p>Figures 7-9 present a breakdown of the publishers, years, and answer lengths that are present in our crossword dataset.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: We build our dataset by collecting data from 26 publishers. Using a diverse set of publishers is beneficial as each publisher has different question types, answer distributions, and puzzle idiosyncrasies.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Our dataset spans over 70 years of crossword puzzles. The dip in puzzles in 1993-1996 is due to an unavailability of NYT puzzles from those years.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: The answers in our dataset span many different lengths; longer answers are typically more difficult multi-word expressions or theme answers.</p>
<h2>C American Crossword Puzzle Tournament Details</h2>
<table>
<thead>
<tr>
<th>System</th>
<th>Year</th>
<th>Score</th>
<th>Rank</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proverb</td>
<td>1998</td>
<td>6,215</td>
<td>213th</td>
</tr>
<tr>
<td>Dr.Fill</td>
<td>2012</td>
<td>10,060</td>
<td>141st</td>
</tr>
<tr>
<td>Dr.Fill</td>
<td>2013</td>
<td>10,550</td>
<td>92nd</td>
</tr>
<tr>
<td>Dr.Fill</td>
<td>2014</td>
<td>10,790</td>
<td>67th</td>
</tr>
<tr>
<td>Dr.Fill</td>
<td>2015</td>
<td>10,920</td>
<td>55th</td>
</tr>
<tr>
<td>Dr.Fill</td>
<td>2016</td>
<td>11,205</td>
<td>41st</td>
</tr>
<tr>
<td>Dr.Fill</td>
<td>2017</td>
<td>11,795</td>
<td>11th</td>
</tr>
<tr>
<td>Dr.Fill</td>
<td>2018</td>
<td>10,740</td>
<td>78th</td>
</tr>
<tr>
<td>Dr.Fill</td>
<td>2019</td>
<td>11,795</td>
<td>14th</td>
</tr>
<tr>
<td>BCS QA + Dr.Fill</td>
<td>2021</td>
<td>12,825</td>
<td>1st</td>
</tr>
<tr>
<td>BCS QA + BP + LS</td>
<td>2021</td>
<td>13,065</td>
<td>1st</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance over the years in the American Crossword Puzzle Tournament. Dr.Fill has steadily improved due to system changes and increased training data. We also provide a retrospective evaluation of our final system (bottom row). Note that the 2020 ACPT was cancelled due to COVID-19.</p>
<h3>Scoring System</h3>
<p>The main portion of the American Crossword Puzzle Tournament consists of seven crossword puzzles. Competitors are scored based on their accuracy and speed. For each puzzle, the judges award:</p>
<ul>
<li>10 points for each correct word in the grid,</li>
<li>150 bonus points if the puzzle is solved perfectly,</li>
<li>25 bonus points for each full minute of time remaining when the puzzle is completed. This bonus is reduced by 25 points for each incorrect letter but can never be negative.</li>
</ul>
<p>The total score for the seven puzzles determines the final results, aside from a special playoff for the top three human competitors. Table 5 shows scores over the years for the American Crossword Puzzle Tournament, including our 2021 submission.</p>
<h2>D Additional Analysis Results</h2>
<p>Figure 10 shows our accuracy broken down by day of the week. Monday and Tuesday NYT puzzles—ones designed to be easier for humans—are also easy for computer systems. On the other hand, Thursday NYT puzzles, which often contain unusual theme entries such as placing multiple letters into a single grid, are the most difficult. Our system is unaware of these special themes, but the</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: We compare our system's accuracy on NYT puzzles to the previous state-of-the-art Dr.Fill system and break down the results by day of the week. Both systems succeed on early week puzzles but struggle on Thursday puzzles that often contain unusual themes.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: The chance that the BCS correctly fills in an answer as a function of the rank of the answer under its QA model. If the QA model predicts the answer in its top 1,000 candidates, it is usually filled in correctly.</p>
<p>Dr.Fill system includes various methods to detect and resolve them and is thus more competitive on Thursday NYT puzzles. Finally, our system provides the largest gains on Saturday NYT puzzles which contain many of the hardest clues from a QA perspective.</p>
<p>We also compute results on the meless NYT puzzles. Themed puzzles range from topical similarity between answers in a puzzle, to multiple words ending with the same suffix, to multiple letters fitting inside a single square (i.e., rebus puzzles). For evaluation purposes, we consider them puzzles to be any puzzle that contains a rebus or a circled letter according to XWord Info, but this does not capture all possible themes.</p>
<p>https://www.xwordinfo.com/rebus</p>
<p>https://www.xwordinfo.com/circles</p>
<table>
<thead>
<tr>
<th style="text-align: center;">H</th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">H</th>
<th style="text-align: center;">I</th>
<th style="text-align: center;">D</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">S</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">E</th>
<th style="text-align: center;">D</th>
<th style="text-align: center;">S</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">V</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
</tr>
<tr>
<td style="text-align: center;">M</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;">M</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">D</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">A</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
</tr>
<tr>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">L</td>
</tr>
<tr>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">I</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">A</td>
</tr>
</tbody>
</table>
<p>(a) Before Local Search</p>
<table>
<thead>
<tr>
<th style="text-align: center;">H</th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;">I</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">S</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">E</th>
<th style="text-align: center;">D</th>
<th style="text-align: center;">S</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">V</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
</tr>
<tr>
<td style="text-align: center;">M</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;">M</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">D</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">A</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
</tr>
<tr>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">L</td>
</tr>
<tr>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">I</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">A</td>
</tr>
</tbody>
</table>
<p>(c) Step #2</p>
<table>
<thead>
<tr>
<th style="text-align: center;">H</th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">H</th>
<th style="text-align: center;">I</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">S</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">E</th>
<th style="text-align: center;">D</th>
<th style="text-align: center;">S</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">V</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
</tr>
<tr>
<td style="text-align: center;">M</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;">M</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">D</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">A</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
</tr>
<tr>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">L</td>
</tr>
<tr>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">I</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">A</td>
</tr>
</tbody>
</table>
<p>(b) Step #1</p>
<table>
<thead>
<tr>
<th style="text-align: center;">H</th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;">I</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">S</th>
<th style="text-align: center;">N</th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">E</th>
<th style="text-align: center;">D</th>
<th style="text-align: center;">S</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">V</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
</tr>
<tr>
<td style="text-align: center;">M</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;">M</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">D</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">A</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
</tr>
<tr>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">D</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">L</td>
</tr>
<tr>
<td style="text-align: center;">S</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">I</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">A</td>
</tr>
</tbody>
</table>
<p>(d) Step #3</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Clue</th>
<th style="text-align: center;">Gold</th>
<th style="text-align: center;">Before</th>
<th style="text-align: center;">Step 1</th>
<th style="text-align: center;">Step 2</th>
<th style="text-align: center;">Step 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Beloved, in Arabic</td>
<td style="text-align: center;">HABIB</td>
<td style="text-align: center;">HADID</td>
<td style="text-align: center;">HAHIB</td>
<td style="text-align: center;">HABIB</td>
<td style="text-align: center;">HABIB</td>
</tr>
<tr>
<td style="text-align: center;">Ill-advised opinions</td>
<td style="text-align: center;">BADTAKES</td>
<td style="text-align: center;">HOTTAKES</td>
<td style="text-align: center;">HODTAKES</td>
<td style="text-align: center;">BADTAKES</td>
<td style="text-align: center;">BADTAKES</td>
</tr>
<tr>
<td style="text-align: center;">Feeling on a lo-o-ong car trip</td>
<td style="text-align: center;">BORATE</td>
<td style="text-align: center;">DERATE</td>
<td style="text-align: center;">BERATE</td>
<td style="text-align: center;">BERATE</td>
<td style="text-align: center;">BORATE</td>
</tr>
<tr>
<td style="text-align: center;">Italian herbal liqueur</td>
<td style="text-align: center;">AMARO</td>
<td style="text-align: center;">AMORE</td>
<td style="text-align: center;">AMORE</td>
<td style="text-align: center;">AMARE</td>
<td style="text-align: center;">AMARO</td>
</tr>
<tr>
<td style="text-align: center;">Not radical</td>
<td style="text-align: center;">MODERNISTS</td>
<td style="text-align: center;">MOTERNISTS</td>
<td style="text-align: center;">MODERNISTS</td>
<td style="text-align: center;">MODERNISTS</td>
<td style="text-align: center;">MODERNISTS</td>
</tr>
<tr>
<td style="text-align: center;">Long fur scarfs</td>
<td style="text-align: center;">STOLI</td>
<td style="text-align: center;">STOGA</td>
<td style="text-align: center;">STOLA</td>
<td style="text-align: center;">STOLI</td>
<td style="text-align: center;">STOLI</td>
</tr>
<tr>
<td style="text-align: center;">Outcome of a coin flip, e.g.,</td>
<td style="text-align: center;">PURECHANCE</td>
<td style="text-align: center;">PURAAHANCE</td>
<td style="text-align: center;">PURAAHANCE</td>
<td style="text-align: center;">PURECHANCE</td>
<td style="text-align: center;">PURECHANCE</td>
</tr>
<tr>
<td style="text-align: center;">Choose randomly, in a way</td>
<td style="text-align: center;">CASTLES</td>
<td style="text-align: center;">TASTGAS</td>
<td style="text-align: center;">CASTLAS</td>
<td style="text-align: center;">CASTLES</td>
<td style="text-align: center;">CASTLES</td>
</tr>
<tr>
<td style="text-align: center;">Like toreadors, again and again</td>
<td style="text-align: center;">CHARADE</td>
<td style="text-align: center;">CHARADS</td>
<td style="text-align: center;">CHARADS</td>
<td style="text-align: center;">CHARADE</td>
<td style="text-align: center;">CHARADE</td>
</tr>
<tr>
<td style="text-align: center;">"Get 'em!"</td>
<td style="text-align: center;">SIC</td>
<td style="text-align: center;">SAA</td>
<td style="text-align: center;">SAA</td>
<td style="text-align: center;">SAC</td>
<td style="text-align: center;">SIC</td>
</tr>
<tr>
<td style="text-align: center;">Worrisome uncertainties</td>
<td style="text-align: center;">BIGIFS</td>
<td style="text-align: center;">BOGIES</td>
<td style="text-align: center;">BIGIES</td>
<td style="text-align: center;">BIGINS</td>
<td style="text-align: center;">BIGINS</td>
</tr>
<tr>
<td style="text-align: center;">Like taxis and Julius Caesar, once</td>
<td style="text-align: center;">HAILED</td>
<td style="text-align: center;">GAOLED</td>
<td style="text-align: center;">HAILED</td>
<td style="text-align: center;">HAILED</td>
<td style="text-align: center;">HAILED</td>
</tr>
<tr>
<td style="text-align: center;">Immunologist Anthony</td>
<td style="text-align: center;">FAUCI</td>
<td style="text-align: center;">EUACI</td>
<td style="text-align: center;">ELUCI</td>
<td style="text-align: center;">NAUCI</td>
<td style="text-align: center;">NAUCI</td>
</tr>
<tr>
<td style="text-align: center;">Suffix with coward</td>
<td style="text-align: center;">ICE</td>
<td style="text-align: center;">ICS</td>
<td style="text-align: center;">ICS</td>
<td style="text-align: center;">ICE</td>
<td style="text-align: center;">ICE</td>
</tr>
</tbody>
</table>
<p>Figure 12: Top: We show a larger version of Figure 4. Bottom: The clues and associated answers after each step.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Our bi-encoder model is a "closed-book" QA model because it does not have "open-book" access to external knowledge sources such as Wikipedia (Roberts et al., 2020). We found in preliminary experiments that open-book models struggle as most crossword answers are not present or are difficult to retrieve from knowledge sources such as Wikipedia.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8027 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8027</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8027</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-10d2842131634263b5a6875319ff53c0da6a7398</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/10d2842131634263b5a6875319ff53c0da6a7398" target="_blank">Agent-as-a-Judge: Evaluate Agents with Agents</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems, is introduced, an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process.</p>
                <p><strong>Paper Abstract:</strong> Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8027.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8027.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent-as-a-Judge framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework introduced in this paper that uses agentic systems to evaluate other agentic systems, providing intermediate, trajectory-level feedback through modular agent components (graph, locate, read, search, retrieve, ask, memory, planning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-2024-05-13 (used as backend in developer agents/evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4o-2024-05-13</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (code generation / agent evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Agent-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>An agentic evaluator that inspects generated workspaces and execution trajectories using modular capabilities (graph, locate, read, search/retrieve, ask, memory, planning) to judge whether hierarchical requirements are satisfied and to provide intermediate feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Alignment Rate; Judge Shift; Requirements Met (I); Requirements Met (D); Task Solve Rate; PR Curves; cost (USD) and time (minutes)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Alignment Rate: % of requirement evaluations matching Human-as-a-Judge consensus. Judge Shift: absolute percentage-point deviation from Human-as-a-Judge consensus. Requirements Met (I): % requirements satisfied ignoring dependencies. Requirements Met (D): % requirements satisfied considering DAG dependencies. Task Solve Rate: % tasks with all requirements met. Cost in USD and time in minutes.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DevAI (the AI Developer Dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Compared against three expert human evaluators (231a, 38bb, cn90) and their consensus; human evaluation entailed two rounds (initial independent labeling and a consensus debate); total human time reported = 86.5 hours; consensus used as reference.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Agent-as-a-Judge alignment rates: e.g., 88.52% / 83.88% / 90.44% (various settings) and up to 95.08% alignment with human majority vote; Judge Shift as low as 0.27% in some cases; cost reported $30.58 and time 118.43 minutes for full DevAI evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Agent-as-a-Judge aligns more closely with human consensus than LLM-as-a-Judge (alignment ~90% vs ~70%); in some cases it outperforms individual human evaluators and approaches the majority-vote consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Sensitive to noisy historical judgments (memory module can propagate errors), requires high-quality factual information, some modules (retrieve) show mixed benefit depending on the judged agent's trajectory; class-imbalance in requirements (few positives) can skew simple accuracy metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8027.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8027.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation approach that uses large language models to judge outputs of other LLMs or agents, typically operating in a black-box style without agentic intermediate-feedback modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (model evaluation / NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Uses an LLM to read outputs/artifacts and produce judgments (pass/fail or graded) for tasks; typically does not leverage agentic modules for trajectory-level inspection and intermediate feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Alignment Rate; Judge Shift; Requirements Met (I/D); Task Solve Rate; PR Curves</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same definitions as Agent-as-a-Judge: Alignment Rate is fraction of labels matching human consensus; Judge Shift is absolute deviation from consensus; Requirements Met metrics measured as percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DevAI (used as testbed); referenced other benchmarks in discussion</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Compared to three human evaluators' consensus; used both black-box and gray-box settings (gray-box includes access to trajectory data).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM-as-a-Judge achieved lower alignment rates (examples: ~68.86% / 71.85% / 70.76% in some settings) and larger Judge Shifts (up to ~31% in particular cases) compared to Agent-as-a-Judge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM-as-a-Judge aligns less well with human consensus than Agent-as-a-Judge (alignment ~60–72% reported vs ~90% for Agent-as-a-Judge in some settings); performs better in gray-box when given trajectory data but still underperforms agentic judge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Lacks intermediate trajectory-level modules, can be misled in class-imbalanced settings (e.g., by always predicting negative), sensitive to prompt/template choices, and can perform worse without contextual selection mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8027.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8027.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-as-a-Judge evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual expert evaluation protocol in which human experts inspect generated artifacts and trajectories to determine whether hierarchical requirements are satisfied; used as the consensus reference in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (software development evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human-as-a-Judge (expert consensus)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Three expert evaluators independently label whether each of 365 hierarchical requirements across 55 tasks are satisfied; after initial independent labeling, evaluators debate to reach a consensus used as the ground reference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Requirements Met (I); Requirements Met (D); Task Solve Rate; Inter-evaluator disagreement rate; majority-vote error rate</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Requirements Met (I/D) as % requirements satisfied (independent vs considering dependencies); disagreement rate = % labels differing between pairs; majority-vote error measured vs consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DevAI (ground truth consensus derived here)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three named expert evaluators (231a, 38bb, cn90); initial independent reviews took ~58 human hours, consensus rounds added ~28.5 hours (total 86.5 hours); disagreement rates between pairs ~10–30%; majority vote reduced error to ~6.01% relative to consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Human-as-a-Judge results: GPT-Pilot and OpenHands satisfied ~29% of requirements (ignoring dependencies ~44%); Task Solve Rate low (only one full task met all requirements across methods).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used as the reference baseline (consensus) for alignment measurements of automated judges.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Expensive and time-consuming; inter-annotator disagreement notable; consensus not perfect ground truth and remains costly to obtain at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8027.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8027.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DevAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DevAI: the AI Developer Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new benchmark introduced in this paper consisting of 55 realistic AI development tasks with 365 hierarchical requirements and 125 preferences, organized as directed acyclic graphs to provide intermediate evaluation signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (code generation / software engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset / benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>DevAI hierarchical-requirements evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each task includes a natural-language query, DAG-structured requirements (milestones) and preferences; agent outputs and trajectories are judged against these requirements to provide non-sparse, intermediate feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Requirements Met (I); Requirements Met (D); Task Solve Rate; alignment metrics when compared to human consensus</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Requirements counts out of 365; rates are percentages of requirements satisfied per task or overall; DAG dependencies alter the effective success when considering prerequisites.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DevAI (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three expert evaluators judged outputs against DevAI requirements and reached consensus; DevAI also used for automated judges in black-box and gray-box modes.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>DevAI size: 55 tasks, 365 requirements, 125 preferences; baseline developer agent performance: requirements met typically ~6.55%–53% depending on agent and setting, task solve rates near 0–5.45%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Serves as testbed for comparing human, LLM, and agentic judges.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Tasks are relatively small-scale by design for cost reasons; possible Goodhart concerns when optimizing to dataset specifics; still a proof-of-concept testbed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8027.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8027.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PR Curves</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Precision-Recall Curves</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class-imbalance-aware evaluation metric used to assess judge performance by balancing precision and recall rather than overall accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistics / machine learning evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Precision-Recall Curve (PR Curve)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Plot precision vs recall across thresholds to evaluate classifier/judge performance under class imbalance; area under PR curve (AUPRC) summarizes performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision and Recall across thresholds; AUPRC (area under PR curve)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Precision = TP/(TP+FP); Recall = TP/(TP+FN); AUPRC is the integral under precision-recall curve; values range 0–1.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied on DevAI evaluation outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used to compare automated judges to human labels where positives (requirements met) are rare.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>PR curves indicate Agent-as-a-Judge outperforms LLM-as-a-Judge and, in some cases, single human evaluators in precision/recall tradeoffs; specific AUPRC numbers not tabulated in main text but curves plotted (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>PR analysis shows Agent-as-a-Judge aligns more with human consensus especially on sparse positive labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>PR curves are necessary because simple accuracy/alignment can be misleading under class imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8027.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8027.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alignment Rate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alignment Rate (to Human Consensus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metric defined in the paper as the percentage of individual requirement judgments that match the Human-as-a-Judge consensus across the DevAI requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation methodology / computer science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Alignment Rate</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Measures how often an automated judge's per-requirement labels agree with the consensus human label; reported as a percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percentage of matching labels</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Alignment Rate = (number of requirement labels equal to human consensus) / (total number of requirements) * 100%.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DevAI</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Consensus used as ground truth; per-requirement labels compared across 365 requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Agent-as-a-Judge reported alignment rates e.g., 92.07%, 90.44%, 88.52% in various comparisons; LLM-as-a-Judge reported lower rates (~60–72%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used to quantify closeness to human consensus; Agent-as-a-Judge shows substantially higher alignment than LLM-as-a-Judge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>High alignment can be trivially achieved by always predicting the majority class in imbalanced settings; hence PR curves and other metrics are also used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8027.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8027.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Judge Shift</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judge Shift</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric introduced in this paper measuring deviation (absolute difference) of an automated judge's results from the Human-as-a-Judge consensus; lower is better.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation methodology / computer science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Judge Shift</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes the absolute percentage-point difference between automated judge metrics (e.g., Requirements Met %) and the Human-as-a-Judge consensus results.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Absolute percentage-point deviation</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Judge Shift = |automated_judge_value - human_consensus_value| (expressed in percentage points)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DevAI</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human consensus used as baseline; Judge Shift reported per agent and per metric.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Examples: LLM-as-a-Judge had Judge Shift up to ~31.42% for OpenHands on some metrics; Agent-as-a-Judge had much lower shifts (e.g., as low as 0.27%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Shows Agent-as-a-Judge is closer to human consensus (smaller Judge Shift) than LLM-as-a-Judge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Single-number deviation may hide asymmetric error types and is sensitive to class imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8027.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8027.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Requirements (I/D)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Requirements Met (Independent / Dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two related metrics that measure the fraction of task requirements satisfied either treating requirements independently (I) or considering their DAG-based dependencies (D).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (benchmark evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>metric / criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Requirements Met (I) and Requirements Met (D)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Requirements Met (I): percent of atomic requirements marked satisfied ignoring prerequisites. Requirements Met (D): percent of requirements satisfied when prerequisites/dependencies (in the DAG) are taken into account.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Percent of requirements satisfied (I and D)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Requirements Met (I) = (# requirements labeled satisfied ignoring dependencies) / (total requirements) * 100%; Requirements Met (D) accounts for dependency DAG—downstream requirements may be considered unmet if prerequisites fail.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DevAI</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used in both human and automated judgments; reported per-developer-agent and per-judge.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Examples from Table 2: Human-as-a-Judge Requirements Met (I) for MetaGPT 22.13%, GPT-Pilot 44.80%, OpenHands 42.89%; Requirements Met (D) lower due to dependencies (e.g., MetaGPT 6.55%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used to compare developer agents and judge types; Agent-as-a-Judge reported Requirements Met values closer to human consensus than LLM-as-a-Judge in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dependencies significantly reduce effective success rates; correctness of dependency modeling is crucial and may affect metric interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8027.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8027.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority Vote / Consensus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority Vote and Consensus Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregation strategies for human labels: initial independent labels by multiple humans, followed by majority vote and an additional debate to reach consensus used as reference ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>human evaluation methodology / statistics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>criterion / aggregation method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Majority Vote and Consensus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Independent human labels combined via majority vote; evaluators later debate to reach a consensus which is used as the final reference; majority vote shown to reduce error relative to individual annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Inter-annotator disagreement rate; majority-vote error rate vs consensus</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Disagreement rate = % pairwise label disagreements; majority-vote error = % mismatch between majority-vote and consensus labels.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>DevAI</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three expert annotators; majority vote reduced error to ~6.01% relative to consensus; consensus build required additional 28.5 hours.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Majority vote had smaller deviation from consensus than individual annotators; consensus used as final human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Ensemble/majority voting improves agreement with consensus and reduces single-annotator noise.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires multiple skilled annotators and extra time; majority vote can still be wrong if annotator accuracy <50% or biases exist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8027.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8027.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Basic Statistics Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Basic developer-agent statistics (cost, time, tokens, files, lines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational metrics recorded for developer agents including average API cost (USD), average time (s), input/output tokens, saved files and saved code lines, used as practical utility measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-2024-05-13 (backend used by developer agents)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-4o-2024-05-13</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (systems evaluation / cost analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>metric collection / operational metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Basic statistics (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collect per-run averages over developer agents for cost (USD), runtime (seconds), token counts, number of saved files, and saved code lines to profile practical resource usage and output verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average Cost (USD); Average Time (s); Average Input/Output Tokens; Average Saved Code Files; Average Saved Code Lines; Average Saved Files</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported as arithmetic means across runs (units: USD, seconds, token counts, file counts, line counts).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Collected from runs on DevAI tasks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>These are automated logs; used as contextual stats for evaluation cost/time comparisons with human judges.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Examples from Table 1: MetaGPT average cost $1.19, avg time 775.29 s; GPT-Pilot avg cost $3.92, avg time 1622.38 s; OpenHands avg cost $6.38, avg time 362.41 s.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used to highlight cost/time advantages of automated judges vs human evaluation (Agent-as-a-Judge cost ~$30.58 vs human ~$1297.50 for full evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Operational metrics do not directly reflect task correctness or quality; differences in internal communications and saving behaviors affect file/line counts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8027.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8027.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICE-Score / CodeBERTScore / G-EVAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ICE-Score, CodeBERTScore, and G-EVAL (LLM-based evaluation metrics referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Existing automated scoring approaches referenced in related work: ICE-Score instructs LLMs to evaluate code, CodeBERTScore uses pretrained code models to score code similarity/quality, and G-EVAL uses GPT-4 for NLG evaluation with improved human alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP/code-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>metrics / evaluation methods (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ICE-Score; CodeBERTScore; G-EVAL</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>ICE-Score: instruct LLMs to score code based on instructions; CodeBERTScore: embedding-based code similarity metric; G-EVAL: uses GPT-4 to evaluate NLG outputs and improves alignment with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Plausibility/human-alignment scores, embedding-based similarity, automated functional correctness proxies</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Varies by method: ICE-Score returns instruction-following scores (scale depends on prompt); CodeBERTScore uses cosine similarity of embeddings; G-EVAL reports human-aligned ratings (scales vary).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Referenced as prior approaches to automated judging and code evaluation; not directly used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Mentioned to motivate need for richer agentic evaluation; LLM-only judges can lack intermediate process understanding and can be brittle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8027.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8027.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Process-Supervised Reward Model (PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Process-Supervised Reward Model (PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed application/outlook: learning reward models supervised by process/trajectory information (intermediate feedback) to improve agentic systems, mentioned as a possible future direction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>reinforcement learning / agent optimization</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>proposed evaluation/optimization approach</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Process-Supervised Reward Model (PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Learn reward functions that incorporate intermediate process signals (from Agent-as-a-Judge) to provide denser feedback for agentic self-improvement and address sparse reward problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not implemented here; conceptually would use reward learning metrics (e.g., improvement in downstream task performance, cumulative reward)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Not specified in paper; would likely be task-dependent and measured in standard RL units (cumulative reward, success rate).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Proposed to be used with agentic evaluation pipelines like Agent-as-a-Judge and DevAI</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Described as future work; potential for compounding errors if reward model relies on imperfect judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>MLLM-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark <em>(Rating: 2)</em></li>
                <li>Swe-bench: Can language models resolve real-world github issues? <em>(Rating: 2)</em></li>
                <li>G-eval: Nlg evaluation using gpt-4 with better human alignment <em>(Rating: 1)</em></li>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 1)</em></li>
                <li>ICE-Score: Instructing large language models to evaluate code <em>(Rating: 1)</em></li>
                <li>CodeBERTScore: Evaluating code generation with pretrained models of code <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8027",
    "paper_id": "paper-10d2842131634263b5a6875319ff53c0da6a7398",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Agent-as-a-Judge",
            "name_full": "Agent-as-a-Judge framework",
            "brief_description": "A framework introduced in this paper that uses agentic systems to evaluate other agentic systems, providing intermediate, trajectory-level feedback through modular agent components (graph, locate, read, search, retrieve, ask, memory, planning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4o-2024-05-13 (used as backend in developer agents/evaluations)",
            "model_size": "gpt-4o-2024-05-13",
            "scientific_domain": "computer science (code generation / agent evaluation)",
            "theory_type": "evaluation framework",
            "evaluation_method_name": "Agent-as-a-Judge",
            "evaluation_method_description": "An agentic evaluator that inspects generated workspaces and execution trajectories using modular capabilities (graph, locate, read, search/retrieve, ask, memory, planning) to judge whether hierarchical requirements are satisfied and to provide intermediate feedback.",
            "evaluation_metric": "Alignment Rate; Judge Shift; Requirements Met (I); Requirements Met (D); Task Solve Rate; PR Curves; cost (USD) and time (minutes)",
            "metric_definition": "Alignment Rate: % of requirement evaluations matching Human-as-a-Judge consensus. Judge Shift: absolute percentage-point deviation from Human-as-a-Judge consensus. Requirements Met (I): % requirements satisfied ignoring dependencies. Requirements Met (D): % requirements satisfied considering DAG dependencies. Task Solve Rate: % tasks with all requirements met. Cost in USD and time in minutes.",
            "dataset_or_benchmark": "DevAI (the AI Developer Dataset)",
            "human_evaluation_details": "Compared against three expert human evaluators (231a, 38bb, cn90) and their consensus; human evaluation entailed two rounds (initial independent labeling and a consensus debate); total human time reported = 86.5 hours; consensus used as reference.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Agent-as-a-Judge alignment rates: e.g., 88.52% / 83.88% / 90.44% (various settings) and up to 95.08% alignment with human majority vote; Judge Shift as low as 0.27% in some cases; cost reported $30.58 and time 118.43 minutes for full DevAI evaluation.",
            "comparison_to_human_generated": true,
            "comparison_results": "Agent-as-a-Judge aligns more closely with human consensus than LLM-as-a-Judge (alignment ~90% vs ~70%); in some cases it outperforms individual human evaluators and approaches the majority-vote consensus.",
            "limitations_noted": "Sensitive to noisy historical judgments (memory module can propagate errors), requires high-quality factual information, some modules (retrieve) show mixed benefit depending on the judged agent's trajectory; class-imbalance in requirements (few positives) can skew simple accuracy metrics.",
            "uuid": "e8027.0",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "LLM-as-a-Judge framework",
            "brief_description": "An evaluation approach that uses large language models to judge outputs of other LLMs or agents, typically operating in a black-box style without agentic intermediate-feedback modules.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (model evaluation / NLP)",
            "theory_type": "evaluation framework",
            "evaluation_method_name": "LLM-as-a-Judge",
            "evaluation_method_description": "Uses an LLM to read outputs/artifacts and produce judgments (pass/fail or graded) for tasks; typically does not leverage agentic modules for trajectory-level inspection and intermediate feedback.",
            "evaluation_metric": "Alignment Rate; Judge Shift; Requirements Met (I/D); Task Solve Rate; PR Curves",
            "metric_definition": "Same definitions as Agent-as-a-Judge: Alignment Rate is fraction of labels matching human consensus; Judge Shift is absolute deviation from consensus; Requirements Met metrics measured as percentages.",
            "dataset_or_benchmark": "DevAI (used as testbed); referenced other benchmarks in discussion",
            "human_evaluation_details": "Compared to three human evaluators' consensus; used both black-box and gray-box settings (gray-box includes access to trajectory data).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "LLM-as-a-Judge achieved lower alignment rates (examples: ~68.86% / 71.85% / 70.76% in some settings) and larger Judge Shifts (up to ~31% in particular cases) compared to Agent-as-a-Judge.",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM-as-a-Judge aligns less well with human consensus than Agent-as-a-Judge (alignment ~60–72% reported vs ~90% for Agent-as-a-Judge in some settings); performs better in gray-box when given trajectory data but still underperforms agentic judge.",
            "limitations_noted": "Lacks intermediate trajectory-level modules, can be misled in class-imbalanced settings (e.g., by always predicting negative), sensitive to prompt/template choices, and can perform worse without contextual selection mechanisms.",
            "uuid": "e8027.1",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Human-as-a-Judge",
            "name_full": "Human-as-a-Judge evaluation",
            "brief_description": "Manual expert evaluation protocol in which human experts inspect generated artifacts and trajectories to determine whether hierarchical requirements are satisfied; used as the consensus reference in this work.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (software development evaluation)",
            "theory_type": "evaluation protocol",
            "evaluation_method_name": "Human-as-a-Judge (expert consensus)",
            "evaluation_method_description": "Three expert evaluators independently label whether each of 365 hierarchical requirements across 55 tasks are satisfied; after initial independent labeling, evaluators debate to reach a consensus used as the ground reference.",
            "evaluation_metric": "Requirements Met (I); Requirements Met (D); Task Solve Rate; Inter-evaluator disagreement rate; majority-vote error rate",
            "metric_definition": "Requirements Met (I/D) as % requirements satisfied (independent vs considering dependencies); disagreement rate = % labels differing between pairs; majority-vote error measured vs consensus.",
            "dataset_or_benchmark": "DevAI (ground truth consensus derived here)",
            "human_evaluation_details": "Three named expert evaluators (231a, 38bb, cn90); initial independent reviews took ~58 human hours, consensus rounds added ~28.5 hours (total 86.5 hours); disagreement rates between pairs ~10–30%; majority vote reduced error to ~6.01% relative to consensus.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Human-as-a-Judge results: GPT-Pilot and OpenHands satisfied ~29% of requirements (ignoring dependencies ~44%); Task Solve Rate low (only one full task met all requirements across methods).",
            "comparison_to_human_generated": null,
            "comparison_results": "Used as the reference baseline (consensus) for alignment measurements of automated judges.",
            "limitations_noted": "Expensive and time-consuming; inter-annotator disagreement notable; consensus not perfect ground truth and remains costly to obtain at scale.",
            "uuid": "e8027.2",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DevAI",
            "name_full": "DevAI: the AI Developer Dataset",
            "brief_description": "A new benchmark introduced in this paper consisting of 55 realistic AI development tasks with 365 hierarchical requirements and 125 preferences, organized as directed acyclic graphs to provide intermediate evaluation signals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (code generation / software engineering)",
            "theory_type": "dataset / benchmark",
            "evaluation_method_name": "DevAI hierarchical-requirements evaluation",
            "evaluation_method_description": "Each task includes a natural-language query, DAG-structured requirements (milestones) and preferences; agent outputs and trajectories are judged against these requirements to provide non-sparse, intermediate feedback.",
            "evaluation_metric": "Requirements Met (I); Requirements Met (D); Task Solve Rate; alignment metrics when compared to human consensus",
            "metric_definition": "Requirements counts out of 365; rates are percentages of requirements satisfied per task or overall; DAG dependencies alter the effective success when considering prerequisites.",
            "dataset_or_benchmark": "DevAI (this paper)",
            "human_evaluation_details": "Three expert evaluators judged outputs against DevAI requirements and reached consensus; DevAI also used for automated judges in black-box and gray-box modes.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "DevAI size: 55 tasks, 365 requirements, 125 preferences; baseline developer agent performance: requirements met typically ~6.55%–53% depending on agent and setting, task solve rates near 0–5.45%.",
            "comparison_to_human_generated": null,
            "comparison_results": "Serves as testbed for comparing human, LLM, and agentic judges.",
            "limitations_noted": "Tasks are relatively small-scale by design for cost reasons; possible Goodhart concerns when optimizing to dataset specifics; still a proof-of-concept testbed.",
            "uuid": "e8027.3",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "PR Curves",
            "name_full": "Precision-Recall Curves",
            "brief_description": "A class-imbalance-aware evaluation metric used to assess judge performance by balancing precision and recall rather than overall accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "statistics / machine learning evaluation",
            "theory_type": "metric",
            "evaluation_method_name": "Precision-Recall Curve (PR Curve)",
            "evaluation_method_description": "Plot precision vs recall across thresholds to evaluate classifier/judge performance under class imbalance; area under PR curve (AUPRC) summarizes performance.",
            "evaluation_metric": "Precision and Recall across thresholds; AUPRC (area under PR curve)",
            "metric_definition": "Precision = TP/(TP+FP); Recall = TP/(TP+FN); AUPRC is the integral under precision-recall curve; values range 0–1.",
            "dataset_or_benchmark": "Applied on DevAI evaluation outputs",
            "human_evaluation_details": "Used to compare automated judges to human labels where positives (requirements met) are rare.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "PR curves indicate Agent-as-a-Judge outperforms LLM-as-a-Judge and, in some cases, single human evaluators in precision/recall tradeoffs; specific AUPRC numbers not tabulated in main text but curves plotted (Figure 7).",
            "comparison_to_human_generated": true,
            "comparison_results": "PR analysis shows Agent-as-a-Judge aligns more with human consensus especially on sparse positive labels.",
            "limitations_noted": "PR curves are necessary because simple accuracy/alignment can be misleading under class imbalance.",
            "uuid": "e8027.4",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Alignment Rate",
            "name_full": "Alignment Rate (to Human Consensus)",
            "brief_description": "Metric defined in the paper as the percentage of individual requirement judgments that match the Human-as-a-Judge consensus across the DevAI requirements.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "evaluation methodology / computer science",
            "theory_type": "metric",
            "evaluation_method_name": "Alignment Rate",
            "evaluation_method_description": "Measures how often an automated judge's per-requirement labels agree with the consensus human label; reported as a percentage.",
            "evaluation_metric": "Percentage of matching labels",
            "metric_definition": "Alignment Rate = (number of requirement labels equal to human consensus) / (total number of requirements) * 100%.",
            "dataset_or_benchmark": "DevAI",
            "human_evaluation_details": "Consensus used as ground truth; per-requirement labels compared across 365 requirements.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Agent-as-a-Judge reported alignment rates e.g., 92.07%, 90.44%, 88.52% in various comparisons; LLM-as-a-Judge reported lower rates (~60–72%).",
            "comparison_to_human_generated": true,
            "comparison_results": "Used to quantify closeness to human consensus; Agent-as-a-Judge shows substantially higher alignment than LLM-as-a-Judge.",
            "limitations_noted": "High alignment can be trivially achieved by always predicting the majority class in imbalanced settings; hence PR curves and other metrics are also used.",
            "uuid": "e8027.5",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Judge Shift",
            "name_full": "Judge Shift",
            "brief_description": "A metric introduced in this paper measuring deviation (absolute difference) of an automated judge's results from the Human-as-a-Judge consensus; lower is better.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "evaluation methodology / computer science",
            "theory_type": "metric",
            "evaluation_method_name": "Judge Shift",
            "evaluation_method_description": "Computes the absolute percentage-point difference between automated judge metrics (e.g., Requirements Met %) and the Human-as-a-Judge consensus results.",
            "evaluation_metric": "Absolute percentage-point deviation",
            "metric_definition": "Judge Shift = |automated_judge_value - human_consensus_value| (expressed in percentage points)",
            "dataset_or_benchmark": "DevAI",
            "human_evaluation_details": "Human consensus used as baseline; Judge Shift reported per agent and per metric.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Examples: LLM-as-a-Judge had Judge Shift up to ~31.42% for OpenHands on some metrics; Agent-as-a-Judge had much lower shifts (e.g., as low as 0.27%).",
            "comparison_to_human_generated": true,
            "comparison_results": "Shows Agent-as-a-Judge is closer to human consensus (smaller Judge Shift) than LLM-as-a-Judge.",
            "limitations_noted": "Single-number deviation may hide asymmetric error types and is sensitive to class imbalance.",
            "uuid": "e8027.6",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Requirements (I/D)",
            "name_full": "Requirements Met (Independent / Dependent)",
            "brief_description": "Two related metrics that measure the fraction of task requirements satisfied either treating requirements independently (I) or considering their DAG-based dependencies (D).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science (benchmark evaluation)",
            "theory_type": "metric / criterion",
            "evaluation_method_name": "Requirements Met (I) and Requirements Met (D)",
            "evaluation_method_description": "Requirements Met (I): percent of atomic requirements marked satisfied ignoring prerequisites. Requirements Met (D): percent of requirements satisfied when prerequisites/dependencies (in the DAG) are taken into account.",
            "evaluation_metric": "Percent of requirements satisfied (I and D)",
            "metric_definition": "Requirements Met (I) = (# requirements labeled satisfied ignoring dependencies) / (total requirements) * 100%; Requirements Met (D) accounts for dependency DAG—downstream requirements may be considered unmet if prerequisites fail.",
            "dataset_or_benchmark": "DevAI",
            "human_evaluation_details": "Used in both human and automated judgments; reported per-developer-agent and per-judge.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Examples from Table 2: Human-as-a-Judge Requirements Met (I) for MetaGPT 22.13%, GPT-Pilot 44.80%, OpenHands 42.89%; Requirements Met (D) lower due to dependencies (e.g., MetaGPT 6.55%).",
            "comparison_to_human_generated": true,
            "comparison_results": "Used to compare developer agents and judge types; Agent-as-a-Judge reported Requirements Met values closer to human consensus than LLM-as-a-Judge in many settings.",
            "limitations_noted": "Dependencies significantly reduce effective success rates; correctness of dependency modeling is crucial and may affect metric interpretation.",
            "uuid": "e8027.7",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Majority Vote / Consensus",
            "name_full": "Majority Vote and Consensus Aggregation",
            "brief_description": "Aggregation strategies for human labels: initial independent labels by multiple humans, followed by majority vote and an additional debate to reach consensus used as reference ground truth.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "human evaluation methodology / statistics",
            "theory_type": "criterion / aggregation method",
            "evaluation_method_name": "Majority Vote and Consensus",
            "evaluation_method_description": "Independent human labels combined via majority vote; evaluators later debate to reach a consensus which is used as the final reference; majority vote shown to reduce error relative to individual annotators.",
            "evaluation_metric": "Inter-annotator disagreement rate; majority-vote error rate vs consensus",
            "metric_definition": "Disagreement rate = % pairwise label disagreements; majority-vote error = % mismatch between majority-vote and consensus labels.",
            "dataset_or_benchmark": "DevAI",
            "human_evaluation_details": "Three expert annotators; majority vote reduced error to ~6.01% relative to consensus; consensus build required additional 28.5 hours.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Majority vote had smaller deviation from consensus than individual annotators; consensus used as final human baseline.",
            "comparison_to_human_generated": null,
            "comparison_results": "Ensemble/majority voting improves agreement with consensus and reduces single-annotator noise.",
            "limitations_noted": "Requires multiple skilled annotators and extra time; majority vote can still be wrong if annotator accuracy &lt;50% or biases exist.",
            "uuid": "e8027.8",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Basic Statistics Metrics",
            "name_full": "Basic developer-agent statistics (cost, time, tokens, files, lines)",
            "brief_description": "Operational metrics recorded for developer agents including average API cost (USD), average time (s), input/output tokens, saved files and saved code lines, used as practical utility measures.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o-2024-05-13 (backend used by developer agents)",
            "model_size": "gpt-4o-2024-05-13",
            "scientific_domain": "computer science (systems evaluation / cost analysis)",
            "theory_type": "metric collection / operational metrics",
            "evaluation_method_name": "Basic statistics (Table 1)",
            "evaluation_method_description": "Collect per-run averages over developer agents for cost (USD), runtime (seconds), token counts, number of saved files, and saved code lines to profile practical resource usage and output verbosity.",
            "evaluation_metric": "Average Cost (USD); Average Time (s); Average Input/Output Tokens; Average Saved Code Files; Average Saved Code Lines; Average Saved Files",
            "metric_definition": "Reported as arithmetic means across runs (units: USD, seconds, token counts, file counts, line counts).",
            "dataset_or_benchmark": "Collected from runs on DevAI tasks",
            "human_evaluation_details": "These are automated logs; used as contextual stats for evaluation cost/time comparisons with human judges.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Examples from Table 1: MetaGPT average cost $1.19, avg time 775.29 s; GPT-Pilot avg cost $3.92, avg time 1622.38 s; OpenHands avg cost $6.38, avg time 362.41 s.",
            "comparison_to_human_generated": false,
            "comparison_results": "Used to highlight cost/time advantages of automated judges vs human evaluation (Agent-as-a-Judge cost ~$30.58 vs human ~$1297.50 for full evaluation).",
            "limitations_noted": "Operational metrics do not directly reflect task correctness or quality; differences in internal communications and saving behaviors affect file/line counts.",
            "uuid": "e8027.9",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ICE-Score / CodeBERTScore / G-EVAL",
            "name_full": "ICE-Score, CodeBERTScore, and G-EVAL (LLM-based evaluation metrics referenced)",
            "brief_description": "Existing automated scoring approaches referenced in related work: ICE-Score instructs LLMs to evaluate code, CodeBERTScore uses pretrained code models to score code similarity/quality, and G-EVAL uses GPT-4 for NLG evaluation with improved human alignment.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP/code-evaluation",
            "theory_type": "metrics / evaluation methods (mentioned in related work)",
            "evaluation_method_name": "ICE-Score; CodeBERTScore; G-EVAL",
            "evaluation_method_description": "ICE-Score: instruct LLMs to score code based on instructions; CodeBERTScore: embedding-based code similarity metric; G-EVAL: uses GPT-4 to evaluate NLG outputs and improves alignment with human judgments.",
            "evaluation_metric": "Plausibility/human-alignment scores, embedding-based similarity, automated functional correctness proxies",
            "metric_definition": "Varies by method: ICE-Score returns instruction-following scores (scale depends on prompt); CodeBERTScore uses cosine similarity of embeddings; G-EVAL reports human-aligned ratings (scales vary).",
            "dataset_or_benchmark": "",
            "human_evaluation_details": "Referenced as prior approaches to automated judging and code evaluation; not directly used in experiments here.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Mentioned to motivate need for richer agentic evaluation; LLM-only judges can lack intermediate process understanding and can be brittle.",
            "uuid": "e8027.10",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Process-Supervised Reward Model (PRM)",
            "name_full": "Process-Supervised Reward Model (PRM)",
            "brief_description": "A proposed application/outlook: learning reward models supervised by process/trajectory information (intermediate feedback) to improve agentic systems, mentioned as a possible future direction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "reinforcement learning / agent optimization",
            "theory_type": "proposed evaluation/optimization approach",
            "evaluation_method_name": "Process-Supervised Reward Model (PRM)",
            "evaluation_method_description": "Learn reward functions that incorporate intermediate process signals (from Agent-as-a-Judge) to provide denser feedback for agentic self-improvement and address sparse reward problems.",
            "evaluation_metric": "Not implemented here; conceptually would use reward learning metrics (e.g., improvement in downstream task performance, cumulative reward)",
            "metric_definition": "Not specified in paper; would likely be task-dependent and measured in standard RL units (cumulative reward, success rate).",
            "dataset_or_benchmark": "Proposed to be used with agentic evaluation pipelines like Agent-as-a-Judge and DevAI",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Described as future work; potential for compounding errors if reward model relies on imperfect judgments.",
            "uuid": "e8027.11",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "MLLM-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark",
            "rating": 2
        },
        {
            "paper_title": "Swe-bench: Can language models resolve real-world github issues?",
            "rating": 2
        },
        {
            "paper_title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
            "rating": 1
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 1
        },
        {
            "paper_title": "ICE-Score: Instructing large language models to evaluate code",
            "rating": 1
        },
        {
            "paper_title": "CodeBERTScore: Evaluating code generation with pretrained models of code",
            "rating": 1
        }
    ],
    "cost": 0.022952999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Agent-as-a-Judge: Evaluate Agents with Agents</h1>
<p>Mingchen Zhuge ${ }^{1,2}$, Changsheng Zhao ${ }^{1}$, Dylan R. Ashley ${ }^{2}$, Wenyi Wang ${ }^{2}$, Dmitrii Khizbullin ${ }^{2}$,<br>Yunyang Xiong ${ }^{1}$, Zechun Liu ${ }^{1}$, Ernie Chang ${ }^{1}$, Raghuraman Krishnamoorthi ${ }^{1}$, Yuandong Tian ${ }^{1}$,<br>Yangyang Shi ${ }^{1}$, Vikas Chandra ${ }^{1}$, Jürgen Schmidhuber ${ }^{2}$<br>${ }^{1}$ Meta AI, ${ }^{2}$ KAUST</p>
<p>Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes-ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems-by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.</p>
<p>Date: October 18, 2024
Correspondence: mingchen.zhuge@kaust.edu.sa, cszhao@meta.com
Dataset: https://huggingface.co/devai-benchmark
Project: https://github.com/metauto-ai/agent-as-a-judge
Note: First four authors made core contributions. KAUST crafted the dataset.
Work done while Mingchen was interning at Meta, with Changsheng leading.</p>
<h2>Meta</h2>
<h2>1 Introduction</h2>
<p>Recent years have seen multimodal agentic systems move from occasionally being able to solve small toy problems to being regularly deployed for challenging real-world problems (the dream of most AI research). Yet, the current evaluation methods and the available benchmarks for agentic systems are struggling to keep up with these rapid advances, dramatically slowing true progress.</p>
<p>We believe that the current issue with evaluating agentic systems stems from the lack of feedback during the intermediate task-solving stages for these nontraditional systems. Agentic systems think more like humans, often act step-by-step (Wooldridge, 1999) and often host very human-like symbolic communications internally to solve problems (Zhuge et al., 2023). And thus agentic systems should be evaluated like a human, with rich evaluative feedback which looks at the full thought and action trajectory; evaluating an agentic system in the traditional way is like evaluating a student using multiple-choice testing-a comparatively unreliable estimator (Park, 2010). For example, while SWE-Bench (Yang et al., 2024a) is widespread, its evaluation method, which relies solely on the final resolve rate for long-term automated repair tasks, does not effectively pinpoint what is happening within agentic systems that affects the resolve rate. On the other hand, performing a better evaluation with a human is prohibitively expensive. We instead propose that agentic systems should be used to evaluate agentic systems. Inspired by LLM-as-a-Judge (Zheng et al., 2024; Fu et al., 2023; Chen et al., 2024b), which uses LLMs to evaluate LLMs, we call this framework Agent-as-a-Judge, of which it is</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 We introduce the Agent-as-a-Judge framework wherein agentic systems are used to evaluate agentic systems. We compare this to LLM-as-a-Judge, which uses LLMs to evaluate LLMs and for which Agent-as-a-Judge is a natural evolution, and Human-as-a-Judge, where skilled human labourers manually evaluate an agentic system.</p>
<p>a key extension to the world of agentic systems (see Figure 1). It not only retains the cost-effectiveness of LLM-as-a-Judge but is also equipped with agentic features, allowing it to provide rich intermediate feedback throughout the entire process, as it acts as an agentic system. We apply the Agent-as-a-Judge systems to the problem of evaluating code generating systems—one of the areas where agentic systems have looked the most promising recently.</p>
<p>In code generation, the development of benchmarks has also lagged behind the rapid advancement of agentic systems. HumanEval (chen2021eval), for example, focuses exclusively on algorithmic problems, while MBPP (Austin et al., 2021) deals with simple programming tasks. Although they are useful for evaluating the basic skills of foundation models, neither of these two reflects the most practical challenges developers face. As a step away from this, SWE-Bench (Jimenez et al., 2023) did introduce more realistic problems from GitHub, offering a fresh approach to evaluation, but still primarily focuses on automated repairs tasks development process. Concerningly, recent research shows that large language models (LLMs) can already solve over 27% of the tasks in SWE-Bench without needing of advanced agentic systems (Xia et al., 2024). Equally concerning, recent work has begun to introduce mechanisms designed specifically for the individual tasks in the SWE-Bench dataset, leading to a lack of real-world generalization and violating Goodhart’s law: "When a measure becomes a target, it ceases to be a good measure" (Goodhart, 1976).</p>
<p>To address the aforementioned issues with the current benchmarks in code generation, we introduce DevAI: the AI Developer Dataset, which contains 55 real-world comprehensive AI app development tasks created by expert annotators. We apply three leading open-source code-generating agentic frameworks to the tasks in DevAI: MetaGPT (Hong et al., 2024b), GPT-Pilot (Pythagora.io, 2023), and OpenHands (Wang et al., 2024d). We evaluate their performance using human judges (a painstaking process), LLM-as-a-Judge (Zheng et al., 2024), and our Agent-as-a-Judge framework.</p>
<p>Through human evaluation, we found that GPT-Pilot and OpenHands were each able to satisfy about 29% of the task requirements in DevAI, but only one full task—showing that DevAI presents a good level of challenge to current systems. When comparing our human judges with our automatic Agent-as-a-Judge framework, we found that Agent-as-a-Judge aligns more closely with the consensus of our human judges (90%) as compared to LLM-as-a-Judge (70%) in all cases tested. In addition, we find that it aligns more closely with this ensemble than the individual human evaluators do, suggesting that—not only is it suitable as a replacement for a human evaluator—but it could in fact be more useful than an average lone human evaluator.</p>
<p>In addition, considering the evaluation cost, Agent-as-a-Judge saves $97.72 \%$ of the time and $97.64 \%$ of the cost compared to involving three human experts.</p>
<p>In summary, the principal contributions of this work are:</p>
<ul>
<li>We release the DevAI dataset, which consists of 55 comprehensive AI development tasks with accompanying tags, individual hierarchical requirements, and individual preferences.</li>
<li>We benchmark three top open-source code generation agentic frameworks in DevAI, providing a more comprehensive analysis than previous evaluations of them.</li>
<li>We introduce the general Agent-as-a-Judge concept, allowing agentic systems a fair and rich evaluation without the traditional costs associated with human involvement.</li>
<li>We demonstrate that an Agent-as-a-Judge outperforms an LLM-as-a-Judge and performs comparably to human evaluators in our proof-of-concept.</li>
</ul>
<p>Tips: We provide a paper outline and the experimental design in Appendices A and B.</p>
<h1>2 DevAI: A Dataset for Automated AI Development</h1>
<p>In this section, we introduce our new DevAI benchmark. We then evaluate three state-of-the-art codegenerating agentic systems on this benchmark in Section 3 and present their basic statistics.</p>
<h3>2.1 Motivation</h3>
<p>Background The code generation domain is an area where agentic systems have seen significant industrial deployment over the past two years (e.g., Devin ${ }^{1}$ and Cursor $^{2}$ ). However, in code generation, there isn't yet a benchmark that accurately reflects realistic user queries for developing complete AI systems. We believe this is because of the difficulty to evaluate such complex, real-world tasks. For example, while many companies advertise their systems based on its performance on benchmarks such as SWE-Bench (Yang et al., 2024a) (for automated repair) or HumanEval (Chen et al., 2021) (for algorithmic problems), these benchmarks cover only a small bit of an actual development process. Moreover, none of them accurately reflect the intermediate stages of development or provide sufficient reward signals for long-horizon development-similar issues are present in OpenAI's recent MLE-Bench (Chan et al., 2024). A benchmark that can evaluate the entire development process-ideally in a way that can help understand the degree to which current AI methods can reduce human labour-is missing.</p>
<p>Topic We chose automated AI development as our main topic. While AI and ML tasks are often more complex, they follow clear, standard procedures. For example, data processing typically comes first in an AI pipeline, and performance reporting goes at the end. We believe this topological nature can help better monitor the development process and provide useful signals to the agentic systems.</p>
<p>Goals An ideal benchmark should address critical issues in automated development by focusing on three key factors. First, it should reflect practical software scenarios, where tasks are often too complex for a single LLM, requiring human or agentic systems. Second, it should emphasize the development process, not just final outcomes (e.g., pass@1 rates offer limited feedback and fail to highlight intermediate problems). Lastly, the evaluation should be computationally cost-effective and efficient, avoiding long training times or excessive manual oversight.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 Distribution of DevAI Tasks (1) DevAI focuses on AI development tasks and so terms such as "dataset," "model," and "results" are particularly common in the queries. (2) The first 53 tasks in DevAI all have a one-paragraph query but of varying lengths (note that task 54 and 55 are excluded here as they are outliers, representing the longest and most complex tasks in the dataset). (3) Each task has one or more tags. The prevalence of supervised learning here reflects the fact that it dominates many machine learning applications. (4) SVM classifiers (Cortes, 1995) and LSTM models (Hochreiter, 1997) are two of the most widely used architectures-a fact reflected by DevAI.</p>
<h1>2.2 The DevAI Dataset</h1>
<p>Motivated by the ideas outlined above, we propose the DevAI dataset. DevAI consists of a curated set of 55 tasks, each defined by (1) a plain text user query that describes an AI development task; (2) a set of plain text requirements (for a total of 365 requirements), each with a set of dependencies connecting them to other requirements; and (3) a set of preferences (for a total of 125 preferences) which represent softer requirements.</p>
<p>DevAI is structured so that an agentic system starts by receiving a user query to begin development. The system is then evaluated on how well it meets the requirements, with preferences serving as optional, softer criteria. An example of one of the DevAI tasks can be seen in Figure 3.</p>
<p>The tasks in DevAI are relatively small-scale but cover commonly used key development techniques. As shown in Figure 2, our tasks are tagged and cover a variety of key areas in AI: supervised learning, reinforcement learning, computer vision, natural language processing, generative models, and others. Each of the tasks is a real-world problem that could be given to a research engineer, while simultaneously being relatively inexpensive computationally to run so as to reduce the cost of evaluating a method on this benchmark. Details of the sample collection and human labeling process for DevAI are provided in Appendix E.</p>
<p>The requirements belonging to each task represent a milestone in the comprehensive development process and are arranged as a directed acyclic graph (similar to the work by He et al. (2021)), with requirements such as visualizing results depending on correct data loading and modeling. This allows for more comprehensive non-sparse feedback than a binary success metric. Furthermore, the inclusion of hierarchical requirements makes simple memorization an inadequate solution strategy, as completing the entire task requires agentic capabilities rather than relying solely on symbolic memorization, as is typical in foundation models.</p>
<h3>2.3 Preliminary Benchmark</h3>
<p>We first conduct experiments to collect development outcomes from different frameworks, which serve as baselines in the DevAI dataset. We test three of the most popular open-source frameworks (which we</p>
<h1>Query</h1>
<p>Hi! Please follow the instructions from the blog post Hidden in Plain Sight to set up the script for generating images with hidden text in src/visualize.py. Ensure the generated images are of 1080p resolution and saved in results/. Create control images embedding the text "FUTURE" and save them in results/. Please manually verify that the hidden text is embedded in the images.</p>
<h2>Requirements</h2>
<ul>
<li>R0</li>
</ul>
<p>Criteria: Follow the instructions from the blog post Hidden in Plain Sight to set up the script for generating images with hidden text in src/visualize.py.
Dependencies $\rightarrow{ }$</p>
<ul>
<li>R1</li>
</ul>
<p>Criteria: Ensure the generated images are of 1080p resolution and saved in results/.
Dependencies $\rightarrow{$ R0 $}$</p>
<ul>
<li>R2</li>
</ul>
<p>Criteria: Create control images embedding the text "FUTURE" and save them in results/.
Dependencies $\rightarrow{$ R1 $}$</p>
<h2>Preferences (Optional)</h2>
<ul>
<li>P0</li>
</ul>
<p>Criteria: The system should be capable of learning and adapting to unfamiliar technologies and tools as required.</p>
<ul>
<li>P1</li>
</ul>
<p>Criteria: After reviewing the blog post, ControlNet should successfully run on Modal to produce images with hidden messages for FUTURE.</p>
<p>Figure 3 A task example in DevAI. This task is adapted from a real-world demo given at https://www.cognitio n.ai/blog/introducing-devin. As this example shows, task requirements in DevAI are structured as a Directed Acyclic Graph (DAG), with nodes representing individual requirements and directed edges showing dependencies. More examples are in Appendix G.
will refer to as "AI developers"): MetaGPT (Hong et al., 2024b), GPT-Pilot (Pythagora.io, 2023), and OpenHands (Wang et al., 2024d) —all selected for their strong community acceptance (each having over 30,000 stars on GitHub).</p>
<p>Experiment Setup All of these three systems require a language model as a back-end engine, for which we use gpt-4o-2024-05-13, a state-of-the-art language model. These AI developers were given a time-limit of 1800 seconds to solve each task and were forcefully halted if they exceeded this time limit (we imposed this constraint, which was visible to the AI developers, as detailed in Appendix I). We capture the outputs generated during the automated development process, including code, files, and other artifacts. Additionally, we record key decisions and actions made by the agentic systems through some custom instrumentation code, resulting in a development trajectory for each of the agentic systems.
Analysis The basic statistics are shown in Table 1. MetaGPT is the most cost-efficient (1.19 USD), while OpenHands is the most expensive (6.38 USD). In terms of development time, OpenHands completes tasks</p>
<p>Table 1 Preliminary Statistics of AI Developers. We compare three leading open-source code agents using metrics such as average cost, average time, and the average number of generated files.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">MetaGPT (Hong et al., 2024b)</th>
<th style="text-align: center;">GPT-Pilot (Pythagora.io, 2023)</th>
<th style="text-align: center;">OpenHands (Wang et al., 2024d)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Basic Statistics</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Version</td>
<td style="text-align: center;">Data Interpreter (Hong et al., 2024a)</td>
<td style="text-align: center;">0.2.13</td>
<td style="text-align: center;">CodeAct v1.9 (Wang et al., 2024c)</td>
</tr>
<tr>
<td style="text-align: center;">(1) Average Cost</td>
<td style="text-align: center;">$\$ 1.19$</td>
<td style="text-align: center;">$\$ 3.92$</td>
<td style="text-align: center;">$\$ 6.38$</td>
</tr>
<tr>
<td style="text-align: center;">(2) Average Time</td>
<td style="text-align: center;">775.29 s</td>
<td style="text-align: center;">1622.38 s</td>
<td style="text-align: center;">362.41 s</td>
</tr>
<tr>
<td style="text-align: center;">(3) Average Input Tokens</td>
<td style="text-align: center;">152863</td>
<td style="text-align: center;">606707</td>
<td style="text-align: center;">1252482</td>
</tr>
<tr>
<td style="text-align: center;">(4) Average Output Tokens</td>
<td style="text-align: center;">28546</td>
<td style="text-align: center;">59707</td>
<td style="text-align: center;">8457</td>
</tr>
<tr>
<td style="text-align: center;">(5) Average Saved Code Files</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">3.84</td>
<td style="text-align: center;">2.53</td>
</tr>
<tr>
<td style="text-align: center;">(6) Average Saved Code Lines</td>
<td style="text-align: center;">11.15</td>
<td style="text-align: center;">273.33</td>
<td style="text-align: center;">96.56</td>
</tr>
<tr>
<td style="text-align: center;">(7) Average Saved Files</td>
<td style="text-align: center;">4.42</td>
<td style="text-align: center;">5.91</td>
<td style="text-align: center;">3.60</td>
</tr>
</tbody>
</table>
<p>in an average of 362.41 s , while GPT-Pilot takes the longest at 1622.38 s . On average, a full evaluation on DevAI with one of these three took around 210.65 USD and 14 hours to perform. While running, GPT-Pilot generates the most output tokens at 59707 tokens, whereas OpenHands processed the most at 1252482 tokens while producing the fewest at 8457 tokens. This suggests that OpenHands's internal communication is more complicated but is more parsimonious in its decisions.</p>
<p>MetaGPT, while being the most cost-effective, generates fewer saved code files ( 0.42 ), suggesting it may be less inclined to save files. In contrast, GPT-Pilot generates the most saved files (3.84), reflecting a more prolific output. The difference in saved code lines, with GPT-Pilot saving 273.33 lines versus MetaGPT's 11.15, underscores GPT-Pilot's extensive output. Meanwhile, OpenHands, despite handling larger inputs, seems less focused on executing code to generate files, as evidenced by its lower file output ( 2.53 saved files). These statistics align with real user experiences (as discussed in Appendix F).</p>
<p>Evaluations Note that the results in Table 1 are not directly indicative of performance but provide valuable insights into the practical utility of DevAI and the performance of AI developers. The generated workspaces (generated files, code, etc.) and trajectories are utilized in subsequent experiments to perform evaluations using Human-as-a-Judge (section 3), LLM-as-a-Judge, and Agent-as-a-Judge (section 4).</p>
<h1>3 Human-as-a-Judge: Manual Evaluation on DevAI</h1>
<p>To determine the pragmatic validity of DevAI and to accurately estimate the actual code-generating abilities of current state-of-the-art agentic systems, in this section, we run and then manually evaluate the application of three AI developer baselines to DevAI. In Section 4, we show how this evaluation can be automated.</p>
<p>Table 2 Human-as-a-Judge for AI Developers. (I) and (D) represent independent performance versus performance considering task dependencies. indicates multiple experts evolved, and means the evaluations use white-box testing (allowing access to the generated workspace, human-collected trajectories, and open-source codebases). The results were derived from expert judgments and deliberations (see Appendix H).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">MetaGPT (Hong et al., 2024b)</th>
<th style="text-align: center;">GPT-Pilot (Pythagora.io, 2023)</th>
<th style="text-align: center;">OpenHands (Wang et al., 2024d)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">(A) Requirements Met (I)</td>
<td style="text-align: center;">$22.13 \%$</td>
<td style="text-align: center;">$44.80 \%$</td>
<td style="text-align: center;">$42.89 \%$</td>
</tr>
<tr>
<td style="text-align: left;">(B) Requirements Met (D)</td>
<td style="text-align: center;">$6.55 \%$</td>
<td style="text-align: center;">$28.96 \%$</td>
<td style="text-align: center;">$28.68 \%$</td>
</tr>
<tr>
<td style="text-align: left;">(C) Self-Termination</td>
<td style="text-align: center;">$41.81 \%$</td>
<td style="text-align: center;">$5.45 \%$</td>
<td style="text-align: center;">$54.54 \%$</td>
</tr>
<tr>
<td style="text-align: left;">(D) Task Solve Rate</td>
<td style="text-align: center;">$0.00 \%$</td>
<td style="text-align: center;">$1.81 \%$</td>
<td style="text-align: center;">$1.81 \%$</td>
</tr>
</tbody>
</table>
<h1>3.1 Benchmark Baselines by Human-as-a-Judge</h1>
<p>Human Evaluation Setup After obtaining the baseline executions and conducting basic statistical analysis, we have three expert human evaluators (referred to here by their anonymous names: 231a, 38bb, and cn90) review the outputs of AI developer baselines to assess whether each requirement was satisfied. We have two rounds of human evaluations. To capture the bias inherent in typical human evaluation (this is desirable to capture here as it represents a likely scenario in deployment), in the first round, our evaluators first discussed the basic standards but were given minimal instructions. The templates the evaluators were given for the evaluation and their self-reported post-hoc descriptions of how they resolved ambiguities are reported in Figure 12 in Appendix H.</p>
<p>After the initial round of human evaluations (which totaled an estimated total of 58 human hours), we asked our evaluators to discuss and reach a consensus on their assessments (which took an estimated total of 28.5 additional human hours). This consensus, achieved after long sessions of debate, was used as the final human evaluation result for each method.</p>
<p>Performance Analysis The results of this experiment are shown in Table 2. We found that the two bestperforming methods (GPT-Pilot and OpenHands) could satisfy about $29 \%$ of the requirements (or around $44 \%$ if prerequisites are ignored) but only on one task could they meet all the requirements. This highlights that DevAI offers a considerable but appropriate level of challenge for current and future methods. Moreover, the fulfillment of intermediate requirements aligns with our expectations, as discussed in Section 2, that DevAI provides richer feedback by uncovering how agentic systems falter during the process instead of just focusing on a single performance metric at the end.</p>
<h3>3.2 Judging Human-as-a-Judge</h3>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4 Between the three human evaluators, a large amount of disagreement was observed in their individual evaluations-highlighting the inherent unreliability of a single human evaluation.</p>
<p>Disagreement Analysis To analyze the presence of inductive bias and the reliability of the Human-as-a-Judge paradigm here, we calculate the disagreement rate between individual evaluators (shown in Figure 4). The results indicate that the disagreement rates between pairs of evaluators range from around $10 \%$ to $30 \%$.</p>
<p>Due to the complexity of a complete AI development task, which typically involves multiple steps with varying outcomes at each step, humans can easily make errors when critical information is missed, such as environment feedback indicating small but severe coding errors or bugs. Additionally, some disagreements are not necessarily incorrect but arise from differing perspectives on how ambiguity should be resolved.</p>
<p>Error Analysis As previously noted, the evaluators engaged in a round of debating after their initial evaluations until they reached a consensus on each requirement in each task (with the results of this consensus evaluation shown in Table 2).</p>
<p>In our Human-as-a-Judge pipeline, evaluators could be convinced by evidence from others and acknowledge their judgment errors, adjusting their answers accordingly. This can be used to approximate individual errors. If the consensus evaluation more accurately predicts any extant ground truth, we would expect the majority vote from the individual evaluations to more closely approximate this than any single evaluation, due to the fundamental properties of ensemble classifiers (see Hastie et al. (2009)).</p>
<p>While the consensus evaluation may not represent the absolute ground truth (we acknowledge that some quantity of error likely would still exist even after this procedure), we expect the consensus evaluation to more accurately approximate any extant ground truth (Clemen, 1989). If this holds, the majority vote should align more closely with the consensus than with any individual evaluation. As shown in Figure 5, this is the case.</p>
<p>As seen in the results, although significant errors occur among all evaluators, the majority vote effectively corrects most of these errors. Notably, cn9o made the most errors (for example, $23.77 \%$ in evaluating GPT-Pilot). After applying the majority vote from all three evaluators, the overall error rate dropped to $6.01 \%$, demonstrating the inherent benefits of majority voting.</p>
<p>Conclusion Human judgment errors are inevitable. To reduce them, we suggest two methods. First, like in this work, introduce a debate round after each judgment, where individuals present evidence and either persuade others or adjust their own opinions after discussion. This is particularly important when there are only a few evaluators, as majority voting with a small group can still lead to errors (around $5 \%$ compared to consensus evaluation, as shown in Figure 5). The second approach involves assembling a larger panel of experts (more is better when their accuracy exceeds $50 \%$ (Grofman et al., 1983)), with over 5 people recommended by Hastie and Kameda (2005); Larrick and Soll (2006), and relying on a majority vote. However, due to the high cost of engaging more experts and the fact that this is not always feasible in practice, we argue for the former.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5 Mismatch between the individual evaluations and the consensus evaluation. In particular, the majority vote classifier showed the smallest deviation from the consensus evaluation.</p>
<h1>4 Agent-as-a-Judge: Evaluating Agents with Agents</h1>
<p>Human evaluation, while somewhat reliable, is time-consuming and requires substantial expertise. To address this, we propose the Agent-as-a-Judge framework. If such an agentic system could evaluate like a human, it would reduce the need for human involvement and eliminate the trade-off between thoroughness and effort.</p>
<h3>4.1 Proof-of-Concept</h3>
<p>Based on our prior experiences with agent design and by imitating the human evaluation process, we initially designed eight modular, interacting components that form the foundation of our Proof-of-Concept for the Agent-as-a-Judge.
(1) The graph module constructs a graph that captures the entire structure of the project, including files, modules, and dependencies. It can also break down chunks of code into code snippets. (2) The locate module identifies the specific folder or file referred to by a requirement. (3) The read module goes beyond simple file parsing, supporting the reading and understanding of multimodal data across 33 different formats, including code, images, videos and documents. This allows the agent to cross-reference various data streams and verify different kinds of requirement. (4) The search module provides a contextual understanding of code and can quickly retrieve highly relevant code snippets, as well as the nuances behind them (e.g., hidden dependencies). (5) The retrieve module</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6 Initial diagram of Agent-as-a-Judge.</p>
<p>Table 3 AI Judges and Their Shift/Alignment with Human-as-a-Judge. We compare the results of LLM-as-a-Judge and Agent-as-a-Judge with Human-as-a-Judge. (I) represents performance on independent tasks, while (D) represents performance considering task dependencies. Note: $\square$ gray-box settings use carefully manually collected trajectory data (which is nearly inaccessible in practical situations, see Appendix J). In contrast, black-box setting doesn't need to access to such data. The red scores represent the absolute judge shift compared with Human-as-a-Judge (e.g., $2.74 \%)$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">MetaGPT (Hong et al., 2024b)</th>
<th style="text-align: center;">GPT-Pilot (Pythagora.io, 2023)</th>
<th style="text-align: center;">OpenHands (Wang et al., 2024d)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLM-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(a) Requirements Met (I)</td>
<td style="text-align: center;">$19.39 \%(2.74 \%)$</td>
<td style="text-align: center;">$12.56 \%(32.24 \%)$</td>
<td style="text-align: center;">$11.47 \%(31.42 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(b) Requirements Met (D)</td>
<td style="text-align: center;">$1.63 \%(4.92 \%)$</td>
<td style="text-align: center;">$4.09 \%(24.87 \%)$</td>
<td style="text-align: center;">$2.18 \%(26.50 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(c) Task Solve Rate</td>
<td style="text-align: center;">$0.0 \%(0.0 \%)$</td>
<td style="text-align: center;">$0.0 \%(1.81 \%)$</td>
<td style="text-align: center;">$0.0 \%(1.81 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate $\uparrow$</td>
<td style="text-align: center;">$84.15 \%$</td>
<td style="text-align: center;">$65.30 \%$</td>
<td style="text-align: center;">$60.38 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Agent-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(I) Requirements Met (I)</td>
<td style="text-align: center;">$25.40 \%(3.26 \%)$</td>
<td style="text-align: center;">$53.00 \%(8.20 \%)$</td>
<td style="text-align: center;">$42.62 \%(0.27 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(II) Requirements Met (D)</td>
<td style="text-align: center;">$5.73 \%(0.81 \%)$</td>
<td style="text-align: center;">$39.89 \%(10.93 \%)$</td>
<td style="text-align: center;">$26.50 \%(2.17 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(III) Task Solve Rate</td>
<td style="text-align: center;">$0.0 \%(0.0 \%)$</td>
<td style="text-align: center;">$5.45 \%(3.64 \%)$</td>
<td style="text-align: center;">$1.81 \%(0.00 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate $\uparrow$</td>
<td style="text-align: center;">$88.52 \%$</td>
<td style="text-align: center;">$83.88 \%$</td>
<td style="text-align: center;">$90.44 \%$</td>
</tr>
<tr>
<td style="text-align: center;">LLM-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(a) Requirements Met (I)</td>
<td style="text-align: center;">$28.68 \%(6.55 \%)$</td>
<td style="text-align: center;">$38.79 \%(4.10 \%)$</td>
<td style="text-align: center;">$43.16 \%(0.27 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(b) Requirements Met (D)</td>
<td style="text-align: center;">$17.75 \%(11.20 \%)$</td>
<td style="text-align: center;">$33.06 \%(4.10 \%)$</td>
<td style="text-align: center;">$32.24 \%(3.56 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(c) Task Solve Rate</td>
<td style="text-align: center;">$1.81 \%(1.81 \%)$</td>
<td style="text-align: center;">$3.63 \%(1.82 \%)$</td>
<td style="text-align: center;">$7.27 \%(5.46 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate $\uparrow$</td>
<td style="text-align: center;">$68.86 \%$</td>
<td style="text-align: center;">$71.85 \%$</td>
<td style="text-align: center;">$70.76 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Agent-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(I) Requirements Met (I)</td>
<td style="text-align: center;">$23.49 \%(1.35 \%)$</td>
<td style="text-align: center;">$46.44 \%(1.64 \%)$</td>
<td style="text-align: center;">$43.44 \%(0.54 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(II) Requirements Met (D)</td>
<td style="text-align: center;">$6.01 \%(0.54 \%)$</td>
<td style="text-align: center;">$30.60 \%(1.64 \%)$</td>
<td style="text-align: center;">$28.14 \%(0.53 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">(III) Task Solve Rate</td>
<td style="text-align: center;">$0.0 \%(0.00 \%)$</td>
<td style="text-align: center;">$5.45 \%(3.64 \%)$</td>
<td style="text-align: center;">$3.63 \%(1.82 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate $\uparrow$</td>
<td style="text-align: center;">$92.07 \%$</td>
<td style="text-align: center;">$86.61 \%$</td>
<td style="text-align: center;">$90.16 \%$</td>
</tr>
<tr>
<td style="text-align: center;">/ Human-as-a-Judge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate (38bb)</td>
<td style="text-align: center;">$92.63 \%$</td>
<td style="text-align: center;">$90.98 \%$</td>
<td style="text-align: center;">$89.89 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate (cn9o)</td>
<td style="text-align: center;">$83.33 \%$</td>
<td style="text-align: center;">$76.23 \%$</td>
<td style="text-align: center;">$78.15 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate (231a)</td>
<td style="text-align: center;">$92.07 \%$</td>
<td style="text-align: center;">$87.43 \%$</td>
<td style="text-align: center;">$89.07 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Average of individuals</td>
<td style="text-align: center;">$89.34 \%$</td>
<td style="text-align: center;">$84.88 \%$</td>
<td style="text-align: center;">$85.70 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Best of individuals</td>
<td style="text-align: center;">$92.63 \%$</td>
<td style="text-align: center;">$90.98 \%$</td>
<td style="text-align: center;">$89.89 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate (Majority Vote)</td>
<td style="text-align: center;">$95.08 \%$</td>
<td style="text-align: center;">$93.98 \%$</td>
<td style="text-align: center;">$94.26 \%$</td>
</tr>
</tbody>
</table>
<p>extracts information from long texts, identifying relevant segments in trajectories. With context from the above, (6) the ask module determines whether a given requirement is satisfied. (7) The memory module stores historical judgment information, allowing the agent to build on past evaluations. Finally, (8) the planning module plans the following actions, allowing the agent to strategize and sequence tasks based on the current state and the project goals.</p>
<p>Our initial design of the Agent-as-a-Judge, including all its components, is shown in Figure 6, and the operational process of the Agent-as-a-Judge is illustrated in Figure 9.</p>
<p>After conducting comprehensive ablation studies, we found that the modular combination of (1), (2), (3), (5), and (6) achieved the highest performance (see Appendix K). A sample of the dynamic evidence collected by the Agent-as-a-Judge is shown in Appendix M. We hypothesize this is because Agent-as-a-Judge needs high-quality factual information and is sensitive to noise. For example, while our design of the planning module introduces promising decision-making for future actions, the procedure is unstable. Initially, we hoped that historical information from the memory module would help to assess current requirements. However, it</p>
<p>proved detrimental, as any errors in previous judgments could lead to a chain of errors, negatively affecting current decisions. Besides, the current workspaces generated by developer agents, having only hundreds of lines of code, cannot fully benefit from the search module. The details of these findings are explained in Appendix K. Note that a perfect Agent-as-a-Judge is not the focus of this proof of concept, and thus, we leave the utilization of advanced agentic optimization methods for Agent-as-a-Judge, such as automated prompt optimization and workflow design (Zhuge et al., 2024; Hu et al., 2024), for future work.</p>
<h1>4.2 Judging Agent-as-a-Judge and LLM-as-a-Judge</h1>
<p>Judge Shift Judge Shift measures deviation from the Human-as-a-Judge consensus results, with lower values indicating a closer alignment. As shown in table 3, Agent-as-a-Judge consistently outperforms LLM-as-a-Judge across tasks, particularly those with task dependencies. For example, in Requirement (I), Agent-as-a-Judge shows a Judge Shift as low as $0.27 \%$, while LLM-as-a-Judge reaches $31.24 \%$ for OpenHands. This underscores Agent-as-a-Judge's stability and suitability for meeting task requirements. Furthermore, in the gray-box setting, both Agent-as-a-Judge and LLM-as-a-Judge show even better results than their performance in the black-box setting.</p>
<p>Alignment Rate The Alignment Rate reflects how closely the AI Judges' evaluations align with human consensus across all 365 requirements. It is defined as the percentage of requirement evaluations that are the same as the Human-as-a-Judge consensus evaluation. Compared to LLM-as-a-Judge, Agent-as-a-Judge consistently achieves a higher Alignment Rate, closely matching human judgments. For example, when evaluating OpenHands, Agent-as-aJudge reaches $92.07 \%$ and $90.44 \%$, surpassing LLM-as-a-Judge's $70.76 \%$ and $60.38 \%$ in both gray-box and black-box settings. This shows that Agent-as-aJudge produces more accurate and human-aligned evaluations, especially in complex scenarios.</p>
<p>PR Curves Judging developer agents is a classimbalanced task, where meeting requirements is much rarer than failing. Metrics like judge shift and alignment rate can be misleading. For example, since MetaGPT rarely meets requirements, LLM-as-a-Judge easily identifies most cases as negative (achieving $84.15 \%$ in the black-box setting). PR Curves offer a clearer performance measure by balancing precision and recall. Agent-as-a-Judge even outperforms any single human evaluator on OpenHands and aligns closest with majority voting. This shows that, in some cases, Agent-as-a-Judge can nearly replace human evaluators.</p>
<h3>4.3 Ablations For Agent-as-a-Judge</h3>
<p>We conduct ablations to evaluate the impact of adding different components on Agent-as-a-Judge's performance. The components analyzed include ask, graph, read, locate, and retrieve. The component ablation study for Agent-as-a-Judge reveals key insights into the performance gains from adding specific functionalities.</p>
<p>With only the ask component, the agent achieves a $65.03 \%$ alignment rate. Adding the graph component increases performance to $75.95 \%$, as the agent can better understand the relationships between files.</p>
<p>The introduction of read further improves the alignment rate to $82.24 \%$, reflecting the value of direct access to the contents of the file. Incorporating locate brings a substantial boost to $90.44 \%$, as the agent can efficiently target files relevant</p>
<p>Figure 7 PR Curves comparing judge Methods.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Table 4 Component Ablation Studies for Agent-as-a-Judge. We analyze the impact of adding various components (ask, graph, read, locate, and retrieve) on the performance of Agent-as-aJudge for judging OpenHands.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">+ ask + graph + read + locate + retrieve</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Agent-as-a-Judge Performance</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Alignment Rate</td>
<td style="text-align: center;">$65.03 \%$ $75.95 \%$ $82.24 \%$ $90.44 \%$ $90.16 \%$</td>
</tr>
</tbody>
</table>
<p>to the requirements. However, adding retrieve does not provide a significant benefit in this case. In contrast, as shown in Table 3, the judgment of MetaGPT and GPT-Pilot indicates that retrieve is useful, as the trajectory provides additional valuable information.</p>
<h1>4.4 Cost Analysis</h1>
<p>The Human-as-a-Judge took the three evaluators a self-reported total of 86.5 hours. With a 15 USD minimum wage (assuming this would buy a subject expert in AI), a full evaluation under DevAI would cost around 1297.50 USD. In comparison, Agent-as-a-Judge cost only 30.58 USD in API calls and took only 118.43 minutes $-2.29 \%$ of the cost and $2.36 \%$ of the time of Human-as-a-Judge. LLM-as-a-Judge was faster at 10.99 minutes, but due to the absence of intelligent context selection by the Agent-as-a-Judge's modules, it still cost 29.63 USD.</p>
<h2>5 Related Work</h2>
<p>Agentic systems and their applications are highly active research areas with numerous recent works having a relation to this work. This section details those works most relevant to ours. We provide a treatment of the less relevant related works in Appendix D.</p>
<p>AI Developers AI in software development is growing fast (Liu et al., 2024). AI-driven developers have been applied to directly imitate software companies (Hong et al., 2024b; Qian et al., 2024a), debug code (Yang et al., 2024a), run data science methods (Guo et al., 2024; Hong et al., 2024a; Li et al., 2024; Qiao et al., 2023), and even write academic papers (Lu et al., 2024a).</p>
<p>Benchmarks for AI developments Benchmarks like MLAgentBench (Huang et al., 2024), ML-Bench (Liu et al., 2023d), SUPER (Bogin et al., 2024), DS-bench (Jing et al., 2024), and MLE-Bench (Chan et al., 2024) all focus on benchmarking agentic systems using AI tasks. However, DevAI distinguishes itself from all of these by focusing on realistic user queries that target a complete development cycle. It further includes a more comprehensive evaluation with multiple hierarchical requirements and preferences for each task. Comparatively, MLAgentBench (Huang et al., 2024) for example, focuses on final performance for a limited set of well-known tasks, which risks overfitting and fails to assess a system's generalization or adaptability.
AI Judges Several works have looked at using AI systems as judges ${ }^{3}$. The work by Chan et al. (2023); Zhao et al. (2024), for example, extends LLM-as-a-Judge to have multiple LLMs in their evaluation process for conversational tasks. Unlike Agent-as-a-Judge, they employ a trivial agentic system and apply it only to evaluate LLMs under traditional evaluation setups. In contrast, (Lu et al., 2024b) uses a single LLM-based evaluator but, unlike LLM-as-a-Judge, applies this to multimodal tasks rather than just for evaluating LLMs. Less relevant are frameworks like those by Chen et al. (2024a); Arora et al. (2024); Mündler et al. (2024), where intermediate signals are used during coding development.</p>
<h2>6 Discussion and Conclusion</h2>
<p>Outlook 1: Intermediate Feedback for Agentic Self-Improvement A key power of the Agent-as-a-Judge, though not fully exploited here but nonetheless clear, is that it provides intermediate feedback that is essential for effective and efficient optimization (Zhuge et al., 2024; Pan et al., 2024). For example, Agarwal et al. (2019) proposes to solve the sparse reward problem in reinforcement learning, by learning auxiliary reward functions that provide intermediate feedback. Perhaps the greatest strength of the Agent-as-a-Judge framework is that an agentic system can use it to identify and fix issues in its solutions to complex, multistage problems on the fly-something older, delayed-feedback methods did not permit. By introducing Agent-as-a-Judge, we create the opportunity to build a process-supervised reward model (PRM) for improving agentic systems (Lightman et al., 2023).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Outlook 2: Flywheel Effect Driven by Agent-as-a-Judge The cycle of mutual improvement between the Agent-as-a-Judge and the evaluated agents, where both evolve together through iterative feedback, presents a promising direction. We hypothesize that an agentic version of a self-play system (Zelikman et al., 2022; Chen et al., 2024e; Wang et al., 2024b), could viably emerge by using the Agent-as-a-Judge as a key mechanism. Furthermore, the ongoing interaction between the Agent-as-a-Judge and the evaluated agents has the potential to create a flywheel effect, where successive incremental improvements reinforce one another, leading to progressively greater optimization and enhanced performance over time (Wang et al., 2022). This iterative process may also serve as a valuable complement to LLM reasoning data, help embedding agentic capabilities into foundation models (Luo et al., 2024).</p>
<p>Conclusion In this work, we introduced the Agent-as-a-Judge method to use agentic systems to evaluate agentic systems. We simultaneously released DevAI: a new benchmark that evaluates the code-generating ability of agentic systems on complete AI development tasks when used with Agent-as-a-Judge. We went on to show that Agent-as-a-Judge outperforms existing methods on this task and that it performs similarly to an ensemble of expert human evaluators. Altogether, we believe that the above opens the door for scaling up agentic far more than before.</p>
<h1>Acknowledgements</h1>
<p>The authors thank Haozhe Liu, Piotr Piekos, Firas Laakom, Matteo Paltenghi for their suggestions or paper review. The research reported in this publication was supported by funding from the King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI under award number 5940 and the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence.</p>
<h2>References</h2>
<p>Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. Learning to generalize from sparse and underspecified rewards. In International conference on machine learning, pages 130-140. PMLR, 2019.</p>
<p>Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, and Nagarajan Natarajan. Masai: Modular architecture for software-engineering ai agents. arXiv preprint arXiv:2406.11638, 2024.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021 .</p>
<p>Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al. Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks. arXiv preprint arXiv:2406.18403, 2024.</p>
<p>Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, and Tushar Khot. Super: Evaluating agents on setting up and executing tasks from research repositories. arXiv preprint arXiv:2409.07440, 2024.</p>
<p>Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. Multipl-e: a scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering, 49(7):3675-3691, 2023.</p>
<p>Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Jacob Ginesin, Edward Berman, George Chakhnashvili, Anton Lozhkov, Carolyn Jane Anderson, and Arjun Guha. Can it edit? evaluating the ability of large language models to follow code editing instructions, 2024. URL https://arxiv.org/abs/2312.12450.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.</p>
<p>Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal A. Patwardhan, Lilian Weng, and Aleksander Mkadry. Mle-bench: Evaluating machine learning agents on machine learning engineering. 2024. URL https://api.semanticscholar.org/CorpusID:273233550.</p>
<p>Harrison Chase. LangChain. https://github.com/hwchase17/langchain, 2022.
Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, et al. Coder: Issue resolving with multi-agent and task graphs. arXiv preprint arXiv:2406.01304, 2024a.</p>
<p>Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. arXiv preprint arXiv:2402.04788, 2024b.</p>
<p>Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, et al. Gui-world: A dataset for gui-oriented multimodal llm-based agents. arXiv preprint arXiv:2406.10819, 2024c.</p>
<p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or llms as the judge? a study on judgement biases. arXiv preprint arXiv:2402.10669, 2024d.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024e.</p>
<p>Ching-An Cheng, Allen Nie, and Adith Swaminathan. Trace is the new autodiff-unlocking efficient optimization of computational workflows. arXiv preprint arXiv:2406.16218, 2024.</p>
<p>Robert T Clemen. Combining forecasts: A review and annotated bibliography. International journal of forecasting, 5 (4):559-583, 1989 .</p>
<p>Corinna Cortes. Support-vector networks. Machine Learning, 1995.
Yijiang River Dong, Tiancheng Hu, and Nigel Collier. Can llm be a personalized judge? arXiv preprint arXiv:2406.11657, 2024.</p>
<p>Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, and Cheng Yang. Multi-agent software development through cross-team collaboration. arXiv preprint arXiv:2406.08979, 2024.</p>
<p>Usama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth. From data mining to knowledge discovery in databases. AI magazine, 17(3):37-37, 1996.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.</p>
<p>Charles Goodhart. Monetary relationships: a view from Threadneedle Street. University of Warwick, 1976.
Significant Gravitas. Auto-gpt. GitHub repository, 2023.
Bernard Grofman, Guillermo Owen, and Scott L Feld. Thirteen theorems in search of the truth. Theory and decision, 15(3):261-278, 1983.</p>
<p>Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. Ds-agent: Automated data science by empowering large language models with case-based reasoning. arXiv preprint arXiv:2402.17453, 2024.</p>
<p>Md Mahim Anjum Haque. Fixeval: Execution-based evaluation of program fixes for competitive programming problems. PhD thesis, Virginia Tech, 2023.</p>
<p>Reid Hastie and Tatsuya Kameda. The robust beauty of majority rules in group decisions. Psychological review, 112 (2):494, 2005.</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2 edition, 2009. doi: 10.1007/978-0-387-84858-7.</p>
<p>Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art. Knowledge-based systems, 212: 106622, 2021.</p>
<p>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.</p>
<p>S Hochreiter. Long short-term memory. Neural Computation MIT-Press, 1997.
Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, et al. Data interpreter: An llm agent for data science. arXiv preprint arXiv:2402.18679, 2024a.</p>
<p>Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024b.</p>
<p>Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint arXiv:2408.08435, 2024.
Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. arXiv preprint arXiv:2312.13010, 2023.</p>
<p>Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation. In Forty-first International Conference on Machine Learning, 2024.</p>
<p>Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024.</p>
<p>Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, and Peter Clark. Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. arXiv preprint arXiv:2406.06769, 2024.</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.</p>
<p>Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, and Huaming Chen. From llms to llm-based agents for software engineering: A survey of current, challenges and future. arXiv preprint arXiv:2408.02479, 2024.</p>
<p>Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, and Dong Yu. Dsbench: How far are data science agents to becoming data science experts? arXiv preprint arXiv:2409.07703, 2024.</p>
<p>Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023.</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.</p>
<p>Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pages 18319-18345. PMLR, 2023.
langchain ai. LangGraph. https://github.com/langchain-ai/langgraph, 2024.
Richard P Larrick and Jack B Soll. Intuitions about combining opinions: Misappreciation of the averaging principle. Management science, 52(1):111-127, 2006.</p>
<p>V Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Proceedings of the Soviet physics doklady, 1966.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023.</p>
<p>Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. Mlr-copilot: Autonomous machine learning research based on large language models agents. arXiv preprint arXiv:2408.14033, 2024.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378 (6624):1092-1097, 2022.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, and Yiling Lou. Large language model-based agents for software engineering: A survey. arXiv preprint arXiv:2409.02977, 2024.</p>
<p>Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091, 2023a.</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023b.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023c.</p>
<p>Yuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu, Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan Hu, Zengxian Yang, Kaikai An, et al. Ml-bench: Large language models leverage open-source libraries for machine learning tasks. arXiv preprint arXiv:2311.09835, 2023d.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024a.</p>
<p>Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation. Advances in Neural Information Processing Systems, 36, 2024b.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Qingwei Lin, Jianguang Lou, Shifeng Chen, Yansong Tang, and Weizhu Chen. Arena learning: Build data flywheel for llms post-training via simulated chatbot arena. arXiv preprint arXiv:2407.10627, 2024.</p>
<p>Niels Mündler, Mark Niklas Müller, Jingxuan He, and Martin Vechev. Code agents are state of the art software testers. arXiv preprint arXiv:2406.12952, 2024.</p>
<p>Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. In First Conference on Language Modeling, 2024.</p>
<p>Jooyong Park. Constructive multiple-choice testing system. British Journal of Educational Technology, 41(6):1054-1064, 2010. doi: https://doi.org/10.1111/j.1467-8535.2010.01058.x. URL https://bera-journals.onlinelibrary.wile y.com/doi/abs/10.1111/j.1467-8535.2010.01058.x.</p>
<p>Huy Nhat Phan, Phong X Nguyen, and Nghi DQ Bui. Hyperagent: Generalist software engineering agents to solve coding tasks at scale. arXiv preprint arXiv:2409.16299, 2024.</p>
<p>Pythagora.io. Gpt-pilot: Your ai copilot for software development. https://github.com/Pythagora-io/gpt-pilot, 2023. URL https://github.com/Pythagora-io/gpt-pilot. GitHub repository.</p>
<p>Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15174-15186, 2024a.</p>
<p>Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Scaling large-language-model-based multi-agent collaboration. arXiv preprint arXiv:2406.07155, 2024b.</p>
<p>Bo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang Dong, Jue Zhang, Lu Wang, et al. Taskweaver: A code-first agent framework. arXiv preprint arXiv:2311.17541, 2023.</p>
<p>Vyas Raina, Adian Liusie, and Mark Gales. Is llm-as-a-judge robust? investigating universal adversarial attacks on zero-shot llm assessment. arXiv preprint arXiv:2402.14016, 2024.</p>
<p>N Reimers. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.
Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333-389, 2009.</p>
<p>Lin Shi, Weicheng Ma, and Soroush Vosoughi. Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms. arXiv preprint arXiv:2406.07791, 2024.</p>
<p>Linxin Song, Jiale Liu, Jieyu Zhang, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, and Chi Wang. Adaptive in-conversation team building for language model agents. arXiv preprint arXiv:2405.19425, 2024.</p>
<p>Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A multimodal agent for red dead redemption ii as a case study. arXiv preprint arXiv:2403.03186, 2024.</p>
<p>Wei Tao, Yucheng Zhou, Wenqiang Zhang, and Yu Cheng. Magis: Llm-based multi-agent framework for github issue resolution. arXiv preprint arXiv:2403.17927, 2024.</p>
<p>Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. arXiv preprint arXiv:2406.12624, 2024.</p>
<p>Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Debugbench: Evaluating debugging capability of large language models. arXiv preprint arXiv:2401.04621, 2024.</p>
<p>Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian Moghaddam, and Neel Sundaresan. Autodev: Automated ai-driven development. arXiv preprint arXiv:2403.08299, 2024.</p>
<p>Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014, 2024a.</p>
<p>Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024b.</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. arXiv preprint arXiv:2402.01030, 2024c.</p>
<p>Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024d.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022.</p>
<p>Rüdiger Wirth and Jochen Hipp. Crisp-dm: Towards a standard process model for data mining. In Proceedings of the 4th international conference on the practical applications of knowledge discovery and data mining, volume 1, pages 29-39. Manchester, 2000.</p>
<p>Michael Wooldridge. Intelligent agents. Multiagent systems: A modern approach to distributed artificial intelligence, 1: $27-73,1999$.</p>
<p>Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.</p>
<p>Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, and Qingyun Wu. Stateflow: Enhancing llm task-solving through state-driven workflows. arXiv preprint arXiv:2403.11322, 2024a.</p>
<p>Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024b.</p>
<p>Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024.</p>
<p>Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi, Ziniu Hu, Philip Torr, Bernard Ghanem, and Guohao Li. Can large language model agents simulate human trust behaviors? arXiv preprint arXiv:2402.04559, 2024.</p>
<p>Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024.</p>
<p>Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie, Yongchao Chen, Shilong Liu, Bochen Qian, et al. Crab: Cross-environment agent benchmark for multimodal language model agents. arXiv preprint arXiv:2407.01511, 2024.</p>
<p>John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024a.</p>
<p>Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.</p>
<p>Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, et al. Matplotagent: Method and evaluation for llm-based agentic scientific data visualization. arXiv preprint arXiv:2402.11453, 2024b.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022.</p>
<p>Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023.</p>
<p>Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Deli Zhao, and Lidong Bing. Auto arena of llms: Automating llm evaluations with agent peer-battles and committee discussions. arXiv preprint arXiv:2405.20267, 2024.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. Codebertscore: Evaluating code generation with pretrained models of code. arXiv preprint arXiv:2302.05527, 2023a.</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023b.</p>
<p>Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, et al. Agents: An open-source framework for autonomous language agents. arXiv preprint arXiv:2309.07870, 2023c.</p>
<p>Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532, 2024 .</p>
<p>Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Róbert Csordás, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in natural languagebased societies of mind. arXiv preprint arXiv:2305.17066, 2023.</p>
<p>Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jurgen Schmidhuber. Language agents as optimizable graphs. arXiv preprint arXiv:2402.16823, 2024.</p>
<p>Terry Yue Zhuo. Ice-score: Instructing large language models to evaluate code. arXiv preprint arXiv:2304.14317, 2023.
Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024.</p>
<h1>A Outline of this Paper</h1>
<h2>Paper: Agent-as-a-Judge: Evaluating Agents with Agents</h2>
<h2>Key Logic</h2>
<ul>
<li>Step 1: Concept Proposal</li>
</ul>
<p>Description: We propose the Agent-as-a-Judge concept, an extension of the LLM-as-a-Judge framework, aimed at evaluating agentic systems using other agentic systems.</p>
<ul>
<li>Step 2: Dataset Creation</li>
</ul>
<p>Description: To address the lack of suitable datasets for evaluating agentic systems in automated AI development, we introduce DevAI, a new dataset consisting of 55 realistic AI code generation tasks. This also serves as a testbed for the Agent-as-a-Judge proof-of-concept.</p>
<ul>
<li>Step 3: Baseline Evaluation of Developer Agents (Experiment Level 1)</li>
</ul>
<p>Description: In the first level of experiments, we select three popular open-source developer agents: MetaGPT, GPT-Pilot, and OpenHands. These agents are evaluated on the DevAI tasks to establish performance baselines.</p>
<ul>
<li>Step 4: Conducting Human-as-a-Judge Evaluation</li>
</ul>
<p>Description: We conduct a Human-as-a-Judge experiment, where three human experts assess the performance of the developer agents on the same DevAI tasks.</p>
<ul>
<li>Step 5: Human-as-a-Judge Analysis (Experiment Level 2)</li>
</ul>
<p>Description: In the second level of experiments, we statistically analyze the results of Human-as-a-Judge evaluations, focusing on the costs of human labor and potential biases, highlighting the challenges of relying on human evaluation for complex tasks.</p>
<ul>
<li>Step 6: Agent-as-a-Judge Implementation</li>
</ul>
<p>Description: We design and implement the Agent-as-a-Judge proof-of-concept to evaluate code generation on the DevAI dataset. This system incorporates modules such as graph, search, read, and ask, providing multi-dimensional evaluation metrics.</p>
<ul>
<li>Step 7: Comparing AI Judge Systems (Experiment Level 3)</li>
</ul>
<p>Description: In the third level of experiments, we compare three judgment systems: Agent-as-a-Judge, LLM-as-a-Judge, and Human-as-a-Judge, all applied to the same DevAI tasks. Our results show that Agent-as-a-Judge performs comparably to human evaluators and surpasses LLM-as-a-Judge in more complex reasoning and evaluation tasks.</p>
<h2>Future Directions</h2>
<ul>
<li>Direction 1: Intermediate Feedback for Agentic Self-Improvement</li>
</ul>
<p>Description: Agent-as-a-Judge offers intermediate feedback, crucial for reinforcement learning, where rewards are sparse but vital for improvement. It also enables real-time issue identification and resolution in complex, multi-stage tasks, overcoming the limitations of delayed feedback.</p>
<ul>
<li>Direction 2: Flywheel Effect Driven by Agent-as-a-Judge</li>
</ul>
<p>Description: The iterative feedback cycle between the Agent-as-a-Judge and evaluated agents (such as Developer Agents here) could create a flywheel effect, where mutual improvements lead to progressively greater optimization. This dynamic could drive an agentic self-play system and complement LLM reasoning data to embed agentic features into foundation models.</p>
<p>Figure 8 We Outline the Logical Flow of the Agent-as-a-Judge Framework.</p>
<h1>B Experiment Designs</h1>
<p>This section outlines the experimental designs aimed at evaluating developer agents' performance, analyzing human evaluations, and comparing AI-based judging systems. The experiments are structured across three levels, as illustrated below.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<h2>B. 1 Summary of Experiments</h2>
<p>The experiments are categorized into three levels as follows:</p>
<h2>Level 1</h2>
<p>Experiment 1a: Basic performance statistics for developer agents (Section 2.3)
Experiment 1b: Human evaluations of developer agents (Section 3.1)</p>
<h2>Level 2</h2>
<p>Experiment 2a: Error analysis of human evaluations (Section 3.2)</p>
<h2>Level 3</h2>
<p>Experiment 3a: AI judge baselines (Section 4.2)
Experiment 3b: Ablation studies for Agent-as-a-Judge (Section 4.3)</p>
<h2>B. 2 Judges and Subjects of Evaluation</h2>
<p>The following table summarizes the judge and the subject being evaluated in each experiment:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Experiment</th>
<th style="text-align: center;">Who is the Judge?</th>
<th style="text-align: center;">Who is being Judged?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Section 2.3</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Developer Agents</td>
</tr>
<tr>
<td style="text-align: center;">Section 3.1</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Developer Agents</td>
</tr>
<tr>
<td style="text-align: center;">Section 3.2</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: center;">Section 4.2</td>
<td style="text-align: center;">(1) LLM-as-a-Judge</td>
<td style="text-align: center;">(1) Developer Agents</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(2) Agent-as-a-Judge</td>
<td style="text-align: center;">(2) Developer Agents</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(3) Human</td>
<td style="text-align: center;">(3) LLM-as-a-Judge</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(4) Human</td>
<td style="text-align: center;">(4) Agent-as-a-Judge</td>
</tr>
<tr>
<td style="text-align: center;">Section 4.3</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Agent-as-a-Judge</td>
</tr>
</tbody>
</table>
<h1>C Agent-as-a-Judge Pipeline</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9 The pipelines of developer agents and judge agent. Some materials in this figure are from original blog (https://www.factsmachine.ai/p/hidden-in-plain-sight).</p>
<h1>D Extend Related Work</h1>
<p>Our main paper includes mostly related works of AI developers, Benchmarks for AI developments, and AI judges. However, the following works contribute significantly to the community and also relate to this work. We record this work as additional related work.</p>
<p>LLM-based Autonomous Agents Recent developments in LLM-based agents have expanded their capabilities beyond simple task execution to more autonomous problem-solving and decision-making. AutoGPT (Gravitas, 2023) and LangChain (Chase, 2022) provide frameworks for single-agent systems that leverage external tools for more complex tasks. Similarly, research such as CAMEL (Li et al., 2023), MetaGPT (Hong et al., 2024b), NLSOM (Zhuge et al., 2023), AutoGen (Wu et al., 2023) focus on role-based multi-agent communication, improving collaboration among agents. However, the challenge of maintaining coherence in agents' dialogue and preventing hallucination remains prominent (Du et al., 2024; Zhou et al., 2023c). Besides, Agent-trust (Xie et al., 2024) examines if LLM agents, like GPT-4, can simulate human trust behaviors in Trust Games, showing that they can align with human behavior.</p>
<p>Most recently, using graphs to build agents has gained prominence. Earlier work like GPTSwarm (Zhuge et al., 2024) and LangGraph (langchain ai, 2024) proposed using nodes to represent operations and edges to represent the connections between them. In GPTSwarm, multiple agents represented as subgraphs in a graph are connected by optimizable edges, and reinforcement learning is employed to optimize the edges. Following this approach, several agent frameworks have incorporated graphs into their designs (Hong et al., 2024a; Zhou et al., 2024; Qian et al., 2024b). Additionally, various optimization methods have been developed to enhance agent performance further (Wu et al., 2024a; Song et al., 2024; Hu et al., 2024). In practical applications, many studies focus on understanding and interacting with GUIs (Wang et al., 2024a; Chen et al., 2024c; Yang et al., 2023; Xu et al., 2024; Tan et al., 2024; Wu et al., 2024b). For code generation agents (Jin et al., 2024), current research mainly emphasizes automated repair (Yang et al., 2024a; Phan et al., 2024; Tao et al., 2024), computational modular design (Khattab et al., 2023; Cheng et al., 2024), and automated development (Tufano et al., 2024; Huang et al., 2023). Among these, open-sourced frameworks like OpenHands (Wang et al., 2024d) have gained popularity due to their strong user experience. Moreover, scientific discovery (Jansen et al., 2024; Lu et al., 2024a) and ML agents (Yang et al., 2024b) are also receiving increased attention.</p>
<p>LLM-as-a-Judge In the domain of AI evaluation and judgment, frameworks (Zheng et al., 2024; Fu et al., 2023) have pioneered the use of LLMs to assess conversational agents, demonstrating how LLMs can evaluate dialogue quality and consistency. LLM-as-a-judge has also expanded into the multimodal domain, providing clear visual-language feedback (Chen et al., 2024b; Xiong et al., 2024). ICE-Score (Zhuo, 2023) improves on metrics like CodeBERTScore (Zhou et al., 2023a) and G-EVAL Liu et al. (2023c) by better aligning with human preferences and functional correctness. Expanding beyond dialogue, LLMs like CodeR (Chen et al., 2024a) and MASAI (Arora et al., 2024) apply similar judging principles to the code validation process, where AI systems autonomously evaluate and verify computer programs. Our work builds on these advancements by exploring how LLMs can perform more nuanced judgment tasks, further investigating their potential in decision-making across various domains. Recent research also focuses on judging LLM-as-a-Judges (Chen et al., 2024d; Bavaresco et al., 2024; Thakur et al., 2024; Dong et al., 2024; Shi et al., 2024; Raina et al., 2024).</p>
<p>Coding Benchmarks Recent advances in code generation have led to the innovation of various benchmarks to evaluate model performance (Liu et al., 2024). Early benchmarks, such as MBPP (Austin et al., 2021), HumanEval (Chen et al., 2021), and MultiPL-E (Cassano et al., 2023), focus primarily on generating simple functions. While these benchmarks are useful for evaluating the correctness of generated code, they are limited in complexity and do not fully represent the challenges encountered in real-world software development.</p>
<p>As the field progressed, newer benchmarks began to focus on more complex and realistic tasks. APPS (Hendrycks et al., 2021), CodeContests (Li et al., 2022), and LiveCodeBench (Jain et al., 2024) moved toward competitive programming challenges that involve advanced algorithms and data structures. These tasks are more representative of problems encountered in coding competitions and help push models toward more sophisticated problem-solving. DS-1000 (Lai et al., 2023) was introduced to assess the skills of models with data science libraries, evaluating their ability to use APIs and execute complex data analysis workflows.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Additionally, we were pleased to find that a recent industry blog (https://www.cognition.ai/blog/evaluating-coding-ag ents), published 3 weeks before our submission, shares very similar ideas and provides further evidence that the Agent-as-a-Judge could have practical applications in agent systems.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
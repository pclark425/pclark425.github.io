<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5799 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5799</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5799</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-ce913026f693101e54d3ab9152e107034d81fce1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ce913026f693101e54d3ab9152e107034d81fce1" target="_blank">Holistic Evaluation of Language Models</a></p>
                <p><strong>Paper Venue:</strong> Trans. Mach. Learn. Res.</p>
                <p><strong>Paper TL;DR:</strong> HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https://crfm.stanford.edu/helm/latest/.</p>
                <p><strong>Paper Abstract:</strong> Language models (LMs) like GPT‐3, PaLM, and ChatGPT are the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of LMs. LMs can serve many purposes and their behavior should satisfy many desiderata. To navigate the vast space of potential scenarios and metrics, we taxonomize the space and select representative subsets. We evaluate models on 16 core scenarios and 7 metrics, exposing important trade‐offs. We supplement our core evaluation with seven targeted evaluations to deeply analyze specific aspects (including world knowledge, reasoning, regurgitation of copyrighted content, and generation of disinformation). We benchmark 30 LMs, from OpenAI, Microsoft, Google, Meta, Cohere, AI21 Labs, and others. Prior to HELM, models were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: all 30 models are now benchmarked under the same standardized conditions. Our evaluation surfaces 25 top‐level findings. For full transparency, we release all raw model prompts and completions publicly. HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https://crfm.stanford.edu/helm/latest/.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5799.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5799.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC-adapt-separate-vs-joint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple-choice adaptation: separate vs joint prompting strategies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>HELM reports that the way multiple-choice problems are presented (each choice in a separate prompt vs. all choices jointly in one prompt with in‑context examples) can cause very large changes in model accuracy, and that the best adaptation format can differ across models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HellaSwag (multiple-choice commonsense reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Choose the most plausible continuation/ending from multiple candidate answers for a commonsense inference task.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Two adaptation strategies: (a) separate strategy — each answer choice presented in its own 0‑shot prompt (one prompt per candidate); (b) joint (exam-style) strategy — all answer choices presented together in a single prompt with 5 in‑context examples (5‑shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Separate 0‑shot per-choice presentation vs. single 5‑shot joint multiple‑choice prompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 79.1% (separate, 0‑shot per‑choice); accuracy: 30.2% (joint, single 5‑shot multiple‑choice prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>See performance field (79.1% vs 30.2%)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+48.9 percentage points (separate vs joint for OPT on HellaSwag)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>separate presentation dramatically improved measured accuracy relative to the joint multiple-choice prompt</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>HELM notes that model behavior differs strongly depending on how multiple‑choice tasks are encoded into prompts; some models appear to favor binary/probabilistic judgments when each candidate is scored separately, whereas joint presentation with in‑context examples can induce near‑random behavior for some models — implying that prompt structure changes the task the model effectively solves.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Holistic Evaluation of Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5799.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5799.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>prompt-format-sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt formatting and in-context example sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>HELM finds that nearly all evaluated models are highly sensitive to prompt wording, formatting, choice of in‑context examples, and the number of examples, producing large swings in performance across scenarios and metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (all 30 evaluated models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Core scenarios (multiple tasks; aggregated prompting analysis in §8.2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A diverse set of user-facing tasks (question answering, summarization, retrieval, classification, etc.) evaluated under controlled prompting ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>HELM standardized on relatively simple, generic 5‑shot prompts for head‑to‑head comparisons but also performed prompting ablations varying prompt formatting, the specific in‑context exemplars, and number of examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standardized generic 5‑shot prompt (used for cross‑model comparisons) vs. alternative prompt formats (different wording, formatting, exemplar selection, and different numbers of in‑context examples); the paper also compares per‑task adaptation methods (e.g., joint vs separate MC encoding).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varies by model and task; HELM reports very large observed variation in accuracy due to prompting choices (examples include swings from ~30% to ~80% accuracy for the same model and scenario reported in discussions and citations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>See comparisons across multiple ablations in §8.2; no single numeric summary — changes in prompt formatting and example selection can both increase or decrease accuracy substantially depending on model and task.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Paper cites observed swings of up to ~50 percentage points in accuracy for the same (model, scenario) depending on prompt choices (example range 30% to 80% cited).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varied (prompt formatting can either improve or reduce performance, depending on the specific change and the model)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>HELM hypothesizes that different prompts change the implicit task the model solves (e.g., framing, answer space, or probability normalization), and that models differ in which prompting decisions maximize accuracy; prompt sensitivity implies that benchmarking must control prompt format to enable fair comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Holistic Evaluation of Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5799.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5799.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>standardized-5shot-adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standardized 5‑shot few‑shot prompting for cross‑model comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>To enable fair head‑to‑head comparisons, HELM adapts all evaluated models using a standardized, simple 5‑shot prompting procedure, while noting that some models might perform better under different adaptation regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>all evaluated models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>All core scenarios (16 core scenarios measured across models)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of user‑facing tasks where models are adapted via few‑shot in‑context prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Standardized 5‑shot prompts: for each instance a prompt includes 5 in‑context examples plus the test input, with simple/generic prompt wording to reflect a generic language interface.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Not a direct comparison in a single numeric experiment in the paper, but HELM notes alternative formats (0‑shot, other numbers of shots, chain‑of‑thought, prompt‑tuning) may yield different results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varies by model and task; HELM uses the standardized 5‑shot results as the primary basis for cross‑model rankings (e.g., text‑davinci‑002 ranks top on many core tasks under this 5‑shot evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>enables standardized comparison but not necessarily optimal for every model — some models could be substantially better with different adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Standardizing on a simple 5‑shot prompt reduces variability across evaluation conditions and reflects the goal of assessing models as generic language interfaces; however, the paper emphasizes that more sophisticated prompting (chain‑of‑thought, prompt tuning) could produce qualitatively different outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Holistic Evaluation of Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5799.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5799.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>instruction_tuning_advantage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction‑tuning improves prompt responsiveness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>HELM finds that instruction‑tuned models (notably text‑davinci‑002 and Anthropic‑LM v4‑s3) perform especially well under prompting across accuracy, robustness, and fairness metrics, suggesting instruction‑tuning changes how format/prompting affects model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Core scenarios (aggregate across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation across 16 core scenarios and multiple metrics (accuracy, robustness, fairness, etc.) using the standardized adaptation procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instruction‑tuned model evaluated with the standardized few‑shot prompting; instruction‑tuning refers to additional fine‑tuning using human instructions/feedback so the model better follows prompt intents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared implicitly against non‑instruction‑tuned models evaluated under the same prompting protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Ranks best across accuracy, robustness, and fairness on HELM's core scenarios (aggregate top performer among 30 models for these metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Instruction‑tuned text‑davinci‑002 and Anthropic‑LM v4‑s3 outperform many much larger non‑instruction‑tuned models under the same prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (instruction‑tuning generally improved performance under prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Instruction‑tuning appears to make models more responsive and robust to natural language prompts, so the same prompt formatting yields better downstream behavior for instruction‑tuned models compared to non‑instruction‑tuned ones.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Holistic Evaluation of Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5799.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5799.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IR-pointwise-binary-format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pointwise binary (Yes/No) prompt formulation for passage re‑ranking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For information retrieval re‑ranking (MS MARCO), HELM adapts LMs to the task by prompting the model per candidate passage with a binary Yes/No question about whether the passage answers the query, requiring well‑calibrated probabilities; this format is computationally intensive but can yield strong retrieval performance in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (evaluated LM suite)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MS MARCO passage ranking (re‑ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a query and a set of candidate passages, rank passages by relevance to the query.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Pointwise formulation: for each (query, passage) pair, present the passage and ask the LM a binary question (e.g., 'Does this passage contain an answer to the query?') and use the model's probability (Yes vs No) to score and rank passages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared conceptually against classical retrieval (BM25) and fine‑tuned neural retrievers; HELM also notes alternative LM formulations could be used (e.g., listwise prompting), but the paper uses the pointwise binary approach.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>HELM reports that the best LMs using this pointwise prompt can outperform classical retrieval methods and, in some settings, be comparable to fine‑tuned neural retrievers, though they still trail state‑of‑the‑art specialized systems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved relative to classical retrieval in best cases; however, computational cost is high</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The pointwise binary prompt forces the LM to produce calibrated probabilities for relevance, enabling ranking, but the naive per‑passage approach is computationally intensive and sensitive to calibration/decoding choices.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Holistic Evaluation of Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5799.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5799.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>format-depends-on-model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model‑dependent optimal prompting/adaptation formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>HELM emphasizes that the adaptation format that maximizes accuracy (prompt wording, exemplar selection, shot count, input arrangement) can differ across models and even produce qualitatively different outcomes for the same scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (across model families)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Representative core scenarios (e.g., multiple choice, QA, classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where adaptation via prompting can be varied (MC encoding strategies, exemplar choice, shot count, formatting).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>HELM studies multiple adaptation variants (joint vs separate MC encoding, different numbers of in‑context exemplars, exemplar selection strategies, prompt wording/formatting) and documents per‑model differences in what works best.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Multiple alternative prompt/adaptation formats compared in ablation studies (see §8.2 and Figure 33); no single format universally optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Performance differs by model and format; HELM documents cases where a format that works well for one model performs poorly for another (e.g., separate vs joint MC behavior differs across models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varied (format can improve or reduce perf depending on model)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Differences likely arise because model architectures, pretraining data, and any instruction‑tuning or fine‑tuning influence how models interpret prompts and context; hence standardization is necessary for fair comparisons but may understate model potential under alternative adaptation choices.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Holistic Evaluation of Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>The Power of Scale for Parameter‑Efficient Prompt Tuning <em>(Rating: 2)</em></li>
                <li>Prefix‑Tuning: Optimizing Continuous Prompts for Generation <em>(Rating: 1)</em></li>
                <li>On the Influence of Demonstration Examples in In‑Context Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5799",
    "paper_id": "paper-ce913026f693101e54d3ab9152e107034d81fce1",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "MC-adapt-separate-vs-joint",
            "name_full": "Multiple-choice adaptation: separate vs joint prompting strategies",
            "brief_description": "HELM reports that the way multiple-choice problems are presented (each choice in a separate prompt vs. all choices jointly in one prompt with in‑context examples) can cause very large changes in model accuracy, and that the best adaptation format can differ across models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPT",
            "model_size": "175B",
            "task_name": "HellaSwag (multiple-choice commonsense reasoning)",
            "task_description": "Choose the most plausible continuation/ending from multiple candidate answers for a commonsense inference task.",
            "problem_format": "Two adaptation strategies: (a) separate strategy — each answer choice presented in its own 0‑shot prompt (one prompt per candidate); (b) joint (exam-style) strategy — all answer choices presented together in a single prompt with 5 in‑context examples (5‑shot).",
            "comparison_format": "Separate 0‑shot per-choice presentation vs. single 5‑shot joint multiple‑choice prompt",
            "performance": "accuracy: 79.1% (separate, 0‑shot per‑choice); accuracy: 30.2% (joint, single 5‑shot multiple‑choice prompt)",
            "performance_comparison": "See performance field (79.1% vs 30.2%)",
            "format_effect_size": "+48.9 percentage points (separate vs joint for OPT on HellaSwag)",
            "format_effect_direction": "separate presentation dramatically improved measured accuracy relative to the joint multiple-choice prompt",
            "explanation_or_hypothesis": "HELM notes that model behavior differs strongly depending on how multiple‑choice tasks are encoded into prompts; some models appear to favor binary/probabilistic judgments when each candidate is scored separately, whereas joint presentation with in‑context examples can induce near‑random behavior for some models — implying that prompt structure changes the task the model effectively solves.",
            "counterexample_or_null_result": null,
            "uuid": "e5799.0",
            "source_info": {
                "paper_title": "Holistic Evaluation of Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "prompt-format-sensitivity",
            "name_full": "Prompt formatting and in-context example sensitivity",
            "brief_description": "HELM finds that nearly all evaluated models are highly sensitive to prompt wording, formatting, choice of in‑context examples, and the number of examples, producing large swings in performance across scenarios and metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (all 30 evaluated models)",
            "model_size": null,
            "task_name": "Core scenarios (multiple tasks; aggregated prompting analysis in §8.2)",
            "task_description": "A diverse set of user-facing tasks (question answering, summarization, retrieval, classification, etc.) evaluated under controlled prompting ablations.",
            "problem_format": "HELM standardized on relatively simple, generic 5‑shot prompts for head‑to‑head comparisons but also performed prompting ablations varying prompt formatting, the specific in‑context exemplars, and number of examples.",
            "comparison_format": "Standardized generic 5‑shot prompt (used for cross‑model comparisons) vs. alternative prompt formats (different wording, formatting, exemplar selection, and different numbers of in‑context examples); the paper also compares per‑task adaptation methods (e.g., joint vs separate MC encoding).",
            "performance": "Varies by model and task; HELM reports very large observed variation in accuracy due to prompting choices (examples include swings from ~30% to ~80% accuracy for the same model and scenario reported in discussions and citations).",
            "performance_comparison": "See comparisons across multiple ablations in §8.2; no single numeric summary — changes in prompt formatting and example selection can both increase or decrease accuracy substantially depending on model and task.",
            "format_effect_size": "Paper cites observed swings of up to ~50 percentage points in accuracy for the same (model, scenario) depending on prompt choices (example range 30% to 80% cited).",
            "format_effect_direction": "varied (prompt formatting can either improve or reduce performance, depending on the specific change and the model)",
            "explanation_or_hypothesis": "HELM hypothesizes that different prompts change the implicit task the model solves (e.g., framing, answer space, or probability normalization), and that models differ in which prompting decisions maximize accuracy; prompt sensitivity implies that benchmarking must control prompt format to enable fair comparisons.",
            "counterexample_or_null_result": null,
            "uuid": "e5799.1",
            "source_info": {
                "paper_title": "Holistic Evaluation of Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "standardized-5shot-adaptation",
            "name_full": "Standardized 5‑shot few‑shot prompting for cross‑model comparison",
            "brief_description": "To enable fair head‑to‑head comparisons, HELM adapts all evaluated models using a standardized, simple 5‑shot prompting procedure, while noting that some models might perform better under different adaptation regimes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "all evaluated models",
            "model_size": null,
            "task_name": "All core scenarios (16 core scenarios measured across models)",
            "task_description": "A suite of user‑facing tasks where models are adapted via few‑shot in‑context prompting.",
            "problem_format": "Standardized 5‑shot prompts: for each instance a prompt includes 5 in‑context examples plus the test input, with simple/generic prompt wording to reflect a generic language interface.",
            "comparison_format": "Not a direct comparison in a single numeric experiment in the paper, but HELM notes alternative formats (0‑shot, other numbers of shots, chain‑of‑thought, prompt‑tuning) may yield different results.",
            "performance": "Varies by model and task; HELM uses the standardized 5‑shot results as the primary basis for cross‑model rankings (e.g., text‑davinci‑002 ranks top on many core tasks under this 5‑shot evaluation).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "enables standardized comparison but not necessarily optimal for every model — some models could be substantially better with different adaptation",
            "explanation_or_hypothesis": "Standardizing on a simple 5‑shot prompt reduces variability across evaluation conditions and reflects the goal of assessing models as generic language interfaces; however, the paper emphasizes that more sophisticated prompting (chain‑of‑thought, prompt tuning) could produce qualitatively different outcomes.",
            "counterexample_or_null_result": null,
            "uuid": "e5799.2",
            "source_info": {
                "paper_title": "Holistic Evaluation of Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "instruction_tuning_advantage",
            "name_full": "Instruction‑tuning improves prompt responsiveness",
            "brief_description": "HELM finds that instruction‑tuned models (notably text‑davinci‑002 and Anthropic‑LM v4‑s3) perform especially well under prompting across accuracy, robustness, and fairness metrics, suggesting instruction‑tuning changes how format/prompting affects model outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_size": null,
            "task_name": "Core scenarios (aggregate across tasks)",
            "task_description": "Evaluation across 16 core scenarios and multiple metrics (accuracy, robustness, fairness, etc.) using the standardized adaptation procedure.",
            "problem_format": "Instruction‑tuned model evaluated with the standardized few‑shot prompting; instruction‑tuning refers to additional fine‑tuning using human instructions/feedback so the model better follows prompt intents.",
            "comparison_format": "Compared implicitly against non‑instruction‑tuned models evaluated under the same prompting protocol.",
            "performance": "Ranks best across accuracy, robustness, and fairness on HELM's core scenarios (aggregate top performer among 30 models for these metrics).",
            "performance_comparison": "Instruction‑tuned text‑davinci‑002 and Anthropic‑LM v4‑s3 outperform many much larger non‑instruction‑tuned models under the same prompt formats.",
            "format_effect_size": null,
            "format_effect_direction": "improved (instruction‑tuning generally improved performance under prompting)",
            "explanation_or_hypothesis": "Instruction‑tuning appears to make models more responsive and robust to natural language prompts, so the same prompt formatting yields better downstream behavior for instruction‑tuned models compared to non‑instruction‑tuned ones.",
            "counterexample_or_null_result": null,
            "uuid": "e5799.3",
            "source_info": {
                "paper_title": "Holistic Evaluation of Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "IR-pointwise-binary-format",
            "name_full": "Pointwise binary (Yes/No) prompt formulation for passage re‑ranking",
            "brief_description": "For information retrieval re‑ranking (MS MARCO), HELM adapts LMs to the task by prompting the model per candidate passage with a binary Yes/No question about whether the passage answers the query, requiring well‑calibrated probabilities; this format is computationally intensive but can yield strong retrieval performance in some settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (evaluated LM suite)",
            "model_size": null,
            "task_name": "MS MARCO passage ranking (re‑ranking)",
            "task_description": "Given a query and a set of candidate passages, rank passages by relevance to the query.",
            "problem_format": "Pointwise formulation: for each (query, passage) pair, present the passage and ask the LM a binary question (e.g., 'Does this passage contain an answer to the query?') and use the model's probability (Yes vs No) to score and rank passages.",
            "comparison_format": "Compared conceptually against classical retrieval (BM25) and fine‑tuned neural retrievers; HELM also notes alternative LM formulations could be used (e.g., listwise prompting), but the paper uses the pointwise binary approach.",
            "performance": "HELM reports that the best LMs using this pointwise prompt can outperform classical retrieval methods and, in some settings, be comparable to fine‑tuned neural retrievers, though they still trail state‑of‑the‑art specialized systems.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved relative to classical retrieval in best cases; however, computational cost is high",
            "explanation_or_hypothesis": "The pointwise binary prompt forces the LM to produce calibrated probabilities for relevance, enabling ranking, but the naive per‑passage approach is computationally intensive and sensitive to calibration/decoding choices.",
            "counterexample_or_null_result": null,
            "uuid": "e5799.4",
            "source_info": {
                "paper_title": "Holistic Evaluation of Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "format-depends-on-model",
            "name_full": "Model‑dependent optimal prompting/adaptation formats",
            "brief_description": "HELM emphasizes that the adaptation format that maximizes accuracy (prompt wording, exemplar selection, shot count, input arrangement) can differ across models and even produce qualitatively different outcomes for the same scenario.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (across model families)",
            "model_size": null,
            "task_name": "Representative core scenarios (e.g., multiple choice, QA, classification)",
            "task_description": "Tasks where adaptation via prompting can be varied (MC encoding strategies, exemplar choice, shot count, formatting).",
            "problem_format": "HELM studies multiple adaptation variants (joint vs separate MC encoding, different numbers of in‑context exemplars, exemplar selection strategies, prompt wording/formatting) and documents per‑model differences in what works best.",
            "comparison_format": "Multiple alternative prompt/adaptation formats compared in ablation studies (see §8.2 and Figure 33); no single format universally optimal.",
            "performance": "Performance differs by model and format; HELM documents cases where a format that works well for one model performs poorly for another (e.g., separate vs joint MC behavior differs across models).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "varied (format can improve or reduce perf depending on model)",
            "explanation_or_hypothesis": "Differences likely arise because model architectures, pretraining data, and any instruction‑tuning or fine‑tuning influence how models interpret prompts and context; hence standardization is necessary for fair comparisons but may understate model potential under alternative adaptation choices.",
            "counterexample_or_null_result": null,
            "uuid": "e5799.5",
            "source_info": {
                "paper_title": "Holistic Evaluation of Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "The Power of Scale for Parameter‑Efficient Prompt Tuning",
            "rating": 2
        },
        {
            "paper_title": "Prefix‑Tuning: Optimizing Continuous Prompts for Generation",
            "rating": 1
        },
        {
            "paper_title": "On the Influence of Demonstration Examples in In‑Context Learning",
            "rating": 1
        }
    ],
    "cost": 0.015689,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Holistic Evaluation of Language Models</h1>
<p>Percy Liang ${ }^{\dagger}$, Rishi Bommasani ${ }^{\dagger}$, Tony Lee ${ }^{\dagger}$, Dimitris Tsipras ${ }^{\ddagger}$, Dilara Soylu ${ }^{\ddagger}$, Michihiro Yasunaga ${ }^{\ddagger}$, Yian Zhang ${ }^{\ddagger}$, Deepak Narayanan ${ }^{\ddagger}$, Yuhuai Wu ${ }^{\ddagger}$, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda pliang@cs.stanford.edu, nlprishi@stanford.edu, tonyhlee@stanford.edu Center for Research on Foundation Models (CRFM) Institute for Human-Centered Artificial Intelligence (HAI) Stanford University</p>
<p>Reviewed on OpenReview: https://openreview. net/forum?id=iO4LZibEqW</p>
<h4>Abstract</h4>
<p>Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios to the extent possible ( $87.5 \%$ of the time), ensuring that metrics beyond accuracy don't fall to the wayside, and that trade-offs across models and metrics are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to more deeply analyze specific aspects (e.g. knowledge, reasoning, memorization/copyright, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, including 21 scenarios that were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just $17.9 \%$ of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to $96.0 \%$ : now all 30 models have been densely benchmarked on a set of core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings concerning the interplay between different scenarios, metrics, and models. For full transparency, we release all raw model prompts and completions publicly ${ }^{1}$ for further analysis, as well as a general modular toolkit for easily adding new scenarios, models, metrics, and prompting strategies. ${ }^{2}$ We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>A helm is a $\Longrightarrow$ Language Model $\Longrightarrow$ wheel for steering a ship...</p>
<p>Figure 1: Language model. A language model takes text (a prompt) and generates text (a completion) probabilistically. Despite their simple interface, language models can be adapted to a wide range of language tasks from question answering to summarization.</p>
<h1>1 Introduction</h1>
<p>Benchmarks orient AI. They encode values and priorities (Ethayarajh \&amp; Jurafsky, 2020; Birhane et al., 2022) that specify directions for the AI community to improve upon (Spärck Jones \&amp; Galliers, 1995; Spärck Jones, 2005; Kiela et al., 2021; Bowman \&amp; Dahl, 2021; Raji et al., 2021). When implemented and interpreted appropriately, they enable the broader community to better understand AI technology and influence its trajectory.</p>
<p>In recent years, the AI technology that has arguably advanced the most is foundation models (Bommasani et al., 2021), headlined by the rise of language models (LMs; Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022). At its core, a language model is a box that takes in text and generates text (Figure 1). Despite their simplicity, when these models are trained on broad data at immense scale, they can be adapted (e.g. prompted or fine-tuned) to myriad downstream scenarios. Yet the immense surface of model capabilities, limitations, and risks remains poorly understood. The rapid development, rising impact, and inadequate understanding demand that we benchmark language models holistically.</p>
<p>But what does it mean to benchmark language models holistically? Language models are general-purpose text interfaces that could be applied across a vast expanse of scenarios. And for each scenario, we may have a broad set of desiderata: models should be accurate, robust, fair, efficient, and so on. In fact, the relative importance of these desiderata often will depend not only on the perspective and values one has, but the scenario itself (e.g. inference efficiency might be of greater importance in mobile applications).</p>
<p>We believe holistic evaluation involves three elements:</p>
<ol>
<li>Broad coverage and recognition of incompleteness. Given language models' vast surface of capabilities and risks, we need to evaluate language models over a broad range of scenarios. Broadening the evaluation has been a continuing trend in the NLP community, going from individual datasets such as SQuAD (Rajpurkar et al., 2016) to small collections of datasets such as SuperGLUE (Wang et al., 2019b) to large collections of datasets such as the GPT-3 evaluation suite (Brown et al., 2020), Eleuther AI LM Harness (Gao et al., 2021b), and BIG-Bench (Srivastava et al., 2022). However, it is neither possible to consider all the scenarios nor all the desiderata that (could) pertain to LMs. Therefore, holistic evaluation should provide a top-down taxonomy and make explicit all the major scenarios and metrics that are missing.</li>
<li>Multi-metric measurement. Societally beneficial systems reflect many values, not just accuracy. Holistic evaluation should represent these plural desiderata, evaluating every desideratum for each scenario considered.</li>
<li>Standardization. Our object of evaluation is the language model, not a scenario-specific system. Therefore, in order to meaningfully compare different LMs, the strategy for adapting an LM to a scenario should be controlled for. Furthermore, each LM should be evaluated on the same scenarios to the extent possible.</li>
</ol>
<p>Overall, holistic evaluation builds transparency by assessing language models in their totality. Rather than honing in on a specific aspect, we strive for a fuller characterization of language models to improve scientific understanding and orient societal impact.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The importance of the taxonomy to HELM. Previous language model benchmarks (e.g. SuperGLUE, EleutherAI LM Evaluation Harness, BIG-Bench) are collections of datasets, each with a standard task framing and canonical metric, usually accuracy (left). In comparison, in HELM we take a top-down approach of first explicitly stating what we want to evaluate (i.e. scenarios and metrics) by working through their underlying structure. Given this stated taxonomy, we make deliberate decisions on what subset we implement and evaluate, which makes explicit what we miss (e.g. coverage of languages beyond English).</p>
<h1>1.1 HELM</h1>
<p>Holistic Evaluation of Language Models (HELM) has two levels: (i) an abstract taxonomy of scenarios and metrics to define the design space for language model evaluation and (ii) a concrete set of implemented scenarios and metrics that were selected to prioritize coverage (e.g. different English varieties), value (e.g. user-facing applications), and feasibility (e.g. limited engineering resources).</p>
<p>Recognition of incompleteness. Benchmarks across AI, including those for language models like SuperGLUE (Wang et al., 2019a), the EleutherAI LM Harness (Gao et al., 2021b), and BIG-bench (Srivastava et al., 2022), are defined by specific choices of scenarios and metrics. Different benchmarks make different decisions on what to prioritize, how to make these decisions, and to what extent these processes are made clear in presenting the benchmark. Since our aim is holistic evaluation, we believe it is necessary to be explicit on the relationship between what we aspire to evaluate and what we actually evaluate. The construction of HELM starts top-down with a taxonomy over scenarios and metrics (see Figure 2). The taxonomy not only facilitates the systematic selection of scenarios and metrics, but it also make explicit what is missing. We view HELM as a living benchmark, and we hope that both the abstract taxonomy and the concrete selection of scenarios and metrics will evolve according to the technology, applications, and social concerns. In §10: missing, we explicitly highlight evaluations HELM lacks that should be prioritized. Often these are ones the entire AI field has historically neglected.</p>
<p>Multi-metric measurement. HELM currently implements a core ${ }^{3}$ set of 16 scenarios and 7 (categories of) metrics. Our scenarios, which are triples of (task, domain, language), span 6 user-facing tasks (e.g. question answering, information retrieval, summarization, toxicity detection), several domains (e.g. news, books), and currently only English (though we cover several English varieties such as African-American English and the English varieties spoken in different English-speaking countries). And our 7 categories of metrics reflect a range of societal considerations (i.e. accuracy, calibration, robustness, fairness, bias, toxicity, efficiency). We emphasize that while we have specific quantitative metrics for all of these considerations, they (e.g. fairness) are complex and contested social constructs that can be operationalized in many different ways. Consistent with our second element of holistic evaluation, we ensure our benchmark attains dense multi-metric measurement: of the 112 possible (core scenario, metric) pairs, we measure 98 ( $87.5 \%$ ) as shown in Table 4.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Many metrics for each use case. In comparison to most prior benchmarks of language technologies, which primarily center accuracy and often relegate other desiderata to their own bespoke datasets (if at all), in HELM we take a multi-metric approach. This foregrounds metrics beyond accuracy and allows one to study the tradeoffs between the metrics.</p>
<p>This multi-metric perspective conveys a position we take on evaluation practices in AI. While most benchmarks primarily foreground accuracy, perhaps deferring the evaluation of other metrics (e.g. the extent to which models generate toxic content) to separate scenarios (e.g. RealToxicityPrompts), we believe it is integral that all of these metrics be evaluated in the same contexts where we expect to deploy models (see Figure 3). In particular, measuring these 7 desiderata for the same scenarios makes explicit potential tradeoffs and helps to ensure these desiderata are not treated as second-class citizens to accuracy (see Friedman \&amp; Nissenbaum, 1996).</p>
<p>Targeted evaluations. In addition to our core set of 16 scenarios, where for each scenario we measure all 7 categories of metrics, HELM has 7 targeted evaluations through 26 additional scenarios and accompanying metrics. These evaluations target linguistic understanding, world and commonsense knowledge, reasoning capabilities, memorization and copyright, disinformation generation, biases, and toxicity generation, providing a deeper dive beyond the core scenarios. This includes 21 scenarios that are either entirely new (e.g. WikiFact) or that have not been used in mainstream language model evaluation (e.g. ICE). While HELM is oriented by a holistic approach that foregrounds societal impact and is reflected in our multimetric perspective, evaluation can also pinpoint specific phenomena to advance scientific understanding (e.g. a model's ability to perform analogical reasoning; see Bommasani et al., 2021, §4.4). For this reason, to make our evaluation results more intelligible, we separate the core scenarios from the targeted evaluations: the core scenarios and multi-metric measurement provide an integrated lens on models, whereas the targeted evaluations isolate specific skills and risks.</p>
<p>Standardization. To build a shared understanding of existing language models, consistent with our third element of holistic evaluation, we benchmark 30 prominent language models on HELM. These models come from 12 organizations: AI21 Labs (e.g. J1-Jumbo v1 (178B)), Anthropic (Anthropic-LM v4-s3 (52B)), BigScience (e.g. BLOOM (176B)), Cohere (e.g. Cohere xlarge v20220609 (52.4B)), EleutherAI (e.g. GPTNeoX (20B)), Google (e.g. UL2 (20B)), Meta (e.g. OPT (175B)), Microsoft/NVIDIA (e.g. TNLG v2 (530B)), OpenAI (e.g. davinci (175B)), Tsinghua University (GLM (130B)), and Yandex (YaLM (100B)). Benchmarking these models is challenging given they vary in accessibility (see Liang et al., 2022): some are open (e.g. GPT-NeoX (20B)), some are limited-access (e.g. davinci (175B)), and some are closed (e.g. Anthropic-LM v4-s3 (52B)). In some cases, very little is known about how these models were built (e.g. the training data and its size are often not known), such as text-davinci-002. What we do know is that several of these models are deployed, either in external-facing commercial APIs (e.g. the OpenAI playground) or products (e.g. GitHub Copilot). That is, several of these models are having direct social impact at present. The absence of an evaluation standard compromises the community's ability to clearly and rigorously understand the overall landscape of language models. To demonstrate how uneven language model evaluation has been, we annotated the datasets used to evaluate more than 40 language models (i.e. all models evaluated in this work along with others like PaLM and Gopher) in Appendix F. We found major models such as T5 (11B) and Anthropic-LM v4-s3 (52B) were not evaluated on a single dataset in common in</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Standardizing language model evaluation. Prior to our effort (top), the evaluation of language models was uneven. Several of our 16 core scenarios had no models evaluated on them, and only a few scenarios (e.g. BoolQ, HellaSwag) had a considerable number of models evaluated on them. Note that this is cumulative: in the top plot, we not only document instances where the work introducing the model evaluated on a given scenario, but any subsequent work evaluated the model on the scenario (e.g. Tay et al. (2022a) in the paper on UL2 (20B) expanded the evaluation of T5 (11B) to include HellaSwag and several other datasets) under any conditions (e.g. fine-tuning, 0-shot prompting, 5-shot prompting). After our evaluation (bottom), models are now evaluated under the same conditions on many scenarios.
their original works (Raffel et al., 2019; Askell et al., 2021). In fact, several models (e.g. J1-Grande v1 (17B), Cohere xlarge v20220609 (52.4B), YaLM (100B)) do not report any public results prior to our effort (to our knowledge). And even for datasets that are frequently evaluated for across all 405 datasets evaluated in major language modeling works (e.g. HellaSwag; many of the datasets within GLUE and SuperGLUE), we find the evaluation conditions vary greatly. On HellaSwag, some prior work reports fine-tuned accuracies (e.g. T5 (11B)), whereas others report prompting accuracies (e.g. davinci (175B)). ${ }^{4}$ Even when works report results through few-shot prompting, the exact details can vary, which in $\S 8.2$ : Prompting-analysis we show leads to wild swings in accuracies (e.g. $30 \%$ to $80 \%$ for the same (model, scenario); see Zhao et al. (2021)).</p>
<p>In Figure 4, we make explicit how our evaluation changes the status quo. Previously, on average models were evaluated on $17.9 \%$ of our core scenarios, even after compiling evaluations dispersed across different prior works. We improve this to $96.0 \% .{ }^{5}$ By both evaluating these models on the same scenarios and by conducting the evaluation under standardized conditions (e.g. using the same few-shot prompting for all models), we facilitate direct head-to-head comparisons.</p>
<p>The importance of adaptation. To benchmark these models, we must specify an adaptation procedure that uses the general-purpose language model to tackle a given scenario (see Bommasani et al., 2021, §4.3). In this work, we adapt all language models through few-shot prompting, as pioneered by GPT-3 (Brown et al., 2020). Furthermore, we opted to choose relatively simple, generic prompts in order to orient the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>development of language models towards generic language interfaces that respond robustly to direct natural language, rather than requiring model-specific incantations. Certainly stronger results could be obtained from more sophisticated prompting (e.g. chain-of-thoughts; Wei et al., 2022c), prompt decomposition (Wu et al., 2022; Press et al., 2022; Arora et al., 2022), and prompt-tuning (Lester et al., 2021; Li \&amp; Liang, 2021), potentially leading to qualitatively different findings (Suzgun et al., 2022). The exploration of adaptation strategies is another dimension of benchmarking which we leave to future work.</p>
<p>Caveats and considerations. Before presenting our empirical findings, we highlight three key considerations. First, while we standardize model evaluation, in particular by evaluating all models for the same scenarios, same metrics, and with the same prompts for 5 -shot prompting, models themselves may be more suitable for particular scenarios, particular metrics, and particular prompts/adaptation methods. To be explicit, while some models may perform poorly under our evaluation, they may perform well in other contexts. Second, while the evaluation itself may be standardized, the computational resources required to train these models may be very different (e.g. resource-intensive models generally fare better in our evaluation), which is partially captured by our measurements of efficiency. Finally, models may also differ significantly in their exposure to the particular data distribution or evaluation instances we use, with the potential for train-test contamination. We emphasize that we have a limited understanding on how contaminated models are, and to what extent this compromises the validity and legitimacy of our evaluation, though we do provide all evidence we are aware of in Appendix G.</p>
<h1>1.2 Empirical findings</h1>
<p>To give a sense of the magnitude of our evaluation, we ran a total of 4,939 runs (i.e. evaluating a specific model on a specific scenario). This amounts to a total cost of $12,169,227,491$ tokens and $17,431,479$ queries across all models, $\$ 38,001$ for the commercial APIs, and about 19,500 GPU hours worth of compute for the open models.</p>
<p>Here is a summary of the high-level findings:</p>
<ol>
<li>The benefits of instruction-tuning. Across the core scenarios, we find that text-davinci-002 performs best on our accuracy, robustness, and fairness metrics, with Anthropic-LM v4-s3 (52B) being in the top 3 for all 3 metrics (despite being more than $10 \times$ smaller in model scale compared to TNLG v2 (530B), which is the second most accurate and fair) as shown in Figure 26. Given the very strong performance of both models, and that they are the only instruction-tuned models we evaluate (beyond the much smaller OpenAI model variants), this suggests instruction-tuning provides a broad set of advantages.</li>
<li>Relating model accuracy with model access. In light of the high accuracies of Anthropic-LM v4-s3 (52B) (closed), TNLG v2 (530B) (closed), and text-davinci-002 (limited-access), we observe a consistent gap on all core scenarios (Figure 28) between the current open models and non-open models. We emphasize that this gap reflects the current snapshot of models we evaluate (Table 5), and that the gap could grow or shrink over time as new models are released. On one hand, we see the recent release of open models (OPT (175B), BLOOM (176B), GLM (130B)) as greatly reducing the gap over the past year, but we also have not evaluated some non-open models (e.g. PaLM, Gopher) that we expect to be quite accurate. In either case, monitoring this gap over time is crucial for tracking the accessibility (or lack thereof) and ultimately the power dynamics associated with language models.</li>
<li>
<p>Calibration. We observe that the relationship between accuracy and calibration (§4.4: METRICScalibration) depends on the scenario and adaptation procedure (Figure 24, Figure 25). As an example, for HellaSwag, ${ }^{6}$ improving accuracy worsens calibration, whereas for OpenBookQA, ${ }^{7}$ improving accuracy improves calibration.
<sup id="fnref11:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
</li>
<li>
<p>Robustness and fairness perturbations. Across all scenarios, we observe strong correlations between accuracy, robustness, and fairness, where robustness and fairness metrics consider worst-case accuracy over a set of perturbations (e.g. typos for robustness, dialect alteration for fairness) - see §4.5: METRICS-ROBUSTNESS, §4.6: METRICS-FAIRNESS for more details. While there is a strong correlation between accuracy and fairness (Figure 24, Figure 25), we do observe trade-offs where the most accurate model is not the most robust or most fair. We also see serious drops in some cases: for example, on NarrativeQA, TNLG v2 (530B) precipitously drops from $72.6 \%$ standard accuracy (i.e. the third-most accurate model) to $38.9 \%$ accuracy in the presence of robustness perturbations. ${ }^{8}$</p>
</li>
<li>Performance disparities. When we have access to demographic metadata, we generally see consistent performance disparities for all models. As an example of racialized dialect disparities, OPT (175B) is the most accurate model on TwitterAAE but its accuracy degrades from 1.506 bits per byte for White English to 2.114 bits per byte for African American English (lower is better). ${ }^{9}$</li>
<li>Generative harms. We find that the biases and toxicity in model generations are largely constant across models and low overall on average for the core scenarios (Figure 24). However, note that even low levels of bias or toxicity could cause non-trivial social harm, and targeted evaluations are needed to obtain a more detailed characterization (§5.6: TARGETED-BIAS, §5.7: TARGETED-TOXICITY).</li>
<li>Accuracy vs. efficiency. We do not see a strong trade-off between accuracy and efficiency (which depends on both the model architecture and the hardware, see §4.9: METRICS-EFFICIENCY) across all 30 models (Figure 24). For each family of models (e.g. different size variants of GPT-3), we find that as models become larger, accuracy consistently improves but with higher training and inference cost. ${ }^{10}$ Overall, we observe that only a subset of all models (across model families) are on the accuracy-efficiency Pareto frontier for each scenario.</li>
<li>Question answering. Across the 9 core question answering scenarios (§3.3: QUESTIONANSWERING), we observe significant heterogeneity in results, though text-davinci-002 is the most accurate model for all 9 scenarios. ${ }^{11}$ In fact, for 6 of the 9 scenarios, there is no open model among the three most accurate models, as generally they are text-davinci-002, Anthropic-LM v4-s3 (52B), and TNLG v2 (530B) in descending order of accuracy.</li>
<li>Information retrieval. We consider the classic task of ranking candidate passages given a query (§3.4: INFORMATIONRETRIEVAL). The best-performing models we evaluate outperform classical retrieval methods and under some settings perform comparably to various fine-tuned neural retrievers, while nonetheless trailing the state of the art. ${ }^{12}$ Because the number of candidates could be large, we create a LM request per passage, which requires the model to produce calibrated probabilities. Our use of LMs for passage ranking is unorthodox, and computationally intensive in its naive implementation, but we include it as a proof of concept.</li>
<li>Summarization. CNN/DailyMail and XSUM have been standard benchmarks for summarization for many years, but we find that automated evaluations on these datasets largely fail to discriminate differences we observed in model quality.</li>
<li>Sentiment analysis. For sentiment analysis on IMDB, many models are quite accurate and wellcalibrated with marginal drops on robustness and fairness perturbations, but the contrast sets of Gardner et al. (2020) highlight clear limitations in model robustness (e.g. one of the most accurate models in GLM (130B) drops by more than $8 \%$ ). ${ }^{13}$</li>
<li>Toxicity detection. For toxicity detection on CivilComments, we find that most models are not particularly accurate: OPT (175B) is one of the most accurate models across all scenarios
<sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p>(Figure 26), but achieves essentially chance accuracy at $50.1 \% .{ }^{14}$ Critically, given the importance of fairness in toxicity detection due the disparate impacts of content moderation, we find that most models are similarly accurate for detecting toxicity in comments mentioning Black and White individuals. However, models vary greatly in their robustness: OPT (175B) drops from $51.3 \%$ standard accuracy to $8.8 \%$ robust accuracy on the Black split, whereas the drop is less precipitous on the White split ( $50.8 \%$ to $24.3 \%$ ).
13. Miscellaneous text classification. For text classification on RAFT, we see significant heterogeneity in which models do well on which subsets/tasks. ${ }^{15}$ text-davinci-002 is consistently accurate across splits when compared with other models, but performs very poorly on the Systematic Review Inclusion split with an accuracy of $40.8 \%$ compared to $97.5 \%$ from several models (e.g. GLM $(130 B))$.
14. Linguistic understanding. The trends in accuracy for language modeling ${ }^{16}$ are quite different from the trends for the core scenarios (Figure 26) . In particular, GPT-NeoX (20B), OPT (175B), BLOOM (176B), GPT-J (6B), and OPT (66B) consistently have the lowest bits-per-byte (lower is better) on The Pile, TwitterAAE, and ICE. In terms of linguistic phenomena, all models perform fairly similarly on BLiMP overall, and further perform very similarly even on each of the specific subsets for morphology, syntax, semantics, and syntax-semantics. We see the widest spread on irregular forms (morphology), where surprisingly the models that tend to be the most accurate for core scenarios (i.e. text-davinci-002, TNLG v2 (530B)) are some of the least accurate for irregular forms, perhaps suggesting they have overgeneralized particular linguistic rules. ${ }^{17}$
15. Knowledge. text-davinci-002 demonstrates superior performance for all knowledge-intensive evaluations, ${ }^{18}$ with a very sizable gap for accuracy on TruthfulQA of $62.0 \%$ compared to second place of $36.2 \%$ from Anthropic-LM v4-s3 (52B). ${ }^{19}$ Further, TNLG v2 (530B) shows strong performance on the highly knowledge-intensive NaturalQuestions (closed-book) and WikiFact scenarios, which generally concurs with the hypothesis that model scale especially contributes to improvements in acquisition of factual knowledge. For example, Anthropic-LM v4-s3 (52B) and TNLG v2 (530B) tend to get very similar accuracies for most scenarios (as suggested by Figure 26), but TNLG v2 (530B) demonstrates a wide margin for these two scenarios ( $38.5 \%$ vs. $28.7 \%$ for NaturalQuestions (closed-book), $34.3 \%$ vs. $22.3 \%$ for WikiFact).
16. Reasoning. For reasoning-intensive scenarios, we find that the code models, especially code-davinci002 , consistently outperform the text models, even on synthetic reasoning scenarios posed in natural language. ${ }^{20}$ This gap is made clear in mathematical reasoning: for GSM8K, code-davinci-002 achieves an accuracy of $52.1 \%$, where the next best model is text-davinci-002 at $35.0 \%$ and no other model surpasses $16 \% .{ }^{21}$ Further, in addition to code-davinci-002, text-davinci-002 is much more accurate than other text models (e.g. $65.1 \%$ accuracy on synthetic reasoning in natural language, whereas the next most accurate text model is OPT (175B) at $29.4 \%$ accuracy, and code-davinci-002 has an accuracy of $72.7 \%$ ).
17. Memorization of copyrighted/licensed material. We find that the likelihood of direct regurgitation of long copyrighted sequences is somewhat uncommon, but it does become noticeable when looking at popular books. ${ }^{22}$ However, we do find the regurgitation risk clearly correlates with model accuracy: text-davinci-002, davinci (175B), and Anthropic-LM v4-s3 (52B) demonstrate the highest amount of verbatim regurgitation in line with their high accuracies.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ol>
<li>Disinformation. We find that the largest models (particularly text-davinci-002 and Anthropic-LM v4-s3 (52B)) are effective at generating realistic headlines that support a given thesis, ${ }^{23}$ but results are more mixed when prompting models to generate text encouraging people to perform certain actions (Table 8). ${ }^{24}$</li>
<li>Targeted biases. For BBQ, text-davinci-002 is the most accurate model by a very wide margin ( $89.5 \%$ accuracy), with the next most accurate models (T0++ (11B), 48.4\%; TNLG v2 (530B), $44.9 \%$ ) being the only other models with accuracies above $40 \%$. We highlight this because we see a very striking relationship on BBQ between model accuracy and model bias for ambiguous contexts. These three models, which are the three most accurate, are the only three models with biases in ambiguous contexts that align with broader social biases/discrimination, whereas all other models show biases in the other direction (Figure 40). In other words, we find that for BBQ the most accurate models are precisely those that are most concerning for social biases in ambiguous contexts, though the trends in disambiguated contexts are less clear.</li>
<li>Targeted toxicity generation. For the core scenarios, we observed the rate of toxicity generation was quite low. Honing in on toxicity generation, all models show much stronger tendencies for toxic generations for toxic prompts in RealToxicityPrompts, as compared to relatively non-toxic prompts in both RealToxicityPrompts and BOLD. ${ }^{25}$ Understanding how these trends change based on the automated toxicity detection model used (currently PerspectiveAPI), as well as when human judgments from diverse stakeholders are used, is a key area for future work.</li>
<li>Comprehensiveness. By evaluating under unified conditions extensively, we expose findings lying in plain sight. In other words, while in many cases we are evaluating models that are available publicly on datasets that are available publicly, we nonetheless surface new findings. As an example, we find text-davinci-002 achieves an accuracy of $74.4 \%$ ROUGE-L on NarrativeQA, which sets a new state-of-the-art across all methods to our knowledge, in this case over the strong QA-specialized UnIFIEDQA-v2 model ( $67.4 \%$ ROUGE-L; Khashabi et al., 2022).</li>
<li>Prompting. All models show significant sensitivity to the formatting of prompt, the particular choice of in-context examples, and the number of in-context examples across all scenarios and for all metrics (see §8.2: PROMPTING-ANALYSIS). In this effort, we consistently work towards standardizing these dimensions (e.g. to ensure models are interoperable/performant using the same prompting practices), but current models differ in what prompting decisions would maximize accuracy. ${ }^{26}$</li>
<li>Multiple choice adaptation method. We find that model performance is extremely sensitive to how multiple choice scenarios are adapted into prompts: for example, accuracy for OPT (175B) on HellaSwag is $79.1 \%$ when each answer choice is presented in a separate 0 -shot prompt (i.e. one of the most accurate models), but drops precipitously to $30.2 \%$ (almost random accuracy) when the answer choices are presented jointly in a single 5 -shot prompt (i.e. in the format of a multiple-choice exam). ${ }^{27}$ Further, even for the same scenario, the adaptation method that maximizes accuracy can differ (and produce qualitatively different results) across models (Figure 33) . This poses a fundamental challenge for what it means to standardize language model evaluation in a fair way across models.</li>
<li>Upstream perplexity and downstream accuracy. Given the myriad scenarios where LMs could provide value, it would be appealing for many reasons if upstream perplexity on language modeling objectives reliably predicted downstream accuracy. Unfortunately, when making these comparisons across model families, even when using bits-per-byte (BPB; which is more comparable than perplexity), we find this type of prediction does not work well: BPB on The Pile is a poor predictor of downstream accuracy (Figure 30) though we note some models are trained on The Pile whereas others are not (Table 13). More broadly, given the many downstream results, we encourage
<sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p>future work to explore new intrinsic/upstream surrogate measures of performance that can be shown to reliably predict downstream results (including for desiderata beyond accuracy) as discussed in Bommasani et al. (2021, §4.4.2)
25. Trends for model scale. We find that model scale, within a model family, reliably predicts model accuracy, but for no scenario is a good predictor of downstream accuracy across all models (Figure 29). However, we see a very clear thresholding effect: all models that win head-to-head model comparisons for accuracy at a rate well above chance (i.e. $&gt;55 \%$ ) are at least 50B parameters (Figure 26). Of these models, which are the 10 most accurate models, some of the most accurate (i.e. in the top 5) are the smallest (Anthropic-LM v4-s3 (52B), Cohere xlarge v20220609 (52.4B)). Overall, scale seems to be a key determinant of accuracy, and scaling within a model family reliable improves accuracy, but it might be inefficient compared to other means (e.g. training with human feedback; compare TNLG v2 (530B) and Anthropic-LM v4-s3 (52B)).</p>
<h1>1.3 Contributions</h1>
<p>To summarize, our contributions are:</p>
<ol>
<li>Taxonomy. We taxonomize the vast design space of language model evaluation into scenarios and metrics. By stating this taxonomy, we can select systematically from this space, which makes explicit both our priorities in benchmark design and the limitations in the benchmark at present (see §10: MISSING).</li>
<li>Broad coverage. Given our taxonomy, we select and implement 16 core scenarios, for which we comprehensively measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, efficiency). We also include 7 targeted evaluations of skills and risks (e.g. knowledge, reasoning, disinformation, copyright), introducing 21 new scenarios that have not been previously used in mainstream language model evaluation.</li>
<li>Evaluation of existing models. We evaluate 30 language models under the standardized conditions of our benchmark, ensuring models can now be directly compared across many scenarios and metrics. These models vary in terms of their public accessibility: 10 are open, 17 are limited-access, and 3 are closed.</li>
<li>Empirical findings. Our extensive evaluation yields a host of findings (§8: EXPERIMENTS), which in some cases reinforce findings in the literature and in others produce new knowledge about today's language models. These results offer guidance for future language model development and ample opportunities for further analysis.</li>
<li>Interactive results and codebase. We provide a public website with all results, underlying model predictions and adaptation details, along an extensible codebase to support the community in taking HELM further. ${ }^{28}$</li>
</ol>
<p>Acknowledging the prior work this effort builds on. To build our holistic evaluation of language models, we directly build on top of many prior works. While we advocate for evaluating language models in their totality, i.e. centralizing many disparate evaluations, we want to be explicit that the underlying works across the AI community should be recognized and cited, as HELM would not exist in its current form without them. In particular, if the results of HELM are used by future work or new models are evaluated on HELM, they should cite the works that created the many datasets/evaluations that constitute HELM. ${ }^{29}$ For this reason, we provide the BibTeX entries for all of these works in the codebase ${ }^{30}$ and explicitly acknowledge the associated work for every evaluation on the website. ${ }^{31}$</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Contents</h1>
<p>1 Introduction ..... 2
1.1 HELM ..... 3
1.2 Empirical findings ..... 6
1.3 Contributions ..... 10
2 Preliminaries ..... 14
2.1 Scenarios ..... 14
2.2 Adaptation ..... 14
2.3 Metrics ..... 15
2.4 Roadmap ..... 15
3 Core scenarios ..... 15
3.1 Taxonomy ..... 16
3.2 Selection ..... 18
3.3 Question answering ..... 19
3.4 Information retrieval ..... 20
3.5 Summarization ..... 21
3.6 Sentiment analysis ..... 23
3.7 Toxicity detection ..... 23
3.8 Miscellaneous text classification ..... 24
4 General metrics ..... 25
4.1 Taxonomy ..... 25
4.2 Selection ..... 26
4.3 Accuracy ..... 27
4.4 Calibration and uncertainty ..... 27
4.5 Robustness ..... 28
4.6 Fairness ..... 30
4.7 Bias and stereotypes ..... 31
4.8 Toxicity ..... 32
4.9 Efficiency ..... 33
5 Targeted evaluations ..... 35
5.1 Language ..... 35
5.2 Knowledge ..... 36
5.3 Reasoning ..... 37
5.4 Memorization \&amp; copyright ..... 39</p>
<p>5.5 Disinformation ..... 40
5.6 Bias ..... 42
5.7 Toxicity ..... 42
6 Models ..... 43
7 Adaptation via prompting ..... 45
8 Experiments and results ..... 47
8.1 Meta-analysis ..... 47
8.2 Prompting analysis ..... 56
8.3 Task-specific results for core scenarios ..... 60
8.4 Targeted evaluations ..... 67
8.5 Human evaluations ..... 73
9 Related work and discussion ..... 75
10 What is missing ..... 77
10.1 Missing scenarios ..... 78
10.2 Missing metrics ..... 79
10.3 Missing targeted evaluations ..... 79
10.4 Missing models ..... 80
10.5 Missing adaptation ..... 80
11 Limitations and future work ..... 81
11.1 Limitations of results ..... 81
11.2 Limitations of HELM implementation ..... 81
11.3 Limitations of HELM design ..... 82
12 Conclusion ..... 83
A Author contributions ..... 118
B Core scenarios ..... 121
B. 1 Question answering ..... 121
B. 2 Information retrieval ..... 125
B. 3 Summarization ..... 127
B. 4 Sentiment analysis ..... 128
B. 5 Toxicity detection ..... 129
B. 6 Miscellaneous text classification ..... 129</p>
<p>C General metrics ..... 130
C. 1 Accuracy ..... 130
C. 2 Calibration and uncertainty ..... 131
C. 3 Robustness ..... 133
C. 4 Fairness ..... 134
C. 5 Bias and stereotypes ..... 134
C. 6 Toxicity ..... 137
C. 7 Efficiency ..... 137
D Perturbations ..... 139
D. 1 Robustness ..... 139
D. 2 Fairness ..... 140
E Targeted evaluations ..... 141
E. 1 Language ..... 141
E. 2 Knowledge ..... 144
E. 3 Reasoning ..... 144
E. 4 Memorization \&amp; copyright ..... 152
E. 5 Disinformation ..... 153
E. 6 Bias ..... 153
E. 7 Toxicity ..... 154
F Comparison with other evaluations ..... 155
G Contamination ..... 155
H Priority system ..... 156
I Models ..... 158
J Adaptation ..... 160
J. 1 Formatting test instances ..... 160
J. 2 Formatting the remainder of the prompt ..... 161
J. 3 Decoding parameters ..... 161
J. 4 Adaptation methods ..... 162</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Evaluation components. Each evaluation run requires the specification of a scenario (what we want), a model with an adaptation process (how we get it), and one or more metrics (how good are the results).</p>
<p>Scenario: MMLU(subject=anatomy)
Input: Which of the following terms describes the
body's ability to maintain its normal state?
References:</p>
<ul>
<li>Anabolism</li>
<li>Catabolism</li>
<li>Tolerance</li>
<li>Homeostasis [correct]</li>
</ul>
<p>Figure 6: Scenario. An example of a multiple choice scenario from MMLU (subject=anatomy), which consists of a list of instances, each with an input and a set of references.</p>
<h1>2 Preliminaries</h1>
<p>We introduce the basic primitives (scenario, adaptation, metric) required to evaluate a language model (Figure 5). With these primitives, we then provide a roadmap for how we holistically evaluate language models.</p>
<h3>2.1 Scenarios</h3>
<p>A scenario instantiates a desired use case for a language model. Useful language models are performant on a variety of scenarios: scenarios are what we want models to do. While practical use cases for language models involve other factors, we operationalize scenarios through a list of instances, divided into a training set and one or more test sets. Each instance consists of (i) an input (a string) and (ii) a list of references. Each reference is a string annotated with properties relevant for evaluation (e.g. is it correct or acceptable?). See Figure 6 for an example scenario.</p>
<h3>2.2 Adaptation</h3>
<p>Adaptation is the procedure that transforms a language model, along with training instances, into a system that can make predictions on new instances. Examples of adaptation procedures include prompting, lightweight-finetuning, and finetuning; we focus on prompting in this work.</p>
<p>We define a language model to be a black box that takes as input a prompt (string), along with decoding parameters (e.g. temperature). The model outputs a completion (string), along with log probabilities of the prompt and completion. We do not assume access to the internal model activations or its training data, which reflects the practical reality of API access available to researchers (Liang et al., 2022). In fact, we do not even make any assumptions about how the language model is constructed. See Figure 7 for how we adapt the example scenario from Figure 6.</p>
<p>Viewing language models as text-to-text abstractions is important for two reasons: First, while the prototypical LM is currently a dense Transformer trained on raw text, LMs could also use an external document store (Lewis et al., 2020c), issue search queries on the web (Nakano et al., 2021), or be trained on human preferences (Ouyang et al., 2022; Bai et al., 2022). We wish to remain agnostic to these implementation details.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">The following are multiple choice questions (with answers) about</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">anatomy.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Question: The pleura</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">A. have no sensory innervation.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">B. are separated by a 2 mm space.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">C. extend into the neck.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">D. are composed of respiratory epithelium.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Answer: C</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">...</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Question: Which of the following terms describes the body's ability</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">to maintain its normal state?</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">A. Anabolism</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">B. Catabolism</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">C. Tolerance</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">D. Homeostasis</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Answer: D. (squat $=0.00$</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Adaptation. During adaptation, we construct a prompt for each evaluation instance which may include in-context training instances as well. Given decoding parameters, a language model generates a completion (in red). The multiple choice example is shown using two different adaptation strategies that we describe subsequently, with left version being the joint strategy (all answer choices are presented at once) and the right version being the separate strategy (each answer choice is presented separately).</p>
<p>Second, the text-to-text abstraction is a convenient general interface that can capture all the (text-only) tasks of interest, an idea that was pioneered by McCann et al. (2018) and Raffel et al. (2019).</p>
<h1>2.3 Metrics</h1>
<p>Once a language model is adapted, we execute the resulting system on the evaluation instances for each scenario, yielding completions with their log probabilities. To determine how well the model performs, we compute metrics over these completions and probabilities. Metrics concretely operationalize the abstract desiderata we require of useful systems. See §4: METRICS for more details. The metrics we compute for our running example (Figure 6) might look like this:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Exact match</th>
<th style="text-align: left;">$:$</th>
<th style="text-align: left;">0.571</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ECE (10-bin)</td>
<td style="text-align: left;">$:$</td>
<td style="text-align: left;">0.221</td>
</tr>
<tr>
<td style="text-align: left;">Exact match (robustness)</td>
<td style="text-align: left;">$:$</td>
<td style="text-align: left;">0.551</td>
</tr>
<tr>
<td style="text-align: left;">Exact match (fairness)</td>
<td style="text-align: left;">$:$</td>
<td style="text-align: left;">0.524</td>
</tr>
<tr>
<td style="text-align: left;">Inference runtime</td>
<td style="text-align: left;">$:$</td>
<td style="text-align: left;">0.147</td>
</tr>
<tr>
<td style="text-align: left;">$\ldots$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h3>2.4 Roadmap</h3>
<p>To evaluate a language model, we must specify a series of runs, where each run is defined by a (scenario, adaptation method, metric) triple. Each of these scenarios, adaptation, and metrics define a complicated and structured space, which one implicitly navigates to make decisions in evaluating a language model. Central to our approach to holistic evaluation is that we make both the space and the decision explicit. In §3: CORE-SCENARIOS and §4: METRICS, we first taxonomize both spaces and then systematically select points from the spaces. This specifies our abstract aspiration and our concrete implementation, which together define HELM. Distinguishing these steps also helps clarify what is fundamentally possible vs. what we, as a specific collective of benchmark designers, chose to prioritize and emphasize. Then, we evaluate 30 models by making a specific choice for adaptation procedure (i.e. 5-shot prompting), though we emphasize many other adaptation procedures could be considered.</p>
<h2>3 Core scenarios</h2>
<p>We taxonomize scenarios as shown in Figure 8 based on (i) a task (e.g. question answering, summarization), which characterizes what we want a system to do; (ii) a domain (e.g. a Wikipedia 2018 dump), which</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Scenario structure. Scenarios are what we want the language model to do. To specify a scenario, we break it down into a task, domain, and language, further subdividing the domain into properties of the text (what), speaker (who), and the time/circumstances (when). Examples of scenarios include (question answering, (clinical notes, doctors, now), English) and (toxicity detection, (tweets, Egypt, Internet-era), Arabic).
characterizes the type of data we want the system to do well on; and (iii) the language or language variety (e.g. Spanish). Tasks, domains, and languages are not atomic or unambiguous constructs: they can be made coarser and finer, but we use them as intuitive structure for the space of scenarios. Given this structure, we deliberately select scenarios based on three overarching principles: (i) coverage of the space, (ii) minimality of the set of selected scenarios, and (iii) prioritizing scenarios that correspond to user-facing tasks. Alongside feasibility given our resources (which we explicitly acknowledge), this defines the core scenarios we evaluate on, which we will measure all metrics on. In $\S 10.1$ : MISSING-SCENARIOS, we highlight regions of the scenario space that we taxonomize but do not currently cover in our benchmark/scenario selection.</p>
<h1>3.1 Taxonomy</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Track</th>
<th style="text-align: left;">Tasks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Computational Social Science and Cultural Analytics</td>
<td style="text-align: left;">No canonical tasks/not task-centric</td>
</tr>
<tr>
<td style="text-align: left;">Dialogue and Interactive Systems</td>
<td style="text-align: left;">Chat-chat dialogue, task-oriented dialogue</td>
</tr>
<tr>
<td style="text-align: left;">Discourse and Pragmatics</td>
<td style="text-align: left;">Discourse parsing, sentence ordering, conference resolution</td>
</tr>
<tr>
<td style="text-align: left;">Ethics and NLP</td>
<td style="text-align: left;">Toxicity and hate speech detection, misinformation and fake news detection</td>
</tr>
<tr>
<td style="text-align: left;">Generation</td>
<td style="text-align: left;">Data-to-text generation,</td>
</tr>
<tr>
<td style="text-align: left;">Information Extraction</td>
<td style="text-align: left;">Named entity recognition, entity linking, entity extraction, relation extraction, event extraction, open information extraction</td>
</tr>
<tr>
<td style="text-align: left;">Information Retrieval and Text Mining</td>
<td style="text-align: left;">Information retrieval and passage retrieval</td>
</tr>
<tr>
<td style="text-align: left;">Interpretability and Analysis of Models for NLP</td>
<td style="text-align: left;">No canonical tasks/not task-centric</td>
</tr>
<tr>
<td style="text-align: left;">Language Grounding to Vision, Robotics and Beyond</td>
<td style="text-align: left;">Image captioning, visual question answering, instruction following, navigation</td>
</tr>
<tr>
<td style="text-align: left;">Linguistic Theories, Cognitive Modeling, and Psycholinguistics</td>
<td style="text-align: left;">No canonical tasks/not task-centric</td>
</tr>
<tr>
<td style="text-align: left;">Machine Learning for NLP</td>
<td style="text-align: left;">Language modeling</td>
</tr>
<tr>
<td style="text-align: left;">Machine Translation and Multilinguality</td>
<td style="text-align: left;">Machine translation</td>
</tr>
<tr>
<td style="text-align: left;">NLP Applications</td>
<td style="text-align: left;">No canonical tasks</td>
</tr>
<tr>
<td style="text-align: left;">Phonology, Morphology, and Word Segmentation</td>
<td style="text-align: left;">Tokenization, lemmatization,</td>
</tr>
<tr>
<td style="text-align: left;">Question Answering</td>
<td style="text-align: left;">Question answering and reading comprehension</td>
</tr>
<tr>
<td style="text-align: left;">Resources and Evaluation</td>
<td style="text-align: left;">No canonical tasks/not task-centric</td>
</tr>
<tr>
<td style="text-align: left;">Semantics: Lexical</td>
<td style="text-align: left;">Word sense disambiguation, word sense induction</td>
</tr>
<tr>
<td style="text-align: left;">Semantics: Sentence-level Semantics, Textual Inference, and Other Areas</td>
<td style="text-align: left;">Semantic parsing, natural language inference, semantic role labeling/slot filling, semantic textual similarity, paraphrase detection</td>
</tr>
<tr>
<td style="text-align: left;">Sentiment Analysis, Syclonic Analysis, and Argument Mining</td>
<td style="text-align: left;">Sentiment analysis, style transfer, argument mining, stance detection, opinion mining, text simplification</td>
</tr>
<tr>
<td style="text-align: left;">Speech and Multimodality</td>
<td style="text-align: left;">Text-to-speech, speech-to-text</td>
</tr>
<tr>
<td style="text-align: left;">Summarization</td>
<td style="text-align: left;">Summarization, sentence compression</td>
</tr>
<tr>
<td style="text-align: left;">Syntax: Tagging, Chunking and Parsing</td>
<td style="text-align: left;">POS tagging, chunking, constituency parsing, dependency parsing, grammar induction, grammatical error correction</td>
</tr>
</tbody>
</table>
<p>Table 1: Taxonomy of tasks. To taxonomize the space of tasks, we leverage the NLP community's taxonomy of subareas as codified by the ACL 2022 list of tracks. For each track, we then expand it into canonical tasks associated with that track.</p>
<p>Tasks. Given the ubiquity of natural language, the field of natural language processing (NLP) considers myriad tasks that correspond to language's many functions (Jurafsky \&amp; Martin, 2000). It is difficult to derive a space of tasks from first principles, so we compile existing sources of tasks. Naturally, given NLP is a task-centric field, we begin with tasks that have been extensively studied by the NLP community. To generate this set, we take the tracks at a major NLP conference (ACL 2022), which reflect the "relevant</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9: Modern use cases for language models. An assortment of (largely novel/historically unexplored) potential use cases for language models. Figure sourced from https://beta.openai.com/ examples/.
topics" of study in NLP at the time of writing. ${ }^{32}$ For each track, we map the associated subarea of NLP to canonical tasks for that track in Table 1. We acknowledge there is some subjectivity in choosing what is "canonical", which was only done so as to make this process manageable.</p>
<p>While these tasks often have long traditions of study in the NLP research community, we make two observations: (i) these tasks often have important intra-task structure (e.g. we refer to all of question answering as one "task", whereas the QA community likely would further decompose QA into finer-grained categories (Rogers et al., 2021)) and (ii) while these tasks have (long) traditions of study in NLP research, they are not the only, or even the most societally/economically impactful, tasks.</p>
<p>For example, the deployment of language models as interfaces by OpenAI, Cohere, and AI21 Labs has introduced use cases beyond what the NLP community has historically studied (see Figure 9 and compare to Table 1). In fact, some of these tasks are fundamentally new: the advent of sufficiently capable technology motivates the consideration of tasks that were not previously conceived (or conceived as within scope for algorithmic systems). Further, these tasks pattern quite differently from what has been traditionally studied in the NLP and AI research communities (see Ouyang et al., 2022). This introduces a fundamental challenge to stating the space of tasks: we will not be able to conceive of the true full space of tasks until we see technology that makes us consider these tasks. And, more broadly, even articulating (let alone covering) the long tail of known potential use cases remains open.</p>
<p>Domains. Domains are a familiar construct in NLP, yet their imprecision complicates systematic coverage of domains. We further decompose domains according to 3 W's:</p>
<ol>
<li>What (genre): the type of text, which captures subject and register differences. Examples: Wikipedia, social media, news, scientific papers, fiction.</li>
<li>When (time period): when the text was created. Examples: 1980s, pre-Internet, present day (e.g. does it cover very recent data?)</li>
<li>Who (demographic group): who generated the data or who the data is about. Examples: Black/White, men/women, children/elderly.
<sup id="fnref10:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<h1>World Languages</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10: The world's languages. Only a tiny percentage of the world's languages are currently represented in language models. There are over 6,000 languages in the world, with estimates varying due to the inherent uncertainty of what constitutes a separate language (Nordhoff \&amp; Hammarström, 2011). This map shows the languages of the world, with each dot representing one language and its color indicating the top-level language family. Data is from Glottolog (Hammarström et al., 2021). Figure and caption sourced from Bommasani et al. (2021, §2.1).</p>
<p>We do not include where the text was created (e.g. country) and how it was created (e.g. hand-written, typed, transcribed from speech or sign), but these may also be relevant. Further, why the text was created is closely related to what it is. To be precise, textual data in the input to the language model (e.g. the question or the passage, if available, in question answering) and the answer (e.g. the summary in summarization) have associated domains that are not necessarily the same. For simplicity, we will assume a dataset has a single domain corresponding to properties of its inputs, though it would be more precise to consider domains associated with all aspects of the input and output.</p>
<p>Languages. The billions of people around the world speak thousands of different languages (see Figure 10). However, in AI and NLP, the vast majority of work has centered on a few high-resourced languages (e.g. English, Chinese), even including languages that have large speaker populations (e.g. there are more than 65 million speakers of Fula, a West African language, but few if any NLP resources exist for Fula; Nguer et al., 2020). With this in mind, we do not extensively taxonomize the world's languages, as we will focus on predominantly evaluating English-only models (with a few exceptions like BLOOM (176B) that are clearly multilingual but we evaluate only for English). Consequently, we instead turn our focus to coverage of English varieties and dialects. In this regard, we note there are several axes of interest in linguistic typology and sociolinguistics; we point to Bommasani et al. (2021, §2.1) and Joshi et al. (2020) for further discussion.</p>
<h3>3.2 Selection</h3>
<p>As a matter of coverage, ideally, we would evaluate a language model on every scenario (i.e. every (task, domain) pair). However, as we demonstrate in our taxonomy, both tasks and domains themselves are rich and expansive spaces. For this reason, rather than striving for coverage of scenarios, we instead aim for coverage of tasks, domains, and languages each independently. This risks not exposing important interactions (e.g. we may be especially interested in toxicity detection for text authored by marginalized groups (Sap et al.,</p>
<p>2019a), but is a decision we make for practical reasons (e.g. availability of datasets, effort to implement scenarios, and computational resources to evaluate LMs on chosen scenarios).</p>
<p>Tasks. To select tasks, we begin with the set we described previously. Since we are studying English language models, we filter infeasible tasks (e.g. multimodal tasks or machine translation are not suitable for unimodal English language models). ${ }^{33}$ Of the remaining tasks, we elect to prioritize user-facing tasks: we believe these tasks will confer much of the direct social impact of language models and aligns with our perspective of language models as interfaces. Consequently, we filter tasks based on our judgments of what is user-facing. ${ }^{34}$ This yields the following tasks: question answering, information retrieval, summarization, sentiment analysis, and toxicity detection. ${ }^{35}$ And to provide some coverage of the long tail of tasks, we include miscellaneous text classification, which represents the non-standard text classification use cases for language technologies historically and at present for language models.</p>
<p>Domains and Languages. Given that we found it more complicated to arrive at an explicit enumeration of domains compared to tasks, ${ }^{36}$ we instead focus on domain coverage during our selection of specific datasets to instantiate scenarios. Similarly, we ensure coverage of the English varieties of different Englishspeaking countries as well as African American English through targeted evaluations that we discuss in §5.1: LANGUAGE. In doing so, we also demonstrate our desire for a minimal set of evaluations (both because evaluations have costs, so larger sets will be more unwieldy, and producing more results often comes at the cost of clarity on how to sift through them). With this in mind, we emphasize that for large regions of the scenario space, specifically in relation to domains (e.g. scenarios involving text written by elderly speakers), there exist very few, if any, datasets in NLP. We hope the community can build on our work by ensuring greater coverage of the domains and scenarios we did not cover in our benchmark by building the necessary and oft-undervalued resources (Jo \&amp; Gebru, 2020; Paullada et al., 2021; Rogers, 2021; Jernite et al., 2022). To facilitate this, we explicitly identify specific scenarios that we recommend prioritizing in $\S 10.1$ : MISSINGSCENARIOS. We also note that there is more to a dataset than just these axes, which determine how well it operationalizes the desired use case (e.g. the quality of crowd-sourced labels in the dataset). Having settled on the tasks we will cover and our approach to domain/language coverage, we detail how we selected the particular datasets for each scenario.</p>
<h1>3.3 Question answering</h1>
<p>Question answering (QA) is a fundamental task in NLP that underpins many real-world applications including web search, chatbots, and personal assistants. QA is very broad in terms of the questions that can be asked and the skills that are required to arrive at the answer, covering general language understanding (§5.1: LANGUAGE), integration of knowledge (§5.2: KNOWLEDGE), and reasoning (§5.3: REASONING) (Gardner et al., 2019; Rogers et al., 2021).</p>
<p>Problem setting. In QA, given a question (e.g. "Where was the painter of the Mona Lisa born?"), the task is to predict the correct answer ("Italy"). The format of question answering may have some variations: in the open-book or reading comprehension setting, additional context to refer to, such as supporting documents</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 11: Example of question answering. An example instance for question answering from MMLU. Different QA scenarios can have significantly different properties, but this example captures the overall structure of question answering.
(e.g. Wikipedia page of "Mona Lisa"), is given to the model. In the multiple-choice setting, answer choices to choose from (e.g. "(A) France (B) Italy") are given to the question. Figure 11 depicts an example.</p>
<p>Datasets and selection process. There are hundreds of question-answering datasets available in NLP, with a rapid increase in the number of datasets in recent years (Rogers et al., 2021). To select questionanswering datasets, we prioritized (i) domain coverage, in terms of the domain of the inputs/contexts and (ii) coverage of component skills required for the datasets (e.g. we deliberately ensured of datasets that required commonsense knowledge and reasoning).</p>
<p>We selected the NaturalQuestions (Kwiatkowski et al., 2019), NarrativeQA (Kočisky et al., 2017), and QuAC (Choi et al., 2018) datasets to ensure domain coverage as these datasets cover web search queries, stories, and conversational questions (i.e. dialogue) respectively. NaturalQuestions consists of questions from queries to Google search and annotations from Wikipedia; we consider both open-book and closed-book variants of NaturalQuestions. NarrativeQA tests reading comprehension through the understanding of books and movie scripts. QuAC (Question Answering in Context) provides freeform questions and answers which are more open-ended and dependent on context.</p>
<p>To these, we add the HellaSwag (Zellers et al., 2019), OpenBookQA (Mihaylov et al., 2018), and TruthfulQA (Lin et al., 2021b) datasets to ensure coverage of commonsense knowledge and reasoning. HellaSwag tests commonsense inference and was created through adversarial filtering to synthesize wrong answers. OpenBookQA is based on open book exams, with a collection of basic science facts and crowd-sourced multiple-choice questions to test understanding and application of these facts. TruthfulQA tests model truthfulness through questions that align with common human misconceptions, spanning law, medicine, finance, and politics, among others, that were adversarially generated using davinci (175B) as the target model.</p>
<p>To further ensure broad coverage of knowledge-intensive question answering across many disciplines, we add the MMLU (Hendrycks et al., 2021c) meta-benchmark of 57 constituent datasets. MMLU (Measuring Massive Multitask Language Understanding) measures multitask accuracy and includes a diverse set of 57 tasks, testing problem solving and general knowledge.</p>
<p>Finally, we add BoolQ (Clark et al., 2019) which, in addition to QuAC, was used to study model robustness to equivariances due to the available contrast set (Gardner et al., 2020). BoolQ is a collection of binary yes/no questions generated through the same process as NaturalQuestions.</p>
<h1>3.4 Information retrieval</h1>
<p>Information retrieval (IR), which refers to the class of tasks concerned with searching large unstructured collections (often text collections), is central to numerous user-facing applications. IR has a long tradition of study (Salton \&amp; Lesk, 1965; Salton, 1971; Spärck Jones, 1972; Salton \&amp; McGill, 1983; Manning et al., 2008; Lin et al., 2021a) and is one of the most widely deployed language technologies. It powers the Web and ecommerce search, and serves as a key component in many knowledge-intensive NLP systems for open-domain question answering or fact checking.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12: Example of information retrieval (passage ranking). An example instance for information retrieval from MS MARCO.</p>
<p>We focus here on the passage ranking task: given a query $q$ and a large corpus $C$ of passages, systems must output a list of the top- $k$ passages from $C$ in decreasing "relevance" to $q$. We specifically study this in the context of re-ranking: since $C$ is typically extremely large (e.g. $|C|&gt;10 M$ passages), we consider only ranking the top- $k$ passages among a set retrieved for $q$ (i.e. $M(q)$ where $|M(q)| \ll|C|$ ) by an efficient external retrieval mechanism (e.g. BM25; Robertson \&amp; Zaragoza, 2009).</p>
<p>IR differs fundamentally from the other tasks we consider in this work, as each test example (i.e. a query) entails processing a large set of passages and will likely invoke the LM numerous times to do so. ${ }^{37}$ Because of this, IR tasks have received very little attention in the few-shot in-context learning with language models, with the exception of the recent zero-shot approach by Sachan et al. (2022).</p>
<p>Problem setting. We address the re-ranking task in a pointwise fashion: we formulate the information retrieval problem using prompting as a binary log-probability problem, similar to Nogueira \&amp; Cho (2019): Given a passage $c_{i}$ and a query $q$, we ask the model whether the passage contains an answer to the query. If the model's answer is Yes with a high probability, we rank the corresponding $c_{i}$ higher, while the No answer with high probability achieves the opposite. Figure 12 depicts an example instance. The rankings produced are then evaluated using standard information retrieval metrics.</p>
<p>Datasets and selection process. We demonstrate the information retrieval task using the MS MARCO ranking datasets. While it is originally a question answering task, the retrieval version of MS MARCO is the largest publicly available collection of relevance judgments and has been central to much of the progress in neural IR over the past several years (Lin et al., 2021a).</p>
<p>We use the original passage ranking dataset accompanying the public MS MARCO leaderboard ${ }^{38}$ (Nguyen et al. 2016; henceforth, the regular track) and the dataset from the TREC 2019 Deep Learning track (Craswell et al. 2020; henceforth, the TREC track). Both datasets evaluate retrieval out of a collection of 9 M passages from the Web. The regular track contains a large number of queries (e.g., over 500,000 training set queries) with sparse relevance judgments: on average, annotators identify only one "positive" (relevant) passage for each query, and every other passage is assumed to be a negative. In contrast to this, the TREC track contains only 43 queries that are more rigorously annotated, with over 9,000 query-passage pairs with associated relevance judgments corresponding to the 43 queries.</p>
<h1>3.5 Summarization</h1>
<p>Text summarization is an established research direction in NLP (Luhn, 1958; Mani, 1999; Spärck Jones, 1999; Nenkova \&amp; McKeown, 2012), with growing practical importance given the ever-increasing volume of text that would benefit from summarization. To effectively summarize, systems must identify and yield the core relevant and informative content in the source document while removing less critical information and avoiding redundancy (Peyrard, 2019). The rise of language models in recent years has dramatically</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{37}$ Effectively, this means that model outputs come from a very large combinatorial space: they are much more constrained than open-ended generation tasks but much less so than standard classification tasks, setting this scenario apart from many others in terms of its automatic evaluation.
${ }^{38}$ https://microsoft.github.io/msmarco/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref11:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
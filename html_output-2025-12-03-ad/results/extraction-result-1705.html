<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1705 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1705</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1705</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-269449539</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.18243v2.pdf" target="_blank">LEGENT: Open Platform for Embodied Agents</a></p>
                <p><strong>Paper Abstract:</strong> Despite advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), their integration into language-grounded, human-like embodied agents remains incomplete, hindering complex real-life task performance in physical environments. Existing integrations often feature limited open sourcing, challenging collective progress in this field. We introduce LEGENT, an open, scalable platform for developing embodied agents using LLMs and LMMs. LEGENT offers a dual approach: a rich, interactive 3D environment with communicable and actionable agents, paired with a user-friendly interface, and a sophisticated data generation pipeline utilizing advanced algorithms to exploit supervision from simulated worlds at scale. In our experiments, an embryonic vision-language-action model trained on LEGENT-generated data surpasses GPT-4V in embodied tasks, showcasing promising generalization capabilities.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1705.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1705.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VILA-7B (VILA-based VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VILA-7B (backbone) / VILA-based Vision-Language-Action model (VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A visual-language foundation model (VILA-7B) used as the backbone to build a vision-language-action model that is fine-tuned on LEGENT-generated embodied trajectories (variants: VILA-Sep and VILA-Joint).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vila: On pretraining for visual language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>VILA-based Vision-Language-Action (VLA) model (VILA-7B backbone; VILA-Sep / VILA-Joint variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>An LMM (VILA-7B) backbone adapted into a vision-language-action model that ingests interleaved egocentric images and text (task descriptions and previous obs/actions) and is supervised to output unified code-like continuous control actions and textual responses; trained via supervised fine-tuning on datasets of generated trajectories from LEGENT. Two fine-tuning variants are reported: VILA-Sep (task-specific fine-tuning per task) and VILA-Joint (joint fine-tuning on both tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>visual-language pretraining (image-text multimodal pretraining / LMM pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Come Here (navigation) and Where Is (embodied question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>3D embodied tasks in the LEGENT environment: 'Come Here' requires navigating (single-room and two-room variants) to a user/location; 'Where Is' requires searching the scene and answering an object-location question (single-room training, two-room held-out generalization test). Environments are realistic, interactive indoor scenes with egocentric vision and object interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>During LMM pretraining: interleaved language and image tokens (no native embodied action tokens); pretraining did not provide direct embodied-action supervision in LEGENT.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous control actions exposed by LEGENT, e.g. move_forward(distance in meters), rotate(degrees), interact()/grab/put/open/close(object_id), speak(text); actions are expressed as continuous values for movement and rotation plus discrete interaction/speech commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Supervised fine-tuning: the VILA backbone is trained to map textual task descriptions plus interleaved observation-action history to the unified action-code outputs used in LEGENT (i.e., learning a direct mapping from perception+language tokens to control token sequences). The paper uses LEGENT-generated trajectories as paired supervision (observations -> ground-truth actions).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Egocentric RGB visual observations (the prototype uses the observation at the end of each continuous action as the image input) and natural language task descriptions; no depth or explicit object-detection sensors were required in the reported prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Prototype experiments used generated training sets of 1k and 10k trajectories for the initial tasks (Come Here 1-room/2-room and Where Is 1-room); evaluation used 100 trajectories per task. Exact numeric learning curves or thresholds are not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Compatibility of VILA with interleaved image-text inputs; LMM's prior visual-language knowledge that can be adapted via supervised fine-tuning; availability of large-scale, paired egocentric-image + action trajectories generated by LEGENT that closely match the model's input-output format.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Prototype limitations include using only end-of-action frames (no full video modeling), relatively small-scale training in experiments (1k/10k trajectories only reported), omission of body-joint control (reducing embodied richness), and potential perception gaps between LMM pretraining distributions and egocentric embodied visual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A VILA backbone fine-tuned on LEGENT-generated image-text-action trajectories can be adapted to 3D embodied navigation and embodied QA tasks and outperforms a multimodal baseline (GPT-4V) that lacks embodied training; provisioning of matched interleaved supervision and continuous control tokenization enables end-to-end mapping from perception+language to control, and limited-scale pretraining + supervised adaptation yields measurable generalization to some unseen settings (e.g., two-room QA) though detailed numeric gains and sample-efficiency improvements are not fully quantified in the prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGENT: Open Platform for Embodied Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1705.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1705.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V(ision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large multimodal model (GPT-4V) referenced as a baseline: a strong vision-and-language model that, per the paper, lacks explicit embodied (3D control) training and thus serves as a comparison point.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A large multimodal model with vision and language capabilities referenced in the paper as a baseline; the model is not trained within LEGENT's embodied setting and therefore lacks direct supervision for continuous control outputs used in the 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>multimodal vision+language pretraining (image-text and language data) as a general LMM (explicit dataset details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Compared on Come Here and Where Is embodied tasks (used as a non-embodied baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same LEGENT tasks used in prototype experiments; GPT-4V is used as a baseline to illustrate that an LMM without embodied fine-tuning is less capable at producing the required control outputs in the LEGENT environment.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language and image tokens; not trained to output LEGENT continuous motor control tokens in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A within paper experiments — GPT-4V lacks direct embodied-action outputs in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>N/A — paper reports GPT-4V lacks embodied training/mapping and is used as a comparison (no mapping instantiation described).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Vision + language modalities (as a multimodal model), but no direct embodied sensorimotor grounding was provided in the baseline evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>N/A in LEGENT — the paper notes GPT-4V's strong vision-language ability but absence of embodied training limits its performance on control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Lack of explicit embodied-control supervision and mismatch between model outputs (language tokens) and required continuous control outputs; no task-specific fine-tuning on egocentric action-labeled trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LEGENT-trained VILA-based models outperform GPT-4V on the evaluated embodied tasks, indicating that multimodal models without embodied fine-tuning do not effectively transfer to continuous 3D control tasks in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGENT: Open Platform for Embodied Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1705.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1705.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based motion planner (code-as-policies)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-written intermediate code instantiated as motion planners (LLM-based motion planner / code-as-policies style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline used in LEGENT where LLMs generate intermediate, annotation-oriented code (e.g., find(object_id), speak(text)) from scene descriptions and task prompts; these code tokens are instantiated as motion planners which compute low-level continuous controls (pathfinding, visibility checks) to produce ground-truth trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LLM-based motion planner (intermediate code -> instantiated motion planners)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>LLMs generate structured intermediate code sequences representing high-level subroutines; each subroutine is realized as a motion planner or executor (e.g., pathfinding to a target, visibility checking) that computes continuous control actions such as rotations and forward motions; used to synthesize egocentric observation-action trajectories for supervised training of LMMs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>large language model pretraining on natural language and (potentially) code corpora (the paper uses general LLMs for code generation; exact pretraining corpora are not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Trajectory generation for LEGENT tasks (e.g., 'Where Is' object-finding and 'Come Here' navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Used to generate step-by-step trajectories: the LLM creates intermediate code commands from textual scene/state descriptions; instantiated planners execute in the simulated 3D environment producing sequences of egocentric observations and continuous control actions until subtask completion (e.g., until the target object enters the agent's field of view).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level intermediate code tokens and instructions (e.g., find(object_id), speak("text"), put(A on B), goto(target)), representing semantic actions at the planner level.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Concrete continuous control commands computed by motion planners: e.g., rotate_right(angle), move_forward(distance), interact(), speak(text).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Explicit two-stage mapping: (1) LLM maps natural language task and state description -> intermediate code tokens (annotation-oriented); (2) each code token is instantiated as a motion planner (pathfinding with navigation mesh, visibility checks, inverse-kinematics for interactions) that computes low-level continuous controls to execute in the 3D environment. LEGENT records the resulting visual observations and low-level controls as trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Access to internal environment state for code-writing and planner instantiation; egocentric images are recorded as observations for the dataset; planners use environment geometry (navigation mesh), object identifiers, and visibility information.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Used to generate datasets of 1k and 10k trajectories for training the VILA-based VLA models in prototype experiments; the LLM-to-planner pipeline produced the paired supervision used for model adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>LLM ability to produce structured, annotation-oriented code from textual scene descriptions; deterministic instantiation of planners that compute optimal or near-optimal controls; full access to internal simulation state enabling accurate ground-truth labeling at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Dependency on internal state and engineered instantiation of planners (not end-to-end learning from raw egocentric inputs); planners may not capture sensorimotor noise or real-world dynamics unless explicitly modeled; generated data and planners reflect simulator assumptions which may limit real-world transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using LLMs to emit intermediate code and instantiating those tokens as motion planners provides an effective and scalable way to generate high-quality ground-truth embodied trajectories (egocentric observations paired with continuous control actions) for supervised training of LMM-based embodied agents; this approach sidesteps expensive manual annotation and avoids unstable reward engineering for large-model training, enabling the creation of datasets (e.g., 1k and 10k trajectories) sufficient for prototype transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGENT: Open Platform for Embodied Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1705.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1705.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2: Vision-language-action models transfer web knowledge to robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work (Brohan et al., 2023) that studies transferring web-scale vision-language knowledge into robotic control (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-2: Vision-language-action models transfer web knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A vision-language-action model reported to transfer web knowledge to robotic control; mentioned in related work as an example of LMMs applied to embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>web-scale vision and language data (as described in the RT-2 paper; LEGENT cites the work but does not detail its pretraining corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic control tasks (as in RT-2 original work; LEGENT only references RT-2 in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Robotic control domains where RT-2 applies its transferred vision-language knowledge to generate or supervise control behaviors (LEGENT does not use RT-2 experimentally).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level language-conditioned action semantics (details in RT-2 paper; not specified in LEGENT).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Robotic controls suitable for manipulation and navigation (details not provided in LEGENT).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an example of prior work that transfers vision-language models to embodied/robotic control; LEGENT positions itself as providing open infrastructure and data-generation pipelines to support similar lines of work at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGENT: Open Platform for Embodied Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1705.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1705.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E: An embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced embodied multimodal language model (PaLM-E / 'Palm-e') cited in related work that integrates language models for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm-e: An embodied multimodal language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PaLM-E (Palm-e)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A multimodal language model applied to embodied tasks and robotics (cited in related work); LEGENT references PaLM-E as part of prior literature connecting LLMs/LMMs and embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>multimodal pretraining combining language and perception (paper does not detail datasets here).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic / embodied control tasks (as in PaLM-E original work; LEGENT only references it).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>General embodied/robotic control benchmarks described in PaLM-E; LEGENT does not run PaLM-E experiments directly.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior work that brings language model capabilities into embodied control; LEGENT emphasizes open platform and data pipelines to enable similar or broader experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGENT: Open Platform for Embodied Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>Palm-e: An embodied multimodal language model. <em>(Rating: 2)</em></li>
                <li>Vila: On pretraining for visual language models. <em>(Rating: 2)</em></li>
                <li>Code as policies: Language model programs for embodied control. <em>(Rating: 2)</em></li>
                <li>Creating multimodal interactive agents with imitation and self-supervised learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1705",
    "paper_id": "paper-269449539",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "VILA-7B (VILA-based VLA)",
            "name_full": "VILA-7B (backbone) / VILA-based Vision-Language-Action model (VLA)",
            "brief_description": "A visual-language foundation model (VILA-7B) used as the backbone to build a vision-language-action model that is fine-tuned on LEGENT-generated embodied trajectories (variants: VILA-Sep and VILA-Joint).",
            "citation_title": "Vila: On pretraining for visual language models.",
            "mention_or_use": "use",
            "model_agent_name": "VILA-based Vision-Language-Action (VLA) model (VILA-7B backbone; VILA-Sep / VILA-Joint variants)",
            "model_agent_description": "An LMM (VILA-7B) backbone adapted into a vision-language-action model that ingests interleaved egocentric images and text (task descriptions and previous obs/actions) and is supervised to output unified code-like continuous control actions and textual responses; trained via supervised fine-tuning on datasets of generated trajectories from LEGENT. Two fine-tuning variants are reported: VILA-Sep (task-specific fine-tuning per task) and VILA-Joint (joint fine-tuning on both tasks).",
            "pretraining_data_type": "visual-language pretraining (image-text multimodal pretraining / LMM pretraining)",
            "pretraining_data_details": null,
            "embodied_task_name": "Come Here (navigation) and Where Is (embodied question answering)",
            "embodied_task_description": "3D embodied tasks in the LEGENT environment: 'Come Here' requires navigating (single-room and two-room variants) to a user/location; 'Where Is' requires searching the scene and answering an object-location question (single-room training, two-room held-out generalization test). Environments are realistic, interactive indoor scenes with egocentric vision and object interaction.",
            "action_space_text": "During LMM pretraining: interleaved language and image tokens (no native embodied action tokens); pretraining did not provide direct embodied-action supervision in LEGENT.",
            "action_space_embodied": "Continuous control actions exposed by LEGENT, e.g. move_forward(distance in meters), rotate(degrees), interact()/grab/put/open/close(object_id), speak(text); actions are expressed as continuous values for movement and rotation plus discrete interaction/speech commands.",
            "action_mapping_method": "Supervised fine-tuning: the VILA backbone is trained to map textual task descriptions plus interleaved observation-action history to the unified action-code outputs used in LEGENT (i.e., learning a direct mapping from perception+language tokens to control token sequences). The paper uses LEGENT-generated trajectories as paired supervision (observations -&gt; ground-truth actions).",
            "perception_requirements": "Egocentric RGB visual observations (the prototype uses the observation at the end of each continuous action as the image input) and natural language task descriptions; no depth or explicit object-detection sensors were required in the reported prototype.",
            "transfer_successful": true,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Prototype experiments used generated training sets of 1k and 10k trajectories for the initial tasks (Come Here 1-room/2-room and Where Is 1-room); evaluation used 100 trajectories per task. Exact numeric learning curves or thresholds are not reported in the paper.",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Compatibility of VILA with interleaved image-text inputs; LMM's prior visual-language knowledge that can be adapted via supervised fine-tuning; availability of large-scale, paired egocentric-image + action trajectories generated by LEGENT that closely match the model's input-output format.",
            "transfer_failure_factors": "Prototype limitations include using only end-of-action frames (no full video modeling), relatively small-scale training in experiments (1k/10k trajectories only reported), omission of body-joint control (reducing embodied richness), and potential perception gaps between LMM pretraining distributions and egocentric embodied visual observations.",
            "key_findings": "A VILA backbone fine-tuned on LEGENT-generated image-text-action trajectories can be adapted to 3D embodied navigation and embodied QA tasks and outperforms a multimodal baseline (GPT-4V) that lacks embodied training; provisioning of matched interleaved supervision and continuous control tokenization enables end-to-end mapping from perception+language to control, and limited-scale pretraining + supervised adaptation yields measurable generalization to some unseen settings (e.g., two-room QA) though detailed numeric gains and sample-efficiency improvements are not fully quantified in the prototype.",
            "uuid": "e1705.0",
            "source_info": {
                "paper_title": "LEGENT: Open Platform for Embodied Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V(ision)",
            "brief_description": "A large multimodal model (GPT-4V) referenced as a baseline: a strong vision-and-language model that, per the paper, lacks explicit embodied (3D control) training and thus serves as a comparison point.",
            "citation_title": "Gpt-4 technical report.",
            "mention_or_use": "mention",
            "model_agent_name": "GPT-4V",
            "model_agent_description": "A large multimodal model with vision and language capabilities referenced in the paper as a baseline; the model is not trained within LEGENT's embodied setting and therefore lacks direct supervision for continuous control outputs used in the 3D tasks.",
            "pretraining_data_type": "multimodal vision+language pretraining (image-text and language data) as a general LMM (explicit dataset details not provided in this paper).",
            "pretraining_data_details": null,
            "embodied_task_name": "Compared on Come Here and Where Is embodied tasks (used as a non-embodied baseline)",
            "embodied_task_description": "Same LEGENT tasks used in prototype experiments; GPT-4V is used as a baseline to illustrate that an LMM without embodied fine-tuning is less capable at producing the required control outputs in the LEGENT environment.",
            "action_space_text": "Natural language and image tokens; not trained to output LEGENT continuous motor control tokens in the experiments.",
            "action_space_embodied": "N/A within paper experiments — GPT-4V lacks direct embodied-action outputs in this study.",
            "action_mapping_method": "N/A — paper reports GPT-4V lacks embodied training/mapping and is used as a comparison (no mapping instantiation described).",
            "perception_requirements": "Vision + language modalities (as a multimodal model), but no direct embodied sensorimotor grounding was provided in the baseline evaluation.",
            "transfer_successful": false,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "N/A in LEGENT — the paper notes GPT-4V's strong vision-language ability but absence of embodied training limits its performance on control tasks.",
            "transfer_failure_factors": "Lack of explicit embodied-control supervision and mismatch between model outputs (language tokens) and required continuous control outputs; no task-specific fine-tuning on egocentric action-labeled trajectories.",
            "key_findings": "LEGENT-trained VILA-based models outperform GPT-4V on the evaluated embodied tasks, indicating that multimodal models without embodied fine-tuning do not effectively transfer to continuous 3D control tasks in this setting.",
            "uuid": "e1705.1",
            "source_info": {
                "paper_title": "LEGENT: Open Platform for Embodied Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM-based motion planner (code-as-policies)",
            "name_full": "LLM-written intermediate code instantiated as motion planners (LLM-based motion planner / code-as-policies style)",
            "brief_description": "A pipeline used in LEGENT where LLMs generate intermediate, annotation-oriented code (e.g., find(object_id), speak(text)) from scene descriptions and task prompts; these code tokens are instantiated as motion planners which compute low-level continuous controls (pathfinding, visibility checks) to produce ground-truth trajectories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "LLM-based motion planner (intermediate code -&gt; instantiated motion planners)",
            "model_agent_description": "LLMs generate structured intermediate code sequences representing high-level subroutines; each subroutine is realized as a motion planner or executor (e.g., pathfinding to a target, visibility checking) that computes continuous control actions such as rotations and forward motions; used to synthesize egocentric observation-action trajectories for supervised training of LMMs.",
            "pretraining_data_type": "large language model pretraining on natural language and (potentially) code corpora (the paper uses general LLMs for code generation; exact pretraining corpora are not specified).",
            "pretraining_data_details": null,
            "embodied_task_name": "Trajectory generation for LEGENT tasks (e.g., 'Where Is' object-finding and 'Come Here' navigation)",
            "embodied_task_description": "Used to generate step-by-step trajectories: the LLM creates intermediate code commands from textual scene/state descriptions; instantiated planners execute in the simulated 3D environment producing sequences of egocentric observations and continuous control actions until subtask completion (e.g., until the target object enters the agent's field of view).",
            "action_space_text": "High-level intermediate code tokens and instructions (e.g., find(object_id), speak(\"text\"), put(A on B), goto(target)), representing semantic actions at the planner level.",
            "action_space_embodied": "Concrete continuous control commands computed by motion planners: e.g., rotate_right(angle), move_forward(distance), interact(), speak(text).",
            "action_mapping_method": "Explicit two-stage mapping: (1) LLM maps natural language task and state description -&gt; intermediate code tokens (annotation-oriented); (2) each code token is instantiated as a motion planner (pathfinding with navigation mesh, visibility checks, inverse-kinematics for interactions) that computes low-level continuous controls to execute in the 3D environment. LEGENT records the resulting visual observations and low-level controls as trajectories.",
            "perception_requirements": "Access to internal environment state for code-writing and planner instantiation; egocentric images are recorded as observations for the dataset; planners use environment geometry (navigation mesh), object identifiers, and visibility information.",
            "transfer_successful": true,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Used to generate datasets of 1k and 10k trajectories for training the VILA-based VLA models in prototype experiments; the LLM-to-planner pipeline produced the paired supervision used for model adaptation.",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "LLM ability to produce structured, annotation-oriented code from textual scene descriptions; deterministic instantiation of planners that compute optimal or near-optimal controls; full access to internal simulation state enabling accurate ground-truth labeling at scale.",
            "transfer_failure_factors": "Dependency on internal state and engineered instantiation of planners (not end-to-end learning from raw egocentric inputs); planners may not capture sensorimotor noise or real-world dynamics unless explicitly modeled; generated data and planners reflect simulator assumptions which may limit real-world transfer.",
            "key_findings": "Using LLMs to emit intermediate code and instantiating those tokens as motion planners provides an effective and scalable way to generate high-quality ground-truth embodied trajectories (egocentric observations paired with continuous control actions) for supervised training of LMM-based embodied agents; this approach sidesteps expensive manual annotation and avoids unstable reward engineering for large-model training, enabling the creation of datasets (e.g., 1k and 10k trajectories) sufficient for prototype transfer.",
            "uuid": "e1705.2",
            "source_info": {
                "paper_title": "LEGENT: Open Platform for Embodied Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2: Vision-language-action models transfer web knowledge to robotic control",
            "brief_description": "A referenced work (Brohan et al., 2023) that studies transferring web-scale vision-language knowledge into robotic control (cited in related work).",
            "citation_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control.",
            "mention_or_use": "mention",
            "model_agent_name": "RT-2",
            "model_agent_description": "A vision-language-action model reported to transfer web knowledge to robotic control; mentioned in related work as an example of LMMs applied to embodied control.",
            "pretraining_data_type": "web-scale vision and language data (as described in the RT-2 paper; LEGENT cites the work but does not detail its pretraining corpora).",
            "pretraining_data_details": null,
            "embodied_task_name": "Robotic control tasks (as in RT-2 original work; LEGENT only references RT-2 in related work).",
            "embodied_task_description": "Robotic control domains where RT-2 applies its transferred vision-language knowledge to generate or supervise control behaviors (LEGENT does not use RT-2 experimentally).",
            "action_space_text": "High-level language-conditioned action semantics (details in RT-2 paper; not specified in LEGENT).",
            "action_space_embodied": "Robotic controls suitable for manipulation and navigation (details not provided in LEGENT).",
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Mentioned as an example of prior work that transfers vision-language models to embodied/robotic control; LEGENT positions itself as providing open infrastructure and data-generation pipelines to support similar lines of work at scale.",
            "uuid": "e1705.3",
            "source_info": {
                "paper_title": "LEGENT: Open Platform for Embodied Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PaLM-E",
            "name_full": "PaLM-E: An embodied multimodal language model",
            "brief_description": "A referenced embodied multimodal language model (PaLM-E / 'Palm-e') cited in related work that integrates language models for embodied tasks.",
            "citation_title": "Palm-e: An embodied multimodal language model.",
            "mention_or_use": "mention",
            "model_agent_name": "PaLM-E (Palm-e)",
            "model_agent_description": "A multimodal language model applied to embodied tasks and robotics (cited in related work); LEGENT references PaLM-E as part of prior literature connecting LLMs/LMMs and embodied control.",
            "pretraining_data_type": "multimodal pretraining combining language and perception (paper does not detail datasets here).",
            "pretraining_data_details": null,
            "embodied_task_name": "Robotic / embodied control tasks (as in PaLM-E original work; LEGENT only references it).",
            "embodied_task_description": "General embodied/robotic control benchmarks described in PaLM-E; LEGENT does not run PaLM-E experiments directly.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Cited as prior work that brings language model capabilities into embodied control; LEGENT emphasizes open platform and data pipelines to enable similar or broader experiments.",
            "uuid": "e1705.4",
            "source_info": {
                "paper_title": "LEGENT: Open Platform for Embodied Agents",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control.",
            "rating": 2,
            "sanitized_title": "rt2_visionlanguageaction_models_transfer_web_knowledge_to_robotic_control"
        },
        {
            "paper_title": "Palm-e: An embodied multimodal language model.",
            "rating": 2,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "Vila: On pretraining for visual language models.",
            "rating": 2,
            "sanitized_title": "vila_on_pretraining_for_visual_language_models"
        },
        {
            "paper_title": "Code as policies: Language model programs for embodied control.",
            "rating": 2,
            "sanitized_title": "code_as_policies_language_model_programs_for_embodied_control"
        },
        {
            "paper_title": "Creating multimodal interactive agents with imitation and self-supervised learning.",
            "rating": 1,
            "sanitized_title": "creating_multimodal_interactive_agents_with_imitation_and_selfsupervised_learning"
        }
    ],
    "cost": 0.01687825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LEGENT: Open Platform for Embodied Agents
11 Aug 2024</p>
<p>Zhili Cheng 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Jinyi Hu 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Zhitong Wang 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Shengding Hu 
Department of Computer Science and Technology
Tsinghua University</p>
<p>An Liu 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Yuge Tu 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Pengkai Li 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Lei Shi 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Zhiyuan Liu 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Maosong Sun 
Department of Computer Science and Technology
Tsinghua University</p>
<p>LEGENT: Open Platform for Embodied Agents
11 Aug 2024F503161F41F6D6CE6699FD76C763F0C0arXiv:2404.18243v2[cs.CL]
Figure 1: Interaction with the embodied agent in LEGENT.These sequential interactions showcase the agent's ability to answer the user's questions and follow the user's instructions.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) (Brown et al., 2020;Achiam et al., 2023;Touvron et al., 2023a,b) and Large Multimodal Models (LMMs) (OpenAI, 2023;Team et al., 2023;Liu et al., 2024;Hu et al., 2024) present inspiring capabilities in understanding and generating human-like text and realistic lack of open-source access to these environments and datasets restricts open-source community-wide progress in this field.Therefore, the academic community urgently requires an open platform that facilitates the integration of language grounding in embodied environments and schemes to generate large-scale training data for embodied agents based on LLMs and LMMs.</p>
<p>Towards this aspiration, we introduce LEGENT, an open and user-friendly platform that enables scalable training of embodied agents based on LLMs and LMMs.LEGENT contains two parts.First, it provides a 3D embodied environment with the following features: (1) Diverse, realistic, and interactive scenes; (2) Human-like agents with egocentric vision capable of executing actions and engaging in direct language interaction with users;</p>
<p>(3) User-friendly interface offering comprehensive support for researchers unfamiliar with 3D environments.Second, LEGENT builds a systematic data generation pipeline for both scene generation and agent behavior, incorporating state-of-the-art algorithms for scene creation (Deitke et al., 2022;Yang et al., 2023b) and trajectory generation.In this way, extensive and diverse trajectories of agent behavior with egocentric visual observations and corresponding actions can be generated at scale for embodied agent training.</p>
<p>To demonstrate the potential of LEGENT, we train a basic vision-language-action model based on LMMs with generated data on two tasks: navigation and embodied question answering.The model processes textual and egocentric visual input and produces controls and textual responses directly.The prototype model outperforms GPT-4V (OpenAI, 2023), which lacks training in an embodied setting.The generalization experiment reveals the LEGENT-trained model's ability to generalize to unseen settings.LEGENT platform and its documentation are publicly available at https://legent.ai.</p>
<p>Related Work</p>
<p>Embodied Environment.Embodied environments are extensively utilized in games (Johnson et al., 2016;Oh et al., 2016;Beattie et al., 2016) and robotics (Kolve et al., 2017;Yan et al., 2018;Xia et al., 2018;Gan et al., 2020;Li et al., 2021;Puig et al., 2023a), with a primary focus on visual AI and reinforcement learning.Some platform focuses on specific embodied tasks, such as ma-nipulation (Yu et al., 2020;Makoviychuk et al., 2021), navigation (Chang et al., 2017;Dosovitskiy et al., 2017), or planning-oriented agents (Puig et al., 2018;Shridhar et al., 2020;Wang et al., 2022).However, the environment setups and data frameworks of existing platforms fall short in accommodating the training of LMMs.LMMs excel in the supervised learning paradigm and necessitate diverse and large-scale data to integrate embodied capability.Existing platforms are not yet ready to scale, including: the primarily supported reinforcement learning methods require careful reward engineering, the diversity of the training data cannot be easily expanded, and collecting data for imitation learning on these platforms requires manual effort.</p>
<p>LMMs-based Embodied Agent.Noteworthy studies have concentrated on developing embodied models capable of end-to-end operation, as demonstrated in the works of Reed et al. (2022); Brohan et al. (2023);Belkhale et al. (2024).However, the datasets and models in these studies are not publicly available.</p>
<p>Scene Generation.Scene generation has demonstrated significant effectiveness in training embodied agents by ProcTHOR (Deitke et al., 2022).Compared to employing manually crafted rules used in ProcTHOR, recent studies (Wen et al., 2023;Yang et al., 2023b;Feng et al., 2024) leverage prior knowledge of LLMs and propose algorithms to generate diverse, high-quality scenes.</p>
<p>Agent Trajectory Generation.Some research focuses on crafting reward functions to guide small policy models (Yu et al., 2023;Xian et al., 2023;Wang et al., 2023;Ma et al., 2023).However, there will be huge costs and instability when applying reward-based training to large foundation models.Meanwhile, pioneering efforts have been made in code generation for robotics (Liang et al., 2023;Singh et al., 2023;Vemprala et al., 2023;Huang et al., 2023) and trajectory generation for imitation learning (Garrett et al., 2021;Kamath et al., 2023;Dalal et al., 2023).These efforts align with our approach to generating large-scale embodied trajectories for training LMMs.</p>
<p>LEGENT</p>
<p>In this section, we introduce our platform LEG-ENT.The design of LEGENT involves scene, agent, and interface.All three components are specially tailored for the integration of LLMs and LMMs, and ensure scalability.</p>
<p>Scene</p>
<p>The design of the scenes in LEGENT emphasizes interactivity and diversity, striving for a versatile and scalable environment that enriches the training of embodied agents for wide application.Realistic Physics.LEGENT provides a realtime simulation that closely mirrors real-world physics based on game engines.It supports realistic effects like gravity, friction, and collision dynamics, improving agents' embodied comprehension or aiding the development of generative world simulators (Yang et al., 2023a).</p>
<p>Diverse Rendering.LEGENT introduces another facet of generalization via diverse rendering.Unlike the fixed stylized renderings in games and the emphasis on photorealism in robotics, LEG-ENT integrates these styles by customizing the rendering functions, which allows easy transitions between rendering styles to accommodate different requirements for flexible usage.</p>
<p>Interactable Objects.In LEGENT, both agents and users can manipulate various fully interactable 3D objects, which enables actions such as picking up, transporting, positioning, and handing over these objects.Additionally, the environment supports interaction with dynamic structures, such as doors and drawers.We anticipate that the scope of these dynamic structures will be significantly broadened through the application of generative methods (Chen et al., 2023).</p>
<p>Scalable Assets.LEGENT supports importing customized objects at runtime, including user-supplied 3D objects, objects from existing datasets (Deitke et al., 2023) and those created by generative models (Siddiqui et al., 2023;Wang et al., 2024), as illustrated in Fig. 3.We choose glTF as the import format for its openness and broad compatibility.This feature grants users the flexibility to customize the scene by strategically placing these assets or integrating them seamlessly into scene generation algorithms.</p>
<p>Agent</p>
<p>The agent is designed with two criteria: emulating human interactions and compatibility with LMMs.</p>
<p>Egocentric Observations.Following the previous study for interactive embodied agents (Team et al., 2021), the agent is equipped with egocentric vision.The egocentric vision is captured by mounting a camera on the agent's head.</p>
<p>Language Interaction.Users and agents can communicate with each other in natural language in LEGENT.Grounding language within the environment has the potential to connect the extensive knowledge in LLMs and LMMs with embodied experience.</p>
<p>Generalizable Actions.Agents in LEGENT are capable of performing a range of actions, including navigation, object manipulation, and communication.Regarding the instantiation of actions, existing literature can be broadly categorized into two types: executable plans (Puig et al., 2018;Shridhar et al., 2020) and control (Kolve et al., 2017;Savva et al., 2019).In executable plans, actions are expressed through sub-steps to complete a task, such as "walk towards apple 1", which depends on internal states and annotations for execution, or requires an additional neural executor module compatible with a planning module (Driess et al., 2023).Control, on the other hand, refers to the action expression like "move forward 1 meter, rotate to the right 30 degrees", which is considered more generalizable.In LEGENT, we use control, targeting generalizing to new environments including real-world settings.The learned actions can be integrated with diverse actuators with the least additional effort.</p>
<p>Another important action design is allowing the agent to execute continuous actions such as moving forward across a continuous distance, as opposed to moving in a grid-by-grid manner.This design offers two advantages for LMMs: (1) It minimizes the inference cost of LMMs by eliminating the need for constant frame-by-frame inference.(2) It addresses the issue of minimal information gain observed when an agent moves incrementally in a stepwise manner, a process that creates less effective data for training large models.This design draws parallels to the use of keyframes in video processing and making direct training of autoregressive LMMs (Alayrac et al., 2022;Awadalla et al., 2023;Lin et al., 2024) feasible.Specifically, the actions currently supported in LEGENT are shown in Table 1.Considering the current capability of LMMs, LEGENT temporarily omits the complex control of agents' body joints.Adding these degrees of freedom to allow more flexible action will be explored in the future.</p>
<p>Realistic Animation.LEGENT features precise humanoid animations using inverse kinematics and spatial algorithms, enabling lifelike movements, as shown in Fig. 4. It is important for enhancing nonverbal interactions in AI systems and contributes to robotic control and text-to-motion research.Also, when combined with egocentric vision, it offers a cost-effective alternative for immersive experiences similar to Ego4D (Grauman et al., 2022), which requires a huge cost to collect.</p>
<p>Interface</p>
<p>Our platform offers a user-friendly interface for researchers to integrate LLMs and LMMs with the embodied environment easily, with little need for expertise in 3D environments.Detailed guidance is available in our documentation.</p>
<p>Playable Interaction.The user interface of LEGENT is designed to be as intuitive as playing a video game with the agent within the environment, utilizing just a keyboard and mouse for navigation and interaction.This interface facilitates straight- forward visual debugging and qualitative analysis and simplifies the process of conducting hands-on demonstrations.</p>
<p>Simple Code.LEGENT is equipped with a Python toolkit to enable the interaction between the agent and the environment.The coding interface of our Python toolkit is simple, with concise code examples available in our documentation.</p>
<p>Scene Generation Interface.Our platform incorporates various scene-generation techniques.Currently, we support methods including procedural generation and LLM-based generation.We provide a straightforward JSON format for specifying a scene, enabling users to easily develop their own scene generation methods.</p>
<p>Agent Trajectory Generation Interface.We offer an agent trajectory generation interface specifically designed for training LMMs.Using this interface, users can create training datasets that consist of egocentric visual records and corresponding ground truth actions paired with task instructions or queries, as elaborated in Section 4.3.</p>
<p>Hardware Requirements.LEGENT is crossplatform.It can run effortlessly on personal computers without demanding particular prerequisites or complex setups, and it facilitates connections to remote servers for training and deployment, thus enhancing its accessibility.</p>
<p>Data Generation</p>
<p>The second part of LEGENT is a scalable data generation pipeline.It aims at exhaustively exploiting the inherent supervision from simulated worlds and supporting large-scale training of general-purpose embodied agents.Here we elaborate on the implementation of our data generation framework.</p>
<p>Scene Generation</p>
<p>Scene generation offers agents with diverse embodied experiences.LEGENT has currently integrated two scene generation methods: (1) Procedure generation efficiently creates large-scale scenes.(2) Language-guided generation captures the semantics of textual queries and leverages common sense knowledge to optimize spatial layouts.</p>
<p>Procedural Generation.We utilize the procedural generation algorithm created by Proc-THOR (Deitke et al., 2022), designed to create realistic indoor scenes at scale by integrating prior knowledge of object placement and spatial relationships.The implementation process starts with drafting a house layout, followed by the placement of large furniture, and ends with the arrangement of small objects.During the process, spatial algorithms are used to prevent object overlap and ensure precise placement.We provide an interface that allows users to input specific conditions for object occurrence and placement, enabling the generation of scenes tailored to specific tasks.In addition, instead of employing human annotators as previous work does, we utilize LLMs for asset annotation, establishing an efficient automatic asset annotation pipeline that facilitates future asset expansion.</p>
<p>Language Guided Generation.We implement methods in Holodeck (Yang et al., 2023b) into LEGENT and offer an LLM-powered interface to generate single or multi-room indoor scenes given any natural language query.This process resembles procedural generation but is driven by LLMs instead of human-written programs.Instead of using the depth-first-search solver in Holodeck, we ask LLMs to determine the exact locations of doors and floor objects, granting LLMs more control over the room layout.Collision detection is used to prevent interference between objects during generation.</p>
<p>Task Generation</p>
<p>We create diverse tasks expressed in language paired with specific scenes, thereby contextualizing each task within the environment.We employ the following two strategies for task generation.</p>
<p>Task Generation for Given Scenes.In this strategy, we serialize the generated scenes into a detailed textual description and present it to LLMs with crafted instructions.LLMs assume the role of human users, generating a variety of tasks.This approach is especially effective for generating diverse tasks automatically.</p>
<p>Scene Generation for Given Tasks.This approach efficiently generates large-scale samples for specific tasks based on the scene generation algorithm.For instance, when the task involves querying an object's location, the algorithm generates a scene that includes the object and its receptacle, inherently creating question-answering annotations.As shown in Table 2, we provide some basic task templates that are ideal for creating large-scale scenes, which are particularly useful for pretraining fundamental capabilities of embodied control, spatial comprehension, and basic language grounding across diverse scenes.</p>
<p>Trajectory Generation</p>
<p>Trajectories for training embodied agents comprise continuous sequences of egocentric observations and actions.The main challenge lies in accurately determining ground-truth actions for each step.</p>
<p>We use LLMs and motion planners to label the ground truth actions.Inspired by pioneering works in code generation for robotics, we utilize LLMs to write intermediate codes from provided state descriptions and instructions.These codes are instantiated as motion planners, designed to calculate the optimal actions at each step given the internal states of the environment.Each motion planner operates in a step-by-step manner in the environment, with visual observations collected during the process.This approach is consistent with the concept of Task and Motion Planning (TAMP) (Garrett et al., 2021) in robotics, where the LLMs and the motion planners respectively fulfill the functions of task planning and motion planning.</p>
<p>We demonstrate this process using an example task "Where is the orange?".As shown in Figure 6, to finish the task, the agent needs to search the room and answer the question.LLMs map the task to the appropriate code usage, determine the object identifier of the orange in the scene, and recognize its placement from the state description, thereby generating the following intermediate code:</p>
<p>1 find (36) # object identifier of orange 2 speak ( " It 's on the sofa .")</p>
<p>Note that the code-writing is annotation-oriented.Even though LLMs can directly answer the question from the state description, it still invokes "find".Then the code "find" is instantiated as a motion planner that utilizes pathfinding algorithms (Hart et al., 1968) incorporating visibility checks.The pathfinding algorithm calculates the waypoints of the shortest path from the agent to the target object using a navigation mesh.The motion planner then calculates the controls of the agent to navigate along these waypoints.For instance, in the first observation shown in Figure 6, the agent needs to rotate 59 degrees to the left to orient to the next waypoint, resulting in the action "rotate_right(-59)".Similarly, in the second observation, the agent needs to perform certain actions to move to the subsequent waypoint.This motion planner concludes when the target object enters the agent's field of view.LEGENT records visual observations and actions during this process as a trajectory, which can be exported as a video or an image-text interleaved sequence.The actions use a unified code representation, compatible with the outputs of LMMs.</p>
<p>Similar to "find", each intermediate code is designed with the ability to generate optimal controls using the internal world states.In addition, each task template mentioned in Section 4.2 is equipped with intermediate code templates, as shown in Table 2, eliminating the need for LLMs in large-scale data generation for specific tasks.</p>
<p>Prototype Experiments</p>
<p>We conduct a prototype experiment to assess the utility of generated data on two embodied tasks: "Come Here" for navigation and "Where Is" for embodied question answering (Das et al., 2018).Task complexity varied from navigating in one room to the more intricate two rooms.We generate 1k and 10k trajectories for the initial three tasks ("Come Here" in one or two rooms and "Where Is" in one room) and assess the models on 100 trajectories across all four tasks.The "Where Is" task in the two-room setting serves as a generalization test, which is not included in the training data.</p>
<p>Due to the lack of powerful video understanding models, we temporarily only use the observation at the end of each continuous action, formulating one trajectory as an image-text interleaved sequence.We utilize VILA-7B (Lin et al., 2024) as our backbone due to its capability in interleaved inputs.As illustrated in Fig. 7, we train the vision-languageaction (VLA) model to predict current action based on task descriptions and interleaved context of previous observations and actions,.</p>
<p>The results presented in igational skills developed from the "Come Here" task in a two-room environment generalize well to the untrained task scenario, enhancing the model's ability to navigate in two rooms for the embodied question answering task.We leave the exploration of more large-scale training in the future work.</p>
<p>Demo of LEGENT</p>
<p>The demo video of LEGENT is available at the link1 , which is partially shown in Fig. 1.The demonstration exemplifies the engagement with embodied agents in LEGENT, primarily leveraging LLMs and motion planners described in Section 4.3.With advancements in LMMs' capability of egocentric perception and control, we foresee the evolution of this demonstration into a fully embodied experience, independent of any extra internal information.We will also pursue this goal by further the data generation for model training.</p>
<p>Conclusion and Future Work</p>
<p>In this work, we present LEGENT, an open platform for developing embodied agents, focusing on integrating LMMs with scalable embodied training.By bridging the gap between embodied AI and LMM's development, we hope LEGENT inspires research in this field.We are committed to the ongoing development of LEGENT, making it more scalable and user-friendly.In our future releases, we prioritize: (1) Building a more diverse data generation pipeline.</p>
<p>(2) Scaling model training.</p>
<p>(3) Unifying humanoid animation with robotic control and refining the physics to make actions more applicable to the real world.(4) Improving scene generation and integrating text-to-3D and image-to-3D methods to support more diverse and realistic scenes.</p>
<p>Figure 2 :
2
Figure 2: Features of LEGENT.</p>
<p>Figure 3 :
3
Figure 3: Examples of importing external assets: usersupplied assets (left); existing datasets (middle); assets generated by generative models (right).</p>
<p>Figure 4 :
4
Figure 4: An example of humanoid animations, demonstrating accurate object grasping and body movement through spatial planning and inverse kinematics.</p>
<p>Figure 5 :
5
Figure 5: Examples of generated scenes.</p>
<p>goto(a) target(a) interact() bring me A goto(a) target(a) interact() goto_user() where is A find(a) speak(C) put A on B goto(a) target(a) interact() goto(b) target(b) interact()</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: A generated trajectory for task "Where is the orange".The actions for the three observations are: 1. rotate_right(-59); 2. move_forward(1.2),rotate_right(-35); 3. speak("It's on the sofa.").</p>
<p>Table 1 :
1
Move forward by a specified distance.Rotate<em> Adjust the view horizontally or vertically.Interact Grab, put, open, or close targeted objects.List of actions in LEGENT.</em> means the action is continuous (meters or degrees).
Actions DescriptionSpeakSend a message.Move*</p>
<p>Table 2 :
2
Currenly provided task templates and intermediate code templates.A is the object's name, and a is the object's environment identifier.C denotes the name of the receptacle on which a is placed.</p>
<p>Table 3
3lead to sev-</p>
<p>Table 3 :
3
Success rates on two embodied tasks.VILA-Sep denotes models fine-tuned separately for each task, whereas VILA-Joint refers to models trained jointly on both tasks.* means generalization test.</p>
<p>https://video.legent.ai</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in neural information processing systems. 202235</p>
<p>Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Yonatan Kalyani Marathe, Samir Bitton, Shiori Gadre, Sagawa, arXiv:2308.01390Openflamingo: An open-source framework for training large autoregressive vision-language models. 2023arXiv preprint</p>
<p>Dhruv Batra, X Angel, Sonia Chang, Andrew J Chernova, Jia Davison, Vladlen Deng, Sergey Koltun, Jitendra Levine, Igor Malik, Roozbeh Mordatch, Mottaghi, arXiv:2011.01975Rearrangement: A challenge for embodied ai. 2020arXiv preprint</p>
<p>. Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, arXiv:1612.038012016Deepmind lab. arXiv preprint</p>
<p>Tianli Suneel Belkhale, Ted Ding, Pierre Xiao, Quon Sermanet, Jonathan Vuong, Yevgen Tompson, Chebotar, arXiv:2403.01823Debidatta Dwibedi, and Dorsa Sadigh. 2024. Rt-h: Action hierarchies using language. arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, arXiv:1709.06158Matter-port3d: Learning from rgb-d data in indoor environments. 2017arXiv preprint</p>
<p>Urdformer: Constructing interactive realistic scenes from real images via simulation and generative modeling. Qiuyu Chen, Marius Memmel, Alex Fang, Aaron Walsman, Dieter Fox, Abhishek Gupta, Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@ CoRL2023. 2023</p>
<p>Murtaza Dalal, Ajay Mandlekar, Caelan Garrett, Ankur Handa, Ruslan Salakhutdinov, Dieter Fox, arXiv:2305.16309Imitating task and motion planning with visuomotor transformers. 2023arXiv preprint</p>
<p>Embodied question answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Objaverse: A universe of annotated 3d objects. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli Vanderbilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Procthor: Large-scale embodied ai using procedural generation. Matt Deitke, Eli Vanderbilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Advances in Neural Information Processing Systems. 202235Aniruddha Kembhavi, and Roozbeh Mottaghi</p>
<p>Carla: An open urban driving simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Conference on robot learning. PMLR2017</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378Palm-e: An embodied multimodal language model. 2023arXiv preprint</p>
<p>Layoutgpt: Compositional visual planning and generation with large language models. Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin , Eric Wang, William Yang, Wang , Advances in Neural Information Processing Systems. 202436</p>
<p>Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, arXiv:2007.04954Threedworld: A platform for interactive multi-modal physical simulation. 2020arXiv preprint</p>
<p>Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Tom Silver, Leslie Pack Kaelbling, Tomás Lozano-Pérez, Integrated task and motion planning. Annual review of control, robotics, and autonomous systems. 20214</p>
<p>Iqa: Visual question answering in interactive environments. Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Ego4d: Around the world in 3,000 hours of egocentric video. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>The symbol grounding problem. Stevan Harnad, Physica D: Nonlinear Phenomena. 421-31990</p>
<p>A formal basis for the heuristic determination of minimum cost paths. Nils J Peter E Hart, Bertram Nilsson, Raphael, IEEE transactions on Systems Science and Cybernetics. 421968</p>
<p>Large multilingual models pivot zero-shot multimodal learning across languages. Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Zhiyuan Liu, Maosong Sun, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>The malmo platform for artificial intelligence experimentation. Matthew Johnson, Katja Hofmann, Tim Hutton, David Bignell, Ijcai. 201616</p>
<p>A new path: Scaling vision-and-language navigation with synthetic instructions and imitation learning. Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu Koh, Alexander Ku, Austin Waters, Yinfei Yang, Jason Baldridge, Zarana Parekh, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-Derbilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, arXiv:1712.05474Ai2-thor: An interactive 3d environment for visual ai. 2017arXiv preprint</p>
<ol>
<li>igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. Chengshu Li, Fei Xia, Roberto Martín-Martín, Michael Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, arXiv:2108.03272arXiv preprint</li>
</ol>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Vila: On pretraining for visual language models. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, Song Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 202436</p>
<p>Jason Yecheng, William Ma, Guanzhi Liang, De-An Wang, Osbert Huang, Dinesh Bastani, Yuke Jayaraman, Zhu, arXiv:2310.12931Linxi Fan, and Anima Anandkumar. 2023. Eureka: Human-level reward design via coding large language models. arXiv preprint</p>
<p>Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>
<p>Control of memory, active perception, and action in minecraft. Junhyuk Oh, Valliappa Chockalingam, Honglak Lee, PMLR. team OpenAI. 2023International conference on machine learning. 2016Gpt-4v(ision) system card</p>
<p>. Xavi Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Ruslan Partsey, Jimmy Yang, Ruta Desai, Alexander William Clegg, Michal Hlavac, Tiffany Min, Theo Gervet, Vladimir Vondrus, Vincent-Pierre Berges, John Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi2023aDevendra Singh ChaplotHabitat 3.0: A co-habitat for humans, avatars and robots</p>
<p>Virtualhome: Simulating household activities via programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, arXiv:2310.13724So Yeon Min, et al. 2023b. Habitat 3.0: A co-habitat for humans, avatars and robots. arXiv preprint</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, arXiv:2205.06175A generalist agent. 2022arXiv preprint</p>
<p>Habitat: A platform for embodied ai research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.03768Alfworld: Aligning text and embodied environments for interactive learning. 2020arXiv preprint</p>
<p>Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, Matthias Nießner, arXiv:2311.15475Meshgpt: Generating triangle meshes with decoder-only transformers. 2023arXiv preprint</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Creating multimodal interactive agents with imitation and self-supervised learning. Deepmind Interactive, Agents Team, Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Mansi Gupta, arXiv:2112.037632021arXiv preprint</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor, arXiv:2306.17582Chatgpt for robotics: Design principles and model abilities. 2023arXiv preprint</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, arXiv:2203.07540Scienceworld: Is your agent smarter than a 5th grader?. 2022arXiv preprint</p>
<p>Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, Chuang Gan, arXiv:2311.014552023arXiv preprint</p>
<p>Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, arXiv:2403.05034Crm: Single image to 3d textured mesh with convolutional reconstruction model. Jun Zhu. 2024arXiv preprint</p>
<p>Zehao Wen, Zichen Liu, Srinath Sridhar, Rao Fu, arXiv:2312.06644Anyhome: Open-vocabulary generation of structured and textured 3d homes. 2023arXiv preprint</p>
<p>Gibson env: Real-world perception for embodied agents. Fei Xia, Zhiyang Amir R Zamir, Alexander He, Jitendra Sax, Silvio Malik, Savarese, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Theophile Zhou Xian, Zhenjia Gervet, Yi-Ling Xu, Tsun-Hsuan Qiao, Yian Wang, Wang, arXiv:2305.10455Towards generalist robots: A promising paradigm via generative simulation. 2023arXiv preprint</p>
<p>Claudia Yan, Dipendra Misra, Andrew Bennnett, Aaron Walsman, Yonatan Bisk, Yoav Artzi, arXiv:1801.07357Chalet: Cornell house agent learning environment. 2018arXiv preprint</p>
<p>Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, arXiv:2310.06114Dale Schuurmans, and Pieter Abbeel. 2023a. Learning interactive real-world simulators. arXiv preprint</p>
<p>Yue Yang, Fan-Yun Sun, Luca Weihs, Eli Vanderbilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, arXiv:2312.09067Holodeck: Language guided generation of 3d embodied ai environments. 2023barXiv preprint</p>
<p>Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, arXiv:2306.11565Homerobot: Openvocabulary mobile manipulation. 2023arXiv preprint</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, Conference on robot learning. PMLR2020</p>
<p>Language to rewards for robotic skill synthesis. Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Lewis Hao-Tien, Chiang, arXiv:2306.08647Jan Humplik, et al. 2023Tom Erez, Leonard HasencleverarXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
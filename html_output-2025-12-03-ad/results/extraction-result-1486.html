<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1486 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1486</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1486</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-9a7f6b5ec7b4b7669abb13400ef6a2e719c8205d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9a7f6b5ec7b4b7669abb13400ef6a2e719c8205d" target="_blank">A Graphical Multi-Fidelity Gaussian Process Model, with Application to Emulation of Heavy-Ion Collisions</a></p>
                <p><strong>Paper Venue:</strong> Technometrics</p>
                <p><strong>Paper TL;DR:</strong> A new Graphical Multi-fidelity Gaussian Process (GMGP) model is proposed, which embeds this DAG structure (capturing scientific dependencies) within a Gaussian process framework and admits a scalable algorithm for recursive computation of the posterior mean and variance along at each depth level of the DAG.</p>
                <p><strong>Paper Abstract:</strong> Abstract With advances in scientific computing and mathematical modeling, complex scientific phenomena such as galaxy formations and rocket propulsion can now be reliably simulated. Such simulations can however be very time-intensive, requiring millions of CPU hours to perform. One solution is multi-fidelity emulation, which uses data of different fidelities to train an efficient predictive model which emulates the expensive simulator. For complex scientific problems and with careful elicitation from scientists, such multi-fidelity data may often be linked by a directed acyclic graph (DAG) representing its scientific model dependencies. We thus propose a new Graphical Multi-fidelity Gaussian Process (GMGP) model, which embeds this DAG structure (capturing scientific dependencies) within a Gaussian process framework. We show that the GMGP has desirable modeling traits via two Markov properties, and admits a scalable algorithm for recursive computation of the posterior mean and variance along at each depth level of the DAG. We also present a novel experimental design methodology over the DAG given an experimental budget, and propose a nonlinear extension of the GMGP via deep Gaussian processes. The advantages of the GMGP are then demonstrated via a suite of numerical experiments and an application to emulation of heavy-ion collisions, which can be used to study the conditions of matter in the Universe shortly after the Big Bang. The proposed model has broader uses in data fusion applications with graphical structure, which we further discuss.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1486.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1486.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heavy-ion multi-stage simulator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heavy-ion collision multi-stage simulator for quark-gluon plasma</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-stage scientific simulator for modeling quark-gluon plasma formation in heavy-ion collisions, where each stage offers multiple model choices (some more accurate/expensive than others), producing a combinatorial set of simulators of varying fidelity used as inputs to the GMGP emulator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Heavy-ion collision multi-stage simulator (quark-gluon plasma)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Three-stage pipeline simulator for quark-gluon plasma; for each stage researchers can select one of several models, producing many composite simulation strategies (e.g., A1+B3+C2) with differing computational cost and accuracy; high-fidelity selections are computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>nuclear physics / high-energy physics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Multi-stage: high-fidelity defined as a particular combination of more-accurate (and time-consuming) stage models (e.g., A1+B3+C2); lower-fidelity approximations are produced by choosing faster/simpler models at one or more stages.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Fidelity varies by choice of model at each stage (some models capture more physics/resolution and cost millions of CPU hours per run; others are simplified and much cheaper). Different composite simulators cannot necessarily be totally ordered by fidelity because accuracy may vary by stage.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GMGP / r-GMGP / d-GMGP (emulator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gaussian-process-based multi-fidelity emulator embedding the simulator dependency DAG (GMGP), a recursive computational variant (r-GMGP), and a nonlinear deep-GP extension (d-GMGP) to model nonlinear links between simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Emulate and predict high-fidelity simulator outputs (observables of heavy-ion collisions / quark-gluon plasma) from limited runs across multiple lower- and higher-fidelity composite simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Predict outputs of the expensive high-fidelity heavy-ion simulator (i.e., emulate high-fidelity behavior using cheaper lower-fidelity simulator data)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Qualitative: GMGP (using the full multi-stage DAG) produced improved predictive accuracy over a standard high-fidelity GP and a KO-path (sequence-based) multi-fidelity model in the paper's numerical experiments and heavy-ion application (see Fig. 2 and Sections 5–6); no numeric metrics reported in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparing using only a KO-path (training along a single path of simulators) versus integrating the full DAG, the GMGP that incorporates the DAG structure and additional lower-fidelity codes yields better predictive performance; KO-path and standard GP performed worse when the multi-stage codes are not totally orderable.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper argues that leveraging lower-fidelity simulations in a DAG can reduce the number of expensive high-fidelity runs needed and proposes nested experimental designs, but it does not state a concrete minimal fidelity requirement; emphasizes using domain elicitation to choose appropriate lower-fidelity simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Kennedy–O'Hagan (KO) style sequence models fail when simulators cannot be ordered by fidelity (KO-path that ignores some simulators yields poorer predictions); imposing artificial total order or ignoring simulators can degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graphical Multi-Fidelity Gaussian Process Model, with Application to Emulation of Heavy-Ion Collisions', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1486.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1486.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T-cell adhesion simulator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T-cell receptor adhesion state simulators (resting and upregulated xTCR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Biological cell-adhesion simulators modeling two T-cell receptor states (resting and upregulated xTCR) that can be run at varying fidelity levels; mentioned as an application where GMGP could apply to reduce computational costs of simulation-based studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>T-cell adhesion simulator (resting and xTCR states)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulations modeling adhesion dynamics of T-cell receptors under two receptor states (resting and xTCR); simulations can be computationally intensive and may be run at multiple fidelity levels per state.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / cellular adhesion (biophysics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Varied: practitioners may run each receptor-state simulation at different fidelity levels (some more detailed and costly, others simplified and faster); no detailed fidelity taxonomy provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Not specified in detail; implied differences are in model sophistication or computational resolution that trade accuracy for speed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GMGP (proposed emulator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graphical multi-fidelity Gaussian process that would embed dependency structure between the two receptor-state simulators and leverage lower-fidelity runs to emulate high-fidelity outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Emulate adhesion simulation outputs across receptor states and fidelities to enable accurate prediction with fewer expensive simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Higher-fidelity simulator outputs / more accurate biological predictions while reducing high-fidelity runs</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graphical Multi-Fidelity Gaussian Process Model, with Application to Emulation of Heavy-Ion Collisions', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1486.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1486.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cosmological N-body simulator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cosmological N-body simulation (dark matter fluid evolution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>N-body cosmological simulator that evolves a dark-matter fluid under gravity; fidelity controlled by number of macro-particles and spatial resolution, producing multi-fidelity simulation choices that can be represented in a DAG and emulated by GMGP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Cosmological N-body simulator</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulates dark matter dynamics using macro-particles evolving under gravity; fidelity controlled by particle count and spatial resolution, with higher-fidelity runs using more particles and finer resolution (costly).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astrophysics / cosmology</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity: many macro-particles and high spatial resolution; Low/medium-fidelity: fewer particles or coarser resolution approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Fidelity determined by discretization (particle number) and spatial resolution; lower-fidelity approximations reduce resolution and sampling of small-scale structure.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GMGP (proposed emulator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graphical multi-fidelity Gaussian process that can exploit stage-wise fidelity controls (e.g., particle count/resolution) to emulate high-fidelity outputs efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Predict high-resolution cosmological simulation outputs (e.g., large-scale structure observables) using mixed-fidelity simulation data.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Emulate/approximate the high-fidelity N-body simulator output using data from lower-fidelity runs</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper notes that fidelity can be controlled by particle count/resolution but does not specify minimal fidelity required for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graphical Multi-Fidelity Gaussian Process Model, with Application to Emulation of Heavy-Ion Collisions', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1486.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1486.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CFD (DNS vs RANS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computational Fluid Dynamics simulators: Direct Numerical Simulation (DNS) and Reynolds-Averaged Navier–Stokes (RANS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DNS is a high-fidelity Navier–Stokes solver resolving all turbulent scales, while RANS is a lower-fidelity turbulence-averaged model; the paper cites this as an example where nonlinear dependencies between fidelity levels motivate the d-GMGP extension.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Computational fluid dynamics simulators (DNS and RANS)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>DNS: high-fidelity direct numerical solution of Navier–Stokes capturing full eddies and vortices and requiring very fine resolution; RANS: lower-fidelity averaged turbulence models that are computationally cheaper and approximate small-scale turbulence.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>fluid dynamics / computational fluid dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>DNS = high-fidelity (resolves full turbulence spectrum); RANS = lower-fidelity (turbulence-averaged, simplified physics).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>DNS captures small-scale eddies and vortical structures; RANS uses averaged equations with turbulence closures, omitting detailed unsteady small-scale features; dependency between RANS->DNS is highly nonlinear.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>d-GMGP (deep GMGP recommended)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep Gaussian process extension of GMGP (d-GMGP) designed to capture nonlinear relationships between lower-fidelity (RANS) and higher-fidelity (DNS) simulators within a DAG framework.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Model and emulate the nonlinear mapping from RANS (cheap) outputs and inputs to DNS (expensive) outputs to predict high-fidelity flow features with fewer DNS runs.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Transfer/emulate DNS-level outputs using RANS and intermediate fidelity data</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Paper recommends using d-GMGP when dependencies are known to be highly nonlinear (e.g., DNS vs RANS) but does not report empirical fidelity-comparison metrics in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Graphical Multi-Fidelity Gaussian Process Model, with Application to Emulation of Heavy-Ion Collisions', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Everett et al. (2021b) <em>(Rating: 2)</em></li>
                <li>Sung et al. (2020) <em>(Rating: 2)</em></li>
                <li>Ho et al. (2022) <em>(Rating: 2)</em></li>
                <li>Perdikaris et al. (2017) <em>(Rating: 2)</em></li>
                <li>Kennedy and O'Hagan (2000) <em>(Rating: 1)</em></li>
                <li>Le Gratiet and Garnier (2014) <em>(Rating: 1)</em></li>
                <li>Konomi and Karagiannis (2021) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1486",
    "paper_id": "paper-9a7f6b5ec7b4b7669abb13400ef6a2e719c8205d",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Heavy-ion multi-stage simulator",
            "name_full": "Heavy-ion collision multi-stage simulator for quark-gluon plasma",
            "brief_description": "A multi-stage scientific simulator for modeling quark-gluon plasma formation in heavy-ion collisions, where each stage offers multiple model choices (some more accurate/expensive than others), producing a combinatorial set of simulators of varying fidelity used as inputs to the GMGP emulator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Heavy-ion collision multi-stage simulator (quark-gluon plasma)",
            "simulator_description": "Three-stage pipeline simulator for quark-gluon plasma; for each stage researchers can select one of several models, producing many composite simulation strategies (e.g., A1+B3+C2) with differing computational cost and accuracy; high-fidelity selections are computationally expensive.",
            "scientific_domain": "nuclear physics / high-energy physics",
            "fidelity_level": "Multi-stage: high-fidelity defined as a particular combination of more-accurate (and time-consuming) stage models (e.g., A1+B3+C2); lower-fidelity approximations are produced by choosing faster/simpler models at one or more stages.",
            "fidelity_characteristics": "Fidelity varies by choice of model at each stage (some models capture more physics/resolution and cost millions of CPU hours per run; others are simplified and much cheaper). Different composite simulators cannot necessarily be totally ordered by fidelity because accuracy may vary by stage.",
            "model_or_agent_name": "GMGP / r-GMGP / d-GMGP (emulator)",
            "model_description": "Gaussian-process-based multi-fidelity emulator embedding the simulator dependency DAG (GMGP), a recursive computational variant (r-GMGP), and a nonlinear deep-GP extension (d-GMGP) to model nonlinear links between simulators.",
            "reasoning_task": "Emulate and predict high-fidelity simulator outputs (observables of heavy-ion collisions / quark-gluon plasma) from limited runs across multiple lower- and higher-fidelity composite simulators.",
            "training_performance": null,
            "transfer_target": "Predict outputs of the expensive high-fidelity heavy-ion simulator (i.e., emulate high-fidelity behavior using cheaper lower-fidelity simulator data)",
            "transfer_performance": "Qualitative: GMGP (using the full multi-stage DAG) produced improved predictive accuracy over a standard high-fidelity GP and a KO-path (sequence-based) multi-fidelity model in the paper's numerical experiments and heavy-ion application (see Fig. 2 and Sections 5–6); no numeric metrics reported in excerpt.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Comparing using only a KO-path (training along a single path of simulators) versus integrating the full DAG, the GMGP that incorporates the DAG structure and additional lower-fidelity codes yields better predictive performance; KO-path and standard GP performed worse when the multi-stage codes are not totally orderable.",
            "minimal_fidelity_discussion": "Paper argues that leveraging lower-fidelity simulations in a DAG can reduce the number of expensive high-fidelity runs needed and proposes nested experimental designs, but it does not state a concrete minimal fidelity requirement; emphasizes using domain elicitation to choose appropriate lower-fidelity simulators.",
            "failure_cases": "Kennedy–O'Hagan (KO) style sequence models fail when simulators cannot be ordered by fidelity (KO-path that ignores some simulators yields poorer predictions); imposing artificial total order or ignoring simulators can degrade performance.",
            "uuid": "e1486.0",
            "source_info": {
                "paper_title": "A Graphical Multi-Fidelity Gaussian Process Model, with Application to Emulation of Heavy-Ion Collisions",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "T-cell adhesion simulator",
            "name_full": "T-cell receptor adhesion state simulators (resting and upregulated xTCR)",
            "brief_description": "Biological cell-adhesion simulators modeling two T-cell receptor states (resting and upregulated xTCR) that can be run at varying fidelity levels; mentioned as an application where GMGP could apply to reduce computational costs of simulation-based studies.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "simulator_name": "T-cell adhesion simulator (resting and xTCR states)",
            "simulator_description": "Simulations modeling adhesion dynamics of T-cell receptors under two receptor states (resting and xTCR); simulations can be computationally intensive and may be run at multiple fidelity levels per state.",
            "scientific_domain": "biology / cellular adhesion (biophysics)",
            "fidelity_level": "Varied: practitioners may run each receptor-state simulation at different fidelity levels (some more detailed and costly, others simplified and faster); no detailed fidelity taxonomy provided.",
            "fidelity_characteristics": "Not specified in detail; implied differences are in model sophistication or computational resolution that trade accuracy for speed.",
            "model_or_agent_name": "GMGP (proposed emulator)",
            "model_description": "Graphical multi-fidelity Gaussian process that would embed dependency structure between the two receptor-state simulators and leverage lower-fidelity runs to emulate high-fidelity outputs.",
            "reasoning_task": "Emulate adhesion simulation outputs across receptor states and fidelities to enable accurate prediction with fewer expensive simulations.",
            "training_performance": null,
            "transfer_target": "Higher-fidelity simulator outputs / more accurate biological predictions while reducing high-fidelity runs",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": null,
            "failure_cases": null,
            "uuid": "e1486.1",
            "source_info": {
                "paper_title": "A Graphical Multi-Fidelity Gaussian Process Model, with Application to Emulation of Heavy-Ion Collisions",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Cosmological N-body simulator",
            "name_full": "Cosmological N-body simulation (dark matter fluid evolution)",
            "brief_description": "N-body cosmological simulator that evolves a dark-matter fluid under gravity; fidelity controlled by number of macro-particles and spatial resolution, producing multi-fidelity simulation choices that can be represented in a DAG and emulated by GMGP.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "simulator_name": "Cosmological N-body simulator",
            "simulator_description": "Simulates dark matter dynamics using macro-particles evolving under gravity; fidelity controlled by particle count and spatial resolution, with higher-fidelity runs using more particles and finer resolution (costly).",
            "scientific_domain": "astrophysics / cosmology",
            "fidelity_level": "High-fidelity: many macro-particles and high spatial resolution; Low/medium-fidelity: fewer particles or coarser resolution approximations.",
            "fidelity_characteristics": "Fidelity determined by discretization (particle number) and spatial resolution; lower-fidelity approximations reduce resolution and sampling of small-scale structure.",
            "model_or_agent_name": "GMGP (proposed emulator)",
            "model_description": "Graphical multi-fidelity Gaussian process that can exploit stage-wise fidelity controls (e.g., particle count/resolution) to emulate high-fidelity outputs efficiently.",
            "reasoning_task": "Predict high-resolution cosmological simulation outputs (e.g., large-scale structure observables) using mixed-fidelity simulation data.",
            "training_performance": null,
            "transfer_target": "Emulate/approximate the high-fidelity N-body simulator output using data from lower-fidelity runs",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper notes that fidelity can be controlled by particle count/resolution but does not specify minimal fidelity required for transfer.",
            "failure_cases": null,
            "uuid": "e1486.2",
            "source_info": {
                "paper_title": "A Graphical Multi-Fidelity Gaussian Process Model, with Application to Emulation of Heavy-Ion Collisions",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "CFD (DNS vs RANS)",
            "name_full": "Computational Fluid Dynamics simulators: Direct Numerical Simulation (DNS) and Reynolds-Averaged Navier–Stokes (RANS)",
            "brief_description": "DNS is a high-fidelity Navier–Stokes solver resolving all turbulent scales, while RANS is a lower-fidelity turbulence-averaged model; the paper cites this as an example where nonlinear dependencies between fidelity levels motivate the d-GMGP extension.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "simulator_name": "Computational fluid dynamics simulators (DNS and RANS)",
            "simulator_description": "DNS: high-fidelity direct numerical solution of Navier–Stokes capturing full eddies and vortices and requiring very fine resolution; RANS: lower-fidelity averaged turbulence models that are computationally cheaper and approximate small-scale turbulence.",
            "scientific_domain": "fluid dynamics / computational fluid dynamics",
            "fidelity_level": "DNS = high-fidelity (resolves full turbulence spectrum); RANS = lower-fidelity (turbulence-averaged, simplified physics).",
            "fidelity_characteristics": "DNS captures small-scale eddies and vortical structures; RANS uses averaged equations with turbulence closures, omitting detailed unsteady small-scale features; dependency between RANS-&gt;DNS is highly nonlinear.",
            "model_or_agent_name": "d-GMGP (deep GMGP recommended)",
            "model_description": "Deep Gaussian process extension of GMGP (d-GMGP) designed to capture nonlinear relationships between lower-fidelity (RANS) and higher-fidelity (DNS) simulators within a DAG framework.",
            "reasoning_task": "Model and emulate the nonlinear mapping from RANS (cheap) outputs and inputs to DNS (expensive) outputs to predict high-fidelity flow features with fewer DNS runs.",
            "training_performance": null,
            "transfer_target": "Transfer/emulate DNS-level outputs using RANS and intermediate fidelity data",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "Paper recommends using d-GMGP when dependencies are known to be highly nonlinear (e.g., DNS vs RANS) but does not report empirical fidelity-comparison metrics in the excerpt.",
            "minimal_fidelity_discussion": null,
            "failure_cases": null,
            "uuid": "e1486.3",
            "source_info": {
                "paper_title": "A Graphical Multi-Fidelity Gaussian Process Model, with Application to Emulation of Heavy-Ion Collisions",
                "publication_date_yy_mm": "2021-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Everett et al. (2021b)",
            "rating": 2
        },
        {
            "paper_title": "Sung et al. (2020)",
            "rating": 2
        },
        {
            "paper_title": "Ho et al. (2022)",
            "rating": 2
        },
        {
            "paper_title": "Perdikaris et al. (2017)",
            "rating": 2
        },
        {
            "paper_title": "Kennedy and O'Hagan (2000)",
            "rating": 1
        },
        {
            "paper_title": "Le Gratiet and Garnier (2014)",
            "rating": 1
        },
        {
            "paper_title": "Konomi and Karagiannis (2021)",
            "rating": 1
        }
    ],
    "cost": 0.013716999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A graphical multi-fidelity Gaussian process model, with application to emulation of heavy-ion collisions</h1>
<p>Yi $\mathrm{Ji}^{1}$, Simon $\mathrm{Mak}^{1}$, Derek Soeder ${ }^{2}$, J-F Paquet ${ }^{2,3}$, and Steffen A. Bass ${ }^{2}$<br>${ }^{1}$ Department of Statistical Science, Duke University<br>${ }^{2}$ Department of Physics, Duke University<br>${ }^{3}$ Department of Physics and Astronomy \&amp; Department of Mathematics, Vanderbilt University</p>
<p>February 29, 2024</p>
<h4>Abstract</h4>
<p>With advances in scientific computing and mathematical modeling, complex scientific phenomena such as galaxy formations and rocket propulsion can now be reliably simulated. Such simulations can however be very time-intensive, requiring millions of CPU hours to perform. One solution is multi-fidelity emulation, which uses data of different fidelities to train an efficient predictive model which emulates the expensive simulator. For complex scientific problems and with careful elicitation from scientists, such multi-fidelity data may often be linked by a directed acyclic graph (DAG) representing its scientific model dependencies. We thus propose a new Graphical Multi-fidelity Gaussian Process (GMGP) model, which embeds this DAG structure (capturing scientific dependencies) within a Gaussian process framework. We show that the GMGP has desirable modeling traits via two Markov properties, and admits a scalable algorithm for recursive computation of the posterior mean and variance along at each depth level of the DAG. We also present a novel experimental design methodology over the DAG given an experimental budget, and propose a nonlinear extension of the GMGP via deep Gaussian processes. The advantages of the GMGP are then demonstrated via a suite of numerical experiments and an application to emulation of heavy-ion collisions, which can be used to study the conditions of matter in the Universe shortly after the Big Bang. The proposed model has broader uses in data fusion applications with graphical structure, which we further discuss.</p>
<p>Keywords: Computer experiments, Gaussian processes, graphical models, nuclear physics, multi-fidelity modeling.</p>
<h1>1 Introduction</h1>
<p>With breakthroughs in scientific computing, computer simulations are quickly replacing physical experiments in modern scientific and engineering problems. These simulations allow scientists to better understand complex scientific problems which may be prohibitively expensive or infeasible for full-scale physical experimentation. This shift to computer experimentation has found success in exciting applications, including cell adhesion simulation (Sung et al., 2020) and rocket design (Mak et al., 2018). Such computer experiments, however, can demand a hefty price in computing resources, requiring millions of CPU hours per run. One solution is emulation (Santner et al., 2019; Gramacy, 2020): a handful of simulations are first run at carefully chosen design points, then an emulator model is fit to efficiently predict the expensive computer simulator. A popular emulator is the Gaussian process (GP) model (Gramacy, 2020), which allows for closed-form expressions for prediction and uncertainty quantification.</p>
<p>As systems become more realistic and complex, computer experiments also become increasingly more expensive, and thus the simulation data needed to train an accurate emulator can be difficult to generate. One way to address this is multi-fidelity emulation. The idea is to collect data from the "high-fidelity" simulator, which is computationally expensive but provides a detailed representation of the modeled science, as well as data from "lower-fidelity" simulators, which make simplifying assumptions on the modeled science but can be performed quickly. One then fits a multi-fidelity emulator using all training data to predict the output from the high-fidelity simulator. The key advantage of multifidelity emulation is that, by leveraging information from cheaper lower-fidelity simulations to enhance predictions for the high-fidelity model, one can train a predictive model with much fewer high-fidelity simulations and thereby lower computational costs.</p>
<p>The development of multi-fidelity emulators is an active research area. A popular framework is the Kennedy-O'Hagan (KO) model (Kennedy and O'Hagan, 2000), which models a sequence of computer simulations from lowest to highest fidelity using a sequence</p>
<p>of GP models linked by a linear autoregressive framework. The KO multi-fidelity model has been applied to a wide range of scientific problems, such as materials science (Pilania et al., 2017) and aerodynamics (López-Lopera et al., 2021). Further developments of the KO model include Le Gratiet and Garnier (2014), which introduced a recursive computation of the predictive mean and variance given nested designs, and Konomi and Karagiannis (2021), which extended this model for non-nested designs and non-stationary responses via a recursive Monte Carlo surrogate based on the Student $t$-process. Ma et al. (2022) explored an empirical Bayes implementation of this $t$-process using Monte Carlo expectation-maximization. Perdikaris et al. (2017) proposed a flexible, nonlinear extension of the KO model using deep GPs (Damianou and Lawrence, 2013).</p>
<p>There is, however, a key limitation for the above methods: they presume the multifidelity data can be ranked from lowest to highest fidelity. This may not be the case in complex scientific problems. Take, e.g., the heavy-ion collision framework in Everett et al. (2021b) for simulating the quark-gluon plasma, the state of nuclear matter that once filled the Universe shortly after the Big Bang, and that can now be produced and explored in collisions of heavy nuclei. This multi-stage simulation consists of three stages, as shown in Fig. 1. For each stage, the physicists choose one of several potential models (some more accurate but time-consuming, others less accurate but quick), resulting in many ways to perform the full plasma simulation. Here, it is difficult to rank different simulation strategies in a sequence from lowest to highest fidelity, since some may be more accurate for one stage but less accurate for another. For example, a physicist may choose the combination $A_{1}+B_{3}+C_{2}$ as the high-fidelity simulator $H$ for a study, and use $A_{1}+B_{1}+C_{2}$ and $A_{1}+B_{2}+C_{1}$ as two lower-fidelity approximations $L_{1}$ and $L_{2}$. To apply existing models, one may have to either (i) ignore data from certain simulators to achieve an ordering from lowest to highest fidelity, or (ii) impose an artificial ordering which is not justified by the science. As we shall see later, both approaches do not make full use of the underlying multi-fidelity structure, and thus may not achieve good predictive performance given a</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Visualizing the multi-stage multi-fidelity simulator for the quark-gluon plasma.
limited computing budget.
To address this, we present a new Graphical Multi-fidelity Gaussian process (GMGP) model, which utilizes a directed acyclic graph (DAG) to capture scientific dependencies between simulation models with different fidelities. This DAG structure is elicited via a careful inspection of the scientific models, which we discuss later. The GMGP embeds this DAG within a Gaussian process framework, thus allowing for a more structured and science-driven approach for pooling information from lower-fidelity data for high-fidelity prediction. We show that the GMGP has desirable modeling traits via two Markov properties, and admits an elegant recursive formulation that allows efficient computation of the posterior mean and variance over each depth level of the DAG. We also present a flexible nonlinear extension of the GMGP which leverages deep GPs (Damianou and Lawrence, 2013). Finally, to maximize predictive power, we propose an efficient experimental design framework for allocating multi-fidelity runs over the DAG given a computational budget. Numerical experiments and an application to heavy-ion collisions demonstrate the improved performance of the GMGP over existing multi-fidelity models.</p>
<p>While the GMGP is motivated from our nuclear physics problem, it has broad applications for other modern scientific problems. Take, e.g., the adhesion of T-cell molecules (Sung et al., 2020), which plays an important role in the development of immunotherapy</p>
<p>cancer treatments (Harjunpää et al., 2019). The adhesion of T-cells can be modeled via two states of T-cell receptor (TCR), a resting state and an upregulated xTCR state, both of which can be modeled via computer simulations. Such simulations, however, can be very time-intensive, and a biologist may choose to run each state at varying fidelity levels, similar to the earlier heavy-ion simulator. Another application is the simulation of cosmological $N$-body problems in astrophysics, where a dark matter fluid is evolved via gravitational force. Here, the fidelity of the simulator can be controlled at different stages (Ho et al., 2022), e.g., the number of macro-particles sampled or the resolution of the spatial domain, which again yields a rich multi-stage simulation framework. The GMGP can leverage this multi-stage structure for efficient scientific computing in astrophysics, where multi-fidelity methods have already shown great promise (Ho et al., 2022).</p>
<p>This work can also be viewed through the broader lens of science-driven predictive modeling, which aims to embed known scientific principles as prior knowledge for predictive modeling. In recent years, there has been much development on such predictive models for scientific computing, including the integration of scientific information in the form of shape constraints (Golchi et al., 2015; Wang and Berger, 2016), boundary constraints (Ding et al., 2019), spectral information (Chen et al., 2021), and manifold embeddings (Zhang et al., 2021). Here, the GMGP integrates the DAG dependency structure of the multi-fidelity simulators (i.e., the "science") as prior knowledge within the GP model, which then allows for improved predictive accuracy over existing methods.</p>
<p>We note that the implications of the GMGP extend beyond the multi-fidelity setting into broader data fusion applications where one can elicit a graphical structure connecting data sources. One such application is networked multisensor data fusion (Xia et al., 2009), where data are collected over different nodes on a physical sensor network. Such systems are widely used in manufacturing process monitoring and health care problems. A similar problem is distributed data fusion, where multiple agents collectively infer knowledge about a target process by sensing their local environment; this has broad applications in autonomous cars</p>
<p>and unmanned aerial vehicles (Campbell and Ahmed, 2016).
It should also be noted that, for non-GP-based models, there has been some recent work (Gorodetsky et al., 2020a,b) which integrates DAG structure. These papers focus on linear subspace (e.g., polynomial-based) models, which are popular surrogate models in the applied mathematics literature. Our model has three notable distinctions. First, the GMGP makes use of GPs rather than linear subspace models, which provides greater flexibility and robustness in model specification (Gramacy, 2020). Second, by leveraging error bounds for GP interpolation, we introduce a novel design framework for multi-fidelity experiments which maximizes predictive power given a computational budget. Finally, we provide a scalable and probabilistic framework for modeling nonlinear linkages using deep GPs, via recursive computation of the predictive distribution at each depth level of the DAG. We demonstrate this in a suite of numerical experiments and an application.</p>
<p>The paper is organized as follows. Section 2 provides background and motivation. Section 3 presents the GMGP model, its recursive formulation, and extension for nonlinear model dependencies. Section 4 investigates experimental design approaches. Sections 5 and 6 show the effectiveness of the proposed models in a suite of numerical experiments and an application in heavy-ion collisions. Section 7 concludes the paper.</p>
<h1>2 Background \&amp; Motivation</h1>
<h3>2.1 Gaussian process modeling</h3>
<p>We first provide a brief overview of Gaussian processes (GPs). Let $\boldsymbol{x} \in \Omega \subseteq \mathbb{R}^{d}$ be the input parameters for the computer simulator, and $Z(\boldsymbol{x})$ be its corresponding output. A GP surrogate model places the following prior on the simulation response surface $Z$ :</p>
<p>$$
Z(\cdot) \sim \mathcal{G P}(\mu(\cdot), k(\cdot, \cdot))
$$</p>
<p>Here, $\mu(\boldsymbol{x})=\mathbb{E}[Z(\boldsymbol{x})]$ is the mean function of the GP, and $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\operatorname{Cov}\left[Z(\boldsymbol{x}), Z\left(\boldsymbol{x}^{\prime}\right)\right]$ is its covariance function. Without prior knowledge, $\mu(\cdot)$ is typically set as constant, and</p>
<p>$k(\cdot, \cdot)$ is chosen as the squared-exponential or Matérn kernel (Gramacy, 2020). In what follows, we assume the simulators are deterministic (the standard setting for computer experiments), but the proposed models extend analogously for noisy outputs.</p>
<p>Suppose the simulator is run at inputs $\mathcal{D}=\left{\boldsymbol{x}<em n="n">{1}, \cdots, \boldsymbol{x}</em>}\right}$, yielding outputs $\boldsymbol{z}=$ $\left{Z\left(\boldsymbol{x<em n="n">{1}\right), \cdots, Z\left(\boldsymbol{x}</em>)\right)$, with posterior mean and variance:}\right)\right}$. Conditioning on this data, the predictive distribution of $Z$ at a new input point $\boldsymbol{x}$ becomes $[Z(\boldsymbol{x}) \mid \boldsymbol{z}, \mathcal{D}] \sim \mathcal{N}\left(\mu_{n}(\boldsymbol{x}), \sigma_{n}^{2}(\boldsymbol{x</p>
<p>$$
\begin{aligned}
&amp; \mu_{n}(\boldsymbol{x})=\mu(\boldsymbol{x})+\boldsymbol{k}(\boldsymbol{x}, \mathcal{D})^{T} \boldsymbol{K}(\mathcal{D})^{-1}(\boldsymbol{z}-\boldsymbol{\mu}(\mathcal{D})) \
&amp; \sigma_{n}^{2}(\boldsymbol{x})=k(\boldsymbol{x}, \boldsymbol{x})-\boldsymbol{k}(\boldsymbol{x}, \mathcal{D})^{T} \boldsymbol{K}(\mathcal{D})^{-1} \boldsymbol{k}(\boldsymbol{x}, \mathcal{D})
\end{aligned}
$$</p>
<p>Here, $\boldsymbol{k}(\boldsymbol{x}, \mathcal{D})=\left[k\left(\boldsymbol{x}, \boldsymbol{x}<em n="n">{1}\right), \cdots, k\left(\boldsymbol{x}, \boldsymbol{x}</em>}\right)\right]$ is the vector of covariances, $\boldsymbol{\mu}(\mathcal{D})=\left[\mu\left(\boldsymbol{x<em n="n">{1}\right), \cdots\right.$, $\left.\mu\left(\boldsymbol{x}</em>)$ quantifies its uncertainty.}\right)\right]$ is the vector of means, and $\boldsymbol{K}(\mathcal{D})$ is the covariance matrix for the training data. Equation (1) captures the key advantages of GP-based emulators: the closed-form posterior mean $\mu_{n}(\boldsymbol{x})$ provides an efficient emulator of the expensive computer simulator, and the closed-form posterior variance $\sigma_{n}^{2}(\boldsymbol{x</p>
<h1>2.2 The Kennedy-O'Hagan model</h1>
<p>A popular model for multi-fidelity emulation is the Kennedy-O'Hagan model (Kennedy and O'Hagan, 2000), which models a sequence of computer codes with increasing fidelity via a sequence of linear autoregressive GP models. Let $\left{\boldsymbol{z}<em 2="2">{1}, \boldsymbol{z}</em>}, \cdots, \boldsymbol{z<em t="t">{T}\right}$ denote the data generated by $T$ levels of code sorted in increasing accuracy, where $\boldsymbol{z}</em>}=\left{Z_{t}\left(\boldsymbol{x<em i="1">{i}^{t}\right)\right}</em>$. The KO model assumes the multi-fidelity framework:}^{n_{t}}$ is the data from the $t$-th code $Z_{t</p>
<p>$$
Z_{t}(\boldsymbol{x})=\rho_{t-1} \cdot Z_{t-1}(\boldsymbol{x})+\delta_{t}(\boldsymbol{x}), \quad Z_{t-1}(\boldsymbol{x}) \perp \delta_{t}(\boldsymbol{x}), \quad t=2, \cdots, T
$$</p>
<p>In words, Equation (2) presumes that, prior to data, the response surface $Z_{t}(\boldsymbol{x})$ can be decomposed as the lower-fidelity surface $Z_{t-1}(\boldsymbol{x})$ times a dependency parameter $\rho_{t-1}$, plus some discrepancy function $\delta_{t}(\boldsymbol{x})$ which models the systematic bias between the two computer simulations. For priors, the first (i.e., lowest-fidelity) surface is assigned a GP prior $Z_{1}(\boldsymbol{x}) \sim \mathcal{G P}\left(\boldsymbol{h}<em 1="1">{1}(\boldsymbol{x})^{T} \boldsymbol{\beta}</em>}, \sigma_{1}^{2} r_{1}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right)$, and subsequent discrepancy terms are then assigned independent GP priors $\delta_{t}(\boldsymbol{x}) \sim \mathcal{G P}\left(\boldsymbol{h<em t="t">{t}(\boldsymbol{x})^{T} \boldsymbol{\beta}</em>\right)\right)$. Here, the vectors}, \sigma_{t}^{2} r_{t}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (Left) The five-node $D A G$ used in the 20-dimensional test example. (Right) Predictive performance of the standard high-fidelity GP model (top left), the KO-path model (top right) and the GMGP model (bottom). Red lines mark the high-fidelity (HF) outputs at test inputs, dark blue lines mark the predicted outputs, and gray bands visualize the 95\% predictive intervals.
$\boldsymbol{h}<em T="T">{1}(\boldsymbol{x}), \cdots, \boldsymbol{h}</em>$ are their associated coefficients.}(\boldsymbol{x})$ denote pre-defined basis functions which are used for modeling the prior mean of the GPs, and $\boldsymbol{\beta}_{t</p>
<p>An important development in the KO model is the recursive algorithm proposed by Le Gratiet and Garnier (2014), which reduces computational complexity of the KO model. The key idea is to substitute the GP prior $Z_{t-1}(\boldsymbol{x})$ in Equation (2) by the posterior distribution $Z_{t-1}^{*}(\boldsymbol{x})=\left[Z_{t-1}(\boldsymbol{x}) \mid \boldsymbol{z}<em t-1="t-1">{1}, \cdots, \boldsymbol{z}</em>\right]$, which can be shown to follow a GP. This recursive formulation expresses the predictive mean and variance at level $t$ as functions of the mean and variance at level $t-1$, which allows for reduced computational cost by avoiding the inversion of large covariance matrices. With a nested structure for design points, Le Gratiet and Garnier (2014) showed that this recursive formulation yields the same posterior predictive mean and variance as the original KO model, thus justifying the recursive approach. Further discussion will be provided later in Section 3.</p>
<p>One limitation of the KO model (and its extensions) is that it presumes the training data can be ranked from lowest to highest fidelity. Consider a simple example where this is not the case. We take the $d=20-\mathrm{d}$ test function from Welch et al. (1992) as the highfidelity simulator, and generate four lower-fidelity representations (details in Section 5).</p>
<p>The two medium-fidelity codes ( $M_{1}$ and $M_{2}$ ) are obtained via different simplifications on the high-fidelity code $(H)$, and the two low-fidelity codes $\left(L_{1}\right.$ and $\left.L_{2}\right)$ are obtained by different averaging operations on $M_{1}$. This dependence is captured by the "multi-fidelity DAG" in Fig. 2 (left). Here, the KO model is unable to capture this multi-fidelity structure, since the five simulators cannot be ranked in a sequence. One way to apply the KO model, which we call the "KO-path model", is to train it on data along the longest path $L_{1}, M_{1}$, $H$ (see Fig. 2 (left)); we will explore alternative ways in Section 5.</p>
<p>Fig. 2 (right) shows the performance of the standard GP model trained on 25 design points on $H$, the KO-path model trained on 25 points on $H, 50$ points on $M_{1}$ and 75 points on $L_{1}$, and the proposed GMGP trained on the same data with 50 and 75 additional points on $M_{2}$ and $L_{2}$. Further details on experimental set-up are found in Section 5. Compared to a standard GP, the KO-path model appears to provide slightly improved predictive performance. However, the fitted KO model still exhibits poor predictions over the input space. A natural question is whether we can improve predictive accuracy by integrating the underlying multi-fidelity DAG used for data generation (see Fig. 2 (left)). Fig. 2 (right) answers this in the affirmative: by integrating the multi-fidelity DAG within an appropriate GP model, the proposed GMGP can indeed further improve predictive performance.</p>
<h1>3 The Graphical Multi-fidelity Gaussian Process model</h1>
<h3>3.1 GMGP: model specification</h3>
<p>We now present the proposed GMGP model, which generalizes the KO model by embedding the underlying multi-fidelity DAG (capturing scientific dependencies between simulators) as prior information. As noted in the Introduction, our model can be directly applied for broader data fusion settings where data sources can be linked via an underlying graphical structure. For simplicity, we defer such discussion to Section 7 and focus on the multifidelity setting from our application.</p>
<p>Let $V$ be the set of nodes representing different simulation codes, and suppose there is a root node $T=|V| \in V$ representing the highest-fidelity simulator. Let $E$ be the set of directed edges connecting different simulation codes, where an edge $\left(t^{\prime}, t\right)$ is drawn only if node $t$ is a one-step refinement of node $t^{\prime}$, i.e., $t$ is a higher-fidelity refinement of $t^{\prime}$ with no intermediate codes in between. Let $\mathcal{G}=(V, E)$ be the rooted DAG for this multi-fidelity simulation framework. We will discuss later how this "multi-fidelity DAG" can be elicited from a careful inspection of the simulators.</p>
<p>Let $Z_{t}(\boldsymbol{x})$ be the simulation output at input $\boldsymbol{x}$ from code $t, t \in V$. The GMGP assumes the following modeling framework:</p>
<p>$$
\left{\begin{array}{l}
Z_{t}(\boldsymbol{x})=\sum_{t^{\prime} \in \operatorname{Pa}(t)} \rho_{t^{\prime}}(\boldsymbol{x}) Z_{t^{\prime}}(\boldsymbol{x})+\delta_{t}(\boldsymbol{x}), \quad t \in \overline{V_{S}} \
Z_{t^{\prime}}(\boldsymbol{x}) \perp \delta_{t}(\boldsymbol{x}), \quad t^{\prime} \in \operatorname{Pa}(t)
\end{array}\right.
$$</p>
<p>Here, $V_{S} \subset V$ consists of all source nodes in $\mathcal{G}$ (i.e., nodes with an in-degree of 0 ), $\overline{V_{S}}=$ $V \backslash V_{S}$ contains the remaining non-source nodes, and $\operatorname{Pa}(t)=\left{t^{\prime} \in V:\left(t^{\prime}, t\right) \in E\right}$ consists of all parent nodes of $t \in V$ in the DAG $\mathcal{G}$. Note that source nodes represent simulations with no lower-fidelity representations, and non-source nodes represent simulations with at least one lower-fidelity representation. The function $\rho_{t^{\prime}}(\boldsymbol{x})$ captures dependencies between the output at node $t$ and its lower-fidelity form at node $t^{\prime}$. In the absense of prior information on this dependency linking multi-fidelity models, one can instead adopt $\rho_{t}(\boldsymbol{x})=\rho_{t}$; this is the specification used in later numerical experiments.</p>
<p>We further assign the following GP priors on source nodes:</p>
<p>$$
\left{\begin{array}{l}
Z_{t}(\boldsymbol{x}) \sim \mathcal{G P}\left(\boldsymbol{h}<em t="t">{t}(\boldsymbol{x})^{T} \boldsymbol{\beta}</em> \
Z_{t}(\boldsymbol{x}) \perp Z_{t^{\prime}}(\boldsymbol{x}), \quad t, t^{\prime} \in V_{S}, \quad t \neq t^{\prime}
\end{array}\right.
$$}, \sigma_{t}^{2} r_{t}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right), \quad t \in V_{S</p>
<p>Here, $\boldsymbol{h}<em t="t">{t}(\boldsymbol{x})$ is a vector of basis functions for the response surface mean at node $t$, with $\boldsymbol{\beta}</em>)$ and its lower-fidelity representations, we assign independent GP priors:}$ its coefficients. For the discrepancy term $\delta_{t}(\mathbf{x})$, which captures the systematic difference between $Z_{t}(\mathbf{x</p>
<p>$$
\delta_{t}(\boldsymbol{x}) \sim \mathcal{G} \mathcal{P}\left(\boldsymbol{h}<em t="t">{t}(\boldsymbol{x})^{T} \boldsymbol{\beta}</em>
$$}, \sigma_{t}^{2} r_{t}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right), \quad t \in \overline{V_{S}</p>
<p>Note that this allows for different basis functions at different nodes, which can be specified based on prior information. Without such information, the bases can be set as a constant mean, i.e., $\boldsymbol{h}_{t}(\boldsymbol{x}) \equiv 1$, to avoid variance inflation. Here, the kernels at each node employ different length-scale parameters to allow for model flexibility.</p>
<p>While the above specification may seem involved, the intuition is straight-forward. For every non-source node $t \in \overline{V_{S}}$, its parent nodes $\mathrm{Pa}(t)$ contain simulations which are lowerfidelity representations of simulation $t$. Equation (3) presumes that, prior to data, $Z_{t}(\boldsymbol{x})$ can be decomposed as the weighted sum of its parent (lower-fidelity) simulations, plus a discrepancy term $\delta_{t}(\boldsymbol{x})$ to model systematic bias. The key novelty over the KO model is that, instead of pooling information in a sequence from lowest to highest fidelity, the GMGP can integrate information over a more general DAG structure, which better captures model dependencies between simulations of complex systems. By leveraging this graphical dependency structure guided by the underlying scientific models, the GMGP can enjoy improved predictive performance over the KO model, as we show later.</p>
<p>The proposition below outlines two appealing modeling properties of the GMGP:</p>
<p>Proposition 1 The GMGP model satisfies the following Markov properties:
(a) $Z_{t}(\boldsymbol{x}) \perp Z_{t^{\prime}}(\boldsymbol{x})\left|\left{Z_{j}(\boldsymbol{x})\right}<em t="t">{j \in \operatorname{Pa}(t)}, \quad\right.$ for $t^{\prime} \neq t, t^{\prime} \notin \operatorname{Des}(t), t^{\prime} \notin \operatorname{Pa}(t)$,
(b) $Z</em>$ is a directed in-tree.}(\boldsymbol{x}) \perp Z_{t^{\prime}}\left(\boldsymbol{x}^{\prime}\right)\left|\left{Z_{j}(\boldsymbol{x})\right}_{j \in \operatorname{Pa}(t)}, \quad\right.$ for $t^{\prime} \in \operatorname{Pa}(t), \boldsymbol{x}^{\prime} \neq \boldsymbol{x}, \quad$ if $\mathcal{G</p>
<p>Here, $\operatorname{Des}(t)$ denotes the set of descendant nodes for $t$, i.e., nodes $t^{\prime}$ for which there exists a path from $t$ to $t^{\prime}$.</p>
<p>The proof is provided in the online supplement, and the formal definition of a directed in-tree is provided and justified in the next section. We note that, without the directed in-tree structure, there may be DAGs $\mathcal{G}$ that violate property (b).</p>
<p>Property $(a)$ states that, for a node $t$ with input $\boldsymbol{x}$, its output $Z_{t}(\boldsymbol{x})$ and the output $Z_{t^{\prime}}(\boldsymbol{x})$ at another node $t^{\prime}$ (where $t^{\prime}$ is a non-descendant, non-parent node of $t$ ) are conditionally independent, given the simulation output at the parent nodes $\left{Z_{j}(\boldsymbol{x})\right}_{j \in \operatorname{Pa}(t)}$. In other</p>
<p>words, given knowledge of the simulator at its immediate lower-fidelity (i.e., parent) nodes $\mathrm{Pa}(t)$, the output at any simulator $Z_{t^{\prime}}(\boldsymbol{x})$ which is not a higher-fidelity refinement (i.e., not a descendant) of $t$ yields no additional information for predicting the simulator $Z_{t}(\boldsymbol{x})$ at node $t$. This is an intuitive modeling property if the edges in the DAG $\mathcal{G}$ indeed represent model refinements: at fixed input $\boldsymbol{x}$, simulations which are not higher-fidelity refinements of $t$ should yield little (if any) additional information on $t$ given its closest lower-fidelity representations. Property $(a)$ can be viewed as an extension of the conditional independence property for Bayesian network (Stephenson, 2000). Under a specific form for the multifidelity DAG $\mathcal{G}$ (which we justify in the next section), Property $(b)$ states that, conditioning on $\left{Z_{j}(\boldsymbol{x})\right}<em t="t">{j \in \operatorname{Pa}(t)}$, the simulation output $Z</em>)$ at node $t$. This can be viewed as an extension of the Markov property in O'Hagan (1998), which was used to justify the KO model.}(\boldsymbol{x})$ is independent of the parent outputs $Z_{t^{\prime}}\left(\boldsymbol{x}^{\prime}\right)$ at a different input $\boldsymbol{x}^{\prime}$. In other words, given knowledge of the simulator at its immediate lower-fidelity nodes $\mathrm{Pa}(t)$ with input $\boldsymbol{x}$, the output of such simulators at any other inputs $\boldsymbol{x}^{\prime}$ yields no additional information on predicting the output $Z_{t}(\boldsymbol{x</p>
<p>The modeling framework (3)-(5) can then be used to derive the predictive distribution of the highest-fidelity simulation $Z_{T}(\boldsymbol{x})$. Suppose the model parameters $\Theta=\left{\boldsymbol{\beta}<em t="t">{t}, \sigma</em>)\right}}^{2}\right.$, $\left.\rho_{t}(\boldsymbol{x<em 1="1">{t=1}^{T}$ are fixed (these can be estimated via maximum likelihood or a fully Bayesian approach; more on this later). Conditional on $\boldsymbol{z}^{(T)}=\left{\boldsymbol{z}</em>}, \boldsymbol{z<em T="T">{2}, \cdots, \boldsymbol{z}</em>}\right}$, where $\boldsymbol{z<em t="t">{t}=$ $\left{Z</em>}\left(\boldsymbol{x<em i="1">{i}^{t}\right)\right}</em>$ is given by:}^{n_{t}}$ are the observed outputs for simulator $t$, the predictive distribution for the highest-fidelity simulation at new input $\boldsymbol{x</p>
<p>$$
\left[Z_{T}(\boldsymbol{x}) \mid \boldsymbol{z}^{(T)}, \Theta\right] \sim \mathcal{N}\left(\mu_{Z_{T}}(\boldsymbol{x}), \sigma_{Z_{T}}^{2}(\boldsymbol{x})\right)
$$</p>
<p>where:</p>
<p>$$
\begin{aligned}
\mu_{Z_{T}}(\boldsymbol{x}) &amp; =\left[\sum_{t^{\prime} \in \operatorname{Pa}(T)} \rho_{t^{\prime}}(\boldsymbol{x}) \boldsymbol{h}<em t_prime="t^{\prime">{t^{\prime}}(\boldsymbol{x})^{T} \boldsymbol{\beta}</em>}}+\boldsymbol{h<em T="T">{T}(\boldsymbol{x})^{T} \boldsymbol{\beta}</em>}\right]+\boldsymbol{v<em T="T">{T}(\boldsymbol{x})^{T} \boldsymbol{V}</em>\right) \
\sigma_{Z_{T}}^{2}(\boldsymbol{x}) &amp; =v_{Z_{T}}^{2}(\boldsymbol{x})-\boldsymbol{v}}^{-1}\left(\boldsymbol{z}^{(T)}-H_{T} \boldsymbol{\beta<em T="T">{T}(\boldsymbol{x})^{T} \boldsymbol{V}</em>)
\end{aligned}
$$}^{-1} \boldsymbol{v}_{T}(\boldsymbol{x</p>
<p>Here, $\boldsymbol{v}<em T="T">{T}(\boldsymbol{x})=\operatorname{Cov}\left(Z</em>)$ and}(\boldsymbol{x}), \boldsymbol{z}^{(T)}\right)$ is the covariance vector of the new observation $Z_{T}(\boldsymbol{x</p>
<p>data $\boldsymbol{z}^{(T)}, \boldsymbol{V}<em Z__T="Z_{T">{T}=\operatorname{Var}\left(\boldsymbol{z}^{(T)}\right)$ is the covariance matrix of $\boldsymbol{z}^{(T)}, v</em>}}^{2}(\boldsymbol{x})=\operatorname{Var}\left(Z_{T}(\boldsymbol{x})\right)$ is the prior variance of $Z_{T}(\boldsymbol{x}), H_{T}$ is the matrix of basis functions and $\boldsymbol{\beta}=\left{\boldsymbol{\beta<em T="T">{1}, \cdots, \boldsymbol{\beta}</em>$ from (3).}\right}$ are the coefficients such that $H_{T} \boldsymbol{\beta}$ yields the vector of prior means for $\boldsymbol{z}^{(T)</p>
<p>While Equation (6) provides closed-form expressions for the predictive mean and variance, such expressions can be unwieldy to compute due to the inverse of the large matrix $\boldsymbol{V}<em t="1">{T}$, which has dimensions $\sum</em>$. Motivated by Le Gratiet and Garnier (2014), we consider below a recursive formulation for the GMGP that performs model training at each depth level of the DAG, thus enabling scalable predictions.}^{T} n_{t} \times \sum_{t=1}^{T} n_{t</p>
<h1>3.2 r-GMGP: recursive formulation</h1>
<p>The key idea for the r-GMGP is to recursively perform model training and prediction at each depth level of $\mathcal{G}$, beginning with source (i.e., depth 0 ) nodes, then continuing for higher depth nodes until the highest-fidelity node is reached. This is facilitated by the following recursive computation of the predictive distribution, which we later show yields the desired predictive mean and variance from the GMGP (3) under certain conditions:</p>
<p>$$
\left{\begin{array}{l}
Z_{t}(\boldsymbol{x})=\sum_{t^{\prime} \in \operatorname{Pa}(t)} \rho_{t^{\prime}}(\boldsymbol{x}) Z_{t^{\prime}}^{<em>}(\boldsymbol{x})+\delta_{t}(\boldsymbol{x}), \quad t \in \overline{V_{S}} \
Z_{t^{\prime}}^{</em>}(\boldsymbol{x}) \perp \delta_{t}(\boldsymbol{x}), \quad t^{\prime} \in \operatorname{Pa}(t)
\end{array}\right.
$$</p>
<p>Here, $Z_{t^{\prime}}^{<em>}(\cdot)=\left[Z_{t^{\prime}}(\cdot) \mid\left{\boldsymbol{z}<em _in="\in" _operatorname_Anc="\operatorname{Anc" m="m">{m}\right}</em>}\left(t^{\prime}\right)}, \boldsymbol{z<em t_prime="t^{\prime">{t^{\prime}}, \Theta</em>^{}}\right]$ is the posterior distribution at node $t^{\prime}$, conditional on data from both $t^{\prime}$ and its ancestor nodes $\operatorname{Anc}\left(t^{\prime}\right)$, i.e., nodes $u$ where there exists a path from $u$ to $t^{\prime}$. The exact expression for $Z_{t^{\prime}</em>}(\cdot)$ is given in Equation (8) below, with $T$ replaced by the current node. The same GP priors (5) are assigned for the discrepancy terms $\delta_{t}(\boldsymbol{x})$. The key difference between the recursive GMGP (r-GMGP) formulation (7) and the GMGP (3) is that, in place of $Z_{t^{\prime}}(\boldsymbol{x})$ (the prior of the parent simulation) for the GMGP, the r-GMGP uses $Z_{t^{\prime}}^{*}(\boldsymbol{x})$, the posterior of the parent simulation given data.</p>
<p>Under (7), the posterior distribution of the highest-fidelity simulation $Z_{T}$ becomes:</p>
<p>$$
\left[Z_{T}(\boldsymbol{x}) \mid \boldsymbol{z}^{(T)}, \Theta\right] \sim \mathcal{N}\left(m_{Z_{T}}(\boldsymbol{x}), s_{Z_{T}}^{2}(\boldsymbol{x})\right)
$$</p>
<p>with the posterior mean $m_{Z_{T}}(\boldsymbol{x})$ and variance $s_{Z_{T}}^{2}(\boldsymbol{x})$ :</p>
<p>$$
\begin{aligned}
&amp; m_{Z_{T}}(\boldsymbol{x})=\sum_{t^{\prime} \in \operatorname{Pa}(T)} \rho_{t^{\prime}}(\boldsymbol{x}) m_{Z_{t^{\prime}}}(\boldsymbol{x})+\boldsymbol{h}<em T="T">{T}(\boldsymbol{x})^{T} \boldsymbol{\beta}</em> \
&amp; +\boldsymbol{r}<em T="T">{T}\left(\boldsymbol{x}, \mathcal{D}</em>}\right)^{T} \boldsymbol{R<em T="T">{T}\left(\mathcal{D}</em>}\right)^{-1}\left[\boldsymbol{z<em t_prime="t^{\prime">{T}-\sum</em>} \in \operatorname{Pa}(T)} \boldsymbol{\rho<em T="T">{t^{\prime}}\left(\mathcal{D}</em>}\right) \odot \boldsymbol{z<em T="T">{t^{\prime}}\left(\mathcal{D}</em>}\right)-\boldsymbol{h<em T="T">{T}\left(\mathcal{D}</em>}\right)^{T} \boldsymbol{\beta<em Z__T="Z_{T">{T}\right] \
&amp; s</em>}}^{2}(\boldsymbol{x})=\sum_{t^{\prime} \in \operatorname{Pa}(T)} \rho_{t^{\prime}}^{2}(\boldsymbol{x}) s_{Z_{t^{\prime}}}^{2}(\boldsymbol{x})+\sigma_{T}^{2}\left[1-\boldsymbol{r<em T="T">{T}\left(\boldsymbol{x}, \mathcal{D}</em>}\right)^{T} \boldsymbol{R<em T="T">{T}\left(\mathcal{D}</em>}\right)^{-1} \boldsymbol{r<em T="T">{T}\left(\boldsymbol{x}, \mathcal{D}</em>\right)\right]
\end{aligned}
$$</p>
<p>Here, $\odot$ denotes the Hadamard (entrywise) product, $\mathcal{D}<em i="i">{T}=\left{\boldsymbol{x}</em>\right}}^{T<em T="T">{i=1}^{n</em>}}$ is the set of design points at node $T, \boldsymbol{r<em T="T">{T}\left(\boldsymbol{x}, \mathcal{D}</em>}\right)$ is the correlation vector between $Z_{T}(\boldsymbol{x})$ and $\boldsymbol{z<em T="T">{T}$, and $\boldsymbol{R}</em>}\left(\mathcal{D<em T="T">{T}\right)$ is the correlation matrix of $\boldsymbol{z}</em>$, starting from its leaf nodes to its root. We show later that the predictive equations (8) for r-GMGP are precisely the desired predictive equations (6) for the GMGP model under certain conditions.}$. Equation (8) provides closed-form expressions for the predictive distribution of the highest-fidelity simulation $T$, which depend on only terms related to the current node $T$ and its parent nodes $\operatorname{Pa}(T)$. Indeed, Equation (8) holds for all non-source nodes $t \in \overline{V_{S}}$, with closed-form expressions depending on only node $t$ and its parent nodes $\operatorname{Pa}(t)$. Thus, the desired predictive distribution $\left[Z_{t}(\boldsymbol{x}) \mid \boldsymbol{z}^{(T)}, \Theta\right]$ can be efficiently evaluated, by recursively computing the posterior mean and variance using (8) at each depth level of $\mathcal{G</p>
<p>This recursive approach can yield significant computational savings over a naive evaluation of the original predictive equations (6), since it breaks up the inversion of the large covariance matrix $\boldsymbol{V}<em t="t">{T}$ into inversions of smaller matrices $\boldsymbol{R}</em>}\left(\mathcal{D<em t="t">{t}\right)$ at each depth level of $\mathcal{G}$. More precisely, with $n</em>\left(D \max }$ denoting the sample size at node $t \in V$, this reduces the computational cost of $\mathcal{O}\left(\left(\sum_{t \in V} n_{t}\right)^{3}\right)$ for the original predictive equations (6) to a cost of $\mathcal{O}\left(\sum_{t \in V} n_{t}^{3}\right)$ for the recursive approach in (8). Furthermore, if the inverse at each level of $\mathcal{G}$ can be performed simultaneously via distributed computing, this computational cost can be further reduced to $\mathcal{O<em t="t">{t}\left(n</em>$. When the sample sizes are moderately large at most nodes (which is typically the case for}\right)^{3}\right)$, where $D \leq T$ is the depth of the rooted graph $\mathcal{G</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Examples of in-trees, with $Z_{T}$ denoting the highest-fidelity simulator at the root.
low-fidelity simulations), this recursive computation of the posterior mean and variance at each depth level can yield large computational savings.</p>
<p>We now return to the important question of whether the r-GMGP predictive equations (8) are indeed the same as the desired predictive equations (6) for GMGP. To show this equivalence, we require a specific structure of the multi-fidelity DAG $\mathcal{G}$. Suppose $\mathcal{G}$ is a directed in-tree (Mehlhorn and Sanders, 2008), defined as a rooted tree for which, at any node $t \in V$, there is exactly one path going from node $t$ to the root node $T$ (representing the highest-fidelity simulator $Z_{T}$ ). In-trees are also known as anti-arborescence trees (Korte and Vygen, 2011) in graph theory. Fig. 3 shows several examples of directed in-trees. Under such an assumption on $\mathcal{G}$, the following proposition shows that the r-GMGP indeed yields the desired posterior predictive mean and variance for the highest-fidelity simulation $Z_{T}$ for the GMGP model:</p>
<p>Proposition 2 Suppose the rooted multi-fidelity $D A G \mathcal{G}=(V, E)$ is an in-tree. Further suppose (i) the observations are noise-free, and (ii) the design points are nested over $\mathcal{G}$, such that for any node $t \in V$, its design set $\mathcal{D}<em t_prime="t^{\prime">{t}$ is a subset of the designs at all parent nodes $\mathcal{D}</em>}}, t^{\prime} \in \operatorname{Pa}(t)$. Then, conditional on the parameters $\Theta=\left{\boldsymbol{\beta<em t="t">{t}, \sigma</em>)\right}}^{2}, \rho_{t}(\boldsymbol{x<em Z__T="Z_{T">{t=1}^{T}$ of both models, the posterior predictive mean and variance from GMGP and r-GMGP are the same, i.e., $\mu</em>)$ are the r-GMGP posterior mean and variance in (8).}}(\boldsymbol{x})=m_{Z_{T}}(\boldsymbol{x})$ and $\sigma_{Z_{T}}^{2}(\boldsymbol{x})=s_{Z_{T}}^{2}(\boldsymbol{x})$, where $\mu_{Z_{T}}(\boldsymbol{x})$ and $\sigma_{Z_{T}}^{2}(\boldsymbol{x})$ are the GMGP posterior mean and variance in (6), and $m_{Z_{T}}(\boldsymbol{x})$ and $s_{Z_{T}}^{2}(\boldsymbol{x</p>
<p>The proof (by induction) is provided in the online supplement. This proposition shows that the recursive GMGP formulation indeed yields the same predictive mean and variance</p>
<p>as the GMGP model when the multi-fidelity graph $\mathcal{G}$ forms an in-tree, thus justifying the computational savings from r-GMGP. The assumption of $\mathcal{G}$ being an in-tree implies that, for any simulator (i.e., node), there exists exactly one path in the employed simulation framework along which this model can be refined to the highest-fidelity simulator (i.e., root node). For example, the three heavy-ion collision models in the Introduction form a 3-node in-tree (see Fig. 5 (left)). In practice, such a property can often be satisfied via a careful choice of lower-fidelity simulators to run for training the multi-fidelity emulator. In cases where the simulators cannot be selected and do not form an in-tree, the original GMGP equations (6) may be used for prediction, albeit at higher costs from larger matrix inversions.</p>
<p>For inference on model parameters, we employ a straight-forward extension of the maximum likelihood approach in Le Gratiet and Garnier (2014), which accounts for uncertainties in regression and dependency parameters within a universal co-kriging framework. Details can be found in Le Gratiet and Garnier (2014). In particular, our implementation of rGMGP is built upon the R package MuFiCokriging (Le Gratiet, 2012) for this paper, which is available on CRAN. If fully Bayesian inference is desired on such parameters, one can adapt the Monte Carlo approach in Konomi and Karagiannis (2021).</p>
<p>Finally, we note that while the above formulation presumes independence over source nodes, there may be situations where prior information suggests some correlation may be preferable between these nodes. In such cases, one may adopt correlated GP priors over source nodes, then use the original GMGP without recursive updates.</p>
<h1>3.3 d-GMGP: nonlinear extension</h1>
<p>One potential limitation of the GMGP is that it presumes linear dependencies between the simulation codes over the DAG. Given enough training data, it may be preferable to consider a more sophisticated emulator model that accounts for nonlinear dependencies between nodes on the multi-fidelity DAG. There are multiple ways for modeling this, in-</p>
<p>cluding the deep GP approach in Perdikaris et al. (2017) and the binary tree partition method in Konomi and Karagiannis (2021). Below, we adapt the deep GP approach for two reasons: (i) the output observables in our high-energy physics application are known to be quite smooth (Everett et al., 2021b), thus the smoother deep GP approach is preferable to binary tree partitions; (ii) our extension avoids the need for MCMC sampling in approximating the predictive distribution.</p>
<p>Similar to before, let us assume the multi-fidelity DAG $\mathcal{G}$ is a directed in-tree, with design points nested over $\mathcal{G}$. The d-GMGP model at non-source nodes is formulated as:</p>
<p>$$
\left{\begin{array}{l}
Z_{t}(\boldsymbol{x})=f_{t}\left(\left{Z_{t^{\prime}}(\boldsymbol{x}): t^{\prime} \in \operatorname{Pa}(t)\right} \cup \boldsymbol{x}\right)+\delta_{t}(\boldsymbol{x}), \quad t \in \overline{V_{S}} \
f_{t}(\cdot) \perp \delta_{t}(\cdot)
\end{array}\right.
$$</p>
<p>Here, we again assign independent GP priors for the discrepancies $\delta_{t}(\boldsymbol{x}) \sim \mathcal{G} \mathcal{P}\left(0, \sigma_{t}^{2} r_{t}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right)$, with similar independent GP priors on source nodes:</p>
<p>$$
\left{\begin{array}{l}
Z_{t}(\boldsymbol{x}) \sim \mathcal{G} \mathcal{P}\left(0, \sigma_{t}^{2} r_{t}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right), \quad t \in V_{S} \
Z_{t}(\boldsymbol{x}) \perp Z_{t^{\prime}}(\boldsymbol{x}), \quad t, t^{\prime} \in V_{S}, \quad t \neq t^{\prime}
\end{array}\right.
$$</p>
<p>The key difference between this new model and the GMGP model (3) is how the lowerfidelity (parent) nodes are integrated for higher-fidelity models. Instead of a weighted sum of the parent codes $\sum_{t^{\prime} \in \operatorname{Pa}(t)} \rho_{t^{\prime}}(\boldsymbol{x}) Z_{t^{\prime}}(\boldsymbol{x})$, the d-GMGP model allows for a more general nonlinear transformation $f_{t}(\cdot)$ of the parent $\operatorname{codes}\left{Z_{t}(\boldsymbol{x}): t \in \operatorname{Pa}(t)\right}$ as well as the control parameters $\boldsymbol{x}$.</p>
<p>Since the transformation $f_{t}$ is unknown in practice, one approach is to assign to it an independent zero-mean GP prior. We can combine the GP priors on $f_{t}$ and $\delta_{t}$ with Equation (9) to obtain the general specification for $Z_{t}(\boldsymbol{x})$ for non-source node $t \in \overline{V_{S}}$ :</p>
<p>$$
Z_{t}(\boldsymbol{x})=g_{t}\left(\left{Z_{t^{\prime}}(\boldsymbol{x}): t^{\prime} \in \operatorname{Pa}(t)\right} \cup \boldsymbol{x}\right) \sim \mathcal{G} \mathcal{P}\left(0, K_{t}\left([\boldsymbol{x}, \boldsymbol{z}],\left[\boldsymbol{x}^{\prime}, \boldsymbol{z}^{\prime}\right]\right)\right), \quad t \in \overline{V_{S}}
$$</p>
<p>where $\boldsymbol{z}=\left{Z_{t^{\prime}}(\boldsymbol{x})\right}<em t_prime="t^{\prime">{t^{\prime} \in \operatorname{Pa}(t)}$ and $\boldsymbol{z}^{\prime}=\left{Z</em>\right)\right}}}\left(\boldsymbol{x}^{\prime<em t="t">{t^{\prime} \in \operatorname{Pa}(t)}$. Note that the kernel $K</em>(t)$. Viewed this way, the proposed d-GMGP model can be seen as an}$ involves both the control parameters $\boldsymbol{x}$ and the simulation outputs $Z_{t^{\prime}}(\boldsymbol{x})$ from lower-fidelity (parent) nodes $t^{\prime} \in \operatorname{Pa</p>
<p>extension of the deep GP model (see, e.g., Damianou and Lawrence, 2013), where the GP outputs at each node are linked by the multi-fidelity DAG elicted from model dependencies (i.e., the "science"). Compared to a full-blown deep GP model, which typically requires the estimation of thousands of variational parameters, the d-GMGP model (11) with the kernel choice below requires much fewer parameters for estimation, all the while providing the desired nonlinear dependency over the graph.</p>
<p>For the kernel $K_{t}$ in (11), one specification we found quite effective is the following:</p>
<p>$$
K_{t}\left([\boldsymbol{x}, \boldsymbol{z}],\left[\boldsymbol{x}^{\prime}, \boldsymbol{z}^{\prime}\right]\right)=K_{\mathrm{SE}, \rho}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\left[K_{\mathrm{LIN}}\left(\boldsymbol{z}, \boldsymbol{z}^{\prime}\right)+K_{\mathrm{SE}}\left(\boldsymbol{z}, \boldsymbol{z}^{\prime}\right)\right]+K_{\mathrm{SE}, \delta}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right), \quad t \in \overline{V_{S}}
$$</p>
<p>Here, $K_{\mathrm{LIN}}\left(\boldsymbol{z}, \boldsymbol{z}^{\prime}\right)=\sigma^{2} \boldsymbol{z}^{T} \boldsymbol{z}^{\prime}$ is a linear kernel, $\sigma^{2}$ is a variance parameter, and $K_{\mathrm{SE}}\left(\boldsymbol{z}, \boldsymbol{z}^{\prime}\right)$, $K_{\mathrm{SE}, \rho}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$ and $K_{\mathrm{SE}, \delta}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$ are separate anisotropic squared-exponential kernels. This is motivated by the kernel choice for the deep multi-fidelity GP models in Perdikaris et al. (2017) and Cutajar et al. (2019). The intuition behind (12) is that it captures both linear and nonlinear dependencies between outputs, as well as correlations between input parameters. When $K_{\mathrm{SE}}\left(\boldsymbol{z}, \boldsymbol{z}^{\prime}\right)=0$, this kernel reduces to a form similar to the r-GMGP, with a probabilistic and non-parametric form for $\rho(\boldsymbol{x})$. In total, the d-GMGP with kernel (12) requires $(1+d)\left|V_{S}\right|+\sum_{t \in \overline{V_{S}}}(4+2 d+|\operatorname{Pa}(t)|)$ hyperparameters to estimate (via maximum likelihood), which is much fewer than that needed for a full-scale deep GP; this is primarily due to the above recursive formulation. Such a deep model, however, requires the estimation of more parameters compared to the r-GMGP and thus requires more computation for inference and prediction; it should therefore be used only when one has prior information on nonlinear dependencies. When model outputs are known to be highly non-stationary and/or discontinuous, the binary tree partition approach in Konomi and Karagiannis (2021) may offer an appealing alternative for computational efficiency.</p>
<p>As before, a recursive formulation can be adopted for efficient fitting of the d-GMGP. The idea is again to recursively perform model training and prediction at each depth level of $\mathcal{G}$. This is achieved by replacing the GP priors $Z_{t^{\prime}}(\cdot), t^{\prime} \in \operatorname{Pa}(t)$ in Equation (9) by the GP posteriors $Z_{t^{\prime}}^{*}(\cdot)=\left[Z_{t^{\prime}}(\cdot) \mid\left{\boldsymbol{z}<em _in="\in" _operatorname_Anc="\operatorname{Anc" m="m">{m}\right}</em>}\left(t^{\prime}\right)}, \boldsymbol{z<em t_prime="t^{\prime">{t^{\prime}}, \Theta</em>(t)$. The desired posterior}}\right], t^{\prime} \in \operatorname{Pa</p>
<p>on the highest-fidelity node $Z_{T}(\cdot)$ can then be computed recursively at each depth level of $\mathcal{G}$, starting from its leaf nodes to its root, as was done for the r-GMGP. Unlike the r-GMGP, however, the predictive distribution for the highest-fidelity node $Z_{T}(\cdot)$ is no longer Gaussian. This recursive formulation allows for efficient approximation of the desired predictive distribution, by propagating the posterior uncertainty at each level using Monte Carlo. Specifically, the posterior distribution at a non-source node $t$ can be evaluated by:</p>
<p>$$
\left[Z_{t}^{<em>}(\boldsymbol{x})\right]=\int\left[Z_{t}(\boldsymbol{x}) \mid \boldsymbol{z}_{t}, \boldsymbol{z}^{</em>}\right] \prod_{t^{\prime} \in \operatorname{Pa}(t)}\left[Z_{t^{\prime}}^{<em>}(\boldsymbol{x})\right] d \boldsymbol{z}^{</em>}
$$</p>
<p>Here, $\boldsymbol{z}<em t_prime="t^{\prime">{t}$ are the observed data on node $t$, and $\boldsymbol{z}^{<em>}=\left{Z_{t^{\prime}}^{</em>}(\boldsymbol{x})\right}</em>(\cdot)$. Compared to full-scale deep GP model, this recursive approach provides a scalable way for propagating predictions and uncertainties to the root node, without the need for complex variational approximations. The full algorithm for d-GMGP prediction is provided in the online supplement.} \in \operatorname{Pa}(t)}$ are the (unknown) outputs on parent nodes at parameters $\boldsymbol{x}$. This can be estimated via Monte Carlo integration on the posterior of parent node outputs $\left[Z_{t^{\prime}}^{*}(\boldsymbol{x})\right], t^{\prime} \in \operatorname{Pa}(t)$. This procedure can then be repeated recursively at each depth level of $\mathcal{G}$ to provide efficient computation of the predictive distribution for the highest-fidelity node $Z_{T</p>
<p>For the d-GMGP, the computational cost for hyperparameter estimation using maximum likelihood is $\mathcal{O}\left(\sum_{t \in V} n_{t}^{3}\right)$ per objective evaluation, which greatly speeds up the $\mathcal{O}\left(\left(\sum_{t \in V} n_{t}\right)^{3}\right)$ cost for standard GP via its recursive formulation. Its prediction then requires sampling from the posterior distribution of each parent node model, then propagating these as inputs of each child node until we reach the root of the tree. Here, the posterior samples needed to achieve a desired accuracy for highest-fidelity prediction can grow exponentially with both the input dimensions and the size of the tree. One solution (which we adopt) is to employ a Gaussian approximation of the posterior predictive distribution at each node, then recursively compute the posterior means and variances over the nodes in DAG $\mathcal{G}$. The latter step can be performed via the closed-form expressions in Girard et al. (2002), and further details of this can be found in Perdikaris et al. (2017).</p>
<p>In choosing between the original GMGP model (which models linear dependencies) or</p>
<p>the above d-GMGP model, we have found that with careful elicitation from scientists, there is often prior knowledge on scientific model dependencies which can help guide this choice. For example, in computational fluid dynamics (see, e.g., Wang, 2016), the dependencies between the high-fidelity direct numerical simulation (Pope, 2000) and the lower-fidelity Reynolds-averaged Navier Stokes simulation (Catalano and Amato, 2003) is known to be highly nonlinear, and captures complex eddies and vortices in turbulent fluid flow. For this case, the d-GMGP should be thus used instead of the original GMGP model. In the absence of such prior information, one can make use of standard model selection techniques (e.g., AIC or BIC) to select the better predictive model from data.</p>
<h1>4 Experimental Design</h1>
<p>Given that the motivation behind the GMGP model is to maximize predictive performance given a computational budget, its experimental design is of crucial importance. This procedure can be split into two steps: (i) the design of training set $\mathcal{D}<em t="t">{t}$ at each node $t \in V$ given fixed sample sizes $n</em>}=\left|\mathcal{D<em t="t">{t}\right|$, and (ii) the allocation of sample sizes $n</em>$ at each node $t \in V$ given a fixed computational budget. For simplicity, we investigate this for the GMGP model, but such designs can naturally be adapted for the more complex d-GMGP model. As before, we assume the underlying DAG is an in-tree.</p>
<h3>4.1 Design given fixed sample sizes</h3>
<p>Consider first step (i). From Proposition 2, an appealing design property is the nested nature of design points over the graph $\mathcal{G}$, i.e., for any node $t \in V$, the design set $\mathcal{D}<em t_prime="t^{\prime">{t}$ is a subset of the designs $\mathcal{D}</em>(t)$. For GP modeling, the space-filling property of design points (Santner et al., 2019) - its uniformity over the prediction space - is also known to be crucial for improving predictive performance. Different notions of space-fillingness have been explored in the literature, including maximin (Johnson et al.,}}$ at any parent node $t^{\prime} \in \operatorname{Pa</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visualizing our nested BFS design on a 2-d space. (Left) The base maximin SLHD design with 5 slices and 10 design points per slice. (Right) BFS allocation of the SLHD design over a three-node DAG. The first slice (red) is allocated to all three nodes, then subsequent slices (green and blue) are used on the lower-fidelity nodes $L_{1}$ and $L_{2}$.</p>
<p>Algorithm 1 Nested BFS Design
Input: DAG with $T=|V|$ nodes; desired sample sizes $\left{n_{t}: t \in V\right}$. Note that all $n_{t}$ 's should be multiples of $n_{T}$.
Output: Design set $\mathcal{D}_{t}$ for each node $t \in V$.</p>
<p>1: Generate a maximin SLHD design (Ba et al., 2015) with $M=\sum_{t=1}^{T} n_{t} / n_{T}$ slices, with each slice containing $n_{T}$ design points.
2: Assign the design points in slices $\left{\sum_{j=1}^{t-1} n_{j} / n_{T}+1, \cdots, \sum_{j=1}^{t} n_{j} / n_{T}\right}$ (denoted as $S_{t}$ ) to node $t, t=1, \cdots, T$.
3: For each node $t$, set $\mathcal{D}<em t="t">{t}=S</em>\right}$ for $t=1, \cdots, T$.} \cup\left{\cup_{t^{\prime} \in \operatorname{Des}(t)} S_{t^{\prime}</p>
<p>1990; Morris and Mitchell, 1995) and minimax designs (Johnson et al., 1990; Mak and Joseph, 2018). We will incorporate these two properties in the design procedure below.</p>
<p>Given sample sizes $n_{1}, \cdots, n_{T}$, we propose a nested experimental design over the DAG $\mathcal{G}$. We make use of the maximin sliced Latin hypercube design (maximin SLHD, Ba et al., 2015; see also Qian, 2012), which provides design points in equal slices (or batches), such that the design points within each slice are space-filling, and the design points between slices are also well spaced-out. Fig. 4 (left) shows an SLHD in $d=2$ dimensions. With this SLHD in hand, we then employ a bottom-up approach to allocate design points over $\mathcal{G}$. We first allocate one slice in the SLHD for the highest-fidelity simulator $T$ (i.e., the root node at the bottom of $\mathcal{G}$ ), then use the remaining slices to fill out design points on subsequent nodes in a breadth-first-traversal (BFS) of $\mathcal{G}$. For the latter step, the design points at each node $t \in V$ are obtained by concatenating the current SLHD slice(s) with</p>            </div>
        </div>

    </div>
</body>
</html>
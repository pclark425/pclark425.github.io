<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3138 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3138</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3138</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-0b315e6d4d800e04caf2f587312ce163e748d10c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0b315e6d4d800e04caf2f587312ce163e748d10c" target="_blank">Numeric Magnitude Comparison Effects in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work investigates how well popular LLMs capture the magnitudes of numbers from a behavioral lens and relies on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that $4<5$) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard benchmarks. Here, we ask a different question, one inspired by cognitive science: How closely do the number representations of LLMscorrespond to those of human language users, who typically demonstrate the distance, size, and ratio effects? We depend on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times. The results reveal surprisingly human-like representations across language models of different architectures, despite the absence of the neural circuitry that directly supports these representations in the human brain. This research shows the utility of understanding LLMs using behavioral benchmarks and points the way to future work on the number representations of LLMs and their cognitive plausibility.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3138.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3138.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An auto-regressive decoder transformer (OpenAI GPT-2) studied in base and large variants; used here to extract contextual embeddings for number tokens and evaluate magnitude-comparison behavioral signatures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Numeric Magnitude Comparison Effects in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (base & large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive decoder transformer. Base ~117M parameters; large variant ~345M parameters (as used in paper). Pretrained on large text corpora; embeddings for single-token numbers were extracted from each hidden layer.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Magnitude comparison (pairwise comparisons of numbers 1-9), implicit numeration (mapping number words to digits), evaluation via distance/size/ratio effects and MDS-derived latent number line</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Numbers are represented as contextual vector embeddings whose pairwise cosine similarities encode discriminability; difficulty of comparison is linked to embedding similarity (linking hypothesis); latent 'mental number line' (MNL) structure (log-compressed) can be recovered via MDS from cosine dissimilarities.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High R^2 fits for distance and ratio effects (e.g., distance-effect R^2 averaged ~0.93–0.96 across layers/formats; ratio-effect R^2 averages ~0.70–0.83 depending on format). MDS 1-D solutions from GPT-2 embeddings (digits especially) show the strongest correlation with log-compressed human MNL among tested models (MDS correlation for digits notably higher; averaged MDS correlation ~0.56 across models, GPT-2 digits highest). Layer-wise analyses show consistent patterns across layers and robust distance/ratio signals.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Size effect (expected from log-compressed MNL) is only reliably observed when inputs are digits, not number words; residuals in MDS show anomalous placements for some numbers (notably 2 and 9), indicating deviations from human MNL; analysis limited to 1–9 so generalization to larger/more complex arithmetic is untested.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Model-variant ablations (base vs large), input-format manipulations (lowercase words, mixed-case words, digits), cased vs uncased BERT comparison (BERT-specific), layer-wise probing (no prompting/fine-tuning or external tools used).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Large variants preserve distance and ratio effects and often strengthen some metrics; digits as input increase strength of size and distance effects compared to number-word formats; layer-wise trends change modestly with model scale (e.g., distance-effect strength tends to decrease across deeper layers while size-effect trends vary). No intervention produced algorithmic arithmetic; interventions changed representational strength but not kind of mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Distance effect: high linear fit R^2 (averaged across layers/formats for models ~0.95; digits average ~0.965 across models, GPT-2 among high scorers). Size effect: averaged R^2 across inputs ~0.54 (GPT-2 base digits ~0.853; overall digits average across models ~0.842). Ratio effect: averaged R^2 ~0.709 for GPT-2 and ~0.789 overall across models; digit format often yields highest ratio R^2. MDS 1-D correlation with log(1..9): GPT-2 digits showed the highest correlation among models (MDS average across layers/formats ~0.56 across models; GPT-2 digits noticeably above average). Exact table values reported in paper (see Tables 2-9, 11-21).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Size effect weak or absent for number-word inputs; anomalies in latent number-line (notably numbers 2 and 9 displaced relative to human log-MNL); evaluation limited to 1--9; models do not perform explicit arithmetic algorithms — only encode relative magnitude structure; behavior depends on input tokenization/format and layer selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors map cosine similarity to human reaction time (linking hypothesis) and find LLM representations reproduce two of three classic human magnitude-comparison effects (distance and ratio); size effect only partially matched (digits). MDS-derived latent number lines approximate the log-compressed human MNL, indicating representational similarity rather than symbolic algorithmic computation like calculators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeric Magnitude Comparison Effects in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3138.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3138.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional Encoder Representations from Transformers (BERT, uncased variant used)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bidirectional encoder transformer (BERT) whose per-token hidden activations were probed across layers to analyze numeric magnitude representations and human-like comparison effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Numeric Magnitude Comparison Effects in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (uncased, base & large variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer. Base ~110M parameters; large variant ~340M parameters (as used in paper). Uncased variant primarily analyzed (cased vs uncased compared in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Magnitude comparison (distance/size/ratio) for numbers 1-9 in three input formats (lowercase words, mixed-case words, digits); MDS recovery of latent number line.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Magnitude is encoded in token embeddings / hidden activations such that cosine similarity between number vectors predicts discriminability; MNL-like ordering emerges (approximate log-compression) in embedding-space distances.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Strong distance-effect fits (high R^2 values across layers, e.g., uncased BERT distance R^2 often >0.95), ratio-effect fits with substantial R^2 values, MDS correlations with log-MNL positive though varying by layer; cased vs uncased show similar behavioral characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Size effect only robust for digit inputs; some layers and variant settings show reduced MDS correlation (negative or small correlations in some later layers in base variant table), suggesting partial and layer-dependent representation rather than explicit numeric algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Comparisons across model scale (base vs large), cased vs uncased variants, input-format changes, layer-wise probing.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Large variants maintain distance/ratio effects; cased vs uncased behave similarly overall though cased sometimes shows higher MDS correlation; size effect stronger in digits and larger variants in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Distance-effect R^2: uncased BERT averaged ~0.95+ across inputs/layers (see Table 2/14). Size-effect R^2: mixed results; uncased base digits R^2 ~0.851 (Table 5); ratio-effect R^2: uncased BERT often high (Table 6 shows 0.906 for LC). MDS correlations vary by layer and variant (see Tables 8-9, 20-21).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Size effect not consistent for number words, layer-wise degradation of some metrics in deeper layers, deviations in MDS residuals (notably numbers 2 and 9), limited scope (1-9) prevents claims about broader arithmetic abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Representational patterns align with human behavioral signatures (distance and ratio effects) via the linking hypothesis; authors stress that models approximate human-like magnitude behavior without implementing explicit symbolic arithmetic routines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeric Magnitude Comparison Effects in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3138.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3138.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Robustly Optimized BERT Pretraining Approach (RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder transformer (RoBERTa) whose layerwise token activations for number tokens were probed to evaluate magnitude comparison effects and latent number-line structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Numeric Magnitude Comparison Effects in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (base & large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer derived from BERT with training optimizations; base ~125M and large ~355M parameter variants used in experiments. Numbers provided as single tokens; hidden activations extracted per layer.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Magnitude comparison (distance/size/ratio for 1–9), implicit numeration (number word vs digit formats), MDS-based latent number line analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Magnitude encoded in contextual embeddings: cosine similarity between number-token vectors maps onto behavioral difficulty (linking hypothesis); embeddings arrange approximately along a log-compressed latent number line recoverable by MDS.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High R^2 for distance effect across layers and formats (Table 2/14); ratio-effect fits with moderate-high R^2; digits often produce stronger size and ratio signals than number words; MDS correlations positive in many layers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Size effect weaker for word formats; MDS residuals show deviations for specific numerals; representation quality and correspondence to log-MNL vary across layers and model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Base vs large variant comparison, input format manipulation, layerwise probing.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Large RoBERTa preserves distance/ratio effects; digits input improves size-effect signals; layer-wise effects vary with model size (some deep-layer decreases in MDS correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Distance-effect R^2 averaged ~0.95+ across layers/formats (Table 3/14); size-effect R^2 for digits ~0.783 (Table 5) and larger variant digits ~0.677 (Table 15); ratio-effect R^2 averages ~0.736 (Table 6). MDS correlations variable but often positive (Tables 8-9).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Limited numeration for number words (size effect weak), layer-dependent breakdowns, residual anomalies for certain numerals (2, 5, 9), no explicit arithmetic computation capability demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Behavioral benchmark alignment: distance and ratio effects mimic human patterns; MDS-derived latent number lines partially approximate human log-compressed MNL, suggesting statistical learning of magnitude rather than symbolic algorithmic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeric Magnitude Comparison Effects in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3138.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3138.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLNet (Generalized Autoregressive Pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive encoder-decoder variant (XLNet) probed layerwise for number-token embeddings to test for human-like magnitude comparison effects and latent magnitude structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Numeric Magnitude Comparison Effects in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLNet (base & large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generalized autoregressive transformer; base ~110M and large ~340M parameter variants examined. Hidden activations for single-token number representations extracted across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Magnitude comparison (distance/size/ratio) on numbers 1–9; format comparisons (lowercase/mixed-case words, digits); MDS-based latent number line analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Contextual hidden vectors encode magnitude relationships; cosine similarity used as proxy for discriminability and mapped to human RTs (linking hypothesis); MNL-like structure emerges in embedding distances.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Distance-effect linear fits yield substantial R^2 (though slightly lower than some encoder models in places); ratio-effect and MDS correlations positive and often improved with digit inputs; layerwise analyses show systematic trends.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Size-effect weaker except with digits; some metrics (distance/ratio/MDS correlation) decline in deeper layers; representation deviates from ideal log-MNL in residuals for particular numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Base vs large variant comparisons, input-format ablations, layerwise probing.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Large variant preserves distance/ratio effects; digit inputs increase size-effect strength; deeper-layer declines similar to other models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Distance-effect R^2 averaged across models/layers ~0.944 (Table 3); size-effect digits averaged ~0.793 (Table 5); ratio-effect averaged ~0.797 (Table 6); MDS correlation averages ~0.56 across models/layers but with layerwise decreases (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Size effect largely absent for number-word inputs; layer-dependent weakening; anomalous MDS placements for certain numerals; limited to 1-9 so not indicative of broader arithmetic ability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Shows human-like distance and ratio behavioral signatures under the linking hypothesis; findings interpreted as emergent magnitude representations from training data statistics, not explicit symbolic arithmetic procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeric Magnitude Comparison Effects in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3138.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3138.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-to-Text Transfer Transformer (T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder transformer (T5) whose encoder hidden activations for single-token numbers were probed to test for magnitude-comparison behavioral effects and latent number-line structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Numeric Magnitude Comparison Effects in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (encoder; base & large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder transformer family; authors used only encoder outputs for embeddings. Base ~110M parameters; large variant ~335M parameters used in experiments. Numbers treated as single tokens and probed across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Magnitude comparison (distance/size/ratio) for numbers 1-9 (lowercase/mixed-case words and digits); MDS latent-number-line recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Magnitude encoded in encoder hidden activations; cosine similarity of number vectors predicts discriminability and maps to human RT via linking hypothesis; MDS recovers approximately log-compressed ordering in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High distance-effect R^2 (Table 2 shows T5 often near top among models), decent ratio-effect fits (Table 6), and MDS correlations positive in many layers; digits improve size-effect signals.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Size effect weaker for number-word formats; layerwise trends vary and some deeper layers show reduced MDS correlations; limited number range restricts claims to magnitude representation not full arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Base vs large variant comparisons, input-format manipulation, layerwise probing.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Large variants maintain or slightly modify layerwise strengths; digits input improves size and ratio signals; overall patterns consistent with other architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Distance-effect R^2 across layers/formats high (averages per Table 3 ~0.953 overall for T5); size-effect R^2 digits ~0.886 (Table 5) and averaged ~0.709; ratio-effect averaged ~0.826 (Table 6); MDS correlations vary with layers (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Size effect limited to digits; deviations in 1-D MDS residuals for particular numbers across models; no demonstration of algorithmic arithmetic or multi-digit computation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Displays human-like distance and ratio behavioral signatures; authors interpret this as evidence that statistical pretraining can create magnitude-like internal representations without explicit symbolic arithmetic mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeric Magnitude Comparison Effects in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3138.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3138.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional and Auto-Regressive Transformers (BART)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder denoising autoencoder transformer (BART) probed for number-token hidden activations and evaluated for distance/size/ratio magnitude effects and latent number-line structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Numeric Magnitude Comparison Effects in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART (base & large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder transformer used for denoising pretraining. Base ~140M parameters; large variant ~406M parameters. Authors used hidden activations for number tokens (encoder side) across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Magnitude comparison (distance/size/ratio) for 1–9 across input formats; MDS for latent number-line recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Vector-space magnitude encoding: pairwise cosine similarity of embeddings predicts discriminability (linking hypothesis); MDS recovers ordering approximating log-compressed MNL in many cases, especially with digit inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High distance-effect R^2 across many layers (Table 2); ratio-effect R^2 high for digits and words in many layers (Table 6); MDS visualizations (example best layers) show clear latent ordering; digits often yield strongest signals.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Size effect less consistent for number-word formats; deeper-layer declines in some metrics; residual analysis shows deviations for numerals such as 2 and 9.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Base vs large variants, input-format manipulations, layerwise probing.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Large BART preserves distance/ratio effects; digits produce stronger size-effect signatures; layerwise trends similar to other models with some deeper-layer degradation in MDS correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Distance-effect R^2 averaged across layers/formats ~0.959 (Table 3); size-effect R^2 digits ~0.885 (Table 5); ratio-effect averaged ~0.838 (Table 6); MDS correlations positive across many layers (Tables 8-9).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Size effect absent for some word formats; MDS residual anomalies; analysis restricted to 1–9, so no claims about multi-digit arithmetic or algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors report BART representations reproduce human-like distance and ratio patterns under the linking hypothesis and approximate a log-compressed MNL in embedding space, indicating emergent magnitude encoding rather than explicit symbolic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Numeric Magnitude Comparison Effects in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do NLP models know numbers? probing numeracy in embeddings <em>(Rating: 2)</em></li>
                <li>Exploring numeracy in word embeddings <em>(Rating: 2)</em></li>
                <li>Numeracy for language models: Evaluating and improving their ability to predict numbers <em>(Rating: 2)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Representing numbers in NLP: a survey and a vision <em>(Rating: 2)</em></li>
                <li>MathQA: Towards interpretable math word problem solving with operation-based formalisms <em>(Rating: 1)</em></li>
                <li>Measuring mathematical problem solving with the MATH dataset <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3138",
    "paper_id": "paper-0b315e6d4d800e04caf2f587312ce163e748d10c",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "GPT-2",
            "name_full": "Generative Pre-trained Transformer 2",
            "brief_description": "An auto-regressive decoder transformer (OpenAI GPT-2) studied in base and large variants; used here to extract contextual embeddings for number tokens and evaluate magnitude-comparison behavioral signatures.",
            "citation_title": "Numeric Magnitude Comparison Effects in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-2 (base & large)",
            "model_description": "Auto-regressive decoder transformer. Base ~117M parameters; large variant ~345M parameters (as used in paper). Pretrained on large text corpora; embeddings for single-token numbers were extracted from each hidden layer.",
            "arithmetic_task_type": "Magnitude comparison (pairwise comparisons of numbers 1-9), implicit numeration (mapping number words to digits), evaluation via distance/size/ratio effects and MDS-derived latent number line",
            "reported_mechanism": "Numbers are represented as contextual vector embeddings whose pairwise cosine similarities encode discriminability; difficulty of comparison is linked to embedding similarity (linking hypothesis); latent 'mental number line' (MNL) structure (log-compressed) can be recovered via MDS from cosine dissimilarities.",
            "evidence_for_mechanism": "High R^2 fits for distance and ratio effects (e.g., distance-effect R^2 averaged ~0.93–0.96 across layers/formats; ratio-effect R^2 averages ~0.70–0.83 depending on format). MDS 1-D solutions from GPT-2 embeddings (digits especially) show the strongest correlation with log-compressed human MNL among tested models (MDS correlation for digits notably higher; averaged MDS correlation ~0.56 across models, GPT-2 digits highest). Layer-wise analyses show consistent patterns across layers and robust distance/ratio signals.",
            "evidence_against_mechanism": "Size effect (expected from log-compressed MNL) is only reliably observed when inputs are digits, not number words; residuals in MDS show anomalous placements for some numbers (notably 2 and 9), indicating deviations from human MNL; analysis limited to 1–9 so generalization to larger/more complex arithmetic is untested.",
            "intervention_type": "Model-variant ablations (base vs large), input-format manipulations (lowercase words, mixed-case words, digits), cased vs uncased BERT comparison (BERT-specific), layer-wise probing (no prompting/fine-tuning or external tools used).",
            "effect_of_intervention": "Large variants preserve distance and ratio effects and often strengthen some metrics; digits as input increase strength of size and distance effects compared to number-word formats; layer-wise trends change modestly with model scale (e.g., distance-effect strength tends to decrease across deeper layers while size-effect trends vary). No intervention produced algorithmic arithmetic; interventions changed representational strength but not kind of mechanism.",
            "performance_metrics": "Distance effect: high linear fit R^2 (averaged across layers/formats for models ~0.95; digits average ~0.965 across models, GPT-2 among high scorers). Size effect: averaged R^2 across inputs ~0.54 (GPT-2 base digits ~0.853; overall digits average across models ~0.842). Ratio effect: averaged R^2 ~0.709 for GPT-2 and ~0.789 overall across models; digit format often yields highest ratio R^2. MDS 1-D correlation with log(1..9): GPT-2 digits showed the highest correlation among models (MDS average across layers/formats ~0.56 across models; GPT-2 digits noticeably above average). Exact table values reported in paper (see Tables 2-9, 11-21).",
            "notable_failure_modes": "Size effect weak or absent for number-word inputs; anomalies in latent number-line (notably numbers 2 and 9 displaced relative to human log-MNL); evaluation limited to 1--9; models do not perform explicit arithmetic algorithms — only encode relative magnitude structure; behavior depends on input tokenization/format and layer selection.",
            "comparison_to_humans_or_symbolic": "Authors map cosine similarity to human reaction time (linking hypothesis) and find LLM representations reproduce two of three classic human magnitude-comparison effects (distance and ratio); size effect only partially matched (digits). MDS-derived latent number lines approximate the log-compressed human MNL, indicating representational similarity rather than symbolic algorithmic computation like calculators.",
            "uuid": "e3138.0",
            "source_info": {
                "paper_title": "Numeric Magnitude Comparison Effects in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "Bidirectional Encoder Representations from Transformers (BERT, uncased variant used)",
            "brief_description": "A bidirectional encoder transformer (BERT) whose per-token hidden activations were probed across layers to analyze numeric magnitude representations and human-like comparison effects.",
            "citation_title": "Numeric Magnitude Comparison Effects in Large Language Models",
            "mention_or_use": "use",
            "model_name": "BERT (uncased, base & large variants)",
            "model_description": "Encoder-only transformer. Base ~110M parameters; large variant ~340M parameters (as used in paper). Uncased variant primarily analyzed (cased vs uncased compared in appendix).",
            "arithmetic_task_type": "Magnitude comparison (distance/size/ratio) for numbers 1-9 in three input formats (lowercase words, mixed-case words, digits); MDS recovery of latent number line.",
            "reported_mechanism": "Magnitude is encoded in token embeddings / hidden activations such that cosine similarity between number vectors predicts discriminability; MNL-like ordering emerges (approximate log-compression) in embedding-space distances.",
            "evidence_for_mechanism": "Strong distance-effect fits (high R^2 values across layers, e.g., uncased BERT distance R^2 often &gt;0.95), ratio-effect fits with substantial R^2 values, MDS correlations with log-MNL positive though varying by layer; cased vs uncased show similar behavioral characteristics.",
            "evidence_against_mechanism": "Size effect only robust for digit inputs; some layers and variant settings show reduced MDS correlation (negative or small correlations in some later layers in base variant table), suggesting partial and layer-dependent representation rather than explicit numeric algorithms.",
            "intervention_type": "Comparisons across model scale (base vs large), cased vs uncased variants, input-format changes, layer-wise probing.",
            "effect_of_intervention": "Large variants maintain distance/ratio effects; cased vs uncased behave similarly overall though cased sometimes shows higher MDS correlation; size effect stronger in digits and larger variants in some settings.",
            "performance_metrics": "Distance-effect R^2: uncased BERT averaged ~0.95+ across inputs/layers (see Table 2/14). Size-effect R^2: mixed results; uncased base digits R^2 ~0.851 (Table 5); ratio-effect R^2: uncased BERT often high (Table 6 shows 0.906 for LC). MDS correlations vary by layer and variant (see Tables 8-9, 20-21).",
            "notable_failure_modes": "Size effect not consistent for number words, layer-wise degradation of some metrics in deeper layers, deviations in MDS residuals (notably numbers 2 and 9), limited scope (1-9) prevents claims about broader arithmetic abilities.",
            "comparison_to_humans_or_symbolic": "Representational patterns align with human behavioral signatures (distance and ratio effects) via the linking hypothesis; authors stress that models approximate human-like magnitude behavior without implementing explicit symbolic arithmetic routines.",
            "uuid": "e3138.1",
            "source_info": {
                "paper_title": "Numeric Magnitude Comparison Effects in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "RoBERTa",
            "name_full": "A Robustly Optimized BERT Pretraining Approach (RoBERTa)",
            "brief_description": "An encoder transformer (RoBERTa) whose layerwise token activations for number tokens were probed to evaluate magnitude comparison effects and latent number-line structure.",
            "citation_title": "Numeric Magnitude Comparison Effects in Large Language Models",
            "mention_or_use": "use",
            "model_name": "RoBERTa (base & large)",
            "model_description": "Encoder-only transformer derived from BERT with training optimizations; base ~125M and large ~355M parameter variants used in experiments. Numbers provided as single tokens; hidden activations extracted per layer.",
            "arithmetic_task_type": "Magnitude comparison (distance/size/ratio for 1–9), implicit numeration (number word vs digit formats), MDS-based latent number line analysis.",
            "reported_mechanism": "Magnitude encoded in contextual embeddings: cosine similarity between number-token vectors maps onto behavioral difficulty (linking hypothesis); embeddings arrange approximately along a log-compressed latent number line recoverable by MDS.",
            "evidence_for_mechanism": "High R^2 for distance effect across layers and formats (Table 2/14); ratio-effect fits with moderate-high R^2; digits often produce stronger size and ratio signals than number words; MDS correlations positive in many layers.",
            "evidence_against_mechanism": "Size effect weaker for word formats; MDS residuals show deviations for specific numerals; representation quality and correspondence to log-MNL vary across layers and model sizes.",
            "intervention_type": "Base vs large variant comparison, input format manipulation, layerwise probing.",
            "effect_of_intervention": "Large RoBERTa preserves distance/ratio effects; digits input improves size-effect signals; layer-wise effects vary with model size (some deep-layer decreases in MDS correlations).",
            "performance_metrics": "Distance-effect R^2 averaged ~0.95+ across layers/formats (Table 3/14); size-effect R^2 for digits ~0.783 (Table 5) and larger variant digits ~0.677 (Table 15); ratio-effect R^2 averages ~0.736 (Table 6). MDS correlations variable but often positive (Tables 8-9).",
            "notable_failure_modes": "Limited numeration for number words (size effect weak), layer-dependent breakdowns, residual anomalies for certain numerals (2, 5, 9), no explicit arithmetic computation capability demonstrated.",
            "comparison_to_humans_or_symbolic": "Behavioral benchmark alignment: distance and ratio effects mimic human patterns; MDS-derived latent number lines partially approximate human log-compressed MNL, suggesting statistical learning of magnitude rather than symbolic algorithmic reasoning.",
            "uuid": "e3138.2",
            "source_info": {
                "paper_title": "Numeric Magnitude Comparison Effects in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "XLNet",
            "name_full": "XLNet (Generalized Autoregressive Pretraining)",
            "brief_description": "An autoregressive encoder-decoder variant (XLNet) probed layerwise for number-token embeddings to test for human-like magnitude comparison effects and latent magnitude structure.",
            "citation_title": "Numeric Magnitude Comparison Effects in Large Language Models",
            "mention_or_use": "use",
            "model_name": "XLNet (base & large)",
            "model_description": "Generalized autoregressive transformer; base ~110M and large ~340M parameter variants examined. Hidden activations for single-token number representations extracted across layers.",
            "arithmetic_task_type": "Magnitude comparison (distance/size/ratio) on numbers 1–9; format comparisons (lowercase/mixed-case words, digits); MDS-based latent number line analyses.",
            "reported_mechanism": "Contextual hidden vectors encode magnitude relationships; cosine similarity used as proxy for discriminability and mapped to human RTs (linking hypothesis); MNL-like structure emerges in embedding distances.",
            "evidence_for_mechanism": "Distance-effect linear fits yield substantial R^2 (though slightly lower than some encoder models in places); ratio-effect and MDS correlations positive and often improved with digit inputs; layerwise analyses show systematic trends.",
            "evidence_against_mechanism": "Size-effect weaker except with digits; some metrics (distance/ratio/MDS correlation) decline in deeper layers; representation deviates from ideal log-MNL in residuals for particular numbers.",
            "intervention_type": "Base vs large variant comparisons, input-format ablations, layerwise probing.",
            "effect_of_intervention": "Large variant preserves distance/ratio effects; digit inputs increase size-effect strength; deeper-layer declines similar to other models.",
            "performance_metrics": "Distance-effect R^2 averaged across models/layers ~0.944 (Table 3); size-effect digits averaged ~0.793 (Table 5); ratio-effect averaged ~0.797 (Table 6); MDS correlation averages ~0.56 across models/layers but with layerwise decreases (Table 9).",
            "notable_failure_modes": "Size effect largely absent for number-word inputs; layer-dependent weakening; anomalous MDS placements for certain numerals; limited to 1-9 so not indicative of broader arithmetic ability.",
            "comparison_to_humans_or_symbolic": "Shows human-like distance and ratio behavioral signatures under the linking hypothesis; findings interpreted as emergent magnitude representations from training data statistics, not explicit symbolic arithmetic procedures.",
            "uuid": "e3138.3",
            "source_info": {
                "paper_title": "Numeric Magnitude Comparison Effects in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "T5",
            "name_full": "Text-to-Text Transfer Transformer (T5)",
            "brief_description": "An encoder-decoder transformer (T5) whose encoder hidden activations for single-token numbers were probed to test for magnitude-comparison behavioral effects and latent number-line structure.",
            "citation_title": "Numeric Magnitude Comparison Effects in Large Language Models",
            "mention_or_use": "use",
            "model_name": "T5 (encoder; base & large)",
            "model_description": "Encoder-decoder transformer family; authors used only encoder outputs for embeddings. Base ~110M parameters; large variant ~335M parameters used in experiments. Numbers treated as single tokens and probed across layers.",
            "arithmetic_task_type": "Magnitude comparison (distance/size/ratio) for numbers 1-9 (lowercase/mixed-case words and digits); MDS latent-number-line recovery.",
            "reported_mechanism": "Magnitude encoded in encoder hidden activations; cosine similarity of number vectors predicts discriminability and maps to human RT via linking hypothesis; MDS recovers approximately log-compressed ordering in many cases.",
            "evidence_for_mechanism": "High distance-effect R^2 (Table 2 shows T5 often near top among models), decent ratio-effect fits (Table 6), and MDS correlations positive in many layers; digits improve size-effect signals.",
            "evidence_against_mechanism": "Size effect weaker for number-word formats; layerwise trends vary and some deeper layers show reduced MDS correlations; limited number range restricts claims to magnitude representation not full arithmetic.",
            "intervention_type": "Base vs large variant comparisons, input-format manipulation, layerwise probing.",
            "effect_of_intervention": "Large variants maintain or slightly modify layerwise strengths; digits input improves size and ratio signals; overall patterns consistent with other architectures.",
            "performance_metrics": "Distance-effect R^2 across layers/formats high (averages per Table 3 ~0.953 overall for T5); size-effect R^2 digits ~0.886 (Table 5) and averaged ~0.709; ratio-effect averaged ~0.826 (Table 6); MDS correlations vary with layers (Table 9).",
            "notable_failure_modes": "Size effect limited to digits; deviations in 1-D MDS residuals for particular numbers across models; no demonstration of algorithmic arithmetic or multi-digit computation.",
            "comparison_to_humans_or_symbolic": "Displays human-like distance and ratio behavioral signatures; authors interpret this as evidence that statistical pretraining can create magnitude-like internal representations without explicit symbolic arithmetic mechanisms.",
            "uuid": "e3138.4",
            "source_info": {
                "paper_title": "Numeric Magnitude Comparison Effects in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BART",
            "name_full": "Bidirectional and Auto-Regressive Transformers (BART)",
            "brief_description": "An encoder-decoder denoising autoencoder transformer (BART) probed for number-token hidden activations and evaluated for distance/size/ratio magnitude effects and latent number-line structure.",
            "citation_title": "Numeric Magnitude Comparison Effects in Large Language Models",
            "mention_or_use": "use",
            "model_name": "BART (base & large)",
            "model_description": "Encoder-decoder transformer used for denoising pretraining. Base ~140M parameters; large variant ~406M parameters. Authors used hidden activations for number tokens (encoder side) across layers.",
            "arithmetic_task_type": "Magnitude comparison (distance/size/ratio) for 1–9 across input formats; MDS for latent number-line recovery.",
            "reported_mechanism": "Vector-space magnitude encoding: pairwise cosine similarity of embeddings predicts discriminability (linking hypothesis); MDS recovers ordering approximating log-compressed MNL in many cases, especially with digit inputs.",
            "evidence_for_mechanism": "High distance-effect R^2 across many layers (Table 2); ratio-effect R^2 high for digits and words in many layers (Table 6); MDS visualizations (example best layers) show clear latent ordering; digits often yield strongest signals.",
            "evidence_against_mechanism": "Size effect less consistent for number-word formats; deeper-layer declines in some metrics; residual analysis shows deviations for numerals such as 2 and 9.",
            "intervention_type": "Base vs large variants, input-format manipulations, layerwise probing.",
            "effect_of_intervention": "Large BART preserves distance/ratio effects; digits produce stronger size-effect signatures; layerwise trends similar to other models with some deeper-layer degradation in MDS correlation.",
            "performance_metrics": "Distance-effect R^2 averaged across layers/formats ~0.959 (Table 3); size-effect R^2 digits ~0.885 (Table 5); ratio-effect averaged ~0.838 (Table 6); MDS correlations positive across many layers (Tables 8-9).",
            "notable_failure_modes": "Size effect absent for some word formats; MDS residual anomalies; analysis restricted to 1–9, so no claims about multi-digit arithmetic or algorithmic computation.",
            "comparison_to_humans_or_symbolic": "Authors report BART representations reproduce human-like distance and ratio patterns under the linking hypothesis and approximate a log-compressed MNL in embedding space, indicating emergent magnitude encoding rather than explicit symbolic arithmetic.",
            "uuid": "e3138.5",
            "source_info": {
                "paper_title": "Numeric Magnitude Comparison Effects in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do NLP models know numbers? probing numeracy in embeddings",
            "rating": 2
        },
        {
            "paper_title": "Exploring numeracy in word embeddings",
            "rating": 2
        },
        {
            "paper_title": "Numeracy for language models: Evaluating and improving their ability to predict numbers",
            "rating": 2
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2
        },
        {
            "paper_title": "Representing numbers in NLP: a survey and a vision",
            "rating": 2
        },
        {
            "paper_title": "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
            "rating": 1
        },
        {
            "paper_title": "Measuring mathematical problem solving with the MATH dataset",
            "rating": 1
        }
    ],
    "cost": 0.01756775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Numeric Magnitude Comparison Effects in Large Language Models</h1>
<p>Raj Sanjay Shah, Vijay Marupudi, Reba Koenen, Khushi Bhardwaj, Sashank Varma<br>Georgia Institute of Technology<br>{rajsanjayshah, vijaymarupudi, rkoenen3, khushi.bhardwaj, varma}@gatech.edu</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that $4&lt;5$ ) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard benchmarks. Here, we ask a different question, one inspired by cognitive science: How closely do the number representations of LLMs correspond to those of human language users, who typically demonstrate the distance, size, and ratio effects? We depend on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times. The results reveal surprisingly human-like representations across language models of different architectures, despite the absence of the neural circuitry that directly supports these representations in the human brain. This research shows the utility of understanding LLMs using behavioral benchmarks and points the way to future work on the number representations of LLMs and their cognitive plausibility.</p>
<h2>1 Introduction</h2>
<p>Humans use symbols - number words such as "three" and digits such as " 3 " - to quantify the world. How humans understand these symbols has been the subject of cognitive science research for half a century. The dominant theory is that people understand number symbols by mapping them to mental representations, specifically magnitude representations (Moyer and Landauer, 1967). This is true for both number words (e.g., "three") and digits (e.g., " 3 "). These magnitude representations are organized as a "mental number line" (MNL), with numbers mapped to points on the line as shown in</p>
<p>Figure 1d. Cognitive science research has revealed that this representation is present in the minds of young children (Ansari et al., 2005) and even nonhuman primates (Nieder and Miller, 2003). Most of this research has been conducted with numbers in the range 1-9, in part, because corpus studies have shown that 0 belongs to a different distribution (Dehaene and Mehler, 1992) and, in part, because larger numbers require parsing place-value notation (Nuerk et al., 2001), a cognitive process beyond the scope of the current study.</p>
<p>Evidence for this proposal comes from magnitude comparison tasks in which people are asked to compare two numbers (e.g., 3 vs. 7) and judge which one is greater (or lesser). Humans have consistently exhibited three effects that suggest recruitment of magnitude representations to understand numbers: the distance effect, the size effect, and the ratio effect (Moyer and Landauer, 1967; Merkley and Ansari, 2010). We review the experimental evidence for these effects, shown in Figure 1, in LLMs. Our behavioral benchmarking approach shifts the focus from what abilities LLMs have in an absolute sense to whether they successfully mimic human performance characteristics. This approach can help differentiate between human tendencies captured by models and the model behaviors due to training strategies. Thus, the current study bridges between Natural Language Processing (NLP), computational linguistics, and cognitive science.</p>
<h3>1.1 Effects of Magnitude Representations</h3>
<p>Physical quantities in the world, such as the brightness of a light or the loudness of a sound, are encoded as logarithmically scaled magnitude representations (Fechner, 1860). Research conducted with human participants and non-human species has revealed that they recruit many of the same brain regions, such as the intra-parietal sulcus, to determine the magnitude of symbolic numbers (Billock and Tsou, 2011; Nieder and Dehaene, 2009).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: The input types, LLMs, and effects in this study. The three effects are depicted in an abstract manner in sub-figures (a), (b), (c).</p>
<p>Three primary magnitude representation effects have been found using the numerical comparison task in studies of humans. First, comparisons show a distance effect: The greater the distance $|x-y|$ between the numbers $x$ vs. $y$, the faster the comparison <em>Moyer and Landauer (1967)</em>. Thus, people compare 1 vs. 9 faster than 1 vs. 2. This is shown in abstract form in Figure 1a. This effect can be explained by positing that people possess an MNL. When comparing two numbers, they first locate each number on this representation, determine which one is “to the right”, and choose that number as the greater one. Thus, the farther the distance between the two points, the easier (and thus faster) the judgment.</p>
<p>Second, comparisons show a size effect: Given two comparisons of the same distance (i.e., of the same value for $|x-y|$), the smaller the numbers, the faster the comparison <em>Parkman (1971)</em>. For example, 1 vs. 2 and 8 vs. 9 both have the same distance (i.e., $|x-y|=1$), but the former involves smaller numbers and is therefore the easier (i.e., faster) judgment. The size effect is depicted in abstract form in Figure 1b. This effect also references the MNL, but a modified version where the points are logarithmically compressed, i.e., the distance from 1 to $x$ is proportional to $\log(x)$; see Figure 1d. To investigate if a logarithmically compressed number line is also present in LLMs, we use multidimensional scaling <em>Ding (2018)</em> on the cosine distances between number embeddings.</p>
<p>Third, comparisons show a ratio effect: The time to compare two numbers $x$ vs. $y$ is a decreasing function of the ratio of the larger number over the smaller number, i.e., $\frac{\max(x,y)}{\min(x,y)}$ <em>Halberda et al. (2008)</em>. This function is nonlinear, as depicted in abstract form in Figure 1c. Here, we assume that this function is a negative exponential, though other functional forms have been proposed in the cognitive science literature. The ratio effect can also be explained by the logarithmically compressed MNL depicted in Figure 1d.</p>
<p>These three effects — distance, size, and ratio — have been replicated numerous times in studies of human adults and children, non-human primates, and many other species <em>Cantlon (2012); Cohen Kadosh et al. (2008)</em>. The MNL model in Figure 1d accounts for these effects (and many others in the mathematical cognition literature). Here, we use LLMs to evaluate a novel scientific hypothesis: that the MNL representation of the human mind is latent in the statistical structure of the linguistic environment, and thus learnable. Therefore, there is less need to posit pre-programmed neural circuitry to explain magnitude effects.</p>
<h3>1.2 LLMs and Behavioral Benchmarks</h3>
<p>Modern NLP models are pre-trained on large corpora of texts from diverse sources such as Wikipedia <em>Wikipedia contributors (2004)</em> and the open book corpus <em>Zhu et al. (2015)</em>. LLMs like BERT <em>Devlin et al. (2018)</em>, ROBERTA <em>Liu et al. (2019)</em> and GPT-2 <em>Radford et al. (2019)</em> learn contextual semantic vector representations of words.</p>
<p>These models have achieved remarkable success on NLP benchmarks (Wang et al., 2018). They can perform as well as humans on a number of language tests such as semantic verification (Bhatia and Richie, 2022) and semantic disambiguation (Lake and Murphy, 2021).</p>
<p>Most benchmarks are designed to measure the absolute performance of LLMs, with higher accuracy signaling "better" models. Human or superhuman performance is marked by exceeding certain thresholds. Here, we ask not whether LLMs can perform well or even exceed human performance at tasks, but whether they show the same performance characteristics as humans while accomplishing the same tasks. We call these behavioral benchmarks. The notion of behavioral benchmarks requires moving beyond accuracy (e.g., scores) as the dominant measure of LLM performance.</p>
<p>As a test case, we look at the distance, size, and ratio effects as behavioral benchmarks to determine whether LLMs understand numbers as humans do, using magnitude representations. This requires a linking hypothesis to map measures of human performance to indices of model performance. Here, we map human response times on numerical comparison tasks to similarity computations on number word embeddings.</p>
<h3>1.3 Research Questions</h3>
<p>The current study investigates the number representations of LLMs and their alignment with the human MNL. It addresses five research questions:</p>
<ol>
<li>Which LLMs, if any, capture the distance, size, and ratio effects exhibited by humans?</li>
<li>How do different layers of LLMs vary in exhibiting these effects?</li>
<li>How do model behaviors change when using larger variants (more parameters) of the same architecture?</li>
<li>Do the models show implicit numeration ("four" = "4"), i.e., do they exhibit these effects equally for all number symbol types or more for some types (e.g., digits) than others (e.g., number words)?</li>
<li>Is the MNL representation depicted in Figure 1d latent in the representations of the models?</li>
</ol>
<h2>2 Related Work</h2>
<p>Research on the numerical abilities of LLMs focuses on several aspects of mathematical reasoning (Thawani et al., 2021), such as magnitude com- parison, numeration (Naik et al., 2019; Wallace et al., 2019), arithmetic word problems (Burns et al., 2021; Amini et al., 2019), exact facts (Lin et al., 2020), and measurement estimation (Zhang et al., 2020). The goal is to improve performance on application-driven tasks that require numerical skills. Research in this area typically attempts to (1) understand the numerical capabilities of pretrained models and (2) propose new architectures that improve numerical cognition abilities (Geva et al., 2020; Dua et al., 2019).</p>
<p>Our work also focuses on the first research direction: probing the numerical capabilities of pretrained models. Prior research by Wallace et al. (2019) judges the numerical reasoning of various contextual and non-contextual models using different tests (e.g., finding the maximum number in a list, finding the sum of two numbers from their word embeddings, decoding the original number from its embedding). These tasks have been presented as evaluation criteria for understanding the numerical capabilities of models. Spithourakis and Riedel (2018) change model architectures to treat numbers as distinct from words. Using perplexity score as a proxy for numerical abilities, they argue that this ability reduces model perplexity in neural machine translation tasks. Other work focuses on finding numerical capabilities through building QA benchmarks for performing discrete reasoning (Dua et al., 2019). Most research in this direction casts different tasks as proxies of numerical abilities of NLP systems (Weiss et al., 2018; Dua et al., 2019; Spithourakis and Riedel, 2018; Wallace et al., 2019; Burns et al., 2021; Amini et al., 2019).</p>
<p>An alternative approach by Naik et al. (2019) tests multiple non-contextual task-agnostic embedding generation techniques to identify the failures in models' abilities to capture the magnitude and numeration effects of numbers. Using a systematic foundation in cognitive science research, we build upon their work in two ways: we (1) use contextual embeddings spanning a wide variety of pre-training strategies, and (2) evaluate models by comparing their behavior to humans. Our work looks at numbers in an abstract sense, and is relevant for the grounding problem studied in artificial intelligence and cognitive science (Harnad, 2023).</p>
<h2>3 Experimental Design</h2>
<p>The literature lacks adequate experimental studies demonstrating magnitude representations of num-</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Category</th>
<th>Size</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Base</td>
<td>Large</td>
</tr>
<tr>
<td>BERT <em>Devlin et al. (2018)</em></td>
<td>Encoder</td>
<td>110M</td>
<td>340M</td>
</tr>
<tr>
<td>RoBERTA <em>Liu et al. (2019)</em></td>
<td>Encoder</td>
<td>125M</td>
<td>355M</td>
</tr>
<tr>
<td>XLNET <em>Yang et al. (2019)</em></td>
<td>Auto-regressive Encoder</td>
<td>110M</td>
<td>340M</td>
</tr>
<tr>
<td>GPT-2 <em>Radford et al. (2019)</em></td>
<td>Auto-regressive Decoder</td>
<td>117M</td>
<td>345M</td>
</tr>
<tr>
<td>T5 <em>Raffel et al. (2019)</em></td>
<td>Encoder</td>
<td>110M</td>
<td>335M</td>
</tr>
<tr>
<td>BART <em>Lewis et al. (2020)</em></td>
<td>Encoder-Decoder</td>
<td>140M</td>
<td>406M</td>
</tr>
</tbody>
</table>
<p>Table 1: Popular Language Models
- bers in LLMs from a cognitive science perspective. The current study addresses this gap. We propose a general methodology for mapping human response times to similarities computed over LLM embeddings. We test for the three primary magnitude representation effects described in section 1.1.</p>
<h3>3.1 Linking Hypothesis</h3>
<p>In studies with human participants, the distance, size, and ratio effects are measured using reaction time. Each effect depends on the assumption that when comparing which of two numbers $x$ and $y$ is relatively easy, humans are relatively fast, and when it is relatively difficult, they are relatively slow. The ease or difficulty of the comparison is a function of $x$ and $y$: $|x-y|$ for the distance effect, $\min(x,y)$ for the size effect, and $\frac{\max(x,y)}{\min(x,y)}$ for the ratio effect. LLMs do not naturally make reaction time predictions. Thus, we require a linking hypothesis to estimate the relative ease or difficulty of comparisons for LLMs. Here we adopt the simple assumption that the greater the similarity of two number representations in an LLM, the longer it takes to discriminate them, i.e., to judge which one is greater (or lesser).</p>
<p>We calculate the similarity of two numbers based on the similarity of their vector representations. Specifically, the representation of a number for a given layer of a given model is the vector of activation across its units. There are many similarity metrics for vector representations <em>Wang and Dong (2020)</em>: Manhattan, Euclidean, cosine, dot product, etc. Here, we choose a standard metric in distributional semantics: the cosine of the angle between the vectors <em>Richie and Bhatia (2021)</em>. This reasoning connects an index of model function (i.e., the similarity of the vector representations of two numbers) to a human behavioral measure (i.e., reaction time). Thus, the more similar the two representations are, the less discriminable they are from each other, and thus the longer the reaction time to select one over the other.</p>
<h3>3.2 Materials</h3>
<p>For these experiments, we utilized three formats for number representations in LLMs: lowercase number words, mixed-cased number words (i.e., the first letter is capitalized), and digits. These formats enable us to explore variations in input tokens and understand numeration in models. Below are examples of the three input types:</p>
<ul>
<li>"one", "two", "three", "four" ... "nine"</li>
<li>"One", "Two", "Three", "Four" ... "Nine"</li>
<li>"1", "2", "3", "4" ... "9"</li>
</ul>
<p>As noted in the Introduction, prior studies of the distance, size and ratio effects in humans have largely focused on numbers ranging from 1 to 9. Our input types are not-affected by tokenization methods as the models under consideration have each input as a separate token.</p>
<h3>3.3 Large Language Models - Design Choices</h3>
<p>Modern NLP models are pre-trained on a large amount of unlabeled textual data from a diverse set of sources. This enables LLMs to learn contextually semantic vector representations of words. We experiment on these vectors to evaluate how one specific dimension of human knowledge - number sense - is captured in different model architectures.</p>
<p>We use popular large language models from Huggingface’s Transformers library <em>Wolf et al. (2020)</em> to obtain vector representations of numbers in different formats. Following the work by Min et al. <em>Min et al. (2021)</em> to determine popular model architectures, we select models from three classes of architectural design: encoder models (e.g., BERT <em>Devlin et al. (2018)</em>), auto-regressive models (e.g., GPT-2 <em>Radford et al. (2019)</em>), and encoder-decoder models (e.g., T5 <em>Raffel et al. (2019)</em>). The final list of models is provided in Table 1.</p>
<p>Operationalization: We investigate the three number magnitude effects as captured in the representations of each layer of the six models for the three number formats. For these experiments, we consider only the obtained hidden layer outputs for the tokens corresponding to the input number word tokens. We ignore the special prefix and suffix tokens of models (e.g., the [cls] token in BERT) for uniformity among different architectures. For the T5-base model, we use only the encoder to obtain model embedding. All models tested use a similar number of model parameters (around 110-140 million parameters). For our studies, we arbitrarily choose the more popular BERT uncased variant as</p>
<p>opposed to the cased version. We compare the two models in Appendix section A. 2 for a complete analysis, showing similar behaviors in the variants. Model size variations for the same architecture are considered in the Appendix section A. 1 to show the impact of model size on the three effects.</p>
<h2>4 Magnitude Representation Effects in LLMs</h2>
<h3>4.1 The Distance Effect</h3>
<table>
<thead>
<tr>
<th style="text-align: center;">Layer</th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">BART</th>
<th style="text-align: center;">RoB</th>
<th style="text-align: center;">XLNET</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">GPT-2</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.979</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.963</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.963</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.957</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.957</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.955</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.972</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.953</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.959</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.950</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.948</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.928</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.946</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.914</td>
</tr>
</tbody>
</table>
<p>Table 2: Distance Effect: Averaged (across the three number formats) $R^{2}$ values of different LLMs for different layers when fitting a linear function. RoB: Robertabase model, BERT: uncased variant.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLMs $\backslash$ Input</th>
<th style="text-align: center;">LC</th>
<th style="text-align: center;">MC</th>
<th style="text-align: center;">Digits</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: center;">0.986</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">0.953</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.959</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.951</td>
</tr>
<tr>
<td style="text-align: left;">XLNET</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.979</td>
<td style="text-align: center;">0.944</td>
</tr>
<tr>
<td style="text-align: left;">BERT (uncased)</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.986</td>
<td style="text-align: center;">0.932</td>
</tr>
<tr>
<td style="text-align: left;">Total Averages</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 5}$</td>
<td style="text-align: center;">0.950</td>
</tr>
<tr>
<td style="text-align: left;">across models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Distance Effect: Averaged (across layers) $R^{2}$ values of different LLMs on the three numbers when fitting a linear function. LC: Lowercase number words, MC: Mixed-case number words.</p>
<p>Recall that the distance effect is that people are slower (i.e., find it more difficult) to compare numbers the closer they are to each other on the MNL. We use the pipeline depicted in Figure 1 to investigate if LLM representations are more similar to each other if the numbers are closer on the MNL.</p>
<p>Evaluation of the distance effect in LLMs is done by fitting a straight line $(a+b x)$ on the cosine similarity vs. distance plot. We first perform two operations on these cosine similarities: (1) We average the similarities across each distance (e.g., the point
at distance 1 on the $x$-axis represents the average similarity of 1 vs. 2,2 vs. $3, \ldots, 8$ vs. 9). (2) We normalize the similarities to be in the range [0, 1]. These decisions allow relative output comparisons across different model architectures, which is not possible using the raw cosine similarities of each LLM. To illustrate model performance, the distance effects for the best-performing layer in terms of $R^{2}$ values for BART are shown in Figure 2 for the three number formats. The high $R^{2}$ values indicate a human-like distance effect.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Layer</th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">BART</th>
<th style="text-align: center;">RoB</th>
<th style="text-align: center;">XLNET</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">GPT-2</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.756</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.597</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.637</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.636</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.744</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.641</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.677</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.627</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.665</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.528</td>
<td style="text-align: center;">0.635</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">0.563</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.635</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.701</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.649</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.659</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.541</td>
<td style="text-align: center;">0.670</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.555</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.672</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">0.756</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.911</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.713</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.744</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.708</td>
</tr>
</tbody>
</table>
<p>Table 4: Size Effect: Averaged (across inputs) $R^{2}$ values of different LLMs on different input layers when fitting a linear function. RoB: Roberta-base model, BERT: uncased variant.</p>
<p>All of the models show strong distance effects for all layers, as shown in Table 2, and for all number formats, as shown in Table 3. Interestingly, LLMs are less likely to reveal the distance effect as layer count increases (Table 2). For example, layer one results in the strongest distance effect while layer twelve is the least representative of the distance effect. With respect to number format, passing digits as inputs tended to produce stronger distance effects than passing number words (Table 3); this pattern was present for four of the six LLMs (i.e., all but T5 and BERT).</p>
<h3>4.2 The Size Effect</h3>
<p>The size effect holds for comparisons of the same distance (e.g., for a distance of 1 , these include 1 vs. 2,2 vs. $3, \ldots, 8$ vs. 9). Among these comparisons, those involving larger numbers (e.g., 8 vs . 9) are made more slowly (i.e., people find them more difficult) than those involving smaller numbers (e.g., 1 vs. 2). That larger numbers are harder to differentiate than smaller numbers aligns with the logarithmically compressed MNL depicted in</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Distance effect for the best-performing layer (9th layer) for the BART model</p>
<p>Figure 1d. This study evaluates whether a given LLM shows a size effect on a given layer for numbers of a given format by plotting the normalized cosine similarities against the size of the comparison, defined as the minimum of the two numbers being compared. For each minimum value (points on the <em>x</em>-axis), we average the similarities for all comparisons to form a single point (vertical compression). We then fit a straight line (<em>ax</em> + <em>b</em>) over the vertically compressed averages (blue line in Figure 3) to obtain the <em>R</em><sup>2</sup> values (scores). To illustrate model performance, the size effects for the best-performing layer of the BERT-uncased model (in terms of <em>R</em><sup>2</sup> values) are shown in Figure 3. Similar to the results for the distance effect, the high <em>R</em><sup>2</sup> values indicate a human-like size effect.</p>
<p>Interestingly, Table 4 generally shows an increasing trend in the layer-wise capability of capturing the size effect across the six LLMs. This is opposite to the trend observed across layers for the distance effect. Table 5 shows that using digits as the input values yields significantly better <em>R</em><sup>2</sup> values than the other number formats. In fact, this is the only number format for which the models produce strong size effects. However, the vertical compression of points fails to capture the spread of points across the <em>y</em>-axis for each point on the <em>x</em>-axis. This spread, a limitation of the size effect analysis, is captured in the ratio effect (section 4.3).</p>
<table>
<thead>
<tr>
<th>LLMs\Input</th>
<th>LC</th>
<th>MC</th>
<th>Digits</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>T5</td>
<td>0.702</td>
<td>0.539</td>
<td>0.886</td>
<td>0.709</td>
</tr>
<tr>
<td>BART</td>
<td>0.614</td>
<td>0.568</td>
<td>0.885</td>
<td>0.689</td>
</tr>
<tr>
<td>RoBERTa</td>
<td>0.520</td>
<td>0.466</td>
<td>0.783</td>
<td>0.59</td>
</tr>
<tr>
<td>XLNET</td>
<td>0.500</td>
<td>0.408</td>
<td>0.793</td>
<td>0.567</td>
</tr>
<tr>
<td>BERT (uncased)</td>
<td>0.803</td>
<td></td>
<td>0.851</td>
<td>0.827</td>
</tr>
<tr>
<td>GPT-2</td>
<td>0.434</td>
<td>0.332</td>
<td>0.853</td>
<td>0.54</td>
</tr>
<tr>
<td>Total Averages across models</td>
<td>0.596</td>
<td>0.519</td>
<td>0.842</td>
<td>0.654</td>
</tr>
</tbody>
</table>
<p>Table 5: Size Effect: Averaged (across layers) <em>R</em><sup>2</sup> values of different LLMs on the three number formats when fitting a linear function. LC: Lowercase number words, MC: Mixed-case number words.</p>
<h3>4.3 The Ratio Effect</h3>
<table>
<thead>
<tr>
<th>LLMs\Input</th>
<th>LC</th>
<th>MC</th>
<th>Digits</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>T5</td>
<td>0.852</td>
<td>0.756</td>
<td>0.868</td>
<td>0.826</td>
</tr>
<tr>
<td>BART</td>
<td>0.786</td>
<td>0.833</td>
<td>0.897</td>
<td>0.838</td>
</tr>
<tr>
<td>RoBERTa</td>
<td>0.714</td>
<td>0.747</td>
<td>0.746</td>
<td>0.736</td>
</tr>
<tr>
<td>XLNET</td>
<td>0.729</td>
<td>0.761</td>
<td>0.901</td>
<td>0.797</td>
</tr>
<tr>
<td>BERT (uncased)</td>
<td>0.906</td>
<td></td>
<td>0.757</td>
<td>0.831</td>
</tr>
<tr>
<td>GPT-2</td>
<td>0.686</td>
<td>0.758</td>
<td>0.681</td>
<td>0.709</td>
</tr>
<tr>
<td>Total Averages across models</td>
<td>0.779</td>
<td>0.793</td>
<td>0.808</td>
<td>0.789</td>
</tr>
</tbody>
</table>
<p>Table 6: Ratio Effect: Averaged (across layers) <em>R</em><sup>2</sup> values of different LLMs on different number formats when fitting a negative exponential function. LC: Lowercase number words, MC: Mixed-case number words.</p>
<table>
<thead>
<tr>
<th>Layer</th>
<th>T5</th>
<th>BART</th>
<th>RoB</th>
<th>XLNET</th>
<th>BERT</th>
<th>GPT-2</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.850</td>
<td>0.820</td>
<td>0.756</td>
<td>0.868</td>
<td>0.837</td>
<td>0.735</td>
<td>0.811</td>
</tr>
<tr>
<td>2</td>
<td>0.865</td>
<td>0.837</td>
<td>0.745</td>
<td>0.828</td>
<td>0.878</td>
<td>0.755</td>
<td>0.819</td>
</tr>
<tr>
<td>3</td>
<td>0.846</td>
<td>0.861</td>
<td>0.725</td>
<td>0.820</td>
<td>0.853</td>
<td>0.738</td>
<td>0.807</td>
</tr>
<tr>
<td>4</td>
<td>0.847</td>
<td>0.859</td>
<td>0.739</td>
<td>0.822</td>
<td>0.820</td>
<td>0.659</td>
<td>0.791</td>
</tr>
<tr>
<td>5</td>
<td>0.851</td>
<td>0.847</td>
<td>0.805</td>
<td>0.825</td>
<td>0.847</td>
<td>0.695</td>
<td>0.812</td>
</tr>
<tr>
<td>6</td>
<td>0.880</td>
<td>0.821</td>
<td>0.800</td>
<td>0.816</td>
<td>0.883</td>
<td>0.703</td>
<td>0.817</td>
</tr>
<tr>
<td>7</td>
<td>0.867</td>
<td>0.811</td>
<td>0.795</td>
<td>0.810</td>
<td>0.883</td>
<td>0.698</td>
<td>0.811</td>
</tr>
<tr>
<td>8</td>
<td>0.824</td>
<td>0.849</td>
<td>0.780</td>
<td>0.780</td>
<td>0.880</td>
<td>0.702</td>
<td>0.803</td>
</tr>
<tr>
<td>9</td>
<td>0.806</td>
<td>0.852</td>
<td>0.780</td>
<td>0.746</td>
<td>0.861</td>
<td>0.705</td>
<td>0.791</td>
</tr>
<tr>
<td>10</td>
<td>0.785</td>
<td>0.821</td>
<td>0.720</td>
<td>0.754</td>
<td>0.779</td>
<td>0.704</td>
<td>0.760</td>
</tr>
<tr>
<td>11</td>
<td>0.755</td>
<td>0.849</td>
<td>0.666</td>
<td>0.781</td>
<td>0.769</td>
<td>0.702</td>
<td>0.754</td>
</tr>
<tr>
<td>12</td>
<td>0.731</td>
<td>0.834</td>
<td>0.516</td>
<td>0.717</td>
<td>0.687</td>
<td>0.708</td>
<td>0.699</td>
</tr>
</tbody>
</table>
<p>Table 7: Ratio Effect: Averaged (across number formats) <em>R</em><sup>2</sup> values of different LLMs on different input layers when fitting a negative exponential function. RoB: Roberta-base model, BERT: uncased variant.</p>
<p>The ratio effect in humans can be thought of as simultaneously capturing both the distance and size effects. Behaviorally, the time to compare <em>x</em> vs. <em>y</em> is a decreasing function of the ratio of the larger number over the smaller number, i.e., of $$\frac{\max(x,y)}{\min(x,y)}$$. In fact, the function is nonlinear as depicted in Figure 1c. For the LLMs, we plot the normalized cosine similarity vs. $$\frac{\max(x,y)}{\min(x,y)}$$. To each plot, we fit the negative exponential function $$a * e^{-bx} + c$$ and</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Size effect for the best-performing layer for the BERT model (layer 11).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ratio effect for the best-performing layer for the BART model (layer 3).</p>
<p>evaluate the resulting <em>R</em>². To illustrate model performance, Figure 4 shows the ratio effects for the best-fitting layer of the BART model for the three number formats. As observed with the distance and size effect, the high <em>R</em>² values of the LLMs indicate a human-like ratio effect in the models.</p>
<table>
<thead>
<tr>
<th>LLMs\Input</th>
<th>LC</th>
<th>MC</th>
<th>Digits</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>T5</td>
<td>0.489</td>
<td>0.526</td>
<td>0.410</td>
<td>0.475</td>
</tr>
<tr>
<td>BART</td>
<td>0.676</td>
<td>0.714</td>
<td>0.678</td>
<td>0.690</td>
</tr>
<tr>
<td>RoBERTa</td>
<td>0.520</td>
<td>0.597</td>
<td>0.587</td>
<td>0.568</td>
</tr>
<tr>
<td>XLNET</td>
<td>0.622</td>
<td>0.620</td>
<td>0.622</td>
<td>0.621</td>
</tr>
<tr>
<td>BERT (uncased)</td>
<td>0.312</td>
<td>0.423</td>
<td>0.368</td>
<td></td>
</tr>
<tr>
<td>GPT-2</td>
<td>0.566</td>
<td>0.513</td>
<td><strong>0.828</strong></td>
<td>0.636</td>
</tr>
<tr>
<td>Total Averages across models</td>
<td>0.531</td>
<td>0.547</td>
<td><strong>0.591</strong></td>
<td>0.560</td>
</tr>
</tbody>
</table>
<p>Table 8: Averaged (across layers) correlations when comparing MDS values with <em>Log</em>₁₀₁ to <em>Log</em>₁₀₉₉ for different LLMs. LC: Lowercase number words, MC: Mixed-case number words.</p>
<h3>4.4 Multidimensional Scaling</h3>
<p>Along with the three magnitude effects, we also investigate whether the number representations of LLMs are consistent with the human MNL. To do so, we utilize multidimensional scaling (Borg and Groenen, 2005; Ding, 2018). MDS offers a method for recovering the latent structure in the matrix of cosine (dis)similarities between the vector representations of all pairs of numbers (for a given LLM, layer, and number format). It arranges each number in a space of <em>N</em> dimensions such that the</p>
<table>
<thead>
<tr>
<th>Layer</th>
<th>T5</th>
<th>BART</th>
<th>RoB</th>
<th>XLNET</th>
<th>BERT</th>
<th>GPT-2</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.686</td>
<td>0.679</td>
<td>0.602</td>
<td>0.595</td>
<td>0.739</td>
<td>0.526</td>
<td>0.638</td>
</tr>
<tr>
<td>2</td>
<td>0.271</td>
<td>0.693</td>
<td>0.763</td>
<td>0.734</td>
<td>0.704</td>
<td>0.669</td>
<td>0.639</td>
</tr>
<tr>
<td>3</td>
<td>0.374</td>
<td>0.657</td>
<td>0.772</td>
<td>0.704</td>
<td>0.456</td>
<td>0.685</td>
<td>0.608</td>
</tr>
<tr>
<td>4</td>
<td>0.385</td>
<td>0.728</td>
<td>0.489</td>
<td>0.621</td>
<td>0.425</td>
<td>0.663</td>
<td>0.552</td>
</tr>
<tr>
<td>5</td>
<td>0.476</td>
<td>0.733</td>
<td>0.597</td>
<td>0.707</td>
<td>0.448</td>
<td>0.615</td>
<td>0.596</td>
</tr>
<tr>
<td>6</td>
<td>0.540</td>
<td>0.739</td>
<td>0.571</td>
<td>0.598</td>
<td>0.465</td>
<td>0.608</td>
<td>0.587</td>
</tr>
<tr>
<td>7</td>
<td>0.687</td>
<td>0.696</td>
<td>0.250</td>
<td>0.677</td>
<td>0.445</td>
<td>0.665</td>
<td>0.570</td>
</tr>
<tr>
<td>8</td>
<td>0.529</td>
<td>0.624</td>
<td>0.594</td>
<td>0.591</td>
<td>0.189</td>
<td>0.624</td>
<td>0.525</td>
</tr>
<tr>
<td>9</td>
<td>0.544</td>
<td>0.718</td>
<td>0.691</td>
<td>0.566</td>
<td>0.400</td>
<td>0.671</td>
<td>0.598</td>
</tr>
<tr>
<td>10</td>
<td>0.502</td>
<td>0.624</td>
<td>0.697</td>
<td>0.563</td>
<td>0.394</td>
<td>0.613</td>
<td>0.566</td>
</tr>
<tr>
<td>11</td>
<td>0.195</td>
<td>0.708</td>
<td>0.602</td>
<td>0.543</td>
<td>-0.013</td>
<td>0.675</td>
<td>0.451</td>
</tr>
<tr>
<td>12</td>
<td>0.509</td>
<td>0.677</td>
<td>0.186</td>
<td>0.557</td>
<td>-0.239</td>
<td>0.615</td>
<td>0.384</td>
</tr>
</tbody>
</table>
<p>Table 9: Averaged (across inputs) correlations of different LLMs on different model layers when comparing MDS values with <em>Log</em>₁₀₁ to <em>Log</em>₁₀₉₉. RoB: Robertabase model, BERT: uncased variant.</p>
<table>
<thead>
<tr>
<th>Number</th>
<th>T5</th>
<th>BART</th>
<th>RoB</th>
<th>XLNET</th>
<th>BERT</th>
<th>GPT-2</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.01</td>
<td>0.00</td>
<td>0.02</td>
<td>0.00</td>
<td>0.02</td>
<td>0.00</td>
<td>0.01</td>
</tr>
<tr>
<td>2</td>
<td>0.10</td>
<td>0.17</td>
<td>0.15</td>
<td>0.17</td>
<td>0.09</td>
<td>0.12</td>
<td><strong>0.13</strong></td>
</tr>
<tr>
<td>3</td>
<td>0.07</td>
<td>0.05</td>
<td>0.07</td>
<td>0.10</td>
<td>0.06</td>
<td>0.10</td>
<td>0.07</td>
</tr>
<tr>
<td>4</td>
<td>0.05</td>
<td>0.04</td>
<td>0.05</td>
<td>0.05</td>
<td>0.03</td>
<td>0.05</td>
<td>0.04</td>
</tr>
<tr>
<td>5</td>
<td>0.17</td>
<td>0.09</td>
<td>0.07</td>
<td>0.05</td>
<td>0.20</td>
<td>0.05</td>
<td><strong>0.11</strong></td>
</tr>
<tr>
<td>6</td>
<td>0.02</td>
<td>0.04</td>
<td>0.08</td>
<td>0.02</td>
<td>0.06</td>
<td>0.04</td>
<td>0.04</td>
</tr>
<tr>
<td>7</td>
<td>0.09</td>
<td>0.08</td>
<td>0.11</td>
<td>0.04</td>
<td>0.20</td>
<td>0.06</td>
<td><strong>0.10</strong></td>
</tr>
<tr>
<td>8</td>
<td>0.04</td>
<td>0.01</td>
<td>0.08</td>
<td>0.01</td>
<td>0.09</td>
<td>0.05</td>
<td>0.05</td>
</tr>
<tr>
<td>9</td>
<td>0.40</td>
<td>0.08</td>
<td>0.17</td>
<td>0.18</td>
<td>0.44</td>
<td>0.17</td>
<td><strong>0.24</strong></td>
</tr>
</tbody>
</table>
<p>Table 10: Residual analysis on MDS outputs in 1 dimension on the base variants of the model. RoB: Robertabase model, BERT: uncased variant.</p>
<p>distance between each pair of points is consistent with the cosine dissimilarity between their vector</p>
<p>representations.
We fix $N=1$ to recover the latent MNL representation for each LLM, layer, and number format. For each solution, we anchor the point for "1" to the left side and evaluate whether the resulting visualization approximates the log compressed MNL as shown in Figure 1d. To quantify this approximation, we calculate the correlation between the positions of the numbers 1 to 9 in the MDS solution and the expected values $(\log (1)$ to $\log (9))$ of the human MNL; see Table 8. All inputs have similar correlation values. Surprisingly, GPT-2 with digits as the number format (and averaged across all layers) shows a considerably higher correlation with the log-compressed MNL than all other models and number formats. The average correlation between latent model number lines and the log compressed MNL decreases over the 12 layers; see Table 9.</p>
<p>We visualize the latent number line of GPT-2 by averaging the cosine dissimilarity matrix across layers and number formats, submitting this to MDS, and requesting a one-dimensional solution; see Figure 5. This representation shows some evidence of log compression, though with a few exceptions. One obvious exception is the right displacement of 2 away from 1. Another is the right displacement of 9 very far from 8.</p>
<p>To better understand if this is a statistical artifact of GPT-2 or a more general difference between number understanding in humans versus LLMs, we perform a residual analysis comparing positions on the model's number line to those on the human MNL. We choose the digits number format, estimate the latent number line representation averaged across the layers of each model, and compute the residual between the position of each number in this representation compared to the human MNL. This analysis is presented in Table 10. For 1, all models show a residual value of less than 0.03 . This makes sense given our decision to anchor the latent number lines to 1 on the left side. The largest residuals are for 2 and 9 , consistent with the anomalies noticed for the GPT-2 solution in Figure 5. These anomalies are a target for future research. We note here that 2 is often privileged even in languages such as Piraha and Mundurucu that have very limited number of word inventories(Gordon, 2004; Pica et al., 2004). Further note that 9 has special significance as a "bargain price numeral" in many cultures, a fact that is often linguistically marked (Pollmann and Jansen, 1996).</p>
<p>Model: GPT2, Averaged pairwise distances
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: MDS visualization on averaged distances of the GPT-2 model for all number formats and layers.</p>
<h3>4.5 Ablation studies: Base vs Large Model Variants</h3>
<p>We investigate changes in model behaviors when increasing the number of parameters for the same architectures. We use the larger variants of each of the LLMs listed in Table 1. The detailed tabular results of the behaviors are presented in Appendix section A.1; see Tables 11, 12, and 13. Here, we summarize key takeaways from the ablation studies:</p>
<ul>
<li>The distance and ratio effects of the large variants of models align with human performance characteristics. Similar to the results for the base variants, the size effect is only observed when the input type is digits.</li>
<li>We observe the same decreasing trend in the layer-wise capability of capturing the distance effect, ratio effect, and the MDS correlation values in the Large variants of LLMs as observed in the base variants. The increasing trend in the layer-wise capability of the size effect is not observed in the Larger LLMs.</li>
<li>Residual analysis shows high deviation for the numbers "2", "5", and "9"; which is in line with our observations for the base variations.</li>
</ul>
<h2>5 Conclusion</h2>
<p>This paper investigates the performance characteristics in various LLMs across numerous configurations, looking for three number-magnitude comparison effects: distance, size, and ratio. Our results show that LLMs show human-like distance and ratio effects across number formats. The size effect is also observed among models for the digit number format, but not for the other number formats, showing that LLMs do not completely capture numeration. Using MDS to scale down the pairwise (dis)similarities between number representations produces varying correspondences between LLMs and the logarithmically compressed MNL of humans, with GPT-2 showing the highest correlation (using digits as inputs). Our residual analysis exhibits high deviation from expected outputs for the numbers $2,5,9$ which we explain through patterns</p>
<p>observed in previous linguistics studies. The behavioral benchmarking of the numeric magnitude representations of LLMs presented here helps us understand the cognitive plausibility of the representations the models learn. Our results show that LLM pre-training allows models to approximately learn human-like behaviors for two out of the three magnitude effects without the need to posit explicit neural circuitry. Future work on building pre-trained architectures to improve numerical cognition abilities should also be evaluated using these three effects.</p>
<h2>6 Limitations</h2>
<p>Limitations to our work are as follows: (1) We only study the three magnitude effects for the number word and digit denotations of the numbers 1 to 9. The effects for the number 0 , numbers greater than 10, decimal numbers, negative numbers, etc. are beyond the scope of this study. Future work can design behavioral benchmark for evaluating whether LLMs shows these effects for these other number classes. (2) The mapping of LLM behaviors to human behaviors and effects might vary for each effect. Thus, we might require a different linking hypothesis for each such effect. (3) We only use the models built for English tasks and do not evaluate multi-lingual models. (4) We report and analyze aggregated scores across different dimensions. There can be some information loss in this aggregation. (5) Our choice of models is limited by certain resource constraints. Future works can explore the use of other foundation / super-large models (1B parameters +) and API-based models like GPT3 and OPT3. (6) The behavioral analysis of this study is one-way: we look for human performance characteristics and behaviors in LLMs. Future research can utilize LLMs to discover new numerical effects and look for the corresponding performance characteristics in humans. This could spur new research in cognitive science. (7) The results show similar outputs to low dimensional human output and show that we do not need explicit neural circuitry for number understanding. We do not suggest models actually are humanlike in how they process numbers.</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math
word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Daniel Ansari, Nicolas Garcia, Elizabeth Lucas, Kathleen Hamon, and Bibek Dhital. 2005. Neural correlates of symbolic number processing in children and adults. Neuroreport, 16(16):1769-1773.</p>
<p>Sudeep Bhatia and Russell Richie. 2022. Transformer networks of human conceptual knowledge. Psychological Review, pages No Pagination Specified-No Pagination Specified.</p>
<p>Vincent A. Billock and Brian H. Tsou. 2011. To honor Fechner and obey Stevens: Relationships between psychophysical and neural nonlinearities. Psychological Bulletin, 137:1-18.
I. Borg and P.J.F. Groenen. 2005. Modern Multidimensional Scaling: Theory and Applications. Springer.</p>
<p>Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. CoRR, abs/2103.03874.</p>
<p>Jessica F. Cantlon. 2012. Math, monkeys, and the developing brain. Proceedings of the National Academy of Sciences, 109(supplement_1):10725-10732.</p>
<p>Roi Cohen Kadosh, Jan Lammertyn, and Veronique Izard. 2008. Are numbers special? An overview of chronometric, neuroimaging, developmental and comparative studies of magnitude representation. Progress in Neurobiology, 84(2):132-147.</p>
<p>Stanislas Dehaene and Jacques Mehler. 1992. Crosslinguistic regularities in the frequency of number words. Cognition, 43(1):1-29.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Cody S. Ding. 2018. Fundamentals of Applied Multidimensional Scaling for Educational and Psychological Research. Springer International Publishing.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Gustav Theodor Fechner. 1860. Elements of psychophysics. 1.</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. CoRR, abs/2004.04487.</p>
<p>Peter Gordon. 2004. Numerical cognition without words: Evidence from amazonia. Science, 306(5695):496-499.</p>
<p>Justin Halberda, Michèle M. M. Mazzocco, and Lisa Feigenson. 2008. Individual differences in nonverbal number acuity correlate with maths achievement. Nature, 455(7213):665-668.</p>
<p>Stevan Harnad. 2023. Symbol grounding problem.
Brenden M. Lake and Gregory L. Murphy. 2021. Word meaning in minds and machines. Psychological review.
M. Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. ArXiv, abs/1910.13461.</p>
<p>Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. 2020. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of PreTrained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6862-6868, Online. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.</p>
<p>Rebecca Merkley and Daniel Ansari. 2010. Using eye tracking to study numerical cognition: The case of the ratio effect. Experimental Brain Research, 206(4):455-460.</p>
<p>Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2021. Recent advances in natural language processing via large pre-trained language models: A survey. CoRR, abs/2111.01243.</p>
<p>Robert S. Moyer and Thomas K. Landauer. 1967. Time required for judgements of numerical inequality. $N a$ ture, 215(5109):1519-1520.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Carolyn Rose, and Eduard Hovy. 2019. Exploring numeracy in word embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3374-3380, Florence, Italy. Association for Computational Linguistics.</p>
<p>Andreas Nieder and Stanislas Dehaene. 2009. Representation of Number in the Brain. Annual Review of Neuroscience, 32(1):185-208.</p>
<p>Andreas Nieder and Earl K. Miller. 2003. Coding of Cognitive Magnitude: Compressed Scaling of Numerical Information in the Primate Prefrontal Cortex. Neuron, 37(1):149-157.</p>
<p>Hans-Christoph Nuerk, Ulrich Weger, and Klaus Willmes. 2001. Decade breaks in the mental number line? Putting the tens and units back in different bins. Cognition, 82(1):B25-B33.</p>
<p>John M. Parkman. 1971. Temporal aspects of digit and letter inequality judgments. Journal of Experimental Psychology, 91(2):191-205.</p>
<p>Pierre Pica, Cathy Lemer, Ve'ronique Izard, and Stanislas Dehaene. 2004. Exact and approximate arithmetic in an amazonian indigene group. Science, 306(5695):499-503.</p>
<p>Thijs Pollmann and Carel Jansen. 1996. The language user as an arithmetician. Cognition, 59(2):219-237.</p>
<p>Alec Radford, Jeff Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer.</p>
<p>Russell Richie and Sudeep Bhatia. 2021. Similarity Judgment Within and Across Categories: A Comprehensive Model Comparison. Cognitive Science, 45(8):e13030.</p>
<p>Georgios P. Spithourakis and Sebastian Riedel. 2018. Numeracy for language models: Evaluating and improving their ability to predict numbers. CoRR, abs/1805.08154.</p>
<p>Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro Szekely. 2021. Representing numbers in NLP: a survey and a vision. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 644-656, Online. Association for Computational Linguistics.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do NLP models know numbers? probing numeracy in embeddings. CoRR, abs/1909.07940.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP.</p>
<p>Jiapeng Wang and Yihong Dong. 2020. Measurement of text similarity: A survey. Information, 11(9).</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On the practical computational power of finite precision mns for language recognition. CoRR, abs/1805.04908.</p>
<p>Wikipedia contributors. 2004. Wikipedia, the free encyclopedia.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. CoRR, abs/1906.08237.</p>
<p>Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. 2020. Do language embeddings capture scales? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4889-4896, Online. Association for Computational Linguistics.</p>
<p>Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV).</p>
<h2>Appendix A Appendix</h2>
<h2>A. 1 Variants in Large Language Models</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Inputs $\backslash$ Effects</th>
<th style="text-align: center;">Averaged <br> Distance <br> Effect</th>
<th style="text-align: center;">Averaged <br> Size <br> Effect</th>
<th style="text-align: center;">Averaged <br> Ratio <br> Effect</th>
<th style="text-align: center;">Averaged MDS <br> Correlation <br> values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lowercase <br> number words</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.593</td>
</tr>
<tr>
<td style="text-align: left;">Mixedcase <br> number words</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.749</td>
<td style="text-align: center;">0.460</td>
</tr>
<tr>
<td style="text-align: left;">Digits</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.548</td>
</tr>
<tr>
<td style="text-align: left;">Total <br> Averages</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.534</td>
</tr>
</tbody>
</table>
<p>Table 11: Averaged distance effect, size effect, ratio effect, and the MDS correlation values for the different input types of the models.</p>
<p>For the models in Table1, we show the three effects for the larger variants. The variants have the same architectures and training methodologies as their base variants but more parameters ( thrice the number of parameters). The in-depth results for the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Number</th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">BART</th>
<th style="text-align: center;">RoB</th>
<th style="text-align: center;">XLNET</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">GPT-2</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">$\mathbf{0 . 1 2}$</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.06</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">$\mathbf{0 . 1 1}$</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.08</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.09</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.09</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">$\mathbf{0 . 2 2}$</td>
</tr>
</tbody>
</table>
<p>Table 12: Residual analysis on MDS outputs in 1 dimension on the large variants of the models. RoB: Robertabase model, BERT: uncased variant.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Layer $\backslash$ Effects</th>
<th style="text-align: center;">Averaged <br> Distance <br> Effect</th>
<th style="text-align: center;">Averaged <br> Size <br> Effect</th>
<th style="text-align: center;">Averaged <br> Ratio <br> Effect</th>
<th style="text-align: center;">Averaged MDS <br> Correlation <br> values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.825</td>
<td style="text-align: center;">0.643</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.963</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.557</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.584</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.544</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.423</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.641</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.483</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.752</td>
<td style="text-align: center;">0.526</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.550</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.773</td>
<td style="text-align: center;">0.625</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.610</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.938</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.526</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.557</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">0.659</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.538</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.656</td>
<td style="text-align: center;">0.755</td>
<td style="text-align: center;">0.562</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.645</td>
<td style="text-align: center;">0.751</td>
<td style="text-align: center;">0.500</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.509</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.550</td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.505</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.690</td>
<td style="text-align: center;">0.527</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.535</td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.553</td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.524</td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.484</td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.414</td>
</tr>
</tbody>
</table>
<p>Table 13: Averaged distance effect, size effect, ratio effect, and MDS correlation values for the 24 layers of the models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLMs $\backslash$ Input</th>
<th style="text-align: center;">LC</th>
<th style="text-align: center;">MC</th>
<th style="text-align: center;">Digits</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.964</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">0.898</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: left;">XLNET</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.855</td>
<td style="text-align: center;">0.910</td>
</tr>
<tr>
<td style="text-align: left;">BERT (uncased)</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.956</td>
</tr>
<tr>
<td style="text-align: left;">Total Averages</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.927</td>
</tr>
<tr>
<td style="text-align: left;">across models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 14: Distance Effect: Averaged (across layers) $R^{2}$ values of different Larger variants of LLMs on different input types when fitting a linear function. LC: Lowercase number words, MC: Mixedcase number words.
three effects are presented in tables $14,16,15,17$, 18, and 19. We also present the MDS correlation values in the same manner as done for base variants; see tables 20 and 21.</p>
<p>Given the large layer count for these model vari-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLMs $\backslash$ Input</th>
<th style="text-align: left;">LC</th>
<th style="text-align: left;">MC</th>
<th style="text-align: left;">Digits</th>
<th style="text-align: left;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: left;">0.720</td>
<td style="text-align: left;">0.730</td>
<td style="text-align: left;">0.840</td>
<td style="text-align: left;">0.763</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: left;">0.697</td>
<td style="text-align: left;">0.644</td>
<td style="text-align: left;">0.380</td>
<td style="text-align: left;">0.574</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: left;">0.468</td>
<td style="text-align: left;">0.267</td>
<td style="text-align: left;">0.677</td>
<td style="text-align: left;">0.471</td>
</tr>
<tr>
<td style="text-align: left;">XLNET</td>
<td style="text-align: left;">0.533</td>
<td style="text-align: left;">0.448</td>
<td style="text-align: left;">0.510</td>
<td style="text-align: left;">0.497</td>
</tr>
<tr>
<td style="text-align: left;">BERT (uncased)</td>
<td style="text-align: left;">0.635</td>
<td style="text-align: left;">0.712</td>
<td style="text-align: left;">0.674</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">0.467</td>
<td style="text-align: left;">0.358</td>
<td style="text-align: left;">0.950</td>
<td style="text-align: left;">0.592</td>
</tr>
<tr>
<td style="text-align: left;">Total Averages</td>
<td style="text-align: left;">0.587</td>
<td style="text-align: left;">0.514</td>
<td style="text-align: left;">0.678</td>
<td style="text-align: left;">0.595</td>
</tr>
<tr>
<td style="text-align: left;">across models</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 15: Size Effect: Averaged (across layers) $R^{2}$ values of different Larger variants of LLMs on different input types when fitting a linear function. LC: Lowercase number words, MC: Mixedcase number words.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Layer</th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">BART</th>
<th style="text-align: center;">RoB</th>
<th style="text-align: center;">XLNET</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">GPT-2</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.972</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.967</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.963</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.979</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.964</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.972</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.968</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.981</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.962</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.958</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.979</td>
<td style="text-align: center;">0.911</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.957</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.981</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.956</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.956</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.944</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.938</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.923</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.939</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.944</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.972</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.940</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.934</td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.897</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.938</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.919</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.900</td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">0.887</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.867</td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.789</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.854</td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.579</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.829</td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.825</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.502</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.805</td>
</tr>
</tbody>
</table>
<p>Table 16: Distance Effect: Averaged (across inputs) $R^{2}$ values of different Larger variants of LLMs for different layers when fitting a linear function. RoB: Roberta-base model, BERT: uncased variant.
ants and the multiple tables, we also present a summarized view of the results in tables $11,12,13$.</p>
<h2>A. 2 Cased vs Uncased BERT</h2>
<p>The behavioral differences between the cased and uncased variants of the BERT architecture are shown in TableA.2. Despite different preprocessing paradigms, both models have similar performance characteristics. The only visible distinction is the higher correlation values for the cased version when compared to the uncased version of the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Layer</th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">BART</th>
<th style="text-align: center;">RoB</th>
<th style="text-align: center;">XLNET</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">GPT-2</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.647</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.549</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.709</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.665</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.587</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.922</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.622</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.563</td>
<td style="text-align: center;">0.632</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.641</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.591</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.608</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.915</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.541</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.599</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.612</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.601</td>
<td style="text-align: center;">0.608</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.351</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.728</td>
<td style="text-align: center;">0.601</td>
<td style="text-align: center;">0.604</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.659</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.751</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.656</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.645</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.611</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">0.598</td>
<td style="text-align: center;">0.567</td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.743</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.453</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.747</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.580</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.559</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.557</td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.558</td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.571</td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.491</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.590</td>
<td style="text-align: center;">0.509</td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.508</td>
</tr>
</tbody>
</table>
<p>Table 17: Size Effect: Averaged (across inputs) $R^{2}$ values of different Larger variants of LLMs for different layers when fitting a linear function. RoB: Roberta-base model, BERT: uncased variant.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLMs $\backslash$ Input</th>
<th style="text-align: left;">LC</th>
<th style="text-align: left;">MC</th>
<th style="text-align: left;">Digits</th>
<th style="text-align: left;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: left;">0.868</td>
<td style="text-align: left;">0.816</td>
<td style="text-align: left;">0.833</td>
<td style="text-align: left;">0.839</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: left;">0.767</td>
<td style="text-align: left;">0.838</td>
<td style="text-align: left;">0.478</td>
<td style="text-align: left;">0.694</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: left;">0.672</td>
<td style="text-align: left;">0.686</td>
<td style="text-align: left;">0.725</td>
<td style="text-align: left;">0.694</td>
</tr>
<tr>
<td style="text-align: left;">XLNET</td>
<td style="text-align: left;">0.617</td>
<td style="text-align: left;">0.649</td>
<td style="text-align: left;">0.711</td>
<td style="text-align: left;">0.659</td>
</tr>
<tr>
<td style="text-align: left;">BERT (uncased)</td>
<td style="text-align: left;">0.786</td>
<td style="text-align: left;">0.732</td>
<td style="text-align: left;">0.759</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT2</td>
<td style="text-align: left;">0.669</td>
<td style="text-align: left;">0.720</td>
<td style="text-align: left;">0.767</td>
<td style="text-align: left;">0.718</td>
</tr>
<tr>
<td style="text-align: left;">Total Averages <br> across models</td>
<td style="text-align: left;">0.730</td>
<td style="text-align: left;">0.749</td>
<td style="text-align: left;">0.707</td>
<td style="text-align: left;">0.718</td>
</tr>
</tbody>
</table>
<p>Table 18: Ratio Effect: Averaged (across layers) $R^{2}$ values of different Larger variants of LLMs on different input types when fitting a negative exponential function. LC: Lowercase number words, MC: Mixedcase number words.
model.</p>
<h2>A. 3 Impact of Distance effect and Size effect in Ratio effect scores</h2>
<p>When interpreting LLM findings on the ratio effect, we observe that they are dominated by the distance effect as compared to the size effect. We observe the same decreasing trend in averaged results over input types in layers; see Table 7 (column: Total Averages). The impact of layer-wise trends can be quantified using regression with the distance effect</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Layer</th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">BART</th>
<th style="text-align: center;">RoB</th>
<th style="text-align: center;">XLNET</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">GPT-2</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.825</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">0.529</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.718</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.798</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">0.736</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.782</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.765</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.763</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.774</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.752</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.753</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.838</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">0.773</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.915</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">0.766</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.742</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">0.726</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.728</td>
<td style="text-align: center;">0.739</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">0.755</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.772</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.822</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.751</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.915</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.741</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.747</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.730</td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.907</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.728</td>
<td style="text-align: center;">0.497</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.728</td>
<td style="text-align: center;">0.723</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.690</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.717</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.671</td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.644</td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.664</td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0.637</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.633</td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">0.622</td>
</tr>
</tbody>
</table>
<p>Table 19: Ratio Effect: Averaged (across inputs) $R^{2}$ values of different Larger variants of LLMs for different layers when fitting a negative exponential function. RoB: Roberta-base model, BERT: uncased variant.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLMs $\backslash$ Input</th>
<th style="text-align: right;">LC</th>
<th style="text-align: center;">MC</th>
<th style="text-align: center;">Digits</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: right;">0.572</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.369</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: right;">0.677</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.515</td>
<td style="text-align: center;">0.580</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: right;">0.669</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.572</td>
</tr>
<tr>
<td style="text-align: left;">XLNET</td>
<td style="text-align: right;">0.498</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.445</td>
</tr>
<tr>
<td style="text-align: left;">BERT (uncased)</td>
<td style="text-align: right;">0.519</td>
<td style="text-align: center;">0.541</td>
<td style="text-align: center;">0.530</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT2</td>
<td style="text-align: right;">0.623</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.711</td>
</tr>
<tr>
<td style="text-align: left;">Total Averages</td>
<td style="text-align: right;">0.593</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">0.534</td>
</tr>
<tr>
<td style="text-align: left;">$\quad$ across models</td>
<td style="text-align: right;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 20: Averaged (across layers) correlation values when comparing MDS values with $\log <em 10="10">{10} 1$ to $\log </em> 9$ for Large variants of different LLMs. LC: Lowercase number words, MC: Mixedcase number words.
and size effect as inputs (column: Total Averages; tables 2, 4) and the ratio effect (column: Total Averages; Table4) as output. Importantly, the distance effect averages are statistically significant predictors of ratio effect averages; see Table 23). These results provide a superficial view of the impact of distance and size effect in the ratio effect scores because of the aggregation performed at different levels of the study.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Layer</th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">BART</th>
<th style="text-align: center;">RoB</th>
<th style="text-align: center;">XLNET</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">GPT-2</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.675</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.590</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.643</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.649</td>
<td style="text-align: center;">0.555</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.557</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.673</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.584</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.698</td>
<td style="text-align: center;">0.674</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">0.544</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.059</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.423</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">0.483</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.526</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.690</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.695</td>
<td style="text-align: center;">0.550</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.598</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.625</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.487</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.610</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.526</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.481</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.557</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.538</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.562</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.500</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.509</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.677</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.550</td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.505</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.077</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.527</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.535</td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.553</td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.656</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.524</td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">-0.019</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.649</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.484</td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">-0.051</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.205</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.414</td>
</tr>
</tbody>
</table>
<p>Table 21: Averaged (across inputs) correlation values of the Large variants of different LLMs on different model layers when comparing MDS values with $\log <em 10="10">{10} 1$ to $\log </em> 9$. RoB: Roberta-base model, BERT: uncased variant.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Variant</th>
<th style="text-align: left;">Effect</th>
<th style="text-align: right;">LC</th>
<th style="text-align: right;">MC</th>
<th style="text-align: right;">Digits</th>
<th style="text-align: right;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Uncased</td>
<td style="text-align: left;">Distance</td>
<td style="text-align: right;">0.976</td>
<td style="text-align: right;">0.944</td>
<td style="text-align: right;">0.960</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Size</td>
<td style="text-align: right;">0.803</td>
<td style="text-align: right;">0.851</td>
<td style="text-align: right;">0.827</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ratio</td>
<td style="text-align: right;">0.906</td>
<td style="text-align: right;">0.757</td>
<td style="text-align: right;">0.831</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MDS (Corr.)</td>
<td style="text-align: right;">0.312</td>
<td style="text-align: right;">0.423</td>
<td style="text-align: right;">0.386</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Distance</td>
<td style="text-align: right;">0.958</td>
<td style="text-align: right;">0.980</td>
<td style="text-align: right;">0.890</td>
<td style="text-align: right;">0.943</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Size</td>
<td style="text-align: right;">0.664</td>
<td style="text-align: right;">0.691</td>
<td style="text-align: right;">0.918</td>
<td style="text-align: right;">0.758</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ratio</td>
<td style="text-align: right;">0.854</td>
<td style="text-align: right;">0.880</td>
<td style="text-align: right;">0.866</td>
<td style="text-align: right;">0.867</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MDS (Corr.)</td>
<td style="text-align: right;">0.621</td>
<td style="text-align: right;">0.553</td>
<td style="text-align: right;">0.487</td>
<td style="text-align: right;">0.554</td>
</tr>
</tbody>
</table>
<p>Table 22: Behavioral differences between the cased and uncased variants of the BERT architecture. LC: Lowercase number words, MC: Mixed-case number words.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Variant</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Coef.</th>
<th style="text-align: center;">Std. Error</th>
<th style="text-align: center;">t Stat</th>
<th style="text-align: center;">P-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base</td>
<td style="text-align: left;">Intercept</td>
<td style="text-align: left;">-0.916</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">-1.722</td>
<td style="text-align: center;">0.119</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Distance Effect</td>
<td style="text-align: left;">1.953</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">4.314</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 1} \oplus$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Size Effect</td>
<td style="text-align: left;">-0.228</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">-1.212</td>
<td style="text-align: center;">0.256</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Intercept</td>
<td style="text-align: left;">-0.188</td>
<td style="text-align: center;">0.075</td>
<td style="text-align: center;">-2.491</td>
<td style="text-align: center;">0.0.021</td>
</tr>
<tr>
<td style="text-align: left;">Large</td>
<td style="text-align: left;">Distance Effect</td>
<td style="text-align: left;">0.700</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">5.997</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 0} \oplus$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Size Effect</td>
<td style="text-align: left;">0.447</td>
<td style="text-align: center;">0.124</td>
<td style="text-align: center;">3.612</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 1} \oplus$</td>
</tr>
</tbody>
</table>
<p>Table 23: Impact of layer-wise trends of distance and size effect on the ratio effect; $\oplus$ indicates statistical significance with p-value less that $0.01, \oplus$ indicates statistical significance with p-value less that 0.00001</p>            </div>
        </div>

    </div>
</body>
</html>
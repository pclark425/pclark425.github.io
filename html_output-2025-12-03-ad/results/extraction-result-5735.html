<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5735 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5735</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5735</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-266690657</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.17617v2.pdf" target="_blank">Large language models for generative information extraction: a survey</a></p>
                <p><strong>Paper Abstract:</strong> Information Extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub (LLM4IE repository).</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5735",
    "paper_id": "paper-266690657",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00964025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Generative Information Extraction: A Survey
31 Oct 2024</p>
<p>Derong Xu derongxu@mail.ustc.edu.cn 
State Key Laboratory of Cognitive Intelligence
University of Science and Technology of China
230000HefeiChina</p>
<p>Department of Data Science
City University of Hong Kong
999077HongkongChina</p>
<p>Wei Chen chenweicw@mail.ustc.edu.cn 
State Key Laboratory of Cognitive Intelligence
University of Science and Technology of China
230000HefeiChina</p>
<p>Wenjun Peng pengwj@mail.ustc.edu.cn 
State Key Laboratory of Cognitive Intelligence
University of Science and Technology of China
230000HefeiChina</p>
<p>Chao Zhang 
State Key Laboratory of Cognitive Intelligence
University of Science and Technology of China
230000HefeiChina</p>
<p>Department of Data Science
City University of Hong Kong
999077HongkongChina</p>
<p>Tong Xu 
Xiangyu Zhao xianzhao@cityu.edu.hk 
State Key Laboratory of Cognitive Intelligence
University of Science and Technology of China
230000HefeiChina</p>
<p>Department of Data Science
City University of Hong Kong
999077HongkongChina</p>
<p>Xian Wu 
Jarvis Research Center
Tencent YouTu Lab
100029BeijingChina</p>
<p>Yefeng Zheng yefengzheng@tencent.com 
Jarvis Research Center
Tencent YouTu Lab
100029BeijingChina</p>
<p>Yang Wang wangyang@chinaconch.com 
Anhui Conch Information Technology Engineering Co., Ltd
241000WuhuChina</p>
<p>Enhong Chen cheneh@ustc.edu.cn 
State Key Laboratory of Cognitive Intelligence
University of Science and Technology of China
230000HefeiChina</p>
<p>Large Language Models for Generative Information Extraction: A Survey
31 Oct 2024074C0A6C2B4919861578D502A972AA0710.1007/sxxxxx-yyy-zzzz-1arXiv:2312.17617v3[cs.CL]Received month dd, yyyy; accepted month dd, yyyyInformation ExtractionLarge Language ModelsReview
Information extraction (IE) aims to extract structural knowledge from plain natural language texts.Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation.As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm.To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field.We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs.Based on a thorough review conducted, we identify several insights in technique and promis-ing research directions that deserve further exploration in future studies.We maintain a public repository and consistently update related works and resources on GitHub (LLM4IE repository).</p>
<p>Introduction</p>
<p>Information Extraction (IE) is a crucial domain in natural language processing (NLP) that converts plain text into structured knowledge (e.g., entities, relations, and events), and serves as a foundational requirement for a wide range of downstream tasks, such as knowledge graph construction [1], knowledge reasoning [2] and question answering [3].Typical IE tasks consist of Named Entity Recognition (NER), Relation Extraction (RE) and Event Extraction (EE) [4,5,6,7].However, performing IE tasks is inherently challenging.This is because IE involves extracting information from various sources and dealing with complex and ever-changing domain requirements [8].Unlike traditional NLP tasks, IE encompasses a broad spectrum of objectives such as entity extraction, relationship extraction, and more.In IE, the extraction targets exhibit intricate structures where entities are presented as span structures (string structures) and relationships are represented as triple structures [4].Additionally, in order to effectively handle different information extraction tasks, it is necessary to employ multiple independent models.These models are trained separately for each specific task, without sharing any resources.However, this approach comes with a drawback: managing a large number of information extraction models becomes costly in terms of the resources needed for construction and training, like annotated corpora.</p>
<p>The emergence of large language models (LLMs), such as GPT-4 [9], has significantly advanced the field of NLP, due to their extraordinary capabilities in text understanding and generation [10,11,12].Pretraining LLMs using auto-regressive prediction allows them to capture the inherent patterns and semantic knowledge within text corpora [13,14,15,16,17,18,19].This enhances LLMs with the capability to perform zero-shot and few-shot learning, enabling them to model various tasks consistently and serving as tools for data augmentation [20,21,22].Furthermore, LLMs can serve as intelligent agents for complex task planning and execution, utilizing memory retrieval and various tools to enhance efficiency and successfully accomplish tasks [23,24,25,26,27].Therefore, there has been a recent surge of interest in generative IE methods [28] that adopt LLMs to generate structural information rather than extracting structural information from plain text.These methods have been proven to be more practical in real-world scenarios compared to discriminated methods [29,30], as they can handle schemas containing millions of entities without significant performance degradation [31].</p>
<p>On the one hand, LLMs have attracted significant attention from researchers in exploring their potentials for various scenarios and tasks of IE.In addition to excelling in individual IE tasks, LLMs possess a remarkable ability to effectively model all IE tasks in a universal format.This is conducted by capturing inter-task dependencies with instructive prompts, and achieves consistent performance [4,5,32,33,34,35,36]. On the other hand, recent works have shown the outstanding generalization of LLMs to not only learn from IE training data through fine-tuning [33,33,37,38,39], but also extract information in few-shot and even zero-shot scenarios relying solely on in-context examples or instructions [40,41,42,43,44].</p>
<p>However, existing surveys [8,45,46] do not provide a comprehensive exploration of these areas for the above two groups of research works: 1) universal frameworks that cater to multiple tasks and 2) cutting-edge learning techniques for scenarios with limited training data.The community ur-</p>
<p>Structural Output</p>
<p>Task-specialized Framework Universal Framework</p>
<p>OR</p>
<p>IE Tasks</p>
<p>IE Techniques</p>
<p>Fig. 1 LLMs have been extensively explored for generative IE.These studies encompass various IE techniques, specialized frameworks designed for a single subtask, and universal frameworks capable of addressing multiple subtasks simultaneously.</p>
<p>gently needs a more in-depth analysis of how LLM can be more appropriately applied to IE tasks to improve the performance of the IE field.This is because there are still challenges and issues in applying LLM to IE in terms of learning and understanding knowledge [47].These challenges include the misalignment between natural language output and structured form [6], hallucination problem in LLMs [48], contextual dependence, high computational resource requirements [49], difficulties in updating internal knowledge [50], etc.</p>
<p>In this survey, we provide a comprehensive exploration of LLMs for generative IE, as illustrated in Fig. 1.To achieve this, we categorize existing methods mainly using two taxonomies: (1) a taxonomy of numerous IE subtasks, which aims to classify different types of information that can be extracted individually or uniformly, and (2) a taxonomy of IE techniques, which categorizes various novel approaches that utilize LLMs for generative IE, particularly on low-resource scenarios.In addition, we present a comprehensive review of studies that specifically focus on the application of IE techniques in various domains.And we discuss studies that aim to evaluate and analyze the performance of LLMs for IE.According to the above division, we construct a taxonomy of related studies as shown in Fig. 2. We also compare several representative methods to gain deeper understanding of their potentials and limitations, and provide insightful analysis on future directions.To the best of our knowledge, this is the first survey on generative IE with LLMs.</p>
<p>The remaining part of this survey is organized as follows: We first introduce the definition of generative IE and target of all subtasks in Section 2.Then, in Section 3, we introduce representative models for each task and universal IE, and compare their performance.In Section 4, we summarize different learning techniques of LLMs for IE.Additionally, we introduce works proposed for special domains in Section 5, and present recent studies that evaluate and analyze the abilities of LLMs on IE tasks in Section 6.Finally, we propose potential research directions for future studies in Section 7. In Section 8, we provide a comprehensive summary of the most commonly used LLMs and datasets statistics, as reference for researchers.</p>
<p>Preliminaries of Generative IE</p>
<p>In this section, we provide a formal definition of discriminative and generative IE and summarize the IE subtasks, as outlined in [46].This survey focuses primarily on the tasks of Named Entity Recognition (NER), Relation Extraction (RE), and Event Extraction (EE) [5,32], as these are tasks that receive the most attention in IE papers.Examples are shown in Fig. 3.</p>
<p>(1) For a discriminative model, the objective is to maximize the likelihood of the data.This involves considering an annotated sentence x and a collection of potentially overlapping triples.t j = (s, r, o):
p cls (t|x) = (s,r,o)∈t j p((s, r, o)|x j )(1)
Another method of discrimination involves generating tags using sequential tagging for each position i.For a sentence x consisting of n words, n different tag sequences are annotated based on the "BIESO" (Begin, Inside, End, Single, Outside) notation schema.During the training of the model, the objective is to maximize the log-likelihood of the target tag sequence by utilizing the hidden vector h i at each position i:
p tag (y|x) = exp(h i , y i ) exp(exp(h i , y ′ i ))(2)
(2) The three types of IE tasks can be formulated in a generative manner.Given an input text (e.g., sentence or document) with a sequence of n tokens X = [x 1 , ..., x n ], a prompt P, and the target extraction sequence Y = [y 1 , ..., y m ], the objective is to maximize the conditional probability in an autoregressive formulation:
p θ (Y|X, P) = m i=1 p θ (y i |X, P, y &lt;i ),(3)
where θ donates the parameters of LLMs, which can be frozen or trainable.In the era of LLMs, several works have proposed appending extra prompts or instructions P to X to enhance the comprehensibility of the task for LLMs [5].Even though the input text X remains the same, the target sequence varies for each task:</p>
<p>• Named Entity Recognition (NER) includes two tasks: Entity Identification and Entity Typing.The former task is concerned with identifying spans of entities, and the latter task focuses on assigning types to these identified entities.• Relation Extraction (RE) may have different settings in different works.We categorize it using three terms following the literature [4,5]: (1) Relation Classification refers to classifying the relation type between two given entities; (2) Relation Triplet refers to identifying the relation type and the corresponding head and tail entity spans; (3) Relation Strict refers to giving the correct relation type, the span, and the type of head and tail entity.</p>
<p>• Event Extraction (EE) can be divided into two subtasks [151]: (1) Event Detection (also known as Event Trigger Extraction in some works) aims to identify and classify the trigger word and type that most clearly represents the occurrence of an event.(2) Event Arguments Extraction aims to identify and classify arguments with specific roles in the events from the sentences.[57], EnTDA [58], Amalvy et al. [59], GPT-NER [42], Cp-NER [60], LLMaAA [61], PromptNER [43], Ma et al. [62], Xie et al. [63], UniNER [64], NAG-NER [65], Su et al. [66], GNER [67], NuNER [68], MetaNER [69], LinkNER [70], SLCoLM [71], Popovivc et al. [72], ProgGen [73], C-ICL [74], Keloth et al. [75], VerifiNER [76] [90], RT [91], Jiang et al. [92], VANER [93], RiVEG [94], LLM-DA [95], etc.</p>
<p>Relation Extraction ( §3.2)</p>
<p>Classification REBEL [39], Li et al. [96], GL [97], Xu et al. [44], QA4RE [98], LLMaAA [61], GPT-RE [99], Ma et al. [62], STAR [100], AugURE [101],</p>
<p>LLMs for Different Information Extraction Tasks</p>
<p>In this section, we first present a introduction to the relevant LLM technologies for IE subtasks, including NER ( §3.1), RE ( §3.2), and EE ( §3.3).We also conduct experimental analysis to evaluate the performance of various methods on representative datasets for three subtasks.Furthermore, we categorize universal IE frameworks into two categories: natural language (NL-LLMs) and code language (Code-LLMs), to discuss how they model the three distinct tasks using a unified paradigm ( §3.4).</p>
<p>Named Entity Recognition</p>
<p>NER is a crucial component of IE and can be seen as a predecessor or subtask of RE and EE.It is also a fundamental task in other NLP tasks, thus attracting significant attention from researchers to explore new possibilities in the era of LLMs [47,90,91,92,93,94,95,108,122,124,125,126,164,165,172,173,177,179,182].Considering the gap between the sequence labeling and generation models, GPT-NER [42] transformed NER into a generative task and proposed a self-verification strategy to rectify the mislabeling of NULL inputs as entities.Xie et al. [63] proposed a trainingfree self-improving framework that uses LLM to predict on the unlabeled corpus to obtain pseudo demonstrations, thereby enhancing the performance of LLM on zero-shot NER.</p>
<p>Table 1 shows the comparison of NER on five main datasets, which are obtained from their original papers.We can observe that:</p>
<p>• 1) the models in few-shot and zero-shot settings still have a huge performance gap behind the SFT and DA.</p>
<p>• 2) Even though there is little difference between backbones, the performance varies greatly between methods under the ICL paradigm.For example, GPT-NER opens up at least a 6% F1 value gap with other methods on each dataset, and up to about 19% higher.</p>
<p>• 3) Compared to ICL, there are only minor differences in performance between different models after SFT, even though the parameters in their backbones can differ by up to a few hundred times.</p>
<p>• 4) The performance of models trained with the SFT paradigm exhibits greater variability across datasets, particularly for universal models.For instance, YAYI-UIE [155] and KnowCoder [160] outperform other models by at least 2.89% and 1.22% respectively on CoNLL03, while experiencing a decrease of 7.04% and 5.55% respectively compared to the best model on GENIA.We hypothesize that this discrepancy may arise from these models being trained on diverse datasets primarily sourced from news and social media domains, whereas GENIA represents a smaller fraction in the training set as it belongs to the biomedical domain; thus resulting in significant distribution gaps between different fields that ultimately impact performance outcomes.Furthermore, universal models necessitate simultaneous training across mul- tiple subtasks, which inevitably exacerbates this distribution gap.</p>
<p>• 5) The EnTDA [58], on the contrary, exhibits exceptional stability and outperforms other methods on all datasets, thereby substantiating the robustness of the DA paradigm in addressing specific tasks.</p>
<p>Relation Extraction</p>
<p>RE also plays an important role in IE, which usually has different setups in different studies as mentioned in Section 2. To address the poor performance of LLMs on RE tasks due to the low incidence of RE in instruction-tuning datasets, as indicated by [187], QA4RE [98] introduced a framework that enhances LLMs' performance by aligning RE tasks with QA tasks.GPT-RE [99] incorporates task-aware representations and enriching demon- As shown in the Table 2 and 3, we statistically found that universal IE models are generally better solving harder Relation Strict problems due to learning the dependencies between multi-tasks [4,33], while the task-specific methods solve simpler RE subtasks (e.g.relation classification).Moreover, when compared to NER, it becomes evident that the performance disparities among models in RE are more pronounced, thereby highlighting the potential of LLM in addressing RE tasks.</p>
<p>Event Extraction</p>
<p>Events can be defined as specific occurrences or incidents that happen in a given context.Recently, many studies [135,138] aim to understand events and capture their correlations by extracting event triggers and arguments using LLMs, which is essential for various reasoning tasks [199].For example, Code4Struct [41] leveraged LLMs to translate text into code to tackle structured prediction tasks, using programming language features to introduce external knowledge and constraints through alignment between structure and code.Considering the interrelation between different arguments in the extended context, PGAD [137] employed a text diffusion model to create a variety of context-aware prompt representations, enhancing both sentencelevel and document-level event argument extraction by identifying multiple role-specific argument span queries and coordinating them with context.</p>
<p>As can be seen from results of recent studies in Table 4, vast majority of current methods are based on SFT paradigm, and only a few methods that use LLMs for either zero-shot or few-shot learning.In addition, generative methods outperform discriminative ones by a wide margin, especially in metric for argument classification task, indicating the great potential of generative LLMs for EE.</p>
<p>Universal Information Extraction</p>
<p>Different IE tasks vary a lot, with different optimization objectives and task-specific schemas, requiring separate models to handle the complexity of different IE tasks, settings, and scenarios [4].As shown in Fig. 2, many works solely focus on a subtask of IE.However, recent advancements in LLMs have led to the proposal of a unified generative framework in several studies [5,32].This framework aims to model all IE tasks, capturing the common abilities of IE and learning the dependencies across multiple tasks.The prompt format for Uni-IE can typically be divided into natural language-based LLMs (NL-LLMs) and code-based LLMs (code-LLMs), as illustrated in Fig. 4. NL-LLMs.NL-based methods unify all IE tasks in a universal natural language schema.For instance, This figure is adopted from [5] and [6].</p>
<p>UIE [4] proposed a unified text-to-structure generation framework that encodes extraction structures, and captured common IE abilities through a structured extraction language.InstructUIE [5] enhanced UIE by constructing expert-written instructions for fine-tuning LLMs to consistently model different IE tasks and capture the inter-task dependency.Additionally, ChatIE [40] explored the use of LLMs like ChatGPT [200] in zero-shot prompting, transforming the task into a multi-turn questionanswering problem.</p>
<p>Code-LLMs.On the other hand, code-based methods unify IE tasks by generating code with a universal programming schema [41].Code4UIE [6] proposed a universal retrieval-augmented code gen-eration framework, which leverages Python classes to define schemas and uses in-context learning to generate codes that extract structural knowledge from texts.Besides, CodeKGC [159] leveraged the structural knowledge inherent in code and employed the schema-aware prompts and rationale-enhanced generation to improve performance.To enable LLMs to adhere to guidelines out-of-the-box, GoLLIE [32] enhanced zero-shot ability on unseen IE tasks by aligning with annotation guidelines.</p>
<p>In general, NL-LLMs are trained on a wide range of text and can understand and generate human language, which allows the prompts and instructions to be conciser and easier to design.However, NL-LLMs may produce unnatural outputs due to the distinct syntax and structure of IE tasks [159], which differ from the training data.Code, being a formalized language, possesses the inherent capability to accurately represent knowledge across diverse schemas, which makes it more suitable for structural prediction [6].But code-based methods often require a substantial amount of text to define a Python class (see Fig. 4), which in turn limits the sample size of the context.Through experimental comparison in Table 1, 2, and 4, we can observe that Uni-IE models after SFT outperform task-specific models in the NER, RE, and EE tasks for most datasets.</p>
<p>Summaries of Tasks</p>
<p>In this section, we explored the three primary tasks within IE and their associated sub-tasks, as well as frameworks that unify these tasks [4].A key observation is the increased application of generative LLMs to NER [67,178], which has seen significant advancements and remains a highly active area of research within IE.In contrast, tasks such as relation extraction and event extraction have seen relatively less application, particularly for strict relation extraction [39] and detection-only event extraction [128].This discrepancy may be attributed to the critical importance of NER, its utility in various downstream tasks, and its relatively simpler structured outputs, which facilitate large-scale supervised fine-tuning [1].</p>
<p>Additionally, a notable trend is the emergence of unified models for IE tasks, leveraging the general text understanding capabilities of modern large models [4,6,156].Several studies have proposed unified generative frameworks that capture the common abilities across IE tasks and learn the dependencies between them.These unified approaches can be broadly categorized into natural language-based and code-based methods, each with distinct advantages and limitations.</p>
<p>Experimental results summarized in Tables 1, 2, 3 and 4 reveal that universal IE models generally perform better on more complex strict relation extraction tasks due to their ability to learn dependencies across multiple tasks.Furthermore, generative methods significantly outperform discriminative ones in event extraction tasks, particularly in argument classification, highlighting the substantial potential of generative LLMs in this domain.</p>
<p>Techniques of LLMs for Generative IE</p>
<p>In this section, we categorize recent methods based on their techniques, including Data Augmentation ( §4.  examples, while avoiding the introduction of unrealistic, misleading, and offset patterns.Recent powerful LLMs also demonstrate remarkable performance in data generation tasks [201,202,203,204,205], which has attracted the attention of many researchers using LLMs to generate synthetic data for IE [44,61,101,127,161,162,163].It can be roughly divided into four strategies according to their techniques, as shown in Fig. 5.</p>
<p>Data Annotation.This strategy directly generates labeled structural data using LLMs.For instance, Zhang et al. [61] proposed LLMaAA to improve accuracy and data efficiency by employing LLMs as annotators within an active learning loop, thereby optimizing both the annotation and training processes.AugURE [101] employed withinsentence pairs augmentation and cross-sentence pairs extraction to enhance the diversity of positive pairs for unsupervised RE, and introduced margin loss for sentence pairs.Li et al. [161] addressed the challenge of document-level relation extraction from a long context, and proposed an automated annota-tion method for DocRE that combines a LLM with a natural language inference module to generate relation triples.</p>
<p>Knowledge Retrieval.This strategy effectively retrieves related information from LLMs for IE, which is similar to retrieval augmentation generation (RAG) [206].PGIM [167] presented a two-stage framework for multimodal NER, which leveraged Chat-GPT as an implicit knowledge base to heuristically retrieve auxiliary knowledge for more efficient entity prediction.Amalvy et al. [59] proposed to improve NER on long documents by generating a synthetic context retrieval training dataset, and training a neural context retriever.Chen et al. [166] focused on the task of multimodal NER and RE, and showcased their approach to enhancing commonsense reasoning skills by employing a range of CoT prompts that encompass different aspects, including nouns, sentences, and multimodal inputs.Additionally, they employed data augmentation techniques such as style, entity, and image manipulation to further improve the performance.</p>
<p>Inverse Generation.This strategy encourages LLMs to generate natural text or questions by utilizing the structural data provided as input, which aligns with the training paradigm of LLMs.For example, SynthIE [168] showed that LLMs can create highquality synthetic data for complex tasks by reversing the task direction, and train new models that outperformed previous benchmarks.Rather than relying on ground-truth targets, which limits generalizability and scalability, STAR [100] generated structures from valid triggers and arguments, then generates passages with LLMs by designing finegrained instructions, error identification, and iterative revision.In order to address the challenge of maintaining text coherence while preserving entities, EnTDA [58] proposed a method that involves manipulating the entity list of the original text.This manipulation includes adding, deleting, replacing, or swapping entities.And it further introduced a diversity beam search to enhance diversity in the Entity-to-text generation process.</p>
<p>Synthetic Datasets for Fine-tuning.This strategy involves generating some data for instructiontuning by querying LLMs.Typically, this data is generated by a more powerful model for fine-tuning instructions in dialogues, and then distilled onto a smaller model, enabling it to also acquire stronger zero-shot capabilities [64,67,84].For instance, UniversalNER [64] explored targeted distillation with mission-focused instruction tuning to train student models that excel in open NER, which used Chat-GPT as the teacher model and distilled it into smaller UniversalNER model.GNER [67] proposed the integration of negative instances to enhance existing methods by introducing contextual information and improving label boundaries.The authors trained their model using Pile-NER, a dataset that includes approximately 240K entities across 13K distinct en-tity categories, which are sampled from the Pile Corpus [207] and processed using ChatGPT to generate the entities.The results demonstrate improved zero-shot performance across unseen entity domains.</p>
<p>Prompt Design</p>
<p>Prompt engineering is a technique employed to enhance the capabilities of LLMs without altering their network parameters [49,208,209,210,211,212].</p>
<p>It entails utilizing task-specific instructions, known as prompts, to guide the behavior of model [13,213,214].The practice of prompt design has proven successful in various applications [215,216,217,218].Undoubtedly, effective prompt design also plays a crucial role in improving the performance of LLMs on IE tasks.In this section, we categorize prompt design approaches based on different strategies and provide a detailed explanation of the underlying motivations behind these techniques: Question Answer (QA).LLMs are instructiontuned using a dialogue-based method [219,220], which creates a gap when compared to the structured prediction requirements of the IE task.Consequently, recent efforts have been made to employ a QA prompt approach to enhance LLMs and facilitate the generation of desired results more seamlessly [40,90,96,98,108].For example, QA4RE [98] found that LLMs tend to perform poorly on RE because the instruction-tuning datasets used to train them have a low incidence of RE tasks, and thus proposes reformulating RE as multiple-choice QA to take advantage of the higher prevalence of QA tasks in instruction-tuning datasets.Li et al. [96] analyzed the limitations of existing RE prompts and proposed a new approach called summarize-andask prompting, which transforms zero-shot RE inputs into effective QA format using LLMs recursively.It also showed promise in extracting over-lapping relations and effectively handling the challenge of none-of-the-above relations.ChatIE [40] proposed a two-stage framework to transform the zero-shot IE task into a multi-turn QA problem.</p>
<p>The framework initially identified the different types of elements, and then a sequential IE process is executed for each identified element type.Each stage utilized a multi-turn QA process, where prompts are constructed using templates and previously extracted information.</p>
<p>Chain-of-thought (CoT).CoT [221] is a prompting strategy used with LLMs to enhance their performance, by providing a step-wise and coherent reasoning chain as a prompt to guide the model's response generation.CoT prompting has gained attention in recent years [222], and there is ongoing research exploring its effectiveness on IE tasks [43,91,166,169,170,171]. PromptNER [43] combined LLMs with prompt-based heuristics and entity definitions.It prompted an LLM to generate a list of potential entities and their explanations based on provided entity type definitions.Bian et al. [171] proposed a two-step approach to improve Biomedical NER using LLMs.Their approach involved leveraging CoT to enable the LLM to tackle the Biomedical NER task in a step-by-step manner, breaking it down into entity span extraction and entity type determination.Yuan et al. [170] also proposed CoT prompt as a two-stage approach to guide ChatGPT in performing temporal relation reasoning for temporal RE task.</p>
<p>Self-Improvement.While COT technology can partially elicit the reasoning ability of LLM, it is unavoidable that LLM will still generate factual errors.As a result, there have been efforts [63,73,144] to employ LLMs for iterative self-verification and self-improvement, aiming to rectify the results.For instance, Xie et al. [63] proposed a training-free self-improving framework, which consists of three main steps.First, LLMs made predictions on unlabeled corpus, generating self-annotated dataset through self-consistency.Second, the authors explored different strategies to select reliable annotations.Finally, during inference, demonstrations from reliable self-annotated dataset were retrieved for in-context learning.ProgGen [73] involved guiding LLMs to engage in self-reflection within specific domains, resulting in the generation of domainrelevant attributes that contribute to the creation of training data enriched with attributes.Additionally, ProgGen employd a proactive strategy by generating entity terms in advance and constructing NER context data around these entities, thereby circumventing the challenges LLMs face when dealing with intricate structures.</p>
<p>Zero-shot Learning</p>
<p>The primary challenges in zero-shot learning involve ensuring that the model can effectively generalize to tasks and domains it has not been trained on, while also aligning the pre-training paradigm of LLMs to these novel tasks.Due to the large amount of knowledge embedded within, LLMs show impressive abilities in zero-shot scenarios of unseen tasks [40,223].To achieve zero-shot cross-domain generalization of LLMs in IE tasks, several works have been proposed [5,32,64].These works offered a universal framework for modeling various IE tasks and domains, and introduced innovative training prompts, e.g., instruction [5] and guidelines [32], for learning and capturing the inter-task dependencies of known tasks and generalizing them to unseen tasks and domains.For cross-type generalization, BART-Gen [130] introduced a documentlevel neural model that frames the EE task as conditional generation, which led to improved perfor-mance and strong portability to unseen event types.On the other hand, in order to improve the ability of LLMs under zero shot prompts (no need for finetuning), QA4RE [98] and ChatIE [40] proposed to transform IE into a multi-turn question-answering problem for aligning it with QA task, which is a predominant task in instruction-tuning datasets.Li et al. [96] integrated the chain-of-thought approach and proposed the summarize-and-ask prompting to solve the challenge of ensuring the reliability of outputs from black box LLMs [62].</p>
<p>Constrained Decoding Generation</p>
<p>LLMs are pretrained models that are initially trained on the task of predicting the next token in a sequence.This pretraining allows researchers to leverage the advantages of these models for various NLP tasks [8,224].However, LLMs are primarily designed for generating free-form text and may not perform well on structured prediction tasks where only a limited set of outputs are valid.</p>
<p>To address this challenge, researchers have explored the use of constrained generation for better decoding [4,123,174,175].Constrained decoding generation in autoregressive LLMs refers to the process of generating text while adhering to specific constraints or rules [225,226,227].For example, Geng et al. [174] proposed using grammarconstrained decoding as a solution to control the generation of LMs, ensuring that the output follows a given structure.The authors introduced inputdependent grammars to enhance flexibility, allowing the grammar to depend on the input and generate different output structures for different inputs.Unlike previous methods, which generate information token by token, Zaratiana et al. [123] introduced a new approach for extracting entities and relations by generating a linearized graph with nodes representing text spans and edges representing relation triplets.They used a transformer encoderdecoder architecture with a pointing mechanism and a dynamic vocabulary of spans and relation types, to capture the structural characteristics and boundaries while grounding the output in the original text.</p>
<p>Few-shot Learning</p>
<p>Few-shot learning has access to only a limited number of labeled examples, leading to challenges like overfitting and difficulty in capturing complex relationships [228].Fortunately, scaling up the parameters of LLMs gives them amazing generalization capabilities compared to small pre-trained models, allowing them to achieve excellent performance in few-shot settings [43,91].Paolini et al. [33] proposed the Translation between Augmented Natural Languages framework; Lu et al. [4] proposed a text-to-structure generation framework; and Chen et al. [60] proposed collaborative domain-prefix tuning for NER.These methods have achieved stateof-the-art performance and demonstrated effectiveness in few-shot setting.Despite the success of LLMs, they face challenges in training-free IE because of the difference between sequence labeling and text-generation models [187].To overcome these limitations, GPT-NER [42] introduced a selfverification strategy, while GPT-RE [99] enhanced task-aware representations and incorporates reasoning logic into enriched demonstrations.These methods effectively showcase how GPT can be leveraged for in-context learning.CODEIE [36] and CodeKGC [159] showed that converting IE tasks into code generation tasks with code-style prompts and in-context examples leads to superior performance compared to NL-LLMs.This is because code-style prompts provide a more effective representation of structured output, enabling them to ef-fectively handle the complex dependencies in natural language.</p>
<p>Supervised Fine-tuning</p>
<p>Using all training data to fine-tune LLMs is the most common and promising method [88,110,111,113,129,141,143,229,230,231,232,233], which allows the model to capture the underlying structural patterns in the data, and generalize well to unseen samples.For example, DEEPSTRUCT [151] introduced structure pre-training on a collection of task-agnostic corpora to enhance the structural understanding of language models.UniNER [64] explored targeted distillation and mission-focused instruction tuning to train student models for broad applications, such as NER.GIELLM [34] fine-tuned LLMs using mixed datasets, which are collected to utilize the mutual reinforcement effect to enhance performance on multiple tasks.</p>
<p>Summaries of Techniques</p>
<p>Data augmentation [61,101] is a widely explored direction due to its potential in enhancing model performance.LLMs possess extensive implicit knowledge and strong text generation capabilities, making them well-suited for data annotation tasks [222].However, while data augmentation can expand the training dataset and improve model generalization, they may also introduce noise.For example, knowledge retrieval methods can supply additional context about entities and relationships, enriching the extraction process.Nevertheless, the noise can detract from the overall quality of the extracted information [94,167].</p>
<p>On the other hand, designing effective prompts remains a significant challenge for leveraging LLMs like GPT-4 [9].Although approaches such as QA dialogue and CoT [104] strategies can enhance LLMs' IE capabilities, purely prompt-based methods still lag behind supervised fine-tuning with smaller models.Supervised fine-tuning [5,64,67], including cross-domain and few-shot learning, generally yields better performance, which suggests that combining large-scale LLMs for data annotation with supervised fine-tuning using additional data can optimize performance and reduce manual annotation costs [68,95,164].</p>
<p>In summary, while various techniques for IE using LLMs offer distinct advantages, they also come with challenges.Thoughtfully combining these strategies can yield significant enhancements in IE tasks.</p>
<p>Applications on Specific Domains</p>
<p>It is non-ignorable that LLMs have tremendous potential for extracting information from some specific domains, such as mulitmodal [57,94,166,167], multilingual [83, 133,163], medical [85, 91,162,163,171,172,179,183,184,185,186,187,188,234,235], scientific [47,80,180,181,182], astronomical [164,173], historical [126,189], and legal [78,89,115].Additionally, we present statistical data in Table 5.</p>
<p>For instance, Chen et al. [166] introduced a conditional prompt distillation method that enhances a model's reasoning ability by combining text-image pairs with chain-of-thought knowledge from LLMs, significantly improving performance in multimodal NER and multimodal RE.Tang et al. [162] explored the potential of LLMs in the field of clinical text mining and proposed a novel training approach, which leverages synthetic data to enhance performance and address privacy issues.Dunn et al. [180] presented a sequence-to-sequence approach by using GPT-3 for joint NER and RE from complex scientific text, demonstrating its effectiveness in ex- tracting complex scientific knowledge in material chemistry.Shao et al. [173] explored the use of LLMs to extract astronomical knowledge entities from astrophysical journal articles.Conventional approaches encounter difficulties such as manual labor and limited generalizability.To address these issues, the authors proposed a prompting strategy that incorporates five prompt elements and eight combination prompt, aiming to specifically target celestial object identifiers and telescope names as the experimental objects of interest.González et al. [189] examined the performance of ChatGPT in the NER task specifically on historical texts.The research not only compared ChatGPT with other state-of-the-art language model-based systems but also delved into the challenges encountered in this zero-shot setting.The findings shed light on the limitations of entity identification in historical texts, encompassing concerns related to annotation guidelines, entity complexity, code-switching, and the specificity of prompts.</p>
<p>Evaluation &amp; Analysis</p>
<p>Despite the great success of LLMs in various natural language processing tasks [236,237], their performance in the field of information extraction still have room for improvement [193].To alleviate this problem, recent research has explored the capabilities of LLMs with respect to the major subtasks of IE, i.e., NER [83,190], RE [169,170], and EE [191].Considering the superior reasoning capabilities of LLMs, Xie et al. [190] proposed four reasoning strategies for NER, which are designed to simulate ChatGPT's potential on zero-shot NER.Wadhwa et al. [169] explored the use of LLMs for RE and found that few-shot prompting with GPT-3 achieves near SOTA performance, while Flan-T5 can be improved with chain-of-thought style explanations generated via GPT-3.For EE task, Gao et al. [191] showed that ChatGPT still struggles with it due to the need for complex instructions and a lack of robustness.Along this line, some researchers performed a more comprehensive analysis of LLMs by evaluating multiple IE subtasks simultaneously.Li et al. [195] evaluated ChatGPT's overall ability on IE, including performance, explainability, calibration, and faithfulness.They found that ChatGPT mostly performs worse than BERT-based models in the standard IE setting, but excellently in the OpenIE setting.Furthermore, Han et al. [193] introduced a soft-matching strategy for a more precise evaluation and identified "unannotated spans" as the predominant error type, highlighting potential issues with data annotation quality.</p>
<p>Future Directions</p>
<p>The development of LLMs for generative IE is still in its early stages, and there are numerous opportu-nities for improvement.</p>
<p>Universal IE.Previous generative IE methods and benchmarks are often tailored for specific domains or tasks, limiting their generalizability [51].Although some unified methods [4] using LLMs have been proposed recently, they still suffer from certain limitations (e.g., long context input, and misalignment of structured output).Therefore, further development of universal IE frameworks that can adapt flexibly to different domains and tasks is a promising research direction (such as integrating the insights of task-specific models to assist in constructing universal models).</p>
<p>Low-Resource IE.The generative IE system with LLMs still encounters challenges in resource-limited scenarios [195].There is a need for further exploration of in-context learning of LLMs, particularly in terms of improving the selection of examples.Future research should prioritize the development of robust cross-domain learning techniques [5], such as domain adaptation or multi-task learning, to leverage knowledge from resource-rich domains.Additionally, efficient data annotation strategies with LLMs should also be explored.</p>
<p>Prompt Design for IE.Designing effective instructions is considered to have a significant impact on the performance of LLMs [224,238].One aspect of prompt design is to build input and output pairs that can better align with pre-training stage of LLMs (e.g., code generation) [6].Another aspect is optimizing the prompt for better model understanding and reasoning (e.g., Chain-of-Thought) [96], by encouraging LLMs to make logical inferences or explainable generation.Additionally, researchers can explore interactive prompt design (such as multiturn QA) [98], where LLMs can iteratively refine or provide feedback on the generated extractions automatically.</p>
<p>Open IE.Open IE settings present greater challenges for IE models, as they do not provide a candidate label set and rely solely on the models' ability to comprehend the task.LLMs, with their knowledge and understanding abilities, have significant advantages in some Open IE tasks [64].However, there are still instances of poor performance in more challenging tasks [28], which require further exploration by researchers.In this section, we introduce representative datasets of NER, RE and EE respectively, and show brief summary of each dataset in the Table 6 to help readers better understand these tasks.CoNLL03.The CoNLL03 [239] is a representative dataset for NER, including 1,393 news articles in English and 909 news articles in German.The English portion of the corpus was sourced from the Shared Task dataset curated by Reuters.This dataset encompasses annotations for four distinct entity types: PER (person), LOC (location), ORG (organization), and MISC (including all other types of entities).CoNLL04.The CoNLL04 [240] is a well-known benchmark dataset for RE tasks, comprising sentences extracted from news articles that each contain at least one entity-relation triple.It has four kinds of entities (PER, ORG, LOC, OTH) and five kinds of relations (Kill, Work For, Live In, Org-Based In, Located In).ACE05.Automatic Content Extraction 05 [241] is widely recognized and utilized for IE tasks.It serves as a valuable resource to assess the efficacy of automated systems in extracting structured information from diverse textual sources, encompassing news articles, interviews, reports, etc.Moreover, this dataset covers a broad range of genres including politics, economics, sports, among others.Specifically for the EE task within ACE05, it comprises 599 news documents that encapsulate 33 distinct event types and 22 argument roles.</p>
<p>Benchmarks &amp; Backbones</p>
<p>Benchmarks.</p>
<p>As shown in Table 7, we compiled a comprehensive collection of benchmarks covering various domains and tasks, to provide researchers with a valuable resource that they can query and reference as needed.Moreover, we also summarized the download links for each dataset in our open source repository (LLM4IE repository).</p>
<p>Backbones.</p>
<p>We briefly describe some backbones that are commonly used in the field of generative information extraction, which is shown in Table 8.</p>
<p>Conclusion</p>
<p>In this survey, We first introduced the subtasks of IE and discussed some universal frameworks aim-</p>
<p>Fig. 3
3
Fig. 3 Examples of different IE tasks.</p>
<p>Fig. 4
4
Fig.4The comparison of prompts of NL-LLMs and Code-LLMs for universal IE.Both NL-based and code-based methods attempt to construct a universal schema, but they differ in terms of prompt format and the way they utilize the generation capabilities of LLMs.This figure is adopted from[5] and[6].</p>
<p>Fig. 5
5
Fig. 5 Comparison of data augmentation methods.</p>
<ol>
<li>1
1
Representative Datasets.</li>
</ol>
<p>(</p>
<p>USTC), Hefei, China.He has authored more than 100 top-tier journal and conference papers in related fields, including TKDE, TMC, TMM, TOMM, KDD, SIGIR, WWW, ACM MM, etc.He was the recipient of the Best Paper Award of KSEM 2020, and the Area Chair Award for NLP Application Track of ACL 2023.Xiangyu Zhao is an assistant professor of the school of data science at City University of Hong Kong (CityU).His current research interests include data mining and machine learning.He has published more than 100 papers in top conferences and journals.His research has been awarded ICDM'22 and ICDM'21 Best-ranked Papers, Global Top 100 Chinese New Stars in AI, Huawei Innovation Research Program, CCF-Tencent Open Fund (twice), CCF-Ant Research Fund, Ant Group Research Fund, Tencent Focused Research Fund, and nomination for Joint AAAI/ACM SIGAI Doctoral Dissertation Award.He serves as top data science conference (senior) program committee members and session chairs, and journal guest editors and reviewers.Xian Wu is now a Principal Researcher in Tencent.Before joining Tencent, he worked as a Senior Scientist Manager and a Staff Researcher in Microsoft and IBM Research.Xian Wu received his PhD degree from Shanghai Jiao Tong University.His research interests includes Medical AI, Natural Language Processing and Multi-Modal modeling.Xian Wu has published papers in Nature Computational Science, NPJ digital medicine, T-PAMI, CVPR, NeurIPS, ACL, WWW, KDD, AAAI, IJCAI etc.He also served as PC member of BMJ, T-PAMI, TKDE, TKDD, TOIS, TIST, CVPR, ICCV, AAAI etc. Yefeng Zheng received B.E. and M.E.degrees from Tsinghua University, Beijing, China in 1998 and 2001, respectively, and a Ph.D. degree from University of Maryland, College Park, USA in 2005.After graduation, he worked at Siemens Corporate Research in Princeton, New Jersey, USA on medical image analysis before joining Tencent in Shenzhen, China in 2018.He is now Distinguished Scientist and Director of Tencent Jarvis Research Center, leading the company's initiative on medical artificial intelligence.He has published 300+ papers and invented 80+ US patents.His work has been cited more than 22,000 times with hindex of 74.He is a fellow of IEEE, a fellow of AIMBE, and an Associate Editor of IEEE Transactions on Medical Imaging.Yang Wang is currently working as an Engineer at Anhui Conch Information Technology Engineering Co., Ltd., Wuhu, China.He has more than 10 years of IT project implementation experience in building materials industry, applied for 11 invention patents, published 2 technological papers, and participated in 3 large-scale national science and technology projects.Enhong Chen (CCF Fellow, IEEE Fellow) is a professor of University of Science and Technology of China (USTC).His general area of research includes data mining and machine learning, social network analysis, and recommender systems.He has published more than 300 papers in refereed conferences and journals, including Nature Communications, IEEE/ACM Transactions, KDD, NIPS, IJ-CAI and AAAI, etc.He was on program committees of numerous conferences including KDD, ICDM, and SDM.He received the Best Application Paper Award on KDD-2008, the Best Research Paper Award on ICDM-2011, and the Best of SDM-2015.His research is supported by the National Science Foundation for Distinguished Young Scholars of China.</p>
<p>TEMPGEN [38], Cui et al. [53], Zhang et al. [54], Wang et al. [55], Xia et al. [56], Cai et al.
TypingGET [51], CASENT [52]Named EntityRecognitionYan et al. [37],( §3.1)Identification&amp; TypingInformationExtractionTasks ( §3)LLMs for Generative Information Extraction</p>
<p>, Li et al. [77], Oliveira et al. [78], Lu et al. [79], Bolucu et al. [80], Liu et al. [81], ConsistNER [82], Naguib et al. [83], GLiNER [84], Munnang et al. [85], Zhang et al. [86], LTNER [87], ToNER [88], Nunes et al. [89], Hou et al.</p>
<p>Table 1
1
Comparison of LLMs for named entity recognition (identification and typing) with the Micro-F1 metric (%).† indicates that the model is discriminative.We demonstrate some universal and discriminative models for comparison.IE techniques include Cross-Domain Learning (CDL), Zero-Shot Prompting (ZS Pr), In-Context Learning (ICL), Supervised Fine-Tuning (SFT), Data Augmentation (DA).Uni.denotes whether the model is universal.Onto. 5 denotes the OntoNotes 5.0.Details of datasets and backbones are presented in Section 8.The settings for all subsequent tables are consistent with this format.
Representative Model Paradigm Uni. BackboneACE04 ACE05 CoNLL03 Onto. 5 GENIADEEPSTRUCT [151]CDLGLM-10B-28.144.442.547.2Xie et al. [63] CODEIE [36] Code4UIE [6]ZS Pr ICL ICL√ √GPT-3.5-turbo Code-davinci-002 Text-davinci-003-55.29 54.82 32.27 60.1 60.974.51 82.32 83.6---52.06 --PromptNER [43]ICLGPT-4--83.48-58.44Xie et al. [63]ICLGPT-3.5-turbo-55.5484.51-58.72GPT-NER [42] TANL [33]ICL SFT√Text-davinci-003 T5-base74.2 -73.59 84.990.91 91.782.2 89.864.42 76.4Cui et al. [53]SFTBART--92.55--Yan et al. [37] UIE [4] DEEPSTRUCT [151]SFT SFT SFT√ √BART-large T5-large GLM-10B86.84 84.74 86.89 85.78 -86.993.24 92.99 93.090.38 -87.879.23 -80.8Xia et al. [56] InstructUIE [192]SFT SFT√BART-large Flan-T5-11B87.63 86.22 -86.6693.48 92.9490.63 90.1979.49 74.71UniNER [64] GoLLIE [32]SFT SFT√LLaMA-7B Code-LLaMA-34B87.5 -87.6 89.6-93.189.1 84.680.6 -EnTDA [58] YAYI-UIE [155]DA SFT√T5-base Baichuan2-13B88.21 87.56 -81.7893.88 96.7791.34 87.0482.25 75.21ToNER [88] KnowCoder [160]SFT SFT√Flan-T5-3B LLaMA2-7B88.09 86.68 86.2 86.193.59 95.191.30 88.2-76.7GNER [67]SFTFlan-T5-11B--93.2891.83-USM  † [30] RexUIE  † [197] Mirror  † [198]SFT SFT SFT√ √ √RoBERTa-large DeBERTa-v3-large 87.25 87.23 87.62 87.14 DeBERTa-v3-large 87.16 85.3493.16 93.67 92.73------</p>
<p>Table 2
2
[4]parison of LLMs for relation extraction with the "relation strict"[4]Micro-F1 metric (%).† indicates that the model is discriminative.
Representative Model Technique Uni. BackboneNYT ACE05 ADE CoNLL04 SciERCCodeKGC [159] CODEIE [36] CodeKGC [159] Code4UIE [6]ZS Pr ICL ICL ICL√ √ √ √Text-davinci-003 Code-davinci-002 32.17 14.02 --Text-davinci-003 --Text-davinci-002 54.4 17.542.8 -64.6 58.635.9 53.1 49.8 54.415.3 7.74 24.0 -REBEL [39] UIE [4] InstructUIE [5] GoLLIE [32] YAYI-UIE[155] KnowCoder [160]SFT SFT SFT SFT SFT SFT√ √ √ √ √BART-large T5-large Flan-T5-11B Code-LLaMA-34B Baichuan2-13B LLaMA2-7B91.96 -90.47 -89.97 93.7-66.06 -70.1 -64.582.21 -82.31 -84.41 84.875.35 75.0 78.48 -79.73 73.3-36.53 45.15 -40.94 40.0USM  † [30] RexUIE  † [197]SFT SFT√ √RoBERTa-large DeBERTa-v3-large--67.88 64.87--78.84 78.3937.36 38.37</p>
<p>Table 3
3
Comparison of LLMs for relation classification with the Micro-F1 metric (%).
Representative Model Technique Uni. BackboneTACRED Re-TACRED TACREV SemEvalQA4RE [98]ZS PrText-davinci-00359.461.259.443.3SUMASK [96]ZS PrGPT-3.5-turbo-030179.673.875.1-GPT-RE [99]ICLText-davinci-00372.15--91.9Xu et al. [44]ICLText-davinci-00331.051.831.9-REBEL [39]SFTBART-large-90.36--Xu et al. [44]DAText-davinci-00337.466.241.0-strations with reasoning logic to improve the lowrelevance between entity and relation and the in-
[161]ty to explain input-label mappings.Due to the large number of predefined relation types and uncontrolled LLMs, Li et al.[161]proposed to integrate LLM with a natural language inference module to generate relation triples, enhancing documentlevel relation datasets.</p>
<p>Table 4
4
Comparison of Micro-F1 Values for Event Extraction on ACE05.Evaluation tasks include: Trigger Identification (Trg-I), Trigger Classification (Trg-C), Argument Identification (Arg-I), and Argument Classification (Arg-C).† indicates that the model is discriminative.
Representative ModelTechniqueUni.BackboneTrg-ITrg-CArg-IArg-CCode4Struct [41] Code4UIE [6]ZS Pr ICL√Code-davinci-002 GPT-3.5-turbo-16k---37.450.6 -36.0 21.3Code4Struct [41] TANL [33]ICL SFT√Code-davinci-002 T5-base-72.9-68.462.1 50.158.5 47.6Text2Event [131]SFTT5-large-71.9-53.8BART-Gen [130] UIE [4]SFT SFT√BART-large T5-large---73.3669.9 -66.7 54.79GTEE-DYNPREF [135] DEEPSTRUCT [151]SFT SFT√BART-large GLM-10B-73.572.6 69.8-59.455.8 56.2PAIE [134]SFTBART-large--75.772.7PGAD [137]SFTBART-base--74.170.5QGA-EE [138] InstructUIE [5] GoLLIE [32] YAYI-UIE [155] KnowCoder [160]SFT SFT SFT SFT SFT√ √ √ √T5-large Flan-T5-11B Code-LLaMA-34B Baichuan2-13B LLaMA2-7B------77.13 71.9 65.0 74.275.0 ----72.8 72.94 68.6 62.71 70.3USM  † [30] RexUIE  † [197] Mirror  † [198]SFT SFT SFT√ √ √RoBERTa-large DeBERTa-v3-large DeBERTa-v3-large---72.41 75.17 74.44---55.83 59.15 55.88</p>
<p>Steven Jobs, was an American business magnate, industrial designer, and inventor.He was born on … Steven Jobs was an businessman, inventor, and investor who co-founded Apple Inc.
Related Knowledge[Data Construction from Large LMs]Knowledge RetrievalDistillating conversation dataNatural Language TextInstruction : A virtual assistant answers questions … Conversation : extract entities …Data AnnotationInverse GenerationInstruction-tuningStructural Triplet(Steven Jobs, was, businessman) (Steven Jobs, co-founded, Apple) …[Fine-tuning with Small LMs]3,refers to generating answer without any trainingexamples for the specific IE tasks), ConstrainedDecoding Generation ( §4.4, refers to the processof generating text while adhering to specific con-straints or rule), Few-shot Learning ( §4.5, refersto the generalization from a small number of la-beled examples by training or in-context learning),Supervised Fine-tuning ( §4.6, refers to further train-ing LLMs on IE tasks using labeled data), to high-light the commonly used approaches for adaptingLLMs to IE.4.1 Data AugmentationData augmentation involves generating meaningfuland diverse data to effectively enhance the training
1, refers to enhancing information by applying various transformations to the existing data using LLMs), Prompt Design ( §4.2, refers to the use of task-specific instructions or prompts, to direct the behavior of a model.),Zero-shotLearning ( §4.Front.Comput.Sci., 2024, 0(0): 1-47</p>
<p>Table 5
5
The statistics of research in Specific Domain.
DomainMethodTaskParadigmBackboneCai et al. [57]NERICLGPT-3.5MultimodalPGIM [167] RiVEG [94]NER NERDA DABLIP2, GPT-3.5 Vicuna, LLaMA2, GPT-3.5Chen et al. [166]NER, REDABLIP2, GPT-3.5, GPT-4Meoni et al. [163]NERDAText-davinci-003MultilingualNaguib et al. [83]NERICL-Huang et al. [133]EECDLmBART, mT5Bian et al. [171]NERDAGPT-3.5Hu et al. [184]NERZS PrGPT-3.5, GPT-4Meoni et al. [163]NERDAText-davinci-003Naguib et al. [83]NERICL-VANER [188]NERSFTLLaMA2RT [91]NERICLGPT-4MedicalMunnangi et al. [85] Monajatipoor et al. [179]NER NERZS Pr, ICL, FS FT SFT, ICLGPT-3.5, GPT-4, Claude-2, LLaMA2 -Hu et al. [172]NERZS Pr, ICLGPT-3.5, GPT-4Gutiérrez et al. [187]NER, REICLGPT-3GPT3+R [185]NER, RE-Text-davinci-002Labrak et al. [186]NER, RE-GPT-3.5, Flan-UL2, Tk-Insturct, AlpacaTang et al. [162]NER, REDAGPT-3.5DICE [183]EESFTT5-LargeBölücü et al. [80]NERICLGPT-3.5Dunn et al. [180]NER, RESFTGPT-3ScientificPolyIE [181]NER, REICLGPT-3.5, GPT-4Foppiano et al. [47]NER, RE ZS Pr, ICL, SFTGPT-3.5, GPT-4Dagdelen et al. [182]NER, RESFTGPT-3, LLaMA2AstronomicalShao et al. [173] Evans et al. [164]NER NERZS Pr DAGPT-3.5, GPT-4, Claude-2, LLaMA2 GPT-3.5, GPT-4HistoricalGonzález-Gallardo et al. [189] CHisIEC [126]NER NER, REZS Pr SFT, ICLGPT-3.5 ChatGLM2, Alpaca2, GPT-3.5Nunes et al. [89]NERICLSabiaLegalOliveira et al. [78]NERDAGPT-3Kwak et al. [115]RE, EEICLGPT-4</p>
<p>Table 6
6
A summary of some representative IE datasets.
DatasetSummaryDataset scope: NER;CoNLL03 [239]1,393 English news articles from Reuters; 909 German news articles;4 annotated entity types.Dataset scope: RE;CoNLL04 [240]entity-relation triples from news sentences; 4 entity types;5 relation types.Dataset scope: NER, RE and EE;ACE05 [241]various text types and genres; 7 entity types; 7 relation types;33 event types and 22 argument roles.</p>
<p>Table 7
7
[192]stics of common datasets for information extraction.*denotes the dataset is multimodal.#refers to the number of categories or sentences.The data in the table is partially referenced from InstructUIE[192].Ni X, Li P, Li H. Unified text structuralization with instruction-tuned language models.arXiv preprint arXiv:2303.14956,2023 [123] Zaratiana U, Tomeh N, Holat P, Charnois T.An autoregressive text-to-graph framework for joint entity and relation extraction.In: Thirty-Eighth AAAI Conference on Artificial Intelligence.2024, 19477-19487 [124] Peng L, Wang Z, Yao F, Wang Z, Shang J. Metaie: Distilling a meta model from llm for all kinds of information extraction tasks.
TaskDatasetDomain#Class#Train#Val#TestACE04 [242]News76,202745812ACE05 [241]News77,2999711,060BC5CDR [243]Biomedical24,5604,5814,797Broad Twitter Corpus [244]Social Media36,3381,0012,000CADEC [245]Biomedical15,3401,0971,160CoNLL03 [239]News414,0413,2503,453CoNLLpp [246]News414,0413,2503,453CrossNER-AI [247]Artificial Intelligence14100350431CrossNER-Literature [247]Literary12100400416CrossNER-Music [247]Musical13100380465CrossNER-Politics [247]Political9199540650CrossNER-Science [247]Scientific17200450543FabNER [248]Scientific129,4352,1822,064Few-NERD [249]General66131,76718,82437,468FindVehicle [250]Traffic2121,56520,77720,777NERGENIA [251] HarveyNER [252]Biomedical Social Media5 415,023 3,9671,669 1,3011,854 1,303MIT-Movie [253]Social Media129,7742,4422,442MIT-Restaurant [253]Social Media87,6591,5201,520MultiNERD [254]Wikipedia16134,14410,00010,000NCBI [255]Biomedical45,432923940OntoNotes 5.0 [256]General1859,9248,5288,262ShARe13 [257]Biomedical18,50812,0509,009ShARe14 [258]Biomedical117,4041,36015,850SNAP  *  [259]Social Media44,2901,4321,459TTC [260]Social Meida310,0005001,500Tweebank-NER [261]Social Media41,6397101,201Twitter2015  *  [262]Social Media44,0001,0003,357Twitter2017  *  [259]Social Media43,373723723TwitterNER7 [263]Social Media77,111886576WikiDiverse  *  [264]News136,312755757WNUT2017 [265]Social Media63,3941,0091,287ACE05 [241]News710,0512,4202,050ADE [266]Biomedical13,417427428CoNLL04 [240]News5922231288DocRED [267]Wikipedia963,008300700MNRE  *  [268]Social Media2312,2471,6241,614RENYT [269]News2456,1965,0005,000Re-TACRED [270]News4058,46519,58413,418SciERC [271]Scientific71,366187397SemEval2010 [272]General196,5071,4932,717TACRED [273]News4268,12422,63115,509TACREV [274]News4268,12422,63115,509ACE05 [241]News33/2217,172923832CASIE [275]Cybersecurity5/2611,1891,7783,208GENIA11 [276]Biomedical9/118,7301,0911,092EEGENIA13 [277]Biomedical13/74,000500500PHEE [278]Biomedical2/162,898961968RAMS [279]News139/657,329924871WikiEvents [280]Wikipedia50/595,262378492
Tong Xu is currently working as a Professor at University of Science and Technology of China</p>
<p>Acknowledgements This work was supported in part by the grants from National Natural Science Foundation of China (No.62222213, 62072423).Additionally, this research was partially supported by Research Impact Fund (No.R1015-23), APRC -CityU New Research Initiatives (No.9610565, Startup Grant for New Faculty of CityU), CityU -HKIDS Early Career Research Grant (No.9360163), Hong Kong ITC Innovation and Technology Fund Midstream Research Programme for Universities Project (No.ITS/034/22MS), Hong Kong Environmental and Conservation Fund (No. 88/2022), and SIRG -CityU Strategic Interdisciplinary Research Grant (No.7020046), Huawei (Huawei Innovation Research Program), Tencent (CCF-Tencent Open Fund, Tencent Rhino-Bird Focused Research Program), Ant Group (CCF-Ant Research Fund, Ant Group Research Fund), Alibaba (CCF-Alimama Tech Kangaroo Fund (No. 2024002)), CCF-BaiChuan-Ebtech Foundation Model Fund, and Kuaishou.Table8The common backbones for generative information extraction.We mark the commonly used base and large versions for better reference.Series Model Size Base ModelOpen Source Instruction Tuning RLHFing to unify all IE tasks with LLMs.Additional theoretical and experimental analysis provided insightful exploration for these methods.Then we delved into different IE techniques that apply LLMs for IE and demonstrate their potential for extracting information in specific domains.Finally, we analyzed the current challenges and presented potential future directions.We hope this survey can provide a valuable resource for researchers to explore more efficient utilization of LLMs for IE.His research interests include data mining, multimodal learning, and large language models.
A comprehensive survey on automatic knowledge graph construction. L Zhong, J Wu, Q Li, H Peng, X Wu, ACM Computing Surveys. 5642023</p>
<p>Collaborative policy learning for open knowledge graph reasoning. C Fu, T Chen, M Qu, Jin W Ren, X , Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing2019</p>
<p>Information extraction supported question answering. R Srihari, W Li, X Li, TREC. 1999</p>
<p>Unified structure generation for universal information extraction. Y Lu, Q Liu, D Dai, Xiao X Lin, H Han, X Sun, L Wu, H , Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics1</p>
<p>X Wang, W Zhou, C Zu, H Xia, T Chen, Y Zhang, R Zheng, J Ye, Q Zhang, T Gui, Instructuie, arXiv:2304.08085Multi-task instruction tuning for unified information extraction. 2023arXiv preprint</p>
<p>others . Retrieval-augmented code generation for universal information extraction. Y Guo, Z Li, Jin X Liu, Y Zeng, Y Liu, W Li, X Yang, P Bai, L Guo, J , arXiv:2311.029622023arXiv preprint</p>
<p>Contextualized hybrid prompt-tuning for generation-based event extraction. Y Zhong, T Xu, P Luo, International Conference on Knowledge Science, Engineering and Management. 2023</p>
<p>A survey on neural open information extraction: Current status and future directions. S Zhou, B Yu, A Sun, C Long, J Li, J Sun, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence. L Raedt, the Thirty-First International Joint Conference on Artificial Intelligence2022</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774others . GPT-4 technical report. 2023arXiv preprint</p>
<p>Unimel: A unified framework for multimodal entity linking with large language models. L Qi, H Yongyi, L Defu, Z Zhi, X Tong, Che L Enhong, C , arXiv:2407.161602024arXiv preprint</p>
<p>Large language model based long-tail query rewriting in taobao search. W Peng, G Li, Y Jiang, Z Wang, D Ou, X Zeng, D Xu, T Xu, Chen E , Companion Proceedings of the ACM on Web Conference. 2024. 2024</p>
<p>Notellm-2: Multimodal large representation models for recommendation. C Zhang, H Zhang, S Wu, D Wu, T Xu, Y Gao, Y Hu, Chen E , arXiv:2405.167892024arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Computing Surveys. 5592023</p>
<p>Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models. Y Lyu, Z Li, S Niu, F Xiong, B Tang, W Wang, H Wu, H Liu, T Xu, Chen E , arXiv:2401.170432024arXiv preprint</p>
<p>Retrieve-plan-generation: An iterative planning and answering framework for knowledge-intensive llm generation. Y Lyu, Z Niu, Z Xie, C Zhang, T Xu, Y Wang, Chen E , arXiv:2406.149792024arXiv preprint</p>
<p>MILL: Mutual verification with large language models for zero-shot query expansion. P Jia, Y Liu, X Zhao, X Li, C Hao, S Wang, D Yin, Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American Chapterthe Association for Computational Linguistics1</p>
<p>Large multimodal model compression via iterative efficient pruning and distillation. M Wang, Y Zhao, J Liu, J Chen, C Zhuang, J Gu, R Guo, X Zhao, Companion Proceedings of the ACM Web Conference 2024. 2024</p>
<p>A unified framework for multi-domain ctr prediction via large language models. Z Fu, X Li, C Wu, Y Wang, K Dong, X Zhao, M Zhao, H Guo, R Tang, arXiv:2312.107432023arXiv preprint</p>
<p>P Jia, Y Liu, X Li, X Zhao, Y Wang, Y Du, X Han, X Wei, S Wang, D Yin, arXiv:2405.14702G3: An effective and adaptive framework for worldwide geolocalization using large multi-modality models. 2024arXiv preprint</p>
<p>Notellm: A retrievable large language model for note recommendation. C Zhang, S Wu, H Zhang, T Xu, Y Gao, Y Hu, Chen E , Companion Proceedings of the ACM on Web Conference. 2024. 2024</p>
<p>X Wang, Z Chen, Z Xie, T Xu, Y He, Chen E , arXiv:2406.13618-context former: Lightning-fast compressing context for large language model. 2024arXiv preprint</p>
<p>Fast memorization of prompt improves context awareness of large language models. J Zhu, S Liu, Y Yu, B Tang, Y Yan, Z Li, F Xiong, T Xu, M B Blaschko, Fastmem, arXiv:2406.160692024arXiv preprint</p>
<p>A survey on large language model based autonomous agents. L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, Frontiers of Computer Science. 1861863452024</p>
<p>Enhancing collaborative semantics of language model-driven recommendations via graph-aware learning. Z Guan, L Wu, H Zhao, M He, Fan J , arXiv:2406.132352024arXiv preprint</p>
<p>Qdmr-based planning-andsolving prompting for complex reasoning tasks. J Huang, Q She, W Jiang, H Wu, Y Hao, T Xu, F Wu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation2024</p>
<p>. C Fu, Y Dai, Y Luo, L Li, S Ren, R Zhang, Z Wang, C Zhou, Y Shen, M Zhang, others . Video-mme: The first-ever comprehensive evaluation benchmark of multi. </p>
<p>Documentlevel entity-based extraction as template generation. H Huang, S Tang, N Peng, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>REBEL: Relation extraction by end-to-end language generation. P L H Cabot, R Navigli, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>X Wei, X Cui, N Cheng, X Wang, X Zhang, S Huang, P Xie, J Xu, Y Chen, M Zhang, arXiv:2302.10205Zero-shot information extraction via chatting with ChatGPT. 2023arXiv preprint</p>
<p>Code4Struct: Code generation for few-shot event structure prediction. X Wang, S Li, H Ji, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics1</p>
<p>S Wang, X Sun, X Li, R Ouyang, F Wu, T Zhang, J Li, G Wang, arXiv:2304.10428Gpt-NER: Named entity recognition via large language models. 2023arXiv preprint</p>
<p>D Ashok, Z C Lipton, Promptner, arXiv:2305.15444Prompting for named entity recognition. 2023arXiv preprint</p>
<p>How to unleash the power of large language models for few-shot relation extraction?. X Xu, Y Zhu, X Wang, N Zhang, Proceedings of the Fourth Workshop on Simple and Efficient Natural Language Processing. the Fourth Workshop on Simple and Efficient Natural Language Processing2023</p>
<p>Named entity recognition and relation extraction: State-of-the-art. Z Nasar, S W Jaffry, M K Malik, ACM Computing Surveys. 5412021</p>
<p>Generative knowledge graph construction: A review. H Ye, N Zhang, H Chen, H Chen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Mining experimental data from materials science literature with large language models. L Foppiano, G Lambard, T Amagasa, M Ishii, arXiv:2401.110522024arXiv preprint</p>
<p>H Liu, W Xue, Y Chen, D Chen, X Zhao, K Wang, L Hou, R Li, W Peng, arXiv:2402.00253A survey on hallucination in large vision-language models. 2024arXiv preprint</p>
<p>P Sahoo, A K Singh, S Saha, V Jain, S Mondal, A Chadha, arXiv:2402.07927A systematic survey of prompt engineering in large language models: Techniques and applications. 2024arXiv preprint</p>
<p>Editing factual knowledge and explanatory ability of medical large language models. D Xu, Z Zhang, Z Zhu, Z Lin, Q Liu, X Wu, T Xu, X Zhao, Y Zheng, Chen E , arXiv:2402.180992024arXiv preprint</p>
<p>Generative entity typing with curriculum learning. S Yuan, D Yang, J Liang, Z Li, J Liu, J Huang, Y Xiao, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Calibrated seq2seq models for efficient and generalizable ultra-fine entity typing. Y Feng, A Pratapa, D R Mortensen, 2023Findings of the Association for Computational Linguistics</p>
<p>Template-based named entity recognition using BART. L Cui, Y Wu, J Liu, S Yang, Y Zhang, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Debias for generative extraction in unified NER task. S Zhang, Y Shen, Z Tan, Y Wu, W Lu, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics1</p>
<p>L Wang, R Li, Y Yan, Y Yan, S Wang, W Wu, W Xu, Instructionner, arXiv:2203.03903A multitask instruction-based generative framework for few-shot NER. 2022arXiv preprint</p>
<p>Debiasing generative named entity recognition by calibrating sequence likelihood. Y Xia, Y Zhao, W Wu, S Li, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsShort Papers2</p>
<p>In-context learning for few-shot multimodal named entity recognition. C Cai, Q Wang, B Liang, B Qin, M Yang, K F Wong, R Xu, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Entity-to-text based data augmentation for various named entity recognition tasks. X Hu, Y Jiang, A Liu, Z Huang, P Xie, F Huang, L Wen, S Y Philip, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Learning to rank context for named entity recognition using a synthetic dataset. A Amalvy, V Labatut, R Dufour, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>One model for all domains: Collaborative domain-prefix tuning for cross-domain NER. X Chen, L Li, S Qiao, N Zhang, C Tan, Y Jiang, F Huang, H Chen, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. the Thirty-Second International Joint Conference on Artificial Intelligence2023</p>
<p>Making large language models as active annotators. R Zhang, Y Li, Y Ma, M Zhou, L Zou, Llmaaa, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Large language model is not a good few-shot information extractor, but a good reranker for hard samples!. Y Ma, Y Cao, Y Hong, A Sun, 2023. 2023Findings of the Association for Computational Linguistics</p>
<p>Self-improving for zero-shot named entity recognition with large language models. T Xie, Q Li, Y Zhang, Z Liu, H Wang, Proceedings of the 2024 Conference of the North American Chapter. the 2024 Conference of the North American ChapterShort Papers2</p>
<p>Targeted distillation from large language models for open named entity recognition. W Zhou, S Zhang, Y Gu, M Chen, H Poon, Universalner, The Twelfth International Conference on Learning Representations. 2024</p>
<p>NAG-NER: a unified non-autoregressive generation framework for various NER tasks. X Zhang, M Tan, J Zhang, W Zhu, Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics: Industry Track. the The 61st Annual Meeting of the Association for Computational Linguistics: Industry Track2023</p>
<p>Unified named entity recognition as multi-label sequence generation. J Su, H Yu, International Joint Conference on Neural Networks. 2023</p>
<p>Rethinking negative instances for generative named entity recognition. Y Ding, J Li, P Wang, Z Tang, Y Bowen, M Zhang, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Entity recognition encoder pre-training via llm-annotated data. S Bogdanov, A Constantin, T Bernard, B Crabbé, Bernard E Nuner, arXiv:2402.153432024arXiv preprint</p>
<p>Learning incontext learning for named entity recognition. J Chen, Y Lu, H Lin, J Lou, W Jia, D Dai, H Wu, B Cao, X Han, L Sun, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics1</p>
<p>Linking local named entity recognition models to large language models using uncertainty. Z Zhang, Y Zhao, H Gao, M Hu, Linkner, Proceedings of the ACM on Web Conference. the ACM on Web Conference2024. 2024</p>
<p>Small language model is a good guide for large language model in chinese entity relation extraction. X Tang, J Wang, Q Su, arXiv:2402.143732024arXiv preprint</p>
<p>Embedded named entity recognition using probing classifiers. N Popovič, M Färber, arXiv:2403.117472024arXiv preprint</p>
<p>ProgGen: Generating named entity recognition datasets step-by-step with self-reflexive large language models. Y Heng, C Deng, Y Li, Y Yu, Y Li, R Zhang, C Zhang, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Y Mo, J Yang, J Liu, S Zhang, Wang J Li, Z C-Icl, arXiv:2402.11254Contrastive in-context learning for information extraction. 2024arXiv preprint</p>
<p>Advancing entity recognition in biomedicine via instruction tuning of large language models. K Keloth, Y Hu, Q Xie, X Peng, Y Wang, A Zheng, M Selek, K Raja, C H Wei, Jin Q Others, Bioinformatics. e1632024</p>
<p>Ver-ifiNER: Verification-augmented NER via knowledge-grounded reasoning with large language models. S Kim, K Seo, H Chae, J Yeo, D Lee, Proceedings of the 62nd Annual Meeting of the Association for of portuguese legal named entity recognition through in-context learning. the 62nd Annual Meeting of the Association for of portuguese legal named entity recognition through in-context learning</p>
<p>Knowledge-enriched prompt for lowresource named entity recognition. W Hou, W Zhao, X Liu, W Guo, ACM Transactions on Asian and Low-Resource Language Information Processing. 2352024</p>
<p>Rt: a retrieving and chain-of-thought framework for few-shot medical named entity recognition. M Li, H Zhou, H Yang, R Zhang, Journal of the American Medical Informatics Association. e0952024</p>
<p>P-icl: Point in-context learning for named entity recognition with large language models. G Jiang, Z Ding, Y Shi, D Yang, arXiv:2405.049602024arXiv preprint</p>
<p>J Biana, W Zhai, X Huang, J Zheng, Zhu S Vaner, arXiv:2404.17835Leveraging large language model for versatile and adaptive biomedical named entity recognition. 2024arXiv preprint</p>
<p>LLMs as bridges: Reformulating grounded multimodal named entity recognition. J Li, H Li, D Sun, J Wang, W Zhang, Z Wang, G Pan, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Llm-da: Data augmentation via large language models for few-shot named entity recognition. J Ye, N Xu, Y Wang, J Zhou, Q Zhang, T Gui, X Huang, arXiv:2402.145682024arXiv preprint</p>
<p>Revisiting large language models as zero-shot relation extractors. G Li, P Wang, W Ke, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Guideline learning for in-context information extraction. C Pang, Y Cao, Q Ding, P Luo, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Aligning instruction tasks unlocks large language models as zero-shot relation extractors. K Zhang, B J Gutierrez, Y Su, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>GPT-RE: in-context learning for relation extraction using large language models. Z Wan, F Cheng, Z Mao, Q Liu, H Song, J Li, S Kurohashi, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>STAR: Boosting low-resource event extraction by structureto-text data generation with large language models. M D Ma, X Wang, P N Kung, P J Brantingham, N Peng, Wang W , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Improving unsupervised relation extraction by augmenting diverse sentence pairs. Q Wang, K Zhou, Q Qiao, Y Li, Q Li, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Improving event definition following for zero-shot event detection. Z Cai, P N Kung, A Suvarna, M D Ma, H Bansal, B Chang, P J Brantingham, Wang W Peng, N , arXiv:2403.025862024arXiv preprint</p>
<p>Document-level event argument extraction by conditional generation. S Li, H Ji, J Han, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Text2Event: Controllable sequence-to-structure generation for end-to-end event extraction. Y Lu, H Lin, J Xu, X Han, J Tang, A Li, L Sun, M Liao, S Chen, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Claret: Pre-training a correlation-aware context-to-event transformer for eventcentric generation and classification. Y Zhou, T Shen, X Geng, G Long, D Jiang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics1</p>
<p>Multilingual generative language models for zero-shot cross-lingual event argument extraction. H Huang, I Hsu, P Natarajan, K W Chang, N Peng, Others, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics1</p>
<p>Prompt for extraction? PAIE: Prompting argument interaction for event argument extraction. Y Ma, Z Wang, Y Cao, M Li, M Chen, K Wang, J Shao, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics1</p>
<p>Dynamic prefix-tuning for generative template-based event extraction. X Liu, H Y Huang, G Shi, B Wang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics1</p>
<p>A Monte Carlo language model pipeline for zero-shot sociopolitical event extraction. E Cai, O' Connor, B , NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Context-aware prompt for generation-based event argument extraction with diffusion models. L Luo, Y Xu, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. the 32nd ACM International Conference on Information and Knowledge Management2023</p>
<p>Event extraction as question generation and answering. D Lu, S Ran, J Tetreault, A Jaimes, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsShort Papers2</p>
<p>Contextualized soft prompts for extraction of event arguments. C V Nguyen, H Man, T H Nguyen, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>AMPERE: amr-aware prefix for generationbased event argument extraction model. I Hsu, Z Xie, K Huang, P Natarajan, N Peng, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics1</p>
<p>Enhancing low-resource generative event extraction with auxiliary keyword sub-prompt. J Duan, X Liao, An Y , Wang J Keyee, Big Data Mining and Analytics. 722024</p>
<p>Global constraints with prompting for zero-shot event argument classification. Z Lin, H Zhang, Y Song, Findings of the Association for Computational Linguistics: EACL 2023. 2023</p>
<p>Beyond single-event extraction: Towards efficient document-level multi-event argument extraction. W Liu, L Zhou, D Zeng, Y Xiao, S Cheng, C Zhang, G Lee, M Zhang, Chen W , Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Unleash llms' potential for event argument extraction through hierarchical modeling and pair-wise refinement. F Zhang, C Blum, T Choji, S Shah, A Vempala, Ultra, arXiv:2401.132182024arXiv preprint</p>
<p>Leveraging ChatGPT in pharmacovigilance event extraction: An empirical study. Z Sun, G Pergola, B Wallace, Y He, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European ChapterShort Papers2</p>
<p>LLMs learn task heuristics from demonstrations: A heuristic-driven prompting strategy for document-level event argument extraction. H Zhou, J Qian, Z Feng, L Hui, Z Zhu, K Mao, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics1</p>
<p>DEGREE: A data-efficient generation-based event extraction model. I Hsu, K Huang, E Boschee, S Miller, P Natarajan, K Chang, N Peng, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>Demonstration-enhanced schema-guided generation for low-resource event extraction. G Zhao, X Gong, X Yang, G Dong, S Lu, Li S Demosg, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Eventrl: Enhancing event extraction with outcome supervision for large language models. J Gao, H Zhao, Wang W Yu, C Xu, R , arXiv:2402.114302024arXiv preprint</p>
<p>Textee: Benchmark, reevaluation, reflections, and future challenges in event extraction. H Huang, I H Hsu, T Parekh, Z Xie, Z Zhang, P Natarajan, K W Chang, N Peng, H Ji, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Pretraining of language models for structure prediction. C Wang, X Liu, Z Chen, H Hong, J Tang, Song D Deepstruct, Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>Set learning for generative information extraction. J Li, Y Zhang, B Liang, K F Wong, R Xu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Collabkg: A learnable humanmachine-cooperative information extraction toolkit for (event) knowledge graph construction. X Wei, Y Chen, N Cheng, X Cui, J Xu, W Han, arXiv:2307.007692023arXiv preprint</p>
<p>J Wang, Y Chang, Z Li, An N Ma, Q Hei, L Luo, H Lu, Y Ren, F , arXiv:2401.045070: A large language model project to solve the task of knowledge graph construction. 2024arXiv preprint</p>
<p>Yayi-uie: A chat-enhanced instruction tuning framework for universal information extraction. X Xiao, Y Wang, N Xu, Y Wang, H Yang, M Wang, Y Luo, L Wang, W Mao, D Zeng, arXiv:2312.155482023arXiv preprint</p>
<p>Exploring chat-based unified information extraction using large language models. J Xu, M Sun, Z Zhang, J Zhou, Chatuie, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation2024</p>
<p>Unearthing large scale schema-conditioned information extraction corpus. H Gui, L Yuan, H Ye, N Zhang, M Sun, L Liang, Chen H Iepile, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsShort Papers2</p>
<p>Diluie: constructing diverse demonstrations of in-context learning with large language model for unified information extraction. Q Guo, Y Guo, J Zhao, Neural Computing and Applications2024</p>
<p>Code language model for generative knowledge graph construction. Z Bi, J Chen, Y Jiang, F Xiong, W Guo, H Chen, N Zhang, Codekgc, ACM Transactions on Asian and Low-Resource Language Information Processing. 2332024</p>
<p>Coding structured knowledge into LLMs for universal information extraction. Z Li, Y Zeng, Y Zuo, W Ren, W Liu, M Su, Y Guo, Y Liu, L Lixiang, Z Hu, L Bai, W Li, Y Liu, P Yang, Jin X Guo, J Cheng, X Knowcoder, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics1</p>
<p>Semi-automatic data enhancement for document-level relation extraction with distant supervision from large language models. J Li, Z Jia, Z Zheng, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Does synthetic data generation of LLMs help clinical text mining?. R Tang, X Han, X Jiang, X Hu, arXiv:2303.043602023arXiv preprint</p>
<p>Large language models as instructors: A study on multilingual clinical entity extraction. S Meoni, D Clergerie, E Ryffel, T , The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023</p>
<p>Astroner-astronomy named entity recognition: Is gpt a good domain expert annotator?. J Evans, S Sadruddin, D Souza, J , arXiv:2405.026022024arXiv preprint</p>
<p>Augmenting ner datasets with llms: Towards automated and refined annotation. Y Naraki, R Yamaki, Y Ikeda, T Horie, H Naganuma, arXiv:2404.013342024preprint</p>
<p>Chain-of-thought prompt distillation for multimodal named entity and multimodal relation extraction. F Chen, Y Feng, arXiv:2306.141222023arXiv preprint</p>
<p>Prompting ChatGPT in MNER: enhanced multimodal named entity recognition with auxiliary refined knowledge. J Li, H Li, Z Pan, D Sun, J Wang, W Zhang, G Pan, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction. M Josifoski, M Sakota, M Peyrard, R West, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Revisiting relation extraction in the era of large language models. S Wadhwa, S Amir, B Wallace, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics1</p>
<p>Zero-shot temporal relation extraction with ChatGPT. C Yuan, Q Xie, S Ananiadou, The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023</p>
<p>Inspire the large language model by external knowledge on biomedical named entity recognition. J Bian, J Zheng, Y Zhang, S Zhu, arXiv:2309.122782023arXiv preprint</p>
<p>Improving large language models for clinical named entity recognition via prompt engineering. Y Hu, Q Chen, J Du, X Peng, V K Keloth, X Zuo, Y Zhou, Z Li, X Jiang, Z Lu, Journal of the American Medical Informatics Association. 2592024</p>
<p>Astronomical knowledge entity extraction in astrophysics journal articles via large language models. W Shao, R Zhang, Ji P Fan, D Hu, Y Yan, X Cui, C Tao, Y Mi, L Chen, L , Research in Astronomy and Astrophysics. 246650122024</p>
<p>Flexible grammar-based constrained decoding for language models. S Geng, M Josifosky, M Peyrard, R West, arXiv:2305.139712023arXiv preprint</p>
<p>Autoregressive structured prediction with language models. T Liu, Y E Jiang, N Monath, R Cotterell, M Sachan, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Light-NER: A lightweight tuning paradigm for low-resource NER via pluggable prompting. X Chen, L Li, S Deng, C Tan, C Xu, F Huang, L Si, H Chen, N Zhang, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022</p>
<p>Know-adapter: Towards knowledge-aware parameter-efficient transfer learning for few-shot named entity recognition. B Nie, Y Shao, Y Wang, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation2024</p>
<p>2iner: Instructive and incontext learning on few-shot named entity recognition. J Zhang, X Liu, X Lai, Y Gao, S Wang, Y Hu, Y Lin, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>M Monajatipoor, J Yang, J Stremmel, M Emami, F Mohaghegh, M Rouhsedaghat, K W Chang, arXiv:2404.07376Llms in biomedicine: A study on clinical named entity recognition. 2024arXiv preprint</p>
<p>Structured information extraction from complex scientific text with fine-tuned large language models. A Dunn, J Dagdelen, N Walker, S Lee, A S Rosen, G Ceder, K Persson, A Jain, arXiv:2212.052382022arXiv preprint</p>
<p>PolyIE: A dataset of information extraction from polymer material scientific literature. J Cheung, J Zhuang, Y Li, Y Shetty, P Zhao, W Grampurohit, S Ramprasad, R Zhang, C Dagdelen, J Dunn, A Lee, S Walker, N Rosen, A S Ceder, G Persson, K A Jain, A , arXiv:2311.07715Nature Communications. 182114182023. 2024arXiv preprintStructured information extraction from scientific text with large language models</p>
<p>DICE: data-efficient clinical event extraction with generative models. M D Ma, A Taylor, W Wang, N Peng, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics1</p>
<p>Zero-shot clinical entity recognition using ChatGPT. Y Hu, I Ameer, X Zuo, X Peng, Y Zhou, Z Li, Y Li, J Li, X Jiang, H Xu, arXiv:2303.164162023arXiv preprint</p>
<p>Large language models are fewshot clinical information extractors. M Agrawal, S Hegselmann, H Lang, Y Kim, D Sontag, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022. 1998-2022</p>
<p>A zero-shot and few-shot study of instructionfinetuned large language models applied to clinical and biomedical tasks. Y Labrak, M Rouvier, R Dufour, arXiv:2307.121142023arXiv preprint</p>
<p>Thinking about GPT-3 in-context learning for biomedical IE? think again. B J Gutiérrez, N Mcneal, C Washington, Y Chen, L Li, H Sun, Y Su, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>J Biana, W Zhai, X Huang, J Zheng, Zhu S Vaner, arXiv:2404.17835Leveraging large language model for versatile and adaptive biomedical named entity recognition. 2024arXiv preprint</p>
<p>Yes but.. can chatgpt identify entities in historical documents?. C González-Gallardo, E Boros, N Girdhar, A Hamdi, J G Moreno, A Doucet, 2023ACM/IEEE Joint Conference on Digital Libraries</p>
<p>Empirical study of zero-shot NER with ChatGPT. T Xie, Q Li, J Zhang, Y Zhang, Z Liu, H Wang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Exploring the feasibility of ChatGPT for event extraction. J Gao, H Zhao, C Yu, R Xu, arXiv:2303.038362023arXiv preprint</p>
<p>In-structIE: A Chinese instruction-based information extraction dataset. H Gui, J Zhang, H Ye, N Zhang, arXiv:2305.115272023arXiv preprint</p>
<p>Is information extraction solved by ChatGPT? an analysis of performance, evaluation criteria, robustness and errors. R Han, T Peng, C Yang, B Wang, L Liu, Wan X , arXiv:2305.144502023arXiv preprint</p>
<p>NERetrieve: Dataset for next generation named entity recognition and retrieval. U Katz, M Vetzler, A Cohen, N Goldberg, Y , Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Evaluating ChatGPT's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness. B Li, G Fang, Y Yang, Q Wang, W Ye, W Zhao, S Zhang, arXiv:2304.116332023arXiv preprint</p>
<p>H Fei, M Zhang, M Zhang, T S Chua, Xnlp, arXiv:2308.01846An interactive demonstration system for universal structured nlp. 2023arXiv preprint</p>
<p>RexUIE: A recursive method with explicit schema instructor for universal information extraction. C Liu, F Zhao, Y Kang, J Zhang, X Zhou, C Sun, K Kuang, F Wu, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Mirror: A universal framework for various information extraction tasks. T Zhu, J Ren, Z Yu, M Wu, G Zhang, X Qu, Chen W Wang, Z Huai, B Zhang, M , Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Abductive commonsense reasoning. C Bhagavatula, R L Bras, C Malaviya, K Sakaguchi, A Holtzman, H Rashkin, D Downey, Yih T W, Y Choi, 8th International Conference on Learning Representations. 2020</p>
<p>. OpenAI . Introduce ChatGPT. OpenAI blog. 2023</p>
<p>LLM-powered data augmentation for enhanced crosslingual performance. C Whitehouse, M Choudhury, A Aji, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>A survey on large language models for recommendation. L Wu, Z Zheng, Z Qiu, H Wang, H Gu, T Shen, C Qin, C Zhu, H Zhu, Q Liu, World Wide Web. 275602024</p>
<p>Tomgpt: Reliable text-only training approach for cost-effective multi-modal large language model. Y Chen, Q Wang, S Wu, Y Gao, T Xu, Y Hu, ACM Transactions on Knowledge Discovery from Data. 2024</p>
<p>Bridging gaps in content and knowledge for multimodal entity linking. P Luo, T Xu, C Liu, S Zhang, L Xu, M Li, Chen E , 2024ACM Multimedia</p>
<p>Latex-gcl: Large language models (llms)-based data augmentation for text-attributed graph contrastive learning. H Yang, X Zhao, S Huang, Q Li, G Xu, arXiv:2409.011452024arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, H Wang, arXiv:2312.109972023arXiv preprint</p>
<p>L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, Others, arXiv:2101.00027The pile: An 800gb dataset of diverse text for language modeling. 2020arXiv preprint</p>
<p>Prompt engineering in large language models. G Marvin, N Hellen, D Jjingo, J Nakatumba-Nabende, International Conference on Data Intelligence and Cognitive Informatics. 2023</p>
<p>Logic alignment of non-tuning large language models and online recommendation systems for explainable reason generation. H Zhao, S Zheng, L Wu, B Yu, Wang J Lane, arXiv:2407.028332024arXiv preprint</p>
<p>Z Zheng, Z Qiu, X Hu, L Wu, H Zhu, H Xiong, arXiv:2307.02157Generative job recommendations with large language model. 2023arXiv preprint</p>
<p>Exploring large language model for graph data understanding in online job recommendations. L Wu, Z Qiu, Z Zheng, H Zhu, Chen E , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Harnessing large language models for textrich sequential recommendation. Z Zheng, W Chao, Z Qiu, H Zhu, H Xiong, Proceedings of the ACM on Web Conference. the ACM on Web Conference2024. 2024</p>
<p>Unleashing the potential of prompt engineering in large language models: a comprehensive review. B Chen, Z Zhang, N Langrené, S Zhu, arXiv:2310.147352023arXiv preprint</p>
<p>Z Zhao, F Lin, X Zhu, Z Zheng, T Xu, S Shen, X Li, Z Yin, Chen E Dynllm, arXiv:2405.07580When large language models meet dynamic graph recommendation. 2024arXiv preprint</p>
<p>J Wang, E Shi, S Yu, Z Wu, C Ma, H Dai, Q Yang, Y Kang, J Wu, H Hu, Others, arXiv:2304.14670Prompt engineering for healthcare: Methodologies and applications. 2023arXiv preprint</p>
<p>. D Xu, Z Zhang, Z Lin, X Wu, Z Zhu, T Xu, X Zhao, Y Zheng, Chen E , </p>
<p>Visualization recommendation with prompt-based reprogramming of large language models. X Li, J Zhou, W Chen, D Xu, T Xu, Chen E , Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics1</p>
<p>Speak from heart: An emotionguided llm-based multimodal method for emotional dialogue generation. C Liu, Z Xie, S Zhao, J Zhou, T Xu, M Li, Chen E , Proceedings of the 2024 International Conference on Multimedia Retrieval. the 2024 International Conference on Multimedia Retrieval2024</p>
<p>Are gpt embeddings useful for ads and recommendation?. W Peng, D Xu, T Xu, J Zhang, Chen E , International Conference on Knowledge Science, Engineering and Management. 2023</p>
<p>Are you copying my model? protecting the copyright of large language models for eaas via backdoor watermark. W Peng, J Yi, F Wu, S Wu, B B Zhu, L Lyu, B Jiao, T Xu, G Sun, X Xie, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics1</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, Chi E Le, Q V Zhou, D , Advances in neural information processing systems. 202235</p>
<p>Z Chu, J Chen, Q Chen, W Yu, T He, H Wang, W Peng, M Liu, B Qin, T Liu, arXiv:2309.15402A survey of chain of thought reasoning: Advances, frontiers and future. 2023arXiv preprint</p>
<p>Large language models are zeroshot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. 2022. 2022</p>
<p>S Yin, C Fu, S Zhao, K Li, X Sun, T Xu, Chen E , arXiv:2306.13549A survey on multimodal large language models. 2023arXiv preprint</p>
<p>Efficient guided generation for large language models. B T Willard, R Louf, arXiv-23072023arXiv eprints</p>
<p>Prompt sketching for large language models. L Beurer-Kellner, M N Müller, M Fischer, M Vechev, arXiv:2311.049542023arXiv preprint</p>
<p>L Zheng, L Yin, Z Xie, J Huang, C Sun, C H Yu, S Cao, C Kozyrakis, I Stoica, arXiv:2312.07104Gonzalez J E, others . Efficiently programming large language models using sglang. 2023arXiv preprint</p>
<p>Few-shot named entity recognition: An empirical baseline study. J Huang, C Li, K Subudhi, D Jose, S Balakrishnan, W Chen, B Peng, J Gao, J Han, Proceedings of the 2021 conference on empirical methods in. the 2021 conference on empirical methods in</p>
<p>Z Liu, L Wu, M He, Z Guan, H Zhao, Feng N Dr, arXiv:2406.15504e bridges graphs with large language models through words. 2024arXiv preprint</p>
<p>Z Guan, H Zhao, L Wu, M He, Fan J Langtopo, arXiv:2406.13250Aligning language descriptions of graphs with tokenized topological modeling. 2024arXiv preprint</p>
<p>Scaling up multivariate time series pre-training with decoupled spatialtemporal representations. R Zha, L Zhang, S Li, J Zhou, T Xu, H Xiong, Chen E , 2024 IEEE 40th International Conference on Data Engineering (ICDE). 2024</p>
<p>Comi: Correct and mitigate shortcut learning behavior in deep neural networks. L Zhao, Q Liu, L Yue, Chen W Chen, L Sun, R Song, C , Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>When box meets graph neural network in tag-aware recommendation. F Lin, Z Zhao, X Zhu, D Zhang, S Shen, X Li, T Xu, S Zhang, Chen E , Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>When moe meets llms: Parameter efficient fine-tuning for multi-task medical applications. Q Liu, X Wu, X Zhao, Y Zhu, D Xu, F Tian, Y Zheng, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '24. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '242024</p>
<p>Large language model distilling medication recommendation model. Q Liu, X Wu, X Zhao, Y Zhu, Z Zhang, F Tian, Y Zheng, arXiv:2402.028032024arXiv preprint</p>
<p>Llm4msr: An llm-enhanced paradigm for multi-scenario recommendation. Y Wang, Y Wang, Z Fu, X Li, X Zhao, H Guo, R Tang, arXiv:2406.125292024arXiv preprint</p>
<p>Recommender systems in the era of large language models (llms). Z Zhao, Fan W Li, J Liu, Y Mei, X Wang, Y Wen, Z Wang, F Zhao, X Tang, J Li, Q , IEEE Transactions on Knowledge &amp; Data Engineering. 2024</p>
<p>Reasoning with language model prompting: A survey. S Qiao, Y Ou, N Zhang, X Chen, Y Yao, S Deng, C Tan, F Huang, H Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsLong Papers1</p>
<p>Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. E T K Sang, De Meulder, F , Proceedings of the Seventh Conference on Natural Language Learning. the Seventh Conference on Natural Language Learning2003</p>
<p>A linear programming formulation for global inference in natural language tasks. D Roth, W Yih, Proceedings of the Eighth Conference on Computational Natural Language Learning. the Eighth Conference on Computational Natural Language Learning2004</p>
<p>. C Walker, S Strassel, J Medero, Maeda K Ace, 2005. 2006multilingual training corpus</p>
<p>The automatic content extraction (ACE) program -tasks, data, and evaluation. G R Doddington, A Mitchell, M A Przybocki, L A Ramshaw, S M Strassel, R M Weischedel, Proceedings of the Fourth International Conference on Language Resources and Evaluation. the Fourth International Conference on Language Resources and Evaluation2004</p>
<p>Biocreative V CDR task corpus: a resource for chemical disease relation extraction. J Li, Y Sun, R J Johnson, D Sciaky, C H Wei, R Leaman, A P Davis, C J Mattingly, T C Wiegers, Z Lu, Database J. Biol. Databases Curation. 2016. 2016</p>
<p>Broad Twitter corpus: A diverse named entity recognition resource. L Derczynski, K Bontcheva, I Roberts, 26th International Conference on Computational Linguistics. 2016</p>
<p>Cadec: A corpus of adverse drug event annotations. S Karimi, A Metke-Jimenez, M Kemp, C Wang, J. Biomed. Informatics. 552015</p>
<p>Training named entity tagger from imperfect annotations. Z Wang, J Shang, L Liu, L Lu, J Liu, Han J Crossweigh, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing2019</p>
<p>Evaluating cross-domain named entity recognition. Z Liu, Y Xu, T Yu, W Dai, Z Ji, S Cahyawijaya, A Madotto, P Fung, Crossner, Thirty-Fifth AAAI Conference on Artificial Intelligence. 2021</p>
<p>FabNER": information extraction from manufacturing process science domain literature using named entity recognition. A Kumar, B Starly, J. Intell. Manuf. 3382022</p>
<p>Few-NERD: A fewshot named entity recognition dataset. N Ding, G Xu, Y Chen, X Wang, X Han, P Xie, H Zheng, Z Liu, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021</p>
<p>Find-Vehicle and VehicleFinder: A NER dataset for natural language-based vehicle retrieval and a keyword-based cross-modal vehicle retrieval system. R Guan, K L Man, F Chen, S Yao, R Hu, X Zhu, J Smith, E G Lim, Y Yue, Multimedia Tools and Applications. 2023</p>
<p>GENIA corpus -a semantically annotated corpus for bio-textmining. J Kim, T Ohta, Y Tateisi, J Tsujii, Proceedings of the Eleventh International Conference on Intelligent Systems for Molecular Biology. the Eleventh International Conference on Intelligent Systems for Molecular Biology2003</p>
<p>Crossroads, buildings and neighborhoods: A dataset for fine-grained location recognition. P Chen, H Xu, C Zhang, R Huang, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2022</p>
<p>Asgard: A portable architecture for multilingual dialogue systems. J Liu, P Pasupat, S Cyphers, J R Glass, IEEE International Conference on Acoustics, Speech and Signal Processing. 2013</p>
<p>MultiNERD: A multilingual, multi-genre and fine-grained dataset for named entity recognition (and disambiguation). S Tedeschi, R Navigli, Findings of the Association for Computational Linguistics: NAACL 2022. 2022</p>
<p>NCBI disease corpus: a resource for disease name recognition and concept normalization. R Dogan R I, Leaman, Z Lu, Journal of Biomedical Informatics. 472014</p>
<p>Towards robust linguistic analysis using ontonotes. S Pradhan, A Moschitti, N Xue, H T Ng, A Björkelund, O Uryupina, Y Zhang, Z Zhong, Proceedings of the Seventeenth Conference on Computational Natural Language Learning. the Seventeenth Conference on Computational Natural Language LearningCoNLL2013. 2013</p>
<p>Task 1: ShARe/CLEF eHealth evaluation lab. S Pradhan, N Elhadad, South B R, D Martínez, L M Christensen, A Vogel, H Suominen, W W Chapman, G K Savova, Working Notes for CLEF 2013 Conference. 2013. 2013</p>
<p>Task 2: ShARe/CLEF eHealth evaluation lab. L Mowery, S Velupillai, L M South B R, Christensen, D Martínez, L Kelly, L Goeuriot, N Elhadad, S Pradhan, G K Savova, W W Chapman, Working Notes for CLEF 2014 Conference. 2014. 2014</p>
<p>Visual attention model for name tagging in multimodal social media. D Lu, L Neves, V Carvalho, N Zhang, H Ji, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>Temporallyinformed analysis of named entity recognition. S Rijhwani, D Preotiuc-Pietro, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Annotating the Tweebank corpus on named entity recognition and building NLP models for social media analysis. H Jiang, Y Hua, D Beeferman, D Roy, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation Conference2022</p>
<p>Adaptive co-attention network for named entity recognition in tweets. Q Zhang, J Fu, X Liu, X Huang, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence. the Thirty-Second AAAI Conference on Artificial Intelligence2018</p>
<p>Named entity recognition in twitter: A dataset and analysis on short-term temporal shifts. A Ushio, F Barbieri, V Silva, L Neves, J Camacho-Collados, Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, AACL/IJCNLP 2022. Long Papers. the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, AACL/IJCNLP 202220221</p>
<p>WikiDiverse: A multimodal entity linking dataset with diversified contextual topics and entity types. X Wang, J Tian, M Gui, Z Li, R Wang, M Yan, L Chen, Y Xiao, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics1</p>
<p>Results of the WNUT2017 shared task on novel and emerging entity recognition. L Derczynski, E Nichols, Erp V M, N Limsopatham, Proceedings of the 3rd Workshop on Noisy User-generated Text. the 3rd Workshop on Noisy User-generated Text2017</p>
<p>Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports. H Gurulingappa, A M Rajput, A Roberts, J Fluck, M Hofmann-Apitius, L Toldo, J. Biomed. Informatics. 4552012</p>
<p>DocRED: A large-scale document-level relation extraction dataset. Y Yao, D Ye, P Li, X Han, Y Lin, Z Liu, Z Liu, L Huang, J Zhou, M Sun, Proceedings of the 57th Conference of the Association for Computational Linguistics. the 57th Conference of the Association for Computational Linguistics2019</p>
<p>MNRE: A challenge multimodal dataset for neural relation extraction with visual evidence in social media posts. C Zheng, Z Wu, J Feng, Z Fu, Y Cai, IEEE International Conference on Multimedia and Expo. 2021</p>
<p>Modeling relations and their mentions without labeled text. S Riedel, L Yao, A Mccallum, Machine Learning and Knowledge Discovery in Databases. 2010</p>
<p>Retacred: Addressing shortcomings of the TA-CRED dataset. G Stoica, E A Platanios, B Póczos, Thirty-Fifth AAAI Conference on Artificial Intelligence. 2021</p>
<p>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Y Luan, L He, M Ostendorf, H Hajishirzi, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. I Hendrickx, S N Kim, Z Kozareva, P Nakov, D Ó Séaghdha, S Padó, M Pennacchiotti, L Romano, S Szpakowicz, Proceedings of the 5th International Workshop on Semantic Evaluation. the 5th International Workshop on Semantic Evaluation2010</p>
<p>Position-aware attention and supervised data improve slot filling. Y Zhang, V Zhong, D Chen, G Angeli, C Manning, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017</p>
<p>TACRED revisited: A thorough evaluation of the TA-CRED relation extraction task. C Alt, A Gabryszak, L Hennig, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>CASIE: extracting cybersecurity event information from text. T Satyapanich, F Ferraro, T Finin, The Thirty-Fourth AAAI Conference on Artificial Intelligence. 2020</p>
<p>Overview of Genia event task in BioNLP shared task. J Kim, Y Wang, T Takagi, A Yonezawa, Proceedings of BioNLP Shared Task 2011 Workshop. BioNLP Shared Task 2011 Workshop2011. 2011</p>
<p>The Genia event extraction shared task, 2013 editionoverview. J Kim, Y Wang, Y Yamamoto, Proceedings of the BioNLP Shared Task 2013 Workshop. the BioNLP Shared Task 2013 Workshop2013</p>
<p>PHEE: A dataset for pharmacovigilance event extraction from text. Z Sun, J Li, G Pergola, B C Wallace, B John, N Greene, J Kim, Y He, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Multi-sentence argument linking. S Ebner, P Xia, R Culkin, K Rawlins, B Durme, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Document-level event argument extraction by conditional generation. S Li, H Ji, J Han, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 20212021</p>
<p>Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, Mohamed A Levy, O Stoyanov, V Zettlemoyer, L Bart, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, J. Mach. Learn. Res. 21672020</p>
<p>mT5: A massively multilingual pre-trained text-to-text transformer. L Xue, N Constant, A Roberts, M Kale, R Al-Rfou, A Siddhant, A Barua, C Raffel, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>Scaling instructionfinetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, Y Li, X Wang, M Dehghani, S Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>General language model pretraining with autoregressive blank infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Glm, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics1</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, arXiv:2302.13971Azhar e aF. LLaMA: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Stanford Alpaca, An instruction-following llama model. 2023</p>
<p>W Chiang, L Li, Z Lin, Z Sheng, Y Wu, Z Zhang, H Zheng, L Zhuang, S Zhuang, Y Gonzalez, J E Others, Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality. 2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. T Hugo, M Louis, S Kevin, arXiv:2307.092882023arXiv preprint</p>
<p>B Roziere, J Gehring, F Gloeckle, S Sootla, I Gat, X E Tan, Adi Y Liu, J Remez, T Rapin, J Others, arXiv:2308.12950Code LLaMA: Open foundation models for code. 2023arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Ope-nAI blog. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Others, 201919</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Others, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. 2020. 2020</p>
<p>B Wang, Mesh-Transformer-JAX: Modelparallel implementation of Transformer language model with JAX. 2021</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. 2022. 202235</p>            </div>
        </div>

    </div>
</body>
</html>
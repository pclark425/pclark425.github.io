<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8042 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8042</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8042</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-6c5ae8a0bf1885c2dd62c4787db4ac0f5159141f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c5ae8a0bf1885c2dd62c4787db4ac0f5159141f" target="_blank">Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work introduces FLAMe, a family of Foundational Large Autorater Models that outperform popular proprietary LLM-as-a-Judge models on 8 of 12 autorater benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact.</p>
                <p><strong>Paper Abstract:</strong> As large language models (LLMs) evolve, evaluating their output reliably becomes increasingly difficult due to the high cost of human evaluation. To address this, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on a diverse set of over 100 quality assessment tasks, incorporating 5M+ human judgments curated from publicly released human evaluations. FLAMe outperforms models like GPT-4 and Claude-3 on various held-out tasks, and serves as a powerful starting point for fine-tuning, as shown in our reward model evaluation case study (FLAMe-RM). On Reward-Bench, FLAMe-RM-24B achieves 87.8% accuracy, surpassing GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we introduce FLAMe-Opt-RM, an efficient tail-patch fine-tuning approach that offers competitive RewardBench performance using 25×fewer training datapoints. Our FLAMe variants outperform popular proprietary LLM-as-a-Judge models on 8 of 12 autorater benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis shows that FLAMe is significantly less biased than other LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8042.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8042.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RewardBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RewardBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark of pairwise preference datasets designed to evaluate reward models and autoraters across categories (Chat, Chat Hard, Reasoning (Math+Coding), Safety), where models choose the preferred response between two options.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rewardbench: Evaluating reward models for language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAMe-RM-24B, FLAMe-24B, FLAMe-Opt-RM-24B, GPT-4-0125, GPT-4o, PaLM-2-24B, others</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (e.g., FLAMe-24B, GPT-4-0125, GPT-4o, PaLM-2-24B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / NLP (LLM evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Benchmark-based empirical evaluation of LLM preference models (reward models / autoraters)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pairwise preference evaluation (RewardBench)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Models are given a prompt and two candidate responses and must select which response is preferred according to human judgments; aggregated accuracy across held-out pairwise comparisons and categories is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (% preferred-response matches human preference)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percentage of pairwise queries for which the model selects the same response as the human preference label; reported as percent (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>RewardBench (23 datasets across Chat, Chat Hard, Reasoning, Safety)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>RewardBench is constructed from human pairwise preference annotations across multiple underlying datasets; in FLAMe evaluations the full RewardBench set was used (not subsampled) and other held-out tasks were randomly sampled to 256 examples where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>FLAMe-RM-24B accuracy 87.8%, FLAMe-24B 86.0%, FLAMe-Opt-RM-24B 87.0%; GPT-4-0125 85.9%, GPT-4o 84.7% (reported as of July 15, 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>FLAMe-RM-24B outperforms several proprietary generative models on overall RewardBench accuracy (see reported_results).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>RewardBench exhibits length and token biases across categories (e.g., Chat favors longer outputs), making it possible to game the benchmark; authors caution against relying on a single benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8042.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-AggreFact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-AggreFact (attribution benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregation-style benchmark integrating multiple attribution/factuality datasets to evaluate grounding and factual support of claims against source documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Minicheck: Efficient fact-checking of llms on grounding documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAMe-24B, FLAMe-RM-24B, FLAMe-Opt-RM-24B, GPT-4-0125, GPT-4o, others</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / NLP (factuality / attribution evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Benchmark-based evaluation of factuality/attribution judgments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Attribution / grounding evaluation (LLM-AggreFact)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Autoraters determine whether claims (from LLM outputs) are fully supported by a provided source document; performance aggregated across multiple attribution datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy / aggregated score (reported as numeric score)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion of items where the autorater correctly judges attribution/factual support according to dataset labels; reported as a score (0–100 scale in paper's tables).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>LLM-AggreFact (aggregate of ~10 attribution datasets such as AggreFact, TofuEval, WiCE, ExpertQA, LFQA)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Underlying datasets contain human annotations for claim support; FLAMe evaluates models on held-out attribution tasks; sampling of 256 examples per held-out task except where full set used (RewardBench).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>FLAMe-24B achieves overall 81.1 on LLM-AggreFact; GPT-4-0125 80.6 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>FLAMe variants outperform other LLM-as-a-Judge baselines in three of four attribution use-cases (LLM-FactVerify, Wiki-FactVerify, Summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Some long-form QA attribution categories remain challenging; FLAMe underperforms GPT-4 in Long-form QA attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8042.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoBBLEr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoBBLEr (autorater bias benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark designed to quantify cognitive and operational biases in LLM autoraters across several controlled bias scenarios (Order, Compassion, Length, Egocentric, Bandwagon, Attention).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking cognitive biases in large language models as evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAMe-24B, FLAMe-RM-24B, FLAMe-Opt-RM-24B, GPT-4, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / NLP (bias evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Bias-detection evaluation (behavioral tests for autoraters)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>CoBBLEr bias tests</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Apply controlled perturbations to evaluation prompts/responses to measure bias types (e.g., preference for response order or length); compute bias magnitudes per category and average bias.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Bias scores per category (lower is better); overall avg bias</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Normalized bias measures reported per category (e.g., absolute deviations or proportions); aggregated into an average bias score (e.g., FLAMe avg bias 0.13 vs GPT-4 0.31).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CoBBLEr benchmark (Koo et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Original CoBBLEr uses controlled synthetic prompt/response pairs; in this paper, pairs were reformatted into FLAMe's text-to-text format for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>FLAMe-24B avg bias 0.13 (lower is better) vs GPT-4 0.31; FLAMe matches or outperforms GPT-4 across six bias categories (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>FLAMe variants exhibit significantly less bias than several popular autoraters including GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Bias scores depend on CoBBLEr's controlled scenarios; real-world bias profiles may differ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8042.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pairwise Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise preference evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where annotators compare two candidate responses and indicate which is better according to a given criterion (e.g., helpfulness), widely used for reward-model training and assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Models producing candidate responses evaluated by autoraters (e.g., PaLM-2-24B outputs, others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / NLP (evaluation methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human-annotation-based evaluation (preferences)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pairwise preference judgments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human annotators (or pairwise datasets) present two responses for the same prompt and label which is preferable; used in training (RMs) and as evaluation labels for autoraters.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Preference accuracy / agreement with human labels</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion of pairwise comparisons where the model's choice matches the human label; often reported as percent accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Many underlying datasets (e.g., HelpSteer, PRM800K, CommitPack, HH RLHF datasets) and RewardBench which aggregates pairwise tasks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>FLAMe collection includes >5.3M human judgments with a large fraction being pairwise comparisons; pairwise data used both in training and evaluation (some evaluation tasks sampled to 256 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Pairwise-based evaluations drive RewardBench and other reported accuracies; FLAMe-RM fine-tuned on pairwise mixtures increased RewardBench accuracy from 86.0% to 87.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Training on curated pairwise human judgments improved autorater agreement with human preferences versus baseline instruction-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Pairwise data can be costly to collect and may encode inconsistent annotation standards across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8042.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pointwise Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pointwise evaluation (rating individual responses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Annotators rate individual model outputs on attribute scales (e.g., Likert for coherence or helpfulness) rather than comparing two outputs directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAMe training/evaluation uses pointwise data from summarization and other tasks; evaluated models vary</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / NLP (evaluation methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human-annotation-based attribute scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pointwise rating (Likert scale / attribute scores)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human annotators assign scores to a single response along predefined scales (e.g., 1–5 Likert for coherence); models are trained to predict these scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Correlation with human ratings; accuracy within discrete categories; aggregated scores</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Metrics vary by dataset: e.g., Likert 1–5 scales for pointwise data; evaluation often uses Spearman/Pearson correlation or categorical accuracy when discretized.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SummEval, LENS, some SummFeedback pointwise splits, and other summarization/instruction datasets in FLAMe</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>FLAMe includes pointwise annotations (percentage of 5.3M); during evaluation some pointwise labels were reserved (e.g., SummFeedback) while pairwise used in training.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>FLAMe shows improvements on pointwise tasks (e.g., SummFeedback scores increased versus baselines in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Not a direct comparison to human-generated theories; pointwise labels used as training/eval supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Pointwise ratings are subjective, may have inter-rater variability, and differ across original dataset annotation protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8042.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Likert Scale</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Likert scale ratings (e.g., 1–5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common pointwise human rating scale used to quantify attributes like coherence, helpfulness, or fluency of a single response.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used in datasets evaluated/trained on by FLAMe (e.g., SummEval, SummFeedback)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Human evaluation methodology (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Quantitative human annotation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Likert-scale human ratings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Annotators assign discrete ordinal scores (e.g., 1–5) to responses for specified attributes; models learn to predict or match these ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average rating, correlation (Spearman/Pearson) with human mean scores, categorical accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Scores on an ordinal scale (commonly 1–5); aggregated means and correlations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SummEval, SummFeedback, HelpSteer attributes, others in FLAMe</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>FLAMe curators preserved original rating scales when converting tasks to a unified text-to-text format; many datasets included Likert ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as supervision for training and as held-out evaluation; FLAMe improvements reported on tasks that had pointwise Likert labels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Likert scales can suffer from inter-annotator inconsistency and differences in instruction phrasing across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8042.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLEU (Bilingual Evaluation Understudy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lexical-overlap automatic metric for machine translation and text generation that measures n-gram overlap between candidate and reference texts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BLEU: a method for automatic evaluation of machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (automatic metric applied to model outputs in prior literature)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automatic metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute n-gram precision between candidate and reference texts with brevity penalty to yield a score reflecting surface overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU score (0–100 typically reported as 0–1 or as percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Weighted geometric mean of n-gram precisions with brevity penalty; often reported on 0–100 scale or 0–1.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Standard MT and generation datasets cited in related work (Papineni et al., 2002)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>BLEU is an automatic metric and does not involve human annotators; mentioned as traditional baseline in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned in Related Work as a classical lexical-overlap metric; not used in FLAMe experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Focuses on lexical overlap and often correlates poorly with human judgments for open-ended generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8042.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of recall-based n-gram overlap metrics widely used in summarization to measure similarity between system summaries and reference summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ROUGE: A package for automatic evaluation of summaries</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (automatic metric referenced in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automatic metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute overlaps (e.g., n-gram, longest common subsequence) between candidate and reference texts; commonly used to evaluate summarization quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROUGE-N, ROUGE-L scores (commonly reported as F1 or recall percentages)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Overlap-derived scores (0–100% or 0–1) for n-grams or sequence similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Summarization datasets (e.g., XSum) referenced in related work</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Automatic metric; mentioned as part of the historical toolkit for evaluating generated summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned in Related Work; FLAMe instead trains on human judgments and diverse tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>ROUGE correlates imperfectly with human judgments, especially for abstractive summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8042.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation metric that uses contextual embeddings (BERT) to compute token-level similarity between candidate and reference texts, yielding a precision/recall/F1-like score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bertscore: Evaluating text generation with bert</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (automatic metric referenced in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automatic metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute pairwise cosine similarities between token embeddings from a pretrained transformer and aggregate to produce precision, recall and F1 similarity scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BERTScore P/R/F1 (typically reported 0–1 or 0–100)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Aggregated embedding-similarity based precision/recall/F1 between candidate and reference tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Referenced in Related Work as an embedding-based metric</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Automatic; not directly tied to FLAMe experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned in Related Work; not used as primary evaluation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Captures semantic similarity better than lexical overlap metrics but may miss fine-grained human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8042.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAUVE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAUVE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic metric measuring divergence between the distributions of human text and model-generated text using divergence frontiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Measuring the gap between neural text and human text using divergence frontiers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (metric referenced in FLAMe training dataset list)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automatic distributional evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MAUVE</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Measure distributional divergence between model outputs and human text across embedding spaces to assess how human-like generated text is.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MAUVE score (0–1 or scaled), higher indicating closer alignment to human text distribution</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Area under the divergence frontier curve comparing model and human distributions; often normalized.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MAUVE dataset/metric referenced among FLAMe's generalized quality datasets</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Automatic metric; included among datasets the authors used (MAUVE entries appear in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Included in FLAMe's dataset collection but not reported as a central evaluation metric in main results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Distributional metrics can mask specific errors or factuality issues important to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8042.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEURT / COMET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLEURT and COMET (learned evaluation metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Learned automatic metrics trained to predict human quality ratings for generated text (BLEURT for generation; COMET for MT evaluation), using supervised regression/classification on human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BLEURT: Learning robust metrics for text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (automatic learned metrics cited in Related Work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Learned automatic evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEURT / COMET</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Train supervised models to predict human ratings of text quality using annotated datasets; use predicted scores to evaluate generation systems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEURT/COMET predicted score (continuous), correlation with human ratings often reported</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Model outputs a scalar aligned with human judgments; higher indicates higher predicted quality. Correlations (Pearson/Spearman) commonly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Training corpora of human ratings for generation/MT tasks (e.g., WMT, SummEval); cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>These metrics are trained on human judgments; FLAMe differs by training autoraters directly on diverse human evaluation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned in Related Work; not central to FLAMe experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Learned metrics risk overfitting training annotation styles and may not generalize across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8042.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QAGS / Q^2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QAGS and Q^2 (question-answering based factuality checks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that evaluate factual consistency by generating questions from claims and using QA systems to verify supporting evidence (QAGS / Q^2 approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>QAGS: Question Answering-based Metric for Evaluating Factual Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (automatic factuality assessment approaches referenced in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP factuality evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automatic factuality-checking framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Question-generation plus QA (QAGS / Q^2)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate questions from a summary/claim then answer them using sources; compare answers to determine whether the content is supported by source documents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Factuality score (e.g., proportion of QA pairs supported)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Fraction or percentage of generated Q&A pairs where the answer from the source matches the claim-derived answer; reported as 0–100%.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used in prior factuality datasets (e.g., QAGS referenced in FLAMe's factuality dataset list)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>QAGS is an automatic approach but often validated against human judgments in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned as part of the landscape of factuality metrics; FLAMe trains on human-labelled factuality datasets instead of relying solely on QAGS-style pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>QA-based checks depend on QA system performance and question generation quality; may miss nuanced attribution issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8042.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FActScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FActScore (Fine-grained atomic evaluation of factual precision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-grained factuality evaluation approach/dataset that assesses atomic claim-level factual precision in generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FActScore: Fine-grained atomic evaluation of factual precision in long form text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (dataset/method referenced among factuality resources)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP factuality evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Dataset / evaluation method for factual precision</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>FActScore</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Annotate and evaluate atomic factual claims within generated text for precision and grounding to sources; used to measure hallucination rates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Atomic claim-level factuality scores (precision/recall or percent of claims supported)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion of annotated atomic claims deemed factually supported by source documents; typically 0–100% or fraction.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>FActScore dataset (Min et al., 2023) included among FLAMe's sources</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>FActScore is human-annotated at the claim level; FLAMe included such datasets in its training collection for factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as training data for FLAMe's factuality/attribution capabilities; not reported as a standalone metric in main result tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Fine-grained annotation is labor-intensive and definitions of 'atomic claim' can vary across annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8042.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tail-patch fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tail-patch fine-tuning / tail-patch ablations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight ablation/fine-tuning procedure where a partially trained multitask checkpoint is briefly fine-tuned separately on each task to assess per-task impact, enabling re-weighting of the multitask mixture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2-24B initialized FLAMe checkpoints</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>24B (PaLM-2-24B / FLAMe-24B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning (multitask optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Optimization / ablation method for multitask mixture weights</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Tail-patch ablation and re-weighted mixture optimization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>From a partially trained checkpoint, fine-tune briefly (tail-patch) on each individual training task to measure its effect on target downstream performance (e.g., RewardBench), then group tasks and assign new mixture weights to optimize sample-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Downstream target benchmark performance (e.g., RewardBench Chat Hard score) used to rate task impact; bundle weights set accordingly</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Categorical ratings per task: Helpful (+2), Somewhat helpful (+1), No clear effect (0), Harmful (-1); then aggregate and assign fixed mixing weights (e.g., 100K for general, 30K for category-specific).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied with FLAMe training tasks and RewardBench target evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Tail-patch procedure itself is automated fine-tuning; evaluation of impact uses RewardBench human-derived pairwise labels for assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>FLAMe-Opt-RM (optimized mixture via tail-patch) reaches 87.0% RewardBench using 25× fewer training datapoints versus full FLAMe baseline (5k vs 30k steps).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Tail-patch allowed more sample-efficient fine-tuning matching competitive RewardBench performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Rating thresholds and bundle weights were initially set by intuition and not exhaustively tuned; approach requires a target benchmark to optimize against.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e8042.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-to-text unified format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unified text-to-text task format (instruction + context -> evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standardized text-to-text representation for diverse quality assessment tasks where each example contains an INSTRUCTIONS block, CONTEXT block, and EVALUATION (target) block, enabling multitask instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2-24B -> FLAMe variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>24B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / NLP (data engineering for multitask learning)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Framework / data-format for training autoraters</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Unified text-to-text evaluation/task representation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Convert diverse human-evaluation datasets into a single text-to-text format with explicit INSTRUCTIONS, CONTEXT, and EVALUATION target to enable multitask supervised training and consistent prompting at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Downstream autorater generalization (measured via held-out benchmark accuracies like RewardBench, LLM-AggreFact, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported as accuracy or benchmark scores (percentages) on held-out evaluation tasks after training on unified-format data.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to FLAMe's curated collection of 102 training tasks (5.3M human judgments) and 53 held-out evaluation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human labels from original datasets preserved in conversion; authors spent ~3-4 hours per dataset reviewing and consulting authors to preserve annotation intent.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Training on unified text-to-text FLAMe collection produced models that outperform several baselines on 8/12 autorater benchmarks (e.g., RewardBench improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Conversion required manual crafting of task definitions and may not capture all original annotation nuances; relies on publicly licensed datasets only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e8042.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human judgments (FLAMe collection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curated human judgments (FLAMe collection of 5.3M labels across 102 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, standardized collection of over 5.3 million permissively licensed human judgments across 102 quality-assessment tasks used to train autoraters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used to train FLAMe variants (PaLM-2-24B finetuned to FLAMe-24B etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Training target models: 24B (PaLM-2-24B -> FLAMe-24B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / NLP (dataset for evaluation and training)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human-annotated dataset for training evaluation models</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human judgment supervision (pairwise, pointwise, classification, open-ended)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Aggregate human labels from many existing datasets, standardize them into a unified format, and use supervised multitask training to learn to predict human judgments across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Agreement with human labels (accuracy, correlation), downstream benchmark performance</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy = percent model predictions matching human labels; correlations for pointwise ratings; reported on held-out tasks as percent scores.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>FLAMe collection (released at huggingface.datasets/google/flame-collection) comprised of datasets like HelpSteer, PRM800K, CommitPack, SummEval, FActScore, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Curated from permissively licensed public datasets; included pairwise, pointwise (Likert), classification, and open-ended annotations; authors spent ~3-4 hours per dataset to standardize and consulted original authors.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Training on this human-judgment dataset produced FLAMe-RM-24B which achieved 87.8% on RewardBench and strong performance on other autorater benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>FLAMe models trained on human judgments match or exceed several proprietary LLM-as-a-Judge baselines on many benchmarks, indicating strong alignment to human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human judgments vary in standards/documentation across datasets; reliance on existing annotations can inherit biases present in source data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e8042.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanEval / pass@1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HumanEval (pass@1 coding metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A programming task benchmark (HumanEval) where code samples are executed against unit tests; pass@1 measures the fraction of first-ranked programs that pass all tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models trained on code</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeGen-16B, davinci-002, InCoder-6B (re-ranked by FLAMe variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (e.g., CodeGen-16B, davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Code generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Execution-based evaluation (functional correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>pass@1 on HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate multiple candidate programs per prompt, optionally re-rank using an autorater, then execute the top-ranked program against unit tests; pass@1 is the percent of prompts where the top program passes all tests.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>pass@1 (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Fraction (0–100%) of evaluated problems where the chosen top-ranked program succeeds on all unit tests.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>HumanEval (Chen et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>HumanEval is an automated execution-based benchmark; in FLAMe experiments authors re-ranked 10 code samples generated by each model using FLAMe and measured pass@1 improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Re-ranking with FLAMe-24B improved CodeGen-16B pass@1 from 21.2 to 31.1; similar gains seen for davinci-002 and InCoder-6B (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>FLAMe re-ranking closed a substantial fraction of the gap to Oracle ranking (Oracle pass@1 for CodeGen-16B was 46.9).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>HumanEval is synthetic and small-scale relative to production code workloads; improvements from re-ranking depend on diversity/quality of candidate samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8042.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e8042.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bias categories (CoBBLEr criteria)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Order / Compassion / Length / Egocentric / Bandwagon / Attention bias tests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of targeted bias criteria used to probe autoraters for systematic preferences: response order, naming/compassion effects, response length, self-preference (egocentric), influence by majority statements (bandwagon), and sensitivity to irrelevant context (attention).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking cognitive biases in large language models as evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAMe variants, GPT-4, other evaluated autoraters</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / NLP (behavioral bias evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Behavioral diagnostic tests for autoraters</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>CoBBLEr bias category tests</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Construct controlled evaluation pairs that isolate particular biases (e.g., swap response order, change model name alias, modify length) and measure the autorater's change in judgment to quantify bias per category.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-category bias scores and overall average bias (lower is better)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Biases reported as normalized scores (e.g., absolute deviation from unbiased baseline); aggregated into average bias value (e.g., FLAMe avg 0.13).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>CoBBLEr (Koo et al., 2023) re-formatted into FLAMe unified format for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>CoBBLEr uses synthetic/controlled pairs rather than human labels per se; FLAMe reformatted those pairs for automated autorater evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>FLAMe variants show substantially lower bias scores across categories compared to several baselines (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>FLAMe matched or outperformed GPT-4 on the six bias tests.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Bias probes are constrained by CoBBLEr scenarios; may not capture all real-world bias manifestations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Benchmarking cognitive biases in large language models as evaluators <em>(Rating: 2)</em></li>
                <li>Rewardbench: Evaluating reward models for language modeling <em>(Rating: 2)</em></li>
                <li>Minicheck: Efficient fact-checking of llms on grounding documents <em>(Rating: 2)</em></li>
                <li>BLEU: a method for automatic evaluation of machine translation <em>(Rating: 1)</em></li>
                <li>ROUGE: A package for automatic evaluation of summaries <em>(Rating: 1)</em></li>
                <li>Bertscore: Evaluating text generation with bert <em>(Rating: 1)</em></li>
                <li>Measuring the gap between neural text and human text using divergence frontiers <em>(Rating: 1)</em></li>
                <li>BLEURT: Learning robust metrics for text generation <em>(Rating: 1)</em></li>
                <li>FActScore: Fine-grained atomic evaluation of factual precision in long form text generation <em>(Rating: 1)</em></li>
                <li>QAGS: Question Answering-based Metric for Evaluating Factual Consistency <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8042",
    "paper_id": "paper-6c5ae8a0bf1885c2dd62c4787db4ac0f5159141f",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "RewardBench",
            "name_full": "RewardBench",
            "brief_description": "A benchmark of pairwise preference datasets designed to evaluate reward models and autoraters across categories (Chat, Chat Hard, Reasoning (Math+Coding), Safety), where models choose the preferred response between two options.",
            "citation_title": "Rewardbench: Evaluating reward models for language modeling",
            "mention_or_use": "use",
            "model_name": "FLAMe-RM-24B, FLAMe-24B, FLAMe-Opt-RM-24B, GPT-4-0125, GPT-4o, PaLM-2-24B, others",
            "model_size": "various (e.g., FLAMe-24B, GPT-4-0125, GPT-4o, PaLM-2-24B)",
            "scientific_domain": "Computer Science / NLP (LLM evaluation)",
            "theory_type": "Benchmark-based empirical evaluation of LLM preference models (reward models / autoraters)",
            "evaluation_method_name": "Pairwise preference evaluation (RewardBench)",
            "evaluation_method_description": "Models are given a prompt and two candidate responses and must select which response is preferred according to human judgments; aggregated accuracy across held-out pairwise comparisons and categories is reported.",
            "evaluation_metric": "Accuracy (% preferred-response matches human preference)",
            "metric_definition": "Percentage of pairwise queries for which the model selects the same response as the human preference label; reported as percent (0–100%).",
            "dataset_or_benchmark": "RewardBench (23 datasets across Chat, Chat Hard, Reasoning, Safety)",
            "human_evaluation_details": "RewardBench is constructed from human pairwise preference annotations across multiple underlying datasets; in FLAMe evaluations the full RewardBench set was used (not subsampled) and other held-out tasks were randomly sampled to 256 examples where applicable.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "FLAMe-RM-24B accuracy 87.8%, FLAMe-24B 86.0%, FLAMe-Opt-RM-24B 87.0%; GPT-4-0125 85.9%, GPT-4o 84.7% (reported as of July 15, 2024).",
            "comparison_to_human_generated": null,
            "comparison_results": "FLAMe-RM-24B outperforms several proprietary generative models on overall RewardBench accuracy (see reported_results).",
            "limitations_noted": "RewardBench exhibits length and token biases across categories (e.g., Chat favors longer outputs), making it possible to game the benchmark; authors caution against relying on a single benchmark.",
            "uuid": "e8042.0",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LLM-AggreFact",
            "name_full": "LLM-AggreFact (attribution benchmark)",
            "brief_description": "An aggregation-style benchmark integrating multiple attribution/factuality datasets to evaluate grounding and factual support of claims against source documents.",
            "citation_title": "Minicheck: Efficient fact-checking of llms on grounding documents",
            "mention_or_use": "use",
            "model_name": "FLAMe-24B, FLAMe-RM-24B, FLAMe-Opt-RM-24B, GPT-4-0125, GPT-4o, others",
            "model_size": "various",
            "scientific_domain": "Computer Science / NLP (factuality / attribution evaluation)",
            "theory_type": "Benchmark-based evaluation of factuality/attribution judgments",
            "evaluation_method_name": "Attribution / grounding evaluation (LLM-AggreFact)",
            "evaluation_method_description": "Autoraters determine whether claims (from LLM outputs) are fully supported by a provided source document; performance aggregated across multiple attribution datasets.",
            "evaluation_metric": "Accuracy / aggregated score (reported as numeric score)",
            "metric_definition": "Proportion of items where the autorater correctly judges attribution/factual support according to dataset labels; reported as a score (0–100 scale in paper's tables).",
            "dataset_or_benchmark": "LLM-AggreFact (aggregate of ~10 attribution datasets such as AggreFact, TofuEval, WiCE, ExpertQA, LFQA)",
            "human_evaluation_details": "Underlying datasets contain human annotations for claim support; FLAMe evaluates models on held-out attribution tasks; sampling of 256 examples per held-out task except where full set used (RewardBench).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "FLAMe-24B achieves overall 81.1 on LLM-AggreFact; GPT-4-0125 80.6 (Table 6).",
            "comparison_to_human_generated": null,
            "comparison_results": "FLAMe variants outperform other LLM-as-a-Judge baselines in three of four attribution use-cases (LLM-FactVerify, Wiki-FactVerify, Summarization).",
            "limitations_noted": "Some long-form QA attribution categories remain challenging; FLAMe underperforms GPT-4 in Long-form QA attribution.",
            "uuid": "e8042.1",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "CoBBLEr",
            "name_full": "CoBBLEr (autorater bias benchmark)",
            "brief_description": "A benchmark designed to quantify cognitive and operational biases in LLM autoraters across several controlled bias scenarios (Order, Compassion, Length, Egocentric, Bandwagon, Attention).",
            "citation_title": "Benchmarking cognitive biases in large language models as evaluators",
            "mention_or_use": "use",
            "model_name": "FLAMe-24B, FLAMe-RM-24B, FLAMe-Opt-RM-24B, GPT-4, etc.",
            "model_size": "various",
            "scientific_domain": "Computer Science / NLP (bias evaluation)",
            "theory_type": "Bias-detection evaluation (behavioral tests for autoraters)",
            "evaluation_method_name": "CoBBLEr bias tests",
            "evaluation_method_description": "Apply controlled perturbations to evaluation prompts/responses to measure bias types (e.g., preference for response order or length); compute bias magnitudes per category and average bias.",
            "evaluation_metric": "Bias scores per category (lower is better); overall avg bias",
            "metric_definition": "Normalized bias measures reported per category (e.g., absolute deviations or proportions); aggregated into an average bias score (e.g., FLAMe avg bias 0.13 vs GPT-4 0.31).",
            "dataset_or_benchmark": "CoBBLEr benchmark (Koo et al., 2023)",
            "human_evaluation_details": "Original CoBBLEr uses controlled synthetic prompt/response pairs; in this paper, pairs were reformatted into FLAMe's text-to-text format for evaluation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "FLAMe-24B avg bias 0.13 (lower is better) vs GPT-4 0.31; FLAMe matches or outperforms GPT-4 across six bias categories (Table 3).",
            "comparison_to_human_generated": null,
            "comparison_results": "FLAMe variants exhibit significantly less bias than several popular autoraters including GPT-4.",
            "limitations_noted": "Bias scores depend on CoBBLEr's controlled scenarios; real-world bias profiles may differ.",
            "uuid": "e8042.2",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Pairwise Evaluation",
            "name_full": "Pairwise preference evaluation",
            "brief_description": "A method where annotators compare two candidate responses and indicate which is better according to a given criterion (e.g., helpfulness), widely used for reward-model training and assessment.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Models producing candidate responses evaluated by autoraters (e.g., PaLM-2-24B outputs, others)",
            "model_size": "various",
            "scientific_domain": "Computer Science / NLP (evaluation methodology)",
            "theory_type": "Human-annotation-based evaluation (preferences)",
            "evaluation_method_name": "Pairwise preference judgments",
            "evaluation_method_description": "Human annotators (or pairwise datasets) present two responses for the same prompt and label which is preferable; used in training (RMs) and as evaluation labels for autoraters.",
            "evaluation_metric": "Preference accuracy / agreement with human labels",
            "metric_definition": "Proportion of pairwise comparisons where the model's choice matches the human label; often reported as percent accuracy.",
            "dataset_or_benchmark": "Many underlying datasets (e.g., HelpSteer, PRM800K, CommitPack, HH RLHF datasets) and RewardBench which aggregates pairwise tasks",
            "human_evaluation_details": "FLAMe collection includes &gt;5.3M human judgments with a large fraction being pairwise comparisons; pairwise data used both in training and evaluation (some evaluation tasks sampled to 256 examples).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Pairwise-based evaluations drive RewardBench and other reported accuracies; FLAMe-RM fine-tuned on pairwise mixtures increased RewardBench accuracy from 86.0% to 87.8%.",
            "comparison_to_human_generated": null,
            "comparison_results": "Training on curated pairwise human judgments improved autorater agreement with human preferences versus baseline instruction-tuned models.",
            "limitations_noted": "Pairwise data can be costly to collect and may encode inconsistent annotation standards across datasets.",
            "uuid": "e8042.3",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Pointwise Evaluation",
            "name_full": "Pointwise evaluation (rating individual responses)",
            "brief_description": "Annotators rate individual model outputs on attribute scales (e.g., Likert for coherence or helpfulness) rather than comparing two outputs directly.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FLAMe training/evaluation uses pointwise data from summarization and other tasks; evaluated models vary",
            "model_size": "various",
            "scientific_domain": "Computer Science / NLP (evaluation methodology)",
            "theory_type": "Human-annotation-based attribute scoring",
            "evaluation_method_name": "Pointwise rating (Likert scale / attribute scores)",
            "evaluation_method_description": "Human annotators assign scores to a single response along predefined scales (e.g., 1–5 Likert for coherence); models are trained to predict these scores.",
            "evaluation_metric": "Correlation with human ratings; accuracy within discrete categories; aggregated scores",
            "metric_definition": "Metrics vary by dataset: e.g., Likert 1–5 scales for pointwise data; evaluation often uses Spearman/Pearson correlation or categorical accuracy when discretized.",
            "dataset_or_benchmark": "SummEval, LENS, some SummFeedback pointwise splits, and other summarization/instruction datasets in FLAMe",
            "human_evaluation_details": "FLAMe includes pointwise annotations (percentage of 5.3M); during evaluation some pointwise labels were reserved (e.g., SummFeedback) while pairwise used in training.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "FLAMe shows improvements on pointwise tasks (e.g., SummFeedback scores increased versus baselines in Table 1).",
            "comparison_to_human_generated": null,
            "comparison_results": "Not a direct comparison to human-generated theories; pointwise labels used as training/eval supervision.",
            "limitations_noted": "Pointwise ratings are subjective, may have inter-rater variability, and differ across original dataset annotation protocols.",
            "uuid": "e8042.4",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Likert Scale",
            "name_full": "Likert scale ratings (e.g., 1–5)",
            "brief_description": "A common pointwise human rating scale used to quantify attributes like coherence, helpfulness, or fluency of a single response.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Used in datasets evaluated/trained on by FLAMe (e.g., SummEval, SummFeedback)",
            "model_size": "n/a",
            "scientific_domain": "Human evaluation methodology (NLP)",
            "theory_type": "Quantitative human annotation protocol",
            "evaluation_method_name": "Likert-scale human ratings",
            "evaluation_method_description": "Annotators assign discrete ordinal scores (e.g., 1–5) to responses for specified attributes; models learn to predict or match these ratings.",
            "evaluation_metric": "Average rating, correlation (Spearman/Pearson) with human mean scores, categorical accuracy",
            "metric_definition": "Scores on an ordinal scale (commonly 1–5); aggregated means and correlations reported.",
            "dataset_or_benchmark": "SummEval, SummFeedback, HelpSteer attributes, others in FLAMe",
            "human_evaluation_details": "FLAMe curators preserved original rating scales when converting tasks to a unified text-to-text format; many datasets included Likert ratings.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Used as supervision for training and as held-out evaluation; FLAMe improvements reported on tasks that had pointwise Likert labels.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Likert scales can suffer from inter-annotator inconsistency and differences in instruction phrasing across datasets.",
            "uuid": "e8042.5",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "BLEU",
            "name_full": "BLEU (Bilingual Evaluation Understudy)",
            "brief_description": "A lexical-overlap automatic metric for machine translation and text generation that measures n-gram overlap between candidate and reference texts.",
            "citation_title": "BLEU: a method for automatic evaluation of machine translation",
            "mention_or_use": "mention",
            "model_name": "N/A (automatic metric applied to model outputs in prior literature)",
            "model_size": "N/A",
            "scientific_domain": "NLP evaluation metrics",
            "theory_type": "Automatic metric",
            "evaluation_method_name": "BLEU",
            "evaluation_method_description": "Compute n-gram precision between candidate and reference texts with brevity penalty to yield a score reflecting surface overlap.",
            "evaluation_metric": "BLEU score (0–100 typically reported as 0–1 or as percentage)",
            "metric_definition": "Weighted geometric mean of n-gram precisions with brevity penalty; often reported on 0–100 scale or 0–1.",
            "dataset_or_benchmark": "Standard MT and generation datasets cited in related work (Papineni et al., 2002)",
            "human_evaluation_details": "BLEU is an automatic metric and does not involve human annotators; mentioned as traditional baseline in related work.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Mentioned in Related Work as a classical lexical-overlap metric; not used in FLAMe experiments.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Focuses on lexical overlap and often correlates poorly with human judgments for open-ended generation.",
            "uuid": "e8042.6",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ROUGE",
            "name_full": "ROUGE (Recall-Oriented Understudy for Gisting Evaluation)",
            "brief_description": "A set of recall-based n-gram overlap metrics widely used in summarization to measure similarity between system summaries and reference summaries.",
            "citation_title": "ROUGE: A package for automatic evaluation of summaries",
            "mention_or_use": "mention",
            "model_name": "N/A (automatic metric referenced in related work)",
            "model_size": "N/A",
            "scientific_domain": "NLP evaluation metrics",
            "theory_type": "Automatic metric",
            "evaluation_method_name": "ROUGE",
            "evaluation_method_description": "Compute overlaps (e.g., n-gram, longest common subsequence) between candidate and reference texts; commonly used to evaluate summarization quality.",
            "evaluation_metric": "ROUGE-N, ROUGE-L scores (commonly reported as F1 or recall percentages)",
            "metric_definition": "Overlap-derived scores (0–100% or 0–1) for n-grams or sequence similarity.",
            "dataset_or_benchmark": "Summarization datasets (e.g., XSum) referenced in related work",
            "human_evaluation_details": "Automatic metric; mentioned as part of the historical toolkit for evaluating generated summaries.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Mentioned in Related Work; FLAMe instead trains on human judgments and diverse tasks.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "ROUGE correlates imperfectly with human judgments, especially for abstractive summaries.",
            "uuid": "e8042.7",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "BERTScore",
            "name_full": "BERTScore",
            "brief_description": "An automatic evaluation metric that uses contextual embeddings (BERT) to compute token-level similarity between candidate and reference texts, yielding a precision/recall/F1-like score.",
            "citation_title": "Bertscore: Evaluating text generation with bert",
            "mention_or_use": "mention",
            "model_name": "N/A (automatic metric referenced in related work)",
            "model_size": "N/A",
            "scientific_domain": "NLP evaluation metrics",
            "theory_type": "Automatic metric",
            "evaluation_method_name": "BERTScore",
            "evaluation_method_description": "Compute pairwise cosine similarities between token embeddings from a pretrained transformer and aggregate to produce precision, recall and F1 similarity scores.",
            "evaluation_metric": "BERTScore P/R/F1 (typically reported 0–1 or 0–100)",
            "metric_definition": "Aggregated embedding-similarity based precision/recall/F1 between candidate and reference tokens.",
            "dataset_or_benchmark": "Referenced in Related Work as an embedding-based metric",
            "human_evaluation_details": "Automatic; not directly tied to FLAMe experiments.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Mentioned in Related Work; not used as primary evaluation in this paper.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Captures semantic similarity better than lexical overlap metrics but may miss fine-grained human preferences.",
            "uuid": "e8042.8",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "MAUVE",
            "name_full": "MAUVE",
            "brief_description": "An automatic metric measuring divergence between the distributions of human text and model-generated text using divergence frontiers.",
            "citation_title": "Measuring the gap between neural text and human text using divergence frontiers",
            "mention_or_use": "mention",
            "model_name": "N/A (metric referenced in FLAMe training dataset list)",
            "model_size": "N/A",
            "scientific_domain": "NLP evaluation metrics",
            "theory_type": "Automatic distributional evaluation metric",
            "evaluation_method_name": "MAUVE",
            "evaluation_method_description": "Measure distributional divergence between model outputs and human text across embedding spaces to assess how human-like generated text is.",
            "evaluation_metric": "MAUVE score (0–1 or scaled), higher indicating closer alignment to human text distribution",
            "metric_definition": "Area under the divergence frontier curve comparing model and human distributions; often normalized.",
            "dataset_or_benchmark": "MAUVE dataset/metric referenced among FLAMe's generalized quality datasets",
            "human_evaluation_details": "Automatic metric; included among datasets the authors used (MAUVE entries appear in Table 5).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Included in FLAMe's dataset collection but not reported as a central evaluation metric in main results.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Distributional metrics can mask specific errors or factuality issues important to humans.",
            "uuid": "e8042.9",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "BLEURT / COMET",
            "name_full": "BLEURT and COMET (learned evaluation metrics)",
            "brief_description": "Learned automatic metrics trained to predict human quality ratings for generated text (BLEURT for generation; COMET for MT evaluation), using supervised regression/classification on human judgments.",
            "citation_title": "BLEURT: Learning robust metrics for text generation",
            "mention_or_use": "mention",
            "model_name": "N/A (automatic learned metrics cited in Related Work)",
            "model_size": "N/A",
            "scientific_domain": "NLP evaluation metrics",
            "theory_type": "Learned automatic evaluation metric",
            "evaluation_method_name": "BLEURT / COMET",
            "evaluation_method_description": "Train supervised models to predict human ratings of text quality using annotated datasets; use predicted scores to evaluate generation systems.",
            "evaluation_metric": "BLEURT/COMET predicted score (continuous), correlation with human ratings often reported",
            "metric_definition": "Model outputs a scalar aligned with human judgments; higher indicates higher predicted quality. Correlations (Pearson/Spearman) commonly reported.",
            "dataset_or_benchmark": "Training corpora of human ratings for generation/MT tasks (e.g., WMT, SummEval); cited in related work.",
            "human_evaluation_details": "These metrics are trained on human judgments; FLAMe differs by training autoraters directly on diverse human evaluation tasks.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Mentioned in Related Work; not central to FLAMe experiments.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Learned metrics risk overfitting training annotation styles and may not generalize across domains.",
            "uuid": "e8042.10",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "QAGS / Q^2",
            "name_full": "QAGS and Q^2 (question-answering based factuality checks)",
            "brief_description": "Methods that evaluate factual consistency by generating questions from claims and using QA systems to verify supporting evidence (QAGS / Q^2 approaches).",
            "citation_title": "QAGS: Question Answering-based Metric for Evaluating Factual Consistency",
            "mention_or_use": "mention",
            "model_name": "N/A (automatic factuality assessment approaches referenced in related work)",
            "model_size": "N/A",
            "scientific_domain": "NLP factuality evaluation",
            "theory_type": "Automatic factuality-checking framework",
            "evaluation_method_name": "Question-generation plus QA (QAGS / Q^2)",
            "evaluation_method_description": "Generate questions from a summary/claim then answer them using sources; compare answers to determine whether the content is supported by source documents.",
            "evaluation_metric": "Factuality score (e.g., proportion of QA pairs supported)",
            "metric_definition": "Fraction or percentage of generated Q&A pairs where the answer from the source matches the claim-derived answer; reported as 0–100%.",
            "dataset_or_benchmark": "Used in prior factuality datasets (e.g., QAGS referenced in FLAMe's factuality dataset list)",
            "human_evaluation_details": "QAGS is an automatic approach but often validated against human judgments in prior work.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Mentioned as part of the landscape of factuality metrics; FLAMe trains on human-labelled factuality datasets instead of relying solely on QAGS-style pipelines.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "QA-based checks depend on QA system performance and question generation quality; may miss nuanced attribution issues.",
            "uuid": "e8042.11",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "FActScore",
            "name_full": "FActScore (Fine-grained atomic evaluation of factual precision)",
            "brief_description": "A fine-grained factuality evaluation approach/dataset that assesses atomic claim-level factual precision in generated text.",
            "citation_title": "FActScore: Fine-grained atomic evaluation of factual precision in long form text generation",
            "mention_or_use": "mention",
            "model_name": "N/A (dataset/method referenced among factuality resources)",
            "model_size": "N/A",
            "scientific_domain": "NLP factuality evaluation",
            "theory_type": "Dataset / evaluation method for factual precision",
            "evaluation_method_name": "FActScore",
            "evaluation_method_description": "Annotate and evaluate atomic factual claims within generated text for precision and grounding to sources; used to measure hallucination rates.",
            "evaluation_metric": "Atomic claim-level factuality scores (precision/recall or percent of claims supported)",
            "metric_definition": "Proportion of annotated atomic claims deemed factually supported by source documents; typically 0–100% or fraction.",
            "dataset_or_benchmark": "FActScore dataset (Min et al., 2023) included among FLAMe's sources",
            "human_evaluation_details": "FActScore is human-annotated at the claim level; FLAMe included such datasets in its training collection for factuality.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Used as training data for FLAMe's factuality/attribution capabilities; not reported as a standalone metric in main result tables.",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Fine-grained annotation is labor-intensive and definitions of 'atomic claim' can vary across annotators.",
            "uuid": "e8042.12",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Tail-patch fine-tuning",
            "name_full": "Tail-patch fine-tuning / tail-patch ablations",
            "brief_description": "A lightweight ablation/fine-tuning procedure where a partially trained multitask checkpoint is briefly fine-tuned separately on each task to assess per-task impact, enabling re-weighting of the multitask mixture.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2-24B initialized FLAMe checkpoints",
            "model_size": "24B (PaLM-2-24B / FLAMe-24B variants)",
            "scientific_domain": "Computer Science / Machine Learning (multitask optimization)",
            "theory_type": "Optimization / ablation method for multitask mixture weights",
            "evaluation_method_name": "Tail-patch ablation and re-weighted mixture optimization",
            "evaluation_method_description": "From a partially trained checkpoint, fine-tune briefly (tail-patch) on each individual training task to measure its effect on target downstream performance (e.g., RewardBench), then group tasks and assign new mixture weights to optimize sample-efficiency.",
            "evaluation_metric": "Downstream target benchmark performance (e.g., RewardBench Chat Hard score) used to rate task impact; bundle weights set accordingly",
            "metric_definition": "Categorical ratings per task: Helpful (+2), Somewhat helpful (+1), No clear effect (0), Harmful (-1); then aggregate and assign fixed mixing weights (e.g., 100K for general, 30K for category-specific).",
            "dataset_or_benchmark": "Applied with FLAMe training tasks and RewardBench target evaluation",
            "human_evaluation_details": "Tail-patch procedure itself is automated fine-tuning; evaluation of impact uses RewardBench human-derived pairwise labels for assessment.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "FLAMe-Opt-RM (optimized mixture via tail-patch) reaches 87.0% RewardBench using 25× fewer training datapoints versus full FLAMe baseline (5k vs 30k steps).",
            "comparison_to_human_generated": null,
            "comparison_results": "Tail-patch allowed more sample-efficient fine-tuning matching competitive RewardBench performance.",
            "limitations_noted": "Rating thresholds and bundle weights were initially set by intuition and not exhaustively tuned; approach requires a target benchmark to optimize against.",
            "uuid": "e8042.13",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Text-to-text unified format",
            "name_full": "Unified text-to-text task format (instruction + context -&gt; evaluation)",
            "brief_description": "A standardized text-to-text representation for diverse quality assessment tasks where each example contains an INSTRUCTIONS block, CONTEXT block, and EVALUATION (target) block, enabling multitask instruction tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2-24B -&gt; FLAMe variants",
            "model_size": "24B",
            "scientific_domain": "Computer Science / NLP (data engineering for multitask learning)",
            "theory_type": "Framework / data-format for training autoraters",
            "evaluation_method_name": "Unified text-to-text evaluation/task representation",
            "evaluation_method_description": "Convert diverse human-evaluation datasets into a single text-to-text format with explicit INSTRUCTIONS, CONTEXT, and EVALUATION target to enable multitask supervised training and consistent prompting at inference.",
            "evaluation_metric": "Downstream autorater generalization (measured via held-out benchmark accuracies like RewardBench, LLM-AggreFact, etc.)",
            "metric_definition": "Reported as accuracy or benchmark scores (percentages) on held-out evaluation tasks after training on unified-format data.",
            "dataset_or_benchmark": "Applied to FLAMe's curated collection of 102 training tasks (5.3M human judgments) and 53 held-out evaluation tasks",
            "human_evaluation_details": "Human labels from original datasets preserved in conversion; authors spent ~3-4 hours per dataset reviewing and consulting authors to preserve annotation intent.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Training on unified text-to-text FLAMe collection produced models that outperform several baselines on 8/12 autorater benchmarks (e.g., RewardBench improvements).",
            "comparison_to_human_generated": null,
            "comparison_results": null,
            "limitations_noted": "Conversion required manual crafting of task definitions and may not capture all original annotation nuances; relies on publicly licensed datasets only.",
            "uuid": "e8042.14",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Human judgments (FLAMe collection)",
            "name_full": "Curated human judgments (FLAMe collection of 5.3M labels across 102 tasks)",
            "brief_description": "A large, standardized collection of over 5.3 million permissively licensed human judgments across 102 quality-assessment tasks used to train autoraters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used to train FLAMe variants (PaLM-2-24B finetuned to FLAMe-24B etc.)",
            "model_size": "Training target models: 24B (PaLM-2-24B -&gt; FLAMe-24B variants)",
            "scientific_domain": "Computer Science / NLP (dataset for evaluation and training)",
            "theory_type": "Human-annotated dataset for training evaluation models",
            "evaluation_method_name": "Human judgment supervision (pairwise, pointwise, classification, open-ended)",
            "evaluation_method_description": "Aggregate human labels from many existing datasets, standardize them into a unified format, and use supervised multitask training to learn to predict human judgments across tasks.",
            "evaluation_metric": "Agreement with human labels (accuracy, correlation), downstream benchmark performance",
            "metric_definition": "Accuracy = percent model predictions matching human labels; correlations for pointwise ratings; reported on held-out tasks as percent scores.",
            "dataset_or_benchmark": "FLAMe collection (released at huggingface.datasets/google/flame-collection) comprised of datasets like HelpSteer, PRM800K, CommitPack, SummEval, FActScore, etc.",
            "human_evaluation_details": "Curated from permissively licensed public datasets; included pairwise, pointwise (Likert), classification, and open-ended annotations; authors spent ~3-4 hours per dataset to standardize and consulted original authors.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Training on this human-judgment dataset produced FLAMe-RM-24B which achieved 87.8% on RewardBench and strong performance on other autorater benchmarks.",
            "comparison_to_human_generated": null,
            "comparison_results": "FLAMe models trained on human judgments match or exceed several proprietary LLM-as-a-Judge baselines on many benchmarks, indicating strong alignment to human labels.",
            "limitations_noted": "Human judgments vary in standards/documentation across datasets; reliance on existing annotations can inherit biases present in source data.",
            "uuid": "e8042.15",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "HumanEval / pass@1",
            "name_full": "HumanEval (pass@1 coding metric)",
            "brief_description": "A programming task benchmark (HumanEval) where code samples are executed against unit tests; pass@1 measures the fraction of first-ranked programs that pass all tests.",
            "citation_title": "Evaluating large language models trained on code",
            "mention_or_use": "use",
            "model_name": "CodeGen-16B, davinci-002, InCoder-6B (re-ranked by FLAMe variants)",
            "model_size": "various (e.g., CodeGen-16B, davinci-002)",
            "scientific_domain": "Computer Science / Code generation evaluation",
            "theory_type": "Execution-based evaluation (functional correctness)",
            "evaluation_method_name": "pass@1 on HumanEval",
            "evaluation_method_description": "Generate multiple candidate programs per prompt, optionally re-rank using an autorater, then execute the top-ranked program against unit tests; pass@1 is the percent of prompts where the top program passes all tests.",
            "evaluation_metric": "pass@1 (percentage)",
            "metric_definition": "Fraction (0–100%) of evaluated problems where the chosen top-ranked program succeeds on all unit tests.",
            "dataset_or_benchmark": "HumanEval (Chen et al., 2021)",
            "human_evaluation_details": "HumanEval is an automated execution-based benchmark; in FLAMe experiments authors re-ranked 10 code samples generated by each model using FLAMe and measured pass@1 improvements.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "Re-ranking with FLAMe-24B improved CodeGen-16B pass@1 from 21.2 to 31.1; similar gains seen for davinci-002 and InCoder-6B (Table 7).",
            "comparison_to_human_generated": null,
            "comparison_results": "FLAMe re-ranking closed a substantial fraction of the gap to Oracle ranking (Oracle pass@1 for CodeGen-16B was 46.9).",
            "limitations_noted": "HumanEval is synthetic and small-scale relative to production code workloads; improvements from re-ranking depend on diversity/quality of candidate samples.",
            "uuid": "e8042.16",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Bias categories (CoBBLEr criteria)",
            "name_full": "Order / Compassion / Length / Egocentric / Bandwagon / Attention bias tests",
            "brief_description": "A set of targeted bias criteria used to probe autoraters for systematic preferences: response order, naming/compassion effects, response length, self-preference (egocentric), influence by majority statements (bandwagon), and sensitivity to irrelevant context (attention).",
            "citation_title": "Benchmarking cognitive biases in large language models as evaluators",
            "mention_or_use": "use",
            "model_name": "FLAMe variants, GPT-4, other evaluated autoraters",
            "model_size": "various",
            "scientific_domain": "Computer Science / NLP (behavioral bias evaluation)",
            "theory_type": "Behavioral diagnostic tests for autoraters",
            "evaluation_method_name": "CoBBLEr bias category tests",
            "evaluation_method_description": "Construct controlled evaluation pairs that isolate particular biases (e.g., swap response order, change model name alias, modify length) and measure the autorater's change in judgment to quantify bias per category.",
            "evaluation_metric": "Per-category bias scores and overall average bias (lower is better)",
            "metric_definition": "Biases reported as normalized scores (e.g., absolute deviation from unbiased baseline); aggregated into average bias value (e.g., FLAMe avg 0.13).",
            "dataset_or_benchmark": "CoBBLEr (Koo et al., 2023) re-formatted into FLAMe unified format for evaluation",
            "human_evaluation_details": "CoBBLEr uses synthetic/controlled pairs rather than human labels per se; FLAMe reformatted those pairs for automated autorater evaluation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": null,
            "reported_results": "FLAMe variants show substantially lower bias scores across categories compared to several baselines (Table 3).",
            "comparison_to_human_generated": null,
            "comparison_results": "FLAMe matched or outperformed GPT-4 on the six bias tests.",
            "limitations_noted": "Bias probes are constrained by CoBBLEr scenarios; may not capture all real-world bias manifestations.",
            "uuid": "e8042.17",
            "source_info": {
                "paper_title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Benchmarking cognitive biases in large language models as evaluators",
            "rating": 2
        },
        {
            "paper_title": "Rewardbench: Evaluating reward models for language modeling",
            "rating": 2
        },
        {
            "paper_title": "Minicheck: Efficient fact-checking of llms on grounding documents",
            "rating": 2
        },
        {
            "paper_title": "BLEU: a method for automatic evaluation of machine translation",
            "rating": 1
        },
        {
            "paper_title": "ROUGE: A package for automatic evaluation of summaries",
            "rating": 1
        },
        {
            "paper_title": "Bertscore: Evaluating text generation with bert",
            "rating": 1
        },
        {
            "paper_title": "Measuring the gap between neural text and human text using divergence frontiers",
            "rating": 1
        },
        {
            "paper_title": "BLEURT: Learning robust metrics for text generation",
            "rating": 1
        },
        {
            "paper_title": "FActScore: Fine-grained atomic evaluation of factual precision in long form text generation",
            "rating": 1
        },
        {
            "paper_title": "QAGS: Question Answering-based Metric for Evaluating Factual Consistency",
            "rating": 1
        }
    ],
    "cost": 0.02770575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Foundational Autoraters: <br> Taming Large Language Models for Better Automatic Evaluation</h1>
<p>Tu Vu ${ }^{\text {® }}{ }^{<em>}$ Kalpesh Krishna ${ }^{\text {® }}{ }^{</em>}$ Salaheddin Alzubi ${ }^{\dagger}$<br>Chris Tar ${ }^{\text {® }}{ }^{\ddagger}$ Manaal Faruqui ${ }^{\circledR}{ }^{\ddagger}$ Yun-Hsuan Sung ${ }^{\text {® }}{ }^{\ddagger}$<br>${ }^{\text {A}}$ Google DeepMind, ${ }^{\circ}$ Google, ${ }^{\circ}$ Virginia Tech<br>{ttvu,kalpeshk}@google.com</p>
<h4>Abstract</h4>
<p>As large language models (LLMs) evolve, evaluating their output reliably becomes increasingly difficult due to the high cost of human evaluation. To address this, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on a diverse set of over 100 quality assessment tasks, incorporating 5M+ human judgments curated from publicly released human evaluations. FLAMe outperforms models like GPT-4 and Claude-3 on various held-out tasks, and serves as a powerful starting point for finetuning, as shown in our reward model evaluation case study (FLAMe-RM). On RewardBench, FLAMe-RM-24B achieves $87.8 \%$ accuracy, surpassing GPT-4-0125 ( $85.9 \%$ ) and GPT-4o ( $84.7 \%$ ). Additionally, we introduce FLAMe-Opt-RM, an efficient tail-patch finetuning approach that offers competitive RewardBench performance using $25 \times$ fewer training datapoints. Our FLAMe variants outperform popular proprietary LLM-as-a-Judge models on 8 of 12 autorater benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis shows that FLAMe is significantly less biased than other LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>The growing capabilities of large language models (LLMs) present a key challenge: How can we reliably evaluate their long-form responses? A promising approach is to use the models themselves as autoraters. After large-scale multitask instruction tuning, LLMs can generalize to follow new human instructions (Wei et al., 2022; Sanh et al., 2022;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Longpre et al., 2023; Chung et al., 2024), making them suitable for this task. This is appealing because human evaluation, while essential, is limited by subjectivity (Krishna et al., 2023a), inconsistency among raters (Karpinska et al., 2021), and the high costs of extensive evaluations (Min et al., 2023; Vu et al., 2023; Wei et al., 2024).</p>
<p>Training LLM autoraters on human judgments is essential for aligning them with human preferences (Ouyang et al., 2022). However, gathering these judgments is both costly and time-consuming. Reusing human evaluations from prior research is a promising approach, yet it faces challenges such as inconsistent standards, diverse criteria, inadequate documentation, and privacy or proprietary concerns. On the other hand, training autoraters on model outputs offers consistency (Jiang et al., 2024b; Kim et al., 2024b) but risks reinforcing biases and hallucinations (Gudibande et al., 2023; Muennighoff et al., 2023) and may also breach proprietary LLM service terms. ${ }^{2}$</p>
<p>To address these limitations, we curated and standardized human evaluations from prior research to create FLAMe, a collection of 102 quality assessment tasks comprising more than 5.3 M total human judgments (§3). FLAMe spans a wide variety of task types, from assessing summarization quality to evaluating how well AI assistants follow user instructions. We hypothesized that training on this large and diverse data collection would enable LLM autoraters to learn robust, generalized patterns of human judgment, minimizing the impact of noisy or low-quality human judgments.</p>
<p>For transparency and reproducibility, we use only publicly available human evaluation data with permissive licenses from previous studies (§3.2). To address challenges due to the lack of standardization and documentation, we thoroughly exam-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our FLAMe-24B variants outperform popular proprietary LLM-as-a-Judge models like GPT-4 and Claude-3 on various autorater benchmarks, including RewardBench. As of July 15, 2024, FLAMe-RM, with an overall accuracy of 87.8%, was the top-performing generative model trained exclusively on permissively licensed data on RewardBench, surpassing GPT-4-0125 (85.9%) and GPT-4o (84.7%).</p>
<p>Ined the associated research and consulted the original authors to clarify ambiguities or inconsistencies, spending 3-4 hours per dataset. Inspired by T5 (Raffel et al., 2020), we unify all tasks into a <em>text-to-text</em> format, with manually crafted task definitions and evaluation instructions. This simple and adaptable data format facilitates effective transfer learning, allowing our models to interpret and respond consistently to various tasks (Figure 2).</p>
<p>Our approach can be viewed as developing general-purpose LLM autoraters for various quality assessment tasks. We show that training an instruction-tuned LLM, PaLM-2-24B (Anil et al., 2023), on our FLAMe collection improves zeroshot generalization to a wide range of held-out tasks, outperforming models like GPT-4, Claude-3, and Llama-3 on many tasks. This demonstrates that our large-scale multitask instruction tuning enhances the model's general-purpose quality assessment capabilities.</p>
<p>Motivated by these results, we explore FLAMe's effectiveness as a powerful starting point for finetuning on targeted downstream applications, using reward model evaluation on RewardBench (Lambert et al., 2024) as a case study (FLAMe-RM). Specifically, we slightly fine-tune FLAMe on a mixture of four datasets with human pairwise preference judgments, covering chat, reasoning, and safety. The resulting FLAMe-RM-24B model achieves a notable performance boost on RewardBench, reaching an accuracy of 87.8% (up from 86.0%). As of July 15, 2024, it was <em>the top-performing generative model trained solely on permissively licensed data</em>, outperforming GPT-4-0125 (85.9%) and GPT-4o (84.7%); see Figure 1.</p>
<p>Additionally, we present FLAMe-Opt-RM, a computationally efficient method for optimizing our FLAMe multitask mixture for targeted reward model evaluation on RewardBench. Using a novel <em>tail-patch fine-tuning</em> technique, we evaluate the impact of each dataset on specific RewardBench distributions, enabling us to determine the optimal dataset proportions for our mixture. Finetuning the initial instruction-tuned PaLM-2-24B on this optimized mixture yields competitive RewardBench performance (87.0%) compared to FLAMe (86.0%), using 25× fewer training datapoints.</p>
<p>Overall, our FLAMe variants outperform all popular proprietary LLM-as-a-Judge models we consider on 8 out of 12 autorater evaluation benchmarks (1 held-in and 11 held-out), covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact (Tang et al., 2024). Finally, our analysis shows that FLAMe variants are significantly less biased than other popular LLM-as-a-Judge autoraters on the CoBBLEr bias benchmark (Koo et al., 2023), demonstrating greater robustness to changes in pairwise ordering, response length, and irrelevant context.</p>
<p>In summary, our main contributions are: 1) <strong>Data Collection</strong>: We curated and standardized human evaluations from permissively licensed datasets, creating a collection of over 100 diverse quality assessment tasks with 5M+ human judgments. To facilitate future research, we release our data collection at https://huggingface.co/datasets/google/flame-collection; 2) <strong>LLM Autoraters</strong>: We show that our data collection can be used for training general-purpose LLM autoraters (FLAMe) and optimizing them for specific applications (FLAMe-RM and FLAMe-Opt-RM). Our models outperform popular proprietary LLM-as-a-Judge models on 8 out of 12 autorater benchmarks, covering 53 tasks, including RewardBench and LLM-AggreFact; and 3) <strong>Computationally Efficient Multitask Training</strong>: We propose a tail-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: We unify all quality assessment tasks into a text-to-text format, with manually crafted task definitions and evaluation instructions. Each training example consists of an input-target pair: the input provides task-specific context, while the target contains the expected human evaluation. This format can be easily adapted to novel tasks.
patch fine-tuning method that optimizes our multitask mixture for specific distributions, achieving competitive performance with significantly reduced compute.</p>
<h2>2 Related Work</h2>
<p>Below, we discuss existing literature in the space of autoraters, drawing connections to FLAMe.</p>
<p>Automatic Evaluation Metrics: Traditional metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) focus on lexical overlap between model output and human references. In the BERT era (Devlin et al., 2019), newer methods use pretrained models to measure distributional similarity (Zhao et al., 2019; Zhang et al., 2020) or token probabilities (Thompson and Post, 2020; Yuan et al., 2021). Several approaches assess divergence between text distributions (Gehrmann et al., 2019; Pillutla et al., 2021). Other work fine-tunes models on human ratings for specific tasks like machine translation (Sellam et al., 2020; Rei et al., 2020; Fernandes et al., 2023), summarization (Durmus et al., 2020; Deutsch et al., 2021; Goyal and Durrett,
2021), and QA (Chen et al., 2020; Lin et al., 2022). Unlike task-specific metrics, FLAMe is trained on diverse quality assessment tasks and can adapt to new tasks during inference.</p>
<p>LLM-as-a-Judge Autoraters: Prior work has used LLMs as judges to assess LLM capabilities on various benchmarks (Liu et al., 2023a; Fu et al., 2024; Bai et al., 2023; Wang et al., 2023a; Chiang et al., 2023; Chiang and Lee, 2023; Bubeck et al., 2023). However, these models tend to favor their own generated responses (Liu et al., 2023a; Panickssery et al., 2024; Liu et al., 2023b; Bai et al., 2023), showing biases toward factors like length, order, and entity preference (Koo et al., 2023). In contrast, FLAMe is trained on a broad range of human evaluations, enabling it to learn unbiased, generalized patterns of human judgment (§6.1). Additionally, FLAMe is not tasked with evaluating its own responses, avoiding self-preference bias.</p>
<p>Recent work has also trained general-purpose LLM autoraters. Jiang et al. (2024b) introduce TIGERScore, a Llama-2 model trained on GPT-4-generated error analysis data. Similar methods include InstructScore (Xu et al., 2023b),</p>
<p>Prometheus (Kim et al., 2024a), and Prometheus2 (Kim et al., 2024b). Unlike these, we rely solely on open-source human evaluations instead of model outputs. FLAMe significantly outperforms Prometheus-2 on RewardBench (see Table 2).</p>
<p>Appendix A has related work on reward models.</p>
<h2>3 The FLAMe Collection</h2>
<p>We curated 5.3 M human judgments across 102 training tasks, with an additional 53 tasks reserved for evaluation ( $\S 5.1$ ). Appendix B lists our datasets. Our data covers various task types and LLM capabilities ( $\S 3.2-3.3$ ). We manually crafted task definitions and evaluation instructions, converting all tasks into a unified format (§3.4).</p>
<h3>3.1 Task Definition</h3>
<p>A "task" refers to a specific assignment where a model evaluates aspects of a text (e.g., a machinegenerated summary), alongside its context (the original article), based on given criteria (Figure 2). Each task has its own definition and evaluation guidelines. Multiple tasks can be derived from a single dataset. ${ }^{3}$ Additionally, similar tasks from different datasets are treated as separate. Based on this definition, FLAMe has 102 distinct tasks.</p>
<h3>3.2 Principles for Data Collection</h3>
<p>Our principles for data selection are as follows:
Public, Open-source Data: We use only permissively licensed datasets from HuggingFace (Lhoest et al., 2021), TensorFlow, ${ }^{4}$ or the original authors' GitHub repositories.</p>
<p>Human Annotations: We only use humanlabeled annotations, avoiding those generated by models like GPT-4 due to potential inaccuracies and legal concerns (Gudibande et al., 2023; Muennighoff et al., 2023).</p>
<p>Diverse Task Types: To improve model generalizability, we collect datasets from a diverse set of task types (see breakdown in Figure 3): 1) Pairwise Evaluation: Tasks that involve comparing two responses to determine a preference (e.g., "Which response, $A$ or $B$, is more helpful?"); 2) Pointwise Evaluation: Tasks that involve evaluating specific attributes of individual responses (e.g., "Rate the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: FLAMe data collection breakdown by task type, showing the percentage of datapoints (out of 5.3M) for each task type. Over half of FLAMe is dedicated to standard pairwise ("Which response is better?") and pointwise ("Rate the response on a Likert scale.") evaluation. The remainder includes classification (e.g., "Is the summary fully attributable to the source article? (Yes/No)") and open-ended evaluation (e.g., "Explain why response $A$ is better than response B.").
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: FLAMe data collection breakdown by LLM capability, showing the percentage of datapoints (out of 5.3 M ) for each LLM capability. We focus on standard LLM evaluation pillars: general response quality, factuality, safety, coding, and math. Additionally, we incorporate non-evaluation instruction tuning data (e.g., LIMA) to maintain FLAMe's general-purpose instruction-following capabilities.
overall coherence of the response on a 5-point Likert scale."); 3) Classification: Tasks that involve categorizing responses into predefined categories (e.g., "Does the model output follow the instructions? (Yes/No)"); and 4) Open-ended Evaluation: Tasks that require free-form, unrestricted answers (e.g., "Is the summary fully attributable to the source article? Provide a brief explanation.").</p>
<p>Various LLM Capabilities: We select datasets from the literature that evaluate various LLM capabilities, including factuality, safety, reasoning, instruction-following, long-form generation, creativity, attribution, and coding (§3.3).</p>
<h3>3.3 LLM Capabilities Covered by FLAMe</h3>
<p>FLAMe encompasses key LLM capabilities, as outlined below (see breakdown in Figure 4).</p>
<p>General Response Quality: We assess LLM response quality using datasets that measure attributes like helpfulness, coherence, fluency, cre-</p>
<p>ativity, complexity, and verbosity. These include: Summary Comparisons (SummFeedback) (Stiennon et al., 2020), LMSYS Chatbot Arena conversations (Zheng et al., 2023), HH RLHF Helpfulness (Bai et al., 2022a), WebGPT (Nakano et al., 2021), SummEval (Fabbri et al., 2021), News Summary Evaluation (Goyal et al., 2022), SHP (Ethayarajh et al., 2022), BeaverTails Helpfulness (Ji et al., 2023), SEAHORSE (Clark et al., 2023), HelpSteer (Wang et al., 2023b), etc. For instructionfollowing abilities, we use datasets such as GENIE (Khashabi et al., 2022), InstruSum (Liu et al., 2024), and riSum (Skopek et al., 2023).</p>
<p>Factuality/Attribution: To measure hallucinations in LLM-generated responses, we use several datasets that evaluate factual accuracy and grounding (e.g., checking if claims are supported by source documents). These include: XSum Hallucination (Maynez et al., 2020), QAGS (Wang et al., 2020), WikiBio Hallucination (Manakul et al., 2023), FRANK (Pagnoni et al., 2021), FactScore (Min et al., 2023), VitaminC (Schuster et al., 2021), HaluEval (Li et al., 2023), Q² (Honovich et al., 2021), FaithDial (Dziri et al., 2022a), DialFact (Gupta et al., 2022), BEGIN (Dziri et al., 2022b), and MNLI (Williams et al., 2018), etc. ${ }^{5}$
Mathematical Reasoning: We create data to help FLAMe distinguish between correct and incorrect solutions to mathematical problems. Using PRM800K (Lightman et al., 2024), we extract pairs of human vs. incorrect LLM-generated solutions, along with pairs of (correct, incorrect) LLMgenerated solutions.
Coding: We train FLAMe for code evaluation. Using Code Contests (Li et al., 2022a), CommitPack (Muennighoff et al., 2023), and COFFEE (Moon et al., 2023), we create pairs of (correct, buggy) programs based on coding problems or GitHub issues. FLAMe learns to identify the correct program or fix across programming languages like Python, JavaScript, Java, C++, Go, and Rust.
Safety: Developing safe AI assistants for public use is crucial. To improve safety evaluation, we train FLAMe to identify harmless responses. Our training data includes tasks from HH RLHF Harmlessness (Bai et al., 2022a), HH RLHF Red Teaming (Ganguli et al., 2022), BeaverTails QAClassification and Harmlessness (Ji et al., 2023).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Instruction Tuning: Finally, to preserve our models' instruction-following capabilities, we incorporate instruction tuning data from humanwritten response datasets, including LIMA (Zhou et al., 2023), PRM800K IF (Lightman et al., 2024), ${ }^{6}$ and TULU-2 (Ivison et al., 2023). ${ }^{7}$</p>
<h3>3.4 Unified Task Format</h3>
<p>We standardize our datasets into a unified text-totext format. This preprocessing step takes around 3-4 hours per dataset and includes several key tasks: 1) Comprehensive Review and Author Consultations: We carefully review the associated research and consult with the original authors to clarify ambiguities or inconsistencies; 2) Data Collection: We gather all relevant data files from the corresponding HuggingFace, TensorFlow, or GitHub repositories; 3) Data Extraction: We extract data fields with human quality assessments; 4) Task Definitions and Evaluation Instructions: We write detailed task definitions and evaluation instructions for each task, ensuring consistency and standardization, while adhering to any available instructions provided to the original annotators. These instructions help FLAMe identify input/output formats and specific aspects to assess; and 5) Text-to-Text Format Conversion: We convert all tasks into a unified format (Figure 2). Task definitions, evaluation instructions, and desired output fields are listed under an INSTRUCTIONS block, while input and target field values are placed under CONTEXT and EVALUATION blocks, respectively. This format is easily adaptable to new tasks.</p>
<h2>4 Model</h2>
<p>We fine-tune the instruction-tuned PaLM-2-24B on the FLAMe collection to create general-purpose LLM autoraters that can be prompted to perform various tasks. We train three FLAMe variants: 1) FLAMe-trained with examples-proportional mixture weights (Raffel et al., 2020); 2) FLAMe-RMinitialized with FLAMe and fine-tuned on a balanced mixture of four pairwise evaluation datasets covering chat, reasoning, and safety (§4.2); and 3) FLAMe-Opt-RM—trained with RewardBenchoptimized mixture weights (§4.3).</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h1>4.1 General-purpose Autoraters (FLAMe)</h1>
<p>Our baseline FLAMe model is trained using supervised multitask training on the instruction-tuned PaLM-2-24B for 30K steps. We use examplesproportional mixture weights, capping each task at a maximum of $2^{16}$ examples to avoid oversampling large datasets. FLAMe shows significant generalization improvements across various held-out tasks, outperforming models like GPT-4, Claude-3, and Llama-3 on many tasks (see Figure 1 and Table 1). This supports our hypothesis that large-scale multitask instruction tuning enhances general-purpose quality assessment capabilities.</p>
<h3>4.2 FLAMe for Reward Model Evaluation (FLAMe-RM)</h3>
<p>We delve deeper into FLAMe's potential as a powerful starting point for fine-tuning on specific downstream applications, focusing on reward model evaluation as a case study. We create FLAMe-RM by fine-tuning FLAMe on a balanced mixture of four pairwise evaluation datasets: HelpSteer (Wang et al., 2023b), PRM800K (Lightman et al., 2024), CommitPack (Muennighoff et al., 2023), and HHRLHF Harmlessness (Bai et al., 2022a). Since FLAMe is already trained on these datasets, we fine-tune for only 50 steps. FLAMe-RM significantly boosts FLAMe's RewardBench accuracy from $86.0 \%$ to $87.8 \%$. As of July 15, 2024, FLAMe-RM-24B became the top-performing generative model trained solely on permissively licensed data, surpassing both GPT-4-0125 (85.9\%) and GPT-4o (84.7\%); see Figure 1 and Table 1.</p>
<h3>4.3 Optimizing FLAMe for RewardBench (FLAME-Opt-RM)</h3>
<p>Our baseline approach requires extensive training to attain strong performance on certain downstream tasks like RewardBench (Figure 5). This may stem from suboptimal mixture weights that undersample beneficial tasks. To address this, we introduce a tailpatch ablation strategy that evaluates each dataset's impact on targeted distributions, allowing efficient adjustment of all mixing weight hyperparameters. Fine-tuning the instruction-tuned PaLM-2-24B on this optimized mixture for just 5000 steps achieves competitive RewardBench performance (87.0\%) compared to the baseline FLAMe ( $86.0 \%$ ), using $25 \times$ fewer training datapoints.</p>
<p>We optimized our multitask mixture directly based on RewardBench performance due to the</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of FLAMe-Opt-RM and FLAMe during the first 5000 training steps based on RewardBench Chat Hard performance. FLAMe-Opt-RM, with optimized mixture weights, reaches significantly higher Chat Hard scores faster than FLAMe. For reference, FLAMe scores 66.2 at 30K steps. See Figure 6 in Appendix C for RewardBench safety results.
absence of a development set. Our early experiments showed weak correlations between RewardBench and other held-out tasks, making it hard to create a reliable proxy development set. Our goal here is not to achieve state-of-the-art RewardBench results but to demonstrate how to optimize our multitask mixture for specific distributions. ${ }^{8}$ Furthermore, FLAMe-Opt-RM's strong performance across other held-out tasks (Table 1) indicates that it was not overfitted to RewardBench.</p>
<p>Tail-patch Ablations: Assigning the right mixing weight for each task in our multitask mixture is challenging due to the large number of tasks. Instead, we assess each task's impact on targeted distributions and use that to assign weights. First, we select a checkpoint that has been partially trained on our vanilla mixture, showing decent but not optimal RewardBench performance. ${ }^{9}$ Then, we perform a brief fine-tuning stage ("tail-patch") on each individual training task, limited to 3000 training steps. This is a one-time process for each downstream application and can be done with smaller models to reduce computational costs.
A Re-weighted Mixture: After training a tailpatch on each task, we rate its impact on each RewardBench category using four ratings: Helpful (+2, significant and stable improvement), Some-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>what helpful (+1, slight improvement), No clear effect ( 0 , minimal change), Harmful ( -1 , significant drop). We then group tasks into seven bundles: Generally helpful (tasks with a total rating of $\geq 5$ ), Category-specific, one for each of the five RewardBench categories (most beneficial tasks for each category with performance exceeding a threshold $\tau),{ }^{10}$ and Others for the remaining tasks.</p>
<p>We assign fixed mixing weights to each bundle: $w_{\text {general }}=100 \mathrm{~K}$ for Generally helpful, $w_{\text {specific }}=30 \mathrm{~K}$ for each Category-specific bundle, and $w_{\text {others }}=3 \mathrm{~K}$ for Others. If a task belongs to multiple bundles, its final weight is the sum of the mixture weights from each bundle. ${ }^{11}$ An exception to this rule is that we prioritize the top two tasks in three underperforming categories - Chat Hard, Coding, and Safety - each assigned a fixed weight of $w_{\text {top_ specific }}=200 \mathrm{~K}$. These values were initially set based on our intuition and not extensively tuned.</p>
<h3>4.4 Training Details</h3>
<p>We initialize both FLAMe and FLAMe-Opt-RM with PaLM-2-24B (Anil et al., 2023), instructiontuned on the Flan collection (Longpre et al., 2023), and train for 30 K and 5 K steps, respectively. FLAMe is further fine-tuned for 50 steps to create FLAMe-RM. Our models are trained using T5X (Roberts et al., 2023) with the Adam optimizer (Kingma and Ba, 2015), a learning rate of 0.0001 , and a dropout rate of 0.05 . FLAMe is trained on 256 Cloud TPU chips with a batch size of 32, whereas FLAMe-RM and FLAMe-Opt-RM use 128 Cloud TPU chips with a batch size of $8 .{ }^{12}$</p>
<h2>5 Experiments</h2>
<p>We compare FLAMe to several popular LLM-as-aJudge autoraters (§5.2) using a suite of 12 autorater benchmarks ( 1 held-in and 11 held-out), covering a total of 53 quality assessment tasks (§5.1). Overall, FLAMe variants outperform all LLM-as-a-Judge autoraters on 8 out of 12 benchmarks (§5.3).</p>
<h3>5.1 Evaluation Datasets</h3>
<p>We use a variety of held-in and held-out tasks. Each task is cast into our unified task format (§3.4). For</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>benchmarks with multiple categories (e.g., RewardBench, LLM-AggreFact), we use the same prompt instructions across categories. To minimize API costs, we randomly sample 256 examples per evaluation task, ${ }^{13}$ except for RewardBench, where results are reported for the full set.</p>
<h3>5.1.1 Held-in Evaluations</h3>
<p>HelpSteer (Wang et al., 2023b): We assess FLAMe's performance in rating helpfulness, correctness, coherence, complexity, and verbosity, using HelpSteer's validation data.</p>
<h3>5.1.2 Held-out Evaluations</h3>
<p>RewardBench (Lambert et al., 2024): RewardBench is a popular benchmark for evaluating reward models via pairwise preference tasks, where models select the better response between two options based on a given prompt. It incorporates 23 datasets, covering four categories-Chat, Chat Hard, Reasoning (Math + Coding), and Safety. ${ }^{14}$</p>
<p>LLM-AggreFact (Tang et al., 2024): This benchmark integrates ten attribution datasets to assess the grounding capabilities of autoraters. The autorater evaluates whether a claim is fully supported by a given document.</p>
<p>Other Benchmarks: In addition to RewardBench and LLM-AggreFact, we include a diverse set of held-out pointwise and pairwise evaluation benchmarks, including Summary Comparisons (SummFeedback) (Stiennon et al., 2020); ${ }^{15}$ Helpful, Honest, and Harmless Alignment (HHH) (Askell et al., 2021); AlpacaFarm (Dubois et al., 2023); Paraphrase Evaluation (Dipper) (Krishna et al., 2023b); Sequence Continuation Preference (RankGen) (Krishna et al., 2022); Poem Preference (CoPoet) (Chakrabarty et al., 2022); Literary Translation Comparisons (LitTrans) (Karpinska and Iyyer, 2023); Long-form QA Evaluation (LFQAEval) (Xu et al., 2023a); and Text Continuation Preference (ContrSearch) (Su and Xu, 2022).</p>
<h3>5.2 Evaluated Models</h3>
<p>We compare our models to the original instructiontuned PaLM-2-24B, which was not trained on</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Reward <br> Bench</th>
<th style="text-align: center;">LLM <br> AggreFact</th>
<th style="text-align: center;">Summ <br> Feedback</th>
<th style="text-align: center;">Alpaca <br> Farm</th>
<th style="text-align: center;">Rank <br> Gen</th>
<th style="text-align: center;">Co <br> Poet</th>
<th style="text-align: center;">Contr <br> Search</th>
<th style="text-align: center;">HHH</th>
<th style="text-align: center;">Dipper</th>
<th style="text-align: center;">Lit <br> Trans</th>
<th style="text-align: center;">LFQA <br> Eval</th>
<th style="text-align: center;">Help <br> Steer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama-3-70B-Instruct</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">39.7</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8×7B</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">34.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo-0125</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">32.0</td>
</tr>
<tr>
<td style="text-align: left;">Claude-3-Opus</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">$\mathbf{9 4 . 6}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 6}$</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">41.3</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-0125</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">$\mathbf{9 4 . 6}$</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">$\mathbf{7 7 . 0}$</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">$\mathbf{7 2 . 7}$</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">40.1</td>
</tr>
<tr>
<td style="text-align: left;">our models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2-24B</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-24B</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">$\mathbf{8 1 . 1}$</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">$\mathbf{5 8 . 2}$</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">$\mathbf{6 9 . 9}$</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">$\mathbf{4 8 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-RM-24B</td>
<td style="text-align: center;">$\mathbf{8 7 . 8}$</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">$\mathbf{5 3 . 1}$</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">$\mathbf{5 7 . 5}$</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">46.6</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-Opt-RM-24B</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">$\mathbf{6 9 . 5}$</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">35.9</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance of FLAMe compared to popular LLM-as-a-Judge autoraters across various autorater benchmarks. Overall, FLAMe variants outperform all LLM-as-a-Judge autoraters on 8 out of 12 benchmarks, including RewardBench and LLM-AggreFact. See $\S 5.1$ for the sources of our benchmarks.</p>
<p>FLAMe, to isolate the effects of instruction tuning and FLAMe training. We also evaluate several popular LLM-as-a-Judge autoraters, including Llama-3-70B-Instruct (Meta, 2024), Mixtral $8 \times 7$ B (Jiang et al., 2024a), Claude-3-Opus (Anthropic, 2024), GPT-3.5-turbo-0125 (OpenAI, 2024a), GPT-4-0125 (OpenAI, 2024b), and GPT4o (OpenAI, 2024c). ${ }^{16}$ Additionally, we include several models from the official RewardBench leaderboard, notably Gemini-1.5-Pro (Reid et al., 2024), Prometheus-2-8×7B (Kim et al., 2024b), ArmoRM-Llama-3-8B-v0.1 (Wang et al., 2024a), and NVIDIA's Nemotron-4-340B-Reward and Llama-3-70B-SteerLM-RM (Wang et al., 2024b).</p>
<h3>5.3 Main Results</h3>
<p>Table 1 shows our main results across all evaluation benchmarks. RewardBench and LLM-AggreFact results are shown in Table 2 and Table 6, respectively. Below, we first provide an overview of these results before analyzing them in more detail:</p>
<h2>FLAMe Variants Outperform all LLM-as-aJudge Autoraters on 8 out of 12 Benchmarks:</h2>
<p>Table 1 shows FLAMe's strong generalization to various held-out tasks, highlighting its effectiveness as a versatile LLM autorater. FLAMe provides significant gains over the initial instruction-tuned PaLM-2-24B. Remarkably, our models outperform all state-of-the-art LLM-as-a-Judge autoraters on 8 out of 12 benchmarks. FLAMe variants outperform the next-best model by significant margins on several held-out benchmarks, including ContrSearch (69.9 vs. 57.5 for GPT-4o/GPT-3.5-turbo-0125),</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: As of July 15, 2024, FLAMe-RM-24B outperforms other generative models on the RewardBench leaderboard, achieving the best score (87.8\%) among models trained solely on permissively licensed data.</p>
<p>RankGen (69.5 vs. 66.0 for GPT-4o), AlpacaFarm (58.2 vs. 55.5 for GPT-3.5-turbo-0125), SummFeedback (53.1 vs. 50.8 for Llama-3-70B-Instruct), and RewardBench ( 87.8 vs. 85.9 for GPT-4-0125). Additionally, our models achieve the best heldin performance on HelpSteer ( 48.4 vs. 41.3 for Claude-3-Opus).</p>
<p>On the other hand, FLAMe variants lag behind proprietary models on several benchmarks, including HHH ( 91.4 vs. 94.6 for GPT-4-0125/Claude-3-Opus), LitTrans ( 69.5 vs. 72.7 for GPT-4o), and LFQAEva ( 74.2 vs. 77.0 for GPT-4-0125), indicating that these models may have been optimized for these capabilities.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Autorater</th>
<th style="text-align: center;">Avg. ( $\downarrow$ )</th>
<th style="text-align: center;">Order ( $\downarrow$ )</th>
<th style="text-align: center;">Compassion ( $\downarrow$ )</th>
<th style="text-align: center;">Length ( $\downarrow$ )</th>
<th style="text-align: center;">Egocentric ( $\downarrow$ )</th>
<th style="text-align: center;">Bandwagon ( $\downarrow$ )</th>
<th style="text-align: center;">Attention ( $\downarrow$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">baselines reported in Koo et al. (2023)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Falcon-40B</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">$\mathbf{0 . 0 5}$</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: left;">Cohere-54B</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-70B</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.06</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">our models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-24B</td>
<td style="text-align: center;">$\mathbf{0 . 1 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 8}$</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-RM-24B</td>
<td style="text-align: center;">$\mathbf{0 . 1 3}$</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">$\mathbf{0 . 0 8}$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-Opt-RM-24B</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">$\mathbf{0 . 0 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Autorater bias analysis on the CoBBLEr bias benchmark from Koo et al. (2023). Lower values indicate better or less biased autoraters across all columns. Overall, FLAMe variants exhibit significantly less bias compared to popular LLM-as-a-Judge autoraters like GPT-4. Compared to Table 2 in Koo et al. (2023), we combine first/last numbers for Order/Compassion, report $|\operatorname{bias}-0.5|$ for Length, and only report the order setup in Egocentric.</p>
<p>FLAMe Variants are among the Most Powerful Generative Models on RewardBench: Our results in Table 2 show that FLAMe variants rank among the top generative models on the official RewardBench leaderboard, ${ }^{17}$ demonstrating strong performance in all categories: Chat, Chat Hard, Safety, and Reasoning. Notably, FLAMe-RM-24B achieves an overall score of $87.8 \%$, the highest among generative models trained solely on permissively licensed data, surpassing GPT-4-0125 (85.9) and GPT-4o (84.7). As of July 15, 2024, FLAMeRM-24B ranked second among generative models (below Gemini-1.5-Pro) and sixth overall. We provide an analysis of length and token biases found in RewardBench in Appendix E. Additionally, we discuss our LLMAggreFact results in Appendix D.</p>
<h2>6 Further Analysis of FLAMe</h2>
<p>In this section, we depart from the typical focus on analyzing the effect of factors like model size, data size, and data quality in multitask learning, which have been extensively studied (Raffel et al., 2020; Longpre et al., 2023). Instead, we examine potential biases in our LLM autoraters. We find that our models are significantly less biased than popular LLM-as-a-Judge autoraters. In Appendix F, we further demonstrate FLAMe's potential utility for AI development, particularly in identifying highquality responses for code generation.</p>
<h3>6.1 Autorater Bias Analysis</h3>
<p>A common criticism of LLM-as-a-Judge autoraters is their bias towards certain judgments (Liu et al., 2023a; Panickssery et al., 2024; Liu et al., 2023b; Bai et al., 2023). Here, we evaluate FLAMe</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>variants on the CoBBLEr autorater bias benchmark (Koo et al., 2023).</p>
<p>CoBBLEr measures six types of biases in LLM autoraters: 1) Order: Does the autorater favor a particular response position? 2) Compassion: Does the autorater's judgment change when using the LLM's actual name, like "GPT-4" instead of aliases like "Model A"? 3) Length: Does the autorater prefer longer or shorter outputs? 4) Egocentric: Does the autorater favor outputs it generated itself? 5) Bandwagon: Is the autorater influenced by statements like " $90 \%$ of people prefer response A"? 6) Attention: Does irrelevant context, such as "Response A is about cats." distract the autorater? We reformat the original (prompt,response) pairs from Koo et al. (2023) into our unified FLAMe format (Figure 2) and compare FLAMe variants to other LLM-as-a-Judge autoraters, including GPT-4, reported in Koo et al. (2023).</p>
<p>Table 3 shows that FLAMe variants exhibit significantly lower bias compared to GPT-4 and other autoraters, with an average bias of 0.13 vs. 0.31 for GPT-4 (lower is better). FLAMe matches or outperforms GPT-4 across all six bias categories. These results demonstrate FLAMe's effectiveness as a robust and reliable autorater.</p>
<h2>7 Conclusion</h2>
<p>We curated and standardized human evaluations from permissively licensed datasets, compiling a data collection of over 100 quality assessment tasks with 5M+ human judgments. We demonstrate that this collection can be used for training generalpurpose LLM autoraters and optimizing them for specific downstream applications. Our models outperform popular proprietary LLM autoraters on 8 out of 12 autorater benchmarks, covering 53 tasks.</p>
<h2>Limitations and Future work</h2>
<p>Evaluating LLMs is challenging due to evolving evaluation standards and the need to assess new LLM capabilities. Expanding our data collection with open-source contributions could address this issue. Additionally, our models, trained primarily on English data with a context length of 2048 tokens, might not perform well on multilingual (Freitag et al., 2021) or long-context (Kim et al., 2024c; Karpinska et al., 2024) quality assessment tasks. Finally, in this work, we train our models in a supervised multitask fashion. Exploring alternative training approaches such as RLHF and DPO is a promising direction for future work.</p>
<h2>Ethical Considerations and Risks</h2>
<p>All considerations and risks outlined by prior work for pretrained and instruction-tuned LLMs (Chowdhery et al., 2022; Anil et al., 2023) apply to LLM autoraters. We recommend following standard practice for responsible development of these models (Achiam et al., 2023; Gemini et al., 2023; Reid et al., 2024). Additionally, LLM autoraters raise new risks due to increased quality assessment capabilities. First, our models can inherit and amplify biases from human evaluations, leading to unfair or discriminatory outcomes. For instance, the model may replicate biases related to race, gender, or other sensitive attributes from the training data, potentially harming certain groups. Second, overreliance on LLM autoraters risks automating decisions that need human understanding and empathy. To mitigate these risks, transparency in model development and use, along with robust measures like bias audits, data anonymization, and incorporating diverse perspectives, is essential for promoting fairness, accountability, and trustworthiness.</p>
<h2>Acknowledgments</h2>
<p>We are grateful to Jie Ren, Denny Zhou, and Tania Bedrax-Weiss for their comments on this manuscript. We thank Mohit Iyyer, Daniel Cer, Elizabeth Clark, Jeremiah Liu, Balaji Lakshminarayanan, Clara Huiyi Hu, Aliaksei Severyn, Adam Sadovsky, Yonghui Wu, Quoc Le, Slav Petrov, Séb Arnold, Taylan Bilal, Noah Constant, Colin Raffel, Nan Hua, Marzena Karpinska, Yixiao Song, Tuhin Chakrabarty, the Gemini model quality team, the Descartes team at Google, and the UMass NLP group for useful discussions and valuable feedback at different stages of this project. We
thank the authors of the datasets used in this work, especially Niklas Muennighoff, Hyungjoo Chae, Mounica Maddela, Tanya Goyal, and Yuanhao Wu, for their helpful suggestions and for answering our questions. Finally, we thank Grady Simon, ChungChing Chang, Sho Kannan, Gustavo Hernandez Abrego, and the T5X team for their assistance with the codebase, implementation, and computational resources.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>AI Anthropic. 2024. Introducing the next generation of claude.</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.</p>
<p>Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. 2023. Benchmarking foundation models with language-model-as-an-examiner. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 78142-78167.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. In Advances in Neural Information Processing Systems 31 (NeurIPS), volume 31. Curran Associates, Inc.</p>
<p>Daniel Cer, Mona Diab, Eneko Agirre, Iñigo LopezGazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1-14.</p>
<p>Tuhin Chakrabarty, Vishakh Padmakumar, and He He. 2022. Help me write a poem - instruction tuning as a vehicle for collaborative poetry writing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6848-6863.</p>
<p>Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2020. MOCHA: A dataset for training and evaluating generative reading comprehension metrics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6521-6532.</p>
<p>Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2023. Codet: Code generation with generated tests. In The Eleventh International Conference on Learning Representations (ICLR).</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pages 15607-15631.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai,</p>
<p>Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research (JMLR), 25(70):1-53.</p>
<p>Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, and Ankur Parikh. 2023. SEAHORSE: A multilingual, multifaceted dataset for summarization evaluation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9397-9413.</p>
<p>Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. 2021. Towards question-answering as an automatic metric for evaluating the content quality of a summary. Transactions of the Association for Computational Linguistics (TACL), 9:774-789.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 4171-4186.</p>
<p>Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, and Yejin Choi. 2022a. Is GPT3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages $7250-7274$.</p>
<p>Yao Dou, Chao Jiang, and Wei Xu. 2022b. Improving large-scale paraphrase acquisition and generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9301-9323.</p>
<p>Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 30039-30069.</p>
<p>Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages $5055-5070$.</p>
<p>Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M. Ponti, and Siva Reddy. 2022a. FaithDial: A faithful benchmark for</p>
<p>information-seeking dialogue. Transactions of the Association for Computational Linguistics (TACL), 10:1473-1490.</p>
<p>Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2022b. Evaluating attribution in dialogue systems: The BEGIN benchmark. Transactions of the Association for Computational Linguistics (TACL), 10:1066-1083.</p>
<p>Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. 2022. Understanding dataset difficulty with $V$-usable information. In Proceedings of the 39th International Conference on Machine Learning (ICML), volume 162 of Proceedings of Machine Learning Research (PMLR), pages 5988-6008.</p>
<p>Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics (TACL), 9:391-409.</p>
<p>Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. 2023. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In Proceedings of the Eighth Conference on Machine Translation (WMT), pages 1066-1083.</p>
<p>Markus Freitag, George Foster, David Grangier, Viresh Ramakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics (TACL), 9:1460-1474.</p>
<p>Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. 2023. Incoder: A generative model for code infilling and synthesis. In The Eleventh International Conference on Learning Representations (ICLR).</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2024. GPTScore: Evaluate as you desire. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 6556-6576.</p>
<p>Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858.</p>
<p>Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. 2019. GLTR: Statistical detection and visualization of generated text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL), pages 111-116.</p>
<p>Team Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.</p>
<p>Tanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 1449-1462.</p>
<p>Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356.</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717.</p>
<p>Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. DialFact: A benchmark for fact-checking in dialogue. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 3785-3801.</p>
<p>Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. $q^{2}$ : Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7856-7870.</p>
<p>Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. 2023. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702.</p>
<p>Shankar Iyer, Nikhil Dandekar, and Kornél Csernai. 2017. First Quora Dataset release: Question pairs.</p>
<p>Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2023. Beavertails: Towards improved safety alignment of llm via a humanpreference dataset. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 24678-24704.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024a. Mixtral of experts. arXiv preprint arXiv:2401.04088.</p>
<p>Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. 2024b. TIGERScore: Towards building explainable metric for all text generation tasks. Transactions on Machine Learning Research (TMLR).</p>
<p>Marzena Karpinska, Nader Akoury, and Mohit Iyyer. 2021. The perils of using Mechanical Turk to evaluate open-ended text generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1265-1285.</p>
<p>Marzena Karpinska and Mohit Iyyer. 2023. Large language models effectively leverage document-level context for literary translation, but critical errors persist. In Proceedings of the Eighth Conference on Machine Translation (WMT), pages 419-451.</p>
<p>Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024. One thousand and one pairs: A" novel" challenge for long-context language models. arXiv preprint arXiv:2406.16264.</p>
<p>Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A. Smith, and Daniel Weld. 2022. GENIE: Toward reproducible and standardized human evaluation for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 11444-11458.</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2024a. Prometheus: Inducing finegrained evaluation capability in language models. In The Twelfth International Conference on Learning Representations (ICLR).</p>
<p>Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024b. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535.</p>
<p>Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024c. Fables: Evaluating faithfulness and content selection in book-length summarization. arXiv preprint arXiv:2404.01261.</p>
<p>Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR).</p>
<p>Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. 2023. Benchmarking cognitive biases in large language models as evaluators. arXiv preprint arXiv:2309.17012.</p>
<p>Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason Phang, Samuel R. Bowman, and Ethan Perez. 2023. Pretraining language models with human preferences. In Proceedings of the 40th International Conference on Machine Learning (ICML), volume 202 of Proceedings of Machine Learning Research (PMLR), pages 17506-17533.</p>
<p>Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023a. LongEval: Guidelines for human evaluation of faithfulness in long-form summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</p>
<p>Kalpesh Krishna, Yapei Chang, John Wieting, and Mohit Iyyer. 2022. RankGen: Improving text generation with large ranking models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 199-232.</p>
<p>Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 4940-4957.</p>
<p>Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. 2023b. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 27469-27500.</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787.</p>
<p>Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. 2021. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP), pages 175184 .</p>
<p>Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A large-scale hallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6449-6464.</p>
<p>Ruosen Li, Teerth Patel, and Xinya Du. 2024. PRD: Peer rank and discussion improve large language model based evaluations. Transactions on Machine Learning Research (TMLR).</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, PoSen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022a. Competition-level code generation with alphacode. Science, 378(6624):1092-1097.</p>
<p>Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie Cheung, and Siva Reddy. 2022b. Using interactive feedback to improve the accuracy and explainability of question answering systems post-deployment. In Findings of the Association for Computational Linguistics: ACL 2022 (ACL Findings), pages 926-937.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Let's verify step by step. In The Twelfth International Conference on Learning Representations (ICLR).</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Proceedings of the Workshop of Text Summarization Branches Out (WS), pages 74-81.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 3214-3252.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023a. G-eval: NLG evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2511-2522.</p>
<p>Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin. 2023b. Llms as narcissistic evaluators: When ego inflates evaluation scores. arXiv preprint arXiv:2311.09766.</p>
<p>Yixin Liu, Alexander Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan. 2024. Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization. In Findings of the Association for Computational Linguistics: NAACL 2024 (NAACL Findings), pages 4481-4501.</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. In Proceedings of the 40th International Conference on Machine Learning (ICML), volume 202 of Proceedings of Machine Learning Research (PMLR), pages 22631-22648.</p>
<p>Mounica Maddela, Yao Dou, David Heineman, and Wei Xu. 2023. LENS: A learnable evaluation metric for text simplification. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pages 16383-16408.</p>
<p>Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9004-9017.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1906-1919.</p>
<p>AI Meta. 2024. Introducing meta llama 3: The most capable openly available llm to date. Meta AI.</p>
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 12076-12100.</p>
<p>Seungjun Moon, Yongho Song, Hyungjoo Chae, Dongjin Kang, Taeyoon Kwon, Kai Tzu-iunn Ong, Seung-won Hwang, and Jinyoung Yeo. 2023. Coffee: Boost your code llms by fixing bugs with feedback. arXiv preprint arXiv:2311.07215.</p>
<p>Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. 2023. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations (ICLR).</p>
<p>OpenAI. 2024a. GPT-3.5 Turbo.
OpenAI. 2024b. GPT-4 Turbo and GPT-4.
OpenAI. 2024c. Hello GPT-4o.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John</p>
<p>Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 27730-27744.</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 4812-4829.</p>
<p>Arjun Panickssery, Samuel R Bowman, and Shi Feng. 2024. Llm evaluators recognize and favor their own generations. arXiv preprint arXiv:2404.13076.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311-318.</p>
<p>Zarana Parekh, Jason Baldridge, Daniel Cer, Austin Waters, and Yinfei Yang. 2021. Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for MS-COCO. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (EACL), pages 2855-2870.</p>
<p>Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. Mauve: Measuring the gap between neural text and human text using divergence frontiers. In Advances in Neural Information Processing Systems 34 (NeurIPS), volume 34, pages $4816-4828$.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems 37 (NeurIPS), 36.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR 2020), 21(140):1-67.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702.</p>
<p>Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste</p>
<p>Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530.</p>
<p>Adam Roberts, Hyung Won Chung, Gaurav Mishra, Anselm Levskaya, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Kehang Han, Michelle Casbon, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. 2023. Scaling up models and data with t5x and seqio. Journal of Machine Learning Research (JMLR), 24(377):1-8.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations (ICLR).</p>
<p>Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 624-643.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 7881-7892.</p>
<p>Ondrej Skopek, Rahul Aralikatte, Sian Gooding, and Victor Carbune. 2023. Towards better evaluation of instruction-following: A case-study in summarization. In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pages 221-237.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,</p>
<p>Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems 33 (NeurIPS), volume 33, pages 3008-3021.</p>
<p>Yixuan Su and Jialu Xu. 2022. An empirical study on contrastive search and contrastive decoding for open-ended text generation. arXiv preprint arXiv:2211.10797.</p>
<p>Liyan Tang, Philippe Laban, and Greg Durrett. 2024. Minicheck: Efficient fact-checking of llms on grounding documents. arXiv preprint arXiv:2404.10774.</p>
<p>Brian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90-121.</p>
<p>Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. 2023. Freshllms: Refreshing large language models with search engine augmentation. arXiv preprint arXiv:2310.03214.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 5008-5020.</p>
<p>Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024a. Interpretable preferences via multi-objective reward modeling and mixture-ofexperts. arXiv preprint arXiv:2406.12845.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is ChatGPT a good NLG evaluator? a preliminary study. In Proceedings of the 4th New Frontiers in Summarization Workshop (NewSum), pages 1-11.</p>
<p>Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. 2024b. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673.</p>
<p>Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, et al. 2023b. Helpsteer: Multi-attribute helpfulness dataset for steerlm. arXiv preprint arXiv:2311.09528.</p>
<p>Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics (TACL), 7:625-641.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.</p>
<p>Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations (ICLR).</p>
<p>Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, et al. 2024. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (NAACL), pages 1112-1122.</p>
<p>Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Cheng Niu, Randy Zhong, Juntong Song, and Tong Zhang. 2023a. Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models. arXiv preprint arXiv:2401.00396.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023b. Finegrained human feedback gives better rewards for language model training. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 59008-59033.</p>
<p>Fangyuan Xu, Junyi Jessy Li, and Eunsol Choi. 2022. How do we answer complex questions: Discourse structure of long-form answers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pages 3556-3572.</p>
<p>Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023a. A critical evaluation of evaluations for long-form question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pages 3225-3245.</p>
<p>Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li. 2023b. INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5967-5994.</p>
<p>Xinyan Yu, Sewon Min, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. CREPE: Open-domain question answering with false presuppositions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), pages $10457-10480$.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems 34 (NeurIPS), volume 34, pages 2726327277.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations (ICLR).</p>
<p>Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (ACL), pages 1298-1308.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578.</p>
<p>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. 2023. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 46595-46623.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment. In Advances in Neural Information Processing Systems 36 (NeurIPS), volume 36, pages 55006-55021. Curran Associates, Inc.</p>
<h2>Appendix</h2>
<h2>A Related Work on Reward Models</h2>
<p>Our work relates to the development of reward models (RMs) used to align LLMs with human preferences using reinforcement learning with human feedback (RLHF) (Ouyang et al., 2022; Korbak et al., 2023). In RLHF, human preference data is either used to train standalone discriminative RMs, or directly fed into LLMs via algorithms like DPO (Rafailov et al., 2024) or SLiC-HF (Zhao et al., 2023). While we evaluate our models as RMs in our RewardBench experiments (§5), there are key distinctions: (1) RMs primarily rely on pairwise preference data, ${ }^{18}$ while our models use diverse task types in a unified format; (2) RMs optimize for overall preference, whereas our models can be prompted to judge specific aspects of responses (e.g., safety).</p>
<h2>B List of Training Datasets in FLAMe</h2>
<p>Table 5 shows the list of datasets used in our study.</p>
<h2>C Additional Results for FLAME-Opt-RM</h2>
<p>See Figure 6 for RewardBench safety results.</p>
<h2>D Performance of FLAMe on LLM-Aggregact</h2>
<p>Table 6 presents a breakdown of our attribution results on LLM-AggreFact (Tang et al., 2024), categorized into four common use cases: 1) LLMFactVerify: fact verification of LLM-generated responses, 2) Wiki-FactVerify: evaluating correctness of Wikipedia claims, 3) Summarization: assessing faithfulness of summaries, and 4) Longform QA: evaluating long-form answers to questions. FLAMe variants outperform all other models in three out of the four categories (LLM-FactVerify, Wiki-FactVerify, and Summarization). FLAMe24B achieves the highest overall performance of 81.1, while the next-best baseline model GPT-40125 obtains a score of 80.6. In long-form QA attribution evaluation, our best model FLAMe-OptRM underperforms compared to GPT-4-0125 (74.8 vs. 77.3), aligning with our findings in Table 1.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>E Analyzing Length and Token Bias in RewardBench</p>
<p>In this section, we provide an analysis of length (Appendix E.1) and token (Appendix E.2) bias issues identified in the RewardBench benchmark. Given these issues, we encourage future work to evaluate LLM autoraters on a wide variety of benchmarks (such as our evaluation suite in §5), rather than relying solely on RewardBench.</p>
<h3>E.1 Length Bias in RewardBench</h3>
<p>Table 4 highlights length bias in RewardBench. Overall, RewardBench shows significant imbalance across categories regarding length: Chat Hard, Math, and Coding favor shorter outputs, while Chat leans towards longer outputs. An adversarial submission might strategically select longer or shorter outputs based on prompt categories to achieve higher scores, without necessarily reflecting a genuinely strong preference model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">RewardBench Category</th>
<th style="text-align: center;">\% Preference for Longer Outputs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chat</td>
<td style="text-align: center;">$79.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Chat Hard</td>
<td style="text-align: center;">$29.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Math</td>
<td style="text-align: center;">$6.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Coding</td>
<td style="text-align: center;">$35.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Safety</td>
<td style="text-align: center;">$41.9 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Summary of length bias in RewardBench. Overall, we find that four out of five RewardBench categories show a strong preference towards either longer or shorter outputs.</p>
<h3>E.2 Token Bias in RewardBench</h3>
<p>Besides length bias, we identified token bias in the Math and Safety categories of RewardBench. In Safety, favored responses significantly leaned towards phrases like "I'm sorry", which suggest hedged responses. The word "sorry" appeared nearly $23 \%$ more frequently in preferred responses compared to non-preferred ones. Similarly, the Math split exhibited token bias, where tokens such as "i", "can", "need", "to", "find" were predominantly found in rejected responses.</p>
<h2>F Using FLAMe to Re-rank Decoded Outputs</h2>
<p>In this section, we explore the application of our LLM autoraters in selecting optimal outputs from multiple responses, a method known as "Best-of-N" sampling (Nakano et al., 2021; Krishna et al., 2022). Using FLAMe for re-ranking, we</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparison of FLAMe-Opt-RM and FLAMe during the first 5000 training steps based on RewardBench Safety performance. FLAMe-Opt-RM, with optimized mixture weights, reaches significantly higher Safety scores faster than FLAMe. For reference, FLAMe scores 88.5 at 30K steps.
assess its impact on code generation performance with the HumanEval Python programming benchmark (Chen et al., 2021). We conduct experiments by re-ranking 10 code samples generated by three models: OpenAI's davinci-002, InCoder6B (Fried et al., 2023), and CodeGen-16B (Nijkamp et al., 2023) using a round-robin competition, and then measuring performance with the top-ranked code sample. ${ }^{19}$ Results in Table 7 show that FLAMe provides significant gains in pass@1 accuracy across all three models. Notably, FLAMe improves CodeGen-16B's pass@1 from 21.2 to 31.1, closing nearly $40 \%$ of the gap to the Oracle ranker (46.9).</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Capability</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Output Format</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">General Response Quality</td>
<td style="text-align: center;">BeaverTails Helpfulness</td>
<td style="text-align: center;">Ji et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HH RLHF Helpfulness</td>
<td style="text-align: center;">Bai et al. (2022a)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hurdles LFQA</td>
<td style="text-align: center;">Krishna et al. (2021)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LMSYS Chatbot Arena conversations</td>
<td style="text-align: center;">Zheng et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAUVE</td>
<td style="text-align: center;">Pillutla et al. (2021)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">News Summary Evaluation</td>
<td style="text-align: center;">Goyal et al. (2022)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRD</td>
<td style="text-align: center;">Li et al. (2024)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SHF</td>
<td style="text-align: center;">Ethayarajb et al. (2022)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HelpSteer</td>
<td style="text-align: center;">Wang et al. (2023b)</td>
<td style="text-align: center;">Pairwise, Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Summary Comparisons</td>
<td style="text-align: center;">Stiennon et al. (2020)</td>
<td style="text-align: center;">Pairwise, Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GENIE</td>
<td style="text-align: center;">Khashabi et al. (2022)</td>
<td style="text-align: center;">Pairwise, Pointwise, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fine-grained RLHF</td>
<td style="text-align: center;">Wu et al. (2023b)</td>
<td style="text-align: center;">Pairwise, Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">InstruSum</td>
<td style="text-align: center;">Liu et al. (2024)</td>
<td style="text-align: center;">Pairwise, Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebGPT</td>
<td style="text-align: center;">Nakano et al. (2021)</td>
<td style="text-align: center;">Pairwise, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LENS</td>
<td style="text-align: center;">Maddela et al. (2023)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SummEval</td>
<td style="text-align: center;">Fabbri et al. (2021)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">riSum</td>
<td style="text-align: center;">Skopek et al. (2023)</td>
<td style="text-align: center;">Pointwise, Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FeedbackQA</td>
<td style="text-align: center;">Li et al. (2022b)</td>
<td style="text-align: center;">Pointwise, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoLA</td>
<td style="text-align: center;">Warstadt et al. (2019)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SEAHORSE</td>
<td style="text-align: center;">Clark et al. (2023)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CREPE</td>
<td style="text-align: center;">Yu et al. (2023)</td>
<td style="text-align: center;">Classification, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Scarecrow</td>
<td style="text-align: center;">Dou et al. (2022a)</td>
<td style="text-align: center;">Classification, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Validity LFQA</td>
<td style="text-align: center;">Xu et al. (2022)</td>
<td style="text-align: center;">Classification, Generative</td>
</tr>
<tr>
<td style="text-align: center;">Factuality/Attribution</td>
<td style="text-align: center;">MOCHA</td>
<td style="text-align: center;">Chen et al. (2020)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence Similarity - C $\times$ C</td>
<td style="text-align: center;">Parekh et al. (2021)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence Similarity - STS-B</td>
<td style="text-align: center;">Cer et al. (2017)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WikiBio Hallucination</td>
<td style="text-align: center;">Manakul et al. (2023)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BEGIN</td>
<td style="text-align: center;">Dziri et al. (2022b)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DialFact</td>
<td style="text-align: center;">Gupta et al. (2022)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FActScore</td>
<td style="text-align: center;">Min et al. (2023)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FRANK</td>
<td style="text-align: center;">Pagnoni et al. (2021)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FaithDial</td>
<td style="text-align: center;">Dziri et al. (2022a)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HaloEval</td>
<td style="text-align: center;">Li et al. (2023)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLJ</td>
<td style="text-align: center;">Williams et al. (2018)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MultiPIT</td>
<td style="text-align: center;">Dou et al. (2022b)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PAWS</td>
<td style="text-align: center;">Zhang et al. (2019)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$Q^{2}$</td>
<td style="text-align: center;">Honovich et al. (2021)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAGS</td>
<td style="text-align: center;">Wang et al. (2020)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QQP</td>
<td style="text-align: center;">Iyer et al. (2017)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VitaminC</td>
<td style="text-align: center;">Schuster et al. (2021)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RAGTruth</td>
<td style="text-align: center;">Wu et al. (2023a)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ESNLJ</td>
<td style="text-align: center;">Camburu et al. (2018)</td>
<td style="text-align: center;">Classification, Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">XSum Hallucination</td>
<td style="text-align: center;">Maynez et al. (2020)</td>
<td style="text-align: center;">Generative</td>
</tr>
<tr>
<td style="text-align: center;">Mathematical Reasoning</td>
<td style="text-align: center;">PRM800K</td>
<td style="text-align: center;">Lightman et al. (2024)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;">Coding</td>
<td style="text-align: center;">Code Contests</td>
<td style="text-align: center;">Li et al. (2022a)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COFFEE</td>
<td style="text-align: center;">Moon et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CommitPack</td>
<td style="text-align: center;">Muennighoff et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CommitPack - Bugs</td>
<td style="text-align: center;">Muennighoff et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;">Safety</td>
<td style="text-align: center;">BeaverTails Harmlessness</td>
<td style="text-align: center;">Ji et al. (2023)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HH RLHF Harmlessness</td>
<td style="text-align: center;">Bai et al. (2022a)</td>
<td style="text-align: center;">Pairwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HH RLHF Red Teaming</td>
<td style="text-align: center;">Bai et al. (2022a)</td>
<td style="text-align: center;">Pointwise</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BeaverTails QA-Classification</td>
<td style="text-align: center;">Ji et al. (2023)</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;">Instruction Tuning</td>
<td style="text-align: center;">LIMA</td>
<td style="text-align: center;">Zhou et al. (2023)</td>
<td style="text-align: center;">Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRM800K IF</td>
<td style="text-align: center;">Lightman et al. (2024)</td>
<td style="text-align: center;">Generative</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TULU-2</td>
<td style="text-align: center;">Ivison et al. (2023)</td>
<td style="text-align: center;">Generative</td>
</tr>
</tbody>
</table>
<p>Table 5: A complete list of training datasets in our FLAMe collection, including their output formats and categorized capabilities. We derive multiple tasks from certain datasets. For example, HelpSteer (Wang et al., 2023b) includes human annotations for different attributes of model responses such as Helpfulness, Correctness, Coherence, Complexity, and Verbosity, allowing us to create distinct tasks, each focused on a specific attribute.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Overall</th>
<th style="text-align: center;">LLM-FactVerify</th>
<th style="text-align: center;">Wiki-FactVerify</th>
<th style="text-align: center;">Summarization</th>
<th style="text-align: center;">Long-form QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5-turbo-0125</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">65.4</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8×7B</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">76.6</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3-70B-Instruct</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">$\mathbf{7 7 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Claude-3-Opus</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">75.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">76.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-0125</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">77.3</td>
</tr>
<tr>
<td style="text-align: left;">our models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2-24B</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-24B</td>
<td style="text-align: center;">$\mathbf{8 1 . 1}$</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">72.7</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-RM-24B</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">$\mathbf{8 2 . 6}$</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">$\mathbf{8 5 . 4}$</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-Opt-RM-24B</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">$\mathbf{8 1 . 2}$</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">74.8</td>
</tr>
</tbody>
</table>
<p>Table 6: LLM-AggreFact performance across four common use cases: LLM-FactVerify (ClaimVerify + FactCheck + Reveal), Wiki-FactVerify (WiCE), Summarization (AggreFact + TofuEval), and Long-form QA (ExpertQA + LFQA). FLAMe variants outperform all tested LLM-as-a-Judge models in three out of the four use cases. FLAMe24B achieves the highest overall performance of 81.1 , while the next-best model GPT-4-0125 scores 80.6 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Ranker</th>
<th style="text-align: center;">CodeGen-16B</th>
<th style="text-align: center;">davinci002</th>
<th style="text-align: center;">InCoder-6B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">10 code samples re-ranked in round-robin fashion</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">None</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">14.6</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-24B</td>
<td style="text-align: center;">$\mathbf{3 1 . 1}$</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">$\mathbf{2 2 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">FLAMe-RM-24B</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">$\mathbf{2 3 . 2}$</td>
<td style="text-align: center;">21.3</td>
</tr>
<tr>
<td style="text-align: left;">FLAME-Opt-RM-24B</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">16.5</td>
</tr>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">29.3</td>
</tr>
</tbody>
</table>
<p>Table 7: Pass@1 performance on the HumanEval coding benchmark (Chen et al., 2021). Re-ranking code samples with FLAMe variants significantly improves performance across models.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{19}$ We use relatively weak LLMs from Chen et al. (2023) for two main reasons: (1) to assess the potential benefits of re-ranking with FLAMe, and (2) HumanEval has been extensively used to develop newer LLMs.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{13}$ For tasks with fewer than 256 examples, we use the full evaluation set.
${ }^{14}$ We excluded the "Prior sets" of RewardBench because three out of the four datasets were used in training FLAMe.
${ }^{15}$ During training, we used only pairwise ratings from the dataset and reserved pointwise ratings for evaluation.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
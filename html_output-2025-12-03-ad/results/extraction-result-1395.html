<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1395 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1395</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1395</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-48632df62a1e1c8ed7ad04b3ffd1bc62c133b3af</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/48632df62a1e1c8ed7ad04b3ffd1bc62c133b3af" target="_blank">DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes to learn the prototypes from the recurrent states of the world model, thereby distilling temporal structures from past observations and actions into the prototypes, and proposes the resulting model, DreamerPro, which successfully combines Dreamer with prototypes, making large performance gains on the DeepMind Control suite both in the standard setting and when there are complex background distractions.</p>
                <p><strong>Paper Abstract:</strong> Top-performing Model-Based Reinforcement Learning (MBRL) agents, such as Dreamer, learn the world model by reconstructing the image observations. Hence, they often fail to discard task-irrelevant details and struggle to handle visual distractions. To address this issue, previous work has proposed to contrastively learn the world model, but the performance tends to be inferior in the absence of distractions. In this paper, we seek to enhance robustness to distractions for MBRL agents. Specifically, we consider incorporating prototypical representations, which have yielded more accurate and robust results than contrastive approaches in computer vision. However, it remains elusive how prototypical representations can benefit temporal dynamics learning in MBRL, since they treat each image independently without capturing temporal structures. To this end, we propose to learn the prototypes from the recurrent states of the world model, thereby distilling temporal structures from past observations and actions into the prototypes. The resulting model, DreamerPro, successfully combines Dreamer with prototypes, making large performance gains on the DeepMind Control suite both in the standard setting and when there are complex background distractions. Code available at https://github.com/fdeng18/dreamer-pro .</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1395.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1395.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DREAMER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer (latent imagination model / DreamerV1/V2 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL agent that learns a latent recurrent state-space world model (RSSM) by reconstructing high-dimensional observations and uses latent imagination for policy learning; prioritizes reconstruction-based auxiliary loss to shape representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DREAMER (RSSM-based latent world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent state-space model (RSSM) combining deterministic recurrent encoding h_t (GRU) and stochastic latent s_t; generative decoder reconstructs observations p(o_t | h_t, s_t), predicts rewards p(r_t | h_t, s_t), and a transition prior p(s_t | h_t). A variational encoder q(s_t | h_t, o_t) is trained by maximizing an ELBO containing observation reconstruction, reward log-likelihood, and KL to the prior. Policy learning is performed via backprop through imagined latent rollouts (latent imagination) using the fixed world model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (probabilistic recurrent state-space model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>image-based continuous control (DeepMind Control Suite) and general RL domains</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Observation reconstruction log-likelihood (pixel reconstruction loss) in ELBO; reward log-likelihood; KL divergence between posterior and prior; downstream task returns (RL performance) used indirectly as a fidelity/utility metric.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No explicit numeric reconstruction error reported in this paper; task returns reported (standard DMC Table 1) e.g. Cartpole Swingup Sparse 820±23, Cheetah Run 840±74, Cup Catch 967±3, Finger Spin 559±54, Reacher Easy 721±51, Walker Run 737±26; degraded heavily under natural background (Table 2) with e.g. Cartpole 126±16, Finger Spin 10±1.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent representation is a learned dense embedding (h_t, s_t) without explicit semantic interpretability; the model is effectively a black-box neural latent dynamics model (no claim of interpretable latent factors in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None specific in this paper; representations are learned via reconstruction objective and not inspected or visualized for interpretability here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reconstruction of high-dimensional video observations is described as computationally expensive, especially for long-range video reconstruction; exact FLOPs/parameters/time not provided. Uses standard Dreamer hyperparameters (defaults) with continuous latents and tanh_normal actor in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to reconstruction-free prototypical approaches, DREAMER's reconstruction objective wastes capacity on task-irrelevant visual detail and is computationally heavier due to pixel decoding; empirical wall-clock comparisons not provided but noted as slower due to decoder training and video reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong in standard DMC (see fidelity_performance numbers). Fails under strong natural background distractions (very low returns across tasks in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-fidelity pixel reconstruction does not guarantee robust task performance under visual distractions because reconstruction forces modeling of irrelevant background detail; however, when visual distractions are limited, reconstruction-based Dreamer gives strong sample-efficient RL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High-fidelity reconstruction encourages detailed visual modeling (including task-irrelevant pixels) which reduces task robustness and wastes capacity; removing reconstruction can improve robustness but may require alternative auxiliary losses to shape representations.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses RSSM with GRU deterministic state and stochastic latent; trains decoder for pixel reconstruction as auxiliary objective within ELBO; latent imagination for policy optimization; in the experiments this paper used continuous latents and tanh_normal actor distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically to TPC (contrastive reconstruction-free) and DREAMERPRO (prototypical reconstruction-free). DREAMER outperforms TPC on many standard tasks but fails under visual distractions; DREAMERPRO matches or exceeds DREAMER on standard tasks and outperforms DREAMER and TPC under distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests reconstruction is not necessary for policy learning (decoder not required for latent imagination) and that representation shaping should prioritize task-relevant predictable features rather than pixel-perfect fidelity; no single optimal config beyond using reconstruction-free alternatives in distraction settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1395.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1395.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent State-Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic recurrent latent dynamics model used by Dreamer family: deterministic recurrent hidden state h_t (GRU) combined with stochastic latent s_t, modeling transitions, observations, and rewards for latent rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent State-Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Probabilistic generative model p(o_{1:T}, r_{1:T} | a_{1:T}) factorized via latent states s_t and deterministic recurrence h_t = GRU(h_{t-1}, s_{t-1}, a_{t-1}); observation and reward decoders conditioned on (h_t, s_t); posterior q(s_t | h_t, o_t). Used for latent imagination and policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (probabilistic recurrent latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>image-based RL (DeepMind Control Suite) and other partially observable control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO components: observation reconstruction log-likelihood, reward log-likelihood, KL divergence between posterior and prior; downstream policy return.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Paper does not report standalone RSSM prediction error metrics (e.g., MSE). Fidelity is inferred via downstream task performance of Dreamer-based agents.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latents are learned embeddings (h_t,s_t) without explicit interpretable semantics reported; RSSM is treated as a neural black-box dynamics model in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>RSSM amortized inference + recurrent updates are moderate; major computational cost in Dreamer arises from pixel decoder training, not RSSM recurrence itself. No absolute compute numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>When paired with reconstruction-free objectives (DreamerPro) RSSM can be trained faster in wall-clock since no pixel decoder; contrasted with reconstruction-based RSSM which must train decoder and compute pixel losses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used as core world model for Dreamer and DreamerPro; performance depends on auxiliary objective used (reconstruction vs. prototypical/contrastive).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM provides a compact latent space enabling efficient latent rollouts for policy learning; utility depends on representation shaping (auxiliary losses) to focus on task-relevant, predictable features.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RSSM capacity can be diverted to modeling pixel-level noise if paired with reconstruction objective; pairing RSSM with reconstruction-free objectives (prototypes/contrastive) can prioritize task-relevant dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Deterministic GRU state h_t plus stochastic s_t; variational encoder q(s_t|h_t,o_t); used with both reconstruction and reconstruction-free representation objectives in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>RSSM is the latent dynamics backbone in Dreamer family; alternatives (e.g., simpler predictive encoders, contrastive temporal models) exist but RSSM supports long-horizon imagination used for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends pairing RSSM with auxiliary objectives that distill temporal structure (e.g., Temp loss in DreamerPro) rather than pixel reconstruction when robustness to distractions is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1395.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1395.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DREAMERPRO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerPro (Reconstruction-free Dreamer with prototypical temporal representations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reconstruction-free model-based RL agent that replaces pixel reconstruction with prototypical (SwAV-style) representations and adds a temporal prediction loss from the RSSM states to cluster assignments, improving robustness to visual distractions and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DREAMERPRO (RSSM + prototypical representations + temporal distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>World model uses the RSSM backbone but removes the pixel decoder; observation embeddings are clustered online into K trainable prototypes (SwAV-style) using a momentum encoder and Sinkhorn-Knopp to produce uniformly distributed cluster targets. Two losses train representations: SwAV loss predicting cluster assignments across augmented views, and a Temp loss predicting the same cluster assignments from the temporal latent state z_t = [h_t, s_t], thereby distilling temporal structure into prototypes. Policy learning uses latent imagination identical to Dreamer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RSSM) with prototypical/reconstruction-free representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>image-based continuous control (DeepMind Control Suite) with and without natural background distractions</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Indirect: prediction accuracy of cluster assignments (SwAV/Temp cross-entropy losses), reward log-likelihood, KL divergence; downstream task returns and NDB (Normalized Distance to Best) used to measure overall fidelity/usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Empirically strong: in standard DMC DreamerPro often matches or exceeds Dreamer (e.g., Cheetah Run 897±8 vs Dreamer 840±74; Finger Spin 811±232 vs Dreamer 559±54); mean NDB in standard DMC 0.002±0.003 (best). In natural background DMC DreamerPro outperforms baselines on 5/6 tasks (examples: Cartpole Swingup 671±42, Finger Spin 826±162, Walker Run 394±33) and mean NDB 0.036±0.096.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Prototypes provide a discrete clustered structure over embeddings which is more structured than raw latent vectors; however the paper does not present qualitative visualizations linking prototypes to semantic concepts, so interpretability is limited to cluster-level grouping.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Uses prototypical clustering (SwAV) that produces cluster assignments and prototypes which can in principle be inspected; no explicit interpretable analyses or visualizations are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Designed to be more efficient than contrastive alternatives because prototypical methods avoid very large batch sizes; training also saves cost by removing pixel decoder and reconstruction loss (especially for long videos). Hyperparameters: K=2500 prototypes, prototype dim=32, Sinkhorn iterations=3, momentum η=0.05. No absolute GPU/time numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Claimed to be computationally faster in wall-clock compared to contrastive methods requiring large batches; more efficient than reconstruction-based Dreamer by removing decoder training overhead. No quantitative runtime improvements are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Superior robustness and high returns: achieves best or comparable returns on standard DMC and significantly outperforms baselines (Dreamer and TPC) under natural background distractions (see fidelity_performance numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Replacing reconstruction with prototypical and temporal distillation yields representations that prioritize temporally predictable, task-relevant features; high task returns indicate that high pixel fidelity is not necessary for good policy learning if task-relevant structure is captured.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Eliminating reconstruction reduces computational cost and improves robustness but requires careful design of representation targets (SwAV + Temp). Naively combining SwAV without Temp underperforms (ablation shows both losses necessary). Prototype-based approach trades instance-level contrastive discrimination for cluster-level structure and avoids large-batch computational demands.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: remove pixel decoder; use SwAV-style prototypical clustering with Sinkhorn-Knopp targets produced by a momentum encoder; add Temp loss to predict cluster assignment from RSSM state; choose K = B × T in original SwAV logic but in experiments use K=2500; freeze prototypes for first 10k updates; increase reward loss weight to 1000 in natural background experiments to focus learning on task-relevant signals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to Dreamer (reconstruction-based) and TPC (contrastive reconstruction-free): DreamerPro matches or exceeds Dreamer on standard tasks and substantially outperforms both Dreamer and TPC in the presence of complex visual distractions. Unlike TPC, DreamerPro does not require large batches and is more consistent across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends using both SwAV (cross-view prototypical prediction) and the Temp loss (predicting clusters from temporal state) together; freeze prototypes briefly at start; set prototypes and batch sizing so prototypes spread (K ~ B×T), and upweight reward loss under heavy visual distractions. These choices are empirically validated via ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1395.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1395.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal Predictive Coding (TPC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reconstruction-free, contrastive-style world-model learning approach that applies temporal contrastive objectives to latent dynamics to encourage learning of predictable, controllable features for model-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Temporal predictive coding for model-based planning in latent space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Temporal Predictive Coding (contrastive reconstruction-free world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive temporal objective applied to latent dynamics: learns representations by contrasting future/past embeddings (temporal predictive coding) to capture predictable features useful for planning; used as the auxiliary objective instead of pixel reconstruction in an RSSM-style framework.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model with contrastive temporal representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>model-based planning in RL (DeepMind Control Suite experiments cited here)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Implicit: contrastive loss / mutual information surrogate; downstream RL returns used to evaluate effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In this paper, TPC underperforms Dreamer on many standard tasks (Table 1) but shows better robustness than Dreamer in natural background tasks (Table 2), though DreamerPro outperforms TPC on most distractor tasks. Example numbers: standard mean NDB 0.284±0.272 (worse than Dreamer and DreamerPro); natural background mean NDB 0.222±0.237 (better than Dreamer but worse than DreamerPro).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Contrastive embeddings are dense vectors without direct interpretable structure; no interpretability analyses are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Contrastive methods typically require large batch sizes or memory banks for effective negative sampling; the paper states TPC (contrastive) is computationally more expensive due to batch-size requirements; in their reimplementation they adjusted hyperparameters but no absolute compute numbers are given.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less efficient than prototypical (SwAV-style) methods because of larger batch-size or memory requirements; more robust than reconstruction-based Dreamer under distractions but inconsistent on standard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Mixed: performs worse than Dreamer on many standard DMC tasks but better than Dreamer in natural background setting in some tasks; overall outperformed by DreamerPro in both regimes in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Temporal contrastive objectives help extract predictable features and improve robustness to distractions relative to reconstruction, but contrastive learning can be brittle and less consistent absent distractions; requires careful tuning/batch sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Contrastive temporal methods can improve robustness but at the cost of higher computational demand (large batches) and variable performance across tasks; prototypical methods aim to mitigate these tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Temporal contrastive loss applied alongside RSSM; increased reward loss weight to 1000 in natural background reimplementation to encourage task-relevant learning (authors reimplemented TPC with newer Dreamer defaults for fair comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically here: TPC is more robust than Dreamer in distractor settings but less consistent in standard settings; DreamerPro (prototypical + Temp) shows better consistency and robustness than TPC while being computationally less demanding regarding batch size.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests contrastive methods require large batch sizes or memory banks; no single optimal config specified here, but authors reimplemented TPC with weight-1000 reward loss in distractor setting which improved consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1395.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1395.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SwAV / Prototypes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SwAV (prototypical self-supervised representation learning via online clustering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-contrastive, prototypical clustering method that learns image embeddings by matching them to learned prototypes using online clustering with Sinkhorn-Knopp to produce uniform assignments; adapted here to provide reconstruction-free targets for world model learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised learning of visual features by contrasting cluster assignments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SwAV-style prototypical representation learner (applied to world-model auxiliary objectives)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Convolutional encoder produces embeddings y_t which are projected, L2-normalized and compared to a set of trainable prototypes c_k; a momentum encoder computes scores and Sinkhorn-Knopp yields balanced cluster targets w_t; losses train encoder to predict cluster assignments across augmented views (SwAV loss) and here additionally to predict cluster targets from temporal RSSM states (Temp loss).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>representation learning module / clustering-based auxiliary objective for latent models</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>image representation learning adapted to model-based RL (DeepMind Control Suite)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Cluster assignment prediction accuracy (cross-entropy to Sinkhorn targets); indirectly RL returns and NDB measure downstream fidelity/utility.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>When integrated into DreamerPro, leads to state-of-the-art robustness and competitive performance on standard DMC; exact cluster prediction accuracies not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Prototypes are explicit trainable vectors representing cluster centers which can in principle be examined for semantic meaning; the paper does not present such visualization but prototypes provide more structured latent space than raw embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Online clustering with explicit prototypes and cluster assignments (Sinkhorn balanced assignments) — gives a mechanism to inspect cluster memberships though not explored in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Avoids large-batch contrastive training; requires computing Sinkhorn-Knopp (few iterations, e.g., 3) and maintaining a momentum encoder; hyperparameters: K=2500, proto-dim=32, Sinkhorn iterations=3, momentum η=0.05. Claimed to be more computationally efficient/wall-clock friendly than contrastive alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More efficient than contrastive approaches (which need large batches) and more computationally advantageous than pixel reconstruction (no decoder); no absolute training-time numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As the representation module in DreamerPro, yields improved downstream RL performance and robustness relative to contrastive and reconstruction baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cluster-level targets reduce need to reconstruct pixel detail and encourage embeddings that separate observations while the Temp loss aligns prototypes with temporally predictive latent states, improving task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Prototype clustering trades instance-level discrimination for cluster-level structure; requires choice of K and balanced assignment (Sinkhorn); naive SwAV without temporal distillation (Temp) underperforms in RL context.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use two augmented views consistent across time, momentum encoder to produce targets, Sinkhorn-Knopp for balanced assignments, set K (experimentally 2500), freeze prototypes early, add Temp loss predicting assignments from RSSM states.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to contrastive (TPC) and reconstruction (Dreamer): SwAV-style prototypes used in DreamerPro are more robust than contrastive counterparts under small batch sizes and more robust than reconstruction under visual distractions when combined with temporal distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends combining SwAV loss with temporal distillation (Temp loss) to capture temporal structure; choose K relative to batch (authors used K=2500) and freeze prototypes initially for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1395.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1395.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early two-stage approach to learning latent world models from pixels that trains a VAE and an RNN dynamics model, then evolves controllers in the learned latent space; mentioned as an example of reconstruction-based world models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent world models facilitate policy evolution</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (VAE + RNN latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage pipeline: learn a visual VAE to compress images into latent codes, then learn an RNN to predict latent sequences; controllers are trained (e.g., via evolution) in the latent space for policy generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE + recurrent predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>simulated control tasks and RL domains (original work applied to simple simulated environments)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>VAE reconstruction loss and RNN prediction loss; downstream policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not quantified in this paper; referenced as prior art illustrating reconstruction-based latent modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent VAE codes can be inspected; the original World Models paper showed some interpretability of latent factors, but this paper only references it as a reconstruction-based approach without presenting new interpretability analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>VAE latent visualization (in original work) but not performed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Two-stage approach separates encoder training from dynamics; pixel reconstruction cost present in VAE stage. No compute numbers given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Representative of reconstruction-based methods which can be computationally heavy due to decoder training; referenced as contrast to reconstruction-free approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Historical baseline; not directly evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Demonstrates viability of learning compact latent spaces for policy search, but subject to same drawbacks of reconstruction focusing on pixel detail.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Reconstruction-based latents may capture visual details irrelevant to control. This motivates reconstruction-free alternatives in the present work.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Two-stage VAE + RNN; used as a motivating example rather than a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted in discussion with DreamerPro and other reconstruction-free methods; DreamerPro aims to avoid the weaknesses of reconstruction-based latent models exemplified by World Models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Temporal predictive coding for model-based planning in latent space <em>(Rating: 2)</em></li>
                <li>Unsupervised learning of visual features by contrasting cluster assignments <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Recurrent world models facilitate policy evolution <em>(Rating: 1)</em></li>
                <li>DeepMDP: Learning continuous latent space models for representation learning <em>(Rating: 1)</em></li>
                <li>SOLAR: Deep structured representations for model-based reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1395",
    "paper_id": "paper-48632df62a1e1c8ed7ad04b3ffd1bc62c133b3af",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "DREAMER",
            "name_full": "Dreamer (latent imagination model / DreamerV1/V2 family)",
            "brief_description": "A model-based RL agent that learns a latent recurrent state-space world model (RSSM) by reconstructing high-dimensional observations and uses latent imagination for policy learning; prioritizes reconstruction-based auxiliary loss to shape representations.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "use",
            "model_name": "DREAMER (RSSM-based latent world model)",
            "model_description": "Recurrent state-space model (RSSM) combining deterministic recurrent encoding h_t (GRU) and stochastic latent s_t; generative decoder reconstructs observations p(o_t | h_t, s_t), predicts rewards p(r_t | h_t, s_t), and a transition prior p(s_t | h_t). A variational encoder q(s_t | h_t, o_t) is trained by maximizing an ELBO containing observation reconstruction, reward log-likelihood, and KL to the prior. Policy learning is performed via backprop through imagined latent rollouts (latent imagination) using the fixed world model.",
            "model_type": "latent world model (probabilistic recurrent state-space model)",
            "task_domain": "image-based continuous control (DeepMind Control Suite) and general RL domains",
            "fidelity_metric": "Observation reconstruction log-likelihood (pixel reconstruction loss) in ELBO; reward log-likelihood; KL divergence between posterior and prior; downstream task returns (RL performance) used indirectly as a fidelity/utility metric.",
            "fidelity_performance": "No explicit numeric reconstruction error reported in this paper; task returns reported (standard DMC Table 1) e.g. Cartpole Swingup Sparse 820±23, Cheetah Run 840±74, Cup Catch 967±3, Finger Spin 559±54, Reacher Easy 721±51, Walker Run 737±26; degraded heavily under natural background (Table 2) with e.g. Cartpole 126±16, Finger Spin 10±1.",
            "interpretability_assessment": "Latent representation is a learned dense embedding (h_t, s_t) without explicit semantic interpretability; the model is effectively a black-box neural latent dynamics model (no claim of interpretable latent factors in this paper).",
            "interpretability_method": "None specific in this paper; representations are learned via reconstruction objective and not inspected or visualized for interpretability here.",
            "computational_cost": "Reconstruction of high-dimensional video observations is described as computationally expensive, especially for long-range video reconstruction; exact FLOPs/parameters/time not provided. Uses standard Dreamer hyperparameters (defaults) with continuous latents and tanh_normal actor in experiments.",
            "efficiency_comparison": "Compared to reconstruction-free prototypical approaches, DREAMER's reconstruction objective wastes capacity on task-irrelevant visual detail and is computationally heavier due to pixel decoding; empirical wall-clock comparisons not provided but noted as slower due to decoder training and video reconstruction.",
            "task_performance": "Strong in standard DMC (see fidelity_performance numbers). Fails under strong natural background distractions (very low returns across tasks in Table 2).",
            "task_utility_analysis": "High-fidelity pixel reconstruction does not guarantee robust task performance under visual distractions because reconstruction forces modeling of irrelevant background detail; however, when visual distractions are limited, reconstruction-based Dreamer gives strong sample-efficient RL performance.",
            "tradeoffs_observed": "High-fidelity reconstruction encourages detailed visual modeling (including task-irrelevant pixels) which reduces task robustness and wastes capacity; removing reconstruction can improve robustness but may require alternative auxiliary losses to shape representations.",
            "design_choices": "Uses RSSM with GRU deterministic state and stochastic latent; trains decoder for pixel reconstruction as auxiliary objective within ELBO; latent imagination for policy optimization; in the experiments this paper used continuous latents and tanh_normal actor distribution.",
            "comparison_to_alternatives": "Compared empirically to TPC (contrastive reconstruction-free) and DREAMERPRO (prototypical reconstruction-free). DREAMER outperforms TPC on many standard tasks but fails under visual distractions; DREAMERPRO matches or exceeds DREAMER on standard tasks and outperforms DREAMER and TPC under distractors.",
            "optimal_configuration": "Paper suggests reconstruction is not necessary for policy learning (decoder not required for latent imagination) and that representation shaping should prioritize task-relevant predictable features rather than pixel-perfect fidelity; no single optimal config beyond using reconstruction-free alternatives in distraction settings.",
            "uuid": "e1395.0",
            "source_info": {
                "paper_title": "DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "RSSM",
            "name_full": "Recurrent State-Space Model (RSSM)",
            "brief_description": "A probabilistic recurrent latent dynamics model used by Dreamer family: deterministic recurrent hidden state h_t (GRU) combined with stochastic latent s_t, modeling transitions, observations, and rewards for latent rollouts.",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "use",
            "model_name": "Recurrent State-Space Model (RSSM)",
            "model_description": "Probabilistic generative model p(o_{1:T}, r_{1:T} | a_{1:T}) factorized via latent states s_t and deterministic recurrence h_t = GRU(h_{t-1}, s_{t-1}, a_{t-1}); observation and reward decoders conditioned on (h_t, s_t); posterior q(s_t | h_t, o_t). Used for latent imagination and policy learning.",
            "model_type": "latent world model (probabilistic recurrent latent dynamics)",
            "task_domain": "image-based RL (DeepMind Control Suite) and other partially observable control tasks",
            "fidelity_metric": "ELBO components: observation reconstruction log-likelihood, reward log-likelihood, KL divergence between posterior and prior; downstream policy return.",
            "fidelity_performance": "Paper does not report standalone RSSM prediction error metrics (e.g., MSE). Fidelity is inferred via downstream task performance of Dreamer-based agents.",
            "interpretability_assessment": "Latents are learned embeddings (h_t,s_t) without explicit interpretable semantics reported; RSSM is treated as a neural black-box dynamics model in this work.",
            "interpretability_method": "None described in this paper.",
            "computational_cost": "RSSM amortized inference + recurrent updates are moderate; major computational cost in Dreamer arises from pixel decoder training, not RSSM recurrence itself. No absolute compute numbers provided.",
            "efficiency_comparison": "When paired with reconstruction-free objectives (DreamerPro) RSSM can be trained faster in wall-clock since no pixel decoder; contrasted with reconstruction-based RSSM which must train decoder and compute pixel losses.",
            "task_performance": "Used as core world model for Dreamer and DreamerPro; performance depends on auxiliary objective used (reconstruction vs. prototypical/contrastive).",
            "task_utility_analysis": "RSSM provides a compact latent space enabling efficient latent rollouts for policy learning; utility depends on representation shaping (auxiliary losses) to focus on task-relevant, predictable features.",
            "tradeoffs_observed": "RSSM capacity can be diverted to modeling pixel-level noise if paired with reconstruction objective; pairing RSSM with reconstruction-free objectives (prototypes/contrastive) can prioritize task-relevant dynamics.",
            "design_choices": "Deterministic GRU state h_t plus stochastic s_t; variational encoder q(s_t|h_t,o_t); used with both reconstruction and reconstruction-free representation objectives in experiments.",
            "comparison_to_alternatives": "RSSM is the latent dynamics backbone in Dreamer family; alternatives (e.g., simpler predictive encoders, contrastive temporal models) exist but RSSM supports long-horizon imagination used for policy learning.",
            "optimal_configuration": "Paper recommends pairing RSSM with auxiliary objectives that distill temporal structure (e.g., Temp loss in DreamerPro) rather than pixel reconstruction when robustness to distractions is needed.",
            "uuid": "e1395.1",
            "source_info": {
                "paper_title": "DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "DREAMERPRO",
            "name_full": "DreamerPro (Reconstruction-free Dreamer with prototypical temporal representations)",
            "brief_description": "A reconstruction-free model-based RL agent that replaces pixel reconstruction with prototypical (SwAV-style) representations and adds a temporal prediction loss from the RSSM states to cluster assignments, improving robustness to visual distractions and sample efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DREAMERPRO (RSSM + prototypical representations + temporal distillation)",
            "model_description": "World model uses the RSSM backbone but removes the pixel decoder; observation embeddings are clustered online into K trainable prototypes (SwAV-style) using a momentum encoder and Sinkhorn-Knopp to produce uniformly distributed cluster targets. Two losses train representations: SwAV loss predicting cluster assignments across augmented views, and a Temp loss predicting the same cluster assignments from the temporal latent state z_t = [h_t, s_t], thereby distilling temporal structure into prototypes. Policy learning uses latent imagination identical to Dreamer.",
            "model_type": "latent world model (RSSM) with prototypical/reconstruction-free representation learning",
            "task_domain": "image-based continuous control (DeepMind Control Suite) with and without natural background distractions",
            "fidelity_metric": "Indirect: prediction accuracy of cluster assignments (SwAV/Temp cross-entropy losses), reward log-likelihood, KL divergence; downstream task returns and NDB (Normalized Distance to Best) used to measure overall fidelity/usefulness.",
            "fidelity_performance": "Empirically strong: in standard DMC DreamerPro often matches or exceeds Dreamer (e.g., Cheetah Run 897±8 vs Dreamer 840±74; Finger Spin 811±232 vs Dreamer 559±54); mean NDB in standard DMC 0.002±0.003 (best). In natural background DMC DreamerPro outperforms baselines on 5/6 tasks (examples: Cartpole Swingup 671±42, Finger Spin 826±162, Walker Run 394±33) and mean NDB 0.036±0.096.",
            "interpretability_assessment": "Prototypes provide a discrete clustered structure over embeddings which is more structured than raw latent vectors; however the paper does not present qualitative visualizations linking prototypes to semantic concepts, so interpretability is limited to cluster-level grouping.",
            "interpretability_method": "Uses prototypical clustering (SwAV) that produces cluster assignments and prototypes which can in principle be inspected; no explicit interpretable analyses or visualizations are reported in the paper.",
            "computational_cost": "Designed to be more efficient than contrastive alternatives because prototypical methods avoid very large batch sizes; training also saves cost by removing pixel decoder and reconstruction loss (especially for long videos). Hyperparameters: K=2500 prototypes, prototype dim=32, Sinkhorn iterations=3, momentum η=0.05. No absolute GPU/time numbers provided.",
            "efficiency_comparison": "Claimed to be computationally faster in wall-clock compared to contrastive methods requiring large batches; more efficient than reconstruction-based Dreamer by removing decoder training overhead. No quantitative runtime improvements are reported in the paper.",
            "task_performance": "Superior robustness and high returns: achieves best or comparable returns on standard DMC and significantly outperforms baselines (Dreamer and TPC) under natural background distractions (see fidelity_performance numbers).",
            "task_utility_analysis": "Replacing reconstruction with prototypical and temporal distillation yields representations that prioritize temporally predictable, task-relevant features; high task returns indicate that high pixel fidelity is not necessary for good policy learning if task-relevant structure is captured.",
            "tradeoffs_observed": "Eliminating reconstruction reduces computational cost and improves robustness but requires careful design of representation targets (SwAV + Temp). Naively combining SwAV without Temp underperforms (ablation shows both losses necessary). Prototype-based approach trades instance-level contrastive discrimination for cluster-level structure and avoids large-batch computational demands.",
            "design_choices": "Key choices: remove pixel decoder; use SwAV-style prototypical clustering with Sinkhorn-Knopp targets produced by a momentum encoder; add Temp loss to predict cluster assignment from RSSM state; choose K = B × T in original SwAV logic but in experiments use K=2500; freeze prototypes for first 10k updates; increase reward loss weight to 1000 in natural background experiments to focus learning on task-relevant signals.",
            "comparison_to_alternatives": "Compared to Dreamer (reconstruction-based) and TPC (contrastive reconstruction-free): DreamerPro matches or exceeds Dreamer on standard tasks and substantially outperforms both Dreamer and TPC in the presence of complex visual distractions. Unlike TPC, DreamerPro does not require large batches and is more consistent across tasks.",
            "optimal_configuration": "Paper recommends using both SwAV (cross-view prototypical prediction) and the Temp loss (predicting clusters from temporal state) together; freeze prototypes briefly at start; set prototypes and batch sizing so prototypes spread (K ~ B×T), and upweight reward loss under heavy visual distractions. These choices are empirically validated via ablations.",
            "uuid": "e1395.2",
            "source_info": {
                "paper_title": "DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "TPC",
            "name_full": "Temporal Predictive Coding (TPC)",
            "brief_description": "A reconstruction-free, contrastive-style world-model learning approach that applies temporal contrastive objectives to latent dynamics to encourage learning of predictable, controllable features for model-based planning.",
            "citation_title": "Temporal predictive coding for model-based planning in latent space",
            "mention_or_use": "mention",
            "model_name": "Temporal Predictive Coding (contrastive reconstruction-free world model)",
            "model_description": "Contrastive temporal objective applied to latent dynamics: learns representations by contrasting future/past embeddings (temporal predictive coding) to capture predictable features useful for planning; used as the auxiliary objective instead of pixel reconstruction in an RSSM-style framework.",
            "model_type": "latent world model with contrastive temporal representation learning",
            "task_domain": "model-based planning in RL (DeepMind Control Suite experiments cited here)",
            "fidelity_metric": "Implicit: contrastive loss / mutual information surrogate; downstream RL returns used to evaluate effectiveness.",
            "fidelity_performance": "In this paper, TPC underperforms Dreamer on many standard tasks (Table 1) but shows better robustness than Dreamer in natural background tasks (Table 2), though DreamerPro outperforms TPC on most distractor tasks. Example numbers: standard mean NDB 0.284±0.272 (worse than Dreamer and DreamerPro); natural background mean NDB 0.222±0.237 (better than Dreamer but worse than DreamerPro).",
            "interpretability_assessment": "Contrastive embeddings are dense vectors without direct interpretable structure; no interpretability analyses are reported in this paper.",
            "interpretability_method": "None reported in this paper.",
            "computational_cost": "Contrastive methods typically require large batch sizes or memory banks for effective negative sampling; the paper states TPC (contrastive) is computationally more expensive due to batch-size requirements; in their reimplementation they adjusted hyperparameters but no absolute compute numbers are given.",
            "efficiency_comparison": "Less efficient than prototypical (SwAV-style) methods because of larger batch-size or memory requirements; more robust than reconstruction-based Dreamer under distractions but inconsistent on standard tasks.",
            "task_performance": "Mixed: performs worse than Dreamer on many standard DMC tasks but better than Dreamer in natural background setting in some tasks; overall outperformed by DreamerPro in both regimes in this paper.",
            "task_utility_analysis": "Temporal contrastive objectives help extract predictable features and improve robustness to distractions relative to reconstruction, but contrastive learning can be brittle and less consistent absent distractions; requires careful tuning/batch sizes.",
            "tradeoffs_observed": "Contrastive temporal methods can improve robustness but at the cost of higher computational demand (large batches) and variable performance across tasks; prototypical methods aim to mitigate these tradeoffs.",
            "design_choices": "Temporal contrastive loss applied alongside RSSM; increased reward loss weight to 1000 in natural background reimplementation to encourage task-relevant learning (authors reimplemented TPC with newer Dreamer defaults for fair comparison).",
            "comparison_to_alternatives": "Compared empirically here: TPC is more robust than Dreamer in distractor settings but less consistent in standard settings; DreamerPro (prototypical + Temp) shows better consistency and robustness than TPC while being computationally less demanding regarding batch size.",
            "optimal_configuration": "Paper suggests contrastive methods require large batch sizes or memory banks; no single optimal config specified here, but authors reimplemented TPC with weight-1000 reward loss in distractor setting which improved consistency.",
            "uuid": "e1395.3",
            "source_info": {
                "paper_title": "DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "SwAV / Prototypes",
            "name_full": "SwAV (prototypical self-supervised representation learning via online clustering)",
            "brief_description": "A non-contrastive, prototypical clustering method that learns image embeddings by matching them to learned prototypes using online clustering with Sinkhorn-Knopp to produce uniform assignments; adapted here to provide reconstruction-free targets for world model learning.",
            "citation_title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "mention_or_use": "use",
            "model_name": "SwAV-style prototypical representation learner (applied to world-model auxiliary objectives)",
            "model_description": "Convolutional encoder produces embeddings y_t which are projected, L2-normalized and compared to a set of trainable prototypes c_k; a momentum encoder computes scores and Sinkhorn-Knopp yields balanced cluster targets w_t; losses train encoder to predict cluster assignments across augmented views (SwAV loss) and here additionally to predict cluster targets from temporal RSSM states (Temp loss).",
            "model_type": "representation learning module / clustering-based auxiliary objective for latent models",
            "task_domain": "image representation learning adapted to model-based RL (DeepMind Control Suite)",
            "fidelity_metric": "Cluster assignment prediction accuracy (cross-entropy to Sinkhorn targets); indirectly RL returns and NDB measure downstream fidelity/utility.",
            "fidelity_performance": "When integrated into DreamerPro, leads to state-of-the-art robustness and competitive performance on standard DMC; exact cluster prediction accuracies not reported numerically.",
            "interpretability_assessment": "Prototypes are explicit trainable vectors representing cluster centers which can in principle be examined for semantic meaning; the paper does not present such visualization but prototypes provide more structured latent space than raw embeddings.",
            "interpretability_method": "Online clustering with explicit prototypes and cluster assignments (Sinkhorn balanced assignments) — gives a mechanism to inspect cluster memberships though not explored in the paper.",
            "computational_cost": "Avoids large-batch contrastive training; requires computing Sinkhorn-Knopp (few iterations, e.g., 3) and maintaining a momentum encoder; hyperparameters: K=2500, proto-dim=32, Sinkhorn iterations=3, momentum η=0.05. Claimed to be more computationally efficient/wall-clock friendly than contrastive alternatives.",
            "efficiency_comparison": "More efficient than contrastive approaches (which need large batches) and more computationally advantageous than pixel reconstruction (no decoder); no absolute training-time numbers provided.",
            "task_performance": "As the representation module in DreamerPro, yields improved downstream RL performance and robustness relative to contrastive and reconstruction baselines.",
            "task_utility_analysis": "Cluster-level targets reduce need to reconstruct pixel detail and encourage embeddings that separate observations while the Temp loss aligns prototypes with temporally predictive latent states, improving task utility.",
            "tradeoffs_observed": "Prototype clustering trades instance-level discrimination for cluster-level structure; requires choice of K and balanced assignment (Sinkhorn); naive SwAV without temporal distillation (Temp) underperforms in RL context.",
            "design_choices": "Use two augmented views consistent across time, momentum encoder to produce targets, Sinkhorn-Knopp for balanced assignments, set K (experimentally 2500), freeze prototypes early, add Temp loss predicting assignments from RSSM states.",
            "comparison_to_alternatives": "Compared to contrastive (TPC) and reconstruction (Dreamer): SwAV-style prototypes used in DreamerPro are more robust than contrastive counterparts under small batch sizes and more robust than reconstruction under visual distractions when combined with temporal distillation.",
            "optimal_configuration": "Paper recommends combining SwAV loss with temporal distillation (Temp loss) to capture temporal structure; choose K relative to batch (authors used K=2500) and freeze prototypes initially for stability.",
            "uuid": "e1395.4",
            "source_info": {
                "paper_title": "DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "World Models (Ha & Schmidhuber)",
            "name_full": "Recurrent World Models (Ha & Schmidhuber)",
            "brief_description": "An early two-stage approach to learning latent world models from pixels that trains a VAE and an RNN dynamics model, then evolves controllers in the learned latent space; mentioned as an example of reconstruction-based world models.",
            "citation_title": "Recurrent world models facilitate policy evolution",
            "mention_or_use": "mention",
            "model_name": "World Models (VAE + RNN latent dynamics)",
            "model_description": "Two-stage pipeline: learn a visual VAE to compress images into latent codes, then learn an RNN to predict latent sequences; controllers are trained (e.g., via evolution) in the latent space for policy generation.",
            "model_type": "latent world model (VAE + recurrent predictor)",
            "task_domain": "simulated control tasks and RL domains (original work applied to simple simulated environments)",
            "fidelity_metric": "VAE reconstruction loss and RNN prediction loss; downstream policy performance.",
            "fidelity_performance": "Not quantified in this paper; referenced as prior art illustrating reconstruction-based latent modeling.",
            "interpretability_assessment": "Latent VAE codes can be inspected; the original World Models paper showed some interpretability of latent factors, but this paper only references it as a reconstruction-based approach without presenting new interpretability analysis.",
            "interpretability_method": "VAE latent visualization (in original work) but not performed here.",
            "computational_cost": "Two-stage approach separates encoder training from dynamics; pixel reconstruction cost present in VAE stage. No compute numbers given in this paper.",
            "efficiency_comparison": "Representative of reconstruction-based methods which can be computationally heavy due to decoder training; referenced as contrast to reconstruction-free approaches.",
            "task_performance": "Historical baseline; not directly evaluated in this paper.",
            "task_utility_analysis": "Demonstrates viability of learning compact latent spaces for policy search, but subject to same drawbacks of reconstruction focusing on pixel detail.",
            "tradeoffs_observed": "Reconstruction-based latents may capture visual details irrelevant to control. This motivates reconstruction-free alternatives in the present work.",
            "design_choices": "Two-stage VAE + RNN; used as a motivating example rather than a baseline in experiments.",
            "comparison_to_alternatives": "Contrasted in discussion with DreamerPro and other reconstruction-free methods; DreamerPro aims to avoid the weaknesses of reconstruction-based latent models exemplified by World Models.",
            "uuid": "e1395.5",
            "source_info": {
                "paper_title": "DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Temporal predictive coding for model-based planning in latent space",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "rating": 2
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "Recurrent world models facilitate policy evolution",
            "rating": 1
        },
        {
            "paper_title": "DeepMDP: Learning continuous latent space models for representation learning",
            "rating": 1
        },
        {
            "paper_title": "SOLAR: Deep structured representations for model-based reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.01745575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DREAMERPRO: RECONSTRUCTION-FREE MODEL-BASED REINFORCEMENT LEARNING WITH PROTOTYPICAL REPRESENTATIONS</h1>
<p>Fei Deng<br>Rutgers University<br>fei.deng@rutgers.edu<br>Ingook Jang<br>ETRI<br>ingook@etri.re.kr</p>
<p>Sungjin Ahn<br>Rutgers University<br>sjn.ahn@gmail.com</p>
<h4>Abstract</h4>
<p>Top-performing Model-Based Reinforcement Learning (MBRL) agents, such as DREAMER, learn the world model by reconstructing the image observations. Hence, they often fail to discard task-irrelevant details and struggle to handle visual distractions. To address this issue, previous work has proposed to contrastively learn the world model, but the performance tends to be inferior in the absence of distractions. In this paper, we seek to enhance robustness to distractions for MBRL agents. Specifically, we consider incorporating prototypical representations, which have yielded more accurate and robust results than contrastive approaches in computer vision. However, it remains elusive how prototypical representations can benefit temporal dynamics learning in MBRL, since they treat each image independently without capturing temporal structures. To this end, we propose to learn the prototypes from the recurrent states of the world model, thereby distilling temporal structures from past observations and actions into the prototypes. The resulting model, DREAMERPRO, successfully combines DREAMER with prototypes, making large performance gains on the DeepMind Control suite both in the standard setting and when there are complex background distractions. Code available at https://github.com/fdeng18/dreamer-pro.</p>
<h2>1 INTRODUCTION</h2>
<p>Model-Based Reinforcement Learning (MBRL, Sutton \&amp; Barto, 2018; Sutton, 1991) provides a solution to many problems in contemporary reinforcement learning. It improves sample efficiency by training a policy through simulations of a learned world model. Learning a world model also provides a way to efficiently represent experience data as general knowledge simulatable and reusable in arbitrary downstream tasks. In addition, it allows accurate and safe decisions via planning.</p>
<p>Among recent advances in image-based MBRL, DREAMER is particularly notable as the first MBRL model outperforming popular model-free RL algorithms with better sample efficiency in both continuous control (Hafner et al., 2020) and discrete control (Hafner et al., 2021). Unlike some previous model-based RL methods (Kaiser et al., 2019), it learns a world model that can be rolled out in a compact latent representation space instead of the high-dimensional observation space. Also, policy learning can be done efficiently via backpropagation through the differentiable dynamics model.</p>
<p>In image-based RL, the key problem is to learn low-dimensional state representation and, in the model-based case, also its forward model. Although we can learn such representation directly by maximizing the rewards (Schrittwieser et al., 2020), it is usually very slow to do this due to the reward sparsity. Instead, it is more practical to introduce auxiliary tasks providing richer learning signal to facilitate representation learning without reward (or with sparse reward) (Sutton et al., 2011; Jaderberg et al., 2016). DREAMER achieves this by learning the representation and the dynamics model in a way to reduce the reconstruction error of the observed sequences. However, reconstruction-based representation learning has limitations. First, it is computationally expensive to reconstruct the high-dimensional inputs, especially in models like DREAMER that needs to reconstruct long-range videos. Second, it wastes the representation capacity to learn even the visual</p>
<p>signals that are irrelevant to the task or unpredictable such as noisy background (Burda et al., 2018). Thus, in MBRL it is of particular interest to realize a version of DREAMER without reconstruction.</p>
<p>Recently, there have been remarkable advances in reconstruction-free representation learning in reinforcement learning (Laskin et al., 2020a;b; Yarats et al., 2021c). The currently dominant approach is via contrastive learning. This approach requires pair-wise comparisons to push apart different instances while pulling close an instance and its augmentation. Therefore, this method usually requires a large batch size (so computationally expensive) to perform accurately and robustly. An alternative is the clustering-based or prototype-based approach (Caron et al., 2020). By learning a set of clusters represented by prototypes, it replaces the instance-wise comparison by a comparison to the clusters and thereby avoids the problems of contrastive learning. This approach is shown to perform more accurately and robustly in many applications (Caron et al., 2020; 2021; Yarats et al., 2021b) than the contrastive method while also alleviating the need for maintaining a large batch size. The prototype structure can also be used to implement an exploration method (Yarats et al., 2021b).</p>
<p>However, for reconstruction-free MBRL only the contrastive approach like Temporal Predictive Coding (TPC, Nguyen et al., 2021) has been proposed so far. While TPC consistently outperforms Dreamer in the noisy background settings, for standard DeepMind Control suite (Tassa et al., 2018) it showed quite inconsistent results by performing severely worse than Dreamer on some tasks. Therefore, we hypothesize that this inconsistent behavior may be fixed if the robustness and accuracy of the prototypical representations can be realized in MBRL and further improved with the support of temporal information.</p>
<p>In this paper, we propose a reconstruction-free MBRL agent, called DreamerPro, by combining the prototypical representation learning with temporal dynamics learning. Similar to SwAV (Caron et al., 2020), by encouraging uniform cluster assignment across the batch, we implicitly pull apart the embeddings of different observations. Additionally, we let the temporal latent state to 'reconstruct' the cluster assignment of the observation, thereby relieving the world model from modeling lowlevel details. We evaluate our model on the standard setting of DeepMind Control suite, and also on a natural background setting, where the background is replaced by natural videos irrelevant to the task. The results show that the proposed model consistently outperforms previous methods.</p>
<p>The contributions of the paper are (1) the first reconstruction-free MBRL agent based on the prototypical representation and its temporal dynamics and (2) the demonstration of the consistently improved accuracy and robustness of the proposed model in comparison to a contrastive reconstructionfree MBRL agent and Dreamer for both standard and natural background DMC tasks.</p>
<h1>2 Preliminaries</h1>
<p>In this section, we briefly introduce the world model and learning algorithms used in DreamerV2 (Hafner et al., 2021) which our model builds upon. To indicate the general Dreamer framework (Hafner et al., 2020; 2021), we omit its version number in the rest of the paper.</p>
<h3>2.1 RECONSTRUCTION-BASED WORLD MODEL LEARNING</h3>
<p>Dreamer learns a recurrent state-space model (RSSM, Hafner et al., 2019) to predict forward dynamics and rewards in partially observable environments. At each time step $t$, the agent receives an image observation $o_{t}$ and a scalar reward $r_{t}$ (obtained by previous actions $a_{&lt;t}$ ). The agent then chooses an action $a_{t}$ based on its policy. The RSSM models the observations, rewards, and transitions through a probabilistic generative process:</p>
<p>$$
\begin{aligned}
p\left(o_{1: T}, r_{1: T} \mid a_{1: T}\right) &amp; =\int \prod_{t=1}^{T} p\left(o_{t} \mid s_{\leq t}, a_{&lt;t}\right) p\left(r_{t} \mid s_{\leq t}, a_{&lt;t}\right) p\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}\right) \mathrm{d} s_{1: T} \
&amp; =\int \prod_{t=1}^{T} p\left(o_{t} \mid h_{t}, s_{t}\right) p\left(r_{t} \mid h_{t}, s_{t}\right) p\left(s_{t} \mid h_{t}\right) \mathrm{d} s_{1: T}
\end{aligned}
$$</p>
<p>where the latent variables $s_{1: T}$ are the agent states, and $h_{t}=\operatorname{GRU}\left(h_{t-1}, s_{t-1}, a_{t-1}\right)$ is a deterministic encoding of $s_{&lt;t}$ and $a_{&lt;t}$. To infer the agent states from past observations and actions, a</p>
<p>variational encoder is introduced:</p>
<p>$q\left(s_{1:T} \mid o_{1: T}, a_{1: T}\right)=\prod_{t=1}^{T} q\left(s_{t} \mid s_{&lt;t}, a_{&lt;t}, o_{t}\right)=\prod_{t=1}^{T} q\left(s_{t} \mid h_{t}, o_{t}\right) \text {. }$ (3)</p>
<p>The training objective is to maximize the evidence lower bound (ELBO):</p>
<p>$$
\mathcal{J}<em t="1">{\text {DREAMER }}=\sum</em>}^{T} \mathbb{E<em t="t">{q} \underbrace{\left[\log p\left(o</em>} \mid h_{t}, s_{t}\right)\right.<em _mathrm_O="\mathrm{O">{\mathcal{J}</em>}}^{t}}+\underbrace{\log p\left(r_{t} \mid h_{t}, s_{t}\right)<em _mathrm_R="\mathrm{R">{\mathcal{J}</em>}}^{t}}-\underbrace{D_{\mathrm{KL}}\left(q\left(s_{t} \mid h_{t}, o_{t}\right) | p\left(s_{t} \mid h_{t}\right)\right)<em _mathrm_KL="\mathrm{KL">{\mathcal{J}</em>
$$}}^{t}</p>
<h1>2.2 Policy learning by latent imagination</h1>
<p>DREAMER interleaves policy learning with world model learning. During policy learning, the world model is fixed, and an actor and a critic are trained cooperatively from the latent trajectories imagined by the world model. Specifically, the imagination starts at each non-terminal state $\hat{z}<em t="t">{t}=\left[h</em>}, s_{t}\right]$ encountered during world model learning. Then, at each imagination step $t^{\prime} \geq t$, an action is sampled from the actor's stochastic policy: $\hat{a<em t_prime="t^{\prime">{t^{\prime}} \sim \pi\left(\hat{a}</em>}} \mid \hat{z<em t_prime="t^{\prime">{t^{\prime}}\right)$. The corresponding reward $\hat{r}</em>$ are predicted by the learned world model. Given the imagined trajectories, the actor improves its policy by maximizing the $\lambda$-return (Sutton \&amp; Barto, 2018; Schulman et al., 2018) plus an entropy regularizer that encourages exploration, while the critic is trained to approximate the $\lambda$-return through a squared loss.}+1}$ and next state $\hat{z}_{t^{\prime}+1</p>
<h2>3 DreAMerPro</h2>
<p>To compute the Dreamer training objective, more specifically $\mathcal{J}<em t="t">{\mathrm{O}}^{t}$ in Equation 4, a decoder is required to reconstruct the image observation $o</em>\right]$. Because this reconstruction loss operates in pixel space where all pixels are weighted equally, Dreamer tends to allocate most of its capacity to modeling complex visual patterns that cover a large pixel area (e.g., backgrounds). This leads to poor task performance when those visual patterns are task irrelevant, as shown in previous work (Nguyen et al., 2021).}$ from the state $z_{t}=\left[h_{t}, s_{t</p>
<p>Fortunately, during policy learning, what we need is accurate reward and next state prediction, which are respectively encouraged by $\mathcal{J}<em _mathrm_KL="\mathrm{KL">{\mathrm{R}}^{t}$ and $\mathcal{J}</em>}}^{t}$. In other words, the decoder is not required for policy learning. The main purpose of having the decoder and the associated loss $\mathcal{J<em _mathrm_R="\mathrm{R">{\mathrm{O}}^{t}$, as shown in DREAMER, is to learn meaningful representations that cannot be obtained by $\mathcal{J}</em>$ alone.}}^{t}$ and $\mathcal{J}_{\mathrm{KL}}^{t</p>
<p>The above observations motivate us to improve robustness to visual distractions by replacing the reconstruction-based representation learning in DreAMER with reconstruction-free methods. For this, we take inspiration from recent developments in self-supervised image representation learning, which can be divided into contrastive (van den Oord et al., 2019; Chen et al., 2020; He et al., 2020) and non-contrastive (Grill et al., 2020; Caron et al., 2020) methods. We prefer non-contrastive methods as they can be applied to small batch sizes. This can speed up both world model learning and policy learning (in wall clock time). Therefore, we propose to combine DreAMER with the prototypical representations used in SwAV (Caron et al., 2020), a top-performing non-contrastive representation learning method. We name the resulting model DreAMerPro, and provide the model description in the following.</p>
<p>DREAMERPro uses the same policy learning algorithm as DreAMER, but learns the world model without reconstructing the observations. This is achieved by clustering the observation into a set of $K$ trainable prototypes $\left{c_{1}, \ldots, c_{K}\right}$, and then predicting the cluster assignment from the state as well as an augmented view of the observation. See Figure 1 for an illustration.</p>
<p>Concretely, given a sequence of observations $o_{1: T}$ sampled from the replay buffer, we obtain two augmented views $o_{1: T}^{(1)}, o_{1: T}^{(2)}$ by applying random shifts (Laskin et al., 2020b; Yarats et al., 2021c) with bilinear interpolation (Yarats et al., 2021a). We ensure that the augmentation is consistent across time steps. Each view $i \in{1,2}$ is fed to the RSSM to obtain the states $z_{1: T}^{(i)}$. To predict the cluster assignment from $z_{t}^{(i)}$, we first apply a linear projection followed by $\ell_{2}$-normalization to obtain a vector $x_{t}^{(i)}$ of the same dimension as the prototypes, and then take a softmax over the dot</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: DreamerPro learns the world model through online clustering, eliminating the need for reconstruction. At each time step $t$, it first compares the observation to a set of trainable prototypes $\left{c_{1}, \ldots, c_{K}\right}$ to obtain the target cluster assignment $w_{t}$. Then, it predicts this target from both the world model state $z_{t}$ and another augmented view of the observation. The predictions are improved by optimizing the two objective terms, $\mathcal{J}<em _SwAV="{SwAV" _text="\text">{\text {Temp }}^{t}$ and $\mathcal{J}</em>$ into the prototypes.}}^{t}$, respectively, where the first term crucially distills temporal structures from $z_{t</p>
<p>products of $x_{t}^{(i)}$ and all the prototypes:</p>
<p>$$
\left(u_{t, 1}^{(i)}, \ldots, u_{t, K}^{(i)}\right)=\operatorname{softmax}\left(\frac{x_{t}^{(i)} \cdot c_{1}}{\tau}, \ldots, \frac{x_{t}^{(i)} \cdot c_{K}}{\tau}\right)
$$</p>
<p>Here, $u_{t, k}^{(i)}$ is the predicted probability that state $z_{t}^{(i)}$ maps to cluster $k, \tau$ is a temperature parameter, and the prototypes $\left{c_{1}, \ldots, c_{K}\right}$ are also $\ell_{2}$-normalized.</p>
<p>Analogously, to predict the cluster assignment from an augmented observation $o_{t}^{(i)}$, we feed it to a convolutional encoder (shared with the RSSM), apply a linear projection followed by $\ell_{2}$ normalization, and obtain a vector $y_{t}^{(i)}$. We summarize this process as: $y_{t}^{(i)}=f_{\theta}\left(o_{t}^{(i)}\right)$, where $\theta$ collectively denotes the parameters of the convolutional encoder and the linear projection layer. The prediction probabilities are again given by a softmax:</p>
<p>$$
\left(v_{t, 1}^{(i)}, \ldots, v_{t, K}^{(i)}\right)=\operatorname{softmax}\left(\frac{y_{t}^{(i)} \cdot c_{1}}{\tau}, \ldots, \frac{y_{t}^{(i)} \cdot c_{K}}{\tau}\right)
$$</p>
<p>where $v_{t, k}^{(i)}$ is the predicted probability that observation $o_{t}^{(i)}$ maps to cluster $k$.
To obtain the targets for the above two predictions (i.e., Equations 5 and 6), we apply the SinkhornKnopp algorithm (Cuturi, 2013) to the cluster assignment scores computed from the output of a momentum encoder $f_{\theta}$ (He et al., 2020; Grill et al., 2020; Caron et al., 2021), whose parameters $\hat{\theta}$ are updated using the exponential moving average of $\theta: \hat{\theta} \leftarrow(1-\eta) \hat{\theta}+\eta \theta$. For each observation $o_{t}^{(i)}$, the scores are given by the dot products $\left(\hat{y}<em 1="1">{t}^{(i)} \cdot c</em>}, \ldots, \hat{y<em K="K">{t}^{(i)} \cdot c</em>}\right)$, where $\hat{y<em _hat_theta="\hat{\theta">{t}^{(i)}=f</em>$.}}\left(o_{t}^{(i)}\right)$ is the momentum encoder output. The Sinkhorn-Knopp algorithm is applied to the two augmented batches $\left{o_{1: T}^{(1)}\right},\left{o_{1: T}^{(2)}\right}$ separately to encourage uniform cluster assignment within each augmented batch and avoid trivial solutions. We specifically choose the number of prototypes $K=B \times T$, where $B$ is the batch size, so that the observation embeddings are implicitly pushed apart from each other. The outcome of the Sinkhorn-Knopp algorithm is a set of cluster assignment targets $\left(w_{t, 1}^{(i)}, \ldots, w_{t, K}^{(i)}\right)$ for each observation $o_{t}^{(i)</p>
<p>Now that we have the cluster assignment predictions and targets, the representation learning objective is simply to maximize the prediction accuracies:</p>
<p>$\mathcal{J}<em k="1">{\text{SwAV}}^{t}=\frac{1}{2}\sum</em>\right),$ (7)
$\mathcal{J}}^{K}\left(w_{t,k}^{(1)}\log v_{t,k}^{(2)}+w_{t,k}^{(2)}\log v_{t,k}^{(1)<em k="1">{\text{Temp}}^{t}=\frac{1}{2}\sum</em>\right).$ (8)}^{K}\left(w_{t,k}^{(1)}\log u_{t,k}^{(1)}+w_{t,k}^{(2)}\log u_{t,k}^{(2)</p>
<p>Here, $\mathcal{J}<em _Temp="{Temp" _text="\text">{\text {SwAV }}^{t}$ improves prediction from an augmented view. This is the same loss as used in SwAV (Caron et al., 2020), and is shown to induce useful features for static images. However, it ignores the temporal structure which is crucial in reinforcement learning. Hence, we add a second term, $\mathcal{J}</em>}}^{t}$, that improves prediction from the state of the same view. This has the effect of making the prototypes close to the states that summarize the past observations and actions, thereby distilling temporal structure into the prototypes. From another perspective, $\mathcal{J<em _mathrm_O="\mathrm{O">{\text {Temp }}^{t}$ is similar to $\mathcal{J}</em>$ in the sense that we are now 'reconstructing' the cluster assignment of the observation instead of the observation itself. This frees the world model from modeling complex visual details, allowing more capacity to be devoted to task-relevant features.}}^{t</p>
<p>The overall world model learning objective for DREAMERPRO can be obtained by replacing $\mathcal{J}<em _SwAV="{SwAV" _text="\text">{\mathrm{O}}^{t}$ in Equation 4 with $\mathcal{J}</em>$ :}}^{t}+\mathcal{J}_{\text {Temp }}^{t</p>
<p>$$
\mathcal{J}<em t="1">{\text {DREAMERPRO }}=\sum</em>}^{T} \mathbb{E<em _SwAV="{SwAV" _text="\text">{q}\left|\mathcal{J}</em>}}^{t}+\mathcal{J<em _mathrm_R="\mathrm{R">{\text {Temp }}^{t}+\mathcal{J}</em>\right|
$$}}^{t}-\mathcal{J}_{\mathrm{KL}}^{t</p>
<p>where $\mathcal{J}<em _mathrm_KL="\mathrm{KL">{\mathrm{R}}^{t}$ and $\mathcal{J}</em>$ are now averaged over the two augmented views.}}^{t</p>
<h1>4 EXPERIMENTS</h1>
<p>Environments. We evaluate our model and the baselines on six image-based continuous control tasks from the DeepMind Control (DMC) suite (Tassa et al., 2018). We choose the set of tasks based on those considered in PlaNet (Hafner et al., 2019). Specifically, we replace Cartpole Swingup and Walker Walk with their more challenging counterparts, Cartpole Swingup Sparse and Walker Run, and keep the remaining tasks. In addition to the standard setting, we also consider a natural background setting (Zhang et al., 2021; Nguyen et al., 2021), where the background is replaced by task-irrelevant natural videos randomly sampled from the 'driving car' class in the Kinetics 400 dataset (Kay et al., 2017). Following TPC (Nguyen et al., 2021), we use two separate sets of background videos for training and evaluation. Hence, the natural background setting tests generalization to unseen distractions. We note that the recently released Distracting Control Suite (DCS, Stone et al., 2021) serves a similar purpose. However, the background distractions in DCS seem less challenging, as there are fewer videos and the ground plane is made visible for most tasks. In our preliminary experiments, our model and all the baselines achieved close to zero returns on Cartpole Swingup Sparse in the natural background setting. We therefore switch back to Cartpole Swingup in this setting.
Baselines. Our main baselines are DREAMER (Hafner et al., 2021) and TPC (Nguyen et al., 2021), the state-of-the-art for reconstruction-based and reconstruction-free model-based reinforcement learning, respectively. In particular, TPC has shown better performance than CVRL (Ma et al., 2020) and DBC (Zhang et al., 2021) on the same datasets. The recently proposed PSE (Agarwal et al., 2021) has demonstrated impressive results on DCS. However, it is only shown to work in the model-free setting and requires a pretrained policy, while our model learns both the world model and the policy from scratch.
Implementation details. We implement our model based on a newer version of DREAMER ${ }^{1}$, while the official implementation of $\mathrm{TPC}^{2}$ is based on an older version. For fair comparison, we reimplement TPC based on the newer version. We adopt the default values for the DREAMER hyperparameters, except that we use continuous latents and tanh_normal as the distribution output</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance curves in standard DMC. While TPC underperforms DREAMER on most tasks, DREAMERPRO greatly outperforms DREAMER on Finger Spin and Reacher Easy, achieves better data efficiency on Cup Catch, and is comparable or better than DREAMER on other tasks.</p>
<p>Table 1: Final performance in standard DMC.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">DREAMER</th>
<th style="text-align: center;">TPC</th>
<th style="text-align: center;">DREAMERPRO</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Cartpole Swingup Sparse</td>
<td style="text-align: center;">$\mathbf{8 2 0} \pm \mathbf{2 3}$</td>
<td style="text-align: center;">$770 \pm 9$</td>
<td style="text-align: center;">$\mathbf{8 1 3} \pm \mathbf{3 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Cheetah Run</td>
<td style="text-align: center;">$840 \pm 74$</td>
<td style="text-align: center;">$782 \pm 82$</td>
<td style="text-align: center;">$\mathbf{8 9 7} \pm \mathbf{8}$</td>
</tr>
<tr>
<td style="text-align: left;">Cup Catch</td>
<td style="text-align: center;">$\mathbf{9 6 7} \pm \mathbf{3}$</td>
<td style="text-align: center;">$948 \pm 7$</td>
<td style="text-align: center;">$\mathbf{9 6 1} \pm \mathbf{1 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Finger Spin</td>
<td style="text-align: center;">$559 \pm 54$</td>
<td style="text-align: center;">$524 \pm 127$</td>
<td style="text-align: center;">$\mathbf{8 1 1} \pm \mathbf{2 3 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Reacher Easy</td>
<td style="text-align: center;">$721 \pm 51$</td>
<td style="text-align: center;">$503 \pm 185$</td>
<td style="text-align: center;">$\mathbf{8 7 3} \pm \mathbf{1 2 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Walker Run</td>
<td style="text-align: center;">$737 \pm 26$</td>
<td style="text-align: center;">$222 \pm 29$</td>
<td style="text-align: center;">$\mathbf{7 8 4} \pm \mathbf{2 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Mean and STD of NDB $(\downarrow)$</td>
<td style="text-align: center;">$0.101 \pm 0.12$</td>
<td style="text-align: center;">$0.284 \pm 0.272$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 2} \pm \mathbf{0 . 0 0 3}$</td>
</tr>
</tbody>
</table>
<p>by the actor. We find these changes improve DREAMER's performance in the standard DMC, and therefore use these values for all models in both the standard and the natural background setting. Following TPC, we increase the weight of the reward loss $\mathcal{J}_{\mathrm{R}}^{t}$ to 1000 for all models in the natural background setting to further encourage extraction of task-relevant information. While in the original TPC, this weight is chosen separately for each task from ${100,1000}$, we find the weight of 1000 works consistently better in our re-implementation, which also obtains better results than reported in the original paper.</p>
<p>Evaluation protocol. For each task, we train each model for 1 M environment steps (equivalent to 500 K actor steps, as the action repeat is set to 2 ). The evaluation return is computed every 10 K steps, and averaged over 10 episodes. In all figures and tables, the mean and standard deviation are computed from 3 independent runs. In addition to the episode returns, we also report the Normalized Distance to the Best (NDB). Given the performance $x_{\tau, \alpha}$ of an algorithm $\alpha$ on a task $\tau$, the NDB $\beta_{\tau, \alpha}$ is defined as:</p>
<p>$$
\beta_{\tau, \alpha}=\frac{\max <em _hat_alpha="\hat{\alpha" _tau_="\tau,">{\hat{\alpha}}\left(x</em>{\max }}\right)-x_{\tau, \alpha}<em _hat_alpha="\hat{\alpha" _tau_="\tau,">{\hat{\alpha}}\left(x</em>
$$}}\right)</p>
<p>We report the mean and variance of $\beta_{\tau, \alpha}$ for each algorithm $\alpha$ over all tasks $\tau$, which indicate the bestness and consistency of algorithm $\alpha$, respectively. An optimal algorithm that consistently achieves the best performance across all tasks will have zero mean and zero variance.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance curves in natural background DMC. DreamerPro significantly outperforms TPC on Cartpole Swingup, Finger Spin, Reacher Easy, and Walker Run, while Dreamer completely fails on all tasks.</p>
<p>Table 2: Final performance in natural background DMC.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">DREAMER</th>
<th style="text-align: center;">TPC</th>
<th style="text-align: center;">DREAMERPRO</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Cartpole Swingup</td>
<td style="text-align: center;">$126 \pm 16$</td>
<td style="text-align: center;">$521 \pm 80$</td>
<td style="text-align: center;">$\mathbf{6 7 1} \pm \mathbf{4 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Cheetah Run</td>
<td style="text-align: center;">$30 \pm 2$</td>
<td style="text-align: center;">$\mathbf{4 4 4} \pm \mathbf{3 5}$</td>
<td style="text-align: center;">$349 \pm 61$</td>
</tr>
<tr>
<td style="text-align: left;">Cup Catch</td>
<td style="text-align: center;">$88 \pm 73$</td>
<td style="text-align: center;">$477 \pm 175$</td>
<td style="text-align: center;">$\mathbf{4 9 3} \pm \mathbf{1 0 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Finger Spin</td>
<td style="text-align: center;">$10 \pm 1$</td>
<td style="text-align: center;">$655 \pm 133$</td>
<td style="text-align: center;">$\mathbf{8 2 6} \pm \mathbf{1 6 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Reacher Easy</td>
<td style="text-align: center;">$82 \pm 39$</td>
<td style="text-align: center;">$462 \pm 130$</td>
<td style="text-align: center;">$\mathbf{6 4 1} \pm \mathbf{1 2 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Walker Run</td>
<td style="text-align: center;">$35 \pm 4$</td>
<td style="text-align: center;">$161 \pm 6$</td>
<td style="text-align: center;">$\mathbf{3 9 4} \pm \mathbf{3 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Mean and STD of NDB $(\downarrow)$</td>
<td style="text-align: center;">$0.890 \pm 0.063$</td>
<td style="text-align: center;">$0.222 \pm 0.237$</td>
<td style="text-align: center;">$\mathbf{0 . 0 3 6} \pm \mathbf{0 . 0 9 6}$</td>
</tr>
</tbody>
</table>
<h1>4.1 PERFORMANCE IN STANDARD DMC</h1>
<p>We show the performance curves in Figure 2 and the final performance in Table 1 for the standard setting. In contrast to TPC which underperforms DREAMER on most tasks (most severely on Reacher Easy and Walker Run), DREAMERPRO achieves comparable or even better performance than DREAMER on all tasks. Notably, DREAMERPRO outperforms DREAMER by a large margin on Finger Spin and Reacher Easy, and demonstrates better data efficiency on Cup Catch. We notice a large variance in DREAMERPRO's performance on Finger Spin. Further investigation reveals that DREAMERPRO learned close to optimal behavior (with average episode returns above 950) on two of the seeds, while converged to a suboptimal behavior (with average episode returns around 500) on the other seed. The low variance of DREAMER indicates that it hardly achieved close to optimal behavior. Our results suggest for the first time that prototypical representations (and reconstructionfree representation learning in general) can be beneficial to MBRL even in the absence of strong visual distractions.</p>
<h3>4.2 PERFORMANCE IN NATURAL BACKGROUND DMC</h3>
<p>Figure 3 and Table 2 respectively show the performance curves and final evaluation returns obtained by all models in the natural background setting. DREAMER completely fails on all tasks, showing the inability of reconstruction-based representation learning to deal with complex visual distractions. In</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ablation study. Both $\mathcal{J}<em _Temp="{Temp" _text="\text">{\text {SwAV }}^{t}$ and $\mathcal{J}</em>$ are necessary for achieving good performance.
contrast, DreamerPro achieves the best performance on 5 out of 6 tasks, with large performance gains from TPC on Cartpole Swingup, Finger Spin, Reacher Easy, and Walker Run. These results indicate that the advantage of prototypical representations over contrastive learning in computer vision can indeed be transferred to MBRL for better robustness to visual distractions.}}^{t</p>
<h1>4.3 Ablation Study</h1>
<p>We now show the individual effect of the two loss terms, $\mathcal{J}<em _Temp="{Temp" _text="\text">{\text {SwAV }}^{t}$ and $\mathcal{J}</em>$ alone is not sufficient to provide meaningful cluster assignment targets and learning signals for the convolutional encoder.}}^{t}$, in Figure 4. Here, each of the ablated versions, DreamerPro-No-SwAV and DreamerPro-No-Temp, removes one of the loss terms. We did not investigate removing both terms, as its failure has been shown in DREAMER (Hafner et al., 2020). We observe that both terms are necessary for achieving good performance. In particular, naively combining SwAV with DREAMER (i.e., DreamerPro-No-Temp) leads to inferior performance, as it ignores the temporal structure. On the other hand, $\mathcal{J}_{\text {Temp }}^{t</p>
<h2>5 RELATED WORK</h2>
<p>Self-supervised representation learning for static images. Recent works in self-supervised learning have shown its effectiveness in learning representations from high-dimensional data. CPC (van den Oord et al., 2019) learns representations by maximizing the mutual information between the encoded representations and its future prediction using noise-contrastive estimation. SimCLR (Chen et al., 2020) shows that the contrastive data can be generated using the data in the training mini-batch by applying random augmentations. MoCo (He et al., 2020), on the other hand, improves the contrastive training by generating the representations from a momentum encoder instead of the trained network. Despite the success in some tasks, one weakness of the contrastive approaches is that it require the model to compare a larger amount of samples, which demands large batch sizes or memory banks. To address this problem, some works propose to learn the image representations without discriminating between samples. Particularly, BYOL (Grill et al., 2020) introduces a momentum encoder to provide target representations for the training network. SwAV (Caron et al., 2020) proposes to learn the embeddings by matching them to a set of learned clusters. DINO (Caron et al., 2021) replaces the clusters in SwAV with categorical heads and uses the centering and sharpening technique to prevent representations collapsing. Unlike our model, these works treat each image independently and ignore the temporal structure of the environment, which is crucial in learning the forward dynamics and policy in MBRL.</p>
<p>Representation learning for model-free reinforcement learning. It has been shown that adopting data augmentation techniques like random shifts in the observation space enables robust learning from pixel input in any model-free reinforcement learning algorithm (Laskin et al., 2020b; Yarats et al., 2021c;a). Recent works have also shown that self-supervised representation learning techniques can bring significant improvement to reinforcement learning methods. For example, CURL (Laskin et al., 2020a) performs contrastive learning along with off-policy RL algorithms and shows that it significantly improves sample-efficiency and model performance over pixel-based methods. Other works aim to improve the representation learning quality by combining temporal</p>
<p>prediction models in the representation learning process (Schwarzer et al., 2021a;b; Stooke et al., 2021; Yarats et al., 2021b; Guo et al., 2020; Gregor et al., 2019). However, the main purpose of the temporal prediction models in these works is mainly to obtain the abstract representations of the observations, and they are not shown to support long-horizon imagination.</p>
<p>Model-based reinforcement learning with reconstruction. Model based reinforcement learning from raw pixel data can learn the representation space by minimizing the observation reconstruction loss. World Models (Ha \&amp; Schmidhuber, 2018) learn the latent dynamics of the environment in a two-stage process to evolve their linear controllers in imagination. SOLAR (Zhang et al., 2019) models the dynamics as time-varying linear-Gaussian and solves robotic tasks via guided policy search. Dreamer (Hafner et al., 2020) jointly learns the RSSM and latent state space from observation reconstruction loss. DeepMDP (Gelada et al., 2019) also propose a latent dynamics modelbased method that uses bisimulation metrics and reconstruction loss in Atari. However, reconstruction based methods are susceptible to noise and objects irrelevant to the task in the environment (Nguyen et al., 2021). Furthermore, in a few cases, the latent representation fails to reconstruct small task-relevant objects in the environment (Okada \&amp; Taniguchi, 2021).</p>
<p>Reinforcement learning under visual distractions. A large body of works on robust representation learning focuses on contrastive objectives. For example, CVRL (Ma et al., 2020) proposes to learn representations from complex observations by maximizing the mutual information between an image and its corresponding embedding using contrastive objectives. However, the learning objective of CVRL encourages the representation model to learn as much information as possible, including task-irrelevant information. Dreaming (Okada \&amp; Taniguchi, 2021) and TPC (Nguyen et al., 2021) tackle this problem by incorporating a dynamic model and applying contrastive learning in the temporal dimension, which encourages the model to capture controllable and predictable information in the latent space. Bisimulation metrics method such as DBC (Zhang et al., 2021) and PSE (Agarwal et al., 2021) is another type of representation learning robust to visual distractions. Using the bisimulation metrics that quantify the behavioral similarity between states, these methods make the mode robust to task-irrelevant information. However, DBC cannot generalize to unseen backgrounds (Nguyen et al., 2021), and PSE is only shown to work in the model-free setting and requires a pre-trained policy to compute the similarity metrics, while our model learns both the world model and the policy from scratch.</p>
<h1>6 CONCLUSION</h1>
<p>In this work, we presented the first reconstruction-free MBRL agent based on the prototypical representation and its temporal dynamics. In experiments, we demonstrated the consistently improved accuracy and robustness of the proposed model in comparison to the Temporal Predictive Coding (TPC) agent and the Dreamer agent for both standard and natural background DMC tasks. Our results suggest that there are unexplored broad areas in reconstruction-free MBRL. Interesting future directions are to apply this model on Atari games and to investigate the possibility of learning hierarchical structures such as skills without reconstruction.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work was supported by Electronics and Telecommunications Research Institute (ETRI) grant funded by the Korean government [21ZR1100, A Study of Hyper-Connected Thinking Internet Technology by autonomous connecting, controlling and evolving ways]. The authors would like to thank Tung Nguyen for help with implementing TPC, Jindong Jiang and Ishani Ghose for editing the related work section, and Chang Chen, Parikshit Bansal, and Jaesik Yoon for fruitful discussion.</p>
<h2>REFERENCES</h2>
<p>Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive behavioral similarity embeddings for generalization in reinforcement learning. In International Conference on Learning Representations, 2021.</p>
<p>Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.</p>
<p>Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9912-9924. Curran Associates, Inc., 2020.</p>
<p>Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers, 2021.</p>
<p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 1597-1607. PMLR, 13-18 Jul 2020.</p>
<p>Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.</p>
<p>Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Bellemare. DeepMDP: Learning continuous latent space models for representation learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2170-2179. PMLR, 09-15 Jun 2019.</p>
<p>Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, and Aaron van den Oord. Shaping belief states with generative environment models for rl. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.</p>
<p>Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21271-21284. Curran Associates, Inc., 2020.</p>
<p>Zhaohan Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altché, Remi Munos, and Mohammad Gheshlaghi Azar. Bootstrap latent-predictive representations for multitask reinforcement learning. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 3875-3886. PMLR, 13-18 Jul 2020.</p>
<p>David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.</p>
<p>Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2555-2565. PMLR, 09-15 Jun 2019.</p>
<p>Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020.</p>
<p>Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021.</p>
<p>Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.</p>
<p>Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.</p>
<p>Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.</p>
<p>Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset, 2017.</p>
<p>Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5639-5650. PMLR, 13-18 Jul 2020a.</p>
<p>Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19884-19895. Curran Associates, Inc., 2020b.</p>
<p>Xiao Ma, Siwei Chen, David Hsu, and Wee Sun Lee. Contrastive variational reinforcement learning for complex observations, 2020.</p>
<p>Tung D Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Temporal predictive coding for model-based planning in latent space. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8130-8139. PMLR, 18-24 Jul 2021.</p>
<p>Masashi Okada and Tadahiro Taniguchi. Dreaming: Model-based reinforcement learning by latent imagination without reconstruction, 2021.</p>
<p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.</p>
<p>John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation, 2018.</p>
<p>Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2021a.</p>
<p>Max Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, Devon Hjelm, Philip Bachman, and Aaron Courville. Pretraining representations for data-efficient reinforcement learning, 2021b.</p>
<p>Austin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski. The distracting control suite - a challenging benchmark for reinforcement learning from pixels, 2021.</p>
<p>Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from reinforcement learning. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9870-9879. PMLR, 18-24 Jul 2021.</p>
<p>Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160-163, 1991.</p>
<p>Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pp. 761-768, 2011.</p>
<p>Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller. Deepmind control suite, 2018.</p>
<p>Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding, 2019.</p>
<p>Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning, 2021a.</p>
<p>Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with prototypical representations. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 11920-11931. PMLR, 18-24 Jul 2021b.</p>
<p>Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2021c.</p>
<p>Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In International Conference on Learning Representations, 2021.</p>
<p>Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, and Sergey Levine. SOLAR: Deep structured representations for model-based reinforcement learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7444-7453. PMLR, 09-15 Jun 2019.</p>
<h1>A HYPERPARAMETERS</h1>
<p>For hyperparameters that are shared with DREAMER, we use the default values suggested in the config file in the official implementation of DREAMER, with the following two exceptions. We set rssm.discrete $=$ False and actor.dist $=$ tanh.normal, as we find these changes improve performance over the default setting. The additional hyperparameters introduced in DreamerPro are listed in Table 3. We find it helpful to freeze the prototypes for the first 10 K gradient updates. In the natural background setting, we add a squared loss that encourages the $\ell_{2}$-norm of projections (before $\ell_{2}$-normalization) to be close to 1 . This helps stabilize the model.</p>
<p>Table 3: Additional hyperparameters in DREAMERPRO.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: right;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Number of prototypes $K$</td>
<td style="text-align: right;">2500</td>
</tr>
<tr>
<td style="text-align: left;">Prototype dimension</td>
<td style="text-align: right;">32</td>
</tr>
<tr>
<td style="text-align: left;">Softmax temperature $\tau$</td>
<td style="text-align: right;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Sinkhorn iterations</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: left;">Sinkhorn epsilon</td>
<td style="text-align: right;">0.0125</td>
</tr>
<tr>
<td style="text-align: left;">Momentum update fraction $\eta$</td>
<td style="text-align: right;">0.05</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/danijar/dreamerv2/tree/e783832f01b2c845c195587158c4 e129edabaebb
${ }^{2}$ https://github.com/VinAIResearch/TPC-tensorflow&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8971 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8971</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8971</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-6462f367550af0dab4eda55530c854743469c1d6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6462f367550af0dab4eda55530c854743469c1d6" target="_blank">Self-supervised Graph Masking Pre-training for Graph-to-Text Generation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes graph masking pre-training strategies that neither require supervision signals nor adjust the architecture of the underlying pre-trained encoder-decoder model to achieve new state-of-the-art results on WebNLG+2020 and EventNarrative G2T generation datasets.</p>
                <p><strong>Paper Abstract:</strong> Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text (G2T) generation by processing the linearised version of a graph. However, the linearisation is known to ignore the structural information. Additionally, PLMs are typically pre-trained on free text which introduces domain mismatch between pre-training and downstream G2T generation tasks. To address these shortcomings, we propose graph masking pre-training strategies that neither require supervision signals nor adjust the architecture of the underlying pre-trained encoder-decoder model. When used with a pre-trained T5, our approach achieves new state-of-the-art results on WebNLG+2020 and EventNarrative G2T generation datasets. Our method also shows to be very effective in the low-resource setting.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8971.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8971.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Level-augmented linearisation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearisation of graphs into triples with role tokens and level-marker augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A serialization of graph-structured data into a sequence of triples [S|head, P|relation, O|tail, l] where each element is explicitly tagged with role tokens and augmented by a level marker l indicating the object's distance from the graph root; intended to provide a weak structural signal while keeping input compatible with off-the-shelf pretrained encoder-decoder LMs (T5).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearisation (+ level markers)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph is converted into a set/sequence of triples of the form [S|head, P|relation, O|tail] with explicit role prefix tokens (S|, P|, O|). Every triple is additionally augmented with a level marker l that indicates the distance (level) of the tail/object node from the designated root node, yielding [S|head, P|relation, O|tail, l]. The linearised triples are concatenated into a text-like input for a text-to-text PLM (T5) without modifying model architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF-style triple graphs (DBpedia, Wikidata, EventKG, heterogenous structured records like DART)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize the graph into triple records (one per edge). Prepend role tokens S|, P|, O| to the subject, predicate and object. Compute the object's distance from the root node and append this as a numeric level marker l to the triple. Concatenate triples into the encoder input sequence for T5; no extra embeddings or architectural changes are introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (convert KG triples into natural-language descriptions) used on WebNLG+2020, DART, EventNarrative</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used with the authors' pre-training and fine-tuning pipeline on WebNLG+2020, the model using Triple pre-training + level markers achieved BLEU 57.64, METEOR 42.24, TER 38.86, BERTScore 95.36 (Table 6). Ablation removing level markers reduced BLEU to 56.48 and other metrics (METEOR 41.77, TER 39.94, BERTScore 95.17), indicating measurable gains from the level markers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to vanilla linearisation without level markers (applied to same T5 models), the level-augmented version improved all reported metrics (e.g., +~1.16 BLEU in Triple setting). The paper contrasts this augmentation with prior work that models structure via additional embeddings or adapter layers (Wang et al. 2021; Ribeiro et al. 2021), noting that level markers provide weak positional structure without changing the PLM architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to apply and compatible with off-the-shelf encoder-decoder PLMs (no architectural changes); injects a compact positional/structural signal (distance-from-root) that empirically improves G2T metrics; cheap to compute; retains textual input format so it can leverage existing PLM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Provides only a weak/approximate structural signal (single numeric level) and may not capture complex graph topology (e.g., cycles, multi-rooted graphs) fully; still essentially a flattened representation so some structural relations can be lost relative to structure-aware layers/encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Less effective for graph types where relations are not lexicalised as meaningful words/morphemes (paper notes AMR-like graphs as problematic). Ablation and limitations indicate degraded gains when graph relation/entity sparsity is high (e.g., DART heterogenous/sparse portions, E2E-like sparse relation cases).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-supervised Graph Masking Pre-training for Graph-to-Text Generation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8971.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8971.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triple Masking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triple Prediction (graph masking pre-training: Triple)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised pre-training task that randomly masks entire triples at each graph level in the linearised input (replaced by a single <X> token) and trains the PLM to generate the masked triple as the target, thereby encouraging the model to infer sub-graph content from graph context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph masking pre-training — Triple Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Operates on the level-augmented linearised triples; on each level/sample the method selects one whole triple, replaces it in the encoder input with a single mask token <X>, and constructs the target output to contain the original triple. The model learns to reconstruct masked sub-graphs from the remaining graph text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triple graphs used in WebNLG+2020, DART, EventNarrative</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>From the linearised triple sequence, replace a chosen triple with <X> in the encoder input and set the decoder target to the textual representation of that masked triple (terminated with <Z>). Training uses standard cross-entropy negative log-likelihood loss over the masked parts.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Pre-training objective to improve downstream graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On WebNLG+2020 (full fine-tuning), Triple pre-training + level markers reported BLEU 57.64, METEOR 42.24, TER 38.86, BERTScore 95.36 (Table 6). In low-resource fine-tuning (5% of WebNLG training set), Triple pretraining using the same 5% for pre-training yielded BLEU 52.79 vs. baseline w/o pre-training BLEU 48.52 (Table 3). Performance degrades slightly when using less pretraining data (Table 4: with 5% pretraining data BLEU 56.40 vs 57.64 with 100% pretraining data).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Triple Prediction outperformed a no-pretraining T5-LARGE baseline (No pre-training BLEU 54.86 in Table 6) and was slightly better than Triple+Relation on WebNLG (Triple 57.64 vs Triple+Relation 57.49 BLEU), indicating competitive or superior gains without changing PLM architecture. On DART the improvements were smaller and on some metrics the method did not surpass SotA.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Self-supervised (requires no labels), architecture-preserving (no additional layers), improves structural awareness and downstream G2T performance, particularly strong in low-resource fine-tuning scenarios and robust to different pre-training set sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Learning objective focuses on reconstructing local masked subgraphs and may not capture some global structural properties; relies on the quality/coverage of the pre-training graph data (less gain on heterogenous/sparse datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Less effective on datasets with many very sparse examples or limited relation types (authors note DART contains much sparse data where ~52% examples have only 7 relation types, reducing the utility of pretraining). Also not recommended for graphs where relations are not lexicalised (e.g., AMR), as PLM's pretrained textual knowledge is less applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-supervised Graph Masking Pre-training for Graph-to-Text Generation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8971.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8971.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relation Masking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation Prediction (graph masking pre-training: Relation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised pre-training task that masks individual relation tokens inside triples (replaced by <Y>) and trains the PLM to predict the masked relation from the surrounding subject/object context, encouraging local cohesiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph masking pre-training — Relation Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Given the level-augmented linearised triple input, this strategy randomly selects one relation token per level and replaces it with a <Y> mask; the decoder target is the textual relation string. This objective encourages the model to use local head-tail context to reconstruct relation labels.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triples used in G2T datasets (WebNLG+2020, DART, EventNarrative)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>In the linearised triple sequence, replace a chosen P|relation token by <Y> in the encoder input; build a target output containing the original relation token (and end marker <Z>); train with cross-entropy over relation reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Pre-training objective to improve downstream graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper reports overall G2T performance improvements from Relation-only pretraining but exact standalone Relation-only numbers for WebNLG are not presented in the main tables; Table 1 gives illustrative input-output formatting. Triple+Relation and Triple alone are provided with numbers (Triple: BLEU 57.64; Triple+Relation: BLEU 57.49), and the text states that the performance difference among the three variants is statistically insignificant on WebNLG. Relation-focused pretraining contributes to local relation prediction improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Relation Prediction is compared qualitatively against Triple and Triple+Relation pretraining variants; authors state that Relation focuses on local information (head-tail) while Triple focuses on subgraph reconstruction. Empirically, all variants outperform baseline T5-LARGE and SotA on some datasets; however relation-only is not shown to dramatically outperform the other strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Targets local relation inference, which can improve accurate realization of relation-specific text fragments; self-supervised and architecture-preserving.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Narrow focus on relations may miss broader subgraph-level context; standalone gains vs combined strategies are modest.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May be less helpful when relations are very sparse or when relation labels are not meaningful lexical items (e.g., AMR relations). The paper notes no large advantage over combined strategies in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-supervised Graph Masking Pre-training for Graph-to-Text Generation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8971.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8971.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triple+Relation Masking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Combined Triple and Relation Prediction (Triple+Relation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A joint self-supervised pre-training task that masks both a whole triple (<X>) and additionally masks one relation (<Y>) from triples not connected to the masked triple, training the PLM to predict both masked sub-graphs and masked relations simultaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph masking pre-training — Triple+Relation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Combines the Triple and Relation masking protocols: randomly mask one full triple per level (replace with <X>) and, among triples that do not share subject/object with the masked triple, randomly mask one relation (replace with <Y>). The decoder target contains both the masked triple(s) and masked relation(s), terminated by <Z>, and learning minimizes cross-entropy over these masked outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triple graphs (WebNLG+2020, DART, EventNarrative)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>From the level-augmented linearisation, replace one triple with <X> and one unrelated triple's relation with <Y> in encoder input; build a joint target sequence containing the original triple and relation tokens; train using standard cross-entropy loss.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Pre-training objective used to improve graph-to-text generation (G2T)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On WebNLG+2020 (with level markers) Triple+Relation achieved BLEU 57.49, METEOR 42.19, TER 39.08, BERTScore 95.28 (Table 6). This is slightly below Triple alone in BLEU but differences are small and reported as statistically insignificant across the three strategies in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Per authors, Triple+Relation combines local (relation) and more global (triple) signals and performs comparably to Triple alone; all masking strategies outperform T5-LARGE baseline and prior SotA on WebNLG and EventNarrative datasets. On DART the gains are smaller and the method only matches SotA on BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Aims to capture both local and more global structural signals jointly; self-supervised and does not modify base PLM architecture; performs robustly across settings and benefits from level-marker augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Slight additional complexity in pretraining sampling logic; empirical gains over the simpler Triple-only variant are small in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Same dataset-dependent failure modes as the other masking strategies: reduced benefit on sparse/heterogeneous datasets (DART portions), and less suitability for non-lexicalized graph relations (AMR).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-supervised Graph Masking Pre-training for Graph-to-Text Generation', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Stage-wise finetuning for graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Structural adapters in pretrained language models for amr-to-text generation <em>(Rating: 2)</em></li>
                <li>Structure-aware pre-training for table-to-text generation <em>(Rating: 2)</em></li>
                <li>GAP: A graph-aware language model framework for knowledge graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 1)</em></li>
                <li>WebNLG challenge 2020: Language agnostic delexicalisation for multilingual RDF-to-text generation <em>(Rating: 1)</em></li>
                <li>GPT-too: A language-model-first approach for AMR-to-text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8971",
    "paper_id": "paper-6462f367550af0dab4eda55530c854743469c1d6",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Level-augmented linearisation",
            "name_full": "Linearisation of graphs into triples with role tokens and level-marker augmentation",
            "brief_description": "A serialization of graph-structured data into a sequence of triples [S|head, P|relation, O|tail, l] where each element is explicitly tagged with role tokens and augmented by a level marker l indicating the object's distance from the graph root; intended to provide a weak structural signal while keeping input compatible with off-the-shelf pretrained encoder-decoder LMs (T5).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linearisation (+ level markers)",
            "representation_description": "Each graph is converted into a set/sequence of triples of the form [S|head, P|relation, O|tail] with explicit role prefix tokens (S|, P|, O|). Every triple is additionally augmented with a level marker l that indicates the distance (level) of the tail/object node from the designated root node, yielding [S|head, P|relation, O|tail, l]. The linearised triples are concatenated into a text-like input for a text-to-text PLM (T5) without modifying model architecture.",
            "graph_type": "Knowledge graphs / RDF-style triple graphs (DBpedia, Wikidata, EventKG, heterogenous structured records like DART)",
            "conversion_method": "Serialize the graph into triple records (one per edge). Prepend role tokens S|, P|, O| to the subject, predicate and object. Compute the object's distance from the root node and append this as a numeric level marker l to the triple. Concatenate triples into the encoder input sequence for T5; no extra embeddings or architectural changes are introduced.",
            "downstream_task": "Graph-to-text generation (convert KG triples into natural-language descriptions) used on WebNLG+2020, DART, EventNarrative",
            "performance_metrics": "When used with the authors' pre-training and fine-tuning pipeline on WebNLG+2020, the model using Triple pre-training + level markers achieved BLEU 57.64, METEOR 42.24, TER 38.86, BERTScore 95.36 (Table 6). Ablation removing level markers reduced BLEU to 56.48 and other metrics (METEOR 41.77, TER 39.94, BERTScore 95.17), indicating measurable gains from the level markers.",
            "comparison_to_others": "Compared to vanilla linearisation without level markers (applied to same T5 models), the level-augmented version improved all reported metrics (e.g., +~1.16 BLEU in Triple setting). The paper contrasts this augmentation with prior work that models structure via additional embeddings or adapter layers (Wang et al. 2021; Ribeiro et al. 2021), noting that level markers provide weak positional structure without changing the PLM architecture.",
            "advantages": "Simple to apply and compatible with off-the-shelf encoder-decoder PLMs (no architectural changes); injects a compact positional/structural signal (distance-from-root) that empirically improves G2T metrics; cheap to compute; retains textual input format so it can leverage existing PLM capabilities.",
            "disadvantages": "Provides only a weak/approximate structural signal (single numeric level) and may not capture complex graph topology (e.g., cycles, multi-rooted graphs) fully; still essentially a flattened representation so some structural relations can be lost relative to structure-aware layers/encoders.",
            "failure_cases": "Less effective for graph types where relations are not lexicalised as meaningful words/morphemes (paper notes AMR-like graphs as problematic). Ablation and limitations indicate degraded gains when graph relation/entity sparsity is high (e.g., DART heterogenous/sparse portions, E2E-like sparse relation cases).",
            "uuid": "e8971.0",
            "source_info": {
                "paper_title": "Self-supervised Graph Masking Pre-training for Graph-to-Text Generation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Triple Masking",
            "name_full": "Triple Prediction (graph masking pre-training: Triple)",
            "brief_description": "A self-supervised pre-training task that randomly masks entire triples at each graph level in the linearised input (replaced by a single &lt;X&gt; token) and trains the PLM to generate the masked triple as the target, thereby encouraging the model to infer sub-graph content from graph context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "graph masking pre-training — Triple Prediction",
            "representation_description": "Operates on the level-augmented linearised triples; on each level/sample the method selects one whole triple, replaces it in the encoder input with a single mask token &lt;X&gt;, and constructs the target output to contain the original triple. The model learns to reconstruct masked sub-graphs from the remaining graph text.",
            "graph_type": "Knowledge graphs / RDF triple graphs used in WebNLG+2020, DART, EventNarrative",
            "conversion_method": "From the linearised triple sequence, replace a chosen triple with &lt;X&gt; in the encoder input and set the decoder target to the textual representation of that masked triple (terminated with &lt;Z&gt;). Training uses standard cross-entropy negative log-likelihood loss over the masked parts.",
            "downstream_task": "Pre-training objective to improve downstream graph-to-text generation",
            "performance_metrics": "On WebNLG+2020 (full fine-tuning), Triple pre-training + level markers reported BLEU 57.64, METEOR 42.24, TER 38.86, BERTScore 95.36 (Table 6). In low-resource fine-tuning (5% of WebNLG training set), Triple pretraining using the same 5% for pre-training yielded BLEU 52.79 vs. baseline w/o pre-training BLEU 48.52 (Table 3). Performance degrades slightly when using less pretraining data (Table 4: with 5% pretraining data BLEU 56.40 vs 57.64 with 100% pretraining data).",
            "comparison_to_others": "Triple Prediction outperformed a no-pretraining T5-LARGE baseline (No pre-training BLEU 54.86 in Table 6) and was slightly better than Triple+Relation on WebNLG (Triple 57.64 vs Triple+Relation 57.49 BLEU), indicating competitive or superior gains without changing PLM architecture. On DART the improvements were smaller and on some metrics the method did not surpass SotA.",
            "advantages": "Self-supervised (requires no labels), architecture-preserving (no additional layers), improves structural awareness and downstream G2T performance, particularly strong in low-resource fine-tuning scenarios and robust to different pre-training set sizes.",
            "disadvantages": "Learning objective focuses on reconstructing local masked subgraphs and may not capture some global structural properties; relies on the quality/coverage of the pre-training graph data (less gain on heterogenous/sparse datasets).",
            "failure_cases": "Less effective on datasets with many very sparse examples or limited relation types (authors note DART contains much sparse data where ~52% examples have only 7 relation types, reducing the utility of pretraining). Also not recommended for graphs where relations are not lexicalised (e.g., AMR), as PLM's pretrained textual knowledge is less applicable.",
            "uuid": "e8971.1",
            "source_info": {
                "paper_title": "Self-supervised Graph Masking Pre-training for Graph-to-Text Generation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Relation Masking",
            "name_full": "Relation Prediction (graph masking pre-training: Relation)",
            "brief_description": "A self-supervised pre-training task that masks individual relation tokens inside triples (replaced by &lt;Y&gt;) and trains the PLM to predict the masked relation from the surrounding subject/object context, encouraging local cohesiveness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "graph masking pre-training — Relation Prediction",
            "representation_description": "Given the level-augmented linearised triple input, this strategy randomly selects one relation token per level and replaces it with a &lt;Y&gt; mask; the decoder target is the textual relation string. This objective encourages the model to use local head-tail context to reconstruct relation labels.",
            "graph_type": "Knowledge graphs / RDF triples used in G2T datasets (WebNLG+2020, DART, EventNarrative)",
            "conversion_method": "In the linearised triple sequence, replace a chosen P|relation token by &lt;Y&gt; in the encoder input; build a target output containing the original relation token (and end marker &lt;Z&gt;); train with cross-entropy over relation reconstruction.",
            "downstream_task": "Pre-training objective to improve downstream graph-to-text generation",
            "performance_metrics": "The paper reports overall G2T performance improvements from Relation-only pretraining but exact standalone Relation-only numbers for WebNLG are not presented in the main tables; Table 1 gives illustrative input-output formatting. Triple+Relation and Triple alone are provided with numbers (Triple: BLEU 57.64; Triple+Relation: BLEU 57.49), and the text states that the performance difference among the three variants is statistically insignificant on WebNLG. Relation-focused pretraining contributes to local relation prediction improvements.",
            "comparison_to_others": "Relation Prediction is compared qualitatively against Triple and Triple+Relation pretraining variants; authors state that Relation focuses on local information (head-tail) while Triple focuses on subgraph reconstruction. Empirically, all variants outperform baseline T5-LARGE and SotA on some datasets; however relation-only is not shown to dramatically outperform the other strategies.",
            "advantages": "Targets local relation inference, which can improve accurate realization of relation-specific text fragments; self-supervised and architecture-preserving.",
            "disadvantages": "Narrow focus on relations may miss broader subgraph-level context; standalone gains vs combined strategies are modest.",
            "failure_cases": "May be less helpful when relations are very sparse or when relation labels are not meaningful lexical items (e.g., AMR relations). The paper notes no large advantage over combined strategies in their experiments.",
            "uuid": "e8971.2",
            "source_info": {
                "paper_title": "Self-supervised Graph Masking Pre-training for Graph-to-Text Generation",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Triple+Relation Masking",
            "name_full": "Combined Triple and Relation Prediction (Triple+Relation)",
            "brief_description": "A joint self-supervised pre-training task that masks both a whole triple (&lt;X&gt;) and additionally masks one relation (&lt;Y&gt;) from triples not connected to the masked triple, training the PLM to predict both masked sub-graphs and masked relations simultaneously.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "graph masking pre-training — Triple+Relation",
            "representation_description": "Combines the Triple and Relation masking protocols: randomly mask one full triple per level (replace with &lt;X&gt;) and, among triples that do not share subject/object with the masked triple, randomly mask one relation (replace with &lt;Y&gt;). The decoder target contains both the masked triple(s) and masked relation(s), terminated by &lt;Z&gt;, and learning minimizes cross-entropy over these masked outputs.",
            "graph_type": "Knowledge graphs / RDF triple graphs (WebNLG+2020, DART, EventNarrative)",
            "conversion_method": "From the level-augmented linearisation, replace one triple with &lt;X&gt; and one unrelated triple's relation with &lt;Y&gt; in encoder input; build a joint target sequence containing the original triple and relation tokens; train using standard cross-entropy loss.",
            "downstream_task": "Pre-training objective used to improve graph-to-text generation (G2T)",
            "performance_metrics": "On WebNLG+2020 (with level markers) Triple+Relation achieved BLEU 57.49, METEOR 42.19, TER 39.08, BERTScore 95.28 (Table 6). This is slightly below Triple alone in BLEU but differences are small and reported as statistically insignificant across the three strategies in the paper.",
            "comparison_to_others": "Per authors, Triple+Relation combines local (relation) and more global (triple) signals and performs comparably to Triple alone; all masking strategies outperform T5-LARGE baseline and prior SotA on WebNLG and EventNarrative datasets. On DART the gains are smaller and the method only matches SotA on BERTScore.",
            "advantages": "Aims to capture both local and more global structural signals jointly; self-supervised and does not modify base PLM architecture; performs robustly across settings and benefits from level-marker augmentation.",
            "disadvantages": "Slight additional complexity in pretraining sampling logic; empirical gains over the simpler Triple-only variant are small in reported experiments.",
            "failure_cases": "Same dataset-dependent failure modes as the other masking strategies: reduced benefit on sparse/heterogeneous datasets (DART portions), and less suitability for non-lexicalized graph relations (AMR).",
            "uuid": "e8971.3",
            "source_info": {
                "paper_title": "Self-supervised Graph Masking Pre-training for Graph-to-Text Generation",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Stage-wise finetuning for graph-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Structural adapters in pretrained language models for amr-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Structure-aware pre-training for table-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "GAP: A graph-aware language model framework for knowledge graph-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 1
        },
        {
            "paper_title": "WebNLG challenge 2020: Language agnostic delexicalisation for multilingual RDF-to-text generation",
            "rating": 1
        },
        {
            "paper_title": "GPT-too: A language-model-first approach for AMR-to-text generation",
            "rating": 1
        }
    ],
    "cost": 0.0118405,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-supervised Graph Masking Pre-training for Graph-to-Text Generation</h1>
<p>Jiuzhou Han and Ehsan Shareghi<br>Department of Data Science \&amp; AI, Monash University<br>{jiuzhou.han, ehsan.shareghi}@monash.edu</p>
<h4>Abstract</h4>
<p>Large-scale pre-trained language models (PLMs) have advanced Graph-toText (G2T) generation by processing the linearised version of a graph. However, the linearisation is known to ignore the structural information. Additionally, PLMs are typically pre-trained on free text which introduces domain mismatch between pre-training and downstream G2T generation tasks. To address these shortcomings, we propose graph masking pre-training strategies that neither require supervision signals nor adjust the architecture of the underlying pre-trained encoder-decoder model. When used with a pre-trained T5, our approach achieves new state-of-the-art results on WebNLG+2020 and EventNarrative G2T generation datasets. Our method also shows to be very effective in the low-resource setting. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Graph-to-Text (G2T) generation (Gatt and Krahmer, 2018) is the task of generating natural language from graph-structured data. While there are several tasks that could leverage a G2T component (Zhou et al., 2018; Ji et al., 2020; Chen et al., 2021) the direct generation of text description from knowledge graphs (KGs) have attracted a lot of attention due to its potential in providing a more accessible presentation of knowledge to non-experts (Schmitt et al., 2020).</p>
<p>In parallel, Transformer-based (Vaswani et al., 2017) pre-trained language models (PLMs) such as BART (Lewis et al., 2019), and T5 (Raffel et al., 2019) have facilitated state-of-the-art (SotA) results on several tasks, including earlier SotA results for G2T (Ribeiro et al., 2020; Kale and Rastogi, 2020; Mager et al., 2020). It has been argued that their success, in part, is due to factual memorisation that guides the generation (Ribeiro et al., 2020).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Although PLMs benefit the G2T generation, the linearisation step required to use these models ignores the structural information of the graph (Wang et al., 2021), while explicitly modelling structured data could also lead to catastrophic forgetting of distributional knowledge (Ribeiro et al., 2021).</p>
<p>To address this, Wang et al. (2021) proposed adding extra positional embedding layers to capture the inter-dependency structures of input graphs. Ribeiro et al. (2021) proposed using a structureaware adapter in PLMs to supplement the input with its graph structure. For table data, Xing and Wan (2021) considered the structure of the table input by predicting the surrounding cells for a cell in a table. However, these methods either change the design of the PLMs (limiting their use for other task settings) or require labelled training data to capture the graph structure information.</p>
<p>In this work, we propose self-supervised graph masking pre-training strategies to enhance the structure awareness of PLMs. To achieve this, we formulate several graph masking strategies to inject local and global awareness of the input structure into the PLM. Our method has two key advantages: (i) it does not require to introduce extra layers or change of architecture in the underlying PLM, and (ii) it pre-trains the PLMs in a self-supervised setting on graphs, without requiring labelled training data. Starting from an existing PLM, we further pretrain it with our approach, then the fine-tuning on downstream tasks is done as per usual. We conduct extensive experiments on three G2T generation datasets of diverse graphs. Our empirical findings highlight that our self-supervised strategies significantly outperform a strong underlying T5 baseline and achieve two new SotA results on two of the datasets WebNLG+2020 (Zhou and Lampouras, 2020) and EventNarrative (Colas et al., 2021). Additionally, we show our pre-training strategies are very efficient in utilising data and have a great potential for low-resource setting.</p>
<p>| Pre-training Task | Input (Triples format: [S|head, P|relation, O|tail, $l_{i}$ ]) | Target Output |
| :--: | :--: | :--: |
| Triple Prediction | [<X>, 1], [S|New York City, P|country, 0|United States, 2], [S|New York City, P | is Part Of, 0 | Manhattan, 2], [S|Manhattan, P | leader Name, 0 | Cyrus Vance Jr., 3], [S|Manhattan, P | is Part Of, 0 | New York, 3] | $&lt;$ X&gt; [S|Asser Levy Public Baths, P | location, 0 | New York City] <Z> |
| Relation Prediction | [S|Asset Levy Public Baths, P | location, 0 | New York City, 1], [S|New York City, <Y>, 0 | United States, 2], [S|New York City, P | is Part Of, 0 | Manhattan, 2], [S|Manhattan, P | leader Name, 0 | Cyrus Vance Jr., 3], [S|Manhattan, P | is Part Of, 0 | New York, 3] | $&lt;$ Y&gt; P|country $&lt;$ Z $&gt;$ |
| Triple Prediction + Relation Prediction | [<X>, 1], [S|New York City, P|country, 0|United States, 2], [S|New York City, P | is Part Of, 0 | Manhattan, 2], [S|Manhattan, <Y>, 0 | Cyrus Vance Jr., 3], [S| Manhattan, P | is Part Of, 0 | New York, 3] | $&lt;$ X&gt; [S|Asser Levy Public Baths, P | location, 0 | New York City] <Y> P | leader Name $&lt;$ Z $&gt;$ |</p>
<p>Table 1: The input-output format for our graph masking strategies.</p>
<h2>2 Self-Supervised Graph Masking</h2>
<p>Our desiderata is to infuse structural knowledge into widely used pre-trained encoder-decoder Transformer models, without modifying the model architecture or relying on supervision signal. To achieve this, we propose three self-supervised learning tasks to further pre-train a T5-LARGE (Raffel et al., 2020) model prior to fine-tuning on G2T generation downstream tasks. In this section we first describe our graph linearisation step which prepares the data in the right format for T5 encoder while injecting some weak structural information into the input (§2.1), then we introduce our three graph masking pre-training tasks (§2.2).</p>
<h3>2.1 Linearising a Graph</h3>
<p>We linearise a graph into a set of triples in the format of [subject, predicate, object], representing [head entity, relation, tail entity] for every edge in a graph. Following Wang et al. (2021), we prepend $S|, P|, 0 \mid$ tokens to further specialise each entity or relation with its role in a triple. Additionally, to provide a weak structural signal from the graph, we also augment every triple by a level marker $l$, indicating the distance of its object entity from the root (the node that does not have a parent in the graph). This is similar to (Wang et al., 2021), noting the key difference in that they embed the tree level using an extra layer together with other positional embeddings, but we simply augment the linearised input without adding any extra layers. The final augmented triple has the following format: [S|head entity, P|relation, O|tail entity, $l]$. For a visual example of this, see Appendix A.</p>
<h3>2.2 Graph Masking Pre-training Strategies</h3>
<p>The three self-supervised learning tasks are formulated as follows:
Triple Prediction (Triple). For a linearised graph,
on each level we randomly mask one full triple and replace it with a mask token $&lt;\mathrm{X}&gt;$, which is then used as the target for prediction. The masked triple can be seen as a sub-graph of the original graph. This is to encourage the model to automatically identify the most relevant parts of a full graph related to each of its sub-graph.
Relation Prediction (Relation). In this strategy, we focus on the relations within triples. We randomly mask one relation on each level with a mask token $&lt;\mathrm{Y}&gt;$, and the model is tasked to predict the masked relation as the target. This task requires the model to leverage very local information (i.e., between a head and a tail) to predict the masked relation. Local cohesiveness is expected to translate into better translation of triples into text fragments.
Triple + Relation Prediction (Triple+Relation). This ultimate strategy combines both Triple and Relation Prediction tasks to leverage the benefits of both worlds. In this setting, the Triple Prediction task follows the same protocol as stated above, but for Relation Prediction, we only consider the relation in triples that are not connected with the masked triple. We randomly mask one triple with the mask token $&lt;\mathrm{X}&gt;$. For the triples that do not have common subject or object with the masked triple, we also randomly mask one relation with the mask token $&lt;\mathrm{Y}&gt;$. The model jointly learns to predict both the masked sub-graphs and relations at the same time.</p>
<p>In all pre-training tasks we also add a token $&lt;\mathrm{Z}&gt;$ as the end token in the target output. Table 1 summarises these three pre-training tasks via an example of each kind of graph masking strategy. Graph Masking Pre-training follows the standard crossentropy loss, which is to minimise the negative log-likelihood of the masked part of the graph:</p>
<p>$$
\mathcal{L}<em i="1">{G M P}=-\sum</em>\right)
$$}^{N} \log p\left(m_{i} \mid x_{i</p>
<p>where $m_{i}$ is the masked part of the graph, $x_{i}$ is the unmasked part of the graph, $N$ is the number samples.</p>
<h2>3 Experiments</h2>
<p>In this section we outline the experimental setups (§3.1), followed by downstream G2T generation results in full (§3.2) and low-resource scenarios (§3.3). We also present a set of generated outputs from our models (§3.4), and finish by providing an analysis (§3.5) on the effect of pre-training data size, and an ablation on the role of input augmentation with level markers.</p>
<h3>3.1 Experimental Setups</h3>
<p>Tasks and Datasets. We evaluate on three G2T generation datasets: WebNLG+2020 (Zhou and Lampouras, 2020), DART (Nan et al., 2021), EventNarrative (Colas et al., 2021). WebNLG+20202 contains a set of triples extracted from DBpedia (Auer et al., 2007) and text description for 16 distinct DBpedia categories. DART $^{3}$ is an opendomain heterogeneous structured dataset collected from different sources which cover a broad range of topics. EventNarrative ${ }^{4}$ is a large-scale, eventcentric dataset extracted and paired from existing large-scale data repositories, including Wikidata, Wikipedia, and EventKG (Gottschalk and Demidova, 2018). See Appendix B for full data statistics.
Pre-training Datasets. For each pre-training strategy, we create the pre-training datasets on the graph side of the task training data with the right format.
Evaluation Metrics. We report the automatic evaluation using BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006) which are used in the official WebNLG challenge (Gardent et al., 2017) and BERTScore (Zhang et al., 2020) which considers the semantic meanings of words or phrases.
Baseline, SotA, Our Models. We use the T5-LARGE model as our baseline for fine-tuning. T5-large results are based on the published results (Ribeiro et al., 2020). All our models further pre-train the vanilla T5-LARGE model and are further fine-tuned for G2T generation tasks as usual. We denote our configurations as Triple,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: G2T generation results on 3 datasets.</p>
<p>Relation, Triple+Relation. SotA results for WebNLG and DART are from Clive et al. (2021), and for EventNarrative are based on Colas et al. (2022). Our implementation is based on the Huggingface Library (Wolf et al., 2019). Optimisation was done using Adam (Kingma and Ba, 2015) with a learning rate of $3 \mathrm{e}-5$ and a batch size of 3 both in the pre-training and fine-tuning stages. We used a V100 16GB GPU for all experiments.</p>
<h3>3.2 Graph-to-text Generation</h3>
<p>Task Formulation. G2T generation follows the standard language modelling objective. Given an input graph $\mathcal{G}$, the model aims to generate groundtruth text $y=\left(y_{1}, \ldots, y_{N}\right)$. The objective is to maximise the likelihood of the ground-truth text, which is equivalent to minimise the negative loglikelihood as:</p>
<p>$$
\mathcal{L}<em i="1">{G 2 T}=-\sum</em>\right)
$$}^{N} \log p\left(y_{i} \mid y_{1}, \ldots, y_{i-1} ; \mathcal{G</p>
<p>Results. Table 2 reports the results of fine-tuning the baseline, SotA and our models on three G2T generation tasks. For WebNLG, all of our strategies outperform both the baseline and SotA results. The performance difference among our three variants is statistically insignificant. Similarly, on EventNarrative all our models outperform SotA and baseline. For DART, the improvement over the baseline is not as significant as for the other two datasets, while our method matches SotA on BERTScore but falls behind on the other metrics. We speculate this to be reflective of the heterogeneous nature of DART, which has a large proportion of data with very limited relations (e.g., roughly $52 \%$ of DART contains only 7 types of relations). In this setting,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tr.Size</th>
<th style="text-align: center;">Model Setting</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">TER</th>
<th style="text-align: center;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">w/o pre-training</td>
<td style="text-align: center;">48.52</td>
<td style="text-align: center;">37.44</td>
<td style="text-align: center;">43.97</td>
<td style="text-align: center;">94.66</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">same $5 \%$ for pre-training</td>
<td style="text-align: center;">52.79</td>
<td style="text-align: center;">40.41</td>
<td style="text-align: center;">42.02</td>
<td style="text-align: center;">94.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">remaining $95 \%$ for pre-training</td>
<td style="text-align: center;">50.69</td>
<td style="text-align: center;">39.06</td>
<td style="text-align: center;">42.97</td>
<td style="text-align: center;">94.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o pre-training</td>
<td style="text-align: center;">48.64</td>
<td style="text-align: center;">37.24</td>
<td style="text-align: center;">43.33</td>
<td style="text-align: center;">94.65</td>
</tr>
<tr>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">same $10 \%$ data for pre-training</td>
<td style="text-align: center;">53.56</td>
<td style="text-align: center;">40.45</td>
<td style="text-align: center;">41.19</td>
<td style="text-align: center;">95.03</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">remaining $90 \%$ data for pre-training</td>
<td style="text-align: center;">52.57</td>
<td style="text-align: center;">39.75</td>
<td style="text-align: center;">42.17</td>
<td style="text-align: center;">94.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o pre-training</td>
<td style="text-align: center;">50.35</td>
<td style="text-align: center;">37.87</td>
<td style="text-align: center;">43.82</td>
<td style="text-align: center;">94.66</td>
</tr>
<tr>
<td style="text-align: center;">$25 \%$</td>
<td style="text-align: center;">same $25 \%$ data for pre-training</td>
<td style="text-align: center;">56.04</td>
<td style="text-align: center;">41.57</td>
<td style="text-align: center;">39.38</td>
<td style="text-align: center;">95.24</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">remaining $75 \%$ data for pre-training</td>
<td style="text-align: center;">55.93</td>
<td style="text-align: center;">41.46</td>
<td style="text-align: center;">39.78</td>
<td style="text-align: center;">95.20</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of each model in the low-resource setting on WebNLG+2020 dataset. Tr.Size denotes the amount of data used for downstream task fine-tuning.
the pre-training tasks cannot capture much useful structure information on this sparse data.</p>
<h3>3.3 Low-resource Setting</h3>
<p>We investigated the performance of our methods in low-resource scenario. For this we used Triple as the pre-training strategy and $\mathrm{k} \%(\mathrm{k}=5,10,25)$ of WebNLG+2020 training data for downstream task fine-tuning. We tried two configurations to see if pre-training (still without using the labels) with the same training data would be better than pretraining on the non-overlapping training data: (1) used the same $\mathrm{k} \%$ between pre-training and finetuning, (2) used $100-\mathrm{k} \%$ for pre-training and $\mathrm{k} \%$ for fine-tuning. We compared the results of these two settings with the T5 LARGE which was only task fine-tunined (without additional pre-training). The results are shown in Table 3.</p>
<p>The models using pre-training significantly outperform the models without pre-training. For instance in $5 \%$ training scenario, the pre-trained model with Triple which used the same amount of data for both pre-training and fine-tuning outperforms T5 LARGE by a margin of 4 BLEU scores. This indicates that our graph masking pre-training strategies can effectively improve the performance of the underlying PLM in the low-resource scenario. With the increment of training data, the improvement effect of pre-training method is greater. Moreover, pre-training with the same training data leads to better results compared with using nonoverlapping data. We speculate this happens since the model in this configuration is exposed to learn specific structural knowledge that will be used in the seen training data for fine-tuning downstream tasks. This also suggests a potential for our approach in multi-task learning, which we leave to future work. As the increase of training data, the gap of the performance of pre-training using different parts of data also decreases.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Data</th>
<th style="text-align: right;">Time</th>
<th style="text-align: right;">BLEU</th>
<th style="text-align: right;">METEOR</th>
<th style="text-align: right;">TER</th>
<th style="text-align: right;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: right;">10 h</td>
<td style="text-align: right;">57.64</td>
<td style="text-align: right;">42.24</td>
<td style="text-align: right;">38.86</td>
<td style="text-align: right;">95.36</td>
</tr>
<tr>
<td style="text-align: left;">$75 \%$</td>
<td style="text-align: right;">7.5 h</td>
<td style="text-align: right;">56.92</td>
<td style="text-align: right;">41.98</td>
<td style="text-align: right;">39.07</td>
<td style="text-align: right;">95.29</td>
</tr>
<tr>
<td style="text-align: left;">$50 \%$</td>
<td style="text-align: right;">5 h</td>
<td style="text-align: right;">56.78</td>
<td style="text-align: right;">41.96</td>
<td style="text-align: right;">39.90</td>
<td style="text-align: right;">95.18</td>
</tr>
<tr>
<td style="text-align: left;">$25 \%$</td>
<td style="text-align: right;">2.5 h</td>
<td style="text-align: right;">56.73</td>
<td style="text-align: right;">41.85</td>
<td style="text-align: right;">40.13</td>
<td style="text-align: right;">95.21</td>
</tr>
<tr>
<td style="text-align: left;">$5 \%$</td>
<td style="text-align: right;">0.5 h</td>
<td style="text-align: right;">56.40</td>
<td style="text-align: right;">41.28</td>
<td style="text-align: right;">40.24</td>
<td style="text-align: right;">95.12</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of using different amounts of pretraining data in Triple strategy on WebNLG+2020. Time denotes the pre-training duration.</p>
<h3>3.4 Generated Samples</h3>
<p>We demonstrate two qualitative examples of generated texts on WebNLG+2020 and EventNarrative test sets in Table 5.</p>
<p>For the WebNLG example, while T5 LARGE generates fluent texts but misses to cover the "recorded in" relation. Previous SotA model generates all information from the graph, but it breaks the order of arguments for "preceded By". While our model can not only produce the sentences with correct information.</p>
<p>For the EventNarrative example, the "Russian" information in the reference does not exist in the graph, which should be inferred by the PLM. For T5 LARGE and previous SotA, neither can generate such information, while our model can generate this additional information without missing any information from the graph. See more generated samples in Appendix C.</p>
<h3>3.5 Analysis</h3>
<p>Effect of Pre-training Data Size. To explore how the size of the used pre-training data affects the performance of our strategies in downstream tasks, we experimented on WebNLG+2020 dataset using our Triple strategy. We used $5 \%, 10 \%, 25 \%, 50 \%$, and $100 \%$ of the graph side of training data for pre-training, and the whole training data to finetune the models. We recorded the performance, and training duration in Table 4. As the amount of pre-training data decreased, the performance of the model also decreased slightly. However, even with using $5 \%$ of pre-training data and less than 30 minutes spent on pre-training, our method outperforms both the SotA and T5 LARGE models (Table 2) by a significant margin.
Ablation. To show the contribution of input augmentation with level markers (§2.1), we experimented with Triple and Triple+Relation strategies on WebNLG+2020. We also report the results of using input augmentation with level marker dur-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG+2020</th>
<th style="text-align: left;">EventNarrative</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Reference: The Velvet Underground Squeeze album was succeeded by the rock album Bootleg Series Volume 1: The Quine Tapes, recorded under record label Polydor Records in San Francisco.</p>
<p>T5-Large: The genre of Bootleg Series Volume 1: The Quine Tapes is rock music and was preceded by the album Squeeze The Velvet Underground. The album was released by Polydor Records.</p>
<p>Previous SotA: Squeeze The Velvet Underground was preceded by Bootleg Series Volume 1: The Quine Tapes, which was recorded in San Francisco and released by Polydor Records. The genre of the album is rock music.</p>
<p>Graph Masking Pre-training+T5-Large: Bootleg Series Volume 1: The Quine Tapes, whose genre is rock music, were recorded in San Francisco and are signed to Polydor Records. They were preceded by the album Squeeze The Velvet Underground.</p>
<p>Reference: The First Battle of Ignacewo was one of many clashes of the January Uprising. it took place on may 8,1863 , near the village of Ignacewo, Konin County, which at that time belonged to Russian empire's Congress Poland.</p>
<p>T5-Large: The First Battle of Ignacewo was fought in Ignacewo, Konin County, Congress Poland, during the January Uprising.</p>
<p>Previous SotA: The First Battle of Ignacewo was one of the first battles of the January Uprising. It took place on January 6, 1863, near the village of Konin, in Congress Poland.</p>
<p>Graph Masking Pre-training+T5-Large: The First Battle of Ignacewo was one of battles of the January Uprising. It took place on January 11, 1863, near the village of Ignacewo, Konin County, Russian-controlled Congress Poland.</p>
<p>Table 5: Examples of output texts on WebNLG+2020 and EventNarrative test sets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Pre-training Tasks</th>
<th style="text-align: left;">BLEU</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">TER</th>
<th style="text-align: left;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Triple</td>
<td style="text-align: left;">57.64</td>
<td style="text-align: left;">42.24</td>
<td style="text-align: left;">38.86</td>
<td style="text-align: left;">95.36</td>
</tr>
<tr>
<td style="text-align: left;">-w/o level marker</td>
<td style="text-align: left;">56.48</td>
<td style="text-align: left;">41.77</td>
<td style="text-align: left;">39.94</td>
<td style="text-align: left;">95.17</td>
</tr>
<tr>
<td style="text-align: left;">Triple+Relation</td>
<td style="text-align: left;">57.49</td>
<td style="text-align: left;">42.19</td>
<td style="text-align: left;">39.08</td>
<td style="text-align: left;">95.28</td>
</tr>
<tr>
<td style="text-align: left;">-w/o level marker</td>
<td style="text-align: left;">56.28</td>
<td style="text-align: left;">41.70</td>
<td style="text-align: left;">39.72</td>
<td style="text-align: left;">95.24</td>
</tr>
<tr>
<td style="text-align: left;">No pre-training</td>
<td style="text-align: left;">54.86</td>
<td style="text-align: left;">40.62</td>
<td style="text-align: left;">40.58</td>
<td style="text-align: left;">95.09</td>
</tr>
<tr>
<td style="text-align: left;">-w/o level marker</td>
<td style="text-align: left;">53.60</td>
<td style="text-align: left;">39.52</td>
<td style="text-align: left;">41.48</td>
<td style="text-align: left;">95.02</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation results on WebNLG+2020 dataset.
ing fine-tuning T5 LARGE. The results are shown in Table 6. We observe that the input augmentation with level markers brings improvement across all settings, even when it is only used during finetuning (last two rows of Table 6). We speculate this to be an indication that some useful positional information is augmented to the the linearised input through adding level markers.</p>
<h2>4 Conclusion and Future Work</h2>
<p>We proposed various self-supervised pre-training strategies to improve the structural awareness of PLMs without refining the architecture or relying
on labelled data. Our graph masking strategies outperformed the strong PLM baseline and achieve new state-of-the-art results on WebNLG+2020 and EventNarrative datasets. We demonstrated that our approach is very efficient in utilising even a small pre-training or fine-tuning datasets. For future work, we will explore different graph masking strategies to adapt for different domains of graph.</p>
<h2>5 Limitations</h2>
<p>Since our method leverages the knowledge learned by pretrained language models, it is much more effective for use in scenarios where, unlike AMR graphs, the relations inside the graph correspond to meaningful words or morphemes. Additionally, we observed our method not to work well for the cases, like in E2E (Dusek et al., 2019), that the number of relations or entities are quite sparse.</p>
<h2>6 Ethics Statement</h2>
<p>Our model utilises existing pretrained language models and as such it could inherit the same ethical</p>
<p>concerns involving these models - which are being discussed widely in the community. Our pretraining method itself does not exacerbate this issue.</p>
<h2>References</h2>
<p>Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007. Dbpedia: A nucleus for a web of open data. In The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11-15, 2007, volume 4825 of Lecture Notes in Computer Science, pages 722-735. Springer.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005, pages 65-72. Association for Computational Linguistics.</p>
<p>Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Yang Wang, and William W. Cohen. 2021. Open question answering over tables and text. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Jordan Clive, Kris Cao, and Marek Rei. 2021. Control prefixes for text generation. CoRR, abs/2110.08329.</p>
<p>Anthony Colas, Mehrdad Alvandipour, and Daisy Zhe Wang. 2022. GAP: A graph-aware language model framework for knowledge graph-to-text generation. CoRR, abs/2204.06674.</p>
<p>Anthony Colas, Ali Sadeghian, Yue Wang, and Daisy Zhe Wang. 2021. Eventnarrative: A largescale event-centric dataset for knowledge graph-totext generation. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.</p>
<p>Ondrej Dusek, David M. Howcroft, and Verena Rieser. 2019. Semantic noise matters for neural natural language generation. In Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019, Tokyo, Japan, October 29 - November 1, 2019, pages 421-426. Association for Computational Linguistics.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The webnlg challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation, INLG 2017, Santiago de Compostela, Spain, September 4-7, 2017, pages 124-133. Association for Computational Linguistics.</p>
<p>Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. J. Artif. Intell. Res., 61:65-170.</p>
<p>Simon Gottschalk and Elena Demidova. 2018. Eventkg: A multilingual event-centric temporal knowledge graph. In The Semantic Web - 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3-7, 2018, Proceedings, volume 10843 of Lecture Notes in Computer Science, pages 272287. Springer.</p>
<p>Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, and Minlie Huang. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 725-736. Association for Computational Linguistics.</p>
<p>Mihir Kale and Abhinav Rastogi. 2020. Text-to-text pretraining for data-to-text tasks. In Proceedings of the 13th International Conference on Natural Language Generation, INLG 2020, Dublin, Ireland, December 15-18, 2020, pages 97-102. Association for Computational Linguistics.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.</p>
<p>Manuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, and Salim Roukos. 2020. GPT-too: A language-model-first approach for AMR-to-text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1846-1852, Online. Association for Computational Linguistics.</p>
<p>Linyong Nan, Dragomir R. Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. 2021. DART: open-domain structured data record to text generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2021, Online, June 6-11, 2021, pages 432-447. Association for Computational Linguistics.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311-318. ACL.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text generation. CoRR, abs/2007.08426.</p>
<p>Leonardo F. R. Ribeiro, Yue Zhang, and Iryna Gurevych. 2021. Structural adapters in pretrained language models for amr-to-text generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 4269-4282. Association for Computational Linguistics.</p>
<p>Martin Schmitt, Sahand Sharifzadeh, Volker Tresp, and Hinrich Schütze. 2020. An unsupervised joint system for text generation from knowledge graphs and semantic parsing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7117-7130, Online. Association for Computational Linguistics.</p>
<p>Matthew G. Snover, Bonnie J. Dorr, Richard M. Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, AMTA 2006, Cambridge, Massachusetts, USA, August 8-12, 2006, pages 223-231. Association for Machine Translation in the Americas.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Qingyun Wang, Semih Yavuz, Xi Victoria Lin, Heng Ji, and Nazneen Fatema Rajani. 2021. Stage-wise finetuning for graph-to-text generation. In Proceedings of the ACL-IJCNLP 2021 Student Research Workshop, ACL 2021, Online, JUli 5-10, 2021, pages 16-22. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771.</p>
<p>Xinyu Xing and Xiaojun Wan. 2021. Structure-aware pre-training for table-to-text generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2273-2278, Online. Association for Computational Linguistics.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Giulio Zhou and Gerasimos Lampouras. 2020. WebNLG challenge 2020: Language agnostic delexicalisation for multilingual RDF-to-text generation. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 186-191, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. 2018. Commonsense knowledge aware conversation generation with graph attention. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pages 4623-4629. ijcai.org.</p>
<h2>Appendix</h2>
<h2>A Level Marker Augmentation</h2>
<p>A graph and linearised version of a levelaugmented input is provided in Figure 1.</p>
<h2>B Data Statistics</h2>
<p>The data statistics for tasks used in the paper are summarised in Table 7.</p>
<h2>C Generated Samples</h2>
<p>Table 8 illustrates two qualitative examples of generated texts on WebNLG+2020 and EventNarrative test sets.</p>
<p>For the WebNLG example, T5 LARGE misses to cover the "manufacturer" and "body Style" information. Although previous SotA and our model both can generate correct sentences, the output of our model shows a more complex syntactic structure. For the EventNarrative example, the sentences generated from T5 LARGE have a big difference with the reference sentences and do not cover all</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Domain</th>
<th style="text-align: right;">Examples</th>
<th style="text-align: right;">Train/Dev/Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">WebNLG+2020</td>
<td style="text-align: left;">16 DBpedia Categories</td>
<td style="text-align: right;">38,872</td>
<td style="text-align: right;">$35,426 / 1,667 / 1,779$</td>
</tr>
<tr>
<td style="text-align: left;">EventNarrative</td>
<td style="text-align: left;">Events</td>
<td style="text-align: right;">224,428</td>
<td style="text-align: right;">$179,544 / 22,442 / 22,442$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Wikipedia</td>
<td style="text-align: right;">11,998</td>
<td style="text-align: right;">$62,659 / 6,980 / 12,552$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">15 DBpedia Categories</td>
<td style="text-align: right;">27,731</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Restaurant and Hotel Descriptions</td>
<td style="text-align: right;">42,462</td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Statistics of WebNLG+2020, EventNarrative and DART.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of graph with level markers. The structure-aware input of this graph is: [S I Asser Levy Public Baths, P I location, 0 I New York City, 1], [S I New York City, P I country, 0 I United States, 2], [S I New York City, P I is Part Of, 0 I Manhattan, 2], [S I Manhattan, P I leader Name, 0 I Cyrus Vance Jr., 3], [S I Manhattan, P I is Part Of, 0 I New York, 3].
information from the graph. Previous SotA model misses to cover the "office contested" information, while the output from our model covers all information.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Reference: The Pontiac Rageous was a car with a coupe body style manufactured by Pontiac. Assembled in both Michigan and Detroit, it went into production in 1997, ending in the same year.
T5-Large: The Pontiac Rageous is assembled in Detroit, Michigan. Its production began in 1997 and ended in 1997. The Pontiac Rageous is a 4 door, 5 passenger vehicle.
Previous SotA: The Pontiac Rageous is manufactured by Pontiac in Detroit, Michigan. Its production began in 1997 and ended in 1997. The Pontiac Rageous has a coupe body style.
Graph Masking Pre-training+T5-Large: Pontiac is the manufacturer of the Pontiac Rageous which has a coupe body style. The Pontiac Rageous is assembled in Detroit, Michigan and began production in 1997.</p>
<h1>EventNarrative</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Reference: The First Battle of Ignacewo was one of many clashes of the January Uprising. it took place on may 8,1863 , near the village of Ignacewo, Konin County, which at that time belonged to Russian empire's Congress Poland.
T5-Large: The First Battle of Ignacewo was fought in Ignacewo, Konin County, Congress Poland, during the January Uprising.
Previous SotA: The First Battle of Ignacewo was one of the first battles of the January Uprising. It took place on January 6, 1863, near the village of Konin, in Congress Poland.
Graph Masking Pre-training+T5-Large: The First Battle of Ignacewo was one of battles of the January Uprising. It took place on January 11, 1863, near the village of Ignacewo, Konin County, Russian-controlled Congress Poland.</p>
<p>Table 8: Examples of output texts on WebNLG+2020 and EventNarrative test sets.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://gitlab.com/shimorina/webnlg-dataset/ $-/ t r e e / m a s t e r / r e l e a s e _v 3.8$
${ }^{3}$ https://github.com/Yale-LILY/dart
${ }^{4}$ https://www.kaggle.com/datasets/acolas1/ eventnarration&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
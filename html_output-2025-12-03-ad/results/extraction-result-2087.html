<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2087 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2087</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2087</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-278537074</p>
                <p><strong>Paper Title:</strong> From data extraction to analysis: a comparative study of ELISE capabilities in scientific literature</p>
                <p><strong>Paper Abstract:</strong> The exponential growth of scientific literature presents challenges for pharmaceutical, biotechnological, and Medtech industries, particularly in regulatory documentation, clinical research, and systematic reviews. Ensuring accurate data extraction, literature synthesis, and compliance with industry standards require AI tools that not only streamline workflows but also uphold scientific rigor. This study evaluates the performance of AI tools designed for bibliographic review, data extraction, and scientific synthesis, assessing their impact on decision-making, regulatory compliance, and research productivity. The AI tools assessed include general-purpose models like ChatGPT and specialized solutions such as ELISE (Elevated LIfe SciencEs), SciSpace/Typeset, Humata, and Epsilon. The evaluation is based on three main criteria: Extraction, Comprehension, and Analysis with Compliance and Traceability (ECACT) as additional dimensions. Human experts established reference benchmarks, while AI Evaluator models ensure objective performance measurement. The study introduces the ECACT score, a structured metric assessing AI reliability in scientific literature analysis, regulatory reporting and clinical documentation. Results demonstrate that ELISE consistently outperforms other AI tools, excelling in precise data extraction, deep contextual comprehension, and advanced content analysis. ELISE’s ability to generate traceable, well-reasoned insights makes it particularly well-suited for high-stakes applications such as regulatory affairs, clinical trials, and medical documentation, where accuracy, transparency, and compliance are paramount. Unlike other AI tools, ELISE provides expert-level reasoning and explainability, ensuring AI-generated insights align with industry best practices. ChatGPT is efficient in data retrieval but lacks precision in complex analysis, limiting its use in high-stakes decision-making. Epsilon, Humata, and SciSpace/Typeset exhibit moderate performance, with variability affecting their reliability in critical applications. In conclusion, while AI tools such as ELISE enhance literature review, regulatory writing, and clinical data interpretation, human oversight remains essential to validate AI outputs and ensure compliance with scientific and regulatory standards. For pharmaceutical, biotechnological, and Medtech industries, AI integration must strike a balance between automation and expert supervision to maintain data integrity, transparency, and regulatory adherence.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2087.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2087.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELISE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ELISE (Elevated LIfe SciencEs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized AI literature-analysis system developed by Biolevate combining an advanced document parser (Matsu) with context-aware LLM modules and retrieval-augmented generation to extract, comprehend and analyze biomedical literature with traceability and compliance features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ELISE</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>specialized retrieval-augmented NLP system (LLM + custom parser)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical / regulatory literature analysis</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>structured data extraction (metadata, tables, HRs), summaries, analytical insights and traceable explanations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>primarily in-distribution / incremental for literature synthesis; moderate novelty for inferred numerical approximations from figures (e.g., KM readouts)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>retrieval-augmented generation combining document retrieval, a custom parser (Matsu) and context-aware LLM modules to recombine extracted facts and produce structured outputs and explainable analyses</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>multi-step: comparison against human expert reference answers (three experts), independent AI Evaluator models (Claude 3.5 Sonnet, GPT-4o, o1 Preview) scoring on standardized rubrics; statistical analyses (ANOVA/Tukey) and task-specific checks (traceability and compliance scoring); parser-level NED comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>High generation performance reported: Extraction global average 87.50%; Comprehension mean ≈ 8.23/10; Analysis mean ≈ 7.98/10; stable scores with/without human reference (Comprehension 8.32–8.42, Analysis 7.98–8.13). Performance reported across n=9 articles.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation via human reference + 3 AI Evaluators produced consistent scoring; ELISE showed close alignment with human experts (minimal score variation between human-referenced and autonomous evaluation). No single numeric 'validation accuracy' reported beyond evaluator scores which averaged into ECACT-weighted final score (top-ranked).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported quantitatively; qualitative evidence shows ELISE produced closest approximations and explicitly indicated margin-of-error in figure-read tasks (e.g., EFS approximations) and correctly retrieved a hazard ratio (HR 0.31, 95% CI 0.13–0.74) where others failed. No numeric FPR provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported quantitatively in the paper. Authors report few missed items for ELISE in tested tasks (high extraction completeness) but no numeric FNR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Reported evidence that ELISE maintained performance for tasks requiring inference from figures/tables (moderate novelty) and had minimal degradation when human references were absent; explicit quantitative relationship between novelty level and validation accuracy not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper reports close parity for ELISE between generation quality and validation (ELISE outputs closely matched human references and AI Evaluator scores), indicating a small generation-validation gap for ELISE relative to other tools.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Limited formal UQ: example where ELISE 'explicitly indicated a margin of error' when approximating Kaplan–Meier readouts; no standardized uncertainty scores or calibration metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified with calibration metrics; qualitative claim that ELISE provides explainability and traceability which aids perceived calibration but no numeric calibration quality reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Language-change experiment: maintained Comprehension/Analysis across French/English (minor variation) and better English Extraction in some cases; no formal OOD metrics for transformational discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — relied on proxy metrics including semantic fidelity, completeness, Compliance and Traceability scores, ECACT composite score (weighted sum up to 80), and parser Normalized Edit Distance (NED) for textual parsing (Matsu NED=0.490463 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for high-stakes outputs; study used human reference answers for every article during one evaluation condition and notes human oversight is essential, especially for regulatory contexts. Frequency increases for higher novelty / critical tasks (stated qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (biomedical/regulatory literature) — semi-structured but not formally specified; this increases reliance on expert validation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>ECACT evaluation framework (Extraction, Comprehension, Analysis + Compliance, Traceability), use of independent multi-model AI Evaluators, human expert reference answers, parser improvements (Matsu), RAG architectures and explicit traceability outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>General observation in paper: other tools (ChatGPT, Epsilon, Humata, SciSpace) produced incorrect extractions (e.g., ChatGPT hallucinated DOI, produced incorrect EFS percentages 21/30% vs expected 37/44%), while ELISE succeeded — implying generation varied in quality and some models produced plausible but incorrect outputs (fabrications). The need for human oversight is repeatedly emphasised.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>ELISE's alignment with human experts and stability of autonomous AI Evaluator scores suggest that for a well-engineered, domain-specialized system the generation-validation gap can be small; multi-model AI Evaluators could autonomously judge outputs with minimal bias (anonymized vs identified conditions showed minimal variation).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported quantitatively; paper does not provide compute-time or cost comparisons between generation and validation steps.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2087.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2087.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-4o as used by the evaluators/pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose large language model (OpenAI GPT-4o) used here as a baseline for literature-extraction and summarization tasks; excels at metadata extraction but shows limitations in complex numeric/table interpretation and traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (general-purpose LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific text processing; evaluated here on biomedical literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual summaries, extracted metadata, numeric approximations from figures, references (DOIs)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>primarily in-distribution / generative recombination of training patterns; can produce plausible novel-seeming outputs (e.g., fabricated DOIs) which are not grounded in the document</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>probabilistic token-generation from learned language patterns (pattern extrapolation and recombination from pretraining), not document-grounded unless augmented</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>In this study, ChatGPT outputs were validated against human reference answers and AI Evaluator models (same protocols as for other tools); traceability scoring penalized ChatGPT for lack of document citations</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>High extraction performance (global Extraction ≈ 86.99%); Comprehension ≈ 7.48/10; Analysis ≈ 7.10/10. Performance variable across articles and degraded on tasks requiring precise table/figure reading.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation via AI Evaluators and human references showed ChatGPT scores improved when no human reference was present (indicating possible evaluator bias/self-assessment); lacked traceability leading to lower ECACT ranking when Compliance/Traceability included.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>No quantitative FPR reported; qualitative examples: fabricated DOI (ChatGPT reported a DOI not present in document) and incorrect EFS readouts (reported 21% and 30% vs expected 37% and 44%) indicating non-negligible false-positive fabrications in factual claims.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported quantitatively; qualitative failures include missed/incorrect extraction of exclusion counts and hazard ratios in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performance degrades for tasks requiring out-of-context inferences or precise numeric extraction from figures/tables (higher novelty from raw text), and propensity to fabricate appears higher for such tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper documents asymmetry: ChatGPT generates plausible fluent outputs but lacks traceability; validation penalizes it heavily when traceability/compliance are required, showing generation capability exceeds reliable validation/grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No internal uncertainty quantification output used in these experiments; model did not provide calibrated confidence metrics and sometimes asserted incorrect facts confidently.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not measured quantitatively; qualitative evidence of overconfident hallucinations (e.g., fabricated DOI) implies poor calibration for document-grounded factual claims.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Language robustness: retained similar Comprehension/Analysis across French and English for tested article but Extraction varied; poor performance on figure/table extraction (OOD-like tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Evaluators used semantic fidelity, completeness, and compliance/traceability to judge ChatGPT; ChatGPT often scored well on fluency but poorly on traceability proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for tasks beyond simple metadata extraction; frequency increases for numeric/clinical/regulatory outputs due to hallucination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / unstructured text — ChatGPT's generation suffers without domain-specific grounding</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Paper suggests RAG, traceability enforcement, and human oversight; also using AI Evaluator ensemble to detect inconsistencies, but ChatGPT lacks built-in trace outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>ChatGPT produced fabricated DOI and incorrect numeric readouts (EFS), and its rank dropped when traceability/compliance were included in evaluation, supporting existence of generation > validation reliability gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Strong extraction performance (~87%) shows generation of many correct, verifiable metadata outputs, indicating gap is task-dependent rather than absolute.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2087.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2087.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Epsilon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Epsilon (version 2.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized literature-analysis AI tool aimed at identifying research gaps and suggesting directions; moderate comprehension performance but weak structured extraction in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Epsilon</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>specialized NLP tool (domain-focused LLM/engine)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific literature analysis (biomedical emphasis in evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>research-gap suggestions, summaries, extracted metadata</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution to moderately novel suggestions (gap identification) but limited demonstrated novelty in high-precision extraction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>domain-tuned generation/retrieval-based summarization and suggestion engine</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated against human reference answers and AI Evaluator models; scored on Extraction/Comprehension/Analysis and penalized for Compliance/Traceability failures</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Extraction global average ≈ 37.41% (low); Comprehension ≈ 7.56/10; Analysis ≈ 6.98/10. Performance more variable than ELISE.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Scored moderately by AI Evaluators on comprehension but poor on extraction and compliance, reducing its ECACT ranking; autonomously evaluated scores increased when human references removed (indicating evaluator leniency).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not quantified; authors note failures in extracting numeric/graph data and some misinterpretations — no explicit FPR number.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Less capable for novel or complex extraction tasks (figures/tables); better at higher-level gap suggestion tasks (moderate novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation of gap-suggestions may outpace reliable validation for extraction tasks; the paper notes decreases in overall score when compliance/traceability enforced.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Language experiments showed stable comprehension but lower extraction scores; no formal OOD metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Evaluators used semantic fidelity, completeness, compliance and traceability proxies; Epsilon often penalized on these.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended, especially for extraction and regulatory-use cases where compliance matters.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / semi-structured</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Paper recommends traceability, RAG and human oversight to mitigate Epsilon's validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Low Extraction score (37.41%) vs moderate Comprehension indicates generation of higher-level suggestions is not matched by capacity to ground/validate detailed facts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Moderate comprehension scores suggest Epsilon can produce reasonably coherent, partially correct analyses when tasks are less detail-sensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2087.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2087.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Humata</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Humata (Tilda technologies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI text-analysis service focused on summarization of large documents; in this study it shows moderate comprehension and variable extraction performance, with particularly poor extraction in some language conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Humata</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>document-focused NLP summarization engine</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>document summarization and scientific text analysis</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>summaries, extracted facts, answers to structured queries</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>mainly in-distribution summarization and extraction; not designed for high-novelty hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>document-level summarization using retrieval + LLM-based text generation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated against human references and AI Evaluator models; scored on Extraction/Comprehension/Analysis, with traceability/compliance checks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Extraction variable (global ≈ 52.00% reported elsewhere but language-specific extraction scores showed 0.00 in French vs 3.61 in English for a paired article), Comprehension varied 5.0–8.0 across articles; Analysis ≈ 6.86/10</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>AI Evaluators produced variable scores; Humata showed larger variability and poorer alignment with human experts than ELISE.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not provided numerically; qualitative failures include misinterpretations of exclusion criteria and numeric table extractions in case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not provided numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Struggles more with novel/complex extraction tasks (figures/tables) and cross-lingual extraction; comprehension/analysis less sensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation (summaries) can be fluent, but validation and traceability are weaker leading to reduced reliability for regulatory tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poorer on French extraction in tested pair; otherwise no formal OOD metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Evaluated using semantic fidelity, completeness, compliance and traceability proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended frequently, especially for extraction and cross-lingual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / document-centric</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Paper suggests RAG, improved parsers and human oversight to reduce errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Large variability and cases of misinterpretation (e.g., exclusion criteria) indicate generation can exceed reliable validation/trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Occasional high comprehension/analysis scores indicate capability when tasks are simpler or well-specified.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2087.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2087.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciSpace/Typeset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciSpace (ex-Typeset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature exploration and document-analysis platform evaluated for extraction, comprehension and analysis tasks; showed moderate-to-low extraction ability but acceptable comprehension/analysis in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciSpace / Typeset</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>document search and summarization platform with NLP features</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific literature discovery and summary</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>search results, document summaries, extracted metadata</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution summarization and extraction; not aimed at producing novel scientific hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>retrieval-based summarization and NER-assisted extraction</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared to human references and AI Evaluator models; scored on ECACT criteria including traceability and compliance</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Extraction global ≈ 49.10%; Comprehension and Analysis ~6.66–7.10 depending on metric; variable by article and language (very low extraction in French/English pair: 1.11 and 1.94 respectively on one test).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Scored lower on extraction and traceability; evaluators show variability across evaluators but overall mid-to-low ranks.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not numerically reported; several failure cases where it could not provide exact numbers (e.g., excluded articles) are noted.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not numerically reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performs worse on high-novelty/inference tasks (figure readouts, table cross-reading); better on straightforward retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation (summaries) sometimes plausible but validation fails for structured numeric extractions; gap present for structured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor on certain cross-lingual extraction tasks; no formal OOD metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Evaluated with semantic fidelity, completeness, compliance and traceability proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended, particularly for extraction/registry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / document-centric</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>RAG, improved parsers and human oversight suggested.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Inability to extract exact exclusion counts and failures in figure/table tasks indicates generation often lacked validated grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some moderate comprehension/analysis scores suggest acceptable performance for narrative summarization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2087.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2087.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Evaluator ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Evaluator models (Claude 3.5 Sonnet, GPT-4o, o1 Preview ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of independent high-performing LLMs employed as objective evaluators to score AI tool outputs against human references and document content to reduce human bias in assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Claude 3.5 Sonnet; GPT-4o; o1 Preview (ensemble averaging)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language models used as evaluators / automated scorers</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>meta-evaluation of NLP outputs across scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>numerical scores and justifications (10-point rubric), comparative judgments</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>not generative of scientific claims here but used for evaluation; novelty not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>scoring via structured prompts comparing AI outputs to human references or to document content (anonymized or identified conditions)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used to validate other tools' outputs by scoring against human-defined references and by autonomous scoring from document content alone; scores averaged across models to mitigate bias</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable (they are evaluators); reported to be consistent between anonymized and identified conditions with minimal bias, except one divergence for ChatGPT scoring by Claude 3.5 Sonnet vs GPT-4o/o1 Preview on a single article.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Provided justified scores out of 10 for each response; their averaged scores were used in final tool rankings and shown to be robust (minimal variation due to anonymization).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported; as evaluators they may over/under-score but no numeric FPR of their judgments provided. Paper notes some evaluator leniency when no human reference present (scores inflated for some tools).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Evaluators tended to give higher scores when human references were absent for some tools (suggesting validation leniency increases for ambiguous/novel outputs), but overall anonymization tests showed minimal bias.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>These models were not used to generate scientific claims here; they served to validate other models and demonstrated that automated evaluation can approximate human judgment but may be lenient in absence of human references.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Evaluators provided justifications with each score but no standardized uncertainty/confidence metrics were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; authors averaged multiple evaluators to improve calibration of scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Used rubric-based proxies (semantic fidelity, relevance, completeness) rather than formal correctness tests; also used anonymized vs identified comparisons to detect bias.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Used in study both with and without human references; when without human references some tools' scores increased indicating human benchmarking remains important.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical/textual evaluation domain</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Averaging across multiple independent evaluator LLMs; anonymization tests to detect identity bias; combining with human references when available.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>When human references were removed, some tools' scores rose (0.3–0.9 points), suggesting automated evaluators can overestimate quality of less-grounded generations — indicating a potential validation shortcoming for novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Anonymized vs identified scoring showed minimal variation overall, supporting the viability of multi-model automated evaluation to reduce bias.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2087.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2087.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parsers (Matsu / Megaparse / Llama / Unstructured)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Document parsers evaluated (Matsu = ELISE parser; Megaparse; Llama parser; Unstructured)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tools for converting unstructured document content into structured text for downstream LLM consumption; Matsu (ELISE) showed lower Normalized Edit Distance (NED) indicating better textual parsing performance in a preliminary comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Matsu (ELISE parser); Megaparse; Llama parser; Unstructured</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>document parsing / preprocessing engines</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>document parsing for scientific text extraction</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>structured textual representations (for NER, table extraction, figure interpretation)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>not generative of science; novelty pertains to parsing improvements (Matsu improved parsing quality)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>text extraction, normalization, structural tagging of document elements</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Preliminary Normalized Edit Distance (NED) computed against reference 'global performance metric for textual content' to assess parser fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>NED reported: Megaparse 0.558241; Llama 0.622417; Unstructured 0.574559; Matsu (ELISE) 0.490463 (0 = identical, 1 = completely different) — lower is better, indicating Matsu parsed text more closely to references.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>NED used as proxy validation metric; no further numeric validation beyond NED reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Better parser fidelity (lower NED) supports more accurate downstream extraction and reduces generation-validation gaps for novel inferences from documents; quantitative relationship not deeply explored.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper links parser quality (Matsu lower NED) to ELISE's superior downstream extraction/analysis performance, indicating parsing improvements close the gen-val gap.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Normalized Edit Distance (NED) used as a proxy metric for parser accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Parsers improve downstream automation but human checks still recommended for high-stakes data extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical/document-processing</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Improved parser (Matsu) combined with RAG and traceability to reduce extraction errors; NED used to benchmark parsers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Higher NED parsers correlate with poorer downstream extraction in other tools; ELISE's lower NED aligns with its superior extraction/analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>NED is a proxy and cannot alone guarantee correct scientific interpretation — high parser fidelity does not fully eliminate need for expert validation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating literature reviews conducted by humans versus ChatGPT: comparative study <em>(Rating: 2)</em></li>
                <li>Quality and effectiveness of AI tools for students and researchers for scientific literature review and analysis <em>(Rating: 2)</em></li>
                <li>Artificial intelligence for literature reviews: opportunities and challenges <em>(Rating: 1)</em></li>
                <li>Augmenting large language models with chemistry tools <em>(Rating: 1)</em></li>
                <li>Guidelines for clinical trials using artificial intelligence -SPIRIT-AI and CONSORT-AI <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2087",
    "paper_id": "paper-278537074",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "ELISE",
            "name_full": "ELISE (Elevated LIfe SciencEs)",
            "brief_description": "A specialized AI literature-analysis system developed by Biolevate combining an advanced document parser (Matsu) with context-aware LLM modules and retrieval-augmented generation to extract, comprehend and analyze biomedical literature with traceability and compliance features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ELISE",
            "system_type": "specialized retrieval-augmented NLP system (LLM + custom parser)",
            "scientific_domain": "biomedical / regulatory literature analysis",
            "output_type": "structured data extraction (metadata, tables, HRs), summaries, analytical insights and traceable explanations",
            "novelty_level": "primarily in-distribution / incremental for literature synthesis; moderate novelty for inferred numerical approximations from figures (e.g., KM readouts)",
            "generation_method": "retrieval-augmented generation combining document retrieval, a custom parser (Matsu) and context-aware LLM modules to recombine extracted facts and produce structured outputs and explainable analyses",
            "validation_method": "multi-step: comparison against human expert reference answers (three experts), independent AI Evaluator models (Claude 3.5 Sonnet, GPT-4o, o1 Preview) scoring on standardized rubrics; statistical analyses (ANOVA/Tukey) and task-specific checks (traceability and compliance scoring); parser-level NED comparisons",
            "generation_performance": "High generation performance reported: Extraction global average 87.50%; Comprehension mean ≈ 8.23/10; Analysis mean ≈ 7.98/10; stable scores with/without human reference (Comprehension 8.32–8.42, Analysis 7.98–8.13). Performance reported across n=9 articles.",
            "validation_performance": "Validation via human reference + 3 AI Evaluators produced consistent scoring; ELISE showed close alignment with human experts (minimal score variation between human-referenced and autonomous evaluation). No single numeric 'validation accuracy' reported beyond evaluator scores which averaged into ECACT-weighted final score (top-ranked).",
            "false_positive_rate": "Not reported quantitatively; qualitative evidence shows ELISE produced closest approximations and explicitly indicated margin-of-error in figure-read tasks (e.g., EFS approximations) and correctly retrieved a hazard ratio (HR 0.31, 95% CI 0.13–0.74) where others failed. No numeric FPR provided.",
            "false_negative_rate": "Not reported quantitatively in the paper. Authors report few missed items for ELISE in tested tasks (high extraction completeness) but no numeric FNR.",
            "performance_vs_novelty": "Reported evidence that ELISE maintained performance for tasks requiring inference from figures/tables (moderate novelty) and had minimal degradation when human references were absent; explicit quantitative relationship between novelty level and validation accuracy not provided.",
            "generation_validation_comparison": "Paper reports close parity for ELISE between generation quality and validation (ELISE outputs closely matched human references and AI Evaluator scores), indicating a small generation-validation gap for ELISE relative to other tools.",
            "uncertainty_quantification": "Limited formal UQ: example where ELISE 'explicitly indicated a margin of error' when approximating Kaplan–Meier readouts; no standardized uncertainty scores or calibration metrics reported.",
            "calibration_quality": "Not quantified with calibration metrics; qualitative claim that ELISE provides explainability and traceability which aids perceived calibration but no numeric calibration quality reported.",
            "out_of_distribution_performance": "Language-change experiment: maintained Comprehension/Analysis across French/English (minor variation) and better English Extraction in some cases; no formal OOD metrics for transformational discoveries.",
            "validation_proxy_metrics": "Yes — relied on proxy metrics including semantic fidelity, completeness, Compliance and Traceability scores, ECACT composite score (weighted sum up to 80), and parser Normalized Edit Distance (NED) for textual parsing (Matsu NED=0.490463 reported).",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for high-stakes outputs; study used human reference answers for every article during one evaluation condition and notes human oversight is essential, especially for regulatory contexts. Frequency increases for higher novelty / critical tasks (stated qualitatively).",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (biomedical/regulatory literature) — semi-structured but not formally specified; this increases reliance on expert validation",
            "gap_mitigation_strategies": "ECACT evaluation framework (Extraction, Comprehension, Analysis + Compliance, Traceability), use of independent multi-model AI Evaluators, human expert reference answers, parser improvements (Matsu), RAG architectures and explicit traceability outputs.",
            "evidence_supporting_gap": "General observation in paper: other tools (ChatGPT, Epsilon, Humata, SciSpace) produced incorrect extractions (e.g., ChatGPT hallucinated DOI, produced incorrect EFS percentages 21/30% vs expected 37/44%), while ELISE succeeded — implying generation varied in quality and some models produced plausible but incorrect outputs (fabrications). The need for human oversight is repeatedly emphasised.",
            "evidence_contradicting_gap": "ELISE's alignment with human experts and stability of autonomous AI Evaluator scores suggest that for a well-engineered, domain-specialized system the generation-validation gap can be small; multi-model AI Evaluators could autonomously judge outputs with minimal bias (anonymized vs identified conditions showed minimal variation).",
            "computational_cost_ratio": "Not reported quantitatively; paper does not provide compute-time or cost comparisons between generation and validation steps.",
            "uuid": "e2087.0"
        },
        {
            "name_short": "ChatGPT (GPT-4o)",
            "name_full": "ChatGPT (GPT-4o as used by the evaluators/pipeline)",
            "brief_description": "A general-purpose large language model (OpenAI GPT-4o) used here as a baseline for literature-extraction and summarization tasks; excels at metadata extraction but shows limitations in complex numeric/table interpretation and traceability.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ChatGPT (GPT-4o)",
            "system_type": "large language model (general-purpose LLM)",
            "scientific_domain": "general scientific text processing; evaluated here on biomedical literature",
            "output_type": "textual summaries, extracted metadata, numeric approximations from figures, references (DOIs)",
            "novelty_level": "primarily in-distribution / generative recombination of training patterns; can produce plausible novel-seeming outputs (e.g., fabricated DOIs) which are not grounded in the document",
            "generation_method": "probabilistic token-generation from learned language patterns (pattern extrapolation and recombination from pretraining), not document-grounded unless augmented",
            "validation_method": "In this study, ChatGPT outputs were validated against human reference answers and AI Evaluator models (same protocols as for other tools); traceability scoring penalized ChatGPT for lack of document citations",
            "generation_performance": "High extraction performance (global Extraction ≈ 86.99%); Comprehension ≈ 7.48/10; Analysis ≈ 7.10/10. Performance variable across articles and degraded on tasks requiring precise table/figure reading.",
            "validation_performance": "Validation via AI Evaluators and human references showed ChatGPT scores improved when no human reference was present (indicating possible evaluator bias/self-assessment); lacked traceability leading to lower ECACT ranking when Compliance/Traceability included.",
            "false_positive_rate": "No quantitative FPR reported; qualitative examples: fabricated DOI (ChatGPT reported a DOI not present in document) and incorrect EFS readouts (reported 21% and 30% vs expected 37% and 44%) indicating non-negligible false-positive fabrications in factual claims.",
            "false_negative_rate": "Not reported quantitatively; qualitative failures include missed/incorrect extraction of exclusion counts and hazard ratios in some tasks.",
            "performance_vs_novelty": "Performance degrades for tasks requiring out-of-context inferences or precise numeric extraction from figures/tables (higher novelty from raw text), and propensity to fabricate appears higher for such tasks.",
            "generation_validation_comparison": "Paper documents asymmetry: ChatGPT generates plausible fluent outputs but lacks traceability; validation penalizes it heavily when traceability/compliance are required, showing generation capability exceeds reliable validation/grounding.",
            "uncertainty_quantification": "No internal uncertainty quantification output used in these experiments; model did not provide calibrated confidence metrics and sometimes asserted incorrect facts confidently.",
            "calibration_quality": "Not measured quantitatively; qualitative evidence of overconfident hallucinations (e.g., fabricated DOI) implies poor calibration for document-grounded factual claims.",
            "out_of_distribution_performance": "Language robustness: retained similar Comprehension/Analysis across French and English for tested article but Extraction varied; poor performance on figure/table extraction (OOD-like tasks).",
            "validation_proxy_metrics": "Evaluators used semantic fidelity, completeness, and compliance/traceability to judge ChatGPT; ChatGPT often scored well on fluency but poorly on traceability proxies.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for tasks beyond simple metadata extraction; frequency increases for numeric/clinical/regulatory outputs due to hallucination risk.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / unstructured text — ChatGPT's generation suffers without domain-specific grounding",
            "gap_mitigation_strategies": "Paper suggests RAG, traceability enforcement, and human oversight; also using AI Evaluator ensemble to detect inconsistencies, but ChatGPT lacks built-in trace outputs.",
            "evidence_supporting_gap": "ChatGPT produced fabricated DOI and incorrect numeric readouts (EFS), and its rank dropped when traceability/compliance were included in evaluation, supporting existence of generation &gt; validation reliability gap.",
            "evidence_contradicting_gap": "Strong extraction performance (~87%) shows generation of many correct, verifiable metadata outputs, indicating gap is task-dependent rather than absolute.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2087.1"
        },
        {
            "name_short": "Epsilon",
            "name_full": "Epsilon (version 2.5)",
            "brief_description": "A specialized literature-analysis AI tool aimed at identifying research gaps and suggesting directions; moderate comprehension performance but weak structured extraction in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Epsilon",
            "system_type": "specialized NLP tool (domain-focused LLM/engine)",
            "scientific_domain": "scientific literature analysis (biomedical emphasis in evaluation)",
            "output_type": "research-gap suggestions, summaries, extracted metadata",
            "novelty_level": "in-distribution to moderately novel suggestions (gap identification) but limited demonstrated novelty in high-precision extraction tasks",
            "generation_method": "domain-tuned generation/retrieval-based summarization and suggestion engine",
            "validation_method": "Validated against human reference answers and AI Evaluator models; scored on Extraction/Comprehension/Analysis and penalized for Compliance/Traceability failures",
            "generation_performance": "Extraction global average ≈ 37.41% (low); Comprehension ≈ 7.56/10; Analysis ≈ 6.98/10. Performance more variable than ELISE.",
            "validation_performance": "Scored moderately by AI Evaluators on comprehension but poor on extraction and compliance, reducing its ECACT ranking; autonomously evaluated scores increased when human references removed (indicating evaluator leniency).",
            "false_positive_rate": "Not quantified; authors note failures in extracting numeric/graph data and some misinterpretations — no explicit FPR number.",
            "false_negative_rate": "Not quantified.",
            "performance_vs_novelty": "Less capable for novel or complex extraction tasks (figures/tables); better at higher-level gap suggestion tasks (moderate novelty).",
            "generation_validation_comparison": "Generation of gap-suggestions may outpace reliable validation for extraction tasks; the paper notes decreases in overall score when compliance/traceability enforced.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Language experiments showed stable comprehension but lower extraction scores; no formal OOD metrics.",
            "validation_proxy_metrics": "Evaluators used semantic fidelity, completeness, compliance and traceability proxies; Epsilon often penalized on these.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended, especially for extraction and regulatory-use cases where compliance matters.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / semi-structured",
            "gap_mitigation_strategies": "Paper recommends traceability, RAG and human oversight to mitigate Epsilon's validation gap.",
            "evidence_supporting_gap": "Low Extraction score (37.41%) vs moderate Comprehension indicates generation of higher-level suggestions is not matched by capacity to ground/validate detailed facts.",
            "evidence_contradicting_gap": "Moderate comprehension scores suggest Epsilon can produce reasonably coherent, partially correct analyses when tasks are less detail-sensitive.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2087.2"
        },
        {
            "name_short": "Humata",
            "name_full": "Humata (Tilda technologies)",
            "brief_description": "An AI text-analysis service focused on summarization of large documents; in this study it shows moderate comprehension and variable extraction performance, with particularly poor extraction in some language conditions.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Humata",
            "system_type": "document-focused NLP summarization engine",
            "scientific_domain": "document summarization and scientific text analysis",
            "output_type": "summaries, extracted facts, answers to structured queries",
            "novelty_level": "mainly in-distribution summarization and extraction; not designed for high-novelty hypothesis generation",
            "generation_method": "document-level summarization using retrieval + LLM-based text generation",
            "validation_method": "Validated against human references and AI Evaluator models; scored on Extraction/Comprehension/Analysis, with traceability/compliance checks",
            "generation_performance": "Extraction variable (global ≈ 52.00% reported elsewhere but language-specific extraction scores showed 0.00 in French vs 3.61 in English for a paired article), Comprehension varied 5.0–8.0 across articles; Analysis ≈ 6.86/10",
            "validation_performance": "AI Evaluators produced variable scores; Humata showed larger variability and poorer alignment with human experts than ELISE.",
            "false_positive_rate": "Not provided numerically; qualitative failures include misinterpretations of exclusion criteria and numeric table extractions in case studies.",
            "false_negative_rate": "Not provided numerically.",
            "performance_vs_novelty": "Struggles more with novel/complex extraction tasks (figures/tables) and cross-lingual extraction; comprehension/analysis less sensitive.",
            "generation_validation_comparison": "Generation (summaries) can be fluent, but validation and traceability are weaker leading to reduced reliability for regulatory tasks.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Poorer on French extraction in tested pair; otherwise no formal OOD metrics.",
            "validation_proxy_metrics": "Evaluated using semantic fidelity, completeness, compliance and traceability proxies.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended frequently, especially for extraction and cross-lingual tasks.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / document-centric",
            "gap_mitigation_strategies": "Paper suggests RAG, improved parsers and human oversight to reduce errors.",
            "evidence_supporting_gap": "Large variability and cases of misinterpretation (e.g., exclusion criteria) indicate generation can exceed reliable validation/trustworthiness.",
            "evidence_contradicting_gap": "Occasional high comprehension/analysis scores indicate capability when tasks are simpler or well-specified.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2087.3"
        },
        {
            "name_short": "SciSpace/Typeset",
            "name_full": "SciSpace (ex-Typeset)",
            "brief_description": "A literature exploration and document-analysis platform evaluated for extraction, comprehension and analysis tasks; showed moderate-to-low extraction ability but acceptable comprehension/analysis in some cases.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "SciSpace / Typeset",
            "system_type": "document search and summarization platform with NLP features",
            "scientific_domain": "scientific literature discovery and summary",
            "output_type": "search results, document summaries, extracted metadata",
            "novelty_level": "in-distribution summarization and extraction; not aimed at producing novel scientific hypotheses",
            "generation_method": "retrieval-based summarization and NER-assisted extraction",
            "validation_method": "Compared to human references and AI Evaluator models; scored on ECACT criteria including traceability and compliance",
            "generation_performance": "Extraction global ≈ 49.10%; Comprehension and Analysis ~6.66–7.10 depending on metric; variable by article and language (very low extraction in French/English pair: 1.11 and 1.94 respectively on one test).",
            "validation_performance": "Scored lower on extraction and traceability; evaluators show variability across evaluators but overall mid-to-low ranks.",
            "false_positive_rate": "Not numerically reported; several failure cases where it could not provide exact numbers (e.g., excluded articles) are noted.",
            "false_negative_rate": "Not numerically reported.",
            "performance_vs_novelty": "Performs worse on high-novelty/inference tasks (figure readouts, table cross-reading); better on straightforward retrieval.",
            "generation_validation_comparison": "Generation (summaries) sometimes plausible but validation fails for structured numeric extractions; gap present for structured tasks.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Poor on certain cross-lingual extraction tasks; no formal OOD metrics.",
            "validation_proxy_metrics": "Evaluated with semantic fidelity, completeness, compliance and traceability proxies.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended, particularly for extraction/registry tasks.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / document-centric",
            "gap_mitigation_strategies": "RAG, improved parsers and human oversight suggested.",
            "evidence_supporting_gap": "Inability to extract exact exclusion counts and failures in figure/table tasks indicates generation often lacked validated grounding.",
            "evidence_contradicting_gap": "Some moderate comprehension/analysis scores suggest acceptable performance for narrative summarization tasks.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2087.4"
        },
        {
            "name_short": "AI Evaluator ensemble",
            "name_full": "AI Evaluator models (Claude 3.5 Sonnet, GPT-4o, o1 Preview ensemble)",
            "brief_description": "A set of independent high-performing LLMs employed as objective evaluators to score AI tool outputs against human references and document content to reduce human bias in assessment.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Claude 3.5 Sonnet; GPT-4o; o1 Preview (ensemble averaging)",
            "system_type": "large language models used as evaluators / automated scorers",
            "scientific_domain": "meta-evaluation of NLP outputs across scientific literature",
            "output_type": "numerical scores and justifications (10-point rubric), comparative judgments",
            "novelty_level": "not generative of scientific claims here but used for evaluation; novelty not applicable",
            "generation_method": "scoring via structured prompts comparing AI outputs to human references or to document content (anonymized or identified conditions)",
            "validation_method": "Used to validate other tools' outputs by scoring against human-defined references and by autonomous scoring from document content alone; scores averaged across models to mitigate bias",
            "generation_performance": "Not applicable (they are evaluators); reported to be consistent between anonymized and identified conditions with minimal bias, except one divergence for ChatGPT scoring by Claude 3.5 Sonnet vs GPT-4o/o1 Preview on a single article.",
            "validation_performance": "Provided justified scores out of 10 for each response; their averaged scores were used in final tool rankings and shown to be robust (minimal variation due to anonymization).",
            "false_positive_rate": "Not reported; as evaluators they may over/under-score but no numeric FPR of their judgments provided. Paper notes some evaluator leniency when no human reference present (scores inflated for some tools).",
            "false_negative_rate": "Not reported numerically.",
            "performance_vs_novelty": "Evaluators tended to give higher scores when human references were absent for some tools (suggesting validation leniency increases for ambiguous/novel outputs), but overall anonymization tests showed minimal bias.",
            "generation_validation_comparison": "These models were not used to generate scientific claims here; they served to validate other models and demonstrated that automated evaluation can approximate human judgment but may be lenient in absence of human references.",
            "uncertainty_quantification": "Evaluators provided justifications with each score but no standardized uncertainty/confidence metrics were reported.",
            "calibration_quality": "Not quantified; authors averaged multiple evaluators to improve calibration of scoring.",
            "out_of_distribution_performance": "Not applicable.",
            "validation_proxy_metrics": "Used rubric-based proxies (semantic fidelity, relevance, completeness) rather than formal correctness tests; also used anonymized vs identified comparisons to detect bias.",
            "human_validation_required": false,
            "human_validation_frequency": "Used in study both with and without human references; when without human references some tools' scores increased indicating human benchmarking remains important.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical/textual evaluation domain",
            "gap_mitigation_strategies": "Averaging across multiple independent evaluator LLMs; anonymization tests to detect identity bias; combining with human references when available.",
            "evidence_supporting_gap": "When human references were removed, some tools' scores rose (0.3–0.9 points), suggesting automated evaluators can overestimate quality of less-grounded generations — indicating a potential validation shortcoming for novel outputs.",
            "evidence_contradicting_gap": "Anonymized vs identified scoring showed minimal variation overall, supporting the viability of multi-model automated evaluation to reduce bias.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2087.5"
        },
        {
            "name_short": "Parsers (Matsu / Megaparse / Llama / Unstructured)",
            "name_full": "Document parsers evaluated (Matsu = ELISE parser; Megaparse; Llama parser; Unstructured)",
            "brief_description": "Tools for converting unstructured document content into structured text for downstream LLM consumption; Matsu (ELISE) showed lower Normalized Edit Distance (NED) indicating better textual parsing performance in a preliminary comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Matsu (ELISE parser); Megaparse; Llama parser; Unstructured",
            "system_type": "document parsing / preprocessing engines",
            "scientific_domain": "document parsing for scientific text extraction",
            "output_type": "structured textual representations (for NER, table extraction, figure interpretation)",
            "novelty_level": "not generative of science; novelty pertains to parsing improvements (Matsu improved parsing quality)",
            "generation_method": "text extraction, normalization, structural tagging of document elements",
            "validation_method": "Preliminary Normalized Edit Distance (NED) computed against reference 'global performance metric for textual content' to assess parser fidelity",
            "generation_performance": "NED reported: Megaparse 0.558241; Llama 0.622417; Unstructured 0.574559; Matsu (ELISE) 0.490463 (0 = identical, 1 = completely different) — lower is better, indicating Matsu parsed text more closely to references.",
            "validation_performance": "NED used as proxy validation metric; no further numeric validation beyond NED reported.",
            "false_positive_rate": "Not reported.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Better parser fidelity (lower NED) supports more accurate downstream extraction and reduces generation-validation gaps for novel inferences from documents; quantitative relationship not deeply explored.",
            "generation_validation_comparison": "Paper links parser quality (Matsu lower NED) to ELISE's superior downstream extraction/analysis performance, indicating parsing improvements close the gen-val gap.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not applicable.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "Normalized Edit Distance (NED) used as a proxy metric for parser accuracy.",
            "human_validation_required": true,
            "human_validation_frequency": "Parsers improve downstream automation but human checks still recommended for high-stakes data extraction.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical/document-processing",
            "gap_mitigation_strategies": "Improved parser (Matsu) combined with RAG and traceability to reduce extraction errors; NED used to benchmark parsers.",
            "evidence_supporting_gap": "Higher NED parsers correlate with poorer downstream extraction in other tools; ELISE's lower NED aligns with its superior extraction/analysis.",
            "evidence_contradicting_gap": "NED is a proxy and cannot alone guarantee correct scientific interpretation — high parser fidelity does not fully eliminate need for expert validation.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2087.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating literature reviews conducted by humans versus ChatGPT: comparative study",
            "rating": 2
        },
        {
            "paper_title": "Quality and effectiveness of AI tools for students and researchers for scientific literature review and analysis",
            "rating": 2
        },
        {
            "paper_title": "Artificial intelligence for literature reviews: opportunities and challenges",
            "rating": 1
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools",
            "rating": 1
        },
        {
            "paper_title": "Guidelines for clinical trials using artificial intelligence -SPIRIT-AI and CONSORT-AI",
            "rating": 2
        }
    ],
    "cost": 0.019139,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From data extraction to analysis: a comparative study of ELISE capabilities in scientific literature
12 May 2025</p>
<p>Maxime Gobin maxime@biolevate.com 
Muriel Gosnat 
Seindé Toure 
Lina Faik 
Joel Belafa 
Antoine Villedieu De Torcy 
Florence Armstrong 
Pengfei Zhang 
Jie Wang </p>
<p>Biolevate, ParisFrance</p>
<p>Chengdu University of Traditional Chinese Medicine
China</p>
<p>Southwest Jiaotong University
China</p>
<p>Cq Tan
Chengdu University of Traditional Chinese Medicine
China</p>
<p>From data extraction to analysis: a comparative study of ELISE capabilities in scientific literature
12 May 20255843DD62E7966D9CDD055A1CC854E5E510.3389/frai.2025.1587244RECEIVED 04 March 2025 ACCEPTED 18 April 2025scientific literaturesystematic reviewdata extractionAI toolElevated LIfe SciencE solution
The exponential growth of scientific literature presents challenges for pharmaceutical, biotechnological, and Medtech industries, particularly in regulatory documentation, clinical research, and systematic reviews.Ensuring accurate data extraction, literature synthesis, and compliance with industry standards require AI tools that not only streamline workflows but also uphold scientific rigor.This study evaluates the performance of AI tools designed for bibliographic review, data extraction, and scientific synthesis, assessing their impact on decision-making, regulatory compliance, and research productivity.The AI tools assessed include generalpurpose models like ChatGPT and specialized solutions such as ELISE (Elevated LIfe SciencEs), SciSpace/Typeset, Humata, and Epsilon.The evaluation is based on three main criteria: Extraction, Comprehension, and Analysis with Compliance and Traceability (ECACT) as additional dimensions.Human experts established reference benchmarks, while AI Evaluator models ensure objective performance measurement.The study introduces the ECACT score, a structured metric assessing AI reliability in scientific literature analysis, regulatory reporting and clinical documentation.Results demonstrate that ELISE consistently outperforms other AI tools, excelling in precise data extraction, deep contextual comprehension, and advanced content analysis.ELISE's ability to generate traceable, well-reasoned insights makes it particularly well-suited for high-stakes applications such as regulatory affairs, clinical trials, and medical documentation, where accuracy, transparency, and compliance are paramount.Unlike other AI tools, ELISE provides expert-level reasoning and explainability, ensuring AI-generated insights align with industry best practices.ChatGPT is efficient in data retrieval but lacks precision in complex analysis, limiting its use in high-stakes decision-making.Epsilon, Humata, and SciSpace/Typeset exhibit moderate performance, with variability affecting their reliability in critical applications.In conclusion, while AI tools such as ELISE enhance literature review, regulatory writing, and clinical data interpretation, human oversight remains essential to validate AI outputs and ensure compliance with scientific and regulatory standards.For pharmaceutical, biotechnological, and Medtech industries, AI integration must strike a balance between automation and expert supervision to maintain data integrity, transparency, and regulatory adherence.</p>
<p>Introduction</p>
<p>In recent years, the integration of artificial intelligence (AI) into scientific research has significantly transformed academic publishing, drug development, and biomedical innovation.AI-driven literature analysis tools, or AI tools, have become essential, offering groundbreaking capabilities in processing, analyzing, and summarizing large volumes of scientific papers (Agrawal Gobin et al. 10.3389/frai.2025.1587244Frontiers in Artificial Intelligence 02 frontiersin.orget al., 2024;Danler et al., 2024;Khalifa and Ibrahim, 2024).These tools leverage advanced natural language processing (NLP) techniques to streamline traditionally manual tasks, such as literature reviews and data extraction (Fabiano et al., 2024).The development of sophisticated AI algorithms, fueled by increased computational power and growing data availability, has revolutionized how researchers and industry professionals interact with scientific literature (Zahra et al., 2024).AI-driven literature analysis tools can be broadly categorized into general-purpose models and specialized scientific models, each serving distinct roles in research.General-purpose AI models, such as OpenAI's ChatGPT, are designed for versatility, able to handle a wide array of tasks across various domains.These models excel in summarizing and interpreting general information, offering researchers a flexible tool for diverse inquiries (Ning et al., 2024).In contrast, specialized tools like ELISE (the tool we developed at Biolevate), SciSpace (ex-Typeset), Humata, and Epsilon are tailored to meet the specific needs of scientific and biomedical research, including pharmaceutical, biotech and Medtech applications (Zahra et al., 2024;Tsirmpas et al., 2024).These applications focus on precision and relevance, employing advanced NLP techniques to extract and analyze data with a high level of specificity for regulated and highly technical fields.While general-purpose tools provide broad applicability, specialized tools ensure accuracy and domain relevance, crucial for rigorous academic inquiry.</p>
<p>More specifically, ChatGPT excels in conversational interactions, aiding in literature reviews and hypothesis generation (Khalifa and Ibrahim, 2024).SciSpace/Typeset enhances literature exploration with intuitive interfaces and robust search capabilities (SciSpace, 2024).Humata focuses on AI-driven text analysis, efficiently summarizing large volumes of data (Humata, 2024).Epsilon identifies research gaps, suggesting innovative directions (Epsilon, 2024).Meanwhile, ELISE advances document analysis with sophisticated NLP and data extraction techniques, addressing the growing need for efficient data management in research (ELISE, 2024).</p>
<p>Together, these AI tools enhance research quality and efficiency, facilitating a more comprehensive understanding of complex scientific literature and accelerating the pace of discovery by reducing the time required for knowledge synthesis (Borah et al., 2017;Chakraborty et al., 2024;Mehta et al., 2024).They support key industry processes, from early-stage drug discovery to regulatory documentation preparation, helping companies navigate vast and evolving scientific landscape (Agrawal et al., 2024).For pharmaceutical, biotech, and Medtech companies, AI tools can improve literature surveillance, support regulatory submissions (e.g., EMA/FDA filings), and optimize knowledge management for evidence-based decision-making.</p>
<p>Despite these advantages, integrating AI into scientific research is not without challenges, particularly in ensuring the reliability and consistency of AI-generated insights.A significant issue is the variability in AI-generated responses, which stems from the diverse methodologies and algorithms employed by different tools (Bolaños et al., 2024).This inconsistency complicates the standardization of evaluations, as outputs can vary in quality and relevance.Additionally, the lack of traceability in AI-generated insights raises concerns about the validity of interpretations, particularly in fields requiring high precision, such as biomedical and clinical research (Agrawal et al., 2024;Danler et al., 2024).Furthermore, the absence of a standardized framework for evaluating AI tools poses a threat to research reliability and validity (Ning et al., 2024;Sharma and Ruikar, 2024).</p>
<p>Therefore, there is a pressing need for rigorous and standardized evaluations to ensure that AI tools contribute effectively and reliably to scientific research, safeguarding the rigor and integrity of research outcomes.</p>
<p>This study aims to provide the scientific community and industry stakeholders with a clearer understanding of the potential and limitations of AI-driven literature analysis technologies.It also introduces a structured and independent methodology based on three main criteria (Extraction, Comprehension, and Analysis), to evaluate these applications, including ELISE, a novel AI tool integrating NLP and retrieval strategies to enhance workflows, improve regulatory compliance and optimize R&amp;D processes for pharma, biotech and Medtech organizations.</p>
<p>Methods</p>
<p>Selected articles</p>
<p>To ensure a comprehensive and exhaustive evaluation, a diverse selection of scientific articles was chosen, covering different disciplines and study types.The selected articles are detailed in Table 1.</p>
<p>The current selection emphasized experimental and applied biomedical literature to stress-test AI tools in high-rigor contexts.Future work will include broader disciplinary representation to assess generalizability.This initial dataset includes articles in both English and French, allowing us to test cross-lingual consistency of the ECACT scoring framework.However, broader validation across other languages and domains remains a priority for future work.</p>
<p>The sample was restricted to nine articles to allow for a controlled proof-of-concept analysis.This scale enabled full evaluation across five criteria and three evaluators per article.However, broader validation will require expansion to larger datasets.</p>
<p>AI models used for evaluation</p>
<p>AI tools</p>
<p>Among the most popular, several AI-driven literature analysis tools were selected based on their ability to extract, comprehend, and analyze scientific content.Table 2A summarizes their main characteristics.</p>
<p>AI evaluator models</p>
<p>To assess the performance of the AI tools, independent AI Evaluator models were used, as described in Table 2B.To ensure a neutral and reproducible evaluation process, we selected three independent, high-performing AI models to act as evaluators: GPT-4o (OpenAI), Claude 3.5 Sonnet (Anthropic), and o1 Preview (OpenAI).</p>
<p>These models were intentionally chosen from distinct technical ecosystems and were not used at any stage in the development, training, or fine-tuning of the ELISE engine.This separation was critical to avoid any potential overlap or bias arising from shared components.</p>
<p>The selected models are known for their advanced reasoning and summarization capabilities, making them suitable for structured judgment across multiple scientific domains.Their inclusion was further motivated by public availability, multilingual support, and Each model performed evaluations independently, using randomized prompts under both anonymized and non-anonymized conditions, and their scores were subsequently averaged to control for evaluator-specific bias.</p>
<p>Bias identification and model selection</p>
<p>To identify potential biases in AI-generated evaluations, we conducted a control assessment using Article 5 (Jones et al., 2021) on the Comprehension and Analysis criteria.</p>
<p>-AI tools were evaluated under two conditions: with their identities revealed and anonymized.-A comparison of the scoring results under both conditions allowed us to assess any bias in evaluator decisions.</p>
<p>To ensure transparency, AI tool responses were evaluated under both identified and anonymized conditions.For anonymized evaluations, tool names were replaced with neutral codes (Tool A, Tool B, etc.), and the order of responses was randomized.The evaluators were not informed of the anonymization.This protocol was repeated for all three evaluator models, and scoring results were then compared across both conditions to assess potential bias.Supplementary Table 1 presents the comparative results.</p>
<p>Evaluation criteria</p>
<p>To objectively assess the performance of AI tools, three primary criteria were selected: Extraction, Comprehension, and Analysis.Each criterion was evaluated through a structured questionnaire described in Table 3.</p>
<p>To limit subjectivity in open-ended tasks such as summarization, each question was paired with an expected response profile.Evaluators followed a standardized 10-point rubric (see Supplementary Table 3), and were prompted to assess semantic fidelity, relevance, and completeness rather than superficial similarity.For comparison-based judgments, evaluators used structured prompts to evaluate content quality and alignment with the source article.</p>
<p>Evaluation protocol and scoring</p>
<p>Before evaluation, the questions listed in Table 3 were submitted to each AI tool for every article and every evaluation criterion.The queries were conducted between October 2024 and January 2025, and all responses were collected.The full evaluation process was carried out in January 2025.</p>
<p>Scoring methodology 2.4.1.1 Extraction criterion</p>
<p>-Each response was manually checked by a human reviewer against expected data.-Each answer was scored out of 1, with partial credit assigned proportionally based on the number of correctly retrieved elements.</p>
<p>For instance, if an article was authored by three individuals and the AI tool retrieved only one name, the response received a score of 0.33/1.Similarly, if a study had four different sponsors but only two were correctly identified, the response was scored 0.5/1.-For each article and each AI tool, a percentage score was calculated by summing the points obtained across all questions (see Figure 1A).-First, three independent human experts answered the questions, without knowing the answers of the AIs, to establish a reference baseline.-Second, two different evaluations were performed using automated evaluation prompts provided in Supplementary Figures 1A,B: i Each AI Evaluator model (presented in Table 2B) assessed AI-generated responses against human-defined reference answers.ii AI Evaluator models independently assessed the responses based solely on document content, without human answers, to evaluate autonomous interpretability of AI-generated content.</p>
<p>-Each response was scored out of 10 by the AI Evaluator models and justified by them.-For each article and each AI tool, a percentage score was calculated by summing the points obtained across all questions (see Figure 1B).</p>
<p>Final performance assessment</p>
<p>-To assess global performance, the average score obtained across all evaluated articles was calculated for each AI tool.-Additionally, the standard deviation of scores was determined for each AI tool to measure variability in performance across different articles.</p>
<p>Compliance and traceability scoring</p>
<p>-Compliance score (0-10): evaluates if AI responses strictly adhered to the document content and followed the required guidelines.-Traceability score (0-10): assesses whether AI tools correctly highlighted the relevant sections used in their answers.</p>
<p>Final ECACT score calculation</p>
<p>A weighed global score (ECACT score) proposition was created to reflect the importance of each evaluation criterion:
( ) ( ) ( ) ( ) ( ) = * + * + * + * + * 1.0 1.5 2 1.5 2.0 comp ECACT E C A C T
Where E = Extraction, C = Comprehension, A = Analysis, C comp = Compliance, and T = Traceability, leading to a final score out of 80 points.The weighting system was designed to reflect the relative importance of each criterion in regulated scientific contexts.Analysis and Traceability were prioritized (×2.0) due to their implications for interpretability and compliance, particularly in biomedical and regulatory workflows.A sensitivity analysis exploring alternative weighting schemes is provided in Supplementary Table 2.</p>
<p>Statistical analysis</p>
<p>To ensure statistical robustness and objectivity in comparing the performance of AI tools, a comprehensive statistical framework was applied.All statistical analyses were conducted by a trained statistician (LF, co-author).</p>
<p>First, we performed a one-way ANOVA for each evaluation criterion (Extraction, Comprehension, Analysis) to determine whether differences in performance scores between AI tools were statistically significant.The assumption of homogeneity of variances was assessed using Levene's test, which evaluates whether the variances across groups are equal.A non-significant result (p &gt; 0.05) confirmed that the assumption was met, allowing the use of standard ANOVA procedures.</p>
<p>Second, a two-way ANOVA was carried out to investigate both the main effects and their interaction effects between AI tool identity and evaluation criteria, thereby assessing whether certain models performed differently depending on the evaluation criterion.</p>
<p>Third, we applied post-hoc Tukey Honestly Significant Difference (HSD) tests to identify which pairs of AI tools showed statistically significant differences.This test was chosen for its robustness to multiple comparisons and its suitability for evaluating grouped means.</p>
<p>Additionally, to support transparency and explore the robustness of the ECACT scoring framework, a sensitivity analysis was conducted.Alternative weighting schemes (e.g., equal weights, compliance-prioritized, comprehension-focused) were applied to assess how different weight configurations impacted the final AI tool rankings.Results showed that while some mid-ranking positions Extraction (A) and comprehension/analysis (B) evaluation protocol.shifted, the top-performing (ELISE) and lowest-performing tools remained consistent across weighting conditions (see Supplementary Table 2 and Supplementary Figure 4).All analyses were performed using Python version 3.11.9 and validated independently (statsmodels v.0.14.4 and scipy v.1.14.1 libraries).A p-value threshold of 0.05 was used for significance across all tests.</p>
<p>The full evaluation dataset, question prompts, and scoring rubrics are available in the accompanying GitHub repository. 1</p>
<p>Results</p>
<p>To provide the scientific community with a clearer understanding of the capabilities and limitations of AI-driven literature analysis applications, we developed a structured evaluation methodology based on three key criteria (Extraction, Comprehension and Analysis).Each of these criteria plays a critical role in assessing the overall effectiveness of AI tools in scientific research.</p>
<p>The evaluation process was conducted by analyzing the responses provided by each AI tool to a predefined set of questions (Table 3) across multiple scientific articles (Table 1).The articles were selected to represent a diverse range of disciplines and study types, ensuring a broad assessment of AI tool performance in different editorial contexts.Additionally, two equivalent articles (one in French and the other in English) were included in the dataset to investigate potential variations in AI performance due to language differences.</p>
<p>Extraction performance</p>
<p>The first evaluation criterion, Extraction, assessed the ability of AI tools to accurately identify and retrieve key bibliographic elements such as author names, publication dates, study types, and other fundamental metadata.These elements are essential for organizing, referencing, and citing scientific work.The questions related to this criterion and their expected responses are detailed in Table 3, with answers consisting exclusively of factual data.For each article and each question, the expected number of correct data points was predefined and AI-generated responses were manually evaluated for completeness and accuracy.Each response was assigned a score of 1 point, with proportional credit awarded when only a subset of the expected data was correctly retrieved (data not shown).Then a percentage score was calculated by summing the points obtained across all questions for each article and a global score is obtained for each AI tool.</p>
<p>The results of the Extraction evaluation are presented in Figure 2, illustrating the performance of each AI tool across different articles as well as the global average performance.ChatGPT (Figure 2A) and ELISE (Figure 2B) demonstrate the highest extraction efficiency, consistently achieving scores above 80%, with a minimal variation across articles.In contrast, Epsilon (Figure 2C), Humata (Figure 2D), and SciSpace/Typeset (Figure 2E) exhibited more variable performance, generally ranging between 60 and 70%, with significant fluctuations 1 https://github.com/Biolevate/SL-EVAL-ECACTdepending on the articles.The global performance average (Figure 2F) demonstrates that ELISE (87.50%) and ChatGPT (86.99%) were the most effective tools in extracting standard metadata, significantly outperforming Humata (52.00%),SciSpace/Typeset (49.10%) and Epsilon (37.41%).Statistical analysis revealed significant differences, with ELISE demonstrating a superior performance compared to Humata (p£0.01),SciSpace/Typeset (p£0.001), and Epsilon (p£0.001).</p>
<p>Selection of evaluation models</p>
<p>To further assess the performance of AI tools beyond simple data extraction, we focused on two complex evaluation criteria: Comprehension and Analysis.These criteria require structured, explanatory and context-aware answers, which introduce significant variability in both quality and quantity.Such complexity makes human evaluation challenging, as scoring responses may be influenced by subjectivity and cognitive biases.To mitigate these risks, we established a set of calibrated guidelines (see Table 3) and opted for an AI-based evaluation approach.</p>
<p>A multi-model, independent evaluation was conducted to systematically assess AI tools performance on the Comprehension and Analysis criteria.To ensure robustness, a single reference article (Article 6 - Jones et al., 2021) was used in this evaluation.The primary objective was to determine whether AI Evaluator models introduced biases when grading responses, particularly by comparing identified and anonymized AI tool responses.</p>
<p>The evaluation was conducted using three AI Evaluator models: Claude 3.5 Sonnet, GPT-4o, and o1 Preview (Table 2B).The responses from AI tools were processed separately by each evaluator, and their scores were then averaged (Figure 3).</p>
<p>Results indicate minimal variation between evaluations of identified and anonymized answers, demonstrating that the AI Evaluator models were not influenced by the identity of the AI tools being assessed.This consistency supports the reliability and objectivity of the evaluation framework.</p>
<p>The comparison between identified and anonymized scoring (see Supplementary Figure 1) revealed minimal variation for most tools, confirming that AI Evaluator models were not significantly influenced by tool identities.This supports the robustness and neutrality of the evaluation framework.</p>
<p>However, one notable discrepancy was observed in the evaluation of ChatGPT's responses, where the scores provided by Claude 3.5 Sonnet diverged from those of GPT-4o and o1 Preview.To maintain fairness and accuracy in scoring, as well as to ensure a balanced evaluation, the final assessment of AI tools was determined by incorporating the average results from all three AI Evaluator models.This approach minimizes potential biases and ensures that the final evaluation reflects a comprehensive and standardized assessment of AI-driven comprehension and analysis capabilities.</p>
<p>Comprehension</p>
<p>The second evaluation criterion, Comprehension, aimed to assess the AI tools' ability to interpret and structure key arguments, conclusions and methodological aspects of scientific articles.This criterion is critical for determining how well AI models can process complex scientific content and provide accurate and coherent summaries.</p>
<p>To ensure an objective evaluation, a reference answer set was established by a panel of three human experts, defining expected responses for each question in every article.Then, the AI-generated responses were assessed by AI Evaluator models, which compared them to the human-defined references.Each response was scored on a 10-point scale, with justifications provided by the AI Evaluator models.The scores obtained for each AI tool, AI Evaluator model, and article were averaged (data not shown).The overall final score for each AI tool and article were averaged across the different evaluators and the results are presented in Figure 4.</p>
<p>ELISE (Figure 4B) demonstrated the highest performance in Comprehension, with scores consistently exceeding 8.0, highlighting its ability to process and synthesize scientific information effectively.In contrast, Epsilon (Figure 4C), ChatGPT (Figure 4A) and SciSpace/ Typeset (Figure 4E) exhibited moderate performance, with scores ranging between 7.0 and 8.0.Humata (Figure 4D) displayed greater variability with scores fluctuating between 5.0 and 8.0, indicating inconsistencies in its comprehension capabilities.The global evaluation (Figure 4F) demonstrated that ELISE and Epsilon outperformed ChatGPT, Humata, and SciSpace/Typeset with final scores of: 8.23, 7.56, 7.48, 7.46 and 6.66, respectively.Statistical analysis demonstrated significant differences, reinforcing ELISE's superior comprehension capabilities compared to all other models.</p>
<p>Analysis</p>
<p>The final evaluation criterion, Analysis, focused on assessing each AI tool's ability to engage in critical reasoning, summarize key findings, identify study limitations, and generate meaningful insights.ELISE (Figure 5B) achieved the highest Analysis performance, with scores ranging between 7.0 and 9.0, demonstrating strong critical reasoning capabilities.In contrast, ChatGPT (Figure 5A), SciSpace/ Typeset (Figure 5E), Epsilon (Figure 5C) and Humata (Figure 5D), exhibited lower efficiency, with scores fluctuating between 4.5 and 8.0.Greater performance variability was observed in Humata, SciSpace/ Typeset and ChatGPT show greater variability in performance compared to Epsilon and ELISE, suggesting inconsistencies in their ability to generate structured and insightful interpretations.</p>
<p>The global performance evaluation (Figure 5F) demonstrated ELISE's superior analytical capabilities, with a final score of 7.98, significantly surpassing ChatGPT (7.10), SciSpace/Typeset (7.10), Epsilon (6.98) and Humata (6.86).Statistical analyses demonstrated significant differences, reinforcing ELISE's effectiveness in analyzing scientific content compared to other models.</p>
<p>Language change and AI tools efficiency</p>
<p>To evaluate the impact of language on AI tools performances, we assessed their ability to process identical articles written in French and English (Article 6 and Article 7 - Lowry et al., 2023a).The evaluation was conducted using the Extraction, Comprehension, and Analysis criteria, and the results are presented in Figure 6.</p>
<p>Performance scores for the Extraction criterion revealed a greater variability across languages.ChatGPT achieved 8.06 in French and 7.78 in English, ELISE 6.67 and 8.33, Epsilon 4.56 and 4.00, Humata 0.00 and 3.61, and SciSpace/Typeset 1.11 and 1.94, respectively.These results suggest that ChatGPT and Epsilon maintained stable performance across both languages, while Humata and ELISE exhibited better results in English.</p>
<p>For Comprehension and Analysis criteria, performance variability across languages was minimal for all AI tools, with no strong preference for one language over the other, except for Humata, which consistently performed better in English.The comprehension scores were 7.25 and 7.21 for ChatGPT, 8.38 and 8.63 for ELISE, 8.17 and 7.94 for Epsilon, 6.67 and 7.88 for Humata, 7.63 and 7.42 for SciSpace/Typeset.The analysis criterion followed a similar trend with 7.33 and 6.94 for ChatGPT, 8.28 and 7.92 for ELISE, 7.50 and 6.94 for Epsilon, 7.11 and 8.06 for Humata, 7.67 and 7.83 for SciSpace/Typeset.</p>
<p>These findings highlight that language influences Extraction performance more than Comprehension and Analysis.While some AI tools perform equally well across languages, others exhibit discrepancies, particularly in data retrieval tasks, emphasizing the need for further linguistic adaptation in AI-driven scientific analysis.</p>
<p>Human expertise and AI tools</p>
<p>To further examine AI tools' capabilities, we assessed their performance in Comprehension and Analysis criteria without providing human-validated reference answers.The goal was to evaluate how AI tools perform autonomously when interpreting scientific texts, and the results are presented in Figure 7.</p>
<p>When comparing AI tools' global evaluation scores with and without human expertise as a reference, ELISE consistently demonstrated the highest alignment with expert-level responses.For Comprehension, ELISE's score remained stable (8.32-8.42),whereas other tools demonstrated greater variations: Epsilon (7.56-8.00),), SciSpace/Typeset (7.43-7.76)and Humata (6.66 and 6.82).In the Analysis criterion, ELISE also maintained minimal variation (7.98-8.13),outperforming ChatGPT (7.10-7.81),SciSpace/Typeset (7.10-7.52),Epsilon (6.98-7.83)and Humata (6.86-6.87).</p>
<p>Notably, ChatGPT, Epsilon, and SciSpace/Typeset exhibited the largest score increases when human expertise was not used as a reference, with variations ranging from 0.3 to 0.9 points.These results suggest that AI evaluators models might overestimate AI-generate answers with a more lenient AI tools-assessments considering also some of the evaluators (GPT-4o and o1-Preview) self-assess by scoring ChatGPT answers.In contrast, ELISE consistently produced reliable responses, with minimal variation between the two evaluation settings, reinforcing its alignment with expert-level reasoning.Humata followed a similar trend, but with significantly lower scores, indicating less overall accuracy and robustness compared to ELISE.</p>
<p>To further validate these observations, specific cases where AI tools exhibited string discrepancies were analyzed.The results are visually represented in Figures 8-10.</p>
<p>In the first case (Figure 8), AI tools were required to extract Event-Free Survival (EFS) percentages at 60 months from a Kaplan-Meier curve.Epsilon, Humata and SciSpace/Typeset failed to generate relevant responses, while ChatGPT provided incorrect values (21 and 30% instead of 37 and 44%).In contrast, ELISE generated the closest approximation (40 and 50%) and explicitly indicated a margin of error, demonstrating a more expert-like approach to data interpretation.</p>
<p>In another example (Figure 9), AI tools had to identify the numbers of excluded articles based on specific selection criteria.</p>
<p>While SciSpace/Typeset was unable to provide an exact number, ChatGPT, Epsilon and Humata misinterpret the exclusion criteria, leading to incorrect responses.Only ELISE successfully differentiated between exclusion categories and provided the correct answer (12 articles excluded due to missing full text, demonstrating its superior ability to recognize complex selection criteria and accurately extract relevant numerical data.</p>
<p>The third case (Figure 10) required AI tools to identify a Hazard Ratio (HR) for a specific population within a data table, necessitating both vertical and horizontal reading to locate the expected value.ELISE was the only AI tool capable of retrieving the correct HR (0.31, 95% CI: 0.13-0.74).Moreover, it provided a response even more precise than the expected answer, showcasing its advanced document parsing capabilities and its ability to accurately interpret structured data, a task where all other AI tools failed.</p>
<p>These findings (Figures 7-10 and Supplementary Tables 1-3) reinforce that ELISE is the AI tool that aligns most closely with human expertise across all tested evaluation criteria.Unlike other models, which exhibited greater variability and inconsistencies, ELISE consistently provided responses that matched expert expectations, particularly in challenging tasks involving graph interpretation, inlay detection, and the comprehension of complex scientific data.</p>
<p>Overall results</p>
<p>To provide a comprehensive overview of the AI tools' evaluation, a detailed comparison of scores across criteria (Figure 11A) and an overall averaged comparison (Figure 11B) were conducted.</p>
<p>The results confirm that ELISE consistently achieved high scores across all criteria, demonstrating minimal variation between Extraction, Comprehension and Analysis.</p>
<p>Among the evaluated tools, ChatGPT performed best in Extraction but exhibited lower efficiency in Comprehension and Analysis, indicating its strength in retrieving structured metadata but its relative weakness in processing and interpreting scientific content.In contrast, SciSpace/Typeset, Humata and Epsilon showed poor performance in Extraction but performed moderately better in Comprehension and Analysis, although their results remained less relevant and less consistent than ELISE's.</p>
<p>The global average comparison further reinforces these observations.ELISE emerges as the most effective AI tool, followed by ChatGPT, then SciSpace/Typeset, Humata and Epsilon, which obtained similar but lower overall scores.These findings underscore the importance of an AI tool's ability to handle the entire research workflow, from accurate data extraction to in-depth comprehension  and critical analysis, ensuring its reliability for scientific literature processing across various fields and study types.</p>
<p>Statistical analysis</p>
<p>To validate the observed performance differences among AI tools, a comprehensive statistical analysis was conducted.Results for each criterion were included in the global average evaluation (Figures 2F,  4F, 5F).</p>
<p>A unidirectional ANOVA test (data not shown) confirmed that AI tool performance varied significantly depending on the evaluation criterion, with some tools excelling in certain task underperforming in others.Additionally, two-way ANOVA tests (data not shown) demonstrated that AI tools differed significantly from each other across all criteria, confirming that no single evaluation metric can fully determine an AI tool's effectiveness in scientific literature analysis.</p>
<p>Interestingly, results indicated no significant interaction effect between AI tools and evaluation criteria, meaning that performance rankings remained consistent regardless of the assessment category.This supports the robustness of the conclusions drawn in this study and validates the methodological soundness of the evaluation framework.</p>
<p>ECACT score</p>
<p>To ensure a rigorous and holistic evaluation framework, an ECACT score was developed, incorporating the Extraction, Comprehension and Analysis criteria alongside two additional dimensions: Compliance and Traceability.These complementary criteria are essential to assessing an AI tool's reliability and adherence to scientific best practices.</p>
<p>The Compliance criterion evaluates whether an AI tool follows predefined guidelines and exclusively relies on documented content to generate responses.The traceability criterion assesses the tool's ability to highlight the relevant data sources that were used to generate its answers.These criteria provide a more nuanced understanding of each AI model's transparency and scientific rigor.</p>
<p>To reflect the importance of each criterion, a weighting system was applied.The Analysis criterion received the highest weight, followed by Comprehension, then Extraction.Similarly, Traceability was weighted equivalently to Analysis, while Compliance was weighted at the same level as Comprehension, ensuring a balanced assessment of AI tools' capabilities.</p>
<p>The results, presented in Figure 12, show that ELISE (Figure 12B) demonstrated the highest performance across all evaluated criteria,    extraction, structuring complex information, and streamlining regulatory and scientific documentation.These tools leverage Named Entity Recognition (NER) to efficiently identify key data points, reducing manual workload and accelerating critical decision-making processes.However, despite these advantages, significant challenges remain, particularly in ensuring the reliability, accuracy, and contextual relevance of AI-generated outputs.Many traditional AI struggle to interpret complex technical content, leading to misinterpretations, inconsistencies, and errors in data extraction.These limitations highlight the need for AI solutions capable of handling industry-specific requirements, where precision, compliance, and traceability are essential for regulatory submissions, clinical trials, and scientific validation.</p>
<p>To address these limitations, the integration of Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) offers a promising solution for improving accuracy, reliability, and compliance in AI-driven document analysis.By combining advanced retrieval mechanisms with context-aware generation, this approach reduces dependence on pre-trained datasets, which may contain biases or outdated information, and instead ensures fact-based, real-time content generation (Mostafapour et al., 2024;Doyal et al., 2023).One of the most persistent challenges in regulated industries like pharma and Medtech remains the parsing of unstructured documents, including clinical trial reports, regulatory filings, and research publications.In this study, ELISE's superior performance, closely aligned with human expertise, can be attributed to its advanced parsing capabilities.Unlike other AI tools, ELISE demonstrated a higher efficiency in processing text, formula and table, surpassing industry alternatives such as Megaparse, Llama or Unstructured.Preliminary study demonstrated a Normalized Edit Distance (NED) -how different two elements are by counting the minimum changes (insertions, deletions, substitutions) needed to transform one into the other, normalized by the longest element's length with 0 as identical and 1 completely different-respectively of 0.558241, 0.622417, 0.574559, and 0.490463 for Megaparse, Llama, Unstructured and Matsu (ELISE parser developed by Biolevate) on the global performance metric for textual content.When combined with context-aware modeling, this feature enhances query interpretation, ensure more precise responses and improve data traceability, making ELISE particularly adapted to regulatory and clinical applications.</p>
<p>The reliability of AI-generated outputs remains a key concern for industries where compliance with regulatory framework is non-negotiable.One striking example from this study (detailed in Supplementary Figure 2) highlights a critical issue: ChatGPT provided a DOI for an article when no other AI tools succeeded, but further investigation revealed that this DOI was not present in the original  To ensure a rigorous and unbiased evaluation of AI tools, a reference human answer was established to benchmark AI-generated responses.To further mitigate evaluation biases, AI Evaluator models were incorporated into the scoring methodology.By leveraging LLMs trained on diverse datasets, these evaluators ensure that responses are analyzed based on factual accuracy rather than subjective human biases.This multi-model validation approach minimizes overestimated AI-generated responses and ensures a more reliable assessment of AI performance, particularly in regulatory and clinical settings.</p>
<p>AI tools such as ChatGPT demonstrate remarkable capabilities in processing large volumes of data, offering significant advantages in speed and accessibility.However, their lack of industry-specific knowledge and contextual awareness makes human oversight essential in critical applications.This study addressed this gap by using expert-defined reference answers, ensuring AI-generated content meets the highest standards of accuracy and relevance for pharmaceutical, biotechnological, and medical applications.Moreover, the ethical and regulatory implications of AI-driven research and documentation must be considered, particularly concerning biases, transparency, and compliance with industry standard.While AI can augment scientific workflow, it cannot replace human expertise.Instead, a hybrid model, where AI supports human decision-making while ensuring data integrity and compliance, offers the most reliable and scalable approach for integrating AI into regulated industries (Mehta et al., 2024;Mostafapour et al., 2024;Ahaley et al., 2024;Singh et al., 2024).</p>
<p>One of the key differentiators among AI tools is compliance and traceability.AI solutions like ELISE integrate built-in compliance mechanisms, allowing for systematic verification of extracted data and response relevance.Unlike general-purpose AI models, which lack transparent methodologies, ELISE explicitly highlights source data and provides traceability on how each response was generated.This feature is critical for regulatory bodies and compliance teams in the pharma, biotech, and Medtech industries, where decision-making must be based on verifiable evidence rather than opaque AI-generated summaries.Furthermore, AI tools capable of explaining their reasoning processes, such as ELISE, allow human experts to refine AI queries, optimize search strategies, and improve model training over time, making them more aligned with human expertise (as demonstrated in Supplementary Figure 3).</p>
<p>Despite advancements in Deep Neural Networks (DNNs), Natural Language Processing (NLP), and Transformers architectures, AI models still struggle with high-level analytical reasoning, contextual variation, and long-form document coherence.These challenges impact critical decision-making in pharmaceutical and medical research, where AI-generated insights must be reliable, interpretable, and reproducible.To address these gaps, state-of-the-art techniques such as Pointer-Generator Networks and Sparse Attention Transformers are being implemented to enhance scientific summarization, improve structured data interpretation, and extract meaningful insights from large-scale regulatory of clinical documents (Tsirmpas et al., 2024).</p>
<p>Given the specialized needs of pharma, biotech, and Medtech, AI tools should be designed with modular adaptability, allowing organizations to select and integrate the most suitable models for their specific applications.As AI becomes more deeply embedded in regulatory, clinical, and research workflows, standardized industry guidelines must be established to ensure transparency, compliance, and ethical AI deployment.Human oversight will continue to play a critical role in refining AI-generated insights, ensuring scientific validity, and maintaining alignment with industry regulations, reinforcing the value of a hybrid AI-human approach in optimizing research and clinical decision-making (Bran et al., 2024;Meyer-Szary et al., 2024).AI tools that provide explainability, such as ELISE, play a crucial role in enhancing human-AI collaboration.By offering transparency on how responses are generated, these tools enable users to understand the AI's reasoning process, refine their queries for more precise outputs, and iteratively train the model to align more closely with expert-level expectations (as detailed in the Supplementary Figure 3).This capability is particularly valuable in regulated environments such as pharmaceuticals, biotechnology and Medtech, where interpretability, compliance, and continuous model improvement are essential for integrating AI into decisionmaking workflows.</p>
<p>ECACT score and standardized AI evaluation in regulated industries</p>
<p>The integration of AI tools into pharmaceutical, clinical, and healthcare workflow presents both significant opportunities and operational challenges.While AI optimizes processes such as medical documentation, literature review and regulatory reporting, the variability in AI-generated content quality necessitates a standardized evaluation framework.</p>
<p>Existing regulatory framework, such as SPIRIT-AI and CONSORT-AI, provide essential guidance for AI-driven clinical trials, ensuring transparency, reproducibility, and accountability (McGenity and Treanor, 2021).However, they do not provide standardized methodologies for assessing AI-generated research and regulatory outputs.Similarly, the PRISMA 2020 Checklist, widely used for systematic reviews, lacks specific AI assessment criteria (Page et al., 2021).</p>
<p>To address this gap, this study introduces a dedicated AI evaluation framework based on three core criteria: Extraction, Comprehension, and Analysis, each associated with a structured set of questions.This approach allows progressive assessment from basic data retrieval to advanced contextual analysis, ensuring AI tools are evaluating on their full operational capacity.</p>
<p>Additionally, data transparency in AI-generated response is a critical factor in regulatory compliance.As highlighted in the Danler study, AI models must be assessed not only on their accuracy but also on their ability to justify and trace their responses to verifiable sources.The variability in response quality further reinforces the importance of integrating Compliance and Traceability into AI evaluations (Danler et al., 2024).By incorporating these dimensions into the ECACT score, this study demonstrates that AI tools like ELISE, which follow compliance protocols and ensure traceability, significantly outperform models that do not.For instance, Epsilon, which frequently violates guidelines and generates overly verbose responses, was found to be less reliable despite strong comprehension scores.Similarly, ChatGPT's inability to provide traceable references led to its reassignment from second to last position when these additional criteria were included.</p>
<p>While ChatGPT demonstrates lower performance in traceability and analysis under the ECACT framework, it consistently ranks high in fluency, syntactic clarity, and readability.These traits make it a valuable option for use cases outside regulatory or high-stakes contexts, such as educational summaries, internal research notes, or exploratory drafts.ECACT should thus be interpreted as a scenario-sensitive evaluation tool, guiding tool selection according to task constraints.</p>
<p>Moving forward, the ECACT score should evolve to incorporate additional rating scales to further refine Compliance and Traceability assessments.Moreover, ethical considerations-including data privacy, processing speed, and AI model resource consumptionmust be integrated into AI evaluation frameworks.By establishing a standardized, transparent evaluation methodology, as proposed in this study, AI-driven research and regulatory applications can be optimized while ensuring data integrity and ethical compliance (Danler et al., 2024;Wattanapisit et al., 2023;Tangsrivimol et al., 2025).</p>
<p>The current study is limited to life science and biomedical articles.The sample size (n = 9) was deliberately kept small to allow for detailed, multi-criteria assessment of each tool's performance.However, this limited scale restricts the broader generalizability of the results.Future work will focus on validating the ECACT framework on independent datasets, non-English corpora beyond French-English, and diverse scientific domains such as engineering and social sciences, using larger and more heterogeneous article corpora.</p>
<p>Future versions of ECACT may integrate quantitative metrics of semantic similarity (e.g., cosine distance, ROUGE, BERTScore) to complement evaluator-based assessments and further reduce subjectivity in open-ended tasks.</p>
<p>Conclusion</p>
<p>The study assessed the performance of AI tools in scientific literature analysis, focusing on Extraction, Comprehension, and Analysis criteria while also introducing Compliance and Traceability as critical evaluation dimensions.ELISE emerged as the most effective tool, demonstrating superior performance across all criteria, particularly in data extraction and analytical reasoning, aligning closely with human expertise.ChatGPT exhibited strong efficiency in data retrieval but struggled with deeper comprehension and analysis, limiting its applicability for highly regulated environments.Epsilon, Humata, and SciSpace/Typeset performed moderately, with notable strengths in comprehension but significant weaknesses in structured data extraction, impacting their reliability for complex scientific and regulatory applications.</p>
<p>A key takeaway from this study, is that human oversight remains indispensable in validating AI-generated content, ensuring accuracy, compliance, and contextual relevance, particularly in pharmaceutical, biotechnological, and Medtech applications where data integrity and regulatory adherence are paramount.While AI tools significantly enhance efficiency in literature analysis and knowledge extraction, they must function as augmentative tools rather than standalone solutions.</p>
<p>To address the variability in AI-generated responses and provide a structured evaluation framework, this study introduced the ECACT score, incorporating Extraction, Comprehension, Analysis, Compliance, and Traceability, as key performance indicators.This scoring system ensures that AI tools are assessed not only for their ability to process scientific content but also for their transparency, adherence to guidelines, and ability to justify their outputs.Moving forward, establishing standardized evaluation frameworks such as ECACT will be crucial for integrating AI-driven solutions into research, clinical, and regulatory environments, ensuring that these tools meet the highest standards of scientific rigor, reliability, and ethical compliance.</p>
<p>Publisher's note</p>
<p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers.Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
<p>data's in the document: present in one sentence the global methodology of the study and if it described, present in one sentence different methodologies used in the study) Main results (in one sentence and for each part of the study results section, present the conclusion) Secondary results (in one sentence and for each sub-part of the study results section, if any, present the conclusion) Conclusion of the study (in two sentences) Summary of the abstract (in three sentences) Summary of the study (one sentence for each main section of the study: introduction, method, results, discussions, conclusion) Analysis Interpretation of the mains results (in one sentence for each main result, discuss/enhance results) Limitations of the study (in one sentence for each limitation) Three specific questions (only from data of the document, in one sentence) Prospects for the future (only from data of the document, in one sentence)</p>
<p>FIGURE 2
2
FIGURE 2Extraction performance of AI tools -ELISE and ChatGPT are the most effective tools for extracting standard metadata compared to common AI specialized tools.Results of AI tools evaluations for the Extraction criterion, presented by article: ChatGPT (A), ELISE (B), Epsilon (C), Humata (D), SciSpace/typeset (E) and global average across all AI tools (F).Statistically significant differences with ELISE are marked with asterisks: ns = not significant (p ≥ 0.05), ** = very significant (p ≤ 0.01) and *** = highly significant (p ≤ 0.001).</p>
<p>FIGURE 3 A
3
FIGURE 3A multi-model evaluation ensuring unbiased AI scoring -Global average of AI tools' evaluation by AI Evaluator models (Claude 3.5 Sonnet, GPT-4o, o1 Preview and the average global score (Global) for the Comprehension and Analysis criteria applied to Article 5(Jones et al., 2021).Comparison between identified and anonymized AI tool response.</p>
<p>FIGURE 4 ELISE
4
FIGURE 4 ELISE and Epsilon outperform ChatGPT, Humata, Scispace/Typeset for scientific comprehension.Overall results of AI tools evaluation for the Comprehension criteria: ChatGPT (A), ELISE (B), Epsilon (C), Humata (D), SciSpace/Typeset (E), and Global average of AI tools evaluation (F).Statistically significant differences with ELISE are marked with asterisks: * = significant (p ≤ 0.05), ** = very significant (p ≤ 0.01) and *** = highly significant (p ≤ 0.001).</p>
<p>FIGURE 5 ELISE
5
FIGURE 5 ELISE demonstrates superior analytical capabilities compared to ChatGPT and specialized AI tools.Overall results of AI tools evaluations for the Analysis criterion: ChatGPT (A), ELISE (B), Epsilon (C), Humata (D), SciSpace/Typeset (E) and global average of AI tool evaluations (Global).Statistically significant differences with ELISE are marked with asterisks: ** = very significant (p ≤ 0.01) and *** = highly significant (p ≤ 0.001).</p>
<p>FIGURE 6
6
FIGURE 6Language change can lead to variations in AI tools responses.Overall results of AI tools (ChatGPT, ELISE, Epsilon, Humata and SciSpace/Typeset) for the extraction, comprehension and analysis criteria comparing performance across French and English articles.</p>
<p>FIGURE 7 ELISE
7
FIGURE 7 ELISE's responses align more closely with human expertise than other AI tools.Comparison of AI tool performance in the comprehension (A) and analysis (B) criteria, evaluated with and without human reference answers.</p>
<p>FIGURE 8
8
FIGURE 8Accuracy in extracting event-free survival (EFS) at 60 months from Article 8(Marks et al., 2022).Comparison of AI tool responses, with expected values shown in green, ELISE responses in blue, ChatGPT responses in red, and Other AI tools responses in gray (lines and dots) on Article 8 screenshot with the associated outputs of each AI tools.</p>
<p>FIGURE 9
9
FIGURE 9 Accuracy in identifying articles excluded for missing full text (Article 5 -Jones et al., 2021).Comparison of AI tool responses to the following question: How many articles were excluded for not having full text?With expected values shown in green, ELISE responses in blue and incorrect Other AI tools responses in red on Article 5 screenshot with the associated outputs of each AI tools.</p>
<p>FIGURE 10
10
FIGURE 10Accuracy in identifying hazard ratios (HR) for progression-free survival (Article 2 -Herrera et al., 2024).Comparison of AI tools responses for the question "What is the HR of the Progression-Free Survival in Modified Intent-to-treat Analysis Set for the teenagers?"with expected values shown in green, ELISE responses in blue and Other AI tools responses in gray on Article 2 screenshot with the associated outputs of each AI tools.</p>
<p>FIGURE 12 ECACT
12
FIGURE 12ECACT score evaluation and AI tool comparison.(A-E) Radar charts illustrating AI tools' performance across all evaluation criteria.(F) Global ECACT scores, confirming ELISE's superior reliability and effectiveness across all dimensions.</p>
<p>the literature review, the drafting and structuring of the scientific content.</p>
<p>TABLE 1
1
Overview of selected articles, including title, type, discipline, and citation.
ArticleTitleno.</p>
<p>TABLE 2
2
Overview of selected AI tools (A) and AI evaluator models (B).
AAI toolsProviderRelease dateCitations capacityHighlighting capacityChatGPT (GPT-4o)OpenAINovember 2024NoNoELISE 2.0BiolevateDecember 2024YesYesEpsilon 2.5EpsilonApril 2024YesYesHumataTilda technologiesNovember 2024YesYesSciSpace/Typeset 1.4.12SciSpace (ex-Typeset)November 2024YesYesBAI evaluator modelsCompanyLaunching dateReasoning modelClaude 3.5 SonnetAnthropicJune 2024NOGPT-4oOpenAIMay 2024NOO1 PreviewOpenAIDecember 2024YES
Frontiers in Artificial Intelligence 05 frontiersin.orgFIGURE 1</p>
<p>TABLE 3
3
Criteria and evaluation questions.</p>
<p>ChatGPT had sourced it externally, violating strict data integrity guidelines.Such discrepancies underscore the importance of traceability and compliance features in AI tools, particularly in clinical research, regulatory submissions, and drug development workflows, where data provenance must be verifiable and reproducible.
10.3389/frai.2025.1587244document,Frontiers in Artificial Intelligence17frontiersin.org
Data availability statementThe original contributions presented in the study are publicly available.This data can be found here: https://github.com/Biolevate/SL-EVAL-ECACT.FundingThe author(s) declare that no financial support was received for the research and/or publication of this article.A significant shift in AI tools rankings was observed when Compliance and Traceability were factored into the evaluation, further reinforcing the need for transparency and guideline adherence in AI-driven research tools.Notably, ChatGPT's performance declined significantly due to its lack of traceability, while Epsilon's overall score decreased due to its failure to comply with evaluation guidelines.Conversely, ELISE remained consistently at the top (Figure12F), demonstrating that it is not only the most performant AI tool for scientific literature analysis but also the most reliable in terms of transparency and methodological rigor.Sensitivity analysis of the ECACT scoreTo assess the robustness of the ECACT framework, a sensitivity analysis was performed by modifying the relative weights of each criterion (e.g., equal weights; Compliance and Traceability prioritized).While minor fluctuations were observed in the ranking of mid-performing tools, the top and bottom positions remained consistent.ELISE systematically outperformed other tools across all tested schemes (see Supplementary Table2and Supplementary Figure4), confirming the resilience of the ECACT methodology.4 Discussion and future directions AI tools are increasingly transforming workflows in pharmaceuticals, biotechnology, and Medtech by automating dataConflict of interestMaG, MuG, ST, LF, JB, AV, and FA were employed by the Biolevate, the developer of the AI tool ELISE analyzed in this study.Generative AI statementThe authors declare that Gen AI was used in the creation of this manuscript.This article was partially prepared with the assistance of ELISE, an artificial intelligence tool developed by Biolevate, to supportSupplementary materialThe Supplementary material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/frai.2025.1587244/full#supplementary-material
Augmenting research: The role of artificial intelligence in recognizing topics and ideas. V Agrawal, S Bhardwaj, N Pathak, J Dixit, S Agarwal, M Momin, S S Ahaley, A Pandey, S K Juneja, T S Gupta, S Vijayakumar, 10.4018/979-8-3693-1798-3.ch003Utilizing AI Tools Acad. Res. Writing. 20242024. 2024</p>
<p>ChatGPT in medical writing: a game-changer or a gimmick?. 10.4103/picr.picr_167_23Perspect. Clin. Res. 15</p>
<p>Functions of double-stranded RNA-binding domains in nucleocytoplasmic transport. S Banerjee, P Barraud, 10.4161/15476286.2014.972856RNA Biol. 112014</p>
<p>Artificial intelligence for literature reviews: opportunities and challenges. F Bolaños, A Salatino, F Osborne, E Motta, 10.1007/s10462-024-10902-3Artif. Intell. Rev. 572592024</p>
<p>Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the PROSPERO registry. R Borah, A W Brown, P L Capers, K A Kaiser, 10.1136/bmjopen-2016-012545BMJ Open. 7e0125452017</p>
<p>. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, 2024</p>
<p>Augmenting large language models with chemistry tools. 10.1038/s42256-024-00832-8Nat. Mach Intell. 6</p>
<p>The changing scenario of drug discovery using AI to deep learning: recent advancement, success stories, collaborations, and challenges. C Chakraborty, M Bhattacharya, S S Lee, Z H Wen, Y H Lo, 10.1016/j.omtn.2024.102295Mol. Ther. Nucleic Acids. 351022952024</p>
<p>Quality and effectiveness of AI tools for students and researchers for scientific literature review and analysis. Stud. M Danler, W O Hackl, S B Neururer, B Pfeifer, A S Doyal, D Sender, M Nanda, R A Serrano, 10.7759/cureus.43292doi: 10.7759/cureusHealth Technol. Inform. 313e432922024. 2023Cureus.</p>
<p>. Elise , 2024. accessed February 17, 2025. 2024. accessed February 17, 2025</p>
<p>How to optimize the systematic review process using AI tools. N Fabiano, A Gupta, N Bhambra, B Luu, S Wong, M Maaz, 10.1002/jcv2.12234JCPP Adv. 4e122342024</p>
<p>. A Gatin, P Duchambon, G V D Rest, I Billault, C Sicard-Roselli, 2022</p>
<p>Protein dimerization via Tyr residues: highlight of a slow process with co-existence of numerous intermediates and final products. 10.3390/ijms23031174Int. J. Mol. Sci. 231174</p>
<p>Nivolumab+AVD in advanced-stage classic Hodgkin's lymphoma. A F Herrera, M Leblanc, S M Castellino, H Li, S C Rutherford, A M Evens, 10.1056/NEJMoa2405888N. Engl. J. Med. 3912024</p>
<p>Extracellular HMGB1 blockade inhibits tumor growth through profoundly remodeling immune microenvironment and enhances checkpoint inhibitor-based immunotherapy. P Hubert, P Roncarati, S Demoulin, C Pilard, M Ancion, C Reynders, 10.1136/jitc-2020-001966J. Immunother. Cancer. 9e0019662021. 2024. February 17, 2025</p>
<p>Impact of COVID-19 on mental health in adolescents: a systematic review. E A K Jones, A K Mitra, A R Bhuiyan, 10.3390/ijerph18052470Int. J. Environ. Res. Public Health. 1824702021</p>
<p>Artificial intelligence (AI) and ChatGPT involvement in scientific and medical writing, a new concern for researchers. A scoping review. A A Khalifa, M A Ibrahim, 10.1108/AGJSR-09-2023-0423Arab Gulf J. Sci. Res. 422024</p>
<p>Le système de surveillance des anomalies congénitales de l' Alberta: compte rendu des données sur 40 ans avec prévalence et tendances de certaines anomalies congénitales entre. R B Lowry, T Bedard, X Grevers, S Crawford, S C Greenway, M E Brindle, 10.24095/hpcdp.43.1.04fPromot. Santé Prév. Mal. Chron. Au. Can. 432023a. 1997 et 2019</p>
<p>The Alberta congenital anomalies surveillance system: a 40-year review with prevalence and trends for selected congenital anomalies. R B Lowry, T Bedard, X Grevers, S Crawford, S C Greenway, M E Brindle, 10.24095/hpcdp.43.1.04Health Promot. Chronic Dis. Prev. Can. 432023b. 1997-2019</p>
<p>Addition of four doses of rituximab to standard induction chemotherapy in adult patients with precursor B-cell acute lymphoblastic leukaemia (UKALL14): a phase 3, multicentre, randomised controlled trial. D I Marks, A A Kirkwood, C J Rowntree, M Aguiar, K E Bailey, B Beaton, 10.1016/S2352-3026(22)00038-2Lancet Haematol. 92022</p>
<p>Guidelines for clinical trials using artificial intelligence -SPIRIT-AI and CONSORT-AI †. C Mcgenity, D Treanor, 10.1002/path.5565J. Pathol. 2532021</p>
<p>AI-dependency in scientific writing. V Mehta, V Thomas, A Mathur, 10.1016/j.oor.2024.100269Oral Oncol. Rep. 101002692024</p>
<p>Scientific writing at the dawn of AI. J Meyer-Szary, M Jaguszewski, S Mikulski, 10.5603/cj.94335Cardiol. J. 312024</p>
<p>. M Mostafapour, J H Fortier, K Pacheco, H Murray, G Garber, 2024</p>
<p>Evaluating literature reviews conducted by humans versus ChatGPT: comparative study. 10.2196/56537Jmir Ai. 3e56537</p>
<p>. Y Ning, S Teixayavong, Y Shang, J Savulescu, V Nagaraj, D Miao, 2024</p>
<p>Generative artificial intelligence and ethical considerations in health care: a scoping review and ethics checklist. 10.1016/S2589-7500(24)00143-2Lancet Digit. Health. 6</p>
<p>The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. M J Page, J E Mckenzie, P M Bossuyt, I Boutron, T C Hoffmann, C D Mulrow, 10.1371/journal.pmed.1003583PLoS Med. 18e10035832021</p>
<p>. Scispace, 2024. February 17, 2025</p>
<p>Artificial intelligence at the pen's edge: exploring the ethical quagmires in using artificial intelligence models like ChatGPT for assisted writing in biomedical research. H Sharma, M Ruikar, S Singh, R Kumar, V Maharshi, P K Singh, V Kumari, M Tiwari, 10.4103/picr.picr_196_23Perspect. Clin. Res. 152024. 2024</p>
<p>Harnessing artificial intelligence for advancing medical manuscript composition: applications and ethical considerations. 10.7759/cureus.71744Cureus. 16e71744</p>
<p>Benefits, limits, and risks of ChatGPT in medicine. J A Tangsrivimol, E Darzidehkalani, H U H Virk, Z Wang, J Egger, M Wang, 10.3389/frai.2025.1518049Front. Artif. Intell. 815180492025</p>
<p>Neural natural language processing for long texts: a survey on classification and summarization. D Tsirmpas, I Gkionis, G T Papadopoulos, I Mademlis, 10.1016/j.engappai.2024.108231Eng. Appl. Artif. Intell. 1331082312024</p>
<p>Research progresses and applications of knowledge graph embedding technique in chemistry. C Wang, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysY Yang, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysJ Song, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysNan , Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysX Wattanapisit, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysA Photia, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysA Wattanapisit, Malays Fam. Phys. Off. J. Acad. Fam. Phys. MalaysS , Malays Fam. Phys. Off. J. Acad. Fam. Phys. Malays10.51866/lte.483doi: 10.51866/lte.483J. Chem. Inf. Model. 64692024. 2023Should ChatGPT be considered a medical writer?</p>
<p>The synergy of artificial intelligence and personalized medicine for the enhanced diagnosis, treatment, and prevention of disease. M A Zahra, A Al-Taher, M Alquhaidan, T Hussain, I Ismail, I Raya, 10.1515/dmpt-2024-0003Drug Metab. Pers. Ther. 392024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8517 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8517</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8517</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-49552345</p>
                <p><strong>Paper Title:</strong> <a href="http://export.arxiv.org/pdf/1806.11532" target="_blank">TextWorld: A Learning Environment for Text-based Games</a></p>
                <p><strong>Paper Abstract:</strong> The limits of my language mean the limits of my world</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8517.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8517.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-DQN (LSTM Deep Q-Network)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DQN-style agent that uses an LSTM encoder over textual observations to produce a state representation for Q-value prediction; used in prior text-game work to capture temporal context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language understanding for text-based games using deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A reinforcement-learning agent that encodes observations with an LSTM and uses a DQN head to estimate Q-values for actions; intended to capture sequential information in text-game observations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Homeworld and Fantasyworld (Evennia-based environments)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Two deterministic/stochastic text-game environments used in prior work: Homeworld is small (4 rooms); Fantasyworld is larger and more stochastic. They test language understanding, action selection, and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent (LSTM) working/short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>An LSTM encodes the observation text o = w1...wn into a vector s = LSTM(o), which is then used by the Q-network to score actions; the LSTM hidden state provides temporal context across observation tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>LSTM-encoded observation vector is passed to the Q-network as the state representation; the model does not incorporate a separate retrieval module or explicit episodic store.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Completes 100% of quests in Homeworld and 96% of quests in Fantasyworld (reported completion rates).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No ablation in this TextWorld paper comparing LSTM vs. no-LSTM for that model; the paper mentions transfer experiments in prior work where the LSTM component was transferred to a shuffled environment (Homeworld2) and sped up learning.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The TextWorld paper explicitly notes that prior models (including LSTM-DQN as discussed) are not conditioned on previous actions/observations in the way required for full POMDP handling and thus have limited capacity for partial observability in some setups.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Recurrent encoders (LSTMs) provide useful sequential encoding and can be transferred between related environments, but TextWorld emphasizes that more explicit memory/conditioning on history is needed to fully address partial observability in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextWorld: A Learning Environment for Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8517.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8517.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network (DRRN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that separately embeds observations and textual action candidates and computes Q-values for observation–action pairs, designed for choice-based text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with a natural language action space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DRRN computes Q(s, a_j) by embedding the observation and each candidate action separately and combining them (relevance scoring) to produce Q-values over textual actions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Saving John and Machine of Death (choice-based text games)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Choice-based narrative games with different vocabulary/action sizes; used to evaluate models that score text actions given an observation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>DRRN converged on both games; achieved optimal cumulative reward on Saving John and a suboptimal but stable policy on Machine of Death.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>TextWorld notes that DRRN (like some other prior models) does not condition on previous actions/observations, and therefore lacks the capacity to handle partial observability; no memory ablation experiments are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lack of conditioning on history prevents handling POMDP aspects in parser-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Models that score single-step state-action pairs (like DRRN) are limited in partially observable text games; adding history conditioning or memory is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextWorld: A Learning Environment for Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8517.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8517.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Golovin agent (Kostka et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular agent for classic text-based games that leverages a pre-trained LSTM language model for keyword extraction and a large command corpus to generate candidate actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text-based adventures of the golovin ai agent</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A multi-module system that pre-trains an LSTM language model on fantasy books to extract keywords from scene descriptions and uses a command corpus and heuristics to generate actionable commands in parser-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Pre-trained LSTM language model (size unspecified in TextWorld); used for keyword extraction rather than as a large pretrained LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Curated list of hand-authored games and Treasure Hunter benchmark (evaluated in TextWorld)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Hand-authored Infocom-style games and generated Treasure Hunter tasks; used to evaluate baseline agents including Golovin.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>LSTM-based sequential language model (short-term / contextual encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>A pre-trained LSTM language model is used to extract keywords from current scene descriptions; the agent also maintains other modular state (e.g., inventories/rules) but TextWorld does not detail an explicit episodic memory store for Golovin.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>LSTM-derived keywords inform command generation modules; integration is modular rather than an explicit episodic retrieval mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Performance reported in Table 1 of the paper for the Treasure Hunter one-life tasks (Golovin compared to BYU and random baseline); see Table 1 for average scores per difficulty level.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>TextWorld evaluates Golovin against BYU and a random choice baseline on Treasure Hunter levels but does not provide an ablation isolating the pre-trained LSTM effect.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Baselines including Golovin perform poorly on many hand-authored games and on harder generated tasks; TextWorld does not report detailed failure modes tied specifically to Golovin's memory component.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Pre-training a language model to extract keywords is a practical way to improve command generation in domain-specific text games; modular architectures can leverage such components, but further work is needed for robust generalization and memory over unseen games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextWorld: A Learning Environment for Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8517.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8517.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action Elimination Network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Elimination Network (Haroush et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to reduce the effective action space in parser-based text games by learning to score and eliminate unlikely/failing actions using stored feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning how not to act in text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Action Elimination Network</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A module that estimates the probability an action will fail in the current scene; it is trained using augmented replay data that includes parser/engine feedback so the agent can restrict candidate actions during selection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Parser-based text games (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Text-based games where the action space is the set of word-sequences recognized by the parser; the method aims to reduce the large, sparse action set.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience replay augmented with parser feedback (stored observations/actions/feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Replay buffer augmented with tuples including the game's parser feedback in addition to <observation, action, reward>; these stored examples train an elimination module that assigns scores to actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>During action selection the elimination module scores actions; only top-k scored actions are considered at greedy steps, and low-scoring actions are probabilistically rejected during exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>TextWorld summarizes the approach but does not provide its own ablation; the cited work compares action elimination to standard exploration but TextWorld does not reproduce detailed results.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>If elimination scores are inaccurate, valid actions may be pruned; relies on quality of stored parser feedback and training signal.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Storing parser feedback in the replay buffer and training an action-elimination module is recommended to reduce the effective action space in parser-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextWorld: A Learning Environment for Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8517.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8517.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Map</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Map: Structured memory for deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured external-memory architecture proposed for deep RL that stores spatial or structured representations (referenced in TextWorld as relevant prior work on memory in RL).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural map: Structured memory for deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neural Map (memory architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An external structured memory (map) for RL agents that can store and retrieve spatial/structured information to support long-term reasoning; TextWorld references it as related work for memory mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Referenced as a general RL memory architecture (not evaluated in TextWorld experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external structured memory (map-like / spatial memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Structured map-like memory (detailed architecture not described in TextWorld; cited work should be consulted for specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Not detailed in TextWorld; cited as an example of structured memory approaches applicable to RL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>TextWorld points to structured-memory architectures (e.g., Neural Map) as promising directions for handling tasks requiring long-term memory in text-game RL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextWorld: A Learning Environment for Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8517.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8517.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Treasure Hunter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Treasure Hunter benchmark (TextWorld)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generated TextWorld benchmark where the agent must locate and retrieve a specific object indicated by a nearby 'indicator' object; designed to test affordance extraction, navigation, and memory (remembering the target).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Treasure Hunter task (evaluated with baseline agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generated tasks of varying difficulty (rooms, locked doors, containers) where an indicator near the start tells the agent which object is the goal; the agent must remember the target object across exploration and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Treasure Hunter (TextWorld-generated)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Map-based retrieval tasks with increasing complexity (easy/medium/hard settings) that examine affordances, navigation efficiency, and memory (keeping target in mind while exploring).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic/working memory (task-level memory requirement to remember which object to retrieve)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Not prescribed by the benchmark; TextWorld notes agents should remember which object to retrieve and suggests that training on many related games or augmenting observations can help.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>TextWorld reports baseline results (choice-based random, BYU, Golovin) in Table 1 across difficulty levels; see Table 1 for per-level average scores and steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Baselines are compared on the benchmark, but no ablation isolating memory vs. no-memory is reported in TextWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Baseline agents score poorly on many difficulty levels; single-play (one-life) evaluation highlights inability to memorize/learn per-instance tasks when agents only see each game once.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>TextWorld emphasizes that memory (remembering the target object) is a necessary skill for the Treasure Hunter task and that easing the task by exposing game-state info or simpler grammar reduces the memory burden; it also suggests using many generated games for training to enable generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextWorld: A Learning Environment for Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning with a natural language action space <em>(Rating: 2)</em></li>
                <li>Neural map: Structured memory for deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning how not to act in text-based games <em>(Rating: 2)</em></li>
                <li>Text-based adventures of the golovin ai agent <em>(Rating: 2)</em></li>
                <li>What can you do with a rock? affordance extraction via word embeddings <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8517",
    "paper_id": "paper-49552345",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "LSTM-DQN",
            "name_full": "LSTM-DQN (LSTM Deep Q-Network)",
            "brief_description": "A DQN-style agent that uses an LSTM encoder over textual observations to produce a state representation for Q-value prediction; used in prior text-game work to capture temporal context.",
            "citation_title": "Language understanding for text-based games using deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "LSTM-DQN",
            "agent_description": "A reinforcement-learning agent that encodes observations with an LSTM and uses a DQN head to estimate Q-values for actions; intended to capture sequential information in text-game observations.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Homeworld and Fantasyworld (Evennia-based environments)",
            "benchmark_description": "Two deterministic/stochastic text-game environments used in prior work: Homeworld is small (4 rooms); Fantasyworld is larger and more stochastic. They test language understanding, action selection, and transfer.",
            "memory_used": true,
            "memory_type": "recurrent (LSTM) working/short-term memory",
            "memory_architecture": "An LSTM encodes the observation text o = w1...wn into a vector s = LSTM(o), which is then used by the Q-network to score actions; the LSTM hidden state provides temporal context across observation tokens.",
            "memory_integration_strategy": "LSTM-encoded observation vector is passed to the Q-network as the state representation; the model does not incorporate a separate retrieval module or explicit episodic store.",
            "performance_with_memory": "Completes 100% of quests in Homeworld and 96% of quests in Fantasyworld (reported completion rates).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No ablation in this TextWorld paper comparing LSTM vs. no-LSTM for that model; the paper mentions transfer experiments in prior work where the LSTM component was transferred to a shuffled environment (Homeworld2) and sped up learning.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "The TextWorld paper explicitly notes that prior models (including LSTM-DQN as discussed) are not conditioned on previous actions/observations in the way required for full POMDP handling and thus have limited capacity for partial observability in some setups.",
            "recommendations_or_conclusions": "Recurrent encoders (LSTMs) provide useful sequential encoding and can be transferred between related environments, but TextWorld emphasizes that more explicit memory/conditioning on history is needed to fully address partial observability in text games.",
            "uuid": "e8517.0",
            "source_info": {
                "paper_title": "TextWorld: A Learning Environment for Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network (DRRN)",
            "brief_description": "A model that separately embeds observations and textual action candidates and computes Q-values for observation–action pairs, designed for choice-based text games.",
            "citation_title": "Deep reinforcement learning with a natural language action space",
            "mention_or_use": "mention",
            "agent_name": "DRRN",
            "agent_description": "DRRN computes Q(s, a_j) by embedding the observation and each candidate action separately and combining them (relevance scoring) to produce Q-values over textual actions.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Saving John and Machine of Death (choice-based text games)",
            "benchmark_description": "Choice-based narrative games with different vocabulary/action sizes; used to evaluate models that score text actions given an observation.",
            "memory_used": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_integration_strategy": null,
            "performance_with_memory": null,
            "performance_without_memory": "DRRN converged on both games; achieved optimal cumulative reward on Saving John and a suboptimal but stable policy on Machine of Death.",
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "TextWorld notes that DRRN (like some other prior models) does not condition on previous actions/observations, and therefore lacks the capacity to handle partial observability; no memory ablation experiments are reported here.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Lack of conditioning on history prevents handling POMDP aspects in parser-based games.",
            "recommendations_or_conclusions": "Models that score single-step state-action pairs (like DRRN) are limited in partially observable text games; adding history conditioning or memory is necessary.",
            "uuid": "e8517.1",
            "source_info": {
                "paper_title": "TextWorld: A Learning Environment for Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Golovin",
            "name_full": "Golovin agent (Kostka et al.)",
            "brief_description": "A modular agent for classic text-based games that leverages a pre-trained LSTM language model for keyword extraction and a large command corpus to generate candidate actions.",
            "citation_title": "Text-based adventures of the golovin ai agent",
            "mention_or_use": "use",
            "agent_name": "Golovin",
            "agent_description": "A multi-module system that pre-trains an LSTM language model on fantasy books to extract keywords from scene descriptions and uses a command corpus and heuristics to generate actionable commands in parser-based games.",
            "llm_model_name": null,
            "llm_model_description": "Pre-trained LSTM language model (size unspecified in TextWorld); used for keyword extraction rather than as a large pretrained LLM.",
            "benchmark_name": "Curated list of hand-authored games and Treasure Hunter benchmark (evaluated in TextWorld)",
            "benchmark_description": "Hand-authored Infocom-style games and generated Treasure Hunter tasks; used to evaluate baseline agents including Golovin.",
            "memory_used": true,
            "memory_type": "LSTM-based sequential language model (short-term / contextual encoding)",
            "memory_architecture": "A pre-trained LSTM language model is used to extract keywords from current scene descriptions; the agent also maintains other modular state (e.g., inventories/rules) but TextWorld does not detail an explicit episodic memory store for Golovin.",
            "memory_integration_strategy": "LSTM-derived keywords inform command generation modules; integration is modular rather than an explicit episodic retrieval mechanism.",
            "performance_with_memory": "Performance reported in Table 1 of the paper for the Treasure Hunter one-life tasks (Golovin compared to BYU and random baseline); see Table 1 for average scores per difficulty level.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "TextWorld evaluates Golovin against BYU and a random choice baseline on Treasure Hunter levels but does not provide an ablation isolating the pre-trained LSTM effect.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Baselines including Golovin perform poorly on many hand-authored games and on harder generated tasks; TextWorld does not report detailed failure modes tied specifically to Golovin's memory component.",
            "recommendations_or_conclusions": "Pre-training a language model to extract keywords is a practical way to improve command generation in domain-specific text games; modular architectures can leverage such components, but further work is needed for robust generalization and memory over unseen games.",
            "uuid": "e8517.2",
            "source_info": {
                "paper_title": "TextWorld: A Learning Environment for Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Action Elimination Network",
            "name_full": "Action Elimination Network (Haroush et al.)",
            "brief_description": "A method to reduce the effective action space in parser-based text games by learning to score and eliminate unlikely/failing actions using stored feedback.",
            "citation_title": "Learning how not to act in text-based games",
            "mention_or_use": "mention",
            "agent_name": "Action Elimination Network",
            "agent_description": "A module that estimates the probability an action will fail in the current scene; it is trained using augmented replay data that includes parser/engine feedback so the agent can restrict candidate actions during selection.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Parser-based text games (generic)",
            "benchmark_description": "Text-based games where the action space is the set of word-sequences recognized by the parser; the method aims to reduce the large, sparse action set.",
            "memory_used": true,
            "memory_type": "experience replay augmented with parser feedback (stored observations/actions/feedback)",
            "memory_architecture": "Replay buffer augmented with tuples including the game's parser feedback in addition to &lt;observation, action, reward&gt;; these stored examples train an elimination module that assigns scores to actions.",
            "memory_integration_strategy": "During action selection the elimination module scores actions; only top-k scored actions are considered at greedy steps, and low-scoring actions are probabilistically rejected during exploration.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "TextWorld summarizes the approach but does not provide its own ablation; the cited work compares action elimination to standard exploration but TextWorld does not reproduce detailed results.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "If elimination scores are inaccurate, valid actions may be pruned; relies on quality of stored parser feedback and training signal.",
            "recommendations_or_conclusions": "Storing parser feedback in the replay buffer and training an action-elimination module is recommended to reduce the effective action space in parser-based games.",
            "uuid": "e8517.3",
            "source_info": {
                "paper_title": "TextWorld: A Learning Environment for Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Neural Map",
            "name_full": "Neural Map: Structured memory for deep reinforcement learning",
            "brief_description": "A structured external-memory architecture proposed for deep RL that stores spatial or structured representations (referenced in TextWorld as relevant prior work on memory in RL).",
            "citation_title": "Neural map: Structured memory for deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Neural Map (memory architecture)",
            "agent_description": "An external structured memory (map) for RL agents that can store and retrieve spatial/structured information to support long-term reasoning; TextWorld references it as related work for memory mechanisms.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": null,
            "benchmark_description": "Referenced as a general RL memory architecture (not evaluated in TextWorld experiments).",
            "memory_used": true,
            "memory_type": "external structured memory (map-like / spatial memory)",
            "memory_architecture": "Structured map-like memory (detailed architecture not described in TextWorld; cited work should be consulted for specifics).",
            "memory_integration_strategy": "Not detailed in TextWorld; cited as an example of structured memory approaches applicable to RL.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": null,
            "best_memory_strategy": null,
            "limitations_or_failure_cases": null,
            "recommendations_or_conclusions": "TextWorld points to structured-memory architectures (e.g., Neural Map) as promising directions for handling tasks requiring long-term memory in text-game RL.",
            "uuid": "e8517.4",
            "source_info": {
                "paper_title": "TextWorld: A Learning Environment for Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Treasure Hunter",
            "name_full": "Treasure Hunter benchmark (TextWorld)",
            "brief_description": "A generated TextWorld benchmark where the agent must locate and retrieve a specific object indicated by a nearby 'indicator' object; designed to test affordance extraction, navigation, and memory (remembering the target).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Treasure Hunter task (evaluated with baseline agents)",
            "agent_description": "Generated tasks of varying difficulty (rooms, locked doors, containers) where an indicator near the start tells the agent which object is the goal; the agent must remember the target object across exploration and retrieval.",
            "llm_model_name": null,
            "llm_model_description": null,
            "benchmark_name": "Treasure Hunter (TextWorld-generated)",
            "benchmark_description": "Map-based retrieval tasks with increasing complexity (easy/medium/hard settings) that examine affordances, navigation efficiency, and memory (keeping target in mind while exploring).",
            "memory_used": true,
            "memory_type": "episodic/working memory (task-level memory requirement to remember which object to retrieve)",
            "memory_architecture": null,
            "memory_integration_strategy": "Not prescribed by the benchmark; TextWorld notes agents should remember which object to retrieve and suggests that training on many related games or augmenting observations can help.",
            "performance_with_memory": "TextWorld reports baseline results (choice-based random, BYU, Golovin) in Table 1 across difficulty levels; see Table 1 for per-level average scores and steps.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Baselines are compared on the benchmark, but no ablation isolating memory vs. no-memory is reported in TextWorld.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Baseline agents score poorly on many difficulty levels; single-play (one-life) evaluation highlights inability to memorize/learn per-instance tasks when agents only see each game once.",
            "recommendations_or_conclusions": "TextWorld emphasizes that memory (remembering the target object) is a necessary skill for the Treasure Hunter task and that easing the task by exposing game-state info or simpler grammar reduces the memory burden; it also suggests using many generated games for training to enable generalization.",
            "uuid": "e8517.5",
            "source_info": {
                "paper_title": "TextWorld: A Learning Environment for Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning with a natural language action space",
            "rating": 2
        },
        {
            "paper_title": "Neural map: Structured memory for deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Learning how not to act in text-based games",
            "rating": 2
        },
        {
            "paper_title": "Text-based adventures of the golovin ai agent",
            "rating": 2
        },
        {
            "paper_title": "What can you do with a rock? affordance extraction via word embeddings",
            "rating": 1
        }
    ],
    "cost": 0.02151475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TextWorld: A Learning Environment for Text-based Games
29 Jun 2018</p>
<p>Marc-Alexandre Côté 
Ákos Kádár 
Tilburg University</p>
<p>Xingdi Yuan 
Ben Kybartas 
McGill University</p>
<p>Tavian Barnes 
Emery Fine 
James Moore 
Matthew Hausknecht 
Layla El Asri 
Mahmoud Adada 
Wendy Tay 
Adam Trischler 
Microsoft Research 
Ludwig Wittgenstein 
TextWorld: A Learning Environment for Text-based Games
29 Jun 2018BE32693FA339DA0105C92F8D543DB609arXiv:1806.11532v1[cs.LG]
The limits of my language mean the limits of my world.</p>
<p>Introduction</p>
<p>Text-based games are complex, interactive simulations in which text describes the game state and players make progress by entering text commands.They are fertile ground for language-focused machine learning research.In addition to language understanding, successful play requires skills like long-term memory and planning, exploration (trial and error), and common sense.</p>
<p>Consider Zork [Infocom, 1980], one of the genre's most famous examples.Figure 1 depicts Zork's opening scene along with two player commands and the corresponding system responses.As illustrated, the game uses natural language to describe the state of the world, to accept actions from the player, and to report subsequent changes in the environment.Through sequential decision making, the player works toward goals which may or may not be specified explicitly.In the nomenclature of reinforcement learning (RL), language is the action space and also the observation space [Narasimhan et al., 2015].In text-based games, the observation and action spaces are both combinatorial and compositional -major challenges for reinforcement learning.Furthermore, text-based games are partially observable since descriptive text does not communicate complete information about the underlying game state or may do so ambiguously.As a consequence of Figure 1: Intro to Zork these (and other) challenges, hand-authored games like Zork are beyond the capabilities of current learning algorithms [Narasimhan et al., 2015, Haroush et al., 2018].</p>
<p>To help agents progress toward mastering text games in a controlled and scientific manner, we introduce the TextWorld learning environment.TextWorld is a sandbox environment [Wright, 1996, Sukhbaatar et al., 2015] in which games of varying complexity emerge from a set of underlying world mechanics.In this setting, simpler games can act as stepping stones toward more complex games.Like the Arcade Learning Environment (ALE, [Bellemare et al., 2013]), Gym [Brockman et al., 2016], and CommAI [Baroni et al., 2017], TextWorld enables interactive play-through of a curated set of games.Unlike previous text-based environments, including TextPlayer and PyFiction, TextWorld's sandbox functionality enables users to handcraft games or to construct games automatically through a suite of generative mechanisms.</p>
<p>Specifically, TextWorld features a logic engine that automatically builds game worlds, populates them with objects and obstacles, and generates quests that define a goal state and how to reach it.It automatically generates text descriptions of underlying game states using an extensible vocabulary and a context-free grammar (CFG).Common-sense rules encoded in the logic and grammar govern generated worlds and the quests within them, to make these human-interpretable and consistent with existing games: e.g., keys open locked doors and can be carried or stored in containers; food items can be combined, cooked, and eaten.Furthermore, because the vocabulary contains synonyms for most nouns, verbs, and adjectives, different surface forms can be applied automatically to abstract types to add variety and complexity: e.g., the <container> object may manifest as a chest or cabinet; the <move> action may manifest as walk or go.</p>
<p>TextWorld's generative nature has several downstream implications for learning.First, it means there exists a known and structured representation of the partially observable game state.This enables exact state-tracking [Henderson et al., 2014] and the corresponding assignment of intermediate rewards in training (if desired).Second, agents can be trained on a potentially infinite set of related text games rather than a finite collection as in previous learning environments.By controlling parameters of the generative process for training and test games, TextWorld can be used to investigate curriculum learning, generalization, and transfer learning in RL.Tunable parameters include the length of quests, the size of environments, the number of abstract action and object types, the number of synonyms for each type, complexity of the descriptive grammar, and more.</p>
<p>A powerful feature of language which motivates our interest in text games is that it abstracts away complex physical processes.For instance, through text an agent could learn and use the concept that opening doors provides access to connected rooms without going through the (literal) motions of turning knobs in 3D space and time.This level of abstraction can be useful for studying functions of control, planning, etc. in isolation and in tandem with the function of language itself.The aim of this paper is to introduce TextWorld to the research community.Its primary contributions are:</p>
<p>• A survey of the machine-learning challenges of and approaches to text-based games, including a curated list of hand-authored games with corresponding analysis;</p>
<p>• A detailed description of the TextWorld framework, its features, and how to use it;</p>
<p>• An initial set of simple text games to be used as RL benchmarks;</p>
<p>• Evaluation of several baseline algorithms on both benchmark and hand-authored games.</p>
<p>Subsequent works will more deeply investigate novel approaches to RL for text games.Our hope is that TextWorld becomes a living resource, with contributors developing new benchmarks and algorithms to push the state of the art forward.</p>
<p>The remainder of this paper is organized as follows.In Section 2 we introduce text-based games, formalize them as RL problems and highlight their challenges.In Section 3 we delve into details of the TextWorld framework, how it generates and interacts with text games, and how it may be used to train RL agents.Section 4 describes related frameworks and existing approaches to solving text games, while Section 5 describes some of TextWorld's benchmark tasks and our experimental results.We discuss limitations of the framework and future work in Section 6 before concluding.</p>
<p>Text Games from a Reinforcement Learning Perspective</p>
<p>Text-based games are sequential decision-making problems that can be described naturally by the Reinforcement Learning (RL) formalism.In this section, we define some of the terminology found in text-based games, formalize the text-based environment as an RL problem, discuss challenges faced by RL agents in such environments, and show how these challenges motivate the need for a framework like TextWorld.In the following, an "agent" is a model that takes text information as input and outputs text commands to progress through a game.</p>
<p>Text-based Games</p>
<p>Text-based games are turn-based games usually played through a command line terminal.At each turn, several lines of text describe the state of the game, and the player may enter a text command to change this state in some desirable way (i.e., to move towards a goal).A game's built-in parser or interpreter deciphers player commands and maps them to state changes (events in the game).</p>
<p>The genre became popular in the early 1980s especially with the release of Zork [Infocom, 1980], which featured rich storytelling and an advanced command parser.</p>
<p>Gameplay</p>
<p>Text-based games can be classified according to how the player issues commands (see Figure 2): in parser-based games, the player types text commands character by character; in choice-based games, the player chooses from a given list of command options; and in hypertext-based games, the player clicks on one of several links present in the description.The work in this paper focuses on parser-based games.</p>
<p>Figure 2: Types of text-based games.Image from [He et al., 2015].</p>
<p>In text-based game terminology, each discrete in-game location is called a room (e.g., "Kitchen", "In front of the white house", and so on).A game may contain one or several rooms connected in some topology forming a map.To explore the environment, the player issues navigational commands: go followed by a cardinal direction (north, northeast, etc.) or an orthogonal direction (up, down).Maps vary greatly from one game to another.Room exits and entrances do not always match (e.g., go north and then go south may not return you to where you started).Also, some navigational commands may not have a reciprocal (e.g., in Zork [Infocom, 1980], a trapdoor locks from behind preventing the player from going back).</p>
<p>Most of the time, rooms contain objects the player can interact with (e.g., take sandwich from table, eat sandwich, open door, etc.).Objects are defined by their attributes, which also determine how the player can interact with them (through object affordances).For instance, an object could be portable (e.g., a lamp) or portable and edible (e.g., a sandwich).</p>
<p>One of the biggest challenges when playing parser-based games is figuring out what are the commands that will be understood by the parser (i.e., that are intended by the game's author).Also, depending on the game, the result of some commands may be stochastic (e.g., go up succeeds 75% of the time, but 25% of the time results in the player falling down to a previous room).A detailed list of puzzles and challenges traditionally found in text-based games can be found in Appendix A.</p>
<p>Text-based Games as POMDPs</p>
<p>Fundamentally, text-based games can be seen as partially observable Markov decision processes (POMDP) [Kaelbling et al., 1998] where the environment state is never observed directly.To act optimally, an agent must keep track of all observations, i.e., textual feedback received after entering commands.Although the observation can be augmented with feedback from commands like look and inventory, which describe the agent's surroundings and possessions, this information is still limited to the current room.</p>
<p>Formally, a text-based game is a discrete-time POMDP defined by (S, T, A, Ω, O, R, γ), where S is the set of environment states, T is the set of conditional transition probabilities between states, A is the set of words that are used to compose text commands, Ω is the set of observations, O is a set of conditional observation probabilities, R : S × A → R is the reward function, and γ ∈ [0, 1] is the discount factor.</p>
<p>Environment States (S)</p>
<p>The environment state at turn t in the game is s t ∈ S. It contains the complete internal information of the game, like the position and state of every entity (rooms, objects, player, etc.), much of which is hidden from the agent.When an agent issues a command c t (defined next), the environment transitions to state s t+1 with probability T (s t+1 |s t , c t ).</p>
<p>Actions (A)</p>
<p>At each turn t, the agent issues a text command c t of at least one word.In parserbased games, the interpreter can accept any sequence of characters (of any length) but will only recognize a tiny subset thereof.Furthermore, only a fraction of recognized commands will actually change the state of the world.The resulting action space is enormous and intractable for existing RL algorithms.We make the following two simplifying assumptions:</p>
<p>• Word-level Commands are sequences of at most L words taken from a fixed vocabulary V .</p>
<p>• Syntax Commands have the following structure: verb[noun phrase [adverb phrase]], where [. . .] indicates that the sub-string is optional.In this context, a noun phrase is a string identifying an object (e.g., "the big wooden chest").Similarly, an adverb phrase provides additional context for the command (e.g., "with the red key").To simplify the syntax further, determiners are omitted.1</p>
<p>The agent's action space is some vocabulary V plus a special token <return> that indicates the end of a command.Each action a i t ∈ A is a token, where t is the turn in the game and i indicates the ith token in the command c t .A command is a sequence of n ≤ L tokens c t = [a 1 t , . . ., a n t ] that respects the syntax previously defined and ends with a n t = <return>.</p>
<p>The agent's policy is a mapping between its states and actions.In TextWorld, the agent's policy π θ , where θ are the policy's parameters, maps a state s t and words generated in the command so far to the next word to generate:
a i t = π θ (s t , a 0 t , ..., a i−1 t ).</p>
<p>Observations (Ω)</p>
<p>RL Challenges in Text-based Games</p>
<p>Complex environments make training RL agents challenging for several reasons.Here, we list some conventional challenges known in the RL literature that are also prevalent in text-based games.</p>
<p>Partial Observability</p>
<p>As mentioned, states of text-based games are partially observable.Only the local information such as the current room description and the player's inventory is made available.Moreover, taking into account only the latest observation, it may be impossible to differentiate certain states based on observations.For instance, seeing a blue locked chest in a room, it is important for the agent to know whether or not it collected or saw a blue key in the past.The environment might give the same feedback for two different commands (e.g., "taken" might be the feedback for take blue key or take red apple).Furthermore, important information about the environment might not be apparent in the observation (e.g., whether a chest is locked or not, what it contains, etc.).Observations may also be time-sensitive (e.g., the agent only gets a reward when examining clues for the first time).</p>
<p>Large State Space</p>
<p>With large state spaces, tabular methods for solving RL problems are no longer practical [Sutton and Barto, 2018].Finding good approximate solutions is still an active area of research.In text-based games, the state space is combinatorial and enormous; the number of possible states increases exponentially with the number of rooms and objects.</p>
<p>Large and Sparse Action Space</p>
<p>As with large state spaces, reasoning in an environment with a large number of actions necessitates finding good approximate solution methods to replace the tabular ones [Sutton and Barto, 2018].The text-based game setting is especially challenging since the action space is large and sparse; the space of all word strings is much larger than the space of admissible commands (i.e., commands that actually change the underlying state s t ).In addition, the outcome or even the validity of some commands might depend on a specific event or how much time has passed (e.g., the tide rises and blocks the player in [Bates, 1987]).</p>
<p>Exploration vs. Exploitation Balancing exploration of the environment and the exploitation of known information is a fundamental issue in RL [McFarlane, 2003].Exploration is at the core of text-based games as they cannot be solved by learning a purely reactive controller.Instead, a strategy that promotes directed exploration must be used; the agent must deliberately explore the environment, collecting information about objects and persons encountered along the way (e.g., you never know what is in a box without opening it first).Such information hints about the goal/purpose of the game, what dangers are present, and provides clues that might become handy later in the game for solving puzzles.We expect that agents, like humans, will benefit from exploration driven by curiosity.</p>
<p>Long-term Credit Assignment Knowing which actions were responsible for obtaining a certain reward, especially when rewards are sparse, is another fundamental issue in RL [Sutton and Barto, 2018].Sparse rewards are inherent to text-based games in which the agent must generate a sequence of actions before observing a change in the environment state or getting a reward signal.</p>
<p>For instance, activating a switch might have no immediate effect although it is essential for completing the game.Most text-based games feature sparse rewards, on the order of a single positive reward every 10-20 steps when following an optimal state trajectory.</p>
<p>Additional Challenges</p>
<p>By their very nature, text-based games bring additional challenges related to natural language understanding.</p>
<p>Observation Modality Observations consist in the environment's textual feedback to the previous command.This means that the observation space is unbounded, consisting of arbitrary-length sequences of characters.To simplify things, we assume that an observation is made of spaceseparated words that may or may not be found in an English dictionary.One drawback of looking only at words is that we may lose some information provided by the spacing (e.g., ASCII art in Infidel [Berlyn, 1983] or a sonar map in Seastalker [Galley and Lawrence, 1984]).</p>
<p>Understanding Parser Feedback Text-based games process player input using a parser.The parser varies from game to game in terms of the actions it recognizes.For example, nearly all games recognize actions like get, take and go, but only some games recognize verbs like tickle, swim, dig and bribe.Part of the challenge of playing a parser-based text game is understanding which verbs and objects are recognized by the parser.Making this task more difficult, failure messages vary from game to game when the parser receives an invalid or unrecognized command.</p>
<p>Common-sense Reasoning &amp; Affordance Extraction</p>
<p>To succeed at text-based games, it is necessary to understand how to interact with everyday objects.For example, if the description of a location includes a tree, it is likely that a player could climb or chop the tree, but not eat or drive it.The problem of identifying which verbs are applicable to a given object is called affordance extraction and learning agents must solve it efficiently to make progress in text-based games without exhaustive search.</p>
<p>Language Acquisition Some objects and actions may be named with invented words.Also, modifier words affect how some objects can be interacted with (this is related to affordance extraction).The meaning of these words must be learned on-the-fly while interacting with the environment.Text-based games also use linguistic coreference, since it is more pleasant to humans, which can complicate the task for learning machines.</p>
<p>RL with TextWorld</p>
<p>Solving a single text-based game often corresponds to tackling most of the above challenges at once, which makes it very difficult for existing algorithms.What would be useful is a way of testing and debugging RL agents in simpler settings (e.g., one room with two objects where the goal is to eat the edible one).This is the main purpose of TextWorld's generative functionality (described in Section 3.2).It can be used to focus on desired subsets of the challenges listed above.</p>
<p>First, it is possible to control the size of the state space (e.g., the number of rooms, number of objects, and how many commands are required in order to reach the goal optimally).At the moment, TextWorld has deterministic transitions between states.</p>
<p>It is also possible to control the partial observability of the state by augmenting the agent's observations.The environment can provide the agent with a list of objects present in-game or even provide all information about the current game state.For instance, instead of generating the observation that there is a blue chest, the environment could state that the chest is locked and that inside the chest there is a red apple.In this setting, the agent does not need to explore to determine the layout of the world and the objects it contains.</p>
<p>TextWorld enables one to generate a large number of games and control their shared characteristics (map, objects, goals, etc.).This is useful for focusing, e.g., on language acquisition and affordance extraction: the agent can be trained on games with a fixed number of rooms and object types but with different object names.By interacting with objects, the agent should learn which have a similar function and generalize from one game instance to another.</p>
<p>There are several ways to ease the language generation task.It is possible to restrict the agent's vocabulary to in-game words only or to restrict the verbs that the agent can generate to those understood by the parser.It is also possible to use a simplified grammar where object names are replaced with symbolic tokens (e.g., "You see container1 and container2.").Language generation can be circumvented completely by converting every generated game into a choice-based game.In this case, commands c t are the agent's actions, i.e., the agent's output becomes an index into the set of admissible commands (see Section 3.3.1)rather a sequence of words.</p>
<p>Finally, instead of earning rewards only at the end of a game if the agent is successful, one can also provide intermediate rewards during training based on environment state transitions and the ground truth winning policy (see Section 3.1.1).</p>
<p>The TextWorld Learning Environment</p>
<p>TextWorld2 is a Python framework for training and testing RL agents on text-based games.It enables generation from a game distribution parameterized by the map size, the number of objects, quest length and complexity, richness of text descriptions, and more.The framework's architecture is shown in Figure 3.We detail the Engine component in Section 3.1, which covers how the internal state of generated games is represented and how the transition function is defined.The Generator component is described in Section 3.2, which explains the process of automatic game generation.TextWorld can also be used to play existing text-based games (see a curated list of games in Section 5.1) but provides more limited information from the internal states of such games.</p>
<p>Game Engine</p>
<p>Game generation in TextWorld relies on a simple inference engine that ensures game validity at every step in the generation process.A game is said to be valid if it is possible to reach the end goal from the initial state.Although we could have used an existing problem-solver for this purpose, we did not require the full power of logical programming languages.TextWorld's</p>
<p>TextWorld Generator</p>
<p>Inform 7</p>
<p>TextWorld Engine</p>
<p>Agent Playthrough Statistics</p>
<p>Git-Glulx</p>
<p>Game definition -Map, objects -Quest -Descriptions</p>
<p>Knowledge base</p>
<p>Object types
• Door • Container • Supporter • Food item • ... Actions • Open • Take • Eat • Put • ... Predicates • at/in/on • edible • north_of • open/closed • ... Themes Medieval world • Dungeons • Gothic furniture • Gates • Weapons • ... Home world • Rooms • Furniture • Doors • Food • ...</p>
<p>Game</p>
<p>Figure 3: Overview of the framework.The two main components (in blue) of the proposed pipeline: the game generator and the game engine, which handles interactive play.Inform 7 and Git-Glulx are third-party libraries (in green) and the agent (in red) should be provided by the user.Given some knowledge base, sampled game definitions are first converted to Inform 7 code and compiled into a Glulx executable file.Then, agents interact with the game by communicating with the Git-Glulx interpreter via TextWorld.</p>
<p>Figure 4: Simple environment with one room (kitchen), a container (fridge), a supporter (table), a food item (apple) and nothing in the player's inventory.inference engine implements simple algorithms specific to our environments, such as a one-step forward and backward chaining with or without fact creation (more on this in Section 3.2).In future work our aim is to integrate the TextWorld framework with well established frameworks such as GDL [Genesereth et al., 2005] or STRIPS [Fikes and Nilsson, 1971].</p>
<p>To better explain the framework, let's consider the following simple text-based environment.There is a kitchen with a table and a closed fridge in which there is an apple.A visual representation can be seen in Figure 4.The player is represented by the small avatar and the letter P. Objects of the container type are represented by a chest, supporters (or surfaces) are represented by a table and food-type items are represented by an apple symbol.The anchor symbol next to certain objects means that they are fixed in place (i.e., cannot be taken).</p>
<p>TextWorld's generated text-based games can be represented internally as a Markov Decision Process (MDP) [Puterman, 1994] defined by (S, A, T, R, γ), where S is the set of environment states, A is the set of actions (with A st those available in state s t ∈ S), T (s t , a, s t+1 ) = P(s t+1 |s t , a) is the state transition function that depends on current state s t and action taken a ∈ A st , R : S × A → R is the reward function, and γ ∈ [0, 1] is the discount factor.Text-based games are episodic since they stop when the player reaches one of the winning (goal) states G ⊂ S.</p>
<p>Note that the MDP part of the POMDP, defined in Section 2.2, is semantically equivalent to the one described in this section.The sole exception is the action space; in the POMDP, we assume there exists an underlying function that maps text strings (generated by the agent) to the game actions defined in this section.This is the role of the game's interpreter.</p>
<p>Environment states (S) Game states are defined in terms of logical predicates.Each predicate p(v 1 , . . ., v m ) consists of a symbol p drawn from the alphabet of predicates Σ followed by an mtuple of variables.These predicates define the relations between the entities (objects, player, room, etc.) present in the game.A logical atom is a predicate whose variables are all are bound, i.e., free variables (placeholders) have been substituted by concrete entities.A game state s ∈ S consists in a multiset of logical atoms representing the facts that are currently true about the world, also known as "resources" in linear logic.Taking as an example Figure 4, the state depicted there can be represented as
s t = at(fridge, kitchen) ⊗ at(table, kitchen) ⊗ in(apple, fridge) ⊗ open(fridge) ⊗ at(P, kitchen),
where the symbol ⊗ is the linear logic multiplicative conjunction operator.</p>
<p>The set of winning states G is composed of any state s for which all the winning conditions (a set of facts) hold.The winning conditions are determined during the game generation process (Section 3.2.2).For instance, a winning condition could be as simple as in(apple, I), i.e., the apple being in the player's inventory.</p>
<p>State Transition Function (T )</p>
<p>The state transition function is defined using linear logic [Russell and Norvig, 2016, Ch. 8] and is inspired in part by Ceptre [Martens, 2015], a linear logic programming language.Logical rules are stored in a knowledge base and define what is possible in the game.For the working example, the relevant rules are
open(C) :: $at(P, R) ⊗ $at(C, R) ⊗ closed(C) open(C) close(C) :: $at(P, R) ⊗ $at(C, R) ⊗ open(C) closed(C) take(F, C) :: $at(P, R) ⊗ $at(C, R) ⊗ $open(C) ⊗ in(F, C) in(F, I) take(F, S) :: $at(P, R) ⊗ $at(S, R) ⊗ on(F, S) in(F, I) put(F, S) :: $at(P, R) ⊗ $at(C, R) ⊗ $open(C) ⊗ in(F, I) in(F, C) insert(F, C) :: $at(P, R) ⊗ $at(S, R) ⊗ in(F, I) on(F, S) eat(F, S) :: in(F, I) eaten(F ).
The uppercase italic letters represent variables for objects of a certain type (F : food item, S: supporter, C: container and R: room).The entities P and I represent the player and its inventory.The symbol (lolli) is the linear implication operator.The interpretation of the linear implication is such that it consumes the resources on the left-hand-side (henceforth LHS) and generates resources on the right-hand-side (henceforth RHS).The notation $ is a shorthand meaning a predicate is implicitly carried over to the right-hand side.Given a state and the set of rules, we can perform forward chaining.Applying a rule to state s, whose LHS is satisfied by s, leads to a conclusion, which is a new collection of atoms generated by the RHS of the selected rule.Applying all possible rules for all conclusions leads to a proof-tree with triplets (s, a, s ), where s is an assumption and rule a leads to the conclusion s .Adding control to forward chaining, by only exploring paths where unseen states are introduced, leads to an algorithm that upon termination discovers all s ∈ S in the MDP.Merging the duplicate states in the proof-tree provides the underlying (S, A, T ) of the MDP.</p>
<p>Action Space (A) An action is one of the rules defined in the knowledge base for which all free variables have been "grounded", i.e., substituted by bound variables appropriately.Actions available in the current state, A st , can be obtained by performing a single step of forward-chaining given facts true in s t .In other words, the inference engine is used to retrieve all possible substitutions for every rule that can be applied to s t .In the initial state of the working example, the available actions are close(fridge) ::</p>
<p>$at(P, kitchen) ⊗ $at(fridge, kitchen) ⊗ open(fridge) closed(fridge) take(apple, fridge) ::</p>
<p>$at(P, kitchen) ⊗ $at(fridge, kitchen)</p>
<p>⊗ $open(fridge) ⊗ in(apple, fridge) in(apple, I)</p>
<p>Reward Function (R) In the general case, games generated with TextWorld only provide a positive reward when reaching a winning state s ∈ G.The goal is to maximize the expected discounted sum of rewards received E [ t γ t r t ] where r t = R(s t , a t ).</p>
<p>Intermediate Reward</p>
<p>Tracking the state of the player allows us to determine a winning policy (not necessarily optimal) for any game state.A prerequisite is that a winning policy exists from the player's initial position (this is guaranteed for generated games).If so, then by monitoring state changes we can update the winning policy accordingly: if the agent performs the action dictated by the current winning policy, it progresses to the next desired state and we simply shift the policy forward one time-step; if the agent goes off the winning trajectory we add reciprocal actions to the policy to undo or correct this negative progress; and if the agent changes its state in a way that does not affect the quest, the winning policy does not change.</p>
<p>In addition to the final reward, TextWorld can provide an intermediate reward which is tied to the winning policy.After each command, if the winning policy increases in length, meaning that as a result of the last action, additional commands are required to solve the game, then we assign a negative reward.If the winning policy shortens, meaning the last action brought the agent closer to the goal, we assign a positive reward.Otherwise, the reward is 0.</p>
<p>Game Generation</p>
<p>TextWorld can be used as a sandbox environment in which predefined dynamics govern emergent games.With this sandbox, it is possible to generate a combinatorial (not to say infinite) set of games from which an agent could learn the underlying dynamics.Since we control the generation process, we can construct games where the knowledge to be learned is interpretable to humans.</p>
<p>TextWorld's game generator takes as input a high-level specification of a game and outputs the corresponding executable game source code in the Inform 7 language (Appendix C).The game specification assigns values to parameters such as the number of rooms, the number of objects, the length of the quest, the winning conditions, and options for the text generation (e.g., theme, co-references, adjectives, and so on).</p>
<p>World Generation</p>
<p>TextWorld generates maps through a simple procedure based on the Random Walk algorithm [Pearson, 1905].This enables us to generate a wide variety of room configurations, which in turn makes the set of possible games very large.The map generation process is parameterized by the number of rooms, the grid size of the world, and whether room connections should have doors or not.The grid size influences the compactness of the room configuration, where smaller grids mean more compact maps with potentially more loops.</p>
<p>Once the map is generated, objects are added to the world uniformly across the rooms.Some objects are portable (as opposed to fixed in place), which means they can be nested on or in other objects that have the supporter or container attribute, or placed on the floor of a room or in the player's inventory.</p>
<p>Quest Generation</p>
<p>In TextWorld, the purpose of any game is to reach a winning state.The term quest will be used to represent a sequence of actions the player must perform to win the game.Note that this sequence does not have to be optimal or unique; many trajectories could lead to a winning state.</p>
<p>We define quest generation as the process of determining interesting sequences of actions from which to derive winning conditions.As discussed in Section 3.1, the inference engine in TextWord can perform forward-chaining to construct a tree of all possible action sequences given an environment.However, not all paths are interesting (from a human or RL perspective).For this reason, we impose a dependency constraint on the actions and reject paths containing cycles.The dependency relation is defined as follows: action a t depends on action a t−1 if and only if the RHS of a t−1 generates the resource(s) required by the LHS of a t .The winning condition of a given quest is the set of resources generated by the RHS of the last action.</p>
<p>We can generate quests by modifying the forward chaining algorithm to apply these constraints, calling the resulting process forward quest generation.An example quest is depicted in Figure 5. First, the player opens the door and moves south to enter the kitchen.When in the kitchen, the player opens the fridge, takes the apple, and finally eats it.</p>
<p>Backward Quest Generation</p>
<p>The end goal often defines the nature of a quest and yields significant rewards for the agent.Thus, it is desirable to specify the end goal in quest generation.The forward quest generation algorithm indirectly allows this specification, by generating all possible quests from an initial condition then searching for those that satisfy the ending constraint.However, as the number of states and the length of the desired quest increases, this approach becomes intractable.To remedy this, TextWorld also supports backward chaining.Backward chaining simply reverses forward chaining, starting from a specified end state rather than an initial state.The same dependency between subsequent actions and cycle rejection apply.Extending the World during Quest Generation TextWorld's generator extends forward and backward chaining with a fact creation step, which may occur before sampling a subsequent (or previous) action.Through fact creation, the generative process can add missing objects and facts to the world as needed, which yields more diverse quests and games.Figure 6 shows a simple example of backward quest generation.</p>
<p>Text Generation</p>
<p>The Text Generation module takes logical elements of the game state and renders them into coherent text.This serves as the observation provided to the agent.The engine generates object names, room descriptions, and quest instructions in constrained natural language using a contextfree grammar (CFG) [Chomsky, 1956].The ease of authoring such grammars has led to their adoption in various natural language generation (NLG) systems for games [Ryan et al., 2016].</p>
<p>The module is essentially a set of grammars, each generating particular aspects: e.g., there are separate grammars for creating object names, room descriptions, and instructions, respectively.Observable text is generated by iterating over all elements of the observation and filling templates with the appropriate information.As an example for object description: For a red box, the grammar may return "There is a [object-noun] here.It is [object-adjective]"., which is filled to create "There is a box here.It is red."Some basic maintenance also ensures fluency, e.g., using "an" vs. "a" when a noun begins with a vowel.</p>
<p>Using a context-free grammar gives a degree of textual variation, allowing the same world and quest to be represented a number of ways while also ensuring strict control over the results.While our current grammars function like a text templating system, CFGs offer the possibility of deeper, recursive text generation should it be desired.Our grammars can be extended and modified easily with additional production rules, enabling the generation of simpler or more complex sentence structures that may act as a level of game difficulty.</p>
<p>Object Names Object names are assigned to each term in the game.Names are randomly picked from the CFG and uniquely assigned to objects.The object's type is used to derive the start symbol sent to query the CFG for a name.An object name can be decomposed into two parts: adjective and noun.The adjective is optional, and may be used to create more complex object names, and correspondingly more complex descriptions.Object name generation is in general straightforward, and consists of selecting a random adjective and noun and combining them.So, e.g., given the nouns "box" and "cup", as well as the adjectives "dusty" and "red", there are four possible object names, "dusty box", "red box", "dusty cup" and "red cup".Adjectives are also used as hints to the player; for example, a key's adjective will always match the adjective of what it opens, e.g., a "red key" will open the "red chest".</p>
<p>Room Descriptions</p>
<p>The description of a room is the concatenation of the room-level description of every object it contains, shown typically when entering the room or upon using the look command.The room-level description of an object contains information the player should be aware of upon entering the room (e.g., "There is a chest here.It is open and you can see some gold coins in it.").The room's description also mentions its possible exits (e.g., "There is a path leading north.").It is updated dynamically based on changes to the states of objects in the room, for example listing whether a container is open, closed, or locked, and which objects it contains.</p>
<p>Quest Instructions</p>
<p>We use instructions to explain to the player what to do in a game.An instruction is a piece of text describing a particular action or several different actions.For example, "Retrieve the blue key" could be used to represent the action take blue key, whereas "Take the red key from the locked chest" may represent the sequence of actions unlock chest / open chest / take red key.In TextWorld, instructions may optionally describe every action of a quest (easier), only the final action (harder), or they may force the player to figure out what to do from scratch (goal identification; hardest).Likewise, the ability to combine actions into a single instruction can also be toggled; identifying a sequence of actions from an instruction rather than a single action is an additional challenge.</p>
<p>Text Generation Options TextWorld offers some control over different aspects of the text generation.Objects with similar attributes/states can be grouped together when describing a room (e.g., "In here, you see two red containers: a box and a chest.").Objects mentioned in an instruction can be referred to using one or several of their attributes (e.g., "Take the red edible thing.").Use of coreference (e.g., "There is a chest.It is open.In it, you see nothing interesting.") is also optional.</p>
<p>TextWorld also offers the choice between two themed grammars: house and basic.The house theme describes the world as if the game takes place in a modern house.The second theme uses a simple grammar with almost no linguistic variation (e.g., no adjectives, no multi-word names).In this case, objects with the same attributes use a shared, prototypical prefix for their names followed by a number (e.g., stand42).The basic grammar cuts down the vocabulary and the language complexity to ease the training of neural generative models.These house and basic themes can be seen applied to the same underlying game in Figure 7.</p>
<p>Game Interaction with TextWorld</p>
<p>Most basically, TextWorld can be used to play any text-based games that are interpretable either by Z-machine (via the Frotz interpreter) or by Glulx (via a custom git-glulx interpreter).The framework handles launching and interacting with the necessary game processes.It provides a simple API for game interaction inspired by that of OpenAI's Gym [Brockman et al., 2016].As an example, playing a text-based game requires just a few lines of Python code:</p>
<p>import textworld env = textworld.start("zork1.z5")game_state = env.reset()# Reset/initialize the game.reward, done = 0, False while not done: # Ask the agent for a command.command = agent.act(game_state,reward, done) # Send the command to the game and get the new state.game_state, reward, done = env.step(command)</p>
<p>Game Observation</p>
<p>The game_state object contains useful information3 such as:</p>
<p>Feedback The interpreter's response to the previous command, i.e., any text displayed on screen.</p>
<p>Description</p>
<p>The description of the current room, i.e., the output of the look command.</p>
<p>Inventory The player's inventory, i.e., the output of the inventory command.</p>
<p>Location</p>
<p>The name of the current room.</p>
<p>Score The current score.</p>
<p>Game state information is fairly limited when it comes to existing games, whereas games generated with TextWorld can provide much more information if desired.Additional information includes:</p>
<p>Objective Text describing what the player must do to win the game.</p>
<p>Admissible Commands A list of commands that are guaranteed (i) to be understood by the interpreter and (ii) to affect the game state or to return information of the game state.Using this information in any way corresponds to playing a choice-based game rather than a parser-based one.</p>
<p>Intermediate Reward A numerical value representing how useful the last command was for solving the game (as described in Section 3.1.1).</p>
<p>Winning Policy A list of commands that guarantees winning the game starting from the current game state.</p>
<p>Related Work</p>
<p>Text-based games are hard to solve, even for humans, because they require language understanding, planning, and efficient exploration to a greater degree than perception and reflexes (like most Atari games).Nonetheless, a few researchers have tried different approaches that we report in Section 4.1.Since TextWorld is a new learning environment, we compare it to relevant frameworks in Section 4.2.</p>
<p>Relevant Models
Q(s, (w v , w o )) = Q(s, w o ) + Q(s, w v ) 2 (1) Q(s, w o ) = M LP o (s), Q(s, w v ) = M LP v (s) (2) s = LST M (o), where o = w 1 . . . w n (3)
They test their approach on 2 environments -Homeworld and Fantasyworld -using the Evennia toolkit4 .Homeworld is a small environment with 4 rooms, a vocabulary of 84 words, and 4 quests.Quests are also specified through text; for example, "
Q(s, a j ) = g(f s (o), f a (a j )), where o = w 1 . . . w n (4)
The DRRN model converges on both games when trained with the DQN algorithm with experience replay and Boltzmann exploration.It achieves optimal cumulative-reward on Saving John and a suboptimal but stable policy on Machine of Death.The authors test the DRRN trained on Machine of Death on state-action pairs paraphrased by participants.They show high correlation between the original and paraphrase Q(s, a, θ).</p>
<p>Note that neither the LSTM-DQN nor the DRRN conditions on previous actions or observations.This means that neither has the capacity to deal with partial observability.</p>
<p>Related work has been done to reduce the action space for parser-based games.Haroush et al. [2018] introduce the Action Elimination Network to estimate the probability of an action failing in a given scene.To achieve this, feedback from the game engine is also stored in the replay buffer in addition to the standard <observation, action, reward> triplets.The elimination module is trained with the stored quadruplets and assigns a score to each element in the large set of actions.During -greedy exploration, at the greedy step the agent is only allowed to consider the top-k actions, while during exploration, random actions are rejected with a predefined probability if their score is below a threshold.Fulda et al. [2017] tried to accomplish something similar by training word embeddings to be aware of verb-noun affordances.From that embedding, they manually select a group of verb-noun pairs for which they assume the vector emb(noun) − emb(verb) encodes the affordance relation.Using the average of such vectors gives them an "affordance vector" that can be used to project the embedding of new nouns to a region of the embedding space where relevant verbs should be found.Kostka et al. [2017] build an agent specifically targeting the domain of classic text-based games.They pre-train an LSTM language model on fantasy books that is used in their model to extract important keywords from scene descriptions.Furthermore, they collect a list of possible commands from game solutions and semi-automatically extract a large number of commands from online tutorials and decompiled game sources.Their system follows a modular design where each typical phase of text-adventure gameplay is modeled by a separate algorithm: command generation, battle mode, inventory management, exploration, restart.Their model generates commands by finding keywords in the scene text and cross-referencing the extracted command corpus to find plausible commands.</p>
<p>Frameworks</p>
<p>In the fields of AI in general and RL in particular, games have played a major role in advancing the state of the art.The well-known Arcade Learning Environment (ALE) [Bellemare et al., 2013], which provides an interface to Atari 2600 games, led to human-level videogame play by deep RL algorithms [Mnih et al., 2015].One limitation of ALE, however, is that it does not facilitate research in generalization, metalearning, and transfer learning because the individual games are too distinct from one other.Indeed, most research using ALE focuses on training a separate agent (with the same architecture) for each game [Machado et al., 2017].</p>
<p>ALE-style game collections exist in contrast to sandbox environments, in which games emerge from a set of underlying world mechanics.Perhaps the best known such environment is SimCity [Wright, 1996].Sandboxes generate a series of games that share components, and thereby naturally overcome some limitations of collections.Sandbox environments also allow for the programmatic construction of game variants whose difficulty can be tuned to form a curriculum.</p>
<p>The MazeBase environment [Sukhbaatar et al., 2015], possibly the sandbox framework most similar to TextWorld, enables researchers to generate two-dimensional grid-based games.Each grid point may contain a certain object type, such as an obstacle, a pushable block, a switch, or the goal.Maps are provided in an egocentric text-based representation to the player.One of the main motivations of MazeBase is to foster research in learning algorithms that reuse knowledge and policies acquired in related environments and games.Quests and the language in TextWorld are significantly more complex than in MazeBase.</p>
<p>The CommAI framework [Baroni et al., 2017] emphasizes the ability to generate curricula [Bengio et al., 2009], so that agents may learn incrementally from environments of increasing complexity.</p>
<p>Interaction with the environment takes place at the lower level of bits rather than simplified natural language as in TextWorld.</p>
<p>Recently, multimodal visuo-linguistic environments were introduced to study grounded language learning through RL.Chaplot et al. [2017] customized the VizDoom platform [Kempka et al., 2016] for language grounding experiments: objects with certain properties are placed in a room and the agent is instrcuted which object(s) to find.To perform similar experiments, Hermann et al. [2017] add a language instruction component to the DeepMind Lab 3D navigation environment [Beattie et al., 2016].</p>
<p>Benchmarks</p>
<p>In this section, we describe benchmark games that can be used through TextWorld to evaluate RL agents and algorithms.This set is preliminary; we plan to develop increasingly complex benchmarks in future work.</p>
<p>Curated List</p>
<p>Following Fulda et al. [2017], we compiled a list of 50 hand-authored text games to use as an evaluation set.Games designed for human players require, and can be used to measure, general capabilities like common-sense reasoning, planning, and language understanding.We manually analyzed all games in the set to ensure they are valid, with scores and interesting quests.From the original 50 proposed by Fulda et al. [2017], we replaced 20 which were either parodies of text-based games or did not provide scores.The information we collected for the games can be found in Appendix B.</p>
<p>We evaluate three baselines on the proposed list.BYU5 and Golovin6 are agents developed by Fulda et al. [2017] and Kostka et al. [2017], respectively, and are described in Section 4.1.The third is the Simple baseline, which consists in sampling uniformly a command from a predefined set7 at every step.</p>
<p>Each agent has 1000 steps to get a high score.If the agent loses the game, the game is reset and play resumes until the step limit is reached.If the agent wins (which never happens in practice), the game stops and the evaluation is done.The procedure proposed here is slightly different from that in Fulda et al. [2017], where they allow agents to play each game 1000 times for a maximum of 1000 steps each (i.e., a theoretical limit of a million interactions).Our main motivation for having a small "time" budget is that we are interested in measuring the generalization capabilities of agents.When using this evaluation, we assume the agents are already trained (on other similar games) or encompass some prior knowledge (which is the case for the three baselines).</p>
<p>Figure 9 reports the normalized score (i.e., maximum score achieved divided by the game's max possible score) for each baseline agent on the curated list.Unsurprisingly, agents achieve a rather low score on a few games and zero on many.Using the information we gathered during our analysis, we can make educated guesses as to why some baselines perform well on certain games.For instance, in Advent the player starts with 36 points, which explains why all three baselines have the same score.As another example, the Detective game can be solved with mostly navigational commands.This explains why the Simple agent performs relatively well, since the commands it samples from are mostly navigational.</p>
<p>Treasure Hunter</p>
<p>This benchmark is inspired by the task proposed in Parisotto and Salakhutdinov [2017], where the agent spawns in a randomly generated maze and must find a specific object.A colored "indicator" object near the agent's starting position determines which object the agent must retrieve.The agent earns a positive reward for retrieving the correct object or a negative reward for an incorrect object.There is a limited number of turns.</p>
<p>We adapted this task, which takes place in a 3D environment, to TextWorld.In our setting, the maze is a randomly generated map (see Section 3.2.1) of rooms.We randomly place the agent and two objects on the map.Then, we randomly select which object the agent should recover and mention it in the welcome message (our indicator).In navigating to and obtaining the desired object, the agent may have to complete other tasks like finding keys and unlocking doors.</p>
<p>The aim of this benchmark is to assess skills of affordance extraction (agents should determine verbnoun pairs that change the environment state); efficient navigation (agents should avoid revisiting irrelevant rooms); and memory(agents should remember which object to retrieve).</p>
<p>We define the difficulty levels for this benchmark as follows:</p>
<p>• 1 to 10: Mode: easy, #rooms = 5, quest length linearly increasing from 1 to 5;</p>
<p>• 11 to 20: Mode: medium, #rooms = 10, quest length linearly increasing from 2 to 10;</p>
<p>• 21 to 30: Mode: hard, #rooms = 20, quest length linearly increasing from 3 to 20;</p>
<p>where the modes are defined as</p>
<p>• Easy: Rooms are all empty except where the two objects are placed.Also, connections between rooms have no door;</p>
<p>• Medium: Rooms may be connected by closed doors.Container objects are added, and might need to be opened to find the object;</p>
<p>• Hard: Locked doors and containers are added which may need to be unlocked (and opened) to reach the object.Note that beyond the predefined difficulty levels, this benchmark can be simplified by letting the agent directly tap into the game state information (e.g., feedback, description, inventory and objective) or using a simpler grammar.</p>
<p>Random</p>
<p>Evaluation: One-life Game</p>
<p>One of our desiderata in building TextWorld is the ability to generate unseen games, so that we can train and evaluate an agent's performance on different game sets.To support our claim of this need, we test two state-of-the-art agents on a set of games generated by TextWorld, where the agents only see each game once and therefore cannot memorize them.</p>
<p>Specifically, for each difficulty level described above, we generate 100 games.We run each agent on these games for a maximum of 1000 steps.For each game, when the agent picks up either the right object or the wrong one, it receives +1 or -1 score, respectively, and the game terminates immediately; if the agent exhausts all 1000 steps without finding any object, it receives 0 score.</p>
<p>Evaluation results are provided in Table 1, where we compare a choice-based8 random agent (i.e., at each game-step the baseline model randomly selects one command from the list of admissible commands), the BYU agent and the Golovin agent.We report the average score for different difficulty levels and the average number of steps it took to finish the games (either win, lose, or exhaust).</p>
<p>Current Limitations</p>
<p>Complex Quest Generation We define a quest as a sequence of actions where each action depends of the outcomes of its predecessor (Section 3.2.2).This limits us to simple quests where each action is based on only the immediately previous action.Quests are rarely so straightforward in text adventure games, often being composed of multiple sub-quests.One method for generating more complex quests would be by treating a quest as a directed graph of dependent actions rather than a linear chain.</p>
<p>Time-based Events</p>
<p>In the current implementation, there is no support for triggering state changes without corresponding user actions.This would enable doors that automatically lock after a certain number of player steps or traps that trigger periodically.</p>
<p>Non-Player Characters (NPCs) NPCs are characters that are not controlled by the player and are capable of performing autonomous or reactive actions that alter the game state.NPC interaction is quite common in several text-based games, but is not currently supported by the game generator.</p>
<p>Multi-User Dungeon(MUD) Some text-based games allow multiple users to interact with the world at the same time.Users may need to cooperate with each other or to compete against each other, depending on the game.The framework currently supports only single-agent games.</p>
<p>Text Generation Using a CFG for text generation makes it difficult to ensure continuity between sentences.</p>
<p>Conclusion</p>
<p>We introduced TextWorld, a sandbox learning environment for the training and evaluation of RL agents on text-based games.After surveying the machine-learning challenges of text-based games, we cast them in the formalism of reinforcement learning.We described how the generative mechanisms of TextWorld can be used to work up towards the full complexity of hand-authored games and introduced a preliminary set of benchmark games for this purpose.We evaluated several baseline agents on this set and a curated list of hand-authored games that we analyzed.</p>
<p>In future work, we will develop more complex benchmark tasks and investigate novel approaches to representation learning, language understanding and generation, and RL for text-based games.</p>
<p>A Typical Text-based Obstacles</p>
<p>Language Language presents a number of difficulties Parser complexity (what a parser can and cannot handle), distracting in-game descriptions, sarcasm (e.g., If the game responds "Yeah right, that's totally going to work" when you enter a command, a solid understanding of sarcasm would prevent you from repeating the command), and fictional languages.</p>
<p>Maze One of the oldest text adventure obstacles, the most basic maze is a grid of rooms with similar names and/or descriptions.The player has to find the right path through the maze to reach a goal.Many text-based games take care to innovate/complicate mazes.Some innovations include:</p>
<p>• Mazes whose solution is determined stochastically, either on a play-through by playthrough basis or on a turn by turn basis.</p>
<p>• Mazes whose configuration changes due to a player action, as in Zork III [Infocom, 1982],</p>
<p>where the player can move some walls in the maze in order to solve it.</p>
<p>• Mazes that are not solvable by navigation: In Photopia, for example, the player is wearing a spacesuit while navigating the maze.If the player removes the suit, it is revealed that the player has a pair of wings.The player is then able to fly over the maze.</p>
<p>Clue Hunting Detective games require the player to hunt for clues that will tell them where to go next.In Sherlock, the game begins when the player receives a coded poem that directs them to Westminster Abbey, where they have to solve a number of riddle puzzles in order to find out where to find five different gems, each of which provides the player with further clues.A point of interest here is whether or not the player actually needs to get the hints in order to win the game, or whether the player could win the game if they already knew the contents of the hints.Sherlock [Bates, 1987] is an example of the former -some hints trigger crucial game state changes only once the player examines them.Other games mix state-changing hints with purely informative ones.For example, in Inhumane [Plotkin, 1985], a clue gives you directions through a maze which is still solvable if you don't find the clue.However, the end of the maze yields another clue.When you read this second clue, an important portion of the map becomes available to the player.</p>
<p>Treasure Hunting Some treasures double as clues (as is the case with the gems in Sherlock) while other treasures are necessary in order to unlock further treasures.In Infidel, for example, you are given the opportunity to take some gem clusters early in the game.Towards the end of the game, these clusters are used to unlock a treasure chest.[Berlyn, 1983] Trivia Many games base their puzzles off of trivia/world knowledge, which the player might need to know in order to solve a puzzle.Many puzzles in the Hitchhiker's Guide to the Galaxy game are easier to solve if you are familiar with the source material.For example, if you've read the book, you'd already know that a Babel Fish is able to decode any language in the universe.In the game, then, you'll know what to look for when confronted with an alien language [Adams and Meretzky, 1984].In The Enterprise Incidents, the player must solve a word puzzle whose solution, the word firefly, is not mentioned elsewhere in the game.[Desilets, 2002] In Goldilocks is a FOX, knowledge of fairy tales is required to solve puzzles involving magic beans, bears, and porridge.[Guest, 2002] Sherlock contains intricate riddle poems whose solutions are allusions to famous historical figures buried in Westminster Abbey.The player is also required decode a riddle poem and realize that references to "the conquest" and "the fire" refer to the Battle of Hastings and the Great Fire of London, respectively.Finally, the player is expected to know the dates for these two dates in order to subtract one from the other in order to solve a puzzle.[Bates, 1987] A number of modern textbased games (Curses, OMNIQUEST, All Quiet on the Library Front, Inhumane, and the later Zork games, to name a few) play off of established text adventure tropes, clichés, and catchphrases (common passwords across games include "XYZZY" and "Plugh" from Cave Adventure [Crowther and Woods, 1976], and magic spells learned in the Enchanter series carry over from game to game, although they have to be 'learned' in-game in order to be used).</p>
<p>Self-Maintenance Hunger, thirst, fatigue are the most common survival elements in text adventures.Players might have to keep space open in their inventory for food and drink, and will be prompted from time to time that they are getting hungry or thirsty, or that they might need to rest.Similar obstacles that pop up are: hypothermia (when swimming in Zork III [Infocom, 1982]), the classic lamp from Zork [Infocom, 1980] (which reappears and is innovated on in the sequels, where it can go out if you neglect to shake it or turn it back on), and torches from Infidel (which you are required to regularly dip in oil and light) [Berlyn, 1983].One complication of self-maintenance is whether the action is one time or continuous (in Infidel, for example, you need to gather food and drink, but once they're gathered, you have an unlimited amount).</p>
<p>Combat Combat can come in a variety of forms.At its most basic level, a player will be confronted with an enemy and can thwart it by typing kill enemy.To complicate things further, the player might not be equipped to kill the enemy with their bare hands, and must first acquire a weapon before kill enemy will return "enemy killed".One level higher, the player might have to attack enemy and dodge enemy or aim at X (where X = a weak spot you learn about elsewhere in the game) before killing the enemy.Higher levels include stochastic combat (where attacks may or may not land), optional combat (where the only solution to the fight is to avoid it) and puzzle combat.In Reverberations, for example, the player encounters an enemy in the cosmetics section of a department store.To win the fight, the player must spray the enemy with some nearby perfume.[Glasser, 1996] Time In Wishbringer, the player has a limited amount of time at the start of the game to deliver a note.The player must use this time to collect items and information that will be inaccessible once they've delivered the note.[Moriarty, 1985] In Sherlock, the player is given 48 hours of in-game time to solve the mystery.Certain locations and events are only accessible at certain times.[Bates, 1987] Mechanical Puzzle Some examples can be found in Infidel and Inhumane, where the player has to move objects around a room, leave objects in rooms, and prop open doors with wooden beams in order to avoid being caught in traps.[Berlyn, 1983] [Plotkin, 1985] Persistence In some games, Anchorhead for example, you must repeat talk to commands in order to get more information out of an NPC.[Gentry, 1998] Stochasticity Some examples of stochasticity include randomly generated maze configurations, randomly decided combat action outcomes, and randomly selected code-words for puzzles.</p>
<p>Text-based game players deal with stochasticity with the UNDO action or by saving often and especially right before taking any kind of risky action.</p>
<p>Cruelty Scale</p>
<p>B.1 Game Notes</p>
<p>Below are observations for some of the games we analyzed.</p>
<p>The Acorn Court Written as an Inform demo.Very short game with only one room.Contains one multi-part spatial puzzle.</p>
<p>Adventure First IF.Difficult, stochastically generated mazes, introduces made up words.We used the recompiled with Inform 6 (release 9) version which starts with an initial score of 36 points.</p>
<p>Anchorhead Heavily text based.Requires a combination of world knowledge and knowledge gleaned from in-game texts.When speaking with townspeople, you have to occasionally be persistent.Awareness of Lovecraftian cliches is helpful.</p>
<p>Balances Another Inform demo, features made up words and spells that the player must remember.Rare words used in the context of magic and fantasy are also used.</p>
<p>Deephome Medium length game with an in game library the player can consult in order to figure out the solutions to puzzles.</p>
<p>Dragon Adventure Short game written to introduce children to text based games.</p>
<p>Detective Poorly written game.Navigation isn't very logical (you can walk east into a room whose only exit is to the north).Gameplay is very simple -you only need to use navigation commands to reach the end of the game.</p>
<p>Enchanter Long game with a complex spell-casting system that requires player to memorize and record spells.</p>
<p>Goldilocks is a FOX! Requires trivia knowledge (you need to know about Jack and the Beanstalk in order to know what to do when someone offers you magic beans).Casual language with lots of pop cultural references.</p>
<p>Hitchhiker's Guide to the Galaxy Incredibly difficult to play.Several stochastic obstacles (A maze without a set solution, An object from a subset of objects is randomly made into a crucial object).Features many irrational objects and events, including objects showing up in your inventory without warning.Leans on world knowledge a bit as well, as when the navigation controls change to Port, Starboard, Aft, and Bow/Fore.Some puzzles are solvable if you are familiar with the books.Language is also sarcastic and, at times, deliberately misleads the player.</p>
<p>Infidel Many spatial puzzles.Draws on world knowledge (player must decode hieroglyphics and pictograms).Instruction manual contains the navigation instructions for finding the pyramid.Contains some potential one-way paths (you need to tie a rope to an object in one room in order to re-enter it).</p>
<p>Inhumane Parody of infidel but with a more straightforward, simpler map.Has a meta game where you try to nearly solve puzzles and then kill yourself.Some familiarity with Infidel makes playing it easier.Contains some clue-hunting that doubles as room unlocking, you can also find directions through a maze in the game.Contains a gag ending triggered by entering s instead of south or vice versa as your final command.There are mazes, but there are also some unconnected rooms that have identical names (T-Intersections appear in different areas).</p>
<p>All Quiet on the Library Front Relatively simple treasure hunt with minimal side-quests and Easter eggs.Contains some text-based game in-jokes that do not impact the gameplay, but might be distracting.</p>
<p>Lost pig Game text is written in "cave-person speak".Otherwise a simple pig hunt with some magic spell based puzzles.</p>
<p>The Lurking Horror Medium length game that requires you to understand which rooms are above which other rooms in order to guess the solution to a puzzle.</p>
<p>Spiritwrak Features a massive map with a working subway system.Uses an Enchanter-style spell system.</p>
<p>Trinity Long game with surreal imagery.Map made of several sub-worlds linked through a central hub.</p>
<p>Sherlock: The Riddle of the Crown Jewels Very dependent on world knowledge.Time based.Some word puzzles.Player can easily get stuck in an unwinnable state without knowing it, and is expected to have a familiarity with Victorian England and British history and culture and general.</p>
<p>Wishbringer Designed for beginner players.Features a hint system and an object that grants the player "wishes" that help them bypass puzzles.</p>
<p>Zork I Classic text-based game treasure hunt with dungeon theme.In addition to the treasure hunt component, player has to contend with a thief, combat with a troll, and navigate a maze.Knowledge about The Odyssey can help the player defeat a Cyclops.</p>
<p>Zork II Sequel to Zork I. Contains a maze modelled after a baseball diamond that confused some European players when first released.</p>
<p>Zork III Sequel to Zork II.High score can be achieved without winning the game.Game rewards the player for finding innovative solutions to problems rather than for solving puzzles, navigating to rooms, or retrieving objects.</p>
<p>C Inform 7</p>
<p>Inform refers to a domain-specific programming language and additional tooling for interactive fiction (i.e., text-based games).It is regarded as the most natural language-like programming language.It was originally created in 1993 by Graham Nelson and he later released Inform 7 (briefly known as Natural Inform).We decide to use Inform7 so we could leverage Inform's impressive parser that benefited from more than two decades of tweaks/fixes.Inform 7 source compiles to Inform 6 source, a weakly-typed multiple-inheritance traditional programming language, before compiling to Z or glulx code.See http://inform7.com/for more information on Inform 7. Also, here is a supplemental resource for some of the technical details: http://www.ifwiki.org/index.php/Inform_7_for_Programmers.</p>
<p>Figure 5 :
5
Figure 5: Comic strip showing a simple quest where the player has to find and eat the apple.</p>
<p>Figure 6 :
6
Figure 6: Comic strip showing backward quest generation for the quest open fridge / take apple from fridge / eat apple.The player is first placed in the kitchen.The apple is created and placed in the player's inventory at Step 2. The fridge is created at Step 3, and the apple placed within it.In the last step the fridge is closed.This becomes the game's starting state.</p>
<p>Figure 7 :
7
Figure 7: The same generated game with two themed grammars: house and basic.</p>
<p>Figure 8 :
8
Figure 8: Visualization of the game shown in Figure 7 using TextWorld's integrated game-state viewer.The winning sequence of commands for this game: go south / go south / take tiny grape from chipped shelf / go west / put tiny grape on dusty bench.</p>
<p>Figure 9 :
9
Figure 9: Normalized score for baselines evaluated on the curated list.Games where no score was obtained are omitted.</p>
<p>Figure 10 :
10
Figure 10: A generated Treasure Hunter game with difficulty 20.The green rectangle identifies the quest object, a passkey, which can be found on a rack in the washroom.The player, P, is currently two rooms away in the basement separated by two closed doors.</p>
<p>The text information perceived by the agent at a given turn t in the game is the agent's observation, o t ∈ Ω, which depends on the environment state and the previous command with probability O(o t |s t , c t−1 ).In other words, the function O selects from the environment state what information to show to the agent given the command entered.For instance, if the agent tries to open a chest, the observation returned by the environment might show that the chest is locked.Based on its actions, the agent receives reward signals r t = R(s t , a t ).The agent's goal is to maximize the expected discounted sum of rewards received E [ t γ t r t ].
Reward Function (R) Most text-based games (including those in our curated list) have a scoring mechanism wherebyplayers receive points for completing (sub)quests and reaching new locations. When available, thisscore can be used as a reward signal. Otherwise, one could define reward signals by assigninga positive reward if the agent completes the game. Intermediate rewards might also be inferredfrom the interpreter's feedback. Note that this feedback usually only contains information aboutthe results of individual commands (e.g., "I don't know this verb!", "This chest islocked!") rather than about overall progress.</p>
<p>Not you are sleepy now but you are hungry now" (which indicates that the player should obtain food but should not get into bed).Fantasyworld is much larger, with a vocabulary size of 1340, and has stochastic state transitions.The LSTM-DQN completes 100% of the quests in Homeworld and 96% of quests in Fantasyworld.The authors also perform transfer-learning experiments by defining Homeworld2, which is made by shuffling the rooms and paths from Homeworld.The LSTM-DQN is trained on Homeworld and the LSTM component is transferred to Homeworld2.The transferred agent learns faster than one without training on Homeworld.
He et al. [2015] introduce the Deep Reinforcement Relevance Network (DRRN) for tackling choice-
based text games.They evaluate the DRRN on a deterministic game called "Saving John" and a larger-scale stochastic game called "Machine of Death".These games have vocabulary sizes 1762 and 2258 and action vocabulary sizes of 171 and 419, respectively.The DRRN takes as input the observation o of the state s and action choices a j and computes a Q-value for each possible pair:</p>
<p>Table 1 :
1
BYUGolovin Model Avg.Score Avg.Steps Avg.Score Avg.Steps Avg.Score Avg.Steps Model performance on one-life treasure hunter tasks.
level 10.359.850.7585.180.7818.16level 5-0.1619.43-0.33988.72-0.35135.67level 10-0.1420.74-0.041000-0.05609.16level 110.3043.750.02992.100.04830.45level 150.2763.780.019980.03874.32level 200.2174.800.02962.270.04907.67level 210.3991.150.04952.780.09928.83level 250.26101.670.00974.140.04931.57level 300.26108.380.04927.370.04918.88</p>
<p>Sainbayar Sukhbaatar, Arthur Szlam, Gabriel Synnaeve, Soumith Chintala, and Rob Fergus.Mazebase: A sandbox for learning from games.arXiv preprint arXiv:1511.07401,2015.Richard S Sutton and Andrew G Barto.Reinforcement learning: An introduction.MIT press Cambridge, 2018.
Will Wright. SimCity. Erbe, 1996.</p>
<p>Table 2 :
2
The Cruelty Scale was designed by text-based game author Andrew Plotkin as a broad method of determining the difficulty of text-based games.The main element considered by the scale is whether or not the game can be made impossible to win, and how obvious (or not) this is made to the player.The scale has five ranks: Merciful games are impossible to die or get stuck in, Polite games warn you before you do something that can kill or trap you or otherwise render the game unwinnable.Tough games can kill the player or be rendered unwinnable, but they'll warn the player before this happens.Nasty games can be rendered unwinnable without warning, but there will be an indication that the game is now unwinnable after the fact.Cruel games can be rendered unwinnable, and there's no warning beforehand and no indication afterwards.Information we collected during our analysis of text-based games.
Length Text-based games vary wildly in length, from the solvable-in-twenty-moves inform demoAcorncourt to the 1000+ moves required to complete Anchorhead, text-based games havemany elements that impact their length. These include: Number of possible commands,
number of rooms, length of descriptive text, (Reverberations, for example, is solvable with a relatively low number of commands (roughly 50), but features long descriptive passages for almost every room and command) side quests and multiple endings, and game difficulty.However, text-based game review sites and discussion boards often divide games up into short demo type games, medium uncomplicated games, and long difficult games.</p>
<p>Typical text-based game interpreters disregard determiners.
Code and documentation can be found at http://aka.ms/textworld.
See the framework's documentation for a list of all information obtainable from the game state object.
http://www.evennia.com/
Implementation from https://github.com/danielricks/BYU-Agent-2016.
Implementation from https://github.com/Kostero/text_rpg_ai.
The exact set is: north, south, east, west, up, down, look, inventory, take all, drop and YES.
Because of the compositional properties of language, a random parser-based agent would perform poorly since most generated commands would not make sense. In this work we use a choice-based random agent. It is not directly comparable to the other two agents, but it can give a general idea how difficult the games are in different difficulty levels.
AcknowledgementWe thank Adam Ferguson and Marion Zepf for many helpful discussions on game analysis.We thank Alessandro Sordoni, Greg Yang, Philip Bachman, Ricky Loynd, Samira Ebrahimi Kahou, and Yoshua Bengio for helpful suggestions and comments on research perspectives.We also thank Ruo Yu Tao for his help with the visualization.B Curated List of text-based Games
The hitchhiker's guide to the galaxy. Douglas Adams, Steve Meretzky, 1984</p>
<p>Marco Baroni, Armand Joulin, Allan Jabri, Germàn Kruszewski, Angeliki Lazaridou, Klemen Simonic, Tomas Mikolov, Commai, arXiv:1701.08954Evaluating the first steps towards a useful general ai. 2017arXiv preprint</p>
<p>Bob Bates, Sherlock, The riddle of the crown jewels. 1987</p>
<p>Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, arXiv:1612.03801Víctor Valdés, Amir Sadik, et al. Deepmind lab. 2016arXiv preprint</p>
<p>The arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, J. Artif. Intell. Res.(JAIR). 472013</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningACM2009</p>
<p>. Michael Berlyn, Infidel, 1983</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.015402016Openai gym. arXiv preprint</p>
<p>Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov, arXiv:1706.07230Gated-attention architectures for task-oriented language grounding. 2017arXiv preprint</p>
<p>Three models for the description of language. N Chomsky, 10.1109/tit.1956.1056813IEEE Transactions on Information Theory. 23sep 1956</p>
<p>. William Crowther, Donald Woods Aventure, 1976</p>
<p>The enterprise incidents. Brian Desilets, 2002</p>
<p>Strips: A new approach to the application of theorem proving to problem solving. E Richard, Nils J Fikes, Nilsson, Artificial intelligence. 23-41971</p>
<p>What can you do with a rock? affordance extraction via word embeddings. Nancy Fulda, Daniel Ricks, Ben Murdoch, David Wingate, arXiv:1703.034292017arXiv preprint</p>
<p>. Stu Galley, Jim Lawrence, Seastalker, 1984</p>
<p>General game playing: Overview of the aaai competition. Michael Genesereth, Nathaniel Love, Barney Pell, AI magazine. 262622005</p>
<p>. Michel Gentry, Anchorhead, 1998</p>
<p>. Russell Glasser, Reverberations, 1996</p>
<p>Goldilocks is a fox!. J J Guest, 2002</p>
<p>Learning how not to act in text-based games. Matan Haroush, Tom Zahavy, Daniel J Mankowitz, Shie Mannor, International Conference on Learning Representations -Workshop. 2018</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, arXiv:1511.046362015arXiv preprint</p>
<p>The second dialog state tracking challenge. Matthew Henderson, Blaise Thomson, Jason D Williams, Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL). the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)2014</p>
<p>Grounded language learning in a simulated 3d world. Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojtek Czarnecki, Max Jaderberg, Denis Teplyashin, arXiv:1706.065512017arXiv preprint</p>
<p>. Infocom, I Zork, 1980. 1982</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack, Kaelbling Michael L Littman, Anthony R Cassandra, Artificial intelligence. 1011-21998</p>
<p>Vizdoom: A doom-based ai research platform for visual reinforcement learning. Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, Wojciech Jaśkowski, Computational Intelligence and Games (CIG), 2016 IEEE Conference on. IEEE2016</p>
<p>Text-based adventures of the golovin ai agent. Bartosz Kostka, Jaroslaw Kwiecieli, Jakub Kowalski, Pawel Rychlikowski, Computational Intelligence and Games (CIG), 2017 IEEE Conference on. IEEE2017</p>
<p>Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. C Marlos, Marc G Machado, Erik Bellemare, Joel Talvitie, Matthew Veness, Michael Hausknecht, Bowling, arXiv:1709.060092017arXiv preprint</p>
<p>Ceptre: A language for modeling generative interactive systems. Chris Martens, Eleventh Artificial Intelligence and Interactive Digital Entertainment Conference. 2015</p>
<p>A survey of exploration strategies in reinforcement learning. Roger Mcfarlane, 2003Pleaseinsert\ PrerenderUnicode{â Ĺij}intopreamble]cs526/roger.pdf</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Nature. 51875402015</p>
<p>. Brian Moriarty, Wishbringer, 1985</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, arXiv:1506.089412015arXiv preprint</p>
<p>Emilio Parisotto, Ruslan Salakhutdinov, arXiv:1702.08360Neural map: Structured memory for deep reinforcement learning. 2017arXiv preprint</p>
<p>The problem of the random walk. Karl Pearson, Andrew Plotkin. Inhumane. 723421867. 1905. 1985Nature</p>
<p>Markov Decision Processes: Discrete Stochastic Dynamic Programming. Martin L Puterman, 1994John Wiley &amp; Sons, IncNew York, NY, USA1st edition</p>
<p>Artificial intelligence: a modern approach. J Stuart, Peter Russell, Norvig, Malaysia; Pearson Education Limited. 2016</p>
<p>Expressionist: An Authoring Tool for In-Game Text Generation. James Ryan, Ethan Seither, Michael Mateas, Noah Wardrip-Fruin, 2016Springer International PublishingCham</p>            </div>
        </div>

    </div>
</body>
</html>
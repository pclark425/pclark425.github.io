<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1065 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1065</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1065</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-e89a4fe6e8286eccedd702216153f0f248adb151</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e89a4fe6e8286eccedd702216153f0f248adb151" target="_blank">Gibson Env: Real-World Perception for Embodied Agents</a></p>
                <p><strong>Paper Venue:</strong> 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates developing real-world perception for active agents, proposes Gibson Environment for this purpose, and showcases a set of perceptual tasks learned therein.</p>
                <p><strong>Paper Abstract:</strong> Developing visual perception models for active agents and sensorimotor control in the physical world are cumbersome as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we investigate developing real-world perception for active agents, propose Gibson Environment for this purpose, and showcase a set of perceptual tasks learned therein. Gibson is based upon virtualizing real spaces, rather than artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism "Goggles" enabling deploying the trained models in real-world without needing domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1065.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1065.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gibson</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gibson Virtual Environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A virtual environment built from scanned real-world indoor spaces (572 buildings, 1447 floors) that provides embodied agents with physics, visual streams (RGB, depth, normals, semantics), and a neural view-synthesis + domain-alignment mechanism (Goggles) to enable transfer to real-world perception.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>various (husky, ant, humanoid, others)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied simulated agents (examples: husky wheeled robot, ant legged agent, humanoid) are imported as URDF/MuJoCo models and trained with reinforcement learning (PPO) or other controllers; agents can receive visual (RGB, depth, normals, semantics) and non-visual proprioceptive inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Gibson Environment (database of real scanned indoor spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments are virtualizations of real indoor spaces reconstructed from RGB-D panoramas and 3D meshes; they include semantic and geometric complexity (furniture, clutter, variable room layouts), are physics-enabled via PyBullet (collision, gravity, friction), provide multiple sensory modalities, and span 572 buildings / 1447 floors covering 211k m^2. Rendering uses geometric point-cloud projection plus a neural filler network; Goggles (backward mapping) aligns real images to renderings for deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Specific Surface Area (SSA; ratio of inner mesh surface to convex-hull volume), Navigation Complexity (max A* path length / straight-line distance), scene-class entropy; database size (#spaces, total m^2). Reported numeric metrics: SSA = 1.38 (Gibson), Nav. Complexity = 5.98 (Gibson), #spaces = 572, total coverage = 211000 m^2, scene-class entropy = 3.72.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (SSA 1.38, Nav. Complexity 5.98 relative to synthetic baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of distinct environment instances and floors (#spaces = 572, floors = 1447), coverage area (211k m^2), scene-class entropy (3.72 out of max 5.90) indicating semantic diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (hundreds of distinct real-world spaces; high semantic diversity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Real-world transfer error (depth-estimation error in meters), image-similarity (SSIM, L1), distributional distances (MMD, CORAL); for embodied tasks: episodic reward, collision counts, policy-action-discrepancy (L2 of logits), success/achievement of navigation/stair tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Real-world depth estimation transfer errors: Gibson-trained -> real test = 0.92 m (Table 1); For view-synthesis/domain alignment: f(2s) <-> u(2t) gives SSIM 0.816 and L1 0.051 (Table 3). Rendering speed examples: 256x256 RGBD post-network f = 30.6 FPS (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper argues that realism (semantic complexity) and large variation (many scanned spaces) together improve transfer to real-world perception; Gibson shows higher SSA and nav complexity than synthetic datasets while providing many distinct spaces to train on. It also highlights a practical trade-off: richer perceptual input and realistic spaces increase training difficulty and sample requirements, but yield better generalization and transfer when paired with domain-alignment (Goggles).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Empirically favorable for transfer: Gibson (high complexity, high variation) produced lower real-world transfer depth error (0.92 m) than SUNCG (2.89 m) and Matterport3D (2.11 m) in the authors' experiments (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Train in multi-instance real-scanned virtual spaces (multi-environment training) with reinforcement learning (PPO) for embodied tasks; view-synthesis networks f and u are trained jointly on paired renderings and real images to form a joint domain (Goggles).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalization to real images: Goggles (u) + f reduces domain gap (MMD/CORAL) and yields depth-estimation depth error close to real-trained gold standard (gold: train/test on real = 0.86 m; best f->u combination = 0.91 m). Embodied generalization: agents trained in Gibson successfully generalize to shifted start/target positions (stair-climb tests) and Goggles reduced policy discrepancy between rendered and real inputs (policy-logit L2 discrepancy as low as 0.204 in best case).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Embodied tasks require hundreds to thousands of episodes: Local planning trained with PPO for 150 episodes (~300 iterations, 150k frames); distant navigation required ~1700 episodes (~680k frames) before global navigation emerged; stair-climb acquired skills after ~1700 episodes (~700k timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Using large numbers of real-scanned spaces yields higher semantic and navigational complexity (SSA=1.38, Nav. Complexity=5.98) and better transfer to real images than synthetic datasets. 2) Jointly training view-synthesis forward (f) and backward (u, Goggles) functions reduces domain gap (improved SSIM/L1, lower MMD/CORAL) and yields near-real performance on static recognition tasks. 3) Perceptual (visual) agents learn behaviors that generalize better (e.g., successful distant navigation, stair-climb generalization) but require more training steps (slower learning), indicating a trade-off between training speed and generalization when using richer perceptual inputs. 4) Gibson currently lacks dynamic actors and full material properties, which limits some forms of physics realism and manipulation transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_notes</strong></td>
                            <td>Environment-centric entity capturing complexity/variation metrics and their relationships to transfer and embodied learning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1065.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1065.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Husky (perceptual vs non-perceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Husky wheeled robot agent (perceptual and non-perceptual variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A wheeled simulated robot used in experiments for local planning/obstacle avoidance and distant visual navigation; variants either receive visual inputs (RGB/depth) plus proprioception or only non-visual proprioceptive state, trained with PPO for navigation tasks within Gibson.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>husky (perceptual and non-perceptual variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated wheeled agent (husky UGV model) controlled with a discrete 4-D action space (forward/backward/left/right) trained with Proximal Policy Optimization (PPO). Perceptual variant receives continuous visual stream (depth and/or RGB); non-perceptual variant gets only non-visual proprioceptive sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual robotic agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Gibson Environment — local planning & distant visual navigation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Agents are randomly placed in cluttered, real-scanned indoor spaces; local planning targets are nearby and require obstacle avoidance and short-term planning; distant navigation targets are far-fixed requiring contextual mapping and longer-range planning across complex layouts. Environments include furniture, narrow passages, and realistic scene geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task complexity characterized by navigation distance relative to straight-line distance, clutter and occlusion from real-scanned scenes; training difficulty measured empirically by emergence time (episodes/frames) and reward learning curves. Experiments used collision penalties to emphasize obstacle avoidance.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>local planning = medium-high; distant navigation = high (requires global contextual mapping across complex layouts); Gibson nav complexity overall high (Nav. Complexity 5.98).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation arises from sampling many different Gibson spaces and randomized initial positions and target locations; the training runs used diverse scenes from the Gibson database (hundreds of spaces) and randomized placements per episode.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (agents trained across diverse real-scanned environments and random initializations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Episodic reward (mean reward curves), collisions (penalty counts), success in achieving target (implied), and policy-action-discrepancy (L2 distance of output action logits when inputs differ).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Local planning: perceptual agent achieved higher average reward than non-perceptual over 150 episodes (qualitative improvement; no absolute numeric reward provided). Distant navigation: global navigation behavior emerges after ~1700 episodes (~680k frames) and only the perceptual agent succeeded in the task. Policy-discrepancy metrics (evaluating Goggles): training on f(Is) and testing on u(It) yields logit-L2 discrepancy = 0.204 (best), f(Is)->It gives 0.300, Is->It gives 0.242.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Perceptual input (increased observation complexity) made learning slower but enabled successful solutions in high complexity navigation tasks across varied environments; the paper reports a trade-off where visual/perceptual agents learn slower but generalize and succeed in tasks (distant navigation) that non-perceptual agents cannot.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Successful only for perceptual agents: distant navigation succeeded (emergence after ~1700 episodes) when trained on varied real-scanned environments; non-perceptual failed to accomplish distant navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multi-environment training within Gibson using Proximal Policy Optimization (PPO); some experiments used integrated ideal controllers or PID/nonholonomic controllers for control abstraction, but PPO was used to learn end-to-end policies for the husky experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Perceptual husky generalized to unseen initial placements and complex layouts: only perceptual agent could accomplish distant navigation; Goggles reduced policy discrepancy between rendered and real images (best test discrepancy 0.204), supporting transfer robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Local planning: trained for 150 episodes (~300 iterations, 150k frames). Distant navigation: ~1700 episodes (~680k frames) needed for global navigation to emerge.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Visual inputs enabled emergence of complex behaviors (global navigation) that non-visual agents failed to achieve. 2) Perceptual agents required substantially more interactions (slower learning) but achieved better task success and transfer when paired with domain-alignment (Goggles). 3) Goggles reduced policy discrepancies between simulated renderings and real images, improving prospects for deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_notes</strong></td>
                            <td>Husky experiments demonstrate trade-offs between perceptual richness (complex observations) and sample complexity, and show benefit from environment variation + domain alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1065.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1065.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ant (stair-climb)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ant legged agent (stair-climb task)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated legged agent (OpenAI Roboschool 'ant') trained to descend stairways plausibly using visual inputs; compared perceptual vs non-perceptual variants to evaluate locomotion with complex dynamics in real-scanned environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ant (perceptual and non-perceptual variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated legged agent with an 8-dimensional continuous torque action space; trained with reinforcement learning to control complex dynamics and locomotion on stairways, receiving visual observations (perceptual) or only proprioceptive inputs (non-perceptual).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual robotic agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Gibson Environment — stair-climb task</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Agent placed at top of a realistic, scanned stairway and must reach bottom without flipping; environment contains realistic stair geometry reconstructed from scanning, with limited material properties but accurate geometry for locomotion interactions simulated via PyBullet.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Dynamics complexity: legged control with 8-D continuous action torques, locomotion over stairs (requiring balance and contact-rich interactions). Task complexity measured by episodes/time to acquire skill and generalization to shifted initial/target positions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (complex dynamics and contact-rich locomotion over stairs)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation introduced by random slight shifts of initial and target locations at test time; training used the scanned stair environment (single stair instance in the reported experiment) but tests included perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (test-time randomized initial/target positions; training on same stair instance but with input perturbations at test)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Successful descent without flipping, episodic reward, relative performance vs non-perceptual agent (percentage improvement at test), sample episodes to acquire skill.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Perceptual agent learned slower but at test outperformed non-perceptual agent by 70% (relative performance improvement). Skill acquisition occurred after ~1700 episodes (~700k timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The presence of visual perception (increasing observation complexity) led to slower training but substantially better robustness/generalization to perturbed initial conditions (trade-off: slower learning vs higher generalization under environment variation).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Perceptual agent (high task complexity with moderate variation at test) outperformed non-perceptual by 70% at test, indicating better generalization under environment variation despite increased learning cost.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>End-to-end RL (PPO) on continuous control (8-D torques) in a single scanned stair environment with test-time perturbations to initial/target positions; comparison against a non-visual baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Perceptual agent generalized better to shifted initial/target positions at test and significantly outperformed the non-perceptual baseline (70% better), despite slower training.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Skill emerged after ~1700 episodes (~700k timesteps), indicating substantial sample complexity for perceptual legged locomotion over stairs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Visual perception is beneficial for complex locomotion tasks in realistic geometry: perceptual ant generalized better to variations in start/goal position. 2) There is a cost in sample efficiency: perceptual learning was slower but yielded superior generalization (70% better than non-visual). 3) Demonstrates that embodiment + realistic environment geometry in Gibson can enable learning of contact-rich locomotion behaviors that generalize under modest environmental variation.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_notes</strong></td>
                            <td>Stair-climb highlights trade-off between observation complexity (vision) and training speed versus robustness to variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergence of locomotion behaviours in rich environments <em>(Rating: 2)</em></li>
                <li>Target-driven visual navigation in indoor scenes using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>MINOS: Multimodal indoor simulator for navigation in complex environments <em>(Rating: 2)</em></li>
                <li>AirSim: High-fidelity visual and physical simulation for autonomous vehicles <em>(Rating: 1)</em></li>
                <li>VR Goggles for Robots: Real-to-Sim domain adaptation for visual control <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1065",
    "paper_id": "paper-e89a4fe6e8286eccedd702216153f0f248adb151",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Gibson",
            "name_full": "Gibson Virtual Environment",
            "brief_description": "A virtual environment built from scanned real-world indoor spaces (572 buildings, 1447 floors) that provides embodied agents with physics, visual streams (RGB, depth, normals, semantics), and a neural view-synthesis + domain-alignment mechanism (Goggles) to enable transfer to real-world perception.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "various (husky, ant, humanoid, others)",
            "agent_description": "Embodied simulated agents (examples: husky wheeled robot, ant legged agent, humanoid) are imported as URDF/MuJoCo models and trained with reinforcement learning (PPO) or other controllers; agents can receive visual (RGB, depth, normals, semantics) and non-visual proprioceptive inputs.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "Gibson Environment (database of real scanned indoor spaces)",
            "environment_description": "Environments are virtualizations of real indoor spaces reconstructed from RGB-D panoramas and 3D meshes; they include semantic and geometric complexity (furniture, clutter, variable room layouts), are physics-enabled via PyBullet (collision, gravity, friction), provide multiple sensory modalities, and span 572 buildings / 1447 floors covering 211k m^2. Rendering uses geometric point-cloud projection plus a neural filler network; Goggles (backward mapping) aligns real images to renderings for deployment.",
            "complexity_measure": "Specific Surface Area (SSA; ratio of inner mesh surface to convex-hull volume), Navigation Complexity (max A* path length / straight-line distance), scene-class entropy; database size (#spaces, total m^2). Reported numeric metrics: SSA = 1.38 (Gibson), Nav. Complexity = 5.98 (Gibson), #spaces = 572, total coverage = 211000 m^2, scene-class entropy = 3.72.",
            "complexity_level": "high (SSA 1.38, Nav. Complexity 5.98 relative to synthetic baselines)",
            "variation_measure": "Number of distinct environment instances and floors (#spaces = 572, floors = 1447), coverage area (211k m^2), scene-class entropy (3.72 out of max 5.90) indicating semantic diversity.",
            "variation_level": "high (hundreds of distinct real-world spaces; high semantic diversity)",
            "performance_metric": "Real-world transfer error (depth-estimation error in meters), image-similarity (SSIM, L1), distributional distances (MMD, CORAL); for embodied tasks: episodic reward, collision counts, policy-action-discrepancy (L2 of logits), success/achievement of navigation/stair tasks.",
            "performance_value": "Real-world depth estimation transfer errors: Gibson-trained -&gt; real test = 0.92 m (Table 1); For view-synthesis/domain alignment: f(2s) &lt;-&gt; u(2t) gives SSIM 0.816 and L1 0.051 (Table 3). Rendering speed examples: 256x256 RGBD post-network f = 30.6 FPS (Table 2).",
            "complexity_variation_relationship": "The paper argues that realism (semantic complexity) and large variation (many scanned spaces) together improve transfer to real-world perception; Gibson shows higher SSA and nav complexity than synthetic datasets while providing many distinct spaces to train on. It also highlights a practical trade-off: richer perceptual input and realistic spaces increase training difficulty and sample requirements, but yield better generalization and transfer when paired with domain-alignment (Goggles).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Empirically favorable for transfer: Gibson (high complexity, high variation) produced lower real-world transfer depth error (0.92 m) than SUNCG (2.89 m) and Matterport3D (2.11 m) in the authors' experiments (Table 1).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Train in multi-instance real-scanned virtual spaces (multi-environment training) with reinforcement learning (PPO) for embodied tasks; view-synthesis networks f and u are trained jointly on paired renderings and real images to form a joint domain (Goggles).",
            "generalization_tested": true,
            "generalization_results": "Generalization to real images: Goggles (u) + f reduces domain gap (MMD/CORAL) and yields depth-estimation depth error close to real-trained gold standard (gold: train/test on real = 0.86 m; best f-&gt;u combination = 0.91 m). Embodied generalization: agents trained in Gibson successfully generalize to shifted start/target positions (stair-climb tests) and Goggles reduced policy discrepancy between rendered and real inputs (policy-logit L2 discrepancy as low as 0.204 in best case).",
            "sample_efficiency": "Embodied tasks require hundreds to thousands of episodes: Local planning trained with PPO for 150 episodes (~300 iterations, 150k frames); distant navigation required ~1700 episodes (~680k frames) before global navigation emerged; stair-climb acquired skills after ~1700 episodes (~700k timesteps).",
            "key_findings": "1) Using large numbers of real-scanned spaces yields higher semantic and navigational complexity (SSA=1.38, Nav. Complexity=5.98) and better transfer to real images than synthetic datasets. 2) Jointly training view-synthesis forward (f) and backward (u, Goggles) functions reduces domain gap (improved SSIM/L1, lower MMD/CORAL) and yields near-real performance on static recognition tasks. 3) Perceptual (visual) agents learn behaviors that generalize better (e.g., successful distant navigation, stair-climb generalization) but require more training steps (slower learning), indicating a trade-off between training speed and generalization when using richer perceptual inputs. 4) Gibson currently lacks dynamic actors and full material properties, which limits some forms of physics realism and manipulation transfer.",
            "brief_notes": "Environment-centric entity capturing complexity/variation metrics and their relationships to transfer and embodied learning performance.",
            "uuid": "e1065.0",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Husky (perceptual vs non-perceptual)",
            "name_full": "Husky wheeled robot agent (perceptual and non-perceptual variants)",
            "brief_description": "A wheeled simulated robot used in experiments for local planning/obstacle avoidance and distant visual navigation; variants either receive visual inputs (RGB/depth) plus proprioception or only non-visual proprioceptive state, trained with PPO for navigation tasks within Gibson.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "husky (perceptual and non-perceptual variants)",
            "agent_description": "Simulated wheeled agent (husky UGV model) controlled with a discrete 4-D action space (forward/backward/left/right) trained with Proximal Policy Optimization (PPO). Perceptual variant receives continuous visual stream (depth and/or RGB); non-perceptual variant gets only non-visual proprioceptive sensors.",
            "agent_type": "simulated agent / virtual robotic agent",
            "environment_name": "Gibson Environment — local planning & distant visual navigation tasks",
            "environment_description": "Agents are randomly placed in cluttered, real-scanned indoor spaces; local planning targets are nearby and require obstacle avoidance and short-term planning; distant navigation targets are far-fixed requiring contextual mapping and longer-range planning across complex layouts. Environments include furniture, narrow passages, and realistic scene geometry.",
            "complexity_measure": "Task complexity characterized by navigation distance relative to straight-line distance, clutter and occlusion from real-scanned scenes; training difficulty measured empirically by emergence time (episodes/frames) and reward learning curves. Experiments used collision penalties to emphasize obstacle avoidance.",
            "complexity_level": "local planning = medium-high; distant navigation = high (requires global contextual mapping across complex layouts); Gibson nav complexity overall high (Nav. Complexity 5.98).",
            "variation_measure": "Variation arises from sampling many different Gibson spaces and randomized initial positions and target locations; the training runs used diverse scenes from the Gibson database (hundreds of spaces) and randomized placements per episode.",
            "variation_level": "high (agents trained across diverse real-scanned environments and random initializations)",
            "performance_metric": "Episodic reward (mean reward curves), collisions (penalty counts), success in achieving target (implied), and policy-action-discrepancy (L2 distance of output action logits when inputs differ).",
            "performance_value": "Local planning: perceptual agent achieved higher average reward than non-perceptual over 150 episodes (qualitative improvement; no absolute numeric reward provided). Distant navigation: global navigation behavior emerges after ~1700 episodes (~680k frames) and only the perceptual agent succeeded in the task. Policy-discrepancy metrics (evaluating Goggles): training on f(Is) and testing on u(It) yields logit-L2 discrepancy = 0.204 (best), f(Is)-&gt;It gives 0.300, Is-&gt;It gives 0.242.",
            "complexity_variation_relationship": "Perceptual input (increased observation complexity) made learning slower but enabled successful solutions in high complexity navigation tasks across varied environments; the paper reports a trade-off where visual/perceptual agents learn slower but generalize and succeed in tasks (distant navigation) that non-perceptual agents cannot.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Successful only for perceptual agents: distant navigation succeeded (emergence after ~1700 episodes) when trained on varied real-scanned environments; non-perceptual failed to accomplish distant navigation.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Multi-environment training within Gibson using Proximal Policy Optimization (PPO); some experiments used integrated ideal controllers or PID/nonholonomic controllers for control abstraction, but PPO was used to learn end-to-end policies for the husky experiments.",
            "generalization_tested": true,
            "generalization_results": "Perceptual husky generalized to unseen initial placements and complex layouts: only perceptual agent could accomplish distant navigation; Goggles reduced policy discrepancy between rendered and real images (best test discrepancy 0.204), supporting transfer robustness.",
            "sample_efficiency": "Local planning: trained for 150 episodes (~300 iterations, 150k frames). Distant navigation: ~1700 episodes (~680k frames) needed for global navigation to emerge.",
            "key_findings": "1) Visual inputs enabled emergence of complex behaviors (global navigation) that non-visual agents failed to achieve. 2) Perceptual agents required substantially more interactions (slower learning) but achieved better task success and transfer when paired with domain-alignment (Goggles). 3) Goggles reduced policy discrepancies between simulated renderings and real images, improving prospects for deployment.",
            "brief_notes": "Husky experiments demonstrate trade-offs between perceptual richness (complex observations) and sample complexity, and show benefit from environment variation + domain alignment.",
            "uuid": "e1065.1",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Ant (stair-climb)",
            "name_full": "Ant legged agent (stair-climb task)",
            "brief_description": "A simulated legged agent (OpenAI Roboschool 'ant') trained to descend stairways plausibly using visual inputs; compared perceptual vs non-perceptual variants to evaluate locomotion with complex dynamics in real-scanned environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ant (perceptual and non-perceptual variants)",
            "agent_description": "Simulated legged agent with an 8-dimensional continuous torque action space; trained with reinforcement learning to control complex dynamics and locomotion on stairways, receiving visual observations (perceptual) or only proprioceptive inputs (non-perceptual).",
            "agent_type": "simulated agent / virtual robotic agent",
            "environment_name": "Gibson Environment — stair-climb task",
            "environment_description": "Agent placed at top of a realistic, scanned stairway and must reach bottom without flipping; environment contains realistic stair geometry reconstructed from scanning, with limited material properties but accurate geometry for locomotion interactions simulated via PyBullet.",
            "complexity_measure": "Dynamics complexity: legged control with 8-D continuous action torques, locomotion over stairs (requiring balance and contact-rich interactions). Task complexity measured by episodes/time to acquire skill and generalization to shifted initial/target positions.",
            "complexity_level": "high (complex dynamics and contact-rich locomotion over stairs)",
            "variation_measure": "Variation introduced by random slight shifts of initial and target locations at test time; training used the scanned stair environment (single stair instance in the reported experiment) but tests included perturbations.",
            "variation_level": "medium (test-time randomized initial/target positions; training on same stair instance but with input perturbations at test)",
            "performance_metric": "Successful descent without flipping, episodic reward, relative performance vs non-perceptual agent (percentage improvement at test), sample episodes to acquire skill.",
            "performance_value": "Perceptual agent learned slower but at test outperformed non-perceptual agent by 70% (relative performance improvement). Skill acquisition occurred after ~1700 episodes (~700k timesteps).",
            "complexity_variation_relationship": "The presence of visual perception (increasing observation complexity) led to slower training but substantially better robustness/generalization to perturbed initial conditions (trade-off: slower learning vs higher generalization under environment variation).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Perceptual agent (high task complexity with moderate variation at test) outperformed non-perceptual by 70% at test, indicating better generalization under environment variation despite increased learning cost.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "End-to-end RL (PPO) on continuous control (8-D torques) in a single scanned stair environment with test-time perturbations to initial/target positions; comparison against a non-visual baseline.",
            "generalization_tested": true,
            "generalization_results": "Perceptual agent generalized better to shifted initial/target positions at test and significantly outperformed the non-perceptual baseline (70% better), despite slower training.",
            "sample_efficiency": "Skill emerged after ~1700 episodes (~700k timesteps), indicating substantial sample complexity for perceptual legged locomotion over stairs.",
            "key_findings": "1) Visual perception is beneficial for complex locomotion tasks in realistic geometry: perceptual ant generalized better to variations in start/goal position. 2) There is a cost in sample efficiency: perceptual learning was slower but yielded superior generalization (70% better than non-visual). 3) Demonstrates that embodiment + realistic environment geometry in Gibson can enable learning of contact-rich locomotion behaviors that generalize under modest environmental variation.",
            "brief_notes": "Stair-climb highlights trade-off between observation complexity (vision) and training speed versus robustness to variation.",
            "uuid": "e1065.2",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergence of locomotion behaviours in rich environments",
            "rating": 2
        },
        {
            "paper_title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "MINOS: Multimodal indoor simulator for navigation in complex environments",
            "rating": 2
        },
        {
            "paper_title": "AirSim: High-fidelity visual and physical simulation for autonomous vehicles",
            "rating": 1
        },
        {
            "paper_title": "VR Goggles for Robots: Real-to-Sim domain adaptation for visual control",
            "rating": 2
        }
    ],
    "cost": 0.014734499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Gibson Env: Real-World Perception for Embodied Agents</h1>
<p>Fei Xia† Amir R. Zamir†,2 Zhi-Yang He† Alexander Sax1 Jitendra Malik2 Silvio Savarese1
1 Stanford University 2 University of California, Berkeley
http://gibson.vision/</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p><strong>Figure 1:</strong> Two agents in <em>Gibson Environment</em> for real-world perception. The agent is active, embodied, and subject to constraints of physics and space (a,b). It receives a constant stream of visual observations as if it had an on-board camera (c). It can also receive additional modalities, e.g., depth, semantic labels, or normals (d,e,f). The visual observations are from real-world rather than an artificially designed space.</p>
<h2>Abstract</h2>
<p><em>Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment</em> 1 <em>for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, "Goggles", enabling deploying the trained models in real-world without needing domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.</em></p>
<h2>1. Introduction</h2>
<p>We would like our robotic agents to have compound perceptual and physical capabilities: a drone that autonomously surveys buildings, a robot that rapidly finds victims in a disaster area, or one that safely delivers our packages, just to name a few. Apart from the application perspective, the findings supportive of a close relationship between visual perception and being physically active are prevalent on various fronts: evolutionary and computational biologists have hypothesized a key role for intermixing perception and locomotion in development of complex behaviors and species [65, 95, 24]; neuroscientists have extensively argued for a hand in hand relationship between developing perception and being active [87, 45]; pioneer roboticists have similarly advocated entanglement of the two [15, 16]. This all calls for developing principled perception models specifically with active agents in mind.</p>
<p>By perceptual active agent, we are generally referring to an agent that receives a visual observation from the environment and accordingly effectuates a set of actions which can lead a physical change in the environment (~manipulation) and/or the agent's own particulars (~locomotion). Developing such perceptual agents entails the questions of <em>how</em> and <em>where</em> to do so.</p>
<p><sup>1</sup>Named after <strong>JJ Gibson</strong>, the author of <em>Ecological Approach to Visual Perception</em>, 1979. "We must perceive in order to move, but we must also move in order to perceive" – JJ Gibson [38]</p>
<p><sup>*</sup>Authors contributed equally.</p>
<p>On the how front, the problem has been the focus of a broad set of topics for decades, from classical control $[68,13,53]$ to more recently sensorimotor control [35, $58,59,5]$, reinforcement learning $[6,77,78]$, acting by prediction [30], imitation learning [25], and other concepts $[63,106,97,96]$. These methods generally assume a sensory observation from environment is given and subsequently devise one or a series of actions to perform a task.</p>
<p>A key question is where this sensory observation should come from. Conventional computer vision datasets [34, 28, 61] are passive and static, and consequently, lacking for this purpose. Learning in the physical world, though not impossible [40, 7, 59, 67], is not the ideal scenario. It would bound the learning speed to real-time, incur substantial logistical cost if massively parallelized, and discount rare yet important occurrences. Robots are also often costly and fragile. This has led to popularity of learning-in-simulation with a fruitful history going back to decades ago $[68,56,17]$ and remaining an active topic today. The primary questions around this option are naturally around generalization from simulation to real-world: how to ensure I. the semantic complexity of the simulated environment is a good enough replica of the intricate real-world, and II. the rendered visual observation in simulation is close enough to what a camera in real-world would capture (photorealism).</p>
<p>We attempt to address some of these concerns and propose Gibson, a virtual environment for training and testing real-world perceptual agents. An arbitrary agent, e.g. a humanoid or a car (see Fig. 1) can be imported, it will be then embodied (i.e. contained by its physical body) and placed in a large and diverse set of real spaces. The agent is subject to constraints of space and physics (e.g. collision, gravity) through integration with a physics engine, but can freely perform any mobility task as long as the constraints are satisfied. Gibson provides a stream of visual observation from arbitrary viewpoints as if the agent had an on-board camera. Our novel rendering engine operates notably faster than real-time and works given sparsely scanned spaces, e.g. 1 panorama per 5-10 $m^{2}$.</p>
<p>The main goal of Gibson is to facilitate transferring the models trained therein to real-world, i.e. holding up the results when the stream of images switches to come from a real camera rather than Gibson's rendering engine. This is done by: first, resorting to the world itself to represent its own semantic complexity [85, 15] and forming the environment based off of scanned real spaces, rather than artificial ones [88, 51, 49]. Second, embedding a mechanism to dissolve differences between Gibson's renderings and what a real camera would produce. As a result, an image coming from a real camera vs the corresponding one from Gibson's rendering engine look statistically indistinguishable to the agent, and hence, closing the (perceptual) gap. This is done by employing a neural network based rendering approach which jointly trains a network for making render-
ings look more like real images (forward function) as well as a network which makes real images look like renderings (backward function). The two functions are trained to produce equal outputs, thus bridging the two domains. The backward function resembles deployment-time corrective glasses for the agent, so we call it Goggles.</p>
<p>Finally, we showcase a set of active perceptual tasks (local planning for obstacle avoidance, distant navigation, visual stair climbing) learned in Gibson. Our focus in this paper is on the vision aspect only. The statements should not be viewed to be necessarily generalizable to other aspects of learning in virtual environments, e.g. physics simulation.</p>
<p>Gibson Environment and our software stack are available to public for research purposes at http://gibson.vision/. Visualizations of Gibson space database can be seen here.</p>
<h2>2. Related Work</h2>
<p>Active Agents and Control: As discussed in Sec.1, operating and controlling active agents have been the focus of a massive body of work. A large portion of them are nonlearning based [53, 29, 52], while recent methods have attempted learning visuomotor policies end-to-end [106, 58] taking advantage of imitation learning [73], reinforcement learning $[78,44,77,44,5,6]$, acting by prediction [30] or self-supervision [40, 67, 30, 66, 46]. These methods are all potential users of (ours and other) virtual environments.</p>
<p>Virtual Environments for Learning: Conventionally vision is learned in static datasets [34, 28, 61] which are of limited use when it comes to active agent. Similarly, video datasets [57, 70, 101] are pre-recorded and thus passive. Virtual environments have been a remedy for this, classically [68] and today [106, 36, 31, 83, 47, 41, 11, 9, 72, 8, 98]. Computer games, e.g. Minecraft [49], Doom [51] and GTA5 [69] have been adapted for training and benchmarking learning algorithms. While these simulators are deemed reasonably effective for certain planning or control tasks, the majority of them are of limited use for perception and suffer from oversimplification of the visual world due to using synthetic underlying databases and/or rendering pipeline deficiencies. Gibson addresses some of such concerns by striving to target perception in real-world via using real spaces as its base, a custom neural view synthesizer, and a baked-in adaption mechanism, Goggles.</p>
<h2>Domain Adaptation and Transferring to Real-World:</h2>
<p>With popularity of simulators, different approaches for domain adaption for transferring the results to real world has been investigated [12, 27, 89, 75, 93, 99], e.g. via domain randomization [74, 93] or forming joint spaces [81]. Our approach is relatively simple and makes use of the fact that, in our case, large amounts of paired data for target-source domains are available enabling us to train forward and backward models to form a joint space. This makes us a bakedin mechanism in our environment for adaption, minimizing the need for additional and custom adaptation.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our view synthesis pipeline. The input is a sparse set of RGB-D Panoramas with their global camera pose. (a,b) Each RGB-D panorama is projected to the target camera pose and rendered. (b) View Selection determines from which panorama each target pixel should be picked, favoring panoramas that provide denser pixels for each region. (c) The pixels are selected and local gaps are interpolated with bilinear sampling. (d) A neural network (f) takes in the interpolated image and fills in the dis-occluded regions and fixes artifacts.</p>
<p>View Synthesis and Image-Based Rendering: Rendering novel views of objects and scenes is one of the classic problems in vision and graphics [80, 84, 91, 23, 60]. A number of relevantly recent methods have employed neural networks in a rendering pipeline, e.g. via an encoder-decoder like architecture that directly renders pixels [32, 55, 92] or predicts a flow map for pixels [105]. When some from of 3D information, e.g. depth, is available in the input [42, 62, 20, 82], the pipeline can make use of geometric approaches to be more robust to large viewpoint changes and implausible deformations. Further, when multiple images in the input are available, a smart selection mechanism (often referred to as Image Based Rendering) can help with lighting inconsistencies and handling more difficult and non-lambertian surfaces [43, 64, 94], compared to rendering from a textured mesh or as such entirely geometric methods. Our approach is a combination of above in which we geometrically render a base image for the target view, but resort to a neural network to correct artifacts and fill in the dis-occluded areas, along with jointly training a backward function for mapping real images onto the synthesized one.</p>
<h2>3 Real-World Perceptual Environment</h2>
<p>Gibson includes a neural network based view synthesis (described in Sec. 3.2) and a physics engine (described in Sec. 3.3). The underlying scene database and integrated agents are explained in sections 3.1 and 3.3, respectively.</p>
<h3>3.1 Gibson Database of Spaces</h3>
<p>Gibson's underlying database of spaces includes 572 full buildings composed of 1447 floors covering a total area of 211k m². Each space has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. The base format of the data is similar to 2D-3D-Semantics dataset [10], but is more diverse and includes 2 orders of magnitude more spaces. Various 2D, 3D, and video visualizations of each space in Gibson database can be accessed here. This dataset is released in asset files of Gibson<sup>2</sup>.</p>
<p>We have also integrated 2D-3D-Semantics dataset [10] and Matterport3D [18] in Gibson for optional use.</p>
<h3>3.2 View Synthesis</h3>
<p>Our view synthesis module takes a sparse set of RGB-D panoramas in the input and renders a panorama from an arbitrary novel viewpoint. A 'view' is a 6D camera pose of x, y, z. Cartesian coordinates and roll, pitch, yaw angles, denoted as θ, φ, γ. An overview of our view synthesis pipeline can be seen in Fig. 2. It is composed of a geometric point cloud rendering followed by a neural network to fix artifacts and fill in the dis-occluded areas, jointly trained with a backward function. Each step is described below:</p>
<p>Geometric Point Cloud Rendering. Scans of real spaces include sparsely captured images, leading to a sparse set of sampled lightings from the scene. The quality of sensory depth and 3D meshes are also limited by 3D reconstruction algorithms or scanning devices. Reflective surfaces or small objects are often poorly reconstructed or entirely missing. All these prevent simply rendering from textured meshes to be a sufficient approach to view synthesis.</p>
<p>We instead adopt a two-stage approach, with the first stage being geometrically rendering point clouds: the given RGB-D panoramas are transformed into point clouds and each pixel is projected from equirectangular coordinates to Cartesian coordinates. For the desired target view v<sub>j</sub> = (x<sub>j</sub>, y<sub>j</sub>, z<sub>j</sub>, θ<sub>j</sub>, φ<sub>j</sub>, γ<sub>j</sub>), we choose the nearest k views in the scene database, denoted as v<sub>j,1</sub>, v<sub>j,2</sub>, . . . , v<sub>j,k</sub>. For each view v<sub>j,i</sub>, we transform the point cloud from v<sub>j,i</sub> coordinate to v<sub>j</sub> coordinate with a rigid body transformation and project the point cloud onto an equirectangular image. The pixels may open up and show a gap in-between, when rendered from the target view. Hence, the pixels that are supposed to be occluded may become visible through the gaps. To filter them out, we render an equirectangular depth as seen from the target view v<sub>j</sub> since we have the full reconstruction of the space. We then do a depth test and filter out the pixels with a difference &gt; 0.1m in their depth from the corresponding point in the target equirectangular depth. We now have sparse RGB points projected in equirectangulars for each reference panorama (see Fig. 2 (a)).</p>
<p>The points from all reference panoramas are aggregated to make one panorama using a locally weighted mixture (see Density Map in Fig. 2 (b)). We calculate the point density for each spatial position (average number of points per pixel) of each panorama, denoted as</p>
<p><sup>2</sup>Stanford AI lab has the copyright to all models.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Loss configuration for neural network based view synthesis. The loss contains two terms. The first is to transform the renderings to ground truth target images. The second is to alter ground truth target images to match the transformed rendering. A sample case is shown. (Best viewed on screen and zoomed-in.)</p>
<p><em>d</em><sub>1</sub>, . . . , <em>d</em><sub><em>k</em></sub>. For each position, the weight for view <em>i</em> is exp(λ<sub><em>d</em></sub>d<sub><em>i</em></sub>)/∑<sub>m</sub> exp(λ<sub><em>d</em></sub>d<sub>m</sub>), where λ<sub><em>d</em></sub> is a hyperparameter. Hence, the points in the aggregated panorama are adaptively selected from all views, rather than superimposed blindly which would expose lighting inconsistency and misalignment artifacts.</p>
<p>Finally, we do a bilinear interpolation on the aggregated points in one equirectangular to reduce the empty space between rendered pixels (see Fig. 2 (c)).</p>
<p>See the first row of Fig. 6 which shows the so-far output still includes major artifacts, including stitching marks, deformed objects, or large dis-occluded regions.</p>
<p>Neural Network Based Rendering. We use a neural network, <em>f</em> or "filler", to fix artifacts and generate a more real looking image given the output of geometric point cloud rendering. We use a set of novelties to produce good results efficiently, including a stochastic identity initialization and adding color moment matching in perceptual loss.</p>
<p>Architecture: The architecture and hyperparameters of our convolutional neural network <em>f</em> are detailed in the supplementary material. We utilize dilated convolutions [102] to aggregate contextual information. We use a 18-layer network, with 3 × 3 kernels for dilated convolution layers. The maximal dilation is 32. This allows us to achieve a large receptive field but not shrink the size of the feature map by too much. The minimal feature map size is ¼ ¼ of the original image size. We also use two architectures with the number of kernels being 48 or 256, depending on whether speed or quality is prioritized.</p>
<p>Identity Initialization: Though the output of the point cloud rendering suffers from notable artifacts, it is yet quite close to the ground truth target image numerically. Thus, an identity function (i.e. input image=output image) is a good place for initializing the neural network <em>f</em> at. We develop a stochastic approach to initializing the network at identity, to keep the weights nearly randomly distributed. We initialize <em>half</em> of the weights randomly with Gaussian and <em>freeze</em> them, then optimize the rest with back propagation to make the network's output the same as input. After convergence, the weights are our stochastic identity initialization. Other forms of identity initialization involve manually specifying the kernel weights, e.g. [22], which severely skews the distribution of weights (mostly 0s and some 1s). We found that to lead to slower converge and poorer results.</p>
<p>Loss: We use a perceptual loss [48] defined as:</p>
<p>$$D(I_1, I_2) = \sum_{l} \lambda_l ||\Psi_l(I_1) - \Psi_l(I_2)||<em i_j="i,j">1 + \gamma \sum</em>} ||\bar{I<em i_j="i,j">{1</em>}} - \bar{I<em i_j="i,j">{2</em> ||_1.$$}</p>
<p>For Ψ, we use a pretrained VGG16 [86]. Ψ<sub><em>l</em></sub>(<em>I</em>) denotes the feature map for input image <em>I</em> at <em>l</em>-th convolutional layer. We used all layers except for output layers. λ<sub><em>l</em></sub> is a scaling coefficient normalized with the number of elements in the feature map. We found perceptual loss to be inherently lossy w.r.t. color information (different colors were projected on one point). Therefore, we add a term to enforce matching statistical moments of color distribution. <em>I</em><sub><em>i,j</em></sub> is the average color vector of a 32 × 32 tile of the image which is enforced to be matching between <em>I</em><sub>1</sub> and <em>I</em><sub>2</sub> using L1 distance and γ is a mixture hyperparameter. We found our final setup to produce superior rendering results to GAN based losses (consistent with some recent works [21]).</p>
<h3>3.2.1 Closing the Gap with Real-World: Goggles</h3>
<p>With all of the imperfections in 3D inputs and geometric renderings, it is implausible to gain fully photo-realistic rendering with neural network fixes. Thus a domain gap with real images would remain. Therefore, we instead formulate the rendering problem as forming a joint space [81] (elaborated below) ensuring a correspondence between rendered and real images, and consequently, dissolving the gap.</p>
<p>If one wishes to create a mapping <em>S</em> ↦ <em>T</em> between domain <em>S</em> and domain <em>T</em> by training a function <em>f</em>, usually a loss with the following form is optimized:</p>
<p>$$\mathcal{L} = \mathbb{E}\left[D\left(f(\mathcal{I}_s), \mathcal{I}_t\right)\right],\tag{1}$$</p>
<p>where <em>I</em><sub><em>s</em></sub> ∈ <em>S</em>, <em>I</em><sub><em>t</em></sub> ∈ <em>T</em>, and <em>D</em> is a distance function. However, in our case the mapping between <em>S</em> (renderings) and <em>T</em> (real images) is not bijective, or at least the two directions <em>S</em> ↦ <em>T</em> and <em>T</em> ↦ <em>S</em> do not appear to be equally difficult. As an example, there is no unique solution to dis-occlusion filling, so the domain gap cannot reach zero exercising only <em>S</em> ↦ <em>T</em> direction. Hence, we add another function <em>u</em> to jointly utilize <em>T</em> ↦ <em>S</em> and define the objective to be minimizing the distance between <em>f</em>(I<sub><em>s</em></sub>) and <em>u</em>(I<sub><em>t</em></sub>). Network <em>u</em> is trained to alter an image taken in real-world, I<sub><em>t</em></sub>, to look like the corresponding rendered image in Gibson, I<sub><em>s</em></sub>, after passing through network <em>f</em> (see Fig. 3). Function <em>u</em> can be seen as corrective glasses of the agent, thus the name <em>Goggles</em>.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Physics Integration and Embodiment. A Mujoco humanoid model is dropped onto a stairway demonstrating a physically plausible fall along with the corresponding visual observations by the humanoid's eye. The first and second rows show the physics engine view of 4 sampled time steps and their corresponding rendered RGB views, respectively.</p>
<p>To avoid the trivial solution of all images collapsing to a single point, we add the first term in the following final loss to enforce preserving a one-to-one mapping. The loss for training networks <em>u</em> and <em>f</em> is:</p>
<p>$$
\mathcal{L} = \mathbb{E}\left[D(f(\mathcal{I}_s), \mathcal{I}_t)\right] + \mathbb{E}\left[D(f(\mathcal{I}_s), u(\mathcal{I}_t))\right]. \tag{2}
$$</p>
<p>See Fig. 3 for a visual example. <em>D</em> is the distance defined in Sec 3.2. We use the same network architecture for <em>f</em> and <em>u</em>.</p>
<h3>3.3. Embodiment and Physics Integration</h3>
<p>Perception and physical constraints are closely related. For instance, the perception model of a human-sized agent should seamlessly develop the notion that it does not fit in the gap under the door and hence should not attend such areas when solving a navigation task; a mouse-sized agent though could fit and its perception should attend such areas. It is thus important for the agent to be constantly subject to constraints of space and physics, e.g., collision, gravity, friction, throughout learning.</p>
<p>We integrated Gibson with a physics engine PyBullet [26] which supports rigid body and soft body simulation with discrete and continuous collision detection. We also use PyBullet's built-in fast collision handling system to record agent's certain interactions, such as how many times it collides with physical obstacles. We use Coulomb friction model by default, as scanned models do not come with material property annotations and certain physics aspects, such as friction, cannot be directly simulated.</p>
<p><strong>Agents:</strong> Gibson supports importing arbitrary agents with URDFs. Also, a number of agents are integrated as entry points, including humanoid and ant of Roboschool [4, 79], husky car [1], drone, minitaur [3], Jackrabbot [2]. Agent models are in ROS or Mujoco XML format.</p>
<p><strong>Integrated Controllers:</strong> To enable (optionally) abstracting away low-level control and robot dynamics for the tasks that are wished to be approached in a more high-level manner, we also provide a set of practical and ideal controllers to deduce the complexity of learning to control from scratch. We integrated a PID controller and a Nonholonomic controller as well as an ideal positional controller which completely abstracts away agent's motion dynamics.</p>
<h3>3.4. Additional Modalities</h3>
<p>Besides rendering RGB images, Gibson provides additional channels, such as depth, surface normals, and semantics. Unlike RGB images, these channels are more robust to noise in input data and lighting changes, and we render them directly from mesh files. Geometric modalities, e.g., depth, are provided for all models and semantics are available for 52,561 <em>m</em><sup>2</sup> of area with semantic annotations from 2D-3D-S [10] and Matterport3D [18] datasets.</p>
<p>Similar to other robotic simulation platforms, we also provide configurable proprioceptive sensory data. A typical proprioceptive sensor suite includes information of joint positions, angle velocity, robot orientation with respect to navigation target, position and velocity. We refer to this typical setup as "non-visual sensory" to distinguish from "visual" modalities in the rest of the paper.</p>
<h2>4. Tasks</h2>
<p><strong>Input-Output Abstraction:</strong> Gibson allows defining arbitrary tasks for an agent. To provide a common abstraction for this, we follow the interface of OpenAI Gym [14]: at each timestep, the agent performs an action at the environment; then the environment runs a forward step (integrated with the physics engine) and returns the accordingly rendered visual observation, reward, and termination signal. We also provide utility functions to keyboard operate an agent or visualize a recorded run.</p>
<h3>4.1. Experimental Validation Tasks</h3>
<p>In our experiments, we use a set of sample active perceptual tasks and static-recognition tasks to validate Gibson. The active tasks include:</p>
<p><strong>Local Planning and Obstacle Avoidance:</strong> An agent is randomly placed in an environment and needs to travel to a random nearby target location provided as relative coordinates (similar to flag run [4]). The agent receives no information about the environment except a continuous stream of depth and/or RGB frames and needs to plan perceptually (e.g., go around a couch to reach the target behind).</p>
<p><strong>Distant Visual Navigation:</strong> Similar to the previous task, but the target location is significantly further away and fixed. Agent's initial location is still randomized. This is similar to the task of auto-docking for robots from a distant location. Agent receives no external odometry or GPS information, and needs to form a contextual map to succeed.</p>
<p><strong>Stair Climb:</strong> An (ant [4]) agent is placed on one top of a stairway and the target location is at the bottom. It needs to learn a controller for its complex dynamics to plausibly go down the stairway without flipping, using visual inputs.</p>
<p>To benchmark how close to real images the renderings of Gibson are, we used two static-recognition tasks: depth estimation and scene classification. We train a neural network using (<em>rendering</em>, <em>ground truth</em>) pairs as training</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Sample spaces in Gibson database. The spaces are diverse in terms of size, visuals, and function, e.g. businesses, construction sites, houses. Upper: Sample 3D models. Lower: Sample images from Gibson database (left) and some of other environments [31, 49, 71, 83, 51, 100, 37, 106] (right).</p>
<p>data, but test them on (real image, ground truth). If Gibson renderings are close enough to real images and Goggles mechanism is effective, test results on real images are expected to be satisfactory. This also enables quantifying the impact of Goggles, i.e. using $u(\mathcal{I}_t)$ vs. $\mathcal{I}_s$, $f(\mathcal{I}_s)$, and $\mathcal{I}_t$.</p>
<p><strong>Depth Estimation:</strong> Predicting depth given a single RGB image, similar to [33]. We train 4 networks to predict the depth given one of the following 4 as input images: $\mathcal{I}_s$ (preneural network rendering), $f(\mathcal{I}_s)$ (post-neural network rendering), $u(\mathcal{I}_t)$ (real image seen with Goggles), and $\mathcal{I}_t$ (real image). We compare the performance of these in Sec. 5.3.</p>
<p><strong>Scene Classification:</strong> The same as previous task, but the output is scene classes rather than depth. As our images do not have scene class annotations, we generate them using a well performing network trained on Places dataset [104].</p>
<h2>5. Experimental Results</h2>
<h3>5.1. Benchmarking Space Databases</h3>
<p>The spaces in Gibson database are collected using various scanning devices, including NavVis, Matterport, or DotProduct, covering a diverse set of spaces, e.g. offices, garages, stadiums, grocery stores, gyms, hospitals, houses. All spaces are fully reconstructed in 3D and post processed to fill the holes and enhance the mesh. We benchmark some of the existing synthetic and real databases of spaces (SUNCG [88] and Matterport3D [18]) vs Gibson's using the following metrics in Table 1:</p>
<p><strong>Specific Surface Area (SSA):</strong> the ratio of inner mesh surface and volume of convex hull of the mesh. This is a measure of clutter in the models.</p>
<p><strong>Navigation Complexity:</strong> Longest $A^<em>$ navigation distance between randomly placed two points divided by the straight line distance. We compute the highest navigation complexity $\max_{s_i, s_j} \frac{d_{A^</em>, (s_i, s_j})}{d_{l^*, (s_i, s_j})}$ for every model.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Gibson</th>
<th>SUNCG</th>
<th>Matterport3D</th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of Spaces</td>
<td>572</td>
<td>45622</td>
<td>90</td>
</tr>
<tr>
<td>Total Coverage $m^2$</td>
<td>211k</td>
<td>5.8M</td>
<td>46.6K</td>
</tr>
<tr>
<td>SSA</td>
<td>1.38</td>
<td>0.74</td>
<td>0.92</td>
</tr>
<tr>
<td>Nav. Complexity</td>
<td>5.98</td>
<td>2.29</td>
<td>7.80</td>
</tr>
<tr>
<td>Real-World Transfer Err</td>
<td>0.92§</td>
<td>2.89†</td>
<td>2.11†</td>
</tr>
</tbody>
</table>
<p>Table 1: Benchmarking Space Databases: Comparison of Gibson database with SUNCG [88] (hand designed synthetic), and Matterport3D [18]. § Rendered with Gibson, † rendered with MINOS [76].</p>
<p><strong>Real-World Transfer Error:</strong> We train a neural network for depth estimation using the images of each database and test them on real images of 2D-3D-S dataset [10]. Training images of SUNCG and Matterport3D are rendered using MINOS [76] and our dataset is rendered using Gibson's engine. The training set of each database is 20k random RGB-depth image pairs with 90° field of view. The reported value is average depth estimation error in meters.</p>
<p><strong>Scene Diversity:</strong> We perform scene classification on 10k randomly picked images for each database using a network pretrained on [104]. We report the entropy of the distribution of top-1 classes for each environment. Gibson, SUNCG [88], and THOR [106] gain the scores of 3.72, 2.89, and 3.32, respectively (highest possible entropy = 5.90).</p>
<h3>5.2. Evaluation of View Synthesis</h3>
<p>To train the networks $f$ and $u$ of our neural network based synthesis framework, we sampled 4.3k 1024 × 2048 $\mathcal{I}_s$—$\mathcal{I}_t$ panorama pairs and randomly cropped them to 256 × 256. We use Adam [54] optimizer with learning rate 2 × 10$^{-4}$. We first train $f$ for 50 epochs until convergence, then we train $f$ and $u$ jointly for another 50 epochs with learning rate 2 × 10$^{-5}$. The learning finishes in 3 days on 2 Nvidia Titan X GPUs.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Qualitative results of view synthesis and Goggles. Top to bottom rows show images before neural network correction, after neural network correction, target image seen through Goggles, and target image (i.e. ground truth real image). The first column shows a pano and the rest are sample zoomed-in patches. Note the high similarity between 2nd and 3rd row, signifying the effectiveness of Goggles. (Best viewed on screen and zoomed-in.)</p>
<table>
<thead>
<tr>
<th>Resolution</th>
<th>128x128</th>
<th>256x256</th>
<th>512x512</th>
</tr>
</thead>
<tbody>
<tr>
<td>RGBD, pre network f</td>
<td>109.1</td>
<td>58.5</td>
<td>26.5</td>
</tr>
<tr>
<td>RGBD, post network f</td>
<td>77.7</td>
<td>30.6</td>
<td>14.5</td>
</tr>
<tr>
<td>RGBD, post small network f</td>
<td>87.4</td>
<td>40.5</td>
<td>21.2</td>
</tr>
<tr>
<td>Depth only</td>
<td>253.0</td>
<td>197.9</td>
<td>124.7</td>
</tr>
<tr>
<td>Surface Normal only</td>
<td>207.7</td>
<td>129.7</td>
<td>57.2</td>
</tr>
<tr>
<td>Semantic only</td>
<td>190.0</td>
<td>144.2</td>
<td>55.6</td>
</tr>
<tr>
<td>Non-Visual Sensory</td>
<td>396.1</td>
<td>396.1</td>
<td>396.1</td>
</tr>
</tbody>
</table>
<p>Table 2: Rendering speed (FPS) of Gibson on a single GPU for different resolutions and output configurations. Tested on E5-2697 v4 with Tesla V100 in headless rendering mode. As a faster setup ("small network"), we also trained a smaller filler network with donwsized input geometric renderings. This setup achieves a higher FPS at the expense of inferior visual quality compared to full-size filler network.</p>
<p>Sample renderings and their corresponding real image (ground truth) are shown in Fig. 6. Note that pre-neural network renderings suffer from geometric artifacts which are partially resolved in post-neural network results. Also, though the contrast of the post-neural network images is lower than real ones and color distributions are still different, Goggles could effectively alter the real images to match the renderings (compare 2nd and 3rd rows). In additional, the network f and Goggles u jointly addressed some of the pathological domain gaps. For instance, as lighting fixtures are often thin and shiny, they are not well reconstructed in our meshes and usually fail to render properly. Network f and Goggles learned to just suppress them altogether from images to not let a domain gap remain. The scene out the windows also often have large re-projection errors, so they are usually turned white by f and u.</p>
<p>Appearance columns in Table 3 quantify view synthesis results in terms image similarity metrics L1 and SSIM. They echo that the smallest gap is between f(2s) and u(2t).</p>
<p>Rendering Speed of Gibson is provided in Table 2.</p>
<h3>5.3 Transferring to Real-World</h3>
<p>We quantify the effectiveness of Goggles mechanism in reducing the domain gap between Gibson renderings and real imagery in two ways: via the static-recognition tasks</p>
<table>
<thead>
<tr>
<th>Train</th>
<th>Test</th>
<th>Static Tasks</th>
<th></th>
<th>Appearance</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Scene Class Acc.</td>
<td>Depth Est. Error</td>
<td>SSIM</td>
<td>L1</td>
</tr>
<tr>
<td>2s</td>
<td>2t</td>
<td>0.280</td>
<td>1.026</td>
<td>0.627</td>
<td>0.096</td>
</tr>
<tr>
<td>f(2s)</td>
<td>2t</td>
<td>0.266</td>
<td>1.560</td>
<td>0.480</td>
<td>0.10</td>
</tr>
<tr>
<td>f(2s)</td>
<td>u(2t)</td>
<td>0.291</td>
<td>0.915</td>
<td>0.816</td>
<td>0.051</td>
</tr>
</tbody>
</table>
<p>Table 3: Evaluation of view synthesis and transferring to real-world. Static Tasks column shows on both scene classification task and depth estimation tasks, it is easiest to transfer from f(2s) to u(2t) compared with other cross-domain transfers. Appearance columns compare L1 and SSIM distance metrics for different pairs showing the combination of network f and Goggles u achieves best results.</p>
<p>described in Sec. 4.1 and by comparing image distributions.</p>
<p>Evaluation of transferring to real images via scene classification and depth estimation are summarized in Table. 3. Also, Fig. 7 (a) provides depth estimation results for all feasible train-test combinations for reference. The diagonal values of the 4 × 4 matrix represent training and testing on the same domain. The gold standard is train and test on 2t (real images) which yields the error of 0.86. The closest combination to that in the entire table is train on f(1s) (f output) and test on u(1t) (real image through Goggles) giving 0.91, which signifies the effectiveness of Goggles.</p>
<p>In terms of distributional quantification, we used two metrics of Maximum Mean Discrepancy (MMD) [39] and CORAL [90] to test how well f(2s) and u(2t) domains are aligned. The metrics essentially determine how likely it is for two samples to be drawn from different distributions. We calculate MMD and CORAL values using the features of the last convolutional layer of VGG16 [86] and kernel k(x, y) = xT y. Results are summarized in Fig. 7 (b) and (c). For each metric, f(2s) - u(2t) is smaller than other pairs, showing that the two domains are well matching.</p>
<p>In order to quantitatively show the networks f and u do not give degenerate solutions (i.e. collapsing all images to few points to close the gap by cheating), we use f(2s) and u(2t) as queries to retrieve their nearest neighbor using VGG16 features from 2s and 2t, respectively. Top-1, 2 and 5 accuracies for f(2s) ↦ 2s are 91.6%, 93.5%, 95.6%.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Evaluation of transferring to real-world from Gibson. (a) Error of depth estimation for all train-test combinations. (b,c) MMD and CORAL distributional distances. All tests are in support of Goggles.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Visual Local planning and obstacle avoidance. Reward curves for perceptual vs non-perceptual husky agents and a sample trajectory.</p>
<p>Top-1, 2 and 5 accuracies for $u(\mathcal{I}<em t="t">{t}) \mapsto \mathcal{I}</em>$ are $85.9 \%$, $87.2 \%, 89.6 \%$. This indicates a good correspondence between pre and post neural network images is preserved, and thus, no collapse is observed.</p>
<h3>5.4 Validation Tasks Learned in Gibson</h3>
<p>The results of the active perceptual tasks discussed in Sec. 4.1 are provided here. In each experiment, the nonvisual sensor outputs include agent position, orientation, and relative position to target. The agents are rewarded by the decrease in their distance towards their targets. In Local Planning and Visual Obstacle Avoidance, they receive an additional penalty for every collision.</p>
<h4>Local Planning and Visual Obstacle Avoidance Results</h4>
<p>We trained a perceptual and non-perceptual husky agent according to the setting in Sec. 4.1 with PPO [78] for 150 episodes ( 300 iterations, 150k frames). Both agents have a four-dimensional discrete action space: forward/backward/left/right. The average reward over 10 iterations are plotted in Fig 8. The agent with perception achieves a higher score and developed obstacle avoidance behavior to reach the goal faster.</p>
<h4>Distant Visual Navigation Results</h4>
<p>Fig. 9 shows the target and sample random initial locations as well as the reward curves. Global navigation behavior emerges after 1700 episodes ( 680 k frames), and only the agent with visual state was able to accomplish the task. The action space is the same as previous experiment.</p>
<p>Also, we use the trained policy of distant navigation to evaluate the impact of Goggles on an active task: we go to camera locations where $\mathcal{I}<em t="t">{t}$ is available. Then we measure the policy discrepancy in terms of L2 distance of output action logits when different renderings and $\mathcal{I}</em>}$ are provided as input. Training on $f(\mathcal{I<em t="t">{s})$ and testing on $u(\mathcal{I}</em>})$ yields discrepancy of 0.204 (best), while training on $f(\mathcal{I<em t="t">{s})$ and testing on $\mathcal{I}</em>}$ gives 0.300 and training on $\mathcal{I<em t="t">{s}$ and testing on $\mathcal{I}</em>$</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Distant Visual Navigation. The initial locations and target are shown. The agent succeeds only when provided with visual inputs.</p>
<p>gives 0.242. After the initial release of our work, a paper recently reported an evaluation done on a real robot for adaptation using backward mapping from real images to renderings [103], with positive results. They did not use paired data, unlike Gibson, which would be expected to further enhance the results.</p>
<h4>Stair Climb</h4>
<p>As explained in Sec. 4.1, an ant [4] is trained to perform the complex locomotive task of plausibly climbing down a stairway without flipping. The action space is eight dimensional continuous torque values. We train one perceptual and one non-perceptual agent starting at a fixed initial location, but at test time slightly and randomly move their initial and target location around. They start to acquire stair-climbing skills after 1700 episodes ( 700 k time steps). While the perceptual agent learned slower, it showed better generalizability at test time coping with the location shifts and outperformed the nonperceptual agent by $70 \%$. Full details of this experiment is provided in the supplementary material.</p>
<h3>6 Limitations and Conclusion</h3>
<p>We presented Gibson Environment for developing real-world perception for active agents and validated it using a set of tasks. While we think this is a step forward, there are some limitations that should be noted. First, though Gibson provides a good basis for learning complex navigation and locomotion, currently it does not include dynamic content (e.g. other moving objects) and does not support manipulation. This can be potentially solved by integrating our approach with synthetic objects [19, 50]. Second, we do not have full material properties and no existing physics simulator is optimal; this may lead to physics related domain gaps. Finally, we provided quantitative evaluations of Goggles mechanism for transferring to real world mostly using static recognition tasks. The ultimate test is evaluating Goggles on real robots.</p>
<h4>Acknowledgement</h4>
<p>We gratefully acknowledge the support of Facebook, Toyota (1186781-31-UDARO), ONR MURI (N00014-14-1-0671), ONR (1165419-10-TDAUZ); Nvidia, CloudMinds, Panasonic (1192707-1-GWMSX).</p>
<h2>References</h2>
<p>[1] Husky UGV - Clearpath Robotics. http://wiki.ros. org/Robots/Husky. Accessed: 2017-09-30. 5
[2] Jackrabbot - Stanford Vision and Learning Group. http://cvgl.stanford.edu/projects/ jackrabbot/. Accessed: 2018-01-30. 5
[3] Legged UGVs - Ghost Robotics. https://www. ghostrobotics.io/copy-of-robots. Accessed: 2017-09-30. 5
[4] OpenAI Roboschool. http://blog.openai.com/ roboschool/. Accessed: 2018-02-02. 5, 8
[5] P. Abbeel, A. Coates, and A. Y. Ng. Autonomous helicopter aerobatics through apprenticeship learning. The International Journal of Robotics Research, 29(13):1608-1639, 2010. 2
[6] P. Abbeel, A. Coates, M. Quigley, and A. Y. Ng. An application of reinforcement learning to aerobatic helicopter flight. In Advances in neural information processing systems, pages 1-8, 2007. 2
[7] P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, and S. Levine. Learning to poke by poking: Experiential learning of intuitive physics. In Advances in Neural Information Processing Systems, pages 5074-5082, 2016. 2
[8] P. Ammirato, P. Poirson, E. Park, J. Košecká, and A. C. Berg. A dataset for developing and benchmarking active vision. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 1378-1385. IEEE, 2017. 2
[9] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. van den Hengel. Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2
[10] I. Armeni, A. Sax, A. R. Zamir, and S. Savarese. Joint 2D-3D-Semantic Data for Indoor Scene Understanding. ArXiv e-prints, Feb. 2017. 3, 5, 6
[11] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 47:253-279, 2013. 2
[12] J. Blitzer, R. McDonald, and F. Pereira. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120-128. Association for Computational Linguistics, 2006. 2
[13] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan. Linear matrix inequalities in system and control theory. SIAM, 1994. 2
[14] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. 5
[15] R. A. Brooks. Elephants don't play chess. Robotics and autonomous systems, 6(1-2):3-15, 1990. 1, 2
[16] R. A. Brooks. Intelligence without representation. Artificial intelligence, 47(1-3):139-159, 1991. 1
[17] J. Carbonell and G. Hood. The world modelers project: Objectives and simulator architecture. In Machine Learning, pages 29-34. Springer, 1986. 2
[18] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Nießner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017. 3, 5, 6
[19] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 8
[20] C.-F. Chang, G. Bishop, and A. Lastra. Ldi tree: A hierarchical representation for image-based rendering. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pages 291-298. ACM Press/Addison-Wesley Publishing Co., 1999. 3
[21] Q. Chen and V. Koltun. Photographic image synthesis with cascaded refinement networks. arXiv preprint arXiv:1707.09405, 2017. 4
[22] Q. Chen, J. Xu, and V. Koltun. Fast image processing with fully-convolutional networks. In IEEE International Conference on Computer Vision, 2017. 4
[23] S. E. Chen and L. Williams. View interpolation for image synthesis. In Proceedings of the 20th annual conference on Computer graphics and interactive techniques, pages 279288. ACM, 1993. 3
[24] P. S. Churchland, V. S. Ramachandran, and T. J. Sejnowski. A critique of pure vision. Large-scale neuronal theories of the brain, pages 23-60, 1994. 1
[25] F. Codevilla, M. Müller, A. Dosovitskiy, A. López, and V. Koltun. End-to-end driving via conditional imitation learning. arXiv preprint arXiv:1710.02410, 2017. 2
[26] E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016-2018. 5
[27] H. Daumé III. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009. 2
[28] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248-255. IEEE, 2009. 2
[29] J. P. Desai, J. P. Ostrowski, and V. Kumar. Modeling and control of formations of nonholonomic mobile robots. IEEE transactions on Robotics and Automation, 17(6):905908, 2001. 2
[30] A. Dosovitskiy and V. Koltun. Learning to act by predicting the future. arXiv preprint arXiv:1611.01779, 2016. 2
[31] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun. Carla: An open urban driving simulator. In Conference on Robot Learning, pages 1-16, 2017. 2, 6
[32] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 1538-1546. IEEE, 2015. 3
[33] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in neural information processing systems, pages 2366-2374, 2014. 6</p>
<p>[34] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303338, 2010.2
[35] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel. Deep spatial autoencoders for visuomotor learning. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 512-519. IEEE, 2016. 2
[36] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig. Virtual worlds as proxy for multi-object tracking analysis. In CVPR, 2016. 2
[37] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231-1237, 2013. 6
[38] J. J. Gibson. The ecological approach to visual perception. Psychology Press, 2013. 1
[39] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723-773, 2012. 7
[40] A. Gupta. Supersizing self-supervision: Learning perception and action without human supervision. 2016. 2
[41] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik. Cognitive mapping and planning for visual navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2616-2625, 2017. 2
[42] R. Hartley and A. Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 3
[43] P. Hedman, T. Ritschel, G. Drettakis, and G. Brostow. Scalable inside-out image-based rendering. ACM Transactions on Graphics (TOG), 35(6):231, 2016. 3
[44] N. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang, A. Eslami, M. Riedmiller, et al. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017. 2
[45] R. Held and A. Hein. Movement-produced stimulation in the development of visually guided behavior. Journal of comparative and physiological psychology, 56(5):872, 1963. 1
[46] N. Hirose, A. Sadeghian, M. Vázquez, P. Goebel, and S. Savarese. Gonet: A semi-supervised deep learning approach for traversability estimation. arXiv preprint arXiv:1803.03254, 2018. 2
[47] C. Jiang, Y. Zhu, S. Qi, S. Huang, J. Lin, X. Guo, L.-F. Yu, D. Terzopoulos, and S.-C. Zhu. Configurable, photorealistic image rendering and ground truth synthesis by sampling stochastic grammars representing indoor scenes. arXiv preprint arXiv:1704.00112, 2017. 2
[48] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pages 694-711. Springer, 2016. 4
[49] M. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The malmo platform for artificial intelligence experimentation. In IJCAI, pages 4246-4247, 2016. 2, 6
[50] K. Karsch, V. Hedau, D. Forsyth, and D. Hoiem. Rendering synthetic objects into legacy photographs. In ACM Transactions on Graphics (TOG), volume 30, page 157. ACM, 2011. 8
[51] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Jaśkowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In Computational Intelligence and Games (CIG), 2016 IEEE Conference on, pages 1-8. IEEE, 2016. 2, 6
[52] O. Khatib. Real-time obstacle avoidance for manipulators and mobile robots. In Autonomous robot vehicles, pages 396-404. Springer, 1986. 2
[53] O. Khatib. A unified approach for motion and force control of robot manipulators: The operational space formulation. IEEE Journal on Robotics and Automation, 3(1):4353, 1987. 2
[54] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6
[55] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum. Deep convolutional inverse graphics network. In Advances in Neural Information Processing Systems, pages 2539-2547, 2015. 3
[56] P. Langley, D. Nicholas, D. Klahr, and G. Hood. A simulated world for modeling learning and development. In Proceedings of the Third Conference of the Cognitive Science Society, pages 274-276, 1981. 2
[57] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1-8. IEEE, 2008. 2
[58] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):1-40, 2016. 2
[59] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. The International Journal of Robotics Research, page 0278364917710318, 2016. 2
[60] M. Levoy and P. Hanrahan. Light field rendering. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 31-42. ACM, 1996. 3
[61] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014. 2
[62] W. R. Mark, L. McMillan, and G. Bishop. Post-rendering 3d warping. In Proceedings of the 1997 symposium on Interactive 3D graphics, pages 7-ff. ACM, 1997. 3
[63] R. Mottaghi, H. Bagherinezhad, M. Rastegari, and A. Farhadi. Newtonian scene understanding: Unfolding the dynamics of objects in static images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3521-3529, 2016. 2
[64] R. Ortiz-Cayon, A. Djelouah, and G. Drettakis. A bayesian approach for selective image-based rendering using superpixels. In International Conference on 3D Vision-3DV, 2015. 3
[65] A. R. Parker. On the origin of optics. Optics \&amp; Laser Technology, 43(2):323-329, 2011. 1
[66] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised prediction. arXiv preprint arXiv:1705.05363, 2017. 2</p>
<p>[67] L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 3406-3413. IEEE, 2016. 2
[68] D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural information processing systems, pages 305-313, 1989. 2
[69] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision, pages 102-118. Springer, 2016. 2
[70] M. D. Rodriguez, J. Ahmed, and M. Shah. Action mach a spatio-temporal maximum average correlation height filter for action recognition. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1-8. IEEE, 2008. 2
[71] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. Lopez. The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes. 2016. 6
[72] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3234-3243, 2016. 2
[73] S. Ross, G. J. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In International Conference on Artificial Intelligence and Statistics, pages 627-635, 2011. 2
[74] F. Sadeghi and S. Levine. rl: Real singleimage flight without a single real image. arxiv preprint. arXiv preprint arXiv:1611.04201, 12, 2016. 2
[75] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In European conference on computer vision, pages 213-226. Springer, 2010. 2
[76] M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun. Minos: Multimodal indoor simulator for navigation in complex environments. arXiv preprint arXiv:1712.03931, 2017. 6
[77] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1889-1897, 2015. 2
[78] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2, 8
[79] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal Policy Optimization Algorithms. ArXiv e-prints, July 2017. 5
[80] S. M. Seitz and C. R. Dyer. View morphing. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 21-30. ACM, 1996. 3
[81] O. Sener, H. O. Song, A. Saxena, and S. Savarese. Learning transferrable representations for unsupervised domain adaptation. In Advances in Neural Information Processing Systems, pages 2110-2118, 2016. 2, 4
[82] J. Shade, S. Gortler, L.-w. He, and R. Szeliski. Layered depth images. In Proceedings of the 25th annual conference
on Computer graphics and interactive techniques, pages 231-242. ACM, 1998. 3
[83] S. Shah, D. Dey, C. Lovett, and A. Kapoor. Airsim: Highfidelity visual and physical simulation for autonomous vehicles. In Field and Service Robotics, 2017. 2, 6
[84] E. Shechtman, A. Rav-Acha, M. Irani, and S. Seitz. Regenerative morphing. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 615-622. IEEE, 2010. 3
[85] H. A. Simon. The sciences of the artificial. MIT press, 1996. 2
[86] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 4, 7
[87] L. Smith and M. Gasser. The development of embodied cognition: Six lessons from babies. Artificial life, 11(1-2):13-29, 2005. 1
[88] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic scene completion from a single depth image. IEEE Conference on Computer Vision and Pattern Recognition, 2017. 2, 6
[89] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In AAAI, volume 6, page 8, 2016. 2
[90] B. Sun and K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Computer Vision-ECCV 2016 Workshops, pages 443-450. Springer, 2016. 7
[91] S. Suwajanakorn, I. Kemelmacher-Shlizerman, and S. M. Seitz. Total moving face reconstruction. In European Conference on Computer Vision, pages 796-812. Springer, 2014. 3
[92] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view 3d models from single images with a convolutional network. In European Conference on Computer Vision, pages 322-337. Springer, 2016. 3
[93] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. arXiv preprint arXiv:1703.06907, 2017. 2
[94] M. Waechter, N. Moehrle, and M. Goesele. Let there be color! large-scale texturing of 3d reconstructions. In European Conference on Computer Vision, pages 836-850. Springer, 2014. 3
[95] D. M. Wolpert and Z. Ghahramani. Computational principles of movement neuroscience. Nature neuroscience, 3:1212-1217, 2000. 1
[96] J. Wu, J. J. Lim, H. Zhang, J. B. Tenenbaum, and W. T. Freeman. Physics 101: Learning physical object properties from unlabeled videos. In $B M V C$, volume 2, page 7, 2016. 2
[97] J. Wu, I. Yildirim, J. J. Lim, B. Freeman, and J. Tenenbaum. Galileo: Perceiving physical object properties by integrating a physics engine with deep learning. In Advances in neural information processing systems, pages 127-135, 2015. 2
[98] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian. Building generalizable agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209, 2018. 2</p>
<p>[99] M. Wulfmeier, A. Bewley, and I. Posner. Addressing appearance change in outdoor robotics with adversarial domain adaptation. arXiv preprint arXiv:1703.01461, 2017. 2
[100] B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, and A. Sumner. Torcs, the open racing car simulator. Software available at http://torcs. sourceforge. net, 4, 2000. 6
[101] H. Xu, Y. Gao, F. Yu, and T. Darrell. End-to-end learning of driving models from large-scale video datasets. arXiv preprint arXiv:1612.01079, 2016. 2
[102] F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122, 2015. 4
[103] J. Zhang, L. Tai, Y. Xiong, M. Liu, J. Boedecker, and W. Burgard. Vr goggles for robots: Real-to-sim domain adaptation for visual control. arXiv preprint arXiv:1802.00265, 2018. 8
[104] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. 6
[105] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View synthesis by appearance flow. In European Conference on Computer Vision, pages 286-301. Springer, 2016. 3
[106] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. FeiFei, and A. Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 3357-3364. IEEE, 2017. 2, 6</p>            </div>
        </div>

    </div>
</body>
</html>
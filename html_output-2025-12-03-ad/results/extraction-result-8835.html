<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8835 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8835</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8835</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-273350718</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.11370v1.pdf" target="_blank">Enhance Graph Alignment for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Graph-structured data is prevalent in the real world. Recently, due to the powerful emergent capabilities, Large Language Models (LLMs) have shown promising performance in modeling graphs. The key to effectively applying LLMs on graphs is converting graph data into a format LLMs can comprehend. Graph-to-token approaches are popular in enabling LLMs to process graph information. They transform graphs into sequences of tokens and align them with text tokens through instruction tuning, where self-supervised instruction tuning helps LLMs acquire general knowledge about graphs, and supervised fine-tuning specializes LLMs for the downstream tasks on graphs. Despite their initial success, we find that existing methods have a misalignment between self-supervised tasks and supervised downstream tasks, resulting in negative transfer from self-supervised fine-tuning to downstream tasks. To address these issues, we propose Graph Alignment Large Language Models (GALLM) to benefit from aligned task templates. In the self-supervised tuning stage, we introduce a novel text matching task using templates aligned with downstream tasks. In the task-specific tuning stage, we propose two category prompt methods that learn supervision information from additional explanation with further aligned templates. Experimental evaluations on four datasets demonstrate substantial improvements in supervised learning, multi-dataset generalizability, and particularly in zero-shot capability, highlighting the model's potential as a graph foundation model.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8835.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8835.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>graph-to-token</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-Token alignment approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modality-alignment approach that converts graph information into a sequence of graph tokens, projects those token embeddings into the LLM text embedding space via a learned projector, and concatenates them with instruction text embeddings for instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-to-token</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode nodes (and optionally small subgraphs) as a fixed-length sequence of graph tokens; map the graph-token embeddings into the LLM's text embedding space with a projector (typically a linear layer); replace placeholder token embeddings in the LLM input with these projected graph embeddings and concatenate with task instruction embeddings to create the final input sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs (citation networks, social networks) as used in experiments (Cora, Pubmed, Arxiv, Instagram).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Sample an h-hop subgraph around a center node, build a fixed-size computational tree (see Neighbor Detail Template), perform level-order traversal to a fixed-length node sequence; encode node textual attributes (Sentence-BERT in experiments) to embeddings X; apply a linear projector to map X to LLM text-embedding space; replace placeholder token embeddings with projected graph embeddings and prepend/append task-specific instruction embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification, link prediction (also evaluated for self-supervised pretext text-matching and zero-shot transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As part of the full GALLM pipeline (graph-to-token + other modules). Node classification (supervised): GALLM-mp (manual category prompts) — Cora Acc 0.8965 F1 0.8934; Pubmed Acc 0.9115 F1 0.9105; Arxiv Acc 0.7330 F1 0.5664; Instagram Acc 0.6460 F1 0.5583. GALLM-sp (soft prompts) — Cora Acc 0.8854 F1 0.8757; Pubmed Acc 0.9087 F1 0.9086; Arxiv Acc 0.7391 F1 0.5775; Instagram Acc 0.6274 F1 0.5203. Link prediction: GALLM-sp Cora Acc 0.6230 F1 0.6154; GALLM-mp Cora Acc 0.6156 F1 0.5975; Pubmed GALLM-sp Acc 0.7321 F1 0.7319; GALLM-mp Acc 0.7373 F1 0.7373. Zero-shot (self-supervised only): GALLM-mp Cora 0.5804, Pubmed 0.4791, Arxiv 0.1309 (see Table 4). Ablation (remove text-matching or category prompts) shows measurable drops (e.g., GALLM-mp w/o text-matching on Cora Acc falls from 0.8965 to 0.8724).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Directly compared against other LLM-for-graph methods (GraphGPT, LLaGA) and standard GNNs; the GALLM pipeline that uses the graph-to-token approach outperforms GraphGPT and LLaGA on node classification and link prediction in the reported tables. The paper also reports negative transfer when alternative self-supervised tasks (node description, graph matching) were used instead of the aligned text-matching pretext.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables LLMs to ingest graph structure while leveraging textual node attributes; projector + tokenized graph sequence is parameter-efficient (LLM backbone frozen) and compatible with instruction tuning; produces strong supervised, multi-dataset and zero-shot results in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on careful template and projector design; fixed-length sampling can drop neighbors and introduce sampling bias; quality depends on node textual attributes and the projector; requires a sampling and tokenization pipeline (computational tree) so can lose information relative to full-graph encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>The paper identifies negative transfer when the self-supervised pretext is not aligned with downstream discriminative templates (e.g., node description or graph matching caused performance decreases in preliminary study). Performance can degrade when category prompts are removed (ablation w/o cp) or when self-supervised text-matching is omitted (ablation w/o tm). Zero-shot performance remains limited on some datasets (Arxiv zero-shot 0.1309) despite improvements over baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhance Graph Alignment for Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8835.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8835.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeighborDetail</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neighbor Detail Template (fixed-size sampled computational tree)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-token graph description template that builds a fixed-size computational tree centered at a node, samples a fixed number of neighbors per hop, performs a level-order traversal to produce a fixed-length node sequence suitable as graph tokens for LLM input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Neighbor Detail Template</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct an h-hop computational tree centered on the target node; for each hop, sample a fixed number of neighbors (fill with pad tokens if fewer exist); perform level-order traversal to obtain a fixed-length ordered sequence of node tokens; use these tokens as the graph-token sequence input to the projector and LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (citation networks such as Cora, Pubmed, Arxiv; social networks like Instagram used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each center node, sample a fixed number of neighbors per hop (e.g., 10 per hop in experiments), pad missing positions with pad tokens (zero vectors), then perform level-order traversal to a fixed-length node token sequence (the center node as first token).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used as the graph-description input for node classification, link prediction and the self-supervised text-matching pretext during instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as part of GALLM results listed under graph-to-token (see node classification and link prediction numbers in previous entry). The experiments used a fixed sample size of 10 per hop and maximum input length 2048.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compatibility study: the paper also tested an alternative template (Text-Graph Grounding pre-trained GNNs from GraphGPT) and reported that GALLM improves performance across different templates (Figure 4), indicating this template is compatible but not uniquely superior in the GALLM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Produces a fixed-length, structured node sequence that captures local neighborhood information for LLM consumption; straightforward to implement and deterministic; handles variable-degree nodes via sampling and padding.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Fixed sampling may omit relevant neighbors (information loss) or introduce sampling bias; padding introduces artificial tokens; sequence order may not capture richer graph connectivity beyond the sampled tree view.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly enumerated, but potential failure modes include poor representation when important neighbors are omitted by sampling, and sensitivity to the chosen sample size/hop parameter that can under- or over-represent local structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhance Graph Alignment for Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8835.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8835.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-graph-grounding-template</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-Graph Grounding Pre-trained GNNs template (GraphGPT-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph description template that samples subgraphs and uses a pre-trained GNN aligned with text via contrastive/text-graph grounding to produce pre-aligned graph token embeddings for LLM instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text-Graph Grounding Template (GraphGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Sample a subgraph centered on a node, encode nodes/subgraph with a pre-trained GNN that was text-graph grounded (contrastive alignment between node features and textual representations), producing graph-token embeddings already partially aligned to textual embedding space for downstream instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (citation datasets used in compatibility experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Subgraph sampling followed by GNN encoding; the GNN has been pre-trained with a text-graph contrastive grounding objective so its outputs are compatible with LLM text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification and other graph tasks in compatibility experiments (used in Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper reports that GALLM 'boosts model performance across different templates and base models' when this template is used (compatibility evaluation), but does not provide standalone numeric metrics for this template alone in isolation from the rest of the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared qualitatively in a compatibility test against the Neighbor Detail Template; GALLM improvements persist under both templates, indicating model-level gains are not template-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Pre-aligns graph token representations with text representations via contrastive grounding, potentially simplifying downstream projector/alignment training.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on pre-training of a graph encoder and the availability of text-graph contrastive pretraining resources; complexity increases due to pretraining step.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly reported; general limitations include dependency on quality of pre-training and possible mismatches between pretraining objectives and downstream discriminative templates if not aligned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhance Graph Alignment for Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8835.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8835.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-matching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text Matching self-supervised task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative self-supervised pretext introduced in this paper where the LLM is trained to select the most relevant candidate text (the central node's textual attribute) given a node-centered subgraph and a candidate set of texts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>text matching task (self-supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Given the graph-token representation of a node-centered subgraph and a set of candidate texts (one positive = center node's text, several negatives), the LLM must choose the correct positive text. Negative sampling mixes easy negatives (non-neighbors) and hard negatives (neighbor nodes' texts).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs where nodes have textual attributes (paper titles/abstracts, user meta-data).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use the Neighbor Detail Template (or equivalent) to produce a node sequence for the center node; obtain candidate texts from node textual attributes; sample negative candidates with a mix of hard negatives from neighbors and easy negatives from non-neighbors (experiments used 3 hard and 7 easy negatives per positive); formulate an instruction prompt requiring selection among candidate texts; train by maximizing likelihood of correct selection.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Designed to transfer to discriminative downstream tasks such as node classification and link prediction by aligning pretext template with downstream templates (where candidate texts are replaced with candidate labels).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Self-supervised (zero-shot) performance: GALLM-mp (text matching only, no supervised fine-tuning) — Cora 0.5804, Pubmed 0.4791, Arxiv 0.1309 (Table 4), substantially higher than GraphGPT (e.g., GraphGPT Cora 0.0776) and LLaGA in the same setting. Ablation: removing text-matching (w/o tm) reduces supervised performance (e.g., GALLM-mp Cora Acc from 0.8965 to 0.8724; see Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Contrasted with other self-supervised tasks used in prior LLM-for-graph works: node description (LLaGA) and graph matching (GraphGPT). Those tasks are generative/reordering and were observed to cause negative transfer for discriminative downstream tasks; text-matching (discriminative template) aligned better and led to positive transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Template-alignment with discriminative downstream tasks yields better knowledge transfer, improved zero-shot and supervised performance; explicitly leverages node textual attributes and LLMs' strengths in text understanding; robustness improved by mixing hard and easy negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires informative textual attributes for nodes (poor/noisy text will reduce effectiveness); needs careful negative sampling design; design assumes downstream tasks can be expressed in selection/discriminative templates.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Although improved versus baselines, zero-shot performance remains dataset-dependent (e.g., Arxiv zero-shot still low 0.1309). If textual attributes are absent or noisy, task utility will likely drop. Negative transfer observed with other pretexts underscores requirement for template alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhance Graph Alignment for Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8835.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8835.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>category-manual-prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Category-enhanced Manual Prompt Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task-specific tuning method that augments discrete category labels with explanatory natural-language sentences (manually written or generated by a strong LLM) to provide sentence-level category context in the downstream instruction prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>category-enhanced manual prompt</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Append an explanatory sentence to each candidate category in the instruction prompt (e.g., 'category: case-based, which analyzes real-world cases to draw conclusions;'). Explanations are manually authored or generated by an external LLM and included verbatim in prompts during task-specific tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Used on text-attributed graphs for downstream discriminative tasks (node classification, link prediction reformulated as binary classification).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>During Stage 2 task-specific tuning, include category explanatory sentences in the instruction text concatenated with candidate category labels; feed the combined instruction + projected graph tokens into the frozen LLM and fine-tune the projector (manual prompt case tunes projector only).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification (primary), link prediction (binary categories enhanced).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GALLM-mp (manual prompt) supervised node-classification results: Cora Acc 0.8965 F1 0.8934; Pubmed Acc 0.9115 F1 0.9105; Arxiv Acc 0.7330 F1 0.5664; Instagram Acc 0.6460 F1 0.5583. Ablation removing category prompts (w/o cp) reduced performance (example: Cora Acc drops to 0.8780, F1 to 0.8667; see Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared with category-enhanced soft prompts: manual prompts outperform soft prompts when labeled data is scarce (Cora, Pubmed, Instagram), while soft prompts can outperform manual prompts on large-label datasets like Arxiv.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Distills external knowledge into category labels, improves LLM comprehension of categories without adding trainable parameters (only projector tuned), effective in low-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Discrete manual prompts can be brittle — small wording changes may notably affect performance; prompt creation requires human expertise or extra LLM calls to generate explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performance is sensitive to prompt wording (manual prompt instability cited). If explanatory text is incorrect or misleading, downstream performance can suffer. Manual prompts may not scale well across many tasks or label sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhance Graph Alignment for Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8835.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8835.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>category-soft-prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Category-enhanced Soft Prompt Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous prompt tuning method where category explanatory content is represented by trainable virtual tokens whose embeddings are encoded by a small soft-prompt encoder and optimized (alongside the projector) via backpropagation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>category-enhanced soft prompt</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Replace discrete explanatory sentences with a small number of virtual soft prompt tokens per category (e.g., 3 tokens); initialize embeddings randomly; pass them through a soft prompt encoder (two-layer MLP) and concatenate the output with discrete prompt tokens and projected graph tokens as LLM input; train soft prompt embeddings and encoder (and projector) via gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs for node classification and link prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>During Stage 2, insert virtual tokens (e.g., '<soft>' tokens) after each category label in the instruction; the embeddings are trainable and processed by a small encoder; the pipeline tunes these embeddings and encoder parameters jointly with the graph projector.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification and link prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GALLM-sp (soft prompts) supervised node-classification: Cora Acc 0.8854 F1 0.8757; Pubmed Acc 0.9087 F1 0.9086; Arxiv Acc 0.7391 F1 0.5775; Instagram Acc 0.6274 F1 0.5203. Soft prompts perform better than manual prompts on large-label dataset Arxiv (Arxiv Acc 0.7391 vs manual 0.7330). Ablations show removing self-supervised text-matching reduces soft-prompt performance as well.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to manual category prompts: soft prompts introduce trainable parameters and can adapt to abundant labeled data, producing stronger performance on large datasets, while manual prompts are preferable in low-label regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Learns explanatory category context automatically (no manual prompt design), more stable to small wording changes, adaptable via gradient updates and beneficial when sufficient labels are available.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Introduces additional trainable parameters and thus needs more labeled data to realize gains; increases training complexity (soft prompt encoder, embeddings) and may overfit on small datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Per experiments, soft prompts are less effective than manual prompts in low-data settings (Cora, Pubmed, Instagram). Removal of category prompts reduces performance (w/o cp ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhance Graph Alignment for Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8835.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8835.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>graph-projector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Projector (placeholder replacement + linear projector)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight mapping module (typically a linear layer) that projects graph token embeddings into the LLM text embedding space and replaces specified LLM placeholder token embeddings with the projected graph embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph projector (linear layer) + placeholder substitution</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A learned projector (linear layer in experiments) maps graph-derived embeddings (e.g., Sentence-BERT node embeddings or pre-trained GNN outputs) to the dimensionality and space of LLM text embeddings; during input construction, special placeholder tokens in the instruction embedding sequence are replaced with these projected graph embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Used with text-attributed graphs for node-level tasks in the paper's LLM-based pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Encode node textual attributes with Sentence-BERT (or pre-trained GNN outputs under alternative templates), then apply the projector to obtain embeddings compatible with the LLM embedding layer; construct instruction text embeddings with placeholder positions; substitute placeholders with projected graph embeddings to form model input.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification, link prediction, self-supervised text-matching and instruction tuning stages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GALLM training strategy typically freezes LLM backbone and fine-tunes only the projector (and soft prompts where applicable). The reported GALLM performance (see other entries) is achieved with this lightweight tuning approach — e.g., strong supervised results (GALLM-mp and GALLM-sp numbers in Table 2) while only tuning the projector in Stage 1 and Stage 2 (manual prompt); soft prompt tuning adds soft prompt embeddings and encoder to tuned parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Contrasted with full LLM fine-tuning approaches: projector-only tuning is parameter-efficient and yields strong gains in the paper's experiments; using a pre-trained GNN + grounding template is another path (GraphGPT style) but still requires a mapping/alignment step.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Parameter-efficient (freezes large LLM weights), computationally cheaper to tune, simplifies alignment by learning a small mapping into LLM embedding space, supports modular replacement of graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May limit ultimate performance compared to full-model fine-tuning if projector capacity is insufficient; success depends on the projector's capacity and quality of the input graph embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly enumerated; ablation studies show that tuning only the projector without aligned pretexts (i.e., omitting text-matching or category prompts) reduces performance, indicating that projector tuning needs compatible task templates to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhance Graph Alignment for Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GraphGPT: Graph instruction tuning for large language models <em>(Rating: 2)</em></li>
                <li>Llaga: Large language and graph assistant <em>(Rating: 2)</em></li>
                <li>Graphtext: Graph reasoning in text space <em>(Rating: 2)</em></li>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data? <em>(Rating: 2)</em></li>
                <li>Zerog: Investigating cross-dataset zero-shot transferability in graphs <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8835",
    "paper_id": "paper-273350718",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "graph-to-token",
            "name_full": "Graph-to-Token alignment approach",
            "brief_description": "A modality-alignment approach that converts graph information into a sequence of graph tokens, projects those token embeddings into the LLM text embedding space via a learned projector, and concatenates them with instruction text embeddings for instruction tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "graph-to-token",
            "representation_description": "Encode nodes (and optionally small subgraphs) as a fixed-length sequence of graph tokens; map the graph-token embeddings into the LLM's text embedding space with a projector (typically a linear layer); replace placeholder token embeddings in the LLM input with these projected graph embeddings and concatenate with task instruction embeddings to create the final input sequence.",
            "graph_type": "text-attributed graphs (citation networks, social networks) as used in experiments (Cora, Pubmed, Arxiv, Instagram).",
            "conversion_method": "Sample an h-hop subgraph around a center node, build a fixed-size computational tree (see Neighbor Detail Template), perform level-order traversal to a fixed-length node sequence; encode node textual attributes (Sentence-BERT in experiments) to embeddings X; apply a linear projector to map X to LLM text-embedding space; replace placeholder token embeddings with projected graph embeddings and prepend/append task-specific instruction embeddings.",
            "downstream_task": "Node classification, link prediction (also evaluated for self-supervised pretext text-matching and zero-shot transfer).",
            "performance_metrics": "As part of the full GALLM pipeline (graph-to-token + other modules). Node classification (supervised): GALLM-mp (manual category prompts) — Cora Acc 0.8965 F1 0.8934; Pubmed Acc 0.9115 F1 0.9105; Arxiv Acc 0.7330 F1 0.5664; Instagram Acc 0.6460 F1 0.5583. GALLM-sp (soft prompts) — Cora Acc 0.8854 F1 0.8757; Pubmed Acc 0.9087 F1 0.9086; Arxiv Acc 0.7391 F1 0.5775; Instagram Acc 0.6274 F1 0.5203. Link prediction: GALLM-sp Cora Acc 0.6230 F1 0.6154; GALLM-mp Cora Acc 0.6156 F1 0.5975; Pubmed GALLM-sp Acc 0.7321 F1 0.7319; GALLM-mp Acc 0.7373 F1 0.7373. Zero-shot (self-supervised only): GALLM-mp Cora 0.5804, Pubmed 0.4791, Arxiv 0.1309 (see Table 4). Ablation (remove text-matching or category prompts) shows measurable drops (e.g., GALLM-mp w/o text-matching on Cora Acc falls from 0.8965 to 0.8724).",
            "comparison_to_others": "Directly compared against other LLM-for-graph methods (GraphGPT, LLaGA) and standard GNNs; the GALLM pipeline that uses the graph-to-token approach outperforms GraphGPT and LLaGA on node classification and link prediction in the reported tables. The paper also reports negative transfer when alternative self-supervised tasks (node description, graph matching) were used instead of the aligned text-matching pretext.",
            "advantages": "Enables LLMs to ingest graph structure while leveraging textual node attributes; projector + tokenized graph sequence is parameter-efficient (LLM backbone frozen) and compatible with instruction tuning; produces strong supervised, multi-dataset and zero-shot results in experiments.",
            "disadvantages": "Relies on careful template and projector design; fixed-length sampling can drop neighbors and introduce sampling bias; quality depends on node textual attributes and the projector; requires a sampling and tokenization pipeline (computational tree) so can lose information relative to full-graph encodings.",
            "failure_cases": "The paper identifies negative transfer when the self-supervised pretext is not aligned with downstream discriminative templates (e.g., node description or graph matching caused performance decreases in preliminary study). Performance can degrade when category prompts are removed (ablation w/o cp) or when self-supervised text-matching is omitted (ablation w/o tm). Zero-shot performance remains limited on some datasets (Arxiv zero-shot 0.1309) despite improvements over baselines.",
            "uuid": "e8835.0",
            "source_info": {
                "paper_title": "Enhance Graph Alignment for Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "NeighborDetail",
            "name_full": "Neighbor Detail Template (fixed-size sampled computational tree)",
            "brief_description": "A graph-to-token graph description template that builds a fixed-size computational tree centered at a node, samples a fixed number of neighbors per hop, performs a level-order traversal to produce a fixed-length node sequence suitable as graph tokens for LLM input.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Neighbor Detail Template",
            "representation_description": "Construct an h-hop computational tree centered on the target node; for each hop, sample a fixed number of neighbors (fill with pad tokens if fewer exist); perform level-order traversal to obtain a fixed-length ordered sequence of node tokens; use these tokens as the graph-token sequence input to the projector and LLM.",
            "graph_type": "Text-attributed graphs (citation networks such as Cora, Pubmed, Arxiv; social networks like Instagram used in experiments).",
            "conversion_method": "For each center node, sample a fixed number of neighbors per hop (e.g., 10 per hop in experiments), pad missing positions with pad tokens (zero vectors), then perform level-order traversal to a fixed-length node token sequence (the center node as first token).",
            "downstream_task": "Used as the graph-description input for node classification, link prediction and the self-supervised text-matching pretext during instruction tuning.",
            "performance_metrics": "Used as part of GALLM results listed under graph-to-token (see node classification and link prediction numbers in previous entry). The experiments used a fixed sample size of 10 per hop and maximum input length 2048.",
            "comparison_to_others": "Compatibility study: the paper also tested an alternative template (Text-Graph Grounding pre-trained GNNs from GraphGPT) and reported that GALLM improves performance across different templates (Figure 4), indicating this template is compatible but not uniquely superior in the GALLM pipeline.",
            "advantages": "Produces a fixed-length, structured node sequence that captures local neighborhood information for LLM consumption; straightforward to implement and deterministic; handles variable-degree nodes via sampling and padding.",
            "disadvantages": "Fixed sampling may omit relevant neighbors (information loss) or introduce sampling bias; padding introduces artificial tokens; sequence order may not capture richer graph connectivity beyond the sampled tree view.",
            "failure_cases": "Not explicitly enumerated, but potential failure modes include poor representation when important neighbors are omitted by sampling, and sensitivity to the chosen sample size/hop parameter that can under- or over-represent local structure.",
            "uuid": "e8835.1",
            "source_info": {
                "paper_title": "Enhance Graph Alignment for Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "text-graph-grounding-template",
            "name_full": "Text-Graph Grounding Pre-trained GNNs template (GraphGPT-style)",
            "brief_description": "A graph description template that samples subgraphs and uses a pre-trained GNN aligned with text via contrastive/text-graph grounding to produce pre-aligned graph token embeddings for LLM instruction tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Text-Graph Grounding Template (GraphGPT)",
            "representation_description": "Sample a subgraph centered on a node, encode nodes/subgraph with a pre-trained GNN that was text-graph grounded (contrastive alignment between node features and textual representations), producing graph-token embeddings already partially aligned to textual embedding space for downstream instruction tuning.",
            "graph_type": "Text-attributed graphs (citation datasets used in compatibility experiments).",
            "conversion_method": "Subgraph sampling followed by GNN encoding; the GNN has been pre-trained with a text-graph contrastive grounding objective so its outputs are compatible with LLM text embeddings.",
            "downstream_task": "Node classification and other graph tasks in compatibility experiments (used in Figure 4).",
            "performance_metrics": "The paper reports that GALLM 'boosts model performance across different templates and base models' when this template is used (compatibility evaluation), but does not provide standalone numeric metrics for this template alone in isolation from the rest of the pipeline.",
            "comparison_to_others": "Compared qualitatively in a compatibility test against the Neighbor Detail Template; GALLM improvements persist under both templates, indicating model-level gains are not template-specific.",
            "advantages": "Pre-aligns graph token representations with text representations via contrastive grounding, potentially simplifying downstream projector/alignment training.",
            "disadvantages": "Relies on pre-training of a graph encoder and the availability of text-graph contrastive pretraining resources; complexity increases due to pretraining step.",
            "failure_cases": "Not explicitly reported; general limitations include dependency on quality of pre-training and possible mismatches between pretraining objectives and downstream discriminative templates if not aligned.",
            "uuid": "e8835.2",
            "source_info": {
                "paper_title": "Enhance Graph Alignment for Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "text-matching",
            "name_full": "Text Matching self-supervised task",
            "brief_description": "A discriminative self-supervised pretext introduced in this paper where the LLM is trained to select the most relevant candidate text (the central node's textual attribute) given a node-centered subgraph and a candidate set of texts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "text matching task (self-supervised)",
            "representation_description": "Given the graph-token representation of a node-centered subgraph and a set of candidate texts (one positive = center node's text, several negatives), the LLM must choose the correct positive text. Negative sampling mixes easy negatives (non-neighbors) and hard negatives (neighbor nodes' texts).",
            "graph_type": "Text-attributed graphs where nodes have textual attributes (paper titles/abstracts, user meta-data).",
            "conversion_method": "Use the Neighbor Detail Template (or equivalent) to produce a node sequence for the center node; obtain candidate texts from node textual attributes; sample negative candidates with a mix of hard negatives from neighbors and easy negatives from non-neighbors (experiments used 3 hard and 7 easy negatives per positive); formulate an instruction prompt requiring selection among candidate texts; train by maximizing likelihood of correct selection.",
            "downstream_task": "Designed to transfer to discriminative downstream tasks such as node classification and link prediction by aligning pretext template with downstream templates (where candidate texts are replaced with candidate labels).",
            "performance_metrics": "Self-supervised (zero-shot) performance: GALLM-mp (text matching only, no supervised fine-tuning) — Cora 0.5804, Pubmed 0.4791, Arxiv 0.1309 (Table 4), substantially higher than GraphGPT (e.g., GraphGPT Cora 0.0776) and LLaGA in the same setting. Ablation: removing text-matching (w/o tm) reduces supervised performance (e.g., GALLM-mp Cora Acc from 0.8965 to 0.8724; see Table 6).",
            "comparison_to_others": "Contrasted with other self-supervised tasks used in prior LLM-for-graph works: node description (LLaGA) and graph matching (GraphGPT). Those tasks are generative/reordering and were observed to cause negative transfer for discriminative downstream tasks; text-matching (discriminative template) aligned better and led to positive transfer.",
            "advantages": "Template-alignment with discriminative downstream tasks yields better knowledge transfer, improved zero-shot and supervised performance; explicitly leverages node textual attributes and LLMs' strengths in text understanding; robustness improved by mixing hard and easy negatives.",
            "disadvantages": "Requires informative textual attributes for nodes (poor/noisy text will reduce effectiveness); needs careful negative sampling design; design assumes downstream tasks can be expressed in selection/discriminative templates.",
            "failure_cases": "Although improved versus baselines, zero-shot performance remains dataset-dependent (e.g., Arxiv zero-shot still low 0.1309). If textual attributes are absent or noisy, task utility will likely drop. Negative transfer observed with other pretexts underscores requirement for template alignment.",
            "uuid": "e8835.3",
            "source_info": {
                "paper_title": "Enhance Graph Alignment for Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "category-manual-prompt",
            "name_full": "Category-enhanced Manual Prompt Tuning",
            "brief_description": "A task-specific tuning method that augments discrete category labels with explanatory natural-language sentences (manually written or generated by a strong LLM) to provide sentence-level category context in the downstream instruction prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "category-enhanced manual prompt",
            "representation_description": "Append an explanatory sentence to each candidate category in the instruction prompt (e.g., 'category: case-based, which analyzes real-world cases to draw conclusions;'). Explanations are manually authored or generated by an external LLM and included verbatim in prompts during task-specific tuning.",
            "graph_type": "Used on text-attributed graphs for downstream discriminative tasks (node classification, link prediction reformulated as binary classification).",
            "conversion_method": "During Stage 2 task-specific tuning, include category explanatory sentences in the instruction text concatenated with candidate category labels; feed the combined instruction + projected graph tokens into the frozen LLM and fine-tune the projector (manual prompt case tunes projector only).",
            "downstream_task": "Node classification (primary), link prediction (binary categories enhanced).",
            "performance_metrics": "GALLM-mp (manual prompt) supervised node-classification results: Cora Acc 0.8965 F1 0.8934; Pubmed Acc 0.9115 F1 0.9105; Arxiv Acc 0.7330 F1 0.5664; Instagram Acc 0.6460 F1 0.5583. Ablation removing category prompts (w/o cp) reduced performance (example: Cora Acc drops to 0.8780, F1 to 0.8667; see Table 6).",
            "comparison_to_others": "Compared with category-enhanced soft prompts: manual prompts outperform soft prompts when labeled data is scarce (Cora, Pubmed, Instagram), while soft prompts can outperform manual prompts on large-label datasets like Arxiv.",
            "advantages": "Distills external knowledge into category labels, improves LLM comprehension of categories without adding trainable parameters (only projector tuned), effective in low-data regimes.",
            "disadvantages": "Discrete manual prompts can be brittle — small wording changes may notably affect performance; prompt creation requires human expertise or extra LLM calls to generate explanations.",
            "failure_cases": "Performance is sensitive to prompt wording (manual prompt instability cited). If explanatory text is incorrect or misleading, downstream performance can suffer. Manual prompts may not scale well across many tasks or label sets.",
            "uuid": "e8835.4",
            "source_info": {
                "paper_title": "Enhance Graph Alignment for Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "category-soft-prompt",
            "name_full": "Category-enhanced Soft Prompt Tuning",
            "brief_description": "A continuous prompt tuning method where category explanatory content is represented by trainable virtual tokens whose embeddings are encoded by a small soft-prompt encoder and optimized (alongside the projector) via backpropagation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "category-enhanced soft prompt",
            "representation_description": "Replace discrete explanatory sentences with a small number of virtual soft prompt tokens per category (e.g., 3 tokens); initialize embeddings randomly; pass them through a soft prompt encoder (two-layer MLP) and concatenate the output with discrete prompt tokens and projected graph tokens as LLM input; train soft prompt embeddings and encoder (and projector) via gradient descent.",
            "graph_type": "Text-attributed graphs for node classification and link prediction.",
            "conversion_method": "During Stage 2, insert virtual tokens (e.g., '&lt;soft&gt;' tokens) after each category label in the instruction; the embeddings are trainable and processed by a small encoder; the pipeline tunes these embeddings and encoder parameters jointly with the graph projector.",
            "downstream_task": "Node classification and link prediction.",
            "performance_metrics": "GALLM-sp (soft prompts) supervised node-classification: Cora Acc 0.8854 F1 0.8757; Pubmed Acc 0.9087 F1 0.9086; Arxiv Acc 0.7391 F1 0.5775; Instagram Acc 0.6274 F1 0.5203. Soft prompts perform better than manual prompts on large-label dataset Arxiv (Arxiv Acc 0.7391 vs manual 0.7330). Ablations show removing self-supervised text-matching reduces soft-prompt performance as well.",
            "comparison_to_others": "Compared to manual category prompts: soft prompts introduce trainable parameters and can adapt to abundant labeled data, producing stronger performance on large datasets, while manual prompts are preferable in low-label regimes.",
            "advantages": "Learns explanatory category context automatically (no manual prompt design), more stable to small wording changes, adaptable via gradient updates and beneficial when sufficient labels are available.",
            "disadvantages": "Introduces additional trainable parameters and thus needs more labeled data to realize gains; increases training complexity (soft prompt encoder, embeddings) and may overfit on small datasets.",
            "failure_cases": "Per experiments, soft prompts are less effective than manual prompts in low-data settings (Cora, Pubmed, Instagram). Removal of category prompts reduces performance (w/o cp ablation).",
            "uuid": "e8835.5",
            "source_info": {
                "paper_title": "Enhance Graph Alignment for Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "graph-projector",
            "name_full": "Graph Projector (placeholder replacement + linear projector)",
            "brief_description": "A lightweight mapping module (typically a linear layer) that projects graph token embeddings into the LLM text embedding space and replaces specified LLM placeholder token embeddings with the projected graph embeddings.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "graph projector (linear layer) + placeholder substitution",
            "representation_description": "A learned projector (linear layer in experiments) maps graph-derived embeddings (e.g., Sentence-BERT node embeddings or pre-trained GNN outputs) to the dimensionality and space of LLM text embeddings; during input construction, special placeholder tokens in the instruction embedding sequence are replaced with these projected graph embeddings.",
            "graph_type": "Used with text-attributed graphs for node-level tasks in the paper's LLM-based pipeline.",
            "conversion_method": "Encode node textual attributes with Sentence-BERT (or pre-trained GNN outputs under alternative templates), then apply the projector to obtain embeddings compatible with the LLM embedding layer; construct instruction text embeddings with placeholder positions; substitute placeholders with projected graph embeddings to form model input.",
            "downstream_task": "Node classification, link prediction, self-supervised text-matching and instruction tuning stages.",
            "performance_metrics": "GALLM training strategy typically freezes LLM backbone and fine-tunes only the projector (and soft prompts where applicable). The reported GALLM performance (see other entries) is achieved with this lightweight tuning approach — e.g., strong supervised results (GALLM-mp and GALLM-sp numbers in Table 2) while only tuning the projector in Stage 1 and Stage 2 (manual prompt); soft prompt tuning adds soft prompt embeddings and encoder to tuned parameters.",
            "comparison_to_others": "Contrasted with full LLM fine-tuning approaches: projector-only tuning is parameter-efficient and yields strong gains in the paper's experiments; using a pre-trained GNN + grounding template is another path (GraphGPT style) but still requires a mapping/alignment step.",
            "advantages": "Parameter-efficient (freezes large LLM weights), computationally cheaper to tune, simplifies alignment by learning a small mapping into LLM embedding space, supports modular replacement of graph encoders.",
            "disadvantages": "May limit ultimate performance compared to full-model fine-tuning if projector capacity is insufficient; success depends on the projector's capacity and quality of the input graph embeddings.",
            "failure_cases": "Not explicitly enumerated; ablation studies show that tuning only the projector without aligned pretexts (i.e., omitting text-matching or category prompts) reduces performance, indicating that projector tuning needs compatible task templates to be effective.",
            "uuid": "e8835.6",
            "source_info": {
                "paper_title": "Enhance Graph Alignment for Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GraphGPT: Graph instruction tuning for large language models",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "Llaga: Large language and graph assistant",
            "rating": 2,
            "sanitized_title": "llaga_large_language_and_graph_assistant"
        },
        {
            "paper_title": "Graphtext: Graph reasoning in text space",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data?",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data"
        },
        {
            "paper_title": "Zerog: Investigating cross-dataset zero-shot transferability in graphs",
            "rating": 2,
            "sanitized_title": "zerog_investigating_crossdataset_zeroshot_transferability_in_graphs"
        }
    ],
    "cost": 0.020819499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhance Graph Alignment for Large Language Models
15 Oct 2024</p>
<p>Haitong Luo 
Xuying Meng 
Tianxiang Zhao 
Fali Wang 
Hanyun Cao 
Yujun Zhang 
Suhang Wang </p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>Suhang Wang
Pennsylvania State University State College
USA</p>
<p>Pennsylvania State University State College
USA</p>
<p>Pennsylvania State University State College
USA</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>Haitong Luo
Xuying Meng
Suhang Wang, Tianxiang ZhaoFali Wang</p>
<p>Conference'17, July 2017WashingtonDCUSA</p>
<p>Enhance Graph Alignment for Large Language Models
15 Oct 2024251DAC6B06D2341EEDF3A4970FE68C6AarXiv:2410.11370v1[cs.CL]Large Language ModelsGraph MiningGraph Foundation Models
Graph-structured data is prevalent in the real world.Recently, due to the powerful emergent capabilities, Large Language Models (LLMs) have shown promising performance in modeling graphs.The key to effectively applying LLMs on graphs is converting graph data into a format LLMs can comprehend.Graph-to-token approaches are popular in enabling LLMs to process graph information.They transform graphs into sequences of tokens and align them with text tokens through instruction tuning, where self-supervised instruction tuning helps LLMs acquire general knowledge about graphs, and supervised fine-tuning specializes LLMs for the downstream tasks on graphs.Despite their initial success, we find that existing methods have a misalignment between self-supervised tasks and supervised downstream tasks, resulting in negative transfer from selfsupervised fine-tuning to downstream tasks.To address these issues, we propose Graph Alignment Large Language Models (GALLM) to benefit from aligned task templates.In the self-supervised tuning stage, we introduce a novel text matching task using templates aligned with downstream tasks.In the task-specific tuning stage, we propose two category prompt methods that learn supervision information from additional explanation with further aligned templates.Experimental evaluations on four datasets demonstrate substantial improvements in supervised learning, multi-dataset generalizability, and particularly in zero-shot capability, highlighting the model's potential as a graph foundation model.</p>
<p>INTRODUCTION</p>
<p>The Web has become a universal data repository, linking diverse entities to form intricate graphs.This graph-structured data is crucial in our daily lives, driving applications like Web mining [2,52], content recommendation [31,53], and anomaly detection [30,39].While graphs effectively represent complex interactions, they pose significant challenges in analysis and modeling due to their inherent irregularity and diverse connectivity patterns.</p>
<p>Recently, Large Language Models (LLMs), such as GPTs [1], LLaMA [41] and Claude [33], have emerged as a dominant force in AI research due to their remarkable generative ability.Trained on vast amounts of text data, these models have demonstrated exceptional performance not only in natural language processing tasks [8,34], but also in fields beyond texts such as computer vision [56], recommendation systems [3], and speech recognition [10].The emergent capabilities of LLMs also inspire researchers to apply LLMs to text-attributed graphs, aiming to develop a graph foundation model that functions effectively across multiple scenarios [28].The key to employing LLMs for graph data lies in aligning the graph data with natural language/text embedding space so that LLMs can comprehend the graph [28].</p>
<p>Several efforts have been taken on the alignment of graph data, which can be generally categorized into two categories: graphto-text [5,9,11,27,43,59] and graph-to-token [4,23,40,55] approaches.Graph-to-text methods use natural language to describe graphs.However, such simple strategies often suffer from redundancy and imprecise characterization due to the manual setup of description/prompt templates.In contrast, graph-to-token methods convert graph information into sequences of tokens and then obtain their embeddings.They treat these graph tokens as a new modality, and project these tokens to align the feature space with text embeddings, facilitating the understanding of LLM on graphs.Though graph-to-token methods alleviate the imprecise characterization issue, existing works are suboptimal for alignment.To align new modality tokens, a two-stage instruction tuning paradigm serves as a prevalent strategy in existing works [40,55]: (i) In Stage 1 (i.e., the self-supervised tuning stage), LLMs are finetuned using self-supervised tasks to align new modality tokens with text tokens, allowing them to acquire more domain-related knowledge without being constrained by the amount of supervised data; and (ii) In Stage 2 (i.e., the task-specific tuning stage), the LLM is fine-tuned on specific downstream tasks, thereby infused with task-related knowledge.For example, InstructGLM [55] employs a self-supervised link prediction task to enhance the downstream node classification tasks.GraphGPT [40] proposes graph matching tasks in Stage 1 to boost the performance of downstream node classification and link prediction tasks.LLaGA [4] adopts node description as the self-supervised task.However, the above methods all fall short in the two-stage instruction tuning paradigm, as they overly rely on supervised task-specific tuning and struggle to benefit from self-supervised tuning.</p>
<p>Self-supervised tasks proposed by existing methods [4,40,55] differ significantly from downstream tasks, making it challenging for models to benefit from self-supervised tuning, and even leading to negative transfer [44].As shown in Figure 1, we conduct a preliminary study to compare performance based on different self-supervised tasks, with Vicuna-7B-v1.5-16Kas the base LLM model on Cora [54] and Pubmed [13] datasets.Except for the base model Vicuna without self-supervised tuning, we utilize two abovementioned representative self-supervised tasks, i.e., graph matching (w/ gm) [40] and node description (w/ nd) [4].All models are instruction-tuned for the node classification task, differing in their self-supervised tuning stages.From Figure 1, we can observe that the performance of the models decreases after self-supervised tuning with either node description or graph matching.This negative transfer phenomenon occurs due to the inconsistency between the templates and training objectives of self-supervised tasks and those of the downstream tasks.Specifically, node description is a generative task that requires generating descriptive text for nodes, while graph matching involves reordering nodes based on given texts.In contrast, the downstream node classification task is a discriminative task that requires the LLM to select the correct category from a set of candidate categories.</p>
<p>Therefore, how to enable models to benefit from pretraining tasks in downstream tasks is an important research area.Recent studies [32] in natural language processing suggest that aligning the templates of pretext tasks with those of downstream tasks enhances knowledge transfer.Inspired by this discovery, we propose to enhance graph alignment for LLMs in both self-supervised tuning and task-specific tuning.However, enhancing the alignment poses two challenges: (i) In Stage 1, how to design a self-supervised task that not only aligns its template with downstream tasks but also accommodates the characteristics of LLMs and graph datasets?and (ii) In Stage 2, how to design prompts to ensure downstream tasks are aligned with self-supervised tasks to benefit from pretexts?</p>
<p>In an attempt to address these challenges, we approach the problem from both two stages, aligning the task templates of each stage with those of the other.In the self-supervised tuning stage, to enhance the alignment while leveraging the strengths of LLMs and the characteristics of graph-structured datasets, a new self-supervised task should be proposed.Since downstream tasks for graphs are typically discriminative (e.g., node classification, link prediction), the self-supervised task should be designed in a similar discriminative manner.Also, LLMs exhibit strong ability in natural language processing, and in text-attributed graphs, each node is attributed with texts, these characteristics inspire us to design a task focused on helping LLMs understand the textual attributes of nodes.Therefore, based on these considerations, we propose the self-supervised task, i.e., text matching.In detail, the text matching task trains the LLM to select the most relevant texts from a set of candidates based on the given graph-structured data.Through this task, models comprehend graph-structured data in a discriminate manner, potentially enhancing its performance on downstream tasks.</p>
<p>In the task-specific tuning stage, we introduce category prompt methods to further align with the self-supervised task template and provide additional information to help the LLM understand the task.Specifically, we offer explanatory text for each candidate category, concatenated with its corresponding category.By transforming candidate information from a word level to a sentence level, the templates of downstream tasks are better aligned with the pretexts.We propose two methods: category manual prompt and category soft prompt.In the category manual prompt method, the explanatory text is generated either manually or by a powerful LLM (e.g., GPTs [1]).This externally generated text helps distill external knowledge into the models.In the category soft prompt method, we replace the manually constructed explanatory text with virtual soft prompt tokens, whose embeddings can be further automatically updated through backpropagation.This allows the LLM to learn the content of the category prompts by itself, avoiding the redundant process of manual prompt design.During the two-stage tuning process, we employ a lightweight fine-tuning approach where we freeze the parameters of LLMs and only fine-tune a few parameters (i.e., MLP projectors and several soft prompt token embeddings).By achieving alignment between the two stages, we boost the performance on downstream tasks.Our contributions are:</p>
<p>• We point out that existing graph-to-token approaches for LLMs suffer from the misalignment between self-supervised training and task-specific tuning; • We propose a novel framework, GALLM, which aligns task templates between self-supervised tuning and task-specific tuning, enabling models to benefit from pretexts; and • Extensive experiments on supervised learning, generalizability, and particularly zero-shot capability demonstrate GALLM's effectiveness and its potential as a graph foundational model.</p>
<p>RELATED WORK</p>
<p>In this section, we will review related works about learning on graphs and Large Language Models for Graphs.</p>
<p>Learning on Graphs</p>
<p>Graph-structured data is prevalent in many aspects, such as social recommendations [31,53], molecular structures [45,46], and intelligent transportation [35].However, effectively analyzing and modeling graphs poses significant challenges due to their inherent irregularity and diverse connectivity patterns.Graph Neural Networks (GNNs) [7,21,42] have emerged as a powerful framework for learning from graph data, which enable information propagation and feature extraction through iterative message passing among neighboring nodes.For example, GCN [21] applies a convolutional operation to nodes and their neighbors, enabling the model to learn from local neighborhoods.Furthermore, GAT [42] weighs the importance of neighboring nodes dynamically, focusing on relevant information.Typically, GNNs operate under a supervised setting, where models are trained for a specific task on the input graph and make inferences on the same graph.However, the difficulty of obtaining labeled data [4] limits the performance.To address this issue, some research [15,18,50,51,57,60] has explored selfsupervised learning on graph data.For example, GraphCL [57] and GCA [60] employ contrastive learning as the self-supervised tasks, while GraphMAE [15] and AutoCF [51] leverage generative tasks.Though they have made progress, their reliance on simple GNN architectures limits the models' generalizability and versatility.</p>
<p>Large Language Models for Graphs</p>
<p>In recent years, LLMs [1,33,41] have gained widespread attention for their remarkable capabilities in various NLP tasks.Furthermore, LLMs have expanded their applications beyond NLP [3,10,48].This emergent ability has inspired researchers to explore the use of LLMs in graph-structured data learning [28].One significant branch of research in this area is the LLM-as-predictor approach, which utilizes LLMs as the core and sole backbone for graph learning.These methods convert graph-structured data into formats compatible with LLMs and input them alongside natural language prompts.The key challenge in employing LLMs for graph data lies in aligning the graph data with formats that LLMs can comprehend.Based on the alignment of graph data, these LLM-as-predictor methods can be categorized into two groups: graph-to-text [5,9,11,27,43,59] and graph-to-token [4,40,55] approaches.Graph-to-text methods employ natural language to describe graph data.For instance, LLMtoGraph [27] and NLGraph [43] utilize node or edge lists as input formats for graphs.GraphText [59] bridges the gap between graphs and texts by translating graph structures into text with a graph-syntax tree.Moreover, GPT4Graph [11] and "Talk like a Graph" [9] introduce different graph description templates to transform graphs into languages.However, these methods are limited by the manual setup of templates to describe graphs, leading to redundancy, incomplete representation, and imprecise characterization of graphs' intrinsic features.</p>
<p>Contrastingly, graph-to-token methods attempt to convert graph information into sequences of tokens, which are then concatenated with the embeddings of natural language prompts and input into LLMs.To align graph tokens with text tokens, these methods utilize a projector to map graph tokens into text space through proposed various instruction tuning tasks.For example, InstuctGLM [55] introduces self-supervised link prediction tasks to enhance downstream node classification.GraphGPT [40] employs a twostage instruction tuning approach: in Stage 1, it introduces a graph matching task designed to align the order of graph tokens with text tokens; in Stage 2, it applies task-specific tuning to improve performance on downstream node classification and link prediction tasks.LLaGA [4] proposes three different tasks (i.e., node classification, link prediction, and node description) for mixed instruction tuning.</p>
<p>Although the tasks proposed in these studies have made some progress, they overly rely on supervised instruction tuning and struggle to leverage self-supervised tuning effectively, resulting in limited performance.Specifically, the self-supervised tasks introduced by previous methods differ significantly from downstream tasks, making it challenging for models to benefit from self-supervised tuning, and potentially leading to negative transfer.To address this issue, our model aligns the task templates between the two stages, enhancing performance on downstream tasks through the advantages of self-supervised tasks.</p>
<p>METHODOLOGY</p>
<p>In this section, we detail our proposed framework, illustrated in Figure 2. We utilize instruction tuning to fine-tune LLMs, employing a tuned graph projector to map graph embeddings into the textual space through various tasks (upper left of Figure 2).Our tuning process follows a two-stage paradigm: self-supervised tuning in Stage 1 and task-specific tuning in Stage 2. In Stage 1, we introduce the text matching task aligned with downstream tasks (bottom left of Figure 2).In Stage 2, we present two prompt tuning methods (i.e., manual prompts and soft prompts) to further align the task template and enhance category information (right side of Figure 2).</p>
<p>Notations</p>
<p>A graph-structured data consists of a number of entities and the connecting relationships between them, where entities can be modeled as nodes in the graph, and connecting relationships can be modeled as edges.Formally, given a graph-structured data G = {V, X, E}, V is the set of nodes { 1 , ...,   } , and E = {   } is the set of undirected edges.In this paper, we focus on the text-attributed graph, where nodes are attributed with raw textual contents C = {c  |1 ≤  ≤  }, with c  being the text associated with node   .These textual attributes can be further encoded into −dimensional embeddings X ∈ R  × using pre-trained language models.</p>
<p>When conducting node-level tasks with LLMs, e.g., node classification and link prediction, we usually sample subgraphs centered at nodes as input to LLMs to (i) keep enough graph structure information of target nodes; and (ii) avoid redundant information and large computational cost for handling the whole graph.Formally, we denote
G ℎ 𝑠𝑢𝑏 (𝑣) = (V ℎ 𝑣 , E ℎ 𝑣 , X ℎ 𝑣 )
as the subgraph around node , consisting of ℎ-hop neighbor nodes of  and all interconnection edges.We also denote
N ℎ (𝑣) = {𝑣 ′ |𝑣 ′ ∈ V ℎ 𝑣 , 𝑣 ′ ≠ 𝑣 } as the set of ℎ-hop neighbor nodes in G ℎ 𝑠𝑢𝑏 (𝑣).
With these notations, we will sequentially introduce our instruction tuning pipeline.</p>
<p>Instruction Tuning Pipeline Design</p>
<p>Instruction tuning is an effective method for incorporating additional knowledge into LLMs when applied to specific domains [58].In this work, we employ a two-stage instruction tuning process.To comprehensively represent graph structure information and ensure adaptability to various tasks, we design a uniform instruction tuning pipeline, which is shown in the upper left part in Figure 2. In detail, we utilize generative LLMs as backbones which follow the auto-regressive paradigm and generate the next token as:
𝑃 𝜃 (Y 𝑗 |X, Y &lt; 𝑗 ) = 𝐿𝐿𝑀 𝜃 (X, Y &lt; 𝑗 ),(1)
where X is the input sequence, and  (•) is the Large Language Model we utilize.Y is the target sequence, Y &lt;  is the token sequence before token  and Y  is the target token .With all the target sequence tokens, the training loss can be formulated as:
L 𝜃 = − |Y| ∑︁ 𝑗=1 log 𝑃 𝜃 (Y 𝑗 |X, Y &lt; 𝑗 ).(2)
The core of instruction tuning lies in the design of X and Y.As shown in Figure 3, we further subdivide X into two components: graph information X ℎ and task-specific instructions X  .</p>
<p>Given that LLMs are trained on linguistic data, the graph information should be translated into a format that can be understood by LLMs.We formulate the graph translation procedure as follows:
X 𝑔𝑟𝑎𝑝ℎ = T (G ℎ 𝑠𝑢𝑏 (𝑣)) = T (V ℎ 𝑣 , E ℎ 𝑣 , X ℎ 𝑣 ),(3)
where T (•) is the graph description template, which turns the graph-structured data into sequences of tokens.Previous work [4,40,55] has explored different description templates for translating graph information and here we use the Neighbor Detail Template [4] since it provides an in-depth view of the central node and its surroundings.We also evaluate the compatibility with other graph description templates in Section 4.6.Specifically, in the Neighbor Detail Template, for a given node , we first construct a fixed-size, sampled computational tree centered around  to represent the subgraph.In this tree, node  serves as the root, with its 1-hop neighbors as its children, and the 2-hop neighbors as the children of the 1-hop neighbors, and so forth.For each node, a fixed number of neighbors is considered.For nodes with more neighbors than the specified size, we sample their neighbors; for nodes with fewer neighbors, we supplement them with placeholder nodes.We then conduct a level-order traversal of the computational tree, transforming the subgraph into a fixed-length node sequence.In this way, we translate the graph structure into a node sequence.For example, for the subgraphs in Figure 2, if we set the fixed sample number to 3, the node sequence X ℎ = {x  , x  , x  , x  , x  , x  , x  , x  , x  , x  , x  , x  , x  }, where x  denotes embedding of the pad token and is set to a zero vector.</p>
<p>After obtaining the node sequences, we use a projector to map the graph embedding to the text embedding space, thus allowing LLMs to understand the graph information:
X ′ 𝑔𝑟𝑎𝑝ℎ = 𝑃𝑟𝑜 𝑗𝑒𝑐𝑡𝑜𝑟 (X 𝑔𝑟𝑎𝑝ℎ ).(4)
Typically, we use a linear layer as the projector.With the projected graph embeddings, we concatenate them with the text embeddings of task-specific instruction to form the input sequence for the LLM:
X = 𝐶𝑜𝑛𝑐𝑎𝑡𝑒𝑛𝑎𝑡𝑒 (X 𝑡𝑎𝑠𝑘 , X ′ 𝑔𝑟𝑎𝑝ℎ ).(5)
Here x  denotes the embeddings of the instruction texts encoded by the LLM's embedding layer.In detail, when inputting instruct texts into the LLM, we use placeholder tokens to fill the positions of the graph tokens.After obtaining the text embeddings X  , we replace the embeddings of these placeholder tokens with the graph embeddings X ℎ .The design of effective instruction tuning tasks hinges on definitions of task-specific instructions X  and target sequences Y.We propose a two-stage tuning process that follows the uniform pipeline, differing only in X  and Y. Specifically, we introduce the text matching task for Stage 1 in Section 3.3 and the category prompt methods for Stage 2 in Section 3.4.</p>
<p>To streamline the tuning process, we use a lightweight strategy.During instruction tuning, we freeze the LLM backbone parameters and optimize only a few.Specifically, in both Stage 1 (i.e., Section 3.3) and Stage 2 of the category manual prompt approach (i.e., Section 3.4.1),we fine-tune only the graph projector parameters.In Stage 2 of the category soft prompt approach (i.e., Section 3.4.2),we fine-tune the graph projector, soft prompt embeddings, and soft prompt encoder.This minimal parameter tuning compared to the LLM backbone significantly enhances efficiency.The following sections detail our proposed tasks for both two stages.</p>
<p>Self-Supervised Instruction Tuning</p>
<p>Self-supervised fine-tuning leverages extensive unlabeled data, enabling LLMs to acquire knowledge of graph structures, thereby enhancing the model's performance on downstream tasks.However, the design of self-supervised tasks involves two critical considerations: (i) the template of self-supervised tasks should be aligned with that of downstream tasks to facilitate knowledge transfer, and (ii) the self-supervised tasks must be compatible with the characteristics of LLMs and text-attributed graph datasets.For the first consideration, as downstream tasks for graphs are typically discriminative (e.g., node classification, link prediction), self-supervised tasks should also be designed in a similar discriminative manner.For the second consideration, LLMs exhibit a strong capacity for understanding texts, and in text-attributed graphs, each node is associated with textual features.This motivates us to utilize the textual attributes of nodes in designing the self-supervised task.Based on these considerations, we propose a novel self-supervised task: text matching.</p>
<p>3.</p>
<p>3.1 Self-Supervised Task Design.In the text matching task, we provide graph data and candidate texts to the LLM, requiring it to select the most relevant text for the input graph data.The nodes' textual attributes (e.g., paper titles in the citation network dataset) serve as these candidate texts.Among these, the positive text (i.e., ground truth) corresponds to the textual attribute of the central node; while the negative texts are derived from the textual attributes of randomly sampled nodes.For example, as shown in the bottom left part of Figure 2, the task-specific instruction for citation network can be "Given a node-centered graph: <node sequence>, each node represents a paper.The first token represents the central node of the subgraph and the remaining represents the neighbors.We need to classify the center node into one of the 10 titles: title: c  ; title: c The text matching task aims to select correct text attributes (e.g., titles) for each central node, thereby enhancing LLMs' understanding of graph data.Moreover, since downstream tasks are formulated in a similar discriminative manner, our model requires only a simple modification of the content to be matched during downstream tasks, changing candidate texts to candidate labels.For node classification tasks, candidate labels represent nodes' categories; while for link prediction tasks, they indicate connection presence or absence.Thus, this template alignment helps transfer knowledge from the text matching task to downstream tasks more effectively.</p>
<p>Mixing Negative Sampling Strategy.</p>
<p>Previous works [19,37] have demonstrated that incorporating negative samples of varying difficulty levels can enhance the model's robustness.Inspired by this, we categorize our negative text samples into two types: easy negative samples and hard negative samples.Generally, the textual attributes of neighboring nodes tend to be quite similar.For instance, in a citation network, a paper titled "Hard negative mixing for contrastive learning" is cited by a paper titled "Contrastive Learning with Hard Negative Samples", which results in these two papers being neighboring nodes in the graph, with similar titles.Therefore, when constructing negative samples, we obtain hard negative samples by sampling from neighboring nodes as:
C ℎ𝑎𝑟𝑑 −𝑛𝑒𝑔 ∼ 𝑆𝑎𝑚𝑝𝑙𝑒 (N ℎ 𝑣 ).(6)
We sample easy negative samples from non-neighboring nodes:
C 𝑒𝑎𝑠𝑦−𝑛𝑒𝑔 ∼ 𝑆𝑎𝑚𝑝𝑙𝑒 (V \ N ℎ 𝑣 ).(7)
The final negative sample set is C  = {C − , C ℎ − }.Generally, easy negative samples help the model develop basic graph processing abilities, while hard ones further enhance these</p>
<p>Soft Prompt Encoder</p>
<p>Frozen LLM category: case-based, <virtual token> <virtual token> Tuned Figure 3: Illustration of category-enhanced soft prompt tuning.We show only the category prompt, keeping other parts (e.g., graph data input format) unchanged.In this tuning, the parameters of virtual token embeddings, soft prompt encoder, and graph projector are tuned.</p>
<p>skills.Thus, we include more easy negative samples than hard ones to ensure the model maintains its fundamental abilities while improving its capacity to distinguish challenging samples.</p>
<p>Task-specific Instruction Tuning</p>
<p>In the second stage, we propose task-specific instruction tuning, which customizes LLM's reasoning behavior for downstream tasks.Specifically, the LLMs' reasoning process can be understood as follows: first, LLMs understand the input graph information and the task instructions.Subsequently, LLMs utilize internal parameterization knowledge to perform graph tasks based on the understanding of graph information and task instructions.For instance, in node classification tasks, LLMs first comprehend the input graph and category information, and then categorize the nodes into the most appropriate classes based on their internal parameterized knowledge.However, existing methods typically provide category information at the word level (i.e., the name of categories), which limits LLM's comprehension of categories.Additionally, in self-supervised tasks, the candidate texts are provided at the sentence level.Therefore, to enhance the LLM's understanding of category information and further align downstream task templates with pretexts, we introduce the category prompt tuning method.The goal of our category prompt method is to provide explanatory content for categories, reformulating category information from word levels to sentence levels.Specifically, we propose two category prompt methods: manual prompt tuning and soft prompt tuning.</p>
<p>3.4.1 Category-Enhanced Manual Prompt Tuning.In category-enhanced manual prompt tuning, we construct an explanatory sentence for each category and append it following the category in the input prompt.For instance, for citation networks, the input prompt for the category "case-based" is: "category: case-based, which analyzes real-world cases to draw conclusions;" where the italicized words represent the explanatory text we added.These explanatory texts are meticulously crafted with insights from external experts.Additionally, we harness the capabilities of robust general LLMs (e.g., Chat-GPT [1]) to automatically generate these explanations.This method effectively distills external knowledge into our model, thereby boosting its performance.For link prediction, we reformulate the task into a binary classification template and leverage manual prompts to enhance each category (i.e., connected and unconnected).The input prompts are depicted in the Figure 2. The complete instruction prompts and explanatory texts can be found in Appendix C.</p>
<p>Category-Enhanced Soft Prompt</p>
<p>Tuning.While manual prompts can incorporate external knowledge, the use of discrete manual prompts often results in unstable performance (e.g., changing a sinword in the prompt can lead to a significant drop in performance) [24,25,29].To address this issue, we propose an alternative approach: category-enhanced soft prompt tuning, which is illustrated in Figure 3. prompt methods [24,25,29] use trainable continuous prompt embeddings in concatenation with discrete prompts.In our methods, we replace category manual prompt tokens with virtual soft prompt tokens.For example, the input category text can be: "category: case-based, <soft prompt token> <soft prompt token>;".The embeddings of these virtual prompt tokens are initialized randomly.Subsequently, a soft prompt encoder (i.e., a two-layer MLP) is utilized to further encode these embeddings.After encoding, these continuous prompt embeddings are concatenated with the discrete prompt tokens and provided as input to the LLMs.The category soft prompts are updated via backpropagation to optimize the task objective, enabling the LLM to autonomously learn the explanatory context of categories.This can be formulated as min
𝜃 𝑔𝑝 ,𝜃 𝑠𝑝 L 𝜃 = − ∑︁ |Y| 𝑗=1 log 𝑃 𝜃 (Y 𝑗 |X, Y &lt; 𝑗 ),(8)
where   denotes the parameters of the graph projector, and   denotes the parameters of virtual token embeddings and the soft prompt encoder.Similarly, for link prediction, we utilize virtual soft prompt tokens to enhance each category.</p>
<p>EXPERIMENTS</p>
<p>In this section, we demonstrate the effectiveness of our proposed GALLM across multiple scenarios.Through the experiments, we aim to answer the following questions: (i) Q1: How does our framework GALLM perform compared to baseline models in traditional supervised graph learning settings?(ii) Q2: What is the zero-shot ability of our model, specifically with regard to self-supervised learning and cross-dataset settings?(iii) Q3: What is the multidataset generalization ability of our model across multiple datasets?(iv) Q4: How does each key module in GALLM contribute to enhancing performance?(v) Q5: How compatible is our GALLM with different graph descriptions and base LLMs (i.e., LLaMA 3)?</p>
<p>Experiment Setup</p>
<p>4.1.1Datasets.We choose four widely-used datasets: Cora [54], Pubmed [13], Arxiv [16] and Instagram [20].These datasets originate from different domains, with the first three derived from citation networks and the last from social networks.For citation network datasets, nodes represent academic papers and edges denote citation relationships.Each node is attributed with a title and an abstract and is categorized into distinct research topics.For the social network dataset Instagram, nodes represent users, and edges signify "following" relationships.Users are attributed with follow lists, personal introductions, and tags, and are categorized into two classes: normal users and commercial users.The statistics are presented in Tabel 1 and more details are in Appendix A.3.</p>
<p>Baselines and Implementation Details.</p>
<p>To evaluate our proposed GALLM, we compare it with various models grouped into four categories: (i) Traditional GNN models, including GCN [21], GAT [42], GraphSage [12], and SGC [47]; (ii) Transformer-based models, including UniMP [38] and NodeFormer [49]; (iii) Self-supervised GNNs, including GraphCL [57] and SimGRACE [50]; (iv) State-ofthe-art LLMs for graphs, including GraphGPT [40] and LLaGA [4].</p>
<p>For zero-shot ability assessment, we further include ZeroG [26], which focuses on cross-dataset zero-shot performance.We exclude methods like TAPE [14] and LLM-GNN [6] since they belong to a different branch.These methods are highly task-specific, following a per-model-per-dataset manner and relying on additional explanations, whereas we aim to develop a graph foundation model that operates across multiple scenarios.</p>
<p>For GALLM, we adopt Vicuna-7B-v1.5-16Kas our foundation base model and utilize a pre-trained Sentence-BERT [36] to encode nodes' raw text features.More baseline descriptions and implementation details are in Appendix A.1 and Appendix A.2.</p>
<p>Evaluation Settings.</p>
<p>For node classification, we follow prior works [4,16,40] and randomly split the datasets into training, validation, and test sets with proportions of 6:2:2 for Cora and</p>
<p>Overall Performance Comparison</p>
<p>To answer Q1, we evaluate state-of-the-art models and our proposed GALLM in a supervised setting, where models are trained and tested on the training and test set of the same dataset, respectively.</p>
<p>Performance on Node</p>
<p>Classification.We mainly focus on the node classification task, and the results of our model and baselines are presented in Table 2.In the table, GALLM-sp denotes our model with category-enhanced soft prompt tuning, and GALLM-mp denotes our model with category-enhanced manual prompt tuning.</p>
<p>Based on these results, we can draw the following observations:</p>
<p>• Overall, GALLM achieves state-of-the-art performance, exceeding baselines across all metrics and datasets, which highlights its strength in traditional supervised settings.• Both the category soft prompt and manual prompt methods demonstrate distinct advantages.When labeled data is limited (e.g., Cora, Pubmed, and Instagram), category manual prompts excel by incorporating external knowledge and avoiding the introduction of additional parameters that require fine-tuning.Conversely, in cases of abundant labeled data (e.g., Arxiv), category soft prompts show superior performance.This enhancement is attributed to the model's ability to update the parameters of the soft prompt embeddings by itself, allowing the category prompts to be more effectively tailored to downstream tasks.• LLMs show great potential for tackling complex graph tasks.The Arxiv dataset poses a significant challenge for node classification due to its many node categories.In this dataset, LLM-based methods markedly outperform other approaches, showcasing their emergent abilities in handling complex graph tasks.</p>
<p>4.2.2</p>
<p>Performance on Link Prediction.Additionally, we also conduct evaluations on the link prediction task.Since LLMs output discrete responses rather than continuous values, we choose LLMbased methods as baselines for a fair comparison, and the results are shown in Table 3.The results show that GALLM also surpasses other LLM-based models in link prediction, likely due to aligned task templates that enable models to benefit from pretexts.</p>
<p>Zero-Shot Ability</p>
<p>To answer Q2, we explore our model's zero-shot ability under two settings: the self-supervised setting and the cross-dataset setting.</p>
<p>4.</p>
<p>3.1 Self-Supervised Setting.In the self-supervised setting, the model is trained solely through self-supervised learning (for models with such tasks) and then tested on the same dataset, without using labeled data.We investigate model performances on the Cora, Pubmed, and Arxiv datasets.For self-supervised tasks, GraphGPT employs graph matching, LLaga utilizes node descriptions, and our GALLM is tuned with the text matching task.ZeroG is evaluated directly, as it lacks self-supervised tasks.Experimental results in Table 4 lead to several conclusions: (i) In the self-supervised setting, our model demonstrates remarkable performance.This success is attributed to our self-supervised tasks, which leverage the characteristics of LLMs and graph datasets while aligning with downstream task templates.In this way, the knowledge learned in the self-supervised stage can effectively transfer to downstream tasks, showcasing strong capabilities even without supervised tuning.(ii) In contrast, though LLaGA and GraphGPT are also trained on self-supervised tasks, their task templates and training objectives differ significantly from downstream tasks, thereby restricting the effective application of acquired knowledge.</p>
<p>Generalizablity of GALLM</p>
<p>To address Q3, we explore the generalizability of our model in this section, specifically investigating its ability to handle multiple datasets.In our experiments, models are first trained on node classification tasks using both the Pubmed and Arxiv datasets and then evaluated on each dataset.The results are presented in Table 5.</p>
<p>The results indicate that GALLM demonstrates robustness when trained on multiple datasets.Training a model simultaneously on many different datasets can lead to catastrophic forgetting [22], resulting in performance decline.Traditional GNN models often experience performance degradation during joint training across multiple datasets.In contrast, GraphCL maintains its performance through self-supervised tasks that effectively model graph structures.Also, our GALLM, when trained across different datasets, still achieves superior performance, which is because our approach enables the LLM to accurately learn the graph structure.</p>
<p>Ablation Study</p>
<p>To answer Q4, we verify the importance of the key modules, i.e., the self-supervised text matching tuning, and the category prompt tuning.For both GALLM-sp and GALLM-mp, we construct slim  versions, i.e., GALLM-sp w/o tm and GALLM-mp w/o tm, which are tuned solely on downstream tasks without self-supervised tasks.Also, we construct GALLM w/o cp, where both category manual prompt and category-enhanced soft prompt tuning are removed, yet the models are still fine-tuned with two-stage tasks.The results in Table 6 demonstrate the effectiveness of these modules.</p>
<p>From Table 6, we can observe that (i) self-supervised text matching task is effective for boosting model performance.Compared to the slim version (i.e., both GALLM-sp w/o sp and GALLM-mp w/o mp), the complete GALLM (i.e., both GALLM-sp and GALLMmp) achieves better performance.This improvement is due to the alignment with downstream tasks and adaptation to the characteristics of LLMs and graph datasets.In this way, the knowledge learned during self-supervised tasks can be effectively transferred to downstream tasks.(ii) Category prompt tuning is also crucial for graph learning.Compared to the slim version GALLM w/o cp, the complete GALLM shows enhanced performance.This enhancement is attributed to further alignment with pretexts and the explanatory text provided to LLMs, which not only deepens the LLM's understanding of categories but also facilitates knowledge transfer.</p>
<p>Compatibility Investigation</p>
<p>To answer Q5, we explore the compatibility in this section, specifically focusing on different graph description templates and base LLM models.In our initial experiments, we employ the Neighbor Detail Template and Vicuna-7B.Additionally, we utilize Text-Graph Grounding Pre-trained GNNs [40] (details are in Appendix B) as the graph description template and LLaMA3-8B [41] as the base LLM model to evaluate our approach, separately.We compare the performance against the base model, which excludes the Stage 1 and category prompts in Stage 2, relying solely on task-specific tuning in Stage 2. Figure 4 presents the results for Cora and Pubmed datasets.The findings indicate that our GALLM boosts model performance across different templates and base models, demonstrating the compatibility of our method as a universal approach.</p>
<p>CONCLUSION</p>
<p>In this paper, we propose GALLM, a novel LLM-based graph learning model that integrates Large Language Models into graph learning.We identify the shortcomings of existing methods, particularly their reliance on supervised tuning and ineffective self-supervised tuning.To address this, we align task templates for both stages.In the self-supervised tuning stage, we introduce a text matching task to align with the downstream tasks.In the task-specific tuning stage, we use category prompts to enhance alignment and help LLMs understand the task.This approach allows the model to benefit from self-supervised tuning, improving performance in downstream tasks.Our experiments demonstrate GALLM's potential as a graph foundation model, with exceptional performance across various scenarios.Future work will focus on incorporating more graph tasks and datasets to enhance the model's generalizability.</p>
<p>Figure 1 :
1
Figure 1: Preliminary study on the negative transfer of existing self-supervised tasks.</p>
<p>Figure 2 :
2
Figure 2: The overall architecture of GALLM with two-stage instruction tuning paradigm.</p>
<p>Figure 4 :
4
Figure 4: Compatibility evaluation with different graph description templates and base LLM models.</p>
<p>Table 1 :
1
Statistics of experiment datasets.
Dataset # Nodes # Edges # Class SplittingDomainCora2708542976:2:2citationPubmed 19717 4433836:2:2citationArxiv169343 1166243 406:2:3citationInstagram 11339 14401021:1:8 social network</p>
<p>Table 2 :
2
Performance comparison on node classification under supervised settings.Here bold signifies the best result across all methods, while underline highlights the best baseline result.
MethodDatasets MetricAccCoraF1Pubmed Acc F1AccArxivF1Instagram Acc F1GCN0.8930 0.8795 0.8948 0.8904 0.6765 0.2625 0.6187 0.5524GraghSage0.8782 0.8677 0.9054 0.9051 0.6991 0.3339 0.6062 0.5552GAT0.8708 0.8632 0.8811 0.8760 0.6788 0.2648 0.6064 0.5502SGC0.8875 0.8718 0.8991 0.8943 0.6728 0.2334 0.6220 0.5546BaselinesUniMP NodeFromer 0.8155 0.7927 0.8910 0.8917 0.6869 0.5064 0.5778 0.5385 0.8782 0.8632 0.9034 0.9017 0.6953 0.2977 0.6096 0.5484GraphCL0.8653 0.8450 0.9039 0.9028 0.6611 0.2933 0.6384 0.5512SimGRACE 0.8745 0.8632 0.9049 0.9039 0.6985 0.3264 0.6404 0.5420GraphGPT0.8392 0.8272 0.8176 0.8099 0.7159 0.5458 0.6071 0.5050LLaGA0.8632 0.8434 0.9021 0.9020 0.7323 0.5660 0.6133 0.5549OursGALLM-sp GALLM-mp 0.8965 0.8934 0.9115 0.9105 0.7330 0.5664 0.6460 0.5583 0.8854 0.8757 0.9087 0.9086 0.7391 0.5775 0.6274 0.5203</p>
<p>Table 3 :
3
Performance comparison on link prediction under supervised settings.
MethodDatasets MetricAccCoraF1Pubmed Acc F1BaselinesGraphGPT LLaGA0.5985 0.5985 0.7360 0.7339 0.6156 0.5990 0.7241 0.7239OursGALLM-sp 0.6230 0.6154 0.7321 0.7319 GALLM-mp 0.6156 0.5975 0.7373 0.7373PubMed, and 6:2:3 for Arxiv. For the Instagram dataset, where nodeclassification resembles anomaly detection typically conducted in afew-shot setting, we follow prior work [17] and apply a split ratioof 1:1:8. For link prediction, consistent with previous work [4], werandomly select node pairs from the node classification training setfor training and from the test set for evaluation, ensuring the edge-level training sets match the size of the node-level training sets. Toevaluate the performance of our model, we adopt two widely usedmetrics, i.e., accuracy (Acc) and macro-F1 (F1).</p>
<p>Table 4 :
4
Performance of zero-shot ability.
SettingsSelf-supervised tuningCross-datasetDatasetsCora Pubmed Arxiv Arxiv-Cora Arxiv-InstagramGraphGPT 0.0776 0.3054 0.01130.01110.1659LLaGA0.2989 0.4108 0.00590.25140.1750ZeroG0.4077 0.3747 0.05030.22320.1741GALLM-mp 0.5804 0.4791 0.1309 0.29570.1756</p>
<p>Table 5 :
5
Performance comparison on node classification for generalizability investigation.In the cross-dataset setting, the model learns from a source dataset (including self-supervised learning for models with such tasks) and is then tested on a target dataset, without any access to the target dataset during training.We choose Arxiv as the source dataset with Cora and Instagram as the target dataset.Experimental results are shown in Table4.We can observe that our model also exhibits strong knowledge transfer capabilities.By aligning task templates, the knowledge learned by LLMs can be seamlessly transferred across datasets.Hence, in cross-dataset scenarios, our model demonstrates strong generalization capabilities.
Training setPubmed+ArxivMethodTest setPubmedArxivMetricAccF1AccF1GCN0.8953 0.8918 0.6640 0.2315GraghSage0.8981 0.8965 0.6936 0.3054GAT0.8821 0.8778 0.6626 0.2318SGC0.8864 0.8825 0.6465 0.1963BaselinesUniMP NodeFromer 0.8846 0.8859 0.6510 0.2335 0.9021 0.9019 0.6977 0.3015GraphCL0.9021 0.9006 0.6758 0.3227SimGRACE 0.8993 0.8970 0.6730 0.3145GraphGPT0.7964 0.7911 0.7116 0.4200LLaGA0.9031 0.9024 0.7281 0.3164OursGALLM-mp 0.9120 0.9114 0.7306 0.56034.3.2 cross-dataset Setting.</p>
<p>Table 6 :
6
The ablation study performance.
DatasetCoraPubmedArxivInstagramMetricAccF1AccF1AccF1AccF1GALLM-mp 0.8965 0.8934 0.9115 0.9105 0.7330 0.5664 0.6460 0.5583w/o tm0.8724 0.8604 0.9076 0.9061 0.7285 0.5587 0.6405 0.5580GALLM-sp 0.8854 0.8757 0.9087 0.9086 0.7391 0.5775 0.6274 0.5203w/o tm0.8743 0.8588 0.8988 0.8983 0.7330 0.5664 0.6215 0.5187w/o cp0.8780 0.8667 0.9031 0.9025 0.7310 0.5632 0.5982 0.50210.76 0.78 0.80 0.82 0.84 0.86 0.88 0.90 ScoreAccuarcyCoraF1 GALLM-mp base model0.76 0.78 0.80 0.82 0.90 0.88 0.86 0.84 ScoreAccuarcy PubmedF1(a) Model performance with Text-Graph Grounding Pre-trained GNNs0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94 ScoreAccuarcyCoraF10.80 0.82 0.84 0.86 0.88 0.94 0.92 0.90 ScoreAccuarcy PubmedF1(b) Model performance with LLaMA3-8b
https://github.com/pyg-team/pytorch_geometric Conference'17, July
, Washington, DC, USA Haitong Luo, Xuying Meng, Suhang Wang, Tianxiang Zhao, Fali Wang, Hanyun Cao, and Yujun Zhang
APPENDIX A EXPERIMENTAL SETUP DETAILS A.1 Baseline DescriptionsTo verify the effectiveness of our proposed model, we compare it with various models, which can be divided into four groups: traditional GNN models, transformer-based graph models, selfsupervised GNN models, and LLMs for graphs.The first group includes:• GAT[42]: This method proposes an attention mechanism to aggregate neighborhood information.• GCN[21]: This method achieves neighborhood information aggregation by spectral graph convolutions.• GraphSage[12]: This method learns nodes embedding inductively based on the nodes' ego features and the fixed number of neighborhood features.• SGC[47]: This method simplifies the graph convolution operation by removing nonlinearities and collapsing weight matrices, all the while maintaining strong performance.The second group includes the graph models based on the transformer network:• UniMP[38]: This method adopts a Graph Transformer network to incorporate feature and label propagation at both training and inference time.• NodeFormer[49]: Against graph inconsistency, this method filters dissimilar neighbors based on a fixed threshold while aggregating multi-view neighborhood information.The third group includes the self-supervised GNNs:• GraphCL [57]: This method introduces various graph augmentation techniques to derive diverse graph views, subsequently learning representations by maximizing feature consistency across these distinct augmented views.• SimGRACE[50]: Without graph augmentation techniques, this method perturbs GNN encoders to obtain different views for contrastive learning.The fourth group includes stage-of-the-art Large Language Models for graphs:• GraphGPT[40]: This method encodes graph data with a pretrained GNN and proposes dual-stage instruction tuning to align graph data with natural language.• LLaGA[4]: This method proposes two graph description templates to translate graph data and instruction tuning LLMs with three different tasks.For zero-shot ability investigation, we further include a model focus on zero-shot performance:• ZeroG [26]: This method fine-tunes language models to encode node attributes and class semantics, enabling cross-dataset zeroshot transferability.For these baseline models, we implement them with the source code provided by authors or other researchers 1 .A.2 Implementation DetailsWe employ Vicuna-7B-v1.5-16Kas our foundation base model and utilize a pre-trained Sentence-BERT[36]to encode the nodes' raw text features.We set the batch size to 2 per device and the learning rate is 2 −3 .The warm ratio is set to 3 −2 and the maximum input length of LLMs is 2048.We tune models for 3 epochs in both two stages.For the Neighbor Detail Template, we set the fixed sample size to 10 in each hop.In the text matching task, for each positive text sample, we sample 10 negative samples, consisting of 3 hard negative samples and 7 easy negative samples.In the categoryenhanced soft prompt tuning, we utilize 3 virtual soft prompt tokens for each category.All the experiments are conducted with CUDA 12.1 and torch 2.3.1 on the NVIDIA H800 GPU.A.3 Dataset DetailsWe choose four representative widely-used datasets: Cora[54], Pubmed[13], Arxiv[16]and Instagram[20].These datasets originate from different domains, with the first three derived from citation networks and the last from social networks.For citation network datasets, nodes represent academic papers and edges denote citation relationships.Each node is attributed with a title and an abstract and is categorized into distinct research topics.Specifically: (1) the ArXiv dataset includes papers from the computer science arXiv website, categorized into 40 subject areas;(2) the PubMed dataset comprises papers from the PubMed database, categorized into 3 topics: experimentally induced diabetes, type 1B DETAILS OF OTHER GRAPH DESCRIPTION TEMPLATEIn the Compatibility Investigation in Section 4.6, we replace the graph description template with Text-Graph Grounding Pre-trained GNNs proposed by GraphGPT[40].Firstly, this template samples a subgraph for the given central node to obtain the node sequences, where the central node is the first token and the neighboring nodes make up the remaining tokens.Then it employs a pre-trained GNN model to encode the graph tokens.Specifically, this template adopts a text-graph grounding paradigm, aligning graph features with their corresponding textual representations through contrastive learning.In this way, the graph information can be converted into pre-aligned token sequences.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Graphnli: A graph-based natural language inference model for polarity prediction in online debates. Vibhor Agarwal, Sagar Joglekar, Anthony P Young, Nishanth Sastry, Proceedings of the ACM Web Conference 2022. the ACM Web Conference 20222022</p>
<p>Tallrec: An effective and efficient tuning framework to align large language model with recommendation. Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He, Proceedings of the 17th ACM Conference on Recommender Systems. the 17th ACM Conference on Recommender Systems2023</p>
<p>Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, Zhangyang Wang, arXiv:2402.08170Llaga: Large language and graph assistant. 2024. 2024arXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, ACM SIGKDD Explorations Newsletter. 252024. 2024</p>
<p>Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang, arXiv:2310.04668Label-free node classification on graphs with large language models (llms). 2023. 2023arXiv preprint</p>
<p>Convolutional neural networks on graphs with fast localized spectral filtering. Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst, Advances in neural information processing systems. 292016. 2016</p>
<p>Pre-training of deep bidirectional transformers for language. Devlin, Chang, Lee, Toutanova, Proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: Human language technologies. the 2019 conference of the north american chapter of the association for computational linguistics: Human language technologies20191</p>
<p>. M N Minneapolis, 2019Association for Computational Linguistics</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, arXiv:2310.045602023. 2023arXiv preprint</p>
<p>Prompting large language models with speech recognition abilities. Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2024</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, Shi Han, arXiv:2305.150662023. 2023arXiv preprint</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, 2017. 201730Advances in neural information processing systems</p>
<p>Xiaoxin He, Xavier Bresson, Thomas Laurent, Bryan Hooi, arXiv:2305.19523Explanations as features: Llm-based features for text-attributed graphs. 2023. 202324arXiv preprint</p>
<p>Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann Lecun, Bryan Hooi, arXiv:2305.195232023. 2023arXiv preprint</p>
<p>Graphmae2: A decoding-enhanced masked self-supervised graph learner. Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov, Jie Tang, Proceedings of the ACM web conference 2023. the ACM web conference 20232023</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in neural information processing systems. 332020. 2020</p>
<p>Can GNN be Good Adapter for LLMs. Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, Qi Zhu, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024</p>
<p>Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, Jiliang Tang, arXiv:2006.10141Self-supervised learning on graphs: Deep insights and new direction. 2020. 2020arXiv preprint</p>
<p>Hard negative mixing for contrastive learning. Yannis Kalantidis, Bulent Mert, Noe Sariyildiz, Pion, Advances in neural information processing systems. 332020. 2020Philippe Weinzaepfel, and Diane Larlus</p>
<p>Multimodal post attentive profiling for influencer marketing. Seungbae Kim, Jyun-Yu Jiang, Masaki Nakada, Jinyoung Han, Wei Wang, Proceedings of The Web Conference. The Web Conference2020. 2020</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016. 2016arXiv preprint</p>
<p>Overcoming catastrophic forgetting in neural networks. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Proceedings of the national academy of sciences. 1142017. 2017</p>
<p>GOFA: A Generative One-For-All Model for Joint Graph Language Modeling. Lecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang, arXiv:2407.097092024. 2024arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021. 2021arXiv preprint</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021. 2021arXiv preprint</p>
<p>Zerog: Investigating cross-dataset zero-shot transferability in graphs. Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. Chang Liu, Bo Wu, arXiv:2308.112242023. 2023arXiv preprint</p>
<p>Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S Yu, arXiv:2310.11829Towards graph foundation models: A survey and beyond. 2023. 2023arXiv preprint</p>
<p>Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang, GPT understands, too. AI Open. 2023. 2023</p>
<p>Pick and choose: a GNN-based imbalanced learning approach for fraud detection. Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Yang Hao, Qing He, Proceedings of the web conference 2021. the web conference 20212021</p>
<p>Spectral-Based Graph Neural Networks for Complementary Item Recommendation. Haitong Luo, Xuying Meng, Suhang Wang, Hanyun Cao, Weiyao Zhang, Yequan Wang, Yujun Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Recent advances in natural language processing via large pre-trained language models: A survey. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran, Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, Dan Roth, Comput. Surveys. 562023. 2023</p>
<p>Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, arXiv:2212.09251Discovering language model behaviors with model-written evaluations. 2022. 2022arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 212020. 2020</p>
<p>Graph neural networks for intelligent transportation systems: A survey. Saeed Rahmani, Asiye Baghbani, Nizar Bouguila, Zachary Patterson, IEEE Transactions on Intelligent Transportation Systems. 242023. 2023</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019. 2019arXiv preprint</p>
<p>Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, Stefanie Jegelka, arXiv:2010.04592Contrastive learning with hard negative samples. 2020. 2020arXiv preprint</p>
<p>Masked label prediction: Unified message passing model for semisupervised classification. Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, Yu Sun, arXiv:2009.035092020. 2020arXiv preprint</p>
<p>Rethinking graph neural networks for anomaly detection. Jianheng Tang, Jiajin Li, Ziqi Gao, Jia Li, International Conference on Machine Learning. PMLR2022</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXiv:1710.109032017. 2017arXiv preprint</p>
<p>Can language models solve graph problems in natural language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Afec: Active forgetting of negative transfer in continual learning. Liyuan Wang, Mingtian Zhang, Zhongfan Jia, Qian Li, Chenglong Bao, Kaisheng Ma, Jun Zhu, Yi Zhong, Advances in Neural Information Processing Systems. 342021. 2021</p>
<p>Molecular contrastive learning of representations via graph neural networks. Yuyang Wang, Jianren Wang, Zhonglin Cao, Amir Barati, Farimani , Nature Machine Intelligence. 42022. 2022</p>
<p>A compact review of molecular property prediction with graph neural networks. Oliver Wieder, Stefan Kohlbacher, Mélaine Kuenemann, Arthur Garon, Pierre Ducrot, Thomas Seidel, Thierry Langer, Drug Discovery Today: Technologies. 372020. 2020</p>
<p>Simplifying graph convolutional networks. Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger, International conference on machine learning. PMLR2019</p>
<p>Multimodal large language models: A survey. Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, Philip Yu, 2023 IEEE International Conference on Big Data (BigData). IEEE2023</p>
<p>Nodeformer: A scalable graph structure learning transformer for node classification. Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, Junchi Yan, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Simgrace: A simple framework for graph contrastive learning without data augmentation. Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, Stan Z Li, Proceedings of the ACM Web Conference 2022. the ACM Web Conference 20222022</p>
<p>Automated self-supervised learning for recommendation. Lianghao Xia, Chao Huang, Chunzhen Huang, Kangyi Lin, Tao Yu, Ben Kao, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023</p>
<p>Evidenceaware fake news detection with graph neural networks. Weizhi Xu, Junfei Wu, Qiang Liu, Shu Wu, Liang Wang, Proceedings of the ACM web conference 2022. the ACM web conference 20222022</p>
<p>Consisrec: Enhancing gnn for social recommendation via consistent neighbor aggregation. Liangwei Yang, Zhiwei Liu, Yingtong Dou, Jing Ma, Philip S Yu, Proceedings of the 44th international ACM SIGIR conference on Research and development in information retrieval. the 44th international ACM SIGIR conference on Research and development in information retrieval2021</p>
<p>Revisiting semisupervised learning with graph embeddings. Zhilin Yang, William Cohen, Ruslan Salakhudinov, International conference on machine learning. PMLR2016</p>
<p>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, arXiv:2308.07134Natural language is all a graph needs. 2023. 2023arXiv preprint</p>
<p>Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen, arXiv:2306.13549A survey on multimodal large language models. 2023. 2023arXiv preprint</p>
<p>Graph contrastive learning with augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, Advances in neural information processing systems. 332020. 2020</p>
<p>Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, arXiv:2308.10792Instruction tuning for large language models: A survey. 2023. 2023arXiv preprint</p>
<p>Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang, arXiv:2310.01089Graphtext: Graph reasoning in text space. 2023. 2023arXiv preprint</p>
<p>Graph contrastive learning with adaptive augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang, Proceedings of the web conference 2021. the web conference 20212021</p>            </div>
        </div>

    </div>
</body>
</html>
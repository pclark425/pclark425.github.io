<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6913 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6913</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6913</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-133.html">extraction-schema-133</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <p><strong>Paper ID:</strong> paper-6509455</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1701.00464v1.pdf" target="_blank">Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation</a></p>
                <p><strong>Paper Abstract:</strong> During the last decades, many cognitive architectures (CAs) have been realized adopting different assumptions about the organization and the representation of their knowledge level. Some of them (e.g. SOAR [Laird (2012)]) adopt a classical symbolic approach, some (e.g. LEABRA [O'Reilly and Munakata (2000)]) are based on a purely connectionist model, while others (e.g. CLARION [Sun (2006)] adopt a hybrid approach combining connectionist and symbolic representational levels. Additionally, some attempts (e.g. biSOAR) trying to extend the representational capacities of CAs by integrating diagrammatical representations and reasoning are also available [Kurup and Chandrasekaran (2007)]. In this paper we propose a reflection on the role that Conceptual Spaces, a framework developed by Peter G\"ardenfors [G\"ardenfors (2000)] more than fifteen years ago, can play in the current development of the Knowledge Level in Cognitive Systems and Architectures. In particular, we claim that Conceptual Spaces offer a lingua franca that allows to unify and generalize many aspects of the symbolic, sub-symbolic and diagrammatic approaches (by overcoming some of their typical problems) and to integrate them on a common ground. In doing so we extend and detail some of the arguments explored by G\"ardenfors [G\"ardenfors (1997)] for defending the need of a conceptual, intermediate, representation level between the symbolic and the sub-symbolic one.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6913.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6913.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A geometric, metric-space framework (Gärdenfors) representing entities as points on quality dimensions and concepts as convex regions; similarity corresponds to distance and prototypes to region centroids.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptual spaces: The geometry of thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>high-dimensional geometric / metric space</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Entities are points in a metric space whose axes are cognitively salient quality dimensions; concepts are regions (often convex) in that space, prototypes are centroids, and similarity is inversely related to metric distance.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Accounts for typicality gradients (centrality within regions), similarity-based categorization, composition as region intersection, trajectory-based reasoning for dynamics, and provides an intermediate representational level linking symbolic and sub-symbolic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiment; computational simulation; computational applications</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>typicality ratings and categorization probability/RT; computational modelling of perception and categorization; use in anchoring/robotics experiments</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Typicality effects and prototype-centroid interpretations are naturally captured by convex regions and distance metrics; tessellations (Voronoi) and region intersections model composite concepts (e.g., 'polka-dot zebra').</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Acquisition of appropriate quality dimensions is underspecified (paper notes this as an external learning problem); word-embedding vectors differ in interpretability of dimensions and purpose; no direct large-scale neurophysiological proof provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Gärdenfors (2000)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6913.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6913.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype Theory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concepts represented by prototypical exemplars or averaged feature vectors such that membership/typicality is graded relative to proximity to the prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cognitive representations of semantic categories</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Prototype Theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feature-based vector / exemplar-aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>A category is represented by a prototype (best example); items are judged by similarity to this prototype and typicality is graded continuously.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains typicality effects, faster categorization for prototypical items, centrality/graded membership within categories.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiment</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>typicality rating; categorization probability and reaction time tasks</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Rosch's empirical results reveal typicality gradients inconsistent with classical necessary-and-sufficient feature lists — prototypes better predict typicality and RTs for common-sense categories.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Prototype accounts face compositionality problems for combined concepts (Osherson & Smith style examples like 'pet fish'); not all categorization behavior is prototype-based (mixed exemplar/prototype usage found).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Rosch (1975)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6913.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6913.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exemplar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exemplar Theory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Categories are represented by stored specific instances (exemplars); classification is based on similarity to stored exemplars rather than to an abstract prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning about categories in the absence of memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Exemplar Theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>memory-based instance representations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual knowledge consists of multiple stored exemplars; new stimuli are categorized by comparing to these exemplars and aggregating similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains atypical category membership, fine-grained similarity effects, and variability in strategy across tasks/individuals; aligns with some neural memory systems.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiment; neuropsychological (memory) evidence</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>categorization tasks, recognition/memory paradigms, reaction-time measures</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Empirical work (e.g., Malt, Squire & Knowlton) shows that humans sometimes use exemplar-based strategies and that exemplar representations have neural plausibility (memory systems supporting exemplar storage).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Not all categorization is exemplar-driven; different tasks/subjects use prototypes or a mix, motivating heterogeneous/hybrid models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Squire and Knowlton (1995); Malt (1989)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6913.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6913.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theory-theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory-Theory (concepts as theories / role-based representations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concepts are understood as theoretical terms or mini-theories; category membership depends on the role a concept plays within a richer explanatory structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The big book of concepts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Theory-Theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>relational / role-based conceptual structures</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts correspond to positions or terms within domain theories; exemplars and features are interpreted via causal/explanatory knowledge rather than simple similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains use of background knowledge in categorization, structured inference about categories (e.g., causal properties), and variability in concept use across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiment; conceptual analysis</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>classification with causal/functional probes; tasks requiring domain knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Theory-theory captures inferential aspects of concepts that prototype/exemplar models can't, supporting heterogeneous concept composition.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Does not by itself explain rapid, automatic typicality-based categorization (type 1 phenomena) and may require complementing representations for automatic similarity-based tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Murphy (2002); Machery (2009)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6913.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6913.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic (Classical / Logic-based) Representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Compositional, rule-based representations where concepts are defined by discrete symbols and logical relations; supports explicit inference and algebraic composition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Soar cognitive architecture</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Symbolic Representations (classical/logical)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>symbolic / logical / compositional</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Knowledge is encoded as discrete symbols and structured relations; complex concepts are built compositionally from primitives via syntactic and semantic rules enabling explicit rule-based inference.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Accounts for complex, systematic, and compositional reasoning (type 2 processes), taxonomic classification, and explicit justification of inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational architectures; theoretical arguments</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>formal modeling and cognitive-architecture simulations; tasks requiring explicit rule use or taxonomy construction</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Symbolic systems successfully implement compositionality and logical inference (e.g., SOAR, description logics), but struggle to model typicality and graded membership.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Empirical typicality effects (Rosch) and Osherson & Smith arguments show that purely symbolic/logical (including fuzzy logic) compositional schemes have difficulty accounting for graded prototypicality and some compositional typicality phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Laird (2012); Fodor and Pylyshyn (1988)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6913.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6913.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Connectionist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Networks / Connectionist Representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Distributed sub-symbolic representations implemented by weighted networks; learn feature detectors and mappings from sensory inputs to outputs, including deep hierarchical models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Neural Network / Connectionist Representations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>distributed high-dimensional vectors / deep hierarchical networks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Representations are encoded as distributed activation patterns across units and weights, learned from data; internal layers can be interpreted as transformations between feature spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Excellent for perception, classification, feature extraction, and learning from raw sensory data; alleviate symbol-grounding issues by direct sensory mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational simulation; empirical performance on benchmarks; neuro-inspired frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>classification tasks, deep learning benchmarks (e.g., image recognition, AlphaGo), analysis of hidden-layer activations</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Deep networks (e.g., CNNs) achieve strong performance in perception tasks but show poor transfer/generalization in some commonsense domains and suffer from interpretability ('opacity').</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Connectionist models are criticized for non-compositionality (Fodor & Pylyshyn) and opacity; also limited in commonsense reasoning and cross-domain transfer (authors cite AlphaGo example and Davis & Marcus critique of Watson).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>LeCun et al. (2015); Fodor and Pylyshyn (1988)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6913.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6913.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DualProc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual-Process Framework (System 1 / System 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cognitive architecture distinction between fast, automatic, associative System 1 processes and slow, deliberate, rule-based System 2 processes, applied to conceptual representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Thinking, fast and slow</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Dual-Process Theory (System 1 / System 2)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>process-level functional decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual processing is split: System 1 handles fast, automatic, similarity/typicality-based categorization (implicit concepts), while System 2 handles explicit, compositional, rule-based inference (explicit concepts).</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains co-existence of rapid typicality judgments and slower taxonomic inferences; motivates hybrid architectures mapping conceptual spaces to System 1 and symbolic systems to System 2.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiments; theoretical analyses</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>selection tasks, conjunction fallacy, taxonomic classification tasks, RT comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Different tasks show signatures of fast associative processing vs slow deliberative processing; authors propose assigning Conceptual Spaces to System 1 and symbolic representations to System 2 in architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Not all conceptual phenomena neatly split; authors note hybrid implementations are needed and report systems that integrate both (e.g., CLARION, ACT-R integrations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Stanovich & West (2000); Kahneman (2011); Piccinini (2011)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6913.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6913.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chorus/RBF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chorus of Prototypes / RBF interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Interpretation linking radial-basis-function (RBF) networks and prototype-based representations: RBF units interpreted as prototypes in a conceptual space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Representation, similarity, and the chorus of prototypes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Chorus of Prototypes / RBF Interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>prototype-in-RBF / mapping from neural units to geometric prototypes</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Units in RBF networks correspond to prototype locations in a geometric space; network activation reflects similarity to prototypes, enabling geometric interpretations of hidden-layer representations.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Provides a bridge between neural network hidden units and interpretable prototype-like representations, enabling modeling of similarity and chimeric/intermediate items and reducing opacity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational simulation; theoretical mapping</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>analysis of RBF network activations; computational demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>RBF networks can be read as representing prototypes and hence mapped onto Conceptual Spaces, facilitating similarity-based classification and handling of chimeric cases.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Mapping may be partial and depends on network architecture; general deep network interpretability still challenging beyond RBF-like cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Edelman (1995); Balkenius (1999)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6913.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6913.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OshersonCrit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Osherson & Smith compositionality critique (fuzzy logic counterexample)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Argument that prototype/typicality representations conflict with compositional logic: conjunctions cannot be more typical than constituents under standard logical aggregation (example: 'polka-dot zebra').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the adequacy of prototype theory as a theory of concepts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Compositionality Critique of Prototype Representations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>theoretical critique (logical constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Logical and fuzzy-logical composition rules imply that conjunction truth-values cannot exceed those of conjuncts, contradicting prototypical cases where the conjunction appears more typical than either constituent.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Shows a formal incompatibility between standard compositional logics (including simple fuzzy conjunction) and observed typicality phenomena, motivating non-logical or hybrid representations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>theoretical argument; illustrative examples</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>conceptual examples and logical analysis (e.g., polka-dot zebra)</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Prototype-based typicality ratings (high for 'polka-dot zebra') conflict with compositional constraints of logic/fuzzy conjunctions (bounded by constituents).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Conceptual Spaces and geometric intersection of regions provide a way to reconcile compositional appearance with typicality (authors argue region intersection allows conjunctive typicality exceeding constituent centrality).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Osherson and Smith (1981)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6913.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6913.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WordEmb</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word Embeddings (word2vec / GloVe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Distributional vector-space models that embed word meanings as points in high-dimensional Euclidean space learned from co-occurrence statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient estimation of word representations in vector space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Word Embeddings (Distributional Semantics)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>high-dimensional statistical vector space</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Words are represented as learned vectors capturing statistical co-occurrence patterns; semantic similarity corresponds to vector distance or cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Capture lexical similarity and syntactic/semantic regularities; useful for many NLP tasks but dimensions lack direct cognitive interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational simulation/benchmark performance</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>intrinsic similarity tasks, downstream NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Embeddings model word similarity well but their dimensions are not interpretable as cognitive quality dimensions; authors note embeddings as intermediate between data and conceptual levels.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Not designed to model conceptual-level typicality/prototype phenomena or to provide cognitively meaningful quality dimensions; limited for commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Mikolov et al. (2013); Pennington et al. (2014)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6913.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6913.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NEF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Engineering Framework / Semantic Pointers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biologically inspired framework (NEF) using vector representations (semantic pointers) to implement neural populations performing vector operations and symbolic-like binding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural engineering: Computation, representation, and dynamics in neurobiological systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Neural Engineering Framework (NEF) / Semantic Pointers</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>population-vector / compressed symbolic representations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Neural populations represent vectors (semantic pointers) enabling compression, binding (e.g., circular convolution), and transformation operations that support symbol-like cognitive functions in neural hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains how neural populations can implement structured, compressed vector representations supporting composition/binding and bridging symbolic and subsymbolic processing (used in SPAUN).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational simulation; large-scale neural modeling</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>large-scale model demonstrations (e.g., SPAUN); mapping population activity to vector operations</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Semantic pointers demonstrate how neural systems can encode compositional structures and reduce opacity by giving an interpretable vector-level description of population activity.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Requires significant modeling assumptions; mapping to cognitive-level quality dimensions still nontrivial, but compatible with Conceptual Spaces interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Eliasmith and Anderson (2004); Eliasmith et al. (2012)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6913.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6913.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diagram</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diagrammatic / Analogical Representations (mental models, mental images)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Picture-like or spatial representations (mental images, mental models) that preserve structural/spatial relations and allow analogical/trajectory reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mental models: Towards a cognitive science of language, inference, and consciousness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Diagrammatic / Analogical Representations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>spatial / analog / pictorial representations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Cognitive content is represented in analog, picture-like formats (mental images or diagrammatic models) that preserve spatial or topological relations and support perceptual-like manipulations and trajectory extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Support efficient and transparent reasoning in spatial domains, intuitive planning and prediction via trajectories, and explain phenomena where symbolic logic fails (e.g., round-table 'to the right of' problem).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiments; theoretical/computational examples</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>mental imagery tasks, deductive reasoning tasks modeled with mental models, planning scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Diagrammatic representations handle many spatial reasoning and dynamic identity-tracking tasks more naturally than purely symbolic systems; Conceptual Spaces can unify and generalize diagrammatic properties (trajectory extrapolation in quality space).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Lack of a unified theory and formal framework; not dominant relative to symbolic/connectionist systems but complementary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Johnson-Laird (1983, 2006); Glasgow et al. (1995)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual spaces: The geometry of thought <em>(Rating: 2)</em></li>
                <li>Cognitive representations of semantic categories <em>(Rating: 2)</em></li>
                <li>On the adequacy of prototype theory as a theory of concepts <em>(Rating: 2)</em></li>
                <li>Representation, similarity, and the chorus of prototypes <em>(Rating: 2)</em></li>
                <li>Deep learning <em>(Rating: 1)</em></li>
                <li>Learning about categories in the absence of memory <em>(Rating: 2)</em></li>
                <li>Neural engineering: Computation, representation, and dynamics in neurobiological systems <em>(Rating: 2)</em></li>
                <li>Thinking, fast and slow <em>(Rating: 1)</em></li>
                <li>Efficient estimation of word representations in vector space <em>(Rating: 1)</em></li>
                <li>Are there dimensions in the brain? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6913",
    "paper_id": "paper-6509455",
    "extraction_schema_id": "extraction-schema-133",
    "extracted_data": [
        {
            "name_short": "CS",
            "name_full": "Conceptual Spaces",
            "brief_description": "A geometric, metric-space framework (Gärdenfors) representing entities as points on quality dimensions and concepts as convex regions; similarity corresponds to distance and prototypes to region centroids.",
            "citation_title": "Conceptual spaces: The geometry of thought",
            "mention_or_use": "use",
            "theory_name": "Conceptual Spaces",
            "theory_type": "high-dimensional geometric / metric space",
            "theory_description": "Entities are points in a metric space whose axes are cognitively salient quality dimensions; concepts are regions (often convex) in that space, prototypes are centroids, and similarity is inversely related to metric distance.",
            "functional_claims": "Accounts for typicality gradients (centrality within regions), similarity-based categorization, composition as region intersection, trajectory-based reasoning for dynamics, and provides an intermediate representational level linking symbolic and sub-symbolic systems.",
            "evidence_source": "behavioral experiment; computational simulation; computational applications",
            "experimental_paradigm": "typicality ratings and categorization probability/RT; computational modelling of perception and categorization; use in anchoring/robotics experiments",
            "key_result": "Typicality effects and prototype-centroid interpretations are naturally captured by convex regions and distance metrics; tessellations (Voronoi) and region intersections model composite concepts (e.g., 'polka-dot zebra').",
            "supports_theory": true,
            "counter_evidence": "Acquisition of appropriate quality dimensions is underspecified (paper notes this as an external learning problem); word-embedding vectors differ in interpretability of dimensions and purpose; no direct large-scale neurophysiological proof provided in this paper.",
            "citation": "Gärdenfors (2000)",
            "uuid": "e6913.0",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        },
        {
            "name_short": "Prototype",
            "name_full": "Prototype Theory",
            "brief_description": "Concepts represented by prototypical exemplars or averaged feature vectors such that membership/typicality is graded relative to proximity to the prototype.",
            "citation_title": "Cognitive representations of semantic categories",
            "mention_or_use": "mention",
            "theory_name": "Prototype Theory",
            "theory_type": "feature-based vector / exemplar-aggregate",
            "theory_description": "A category is represented by a prototype (best example); items are judged by similarity to this prototype and typicality is graded continuously.",
            "functional_claims": "Explains typicality effects, faster categorization for prototypical items, centrality/graded membership within categories.",
            "evidence_source": "behavioral experiment",
            "experimental_paradigm": "typicality rating; categorization probability and reaction time tasks",
            "key_result": "Rosch's empirical results reveal typicality gradients inconsistent with classical necessary-and-sufficient feature lists — prototypes better predict typicality and RTs for common-sense categories.",
            "supports_theory": true,
            "counter_evidence": "Prototype accounts face compositionality problems for combined concepts (Osherson & Smith style examples like 'pet fish'); not all categorization behavior is prototype-based (mixed exemplar/prototype usage found).",
            "citation": "Rosch (1975)",
            "uuid": "e6913.1",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        },
        {
            "name_short": "Exemplar",
            "name_full": "Exemplar Theory",
            "brief_description": "Categories are represented by stored specific instances (exemplars); classification is based on similarity to stored exemplars rather than to an abstract prototype.",
            "citation_title": "Learning about categories in the absence of memory",
            "mention_or_use": "mention",
            "theory_name": "Exemplar Theory",
            "theory_type": "memory-based instance representations",
            "theory_description": "Conceptual knowledge consists of multiple stored exemplars; new stimuli are categorized by comparing to these exemplars and aggregating similarity.",
            "functional_claims": "Explains atypical category membership, fine-grained similarity effects, and variability in strategy across tasks/individuals; aligns with some neural memory systems.",
            "evidence_source": "behavioral experiment; neuropsychological (memory) evidence",
            "experimental_paradigm": "categorization tasks, recognition/memory paradigms, reaction-time measures",
            "key_result": "Empirical work (e.g., Malt, Squire & Knowlton) shows that humans sometimes use exemplar-based strategies and that exemplar representations have neural plausibility (memory systems supporting exemplar storage).",
            "supports_theory": true,
            "counter_evidence": "Not all categorization is exemplar-driven; different tasks/subjects use prototypes or a mix, motivating heterogeneous/hybrid models.",
            "citation": "Squire and Knowlton (1995); Malt (1989)",
            "uuid": "e6913.2",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        },
        {
            "name_short": "Theory-theory",
            "name_full": "Theory-Theory (concepts as theories / role-based representations)",
            "brief_description": "Concepts are understood as theoretical terms or mini-theories; category membership depends on the role a concept plays within a richer explanatory structure.",
            "citation_title": "The big book of concepts",
            "mention_or_use": "mention",
            "theory_name": "Theory-Theory",
            "theory_type": "relational / role-based conceptual structures",
            "theory_description": "Concepts correspond to positions or terms within domain theories; exemplars and features are interpreted via causal/explanatory knowledge rather than simple similarity.",
            "functional_claims": "Explains use of background knowledge in categorization, structured inference about categories (e.g., causal properties), and variability in concept use across contexts.",
            "evidence_source": "behavioral experiment; conceptual analysis",
            "experimental_paradigm": "classification with causal/functional probes; tasks requiring domain knowledge",
            "key_result": "Theory-theory captures inferential aspects of concepts that prototype/exemplar models can't, supporting heterogeneous concept composition.",
            "supports_theory": true,
            "counter_evidence": "Does not by itself explain rapid, automatic typicality-based categorization (type 1 phenomena) and may require complementing representations for automatic similarity-based tasks.",
            "citation": "Murphy (2002); Machery (2009)",
            "uuid": "e6913.3",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        },
        {
            "name_short": "Symbolic",
            "name_full": "Symbolic (Classical / Logic-based) Representations",
            "brief_description": "Compositional, rule-based representations where concepts are defined by discrete symbols and logical relations; supports explicit inference and algebraic composition.",
            "citation_title": "The Soar cognitive architecture",
            "mention_or_use": "mention",
            "theory_name": "Symbolic Representations (classical/logical)",
            "theory_type": "symbolic / logical / compositional",
            "theory_description": "Knowledge is encoded as discrete symbols and structured relations; complex concepts are built compositionally from primitives via syntactic and semantic rules enabling explicit rule-based inference.",
            "functional_claims": "Accounts for complex, systematic, and compositional reasoning (type 2 processes), taxonomic classification, and explicit justification of inferences.",
            "evidence_source": "computational architectures; theoretical arguments",
            "experimental_paradigm": "formal modeling and cognitive-architecture simulations; tasks requiring explicit rule use or taxonomy construction",
            "key_result": "Symbolic systems successfully implement compositionality and logical inference (e.g., SOAR, description logics), but struggle to model typicality and graded membership.",
            "supports_theory": true,
            "counter_evidence": "Empirical typicality effects (Rosch) and Osherson & Smith arguments show that purely symbolic/logical (including fuzzy logic) compositional schemes have difficulty accounting for graded prototypicality and some compositional typicality phenomena.",
            "citation": "Laird (2012); Fodor and Pylyshyn (1988)",
            "uuid": "e6913.4",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        },
        {
            "name_short": "Connectionist",
            "name_full": "Neural Networks / Connectionist Representations",
            "brief_description": "Distributed sub-symbolic representations implemented by weighted networks; learn feature detectors and mappings from sensory inputs to outputs, including deep hierarchical models.",
            "citation_title": "Deep learning",
            "mention_or_use": "mention",
            "theory_name": "Neural Network / Connectionist Representations",
            "theory_type": "distributed high-dimensional vectors / deep hierarchical networks",
            "theory_description": "Representations are encoded as distributed activation patterns across units and weights, learned from data; internal layers can be interpreted as transformations between feature spaces.",
            "functional_claims": "Excellent for perception, classification, feature extraction, and learning from raw sensory data; alleviate symbol-grounding issues by direct sensory mapping.",
            "evidence_source": "computational simulation; empirical performance on benchmarks; neuro-inspired frameworks",
            "experimental_paradigm": "classification tasks, deep learning benchmarks (e.g., image recognition, AlphaGo), analysis of hidden-layer activations",
            "key_result": "Deep networks (e.g., CNNs) achieve strong performance in perception tasks but show poor transfer/generalization in some commonsense domains and suffer from interpretability ('opacity').",
            "supports_theory": true,
            "counter_evidence": "Connectionist models are criticized for non-compositionality (Fodor & Pylyshyn) and opacity; also limited in commonsense reasoning and cross-domain transfer (authors cite AlphaGo example and Davis & Marcus critique of Watson).",
            "citation": "LeCun et al. (2015); Fodor and Pylyshyn (1988)",
            "uuid": "e6913.5",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        },
        {
            "name_short": "DualProc",
            "name_full": "Dual-Process Framework (System 1 / System 2)",
            "brief_description": "Cognitive architecture distinction between fast, automatic, associative System 1 processes and slow, deliberate, rule-based System 2 processes, applied to conceptual representation.",
            "citation_title": "Thinking, fast and slow",
            "mention_or_use": "mention",
            "theory_name": "Dual-Process Theory (System 1 / System 2)",
            "theory_type": "process-level functional decomposition",
            "theory_description": "Conceptual processing is split: System 1 handles fast, automatic, similarity/typicality-based categorization (implicit concepts), while System 2 handles explicit, compositional, rule-based inference (explicit concepts).",
            "functional_claims": "Explains co-existence of rapid typicality judgments and slower taxonomic inferences; motivates hybrid architectures mapping conceptual spaces to System 1 and symbolic systems to System 2.",
            "evidence_source": "behavioral experiments; theoretical analyses",
            "experimental_paradigm": "selection tasks, conjunction fallacy, taxonomic classification tasks, RT comparisons",
            "key_result": "Different tasks show signatures of fast associative processing vs slow deliberative processing; authors propose assigning Conceptual Spaces to System 1 and symbolic representations to System 2 in architectures.",
            "supports_theory": true,
            "counter_evidence": "Not all conceptual phenomena neatly split; authors note hybrid implementations are needed and report systems that integrate both (e.g., CLARION, ACT-R integrations).",
            "citation": "Stanovich & West (2000); Kahneman (2011); Piccinini (2011)",
            "uuid": "e6913.6",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        },
        {
            "name_short": "Chorus/RBF",
            "name_full": "Chorus of Prototypes / RBF interpretation",
            "brief_description": "Interpretation linking radial-basis-function (RBF) networks and prototype-based representations: RBF units interpreted as prototypes in a conceptual space.",
            "citation_title": "Representation, similarity, and the chorus of prototypes",
            "mention_or_use": "mention",
            "theory_name": "Chorus of Prototypes / RBF Interpretation",
            "theory_type": "prototype-in-RBF / mapping from neural units to geometric prototypes",
            "theory_description": "Units in RBF networks correspond to prototype locations in a geometric space; network activation reflects similarity to prototypes, enabling geometric interpretations of hidden-layer representations.",
            "functional_claims": "Provides a bridge between neural network hidden units and interpretable prototype-like representations, enabling modeling of similarity and chimeric/intermediate items and reducing opacity.",
            "evidence_source": "computational simulation; theoretical mapping",
            "experimental_paradigm": "analysis of RBF network activations; computational demonstrations",
            "key_result": "RBF networks can be read as representing prototypes and hence mapped onto Conceptual Spaces, facilitating similarity-based classification and handling of chimeric cases.",
            "supports_theory": true,
            "counter_evidence": "Mapping may be partial and depends on network architecture; general deep network interpretability still challenging beyond RBF-like cases.",
            "citation": "Edelman (1995); Balkenius (1999)",
            "uuid": "e6913.7",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        },
        {
            "name_short": "OshersonCrit",
            "name_full": "Osherson & Smith compositionality critique (fuzzy logic counterexample)",
            "brief_description": "Argument that prototype/typicality representations conflict with compositional logic: conjunctions cannot be more typical than constituents under standard logical aggregation (example: 'polka-dot zebra').",
            "citation_title": "On the adequacy of prototype theory as a theory of concepts",
            "mention_or_use": "mention",
            "theory_name": "Compositionality Critique of Prototype Representations",
            "theory_type": "theoretical critique (logical constraints)",
            "theory_description": "Logical and fuzzy-logical composition rules imply that conjunction truth-values cannot exceed those of conjuncts, contradicting prototypical cases where the conjunction appears more typical than either constituent.",
            "functional_claims": "Shows a formal incompatibility between standard compositional logics (including simple fuzzy conjunction) and observed typicality phenomena, motivating non-logical or hybrid representations.",
            "evidence_source": "theoretical argument; illustrative examples",
            "experimental_paradigm": "conceptual examples and logical analysis (e.g., polka-dot zebra)",
            "key_result": "Prototype-based typicality ratings (high for 'polka-dot zebra') conflict with compositional constraints of logic/fuzzy conjunctions (bounded by constituents).",
            "supports_theory": false,
            "counter_evidence": "Conceptual Spaces and geometric intersection of regions provide a way to reconcile compositional appearance with typicality (authors argue region intersection allows conjunctive typicality exceeding constituent centrality).",
            "citation": "Osherson and Smith (1981)",
            "uuid": "e6913.8",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        },
        {
            "name_short": "WordEmb",
            "name_full": "Word Embeddings (word2vec / GloVe)",
            "brief_description": "Distributional vector-space models that embed word meanings as points in high-dimensional Euclidean space learned from co-occurrence statistics.",
            "citation_title": "Efficient estimation of word representations in vector space",
            "mention_or_use": "mention",
            "theory_name": "Word Embeddings (Distributional Semantics)",
            "theory_type": "high-dimensional statistical vector space",
            "theory_description": "Words are represented as learned vectors capturing statistical co-occurrence patterns; semantic similarity corresponds to vector distance or cosine similarity.",
            "functional_claims": "Capture lexical similarity and syntactic/semantic regularities; useful for many NLP tasks but dimensions lack direct cognitive interpretability.",
            "evidence_source": "computational simulation/benchmark performance",
            "experimental_paradigm": "intrinsic similarity tasks, downstream NLP tasks",
            "key_result": "Embeddings model word similarity well but their dimensions are not interpretable as cognitive quality dimensions; authors note embeddings as intermediate between data and conceptual levels.",
            "supports_theory": true,
            "counter_evidence": "Not designed to model conceptual-level typicality/prototype phenomena or to provide cognitively meaningful quality dimensions; limited for commonsense reasoning.",
            "citation": "Mikolov et al. (2013); Pennington et al. (2014)",
            "uuid": "e6913.9",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        },
        {
            "name_short": "NEF",
            "name_full": "Neural Engineering Framework / Semantic Pointers",
            "brief_description": "A biologically inspired framework (NEF) using vector representations (semantic pointers) to implement neural populations performing vector operations and symbolic-like binding.",
            "citation_title": "Neural engineering: Computation, representation, and dynamics in neurobiological systems",
            "mention_or_use": "mention",
            "theory_name": "Neural Engineering Framework (NEF) / Semantic Pointers",
            "theory_type": "population-vector / compressed symbolic representations",
            "theory_description": "Neural populations represent vectors (semantic pointers) enabling compression, binding (e.g., circular convolution), and transformation operations that support symbol-like cognitive functions in neural hardware.",
            "functional_claims": "Explains how neural populations can implement structured, compressed vector representations supporting composition/binding and bridging symbolic and subsymbolic processing (used in SPAUN).",
            "evidence_source": "computational simulation; large-scale neural modeling",
            "experimental_paradigm": "large-scale model demonstrations (e.g., SPAUN); mapping population activity to vector operations",
            "key_result": "Semantic pointers demonstrate how neural systems can encode compositional structures and reduce opacity by giving an interpretable vector-level description of population activity.",
            "supports_theory": true,
            "counter_evidence": "Requires significant modeling assumptions; mapping to cognitive-level quality dimensions still nontrivial, but compatible with Conceptual Spaces interpretation.",
            "citation": "Eliasmith and Anderson (2004); Eliasmith et al. (2012)",
            "uuid": "e6913.10",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        },
        {
            "name_short": "Diagram",
            "name_full": "Diagrammatic / Analogical Representations (mental models, mental images)",
            "brief_description": "Picture-like or spatial representations (mental images, mental models) that preserve structural/spatial relations and allow analogical/trajectory reasoning.",
            "citation_title": "Mental models: Towards a cognitive science of language, inference, and consciousness",
            "mention_or_use": "mention",
            "theory_name": "Diagrammatic / Analogical Representations",
            "theory_type": "spatial / analog / pictorial representations",
            "theory_description": "Cognitive content is represented in analog, picture-like formats (mental images or diagrammatic models) that preserve spatial or topological relations and support perceptual-like manipulations and trajectory extrapolation.",
            "functional_claims": "Support efficient and transparent reasoning in spatial domains, intuitive planning and prediction via trajectories, and explain phenomena where symbolic logic fails (e.g., round-table 'to the right of' problem).",
            "evidence_source": "behavioral experiments; theoretical/computational examples",
            "experimental_paradigm": "mental imagery tasks, deductive reasoning tasks modeled with mental models, planning scenarios",
            "key_result": "Diagrammatic representations handle many spatial reasoning and dynamic identity-tracking tasks more naturally than purely symbolic systems; Conceptual Spaces can unify and generalize diagrammatic properties (trajectory extrapolation in quality space).",
            "supports_theory": true,
            "counter_evidence": "Lack of a unified theory and formal framework; not dominant relative to symbolic/connectionist systems but complementary.",
            "citation": "Johnson-Laird (1983, 2006); Glasgow et al. (1995)",
            "uuid": "e6913.11",
            "source_info": {
                "paper_title": "Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation",
                "publication_date_yy_mm": "2017-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual spaces: The geometry of thought",
            "rating": 2,
            "sanitized_title": "conceptual_spaces_the_geometry_of_thought"
        },
        {
            "paper_title": "Cognitive representations of semantic categories",
            "rating": 2,
            "sanitized_title": "cognitive_representations_of_semantic_categories"
        },
        {
            "paper_title": "On the adequacy of prototype theory as a theory of concepts",
            "rating": 2,
            "sanitized_title": "on_the_adequacy_of_prototype_theory_as_a_theory_of_concepts"
        },
        {
            "paper_title": "Representation, similarity, and the chorus of prototypes",
            "rating": 2,
            "sanitized_title": "representation_similarity_and_the_chorus_of_prototypes"
        },
        {
            "paper_title": "Deep learning",
            "rating": 1,
            "sanitized_title": "deep_learning"
        },
        {
            "paper_title": "Learning about categories in the absence of memory",
            "rating": 2,
            "sanitized_title": "learning_about_categories_in_the_absence_of_memory"
        },
        {
            "paper_title": "Neural engineering: Computation, representation, and dynamics in neurobiological systems",
            "rating": 2,
            "sanitized_title": "neural_engineering_computation_representation_and_dynamics_in_neurobiological_systems"
        },
        {
            "paper_title": "Thinking, fast and slow",
            "rating": 1,
            "sanitized_title": "thinking_fast_and_slow"
        },
        {
            "paper_title": "Efficient estimation of word representations in vector space",
            "rating": 1,
            "sanitized_title": "efficient_estimation_of_word_representations_in_vector_space"
        },
        {
            "paper_title": "Are there dimensions in the brain?",
            "rating": 1,
            "sanitized_title": "are_there_dimensions_in_the_brain"
        }
    ],
    "cost": 0.01606125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation
January 3, 2017</p>
<p>Antonio Lieto 
ICAR-CNR
PalermoItaly</p>
<p>Dip. di Informatica
University of Turin
TorinoItaly</p>
<p>National Research Nuclear University
MEPhI, MoscowRussia</p>
<p>Antonio Chella 
ICAR-CNR
PalermoItaly</p>
<p>University of Palermo
DIID
PalermoItaly</p>
<p>Marcello Frixione 
University of Genoa
DAFIST
GenovaItaly</p>
<p>Conceptual Spaces for Cognitive Architectures: A Lingua Franca for Different Levels of Representation
January 3, 20176893626873C18D1AD035507907C9123010.1016/j.bica.2016.10.005arXiv:1701.00464v1[cs.AI]Preprint submitted to ElsevierKnowledge RepresentationCognitive ArchitecturesCognitive ModellingConceptual Spaces
During the last decades, many cognitive architectures (CAs) have been realizedadopting different assumptions about the organization and the representation of their knowledge level.Some of them (e.g.SOAR [Laird (2012)]) adopt a classical symbolic approach, some (e.g.LEABRA[O'Reilly and Munakata (2000)]) are based on a purely connectionist model, while others (e.g.CLARION [Sun   (2006)]) adopt a hybrid approach combining connectionist and symbolic representational levels.Additionally, some attempts (e.g.biSOAR) trying to extend the representational capacities of CAs by integrating diagrammatical representations and reasoning are also available[Kurup and Chandrasekaran (2007)].In this paper we propose a reflection on the role that Conceptual Spaces, a framework developed by PeterGärdenfors [Gärdenfors (2000)] more than fifteen years ago, can play in the current development of the Knowledge Level in Cognitive Systems and Architectures.In particular, we claim that Conceptual Spaces offer a lingua franca that allows to unify and generalize many aspects of the symbolic, sub-symbolic and diagrammatic approaches (by overcoming some of their typical problems) and to integrate them on a common ground.In doing so we extend and detail some of the arguments explored by Gärdenfors[Gärdenfors (1997)] for defending the need of a conceptual, intermediate, representation level between the symbolic and the sub-symbolic one.In particular we</p>
<p>Introduction</p>
<p>Within the field of cognitive modeling, it is nowadays widely assumed that different kinds of representation are needed in order of accounting for both biological and artificial cognitive systems.Examples are the broad class of neural network representations (including deep neural networks); the vast family of symbolic formalisms (including logic and Bayesian or probabilistic ones); analogical representations such as mental images, diagrammatic representations, mental models, and various kinds of hybrid systems combining in different ways the approaches mentioned above.</p>
<p>All these methods are successful in explaining and modeling certain classes of cognitive phenomena, but no one can account for all aspects of cognition.This problem also holds if we consider some recent successful artificial systems.</p>
<p>For example, the Watson system is based on a probabilistic system able to reason on enormous amounts of data, but it mostly fails to account for trivial common-sense reasoning (see [Davis and Marcus (2015)], p. 94).Similarly, the AlphaGo system [Silver et al. (2016)], based on massive training of deep neural networks, is impressively successful in the well-defined domain of the Go game.However, it is not able to transfer its approach in general or crossdomain settings.In general, this is a classical obstacle of neural networks: in order to solve a particular problem they need to be trained by a suitable and vast training set.Then, however, how to employ the learned strategies to solve similar problems is still an open issue1 .</p>
<p>Based on this evidence, our claim is that the Knowledge Level of cognitive artificial systems and architectures can take advantage of a variety of different representations.In this perspective, the problem arises of their integration in a theoretically and cognitively motivated way.While, in fact, existing hybrid systems and architectures [Sun (2006)] are able to combine different kinds of representations (see for example the class of neuro-symbolic systems [D' Avila Garcez et al. (2008)]), nonetheless this kind of integration is usually ad hoc based [Chella et al. (1998)] or, as we will show in the following sections, is only partially satisfying.Our hypothesis is that Conceptual Spaces can offer a lingua franca that allows to unify and generalize many aspects of the representational approaches mentioned above and to integrate them on common ground.</p>
<p>The paper is organized as follows: in Section 2 we report how, in Cognitive Science research, the problem of conceptual representations intended as a heterogeneous phenomenon has gained attention and experimental support in the last decades.In Section 3, we consider this pluralistic representational stance in the area of Artificial Intelligence by focusing on some of the most widely known representational approaches adopted in literature.Section 4 provides a synthetic description of Conceptual Spaces, the representational framework that we propose as a lingua franca for the connection of the different representational levels used in different CAs.In Section 5 we outline the advantages offered by the Conceptual Spaces representation used as a grounding layer for the classical AI approaches reviewed in Section 3. In doing so we extend and detail some of the arguments explored by Gärdenfors [Gärdenfors (1997)] for defending the need of a conceptual, intermediate, representation level between the symbolic and the sub-symbolic one.Conclusions end the paper.</p>
<p>Heterogeneity of Representations in Cognitive Science: The Case of Concepts</p>
<p>In this Section, we present some empirical evidence from Cognitive Science that favor the hypothesis of the heterogeneity of representations in cognitive systems and architectures.In particular, we take into account two classes of evidence concerning conceptual representations: the description of non-classical concepts (Sect.2.1) and the application of the dual process distinction to conceptual knowledge (Sect.2.2).</p>
<p>Representing Non-Classical Concepts</p>
<p>In Cognitive Science, different theories about how humans represent, organize and reason on their conceptual knowledge have been proposed.In the traditional view, known as the classical or Aristotelian theory, concepts are defined as sets of necessary and sufficient conditions.Such theory was dominant in philosophy and psychology from the antiquity until the mid-70s of the last century, when the empirical results of Eleanor Rosch [Rosch (1975)] demonstrated the inadequacy of such a theory for ordinary common sense concepts.</p>
<p>These results showed that familiar concepts often exhibit typicality effects.The results obtained by Rosch have had a crucial importance for the development of different theories of concepts trying to explain various representational and reasoning aspects concerning typicality.Usually, such theories are grouped into three broad classes: (i) the prototype theories, developed starting from the work of Rosch; (ii) exemplars theories; and (iii) theory-theories (see e.g.[Murphy (2002)] and [Machery (2009)] for a detailed review of such approaches).All of them are assumed to account for some aspects of the typicality effects in conceptualization (such as that one of common-sense categorization).</p>
<p>According to the prototype view, knowledge about categories is stored using prototypes, i.e., representations of the best instance of a category.For example, the concept CAT coincides with a representation of a typical cat.In the simpler versions of this method, prototypes are represented as (possibly weighted) lists of features.</p>
<p>According to the exemplar view, a category is represented as set of specific exemplars explicitly stored within memory: the mental representation of the concept CAT is thus the set of the representations of (some of) the cats encountered during lifetime.</p>
<p>Theory-theories approaches adopt some form of holistic point of view about concepts.According to versions of theory-theories, concepts are analogous to theoretical terms in a scientific theory.For example, the concept CAT is individuated by the role it plays in our mental theory of zoology.In other versions of the approach, concepts themselves are identified with micro-theories of some sort.For example, the concept CAT is a mentally represented micro theory about cats.</p>
<p>Despite such approaches have been historically considered as competitors, since they propose different models, and they have different predictions about how the humans organize and reason on conceptual information, various works (starting from Barbara Malt [Malt (1989)]) showed that they are eventually not mutually exclusive.Rather, they seem to succeed in explaining different classes of cognitive phenomena.In particular, empirical data -i.e., behavioral measures as categorization probability and reaction times -suggest that subjects use different representations to categorize.Some people employ exemplars, a few rely on prototypes, and others appeal to both exemplars and prototypes.Some representations seem to be more suitable for certain tasks, or for certain categories.Also, this distinction seems to have also neural plausibility witnessed by many empirical results (the first in this line is due to Squire and Nolton [Squire and Knowlton (1995)]).</p>
<p>Such experimental results led to the development of the so-called heterogeneous hypothesis about the nature of conceptual representations, according to which concepts do not constitute a unitary phenomenon.In particular, different types of conceptual representations are assumed to exist.All such representations represent different bodies of knowledge associated with the same category.</p>
<p>Each body of conceptual knowledge is thus manipulated by various processes involved in multiple tasks (e.g.recognition, learning, categorization).</p>
<p>Dual-Process oriented Conceptual Representations</p>
<p>A further divide between different kinds of conceptual representations refers to the dual process hypothesis about reasoning and rationality.According to dual process theories [ (Stanovich and West (2000)], [Evans and Frankish (2009)],</p>
<p>[ Kahneman (2011)]) two different types of cognitive processes and systems exist, which have been called respectively System(s) 1 and System(s) 2.</p>
<p>System 1 processes are automatic.They are phylogenetically older and shared by humans and other animal species.They are innate and control instinctive behaviors, so they do not depend on training or particular individual abilities and, in general, they are cognitively undemanding.They are associative and operate in a parallel and fast way.Moreover, System 1 processes are not consciously accessible to the subject.</p>
<p>System 2 processes are phylogenetically recent and are peculiar to the human species.They are conscious and cognitively penetrable (i.e.accessible to consciousness) and based on explicit rule following.As a consequence, if compared to System 1, System 2 processes are sequential and slower, and cognitively demanding.Performances that depend on System 2 processes are usually affected by acquired skills and differences in individual capabilities2 .</p>
<p>The dual process approach was initially proposed to account for systematic errors in reasoning.Such errors (consider, e.g., the classical examples of the selection task or the conjunction fallacy) should be ascribed to fast, associative and automatic type 1 processes, while type 2 is responsible for the slow and cognitively demanding activity of producing answers that are correct concerning the canons of normative rationality.</p>
<p>In general, many aspects of the psychology of concepts have presumably to do with fast, type 1 systems and processes, while others can be plausibly ascribed to type 2.</p>
<p>For example, the categorization process based on typical traits (either pro-totypically represented or based on exemplars or theories) is, presumably, a fast and automatic process which does not require any explicit effort and which could likely be attributed to a type 1 system.On the contrary, there are types of inference that are usually included within conceptual abilities, which are slow and cognitively demanding and which should be attributed to processes that are more likely to be ascribed to type 2. Consider the ability to make explicit high-level inferences involving conceptual knowledge, and the capacity to justify them.Or consider classification: classifying a concept amounts to individuating its more specific superconcepts and its more general subconcepts, or, in other words, to identify implicit superconcept-subconcept relations in a taxonomy.</p>
<p>For human subjects such a process is usually slow, it requires great effort, and it is facilitated by specific training.So, according to the dual process theories, the inferential task of classifying concepts in taxonomies is prima facie a type 2 process.It is qualitatively different from the task of categorizing items as instances of a particular class on the basis of typical traits (e.g. the task of classifying Fido as a dog because it barks, has fur and wags his tail).Therefore, it is plausible that conceptual representation in cognitive systems should be assigned to (at least) two different kinds of components responsible for different tasks.In particular, type 2 processes are involved in complex and cognitively demanding inference tasks, and fast and automatic type 1 processes are involved in categorization based on the common-sense information.A similar theoretical position is defended by Piccinini [Piccinini (2011)], according to which only two kinds of concept exist: implicit and explicit; he correlates implicit and explicit concepts respectively to system 1 and system 2 processes.</p>
<p>More recently, it has been also argued [Frixione and Lieto (2013)] that a cognitively plausible artificial model of conceptual representation should be based on a dual process approach and, as such, formed by different components based on various representations inspired by the previous distinction.Some available systems have been developed based on this hypothesis [Lieto et al. (2015[Lieto et al. ( , to appear 2016))] and integrated with available Cognitive Architectures such as ACT-R Anderson et al. (2004), andCLARION [Sun (2006)].In such systems the type 1 processes have been demanded to the Conceptual Spaces framework (Sect.4), while the type 2 processes have been demanded to standard symbolic representations (Sect.3.1).Systems whose conceptual processing activity is based on the dual process approach have been also recently investigated in the area of computational creativity of robotic agents [Augello et al. (2016)].</p>
<p>Representational Formalisms and Approaches in AI</p>
<p>We claim that the plurality of heterogeneous representations observed in natural cognitive systems and exemplified in the above section is also recommended in the design of cognitive artificial systems and architectures.In AI different types of representational approaches have been proposed and can be successfully used to model various aspects of the conceptual heterogeneity described above.</p>
<p>In the following, we shortly take into account three primary examples: symbolic representations (Sect.3.1), neural networks representations (Sect.3.2), and diagrammatic and analogical representations (Sect.3.3).</p>
<p>Symbolic Representations</p>
<p>Symbolic representations, which in many cases rely on some logic formalism, are usually well suited for dealing with complex reasoning tasks.Such systems are characterized by the compositionality of representations: in a compositional system of representations, we can distinguish between a set of primitive, or atomic, symbols and a set of complex symbols.Complex symbols are generated from primitive symbols through the application of suitable recursive syntactic rules: generally, a potentially infinite set of complex symbols can be generated from a finite set of primitive symbols.The meaning of complex symbols can be determined starting from the meaning of primitive symbols, using recursive semantic rules that work in parallel with syntactic composition rules.</p>
<p>Compositionality has been considered an irrevocable trait of human cognition: in classical cognitive science, it is often assumed that mental representations are indeed compositional.A clear and explicit formulation of this assumption was proposed by Fodor and Pylyshyn as a criticism of neural networks and connectionist systems [Fodor and Pylyshyn (1988)].They claim that the compositionality of mental representations is mandatory to explain fundamental cognitive phenomena (i.e., the generative and systematic character of human cognition) and that neural networks are not compositional.</p>
<p>Compositionality is also a characteristic of symbolic artificial systems, and many knowledge representation formalisms are indeed compositional.In the field of Cognitive Architectures, for example, SOAR is one of the most famous systems exploiting symbolic and compositional representations of knowledge (called chunks) and using pattern matching to select relevant knowledge elements.This system adheres strictly to the Newell and Simon's physical symbol system hypothesis [Newell and Simon (1976)] which states that symbolic processing is a necessary and sufficient condition for intelligent behavior.However, compositionality cannot be easily accommodated with some cognitive phenomena.For example, it is somewhat at odds with the representation of concepts in terms of typicality [Frixione and Lieto (2011)], as we shall see in greater details in Sect.5.1 below.This problem is not limited to the empirical analysis of natural cognitive systems; it is of main relevance also for the design of cognitive artificial systems and architectures.The clash between compositionality and typicality requirements in symbolic representations is evident the field of artificial conceptual modelling.Consider, for example, description logics and ontological languages (e.g.OWL), which are fully compositional but not able to account for typicality 3 .At the same time, as mentioned, representing concepts using typicality is relevant for computational applications (and in particular for 3 In the field of logic-oriented Knowledge Representation (KR) various fuzzy and nonmonotonic extensions of description logics formalisms have been designed to deal with some aspects of non-classical concepts [Giordano et al. (2013)], [Straccia (2002)].Nonetheless, various theoretical and practical problems remain unsolved and, in general, an acceptable KR framework able to provide a practically usable trade-off regarding language expressivity and complexity has been not yet achieved [Frixione and Lieto (2012)].(In particular, on fuzzy logic and typicality effects, see sect.5.1 below.)those of cognitive inspiration).More in general, Fodor and Pylyshyn are true when they claim that neural networks are not compositional.However, it is also true that, in the development of cognitive artificial systems and architectures we do not want to give up with some of the advantages offered by the neural networks.It is likely that compositionality has to do with higher order cognition and with complex, type 2, inferential tasks while neural networks are more appropriate to model type 1 phenomena.The problem remains of the interaction of these two classes of formalisms.</p>
<p>In general, in the symbolic AI tradition, an attempt to mitigate this aspect has been proposed with the Bayesian networks [Nielsen and Jensen (2009)].</p>
<p>Bayesian networks can be considered as a class of symbolic representations, where the relations between concepts are weighted by their strength, calculated through statistical computations.Despite the recent successes of the Bayesian approach for the explanation of many cognitive tasks [Griffiths et al. (2008)], the acceptance of explaining intelligence of both natural and artificial minds in terms of Bayesian Machines is still far from being achieved.Many forms of commonsense knowledge in human cognition do not require Bayesian predictions about what will happen or, in general, to reason probabilistically [Sloman (2014)].In addition, also in these more sophisticated cases of symbolic representations, the problematic aspects of reconciling compositionality and typicality requirements remains, as we shall see in section 5.1, unsolved.</p>
<p>Neural Networks Representations</p>
<p>Neural networks are a class of representations employed successfully in many architectures and in many difficult tasks (see for example the AlphaGo system mentioned above).In general, in the field of CAs, this class of representation has been widely employed to deal with the fast behavior of a dynamic system and for aspects mainly related to learning and perception.Neural networks are particularly well suited for classification tasks.As a consequence, they are widely adopted in many pattern recognition problems in AI: typical case studies concern the recognition of handwritten letters and numbers.</p>
<p>Differently from symbolic representations, neural networks receive input data directly coming from sensory systems, as images, signals, and so on, and thus the problem of grounding representations to entities in the external world (which is notoriously arduous for symbolic systems) is in some sense alleviated.The importance of neural networks for symbol grounding has been discussed by Harnad in a seminal paper [Harnad (1990)].From this point of view, the main advantage of deep neural networks, and in particular of Convolutional Neural Networks, is that they are even closer to sensory data, and therefore they need less or no preprocessing of input data (see, e.g., the recent review by [LeCun et al. (2015)]).</p>
<p>However, representations based on neural networks are problematic in many senses.For example, as already anticipated above, it is challenging to implement compositionality in neural networks [Fodor and Pylyshyn (1988)], [Frixione et al. (1989)].Moreover, it is unclear how to implement complex reasoning and planning tasks, which are naturally modeled by symbolic formalisms.As a consequence, the typical move is to employ some hybrid neuro-symbolic systems.This is the case, for example, of the ACT-R architecture [Anderson et al. (2004)], that employs a sub-symbolic activation of symbolic conceptual chunks representing the encoded knowledge.In some cases, e.g. in ACT-R, this hybrid approach successfully allows to overcome, in a cognitively grounded perspective, many problems of the sub-symbolic and symbolic representations considered in isolation.In other cases, as earlier mentioned, this integration is ad hoc based and does not provide any explanatory model of the underlying processes integrating the representations.In any case, however, the classical well-known problem of neural networks remains their opacity: a neural network behaves as a sort of black box and specific interpretation for the operation of its units and weights is far from trivial (on this aspect, see Sect.5.2).</p>
<p>Diagrammatic and Analogical Representations</p>
<p>In the last decades, many types of representation have been proposed, both in Cognitive Science and in AI, which share some characteristic with pictures or, more in general, with diagrams and analog representations.Consider for example the debate on mental images that affects Cognitive Science since the seventies [Kosslyn et al. (2006)].According to supporters of mental images, some mental representations have the form of pictures in the mind.</p>
<p>There are other examples of analog representations besides mental images.</p>
<p>Consider the notion of mental model as proposed by Philip Johnson-Lard [Johnson- Laird (1983)], [Johnson- Laird (2006)].According to Johnson-Laird, many human cognitive performances (e.g. in the field of deductive reasoning) can be better accounted for by hypothesizing the processing of analog representations called mental models, rather than the manipulation of sentence-like representations such as logical axioms and rules.For example, according to Johnson-Laird, subjects, when performing a deductive inference, first create and merge an analog model of the premises, and then they check the resulting model to draw a conclusion.</p>
<p>Many pictorial, analog or diagrammatic models have been proposed in various fields of Cognitive Science, which take advantage of forms of representations that are picture-like, in the sense that they spatially resemble to what they represent (see e.g.[Glasgow et al. (1995)] and, in the field of planning, [Frixione et al. (2001)]).</p>
<p>This class of representations is heterogeneous, and it is surely not majoritarian if compared to the main streams of symbolic/logic based systems and of neural networks.Moreover, they lack a general theory, and, despite their intuitive appeal, a common and well understood theoretical framework does not exist.However, in spatial domains, they present various advantages.If compared to sub-symbolic models they are much more transparent; when compared with symbolic representations, they are often more intuitive, and they avoid the need of a complete explicit axiomatization.As mentioned above, some attempts also exists trying to embed diagrammatical representation in CAs [Kurup and Chandrasekaran (2007)].</p>
<p>From the empirical point of view, none of the above surveyed families of representations alone is able to account for the whole spectrum of phenomena concerning human cognition.This suggests that, also in artificial systems, a plurality of representational approaches is needed.However, the way in which these representations interact is not clear both from an empirical point and from a computational point of view.</p>
<p>Conceptual Spaces as a Lingua Franca</p>
<p>Our thesis is that geometrical representations, and in particular Conceptual Spaces [Gärdenfors (2000)], constitute a common language that enables the interaction between different types of representations.On one hand, they allow overcoming some limitations of the symbolic systems (see Sect. 5.1) concerning both the common sense and the anchoring problems.On the other hand, they represent a sort of blueprint useful for designing and modelling artificial neural networks in a less opaque way.Moreover, they provide a more abstract level for the interpretation of the underlying neural mechanisms (see Sect. 5.2).</p>
<p>Finally, thanks to their geometrical nature, they offer a unifying framework for interpreting many kinds of diagrammatic and analogical representation (see</p>
<p>Sect. 5.3).</p>
<p>The theory of Conceptual Spaces provides a robust framework for the internal representations in a cognitive agent.In the last fifteen years, such framework has been employed in a vast range of AI applications spanning from visual perception [Chella et al. (1997)] to robotics [Chella et al. (2003)], from question answering [Lieto et al. (2015)] to music perception [Chella (2015)] (see [Zenker and Gärdenfors (2015)] for a recent overview).According to Gärdenfors, Conceptual Spaces represent an intermediate level of representation between the sub-symbolic and the symbolic one.The main feature of a Conceptual Space is given by the introduction of a geometrical framework for the representation of knowledge based on the definition of a number of quality dimensions describing concepts.In brief, a Conceptual Space is a metric space in which entities are characterized by quality dimensions [Gärdenfors (2000)].In some cases, such dimensions can be directly related to perceptual information; examples of this kind are temperature, weight, brightness, pitch.In other cases, dimensions can be more abstract in nature 4 .</p>
<p>To each quality dimension is associated a geometrical (topological or metrical) structure.The central idea behind this approach is that the representation of knowledge can take advantage of the geometrical structure of the Conceptual Spaces.The dimensions of a Conceptual Space represent qualities of the environment independently from any linguistic formalism or description.In this sense, a Conceptual Space comes before any symbolic characterization of cognitive phenomena.A point in a Conceptual Space corresponds to an epistemologically primitive entity at the considered level of analysis.For example, in the case of visual perception, a point in a Conceptual Space is obtained from the measurements of the external world performed, e.g., by a camera, through the subsequent processing of the low-level vision algorithms.</p>
<p>Concepts are represented as regions in Conceptual Spaces.An important aspect of the theory is the definition of a metric function.Following Gärdenfors, the distance between two points in a Conceptual Space, calculated according to a metric function, corresponds to the measure of the perceived similarity between 4 In this paper we will not consider the problem of the acquisition of such representations.We just mention that there are many successful approaches recently proposed in Computational Linguistics and Distributional Semantics [Pennington et al. (2014);Mikolov et al. (2013b,a)] aiming at learning vectorial structures, called word embeddings, from massive amounts of textual documents.Word embeddings represent the meaning of words as points in a high-dimensional Euclidean space, and are in this sense reminiscent of Conceptual Spaces.However, they differ from Conceptual Spaces in at least two crucial ways that limit their usefulness for applications in knowledge representation.First, word embedding models are mainly aimed at modelling word-similarity, and are not aimed at providing a geometric representation of the conceptual information (and a framework able to perform forms of common-sense reasoning based, for example, on prototypes).Moreover, the dimensions of a word embedding space are essentially meaningless since they correspond, given an initial word, to the most statistically relevant words co-occurring with it, while quality dimensions in Conceptual Spaces directly reflect salient cognitive properties of the underlying domain.In this sense the word embeddings can be seen an intermediate step between the data level and the conceptual one in language-oriented technologies.</p>
<p>the entities corresponding to the points themselves.For example, instances (or exemplars) of a concept are represented as points in space, and their similarity can be calculated in a natural way in the terms of their distance according to some suitable distance measure.</p>
<p>A further aspect of Conceptual Space theory has to do with the role of convex sets of points in conceptualization.According to the previously cited work by Rosch Rosch (1975), the so-called natural categories represent the most informative level of categorization in taxonomies of real world entities.They are the most differentiated from one another, and constitute the preferred level for reference.Also, they are the first to be learned by children and categorization at their level is usually faster.</p>
<p>Gärdenfors proposes the Criterion P, according to which natural categories correspond to convex sets in some suitable Conceptual Space.As a consequence, betweenness is significant for natural categories, in that for every pair of points belonging to a convex set (and therefore sharing some features), all the points between them belong to the same set, and they share in their turn the same features.</p>
<p>Natural categories thus correspond to convex regions.In such scenario, therefore, prototypes and typicality effects taking place at the conceptual level have a natural geometrical interpretation: prototypes correspond to the geometrical centroid of the region itself.Then, given a certain concept, a degree of centrality is associated to each point that falls within the corresponding region.This level of centrality may be interpreted as a measure of its typicality.</p>
<p>Conversely, given a set of n prototypes represented as points in a Conceptual</p>
<p>Space, a tessellation of the space in n convex regions can be determined in the terms of the so-called Voronoi diagrams [Okabe et al. (2000)].In sum, one of the main features of Conceptual Spaces is represented by the fact that, differently from the models situated at the sub-symbolic and symbolic level, they provide a natural way of explaining typicality effects on concepts.Their geometrical structure allows a natural way of calculating the semantic similarity among concepts and exemplars by using classical topological (e.g., based on the Region Connection Calculus [Gärdenfors and Williams (2001)]) or metrical distances.</p>
<p>Gärdenfors mostly concentrated on the representation of typicality concerning prototypes.However, Conceptual Spaces allow in a natural way the representation of non-classical concepts also in terms of exemplars [Frixione and Lieto (2013)] (as we said above in Sect.2.1, prototypes and exemplars are two complementary approaches that can explain different aspects of typicality).</p>
<p>On the Advantages of Conceptual Spaces</p>
<p>In the following Sections, we outline some of the advantages offered by Conceptual Spaces representations in dealing with the problems posed by the representational formalisms overviewed previously.Such analysis supports our claim that a grounding of the outlined representations in terms of Conceptual Spaces could overcome some of their limitations.</p>
<p>Prototypes and Compositionality in Symbolic Representations</p>
<p>As we anticipated, compositionality can be hardly accommodated with typicality effects.In this Section, we shall argue that Conceptual Spaces could allow reconciling these two important aspects of conceptual representations.According to a well-known argument ([Fodor (1981)]; [Osherson and Smith (1981)]), prototypes are not compositional.In brief, the argument runs as follows: consider a concept like pet fish.It results from the composition of the concept pet and of the concept fish.However, the prototype of pet fish cannot result from the composition of the prototypes of a pet and a fish: a typical pet is furry and warm, a typical fish is grayish, but a typical pet fish is neither furry and warm nor grayish.</p>
<p>Let us consider a version of this argument against the possibility of reconciling compositionality and typicality effects in symbolic systems that dates back to Osherson and Smith [Osherson and Smith (1981)].Osherson and Smith's original aim was to show that fuzzy logic is inadequate to capture typicality, but, as we shall see, the effect of the argument is general.At first sight, fuzzy logic seems to be a promising approach to face the problem of typicality.Indeed, one consequence of typicality effects is that some members of a category C turn out to be better (i.e. more typical) instances of C than others.For example, a robin is a better example of the category of birds than, say, a penguin or an ostrich.More typical instances of a category are those that share a greater number of characteristic features (e.g. the ability to fly for birds, having fur for mammals, and so on).The fuzzy value of a predicate (say, F ) could be interpreted as a measure of typicality.In facts, given two individuals h and k, it is natural to assume that F (h) &gt; F (k) iff h is a more typical instance of F than k.Pina is presumably a good instance of the concept polka dot zebra; therefore, if such a concept were represented as a fuzzy predicate, then the value of the formula polka dot zebra(P ina) should be close to 1, say:
polka dot zebra(P ina) = .97(1)
On the other hand, Pina is a rather poor (i.e.atypical) instance of the concept zebra; therefore the value of the formula zebra(P ina) should be low, say:
zebra(P ina) = .2(2)
(of course, the specific values are not relevant here; the point is that Pina is more typical as a polka dot zebra than as a zebra).But polka dot zebra can be expressed as the conjunction of the concepts zebra and polka dot thing; i.e.</p>
<p>in logical terms, it holds that:
∀x(polka dot zebra(x) ↔ zebra(x) ∧ polka dot thing(x))(3)
Now, the problem is the following: if we adopt the simplest and most widespread version of fuzzy logic, then the value of a conjunction is calculated as the minimum of the values of its conjuncts.Thus, it is impossible for the value of zebra(P ina) to be .2and that of polka dot zebra(P ina) to be .97at the same time.Of course, there are other types of fuzzy logic, in which the value of a conjunction is not the minimum of the values of the conjuncts.However, a conjunction cannot exceed the value of its conjuncts.Worse still, in general in logic, once a suitable order has been imposed on truth values, it holds that:
val(A ∧ B) ≤ val(A) and val(A ∧ B) ≤ val(B)(4)
So, the problem pointed out by Osherson and Smith does not seem to concern fuzzy logic only.Rather, Osherson and Smith's argument seems to show that, in general, logic-based representations are unlikely to be compatible with typicality effects5 .Moreover, logic-based representations are paradigmatic examples of compositional systems, which fully embody the Fregean principle of compositionality of meaning.</p>
<p>Indeed, the situation is more promising if, instead of logic, we face typicality by adopting a geometrical representation based on Conceptual Spaces.</p>
<p>As previously stated, if we represent a concept as a convex area in a suitable Conceptual Space, then the degree of typicality of a certain individual can be measured as the distance of the corresponding point from the center of the area.</p>
<p>The conjunction of two concepts is represented as the intersection of the two corresponding areas, as in Fig. 2. According to the conceptual space approach, Pina should presumably turn out to be very close to the center of polka dot zebra (i.e. to the intersection between zebra and polka thing).In other words, she should turn out to be a very typical polka dot zebra, despite being very eccentric on both the concepts zebra and polka dot thing; that is to say, she is an atypical zebra and an atypical polka dot thing.This representation better captures our intuitions about typicality.We conclude that the treatment of compositionality and that of some forms of typicality require rather different approaches and forms of representation, and should therefore presumably be assigned to different knowledge components of a cognitive architecture.</p>
<p>Interpretation of Neural Networks</p>
<p>As mentioned, neural networks, although successful in many difficult tasks are particularly well suited for classification tasks and have been widely adopted in CAs.One of the well-known problems of this class of representations is their opacity.A neural network behaves as a sort of black box: specific interpretation is troublesome for the operations of units and weights.In many cases, this is arduous to accept.Let us consider for example the case of medical domain, where it is not sufficient to classify the symptoms of a disease but it is also required to provide a detailed explanation for the provided classification of symptoms.</p>
<p>This problem is much more pressing if we consider the case of deep neural networks where, because of the huge number of the hidden layers, there are much more units and weights to interpret.The opacity of this class of representations is also unacceptable in CAs aiming at providing transparent models of human cognition and that, as such, should be able not only to predict the behavior of a cognitive artificial agent but also to explain it.</p>
<p>A possible way out to this problem is represented by the interpretation of the network in terms of a more abstract geometric point of view.It is true that it is feasible to have a simple geometric interpretation of the operation of a neural network: in fact, the operation of each layer may be described as a functional geometric space where the dimensions are related to the transfer functions of the units of the layer itself.In this interpretation, the connection weights between layers may be described in terms of transformation matrices from one space to another.</p>
<p>However, while the interpretation of the input and output spaces depends on the given training set and the particular design of the network, the interpretation of the hidden spaces is typically tough.However, the literature reports sparse cases where a partial interpretation of the operations of the units is possible: a recent example is reported by Zhou [Zhou et al. (2015)].A more general attempt to interpret the activity of a neural network in terms of information geometry is due to [Amari and Nagaoka (2007)].</p>
<p>We claim that the theory of Conceptual Spaces can be considered as a sort of designing style that helps to model more transparent neural networks, and it can facilitate the grounding and the interpretation of the hidden layers of units.As a consequence, the interpretation of neural network representations in terms of Conceptual Spaces provides a more abstract and transparent view on the underlying behavior of the networks.</p>
<p>Gärdenfors [Gärdenfors (2000)] offers a simple analysis of the relationship between Conceptual Spaces and Self Organising Maps.Hereafter, Balkenius [Balkenius (1999)] proposes a more articulate interpretation of the widely adopted RBF networks in terms of dimensions of a suitable Conceptual Space.Accord-ing to this approach, a neural network built by a set of RBF units can be interpreted as a simple Conceptual Space described by a set of integral quality dimensions.Consequently, a neural network built by a set of sets of RBF units may be geometrically interpreted by a conceptual space made up by sets of integral dimensions.</p>
<p>Additionally, following the Chorus of Prototypes approach proposed by Edelman [Edelman (1995)], the units of an RBF network can be interpreted as prototypes in a suitable Conceptual Space.This interpretation enables the measurement of similarity between the input of the network and the prototypes corresponding to the units.Such an interpretation would have been much more problematic by considering the neural network alone, since this information would have been implicit and hidden.Moreover, it is possible to take into account the delicate cases of Chimeric entities, which are almost equidistant between two or more prototypes.For example, a Chimera is a lion with a goat head, and therefore, it results equidistant between the prototype of the lion and goat prototypes (see [Edelman (1995)]).This aspect is related to the example of the polka dotted zebra provided in the previous Section.In this respect, the capability of accounting for the compositionally based on typicality traits seems to be a crucial feature of the Conceptual Spaces empowering both symbolic and sub-symbolic representations6 .</p>
<p>Finally, a recent work going in the direction of a more abstract interpretation of neural representations, and describing how the population of neural representations can be interpreted as representing vectors obtained through different kind of operations (e.g.compression and recursive binding by using circular convolutions, see [Crawford et al. (2015)],) is obtained by the Semantic Pointers Perspective adopted by the NEF (Neural Engineering framework) [Eliasmith and Anderson (2004)] and representing the core of the biologically inspired SPAUN architecture [Eliasmith et al. (2012)].Such perspective is completely compatible with our proposal of providing a more abstract interpretation of neural mechanisms and representations through multidimensional Conceptual Spaces.To account for such a simple fact by symbolic axioms or rules would require making explicit a huge number of detailed and complicated assertions.Conversely, the adoption of some form of analogic representation such as mental models associated with suitable procedures, e.g.procedures for the generation, revision, and inspection of mental models, would allow facing the problem in a more natural and straightforward way.</p>
<p>Unifying Analogical and Diagrammatic Representations</p>
<p>diagrammatic representation systems [Matessa and Brockett (2007)].As the properties of an object are modified, the point, representing it in the Conceptual Space, moves according to a certain trajectory.Since usually this modifications happens smoothly and not abruptly, several assumptions can be made on this trajectory, e.g., smoothness, and obedience to physical laws [Chella et al. (2004)].</p>
<p>Figuring out the evolution of an object as its future position, or the way in which its features are going to change, can be seen as the extrapolation of a trajectory in a Conceptual Space.To identify again an object that has been occluded for a certain time interval amounts to interpolate its past and present trajectories.In general, this characteristic represents a powerful heuristic to track the identity of an individual object.Also in this case, crucial aspects of diagrammatic representations find a more general and unifying interpretation regarding Conceptual Spaces.</p>
<p>Conclusions</p>
<p>We have proposed Conceptual Spaces as a sort of lingua franca allowing to unify and integrate on a common ground the symbolic, sub-symbolic and diagrammatic approaches and to overcome some well-known problems specific to such representations.In particular, by extending and detailing some of the arguments proposed by Gärdenfors [Gärdenfors (1997)] for defending the need Our proposal may be of particular interest if we consider designing a general cognitive architecture where all these types or representation co-exist, as it is also assumed in the current experimental research in Cognitive Science 7 .In this case, the Conceptual Spaces offer the common ground where all the representations find a theoretically and geometrically well-founded interpretation.</p>
<p>Figure 1 :
1
Figure 1: An exemplar of the concept of Polka Dot Zebra</p>
<p>Figure 2 :
2
Figure 2: Compositionality of Prototypes in Conceptual Spaces</p>
<p>Analogical and diagrammatic representations allow representing in an efficient and intuitive way kinds of information that would require a very complex and cumbersome amount of details if explicitly represented by symbolic and logic-oriented declarative formalisms.Let us consider a simple example that has been discussed by Philip Johnson-Laird [Johnson-Laird (1983)].The relation to be to the right of is usually transitive: if A is to the right of B and B is to the right of C then A is to the right of C. But consider the case in which A, B and C are arranged around, say, a small circular table.In this case, it can happen that C is to the right of B, B is to the right of A but C is not to the right of A: A and C are opposite (see Fig.3below).</p>
<p>Figure 3 :
3
Figure 3: The round table problem</p>
<p>of a conceptual, intermediate, representation level between the symbolic and the sub-symbolic one we have shown how Conceptual Space allow dealing with conceptual typicality effects, which is a classic problematic aspect for symbolic and logic-oriented symbolic approaches.Moreover, Conceptual Spaces enable a more transparent interpretation of underlying neural network representations, by limiting the opacity problems of this class of formalism, and it may constitute a sort of blueprint for the design of such networks.Finally, Conceptual Spaces offer a unifying framework for interpreting many kinds of diagrammatic and analogical representation.</p>
<p>This issue is also explicitly reported by Hassabis in an interview published on Nature http://goo.gl/9fUy4Z.
A shared assumption of the dual process hypothesis is that both systems can be composed in their turn by many sub-systems and processes.
The arguments holds also if we consider to model the indicated situation in terms of a Bayesian network where the strength of the weighted value of the symbolic node polka dot zebra is assumed to be composed by the values of two nodes zebra and polka dot thing, as indicated in the example.
It is worth-noting that also some forms of neuro-symbolic integration currently developed in CAs like ACT-R, and belonging to the class of the neo-connectionist approaches, allows to deal with the the above mentioned problem by providing a series of mechanisms that are able to deal with limited forms of compositionality in neural networks[O'Reilly et al. (2013)] and that can be integrated with additional processes allowing the compatibility with typicality effects. In this respect, such approaches play an equivalent role to that one played by the Conceptual Spaces on these issues. In addition, however, we claim that Conceptual Spaces can offer a unifying framework for interpreting many kinds of diagrammatic and analogical representations (see section 5.3). On these classes of representations, limited work has been done by these hybrid neuro-symbolic systems (including ACT-R)[Matessa and Brockett (2007)]. This is a symptom that the treatment of their representational and reasoning mechanisms is not trivial in these environments and that often they need to be integrated with external
In[Chella et al. (2012)] it is also discussed the possible role that such architectural per-
AcknowledgementsWe thank Salvatore Gaglio and Peter Gärdenfors for the discussions on the topics presented in this article.References
Methods of information geometry. S Amari, -I, H Nagaoka, 2007American Mathematical Soc191</p>
<p>An integrated theory of the mind. J R Anderson, D Bothell, M D Byrne, S Douglass, C Lebiere, Y Qin, Psychological review. 111410362004</p>
<p>Artwork creation by a cognitive architecture integrating computational creativity and dual process approaches. A Augello, I Infantino, A Lieto, G Pilato, R Rizzo, F Vella, Biologically Inspired Cognitive Architectures. 152016</p>
<p>Are there dimensions in the brain? Spinning Ideas, Electronic Essays Dedicated to Peter Gärdenfors on His Fiftieth Birthday. C Balkenius, 1999</p>
<p>A cognitive architecture for music perception exploiting conceptual spaces. A Chella, 2015Applications of Conceptual Spaces. Springerspective can play to perform meta-computation, another crucial element for cognitive architectures</p>
<p>Perceptual anchoring via conceptual spaces. A Chella, S Coradeschi, M Frixione, A Saffiotti, Proceedings of the AAAI-04 Workshop on Anchoring Symbols to Sensor Data. the AAAI-04 Workshop on Anchoring Symbols to Sensor Data2004</p>
<p>A general theoretical framework for designing cognitive architectures: Hybrid and meta-level architectures for bica. A Chella, M Cossentino, S Gaglio, V Seidita, Biologically Inspired Cognitive Architectures. 22012</p>
<p>A cognitive architecture for artificial vision. A Chella, M Frixione, S Gaglio, Artificial Intelligence. 8911997</p>
<p>An architecture for autonomous agents exploiting conceptual representations. A Chella, M Frixione, S Gaglio, Robotics and Autonomous Systems. 2531998</p>
<p>Anchoring symbols to conceptual spaces: the case of dynamic scenarios. A Chella, M Frixione, S Gaglio, Robotics and Autonomous Systems. 4322003</p>
<p>Biologically plausible, humanscale knowledge representation. E Crawford, M Gingerich, C Eliasmith, Cognitive science. 2015</p>
<p>Neural-symbolic cognitive reasoning. D'avila Garcez, A S Lamb, L C Gabbay, D M , 2008Springer Science &amp; Business Media</p>
<p>Commonsense reasoning and commonsense knowledge in artificial intelligence. E Davis, G Marcus, Communications of the ACM. 5892015</p>
<p>Representation, similarity, and the chorus of prototypes. S Edelman, Minds and Machines. 511995</p>
<p>Neural engineering: Computation, representation, and dynamics in neurobiological systems. C Eliasmith, C H Anderson, 2004MIT press</p>
<p>A large-scale model of the functioning brain. C Eliasmith, T C Stewart, X Choo, T Bekolay, T Dewolf, Y Tang, D Rasmussen, Science. 33861112012</p>
<p>J S B Evans, K E Frankish, two minds: Dual processes and beyond. Oxford University Press2009</p>
<p>The present status of the innateness controversy. J A Fodor, Representations: Philosophical Essays on the Foundations of Cognitive Science. J A Fodor, Cambridge, MAMIT Press1981</p>
<p>Connectionism and cognitive architecture: A critical analysis. J A Fodor, Z W Pylyshyn, Cognition. 281-21988</p>
<p>Representing concepts in artificial systems: a clash of requirements. M Frixione, A Lieto, Proc. HCP. HCP2011</p>
<p>Representing concepts in formal ontologies. compositionality vs. typicality effects. M Frixione, A Lieto, Logic and Logical Philosophy. 2142012</p>
<p>Dealing with Concepts: from Cognitive Psychology to Knowledge Representation. M Frixione, A Lieto, Frontiers in Psychological and Behavioural Science. 2013</p>
<p>Symbols and subsymbols for representing knowledge: a catalogue raisonne. M Frixione, G Spinelli, S Gaglio, Proceedings of the 11th international joint conference on Artificial intelligence. the 11th international joint conference on Artificial intelligenceMorgan Kaufmann Publishers Inc19891</p>
<p>Diagrammatic reasoning for planning and intelligent control. M Frixione, G Vercelli, R Zaccaria, Control Systems. 2122001IEEE</p>
<p>Symbolic, conceptual and subconceptual representations. P Gärdenfors, Human and Machine Perception. 1997Springer</p>
<p>Conceptual spaces: The geometry of thought. P Gärdenfors, 2000MIT press</p>
<p>Reasoning about categories in conceptual spaces. P Gärdenfors, M.-A Williams, IJCAI'01Proceedings of the 17th International Joint Conference on Artificial Intelligence. the 17th International Joint Conference on Artificial IntelligenceSan Francisco, CA, USAMorgan Kaufmann Publishers Inc20011</p>
<p>A non-monotonic description logic for reasoning about typicality. L Giordano, V Gliozzi, N Olivetti, G L Pozzato, Artificial Intelligence. 1952013</p>
<p>Diagrammatic reasoning: Cognitive and computational perspectives. J Glasgow, N H Narayanan, B Chandrasekaran, 1995Mit Press</p>
<p>Bayesian models of cognition. T L Griffiths, C Kemp, J B Tenenbaum, 2008Cambridge University Press</p>
<p>The symbol grounding problem. S Harnad, Physica D: Nonlinear Phenomena. 4211990</p>
<p>Mental models: Towards a cognitive science of language, inference, and consciousness. P N Johnson-Laird, 1983Harvard University Press</p>
<p>P N Johnson-Laird, How we reason. USAOxford University Press2006</p>
<p>Thinking, fast and slow. D Kahneman, 2011Macmillan</p>
<p>The case for mental imagery. S M Kosslyn, W L Thompson, G Ganis, 2006Oxford University Press</p>
<p>Modeling memories of large-scale space using a bimodal cognitive architecture. U Kurup, B Chandrasekaran, Proceedings of the eighth international conference on cognitive modeling. Citeseer. the eighth international conference on cognitive modeling. Citeseer2007</p>
<p>The Soar cognitive architecture. J Laird, 2012MIT Press</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, Nature. 52175532015</p>
<p>A common-sense conceptual categorization system integrating heterogeneous proxytypes and the dual process 28 of reasoning. A Lieto, D P Radicioni, V Rho, Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI). the International Joint Conference on Artificial Intelligence (IJCAI)Buenos AiresAAAI Press2015</p>
<p>Dual peccs: A cognitive system for conceptual representation and categorization. A Lieto, D P Radicioni, V Rho, Journal of Experimental &amp; Theoretical Artificial Intelligence. JETAIto appear 2016</p>
<p>Doing without concepts. E Machery, 2009OUP</p>
<p>An on-line investigation of prototype and exemplar strategies in classification. B C Malt, Journal of Experimental Psychology: Learning, Memory, and Cognition. 1545391989</p>
<p>Using a diagram representation system with act-r. M Matessa, A Brockett, Proceedings of sixteenth conference on behavior representation in modeling and simulation. sixteenth conference on behavior representation in modeling and simulationBaltimore, MDSISO2007</p>
<p>T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781Efficient estimation of word representations in vector space. 2013aarXiv preprint</p>
<p>Distributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in neural information processing systems. 2013b</p>
<p>The big book of concepts. G L Murphy, 2002MIT press</p>
<p>Computer science as empirical inquiry: Symbols and search. A Newell, H A Simon, Communications of the ACM. 1931976</p>
<p>Bayesian networks and decision graphs. T D Nielsen, F V Jensen, 2009Springer Science &amp; Business Media</p>
<p>Spatial Tessellations -Concepts and Applications of Voronoi Diagrams. A Okabe, B Boots, K Sugihara, S N Chiu, 2000John Wiley &amp; SonsChichesterSecond Edition</p>
<p>Computational explorations in cognitive neuroscience: Understanding the mind by simulating the brain. R C O'reilly, Y Munakata, 2000MIT press</p>
<p>. R C O'reilly, A A Petrov, J D Cohen, C J Lebiere, S A Herd, T Kriete, I P Calvo, J Symons, 2013How limited systematicity emerges: A computational cognitive neuroscience approach</p>
<p>On the adequacy of prototype theory as a theory of concepts. D N Osherson, E E Smith, Cognition. 911981</p>
<p>Glove: Global vectors for word representation. J Pennington, R Socher, C D Manning, EMNLP. 142014</p>
<p>Two kinds of concept: Implicit and explicit. G Piccinini, Dialogue. 50012011</p>
<p>Cognitive representations of semantic categories. E Rosch, J. Exp. Psychol. Gen. 10431975</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, Nature. 52975872016</p>
<p>How can we reduce the gulf between artificial and natural intelligence?. A Sloman, Proceegings of AIC 2014, 2nd International Workshop on Artificial Intelligence and Cognition. Ceur-Ws, eegings of AIC 2014, 2nd International Workshop on Artificial Intelligence and Cognition20141315</p>
<p>Learning about categories in the absence of memory. L R Squire, B J Knowlton, Proceedings of the National Academy of Sciences. 92261995</p>
<p>Advancing the rationality debate. K E Stanovich, R F West, Behavioral and brain sciences. 23052000</p>
<p>Reasoning within fuzzy description logics. U Straccia, Journal of Artificial Intelligence Research. 142002</p>
<p>The CLARION cognitive architecture: Extending cognitive modeling to social simulation. Cognition and multi-agent interaction. R Sun, 2006</p>
<p>Applications of conceptual spaces -The case for geometric knowledge representation. F Zenker, P Gärdenfors, 2015Springer</p>
<p>B Zhou, A Khosla, A Lapedriza, A Oliva, A Torralba, arXiv:1412.6856Object detectors emerge in deep scene cnns. 2015arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
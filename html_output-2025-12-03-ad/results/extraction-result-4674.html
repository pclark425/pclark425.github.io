<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4674 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4674</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4674</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-269983098</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.14751v2.pdf" target="_blank">AGILE: A Novel Reinforcement Learning Framework of LLM Agents</a></p>
                <p><strong>Paper Abstract:</strong> We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4674.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4674.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AGILE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AGILE (AGent that Interacts and Learns from Environments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end reinforcement learning framework for LLM agents that composes an LLM policy, an external memory module, tools, and an executor; supports proactive human advice-seeking and reflection to distill and store knowledge for future sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AGILE agent (agile-vic13b-ppo / agile-mek7b-ppo)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-centered agent where the LLM is the token-level policy; an executor maps special function-name tokens to environment actions (memory I/O, tool calls, seek advice, reflection, etc.). Memory and tools are external components invoked by the LLM via function tokens; the agent is trained first by imitation learning then by PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Vicuna-13b (agile-vic13b-ppo) and Meerkat-7b (agile-mek7b-ppo)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>ProductQA (new dataset), MedMCQA, HotPotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Complex question answering across sessions: answer product/customer-service questions (ProductQA), multi-choice medical exam questions (MedMCQA), and multi-hop open-domain questions (HotPotQA) while optionally retrieving memory, using search tools, seeking human advice, and reflecting to update memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External long-term memory with embedding-indexed retrieval; memory stores historical QA pairs and distilled knowledge entries (i.e., episodic/knowledge memory combined).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw long-answer/short-answer QA texts and distilled 'knowledge entries' (text), with embeddings (all-MiniLM-L6-v2) used for retrieval indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>On-demand via function tokens: [UpdateMemory] invoked by the LLM (often after [Reflection]) writes selected segments of the context (human answer and reflected distilled knowledge) into memory; memory can be initialized (e.g., MedMCQA) or empty (ProductQA).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Embedding-based similarity search (all-MiniLM-L6-v2); [RetrieveMemory] fetches the most relevant historical QA pair and knowledge entry (ProductQA typically retrieves top 1; MedMCQA retrieves top 5 in training data generation) and appends them to LLM context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ProductQA (agile-vic13b-ppo, averaged over six test groups): advice rate 0.233, short-answer accuracy 0.854, total score 0.784 (seeking advice cost c=0.3). MedMCQA (agile-mek7b-ppo): advice rate 0.316, accuracy 0.852, total score 0.726. HotPotQA (agile-vic13b-ppo): advice rate 0.156, exact-match accuracy 0.675, GPT-4-eval accuracy 0.858, total score 0.628.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ProductQA ablation (w/o Memory): advice rate 0.407, accuracy 0.876, total score 0.754 (average; -4.0% relative total score vs with-memory baseline). MedMCQA ablation (agile-mek7b-w/o Memory): advice rate 0.506, accuracy 0.741, total score 0.539 (large drop vs with-memory 0.726). HotPotQA not reported explicitly for w/o Memory in main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Comprehensive ablation: disabling memory increases advice-seeking and harms total reward (ProductQA: advice rate ↑ from 0.233 to 0.407; total score ↓ 4.0% relative), and substantially harms MedMCQA performance (total score dropped from 0.726 to 0.539). Paper also reports that removing reflection or tools increases advice-seeking; RL training reduces advice-seeking and improves total score. The authors report per-task ablations in Appendix/Tables 5 and 15.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Long-range dependencies across sessions make trajectory-level RL expensive; deciding what to store in memory and estimating the future value of stored knowledge is nontrivial; token-probability-based confidence is unreliable for deciding when to seek advice; privacy constraints (memory access restricted per-product in ProductQA) complicate retrieval; embedding retrieval and memory scope must be carefully designed to avoid leaking sensitive data; sometimes ablations showed mixed effects on immediate accuracy (e.g., ProductQA w/o Memory had higher accuracy but far higher advice rate and lower total reward).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use embedding-based retrieval to fetch relevant QA and distilled knowledge; employ a Reflection step to distill human/expert answers before writing to memory; update memory on-demand (via explicit [UpdateMemory]) rather than indiscriminately; initialize memory appropriately per domain (e.g., preload MedMCQA with training QA pairs); represent the value of memory additions via a session-level advantage term (Ai) and optimize with session-level PPO to handle long trajectories; tune the human-advice cost parameter c to control the accuracy/cost trade-off; restrict memory retrieval scope for privacy-sensitive settings; train decision policies end-to-end with RL (PPO) so the agent learns when to use memory and when to seek advice.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper frames memory use and advice-seeking as explicit actions the LLM can choose (function-token-based), and presents implementation details (executor behavior, embedding model choice) and specific heuristics for proxy rewards quantifying expected future utility of stored knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AGILE: A Novel Reinforcement Learning Framework of LLM Agents', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4674.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4674.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting) framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based approach combining chain-of-thought style reasoning traces with interleaved external actions (tool calls) to enable LLM agents to plan and act interactively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct agent (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prompted LLM baseline that interleaves reasoning traces and actions (e.g., [Search]) to perform multi-step QA and tool use; used as a baseline for HotPotQA in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (used in ReAct-gpt4-prompt baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>HotPotQA (used as evaluation benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop question answering using interleaved reasoning and search actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>ReAct was reimplemented as a baseline and considered suboptimal in its original public implementation; the authors reproduced improved ReAct results with GPT-4 for comparison, but do not analyze ReAct memory strategies in depth.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper notes ReAct baseline lacks an explicit long-term memory accumulation component; when compared to AGILE, ReAct-based baselines are outperformed on HotPotQA.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>No explicit recommendations about memory from ReAct in this paper; AGILE authors highlight the advantage of adding memory + reflection + RL over pure ReAct prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AGILE: A Novel Reinforcement Learning Framework of LLM Agents', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4674.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4674.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALFWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALFWorld</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive text/world environment for evaluating planning and navigation skills of agents in text-based simulated environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ALFWorld: Aligning text and embodied environments for interactive learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ALFWorld agents (in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Text-environment agents (related work) used for planning/navigation benchmarks in prior research; cited to motivate need for long-term memory and interactive agent evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>ALFWorld (text/embodied simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Planning and navigation tasks in text-to-action embodied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Mentioned as an example of an interactive text environment in related work; no experiments or memory analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not analyzed in this paper; cited to emphasize that existing benchmarks do not comprehensively evaluate long-term memory accumulation together with human interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>No specific recommendations; AGILE positions its memory+RL approach as complementary to such benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AGILE: A Novel Reinforcement Learning Framework of LLM Agents', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4674.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4674.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated interactive environment for evaluating agents on science-oriented tasks in text-driven worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ScienceWorld: Is your agent smarter than a 5th grader?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ScienceWorld agents (in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Benchmarks in related work for evaluating interactive text-based agent capabilities (science tasks); cited among environments motivating AGILE's features.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>ScienceWorld (text-based interactive environment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Science reasoning, planning and interaction in a text-simulated environment.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Mentioned in related work as an interactive benchmark; no direct experiments or memory-focused analysis here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>None specific in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AGILE: A Novel Reinforcement Learning Framework of LLM Agents', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4674.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4674.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Webshop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Webshop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A web-environment benchmark for grounded language agents that must plan and use web tools to complete shopping tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Webshop: Towards scalable real-world web interaction with grounded language agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Webshop agents (in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Related benchmark for web-tool usage and planning in e-commerce-like simulations; cited to position ProductQA and AGILE's tool+memory focus.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Webshop (simulated web interaction benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tool usage and planning for web-based shopping tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Cited as prior work focusing on tool usage and planning; AGILE claims ProductQA extends coverage to long-term memory and human interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not directly analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AGILE: A Novel Reinforcement Learning Framework of LLM Agents', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning text and embodied environments for interactive learning <em>(Rating: 2)</em></li>
                <li>ScienceWorld: Is your agent smarter than a 5th grader? <em>(Rating: 2)</em></li>
                <li>Webshop: Towards scalable real-world web interaction with grounded language agents <em>(Rating: 2)</em></li>
                <li>Mind2Web: Towards a generalist agent for the web <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4674",
    "paper_id": "paper-269983098",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "AGILE",
            "name_full": "AGILE (AGent that Interacts and Learns from Environments)",
            "brief_description": "An end-to-end reinforcement learning framework for LLM agents that composes an LLM policy, an external memory module, tools, and an executor; supports proactive human advice-seeking and reflection to distill and store knowledge for future sessions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "AGILE agent (agile-vic13b-ppo / agile-mek7b-ppo)",
            "agent_description": "An LLM-centered agent where the LLM is the token-level policy; an executor maps special function-name tokens to environment actions (memory I/O, tool calls, seek advice, reflection, etc.). Memory and tools are external components invoked by the LLM via function tokens; the agent is trained first by imitation learning then by PPO.",
            "llm_model_name": "Vicuna-13b (agile-vic13b-ppo) and Meerkat-7b (agile-mek7b-ppo)",
            "game_or_benchmark_name": "ProductQA (new dataset), MedMCQA, HotPotQA",
            "task_description": "Complex question answering across sessions: answer product/customer-service questions (ProductQA), multi-choice medical exam questions (MedMCQA), and multi-hop open-domain questions (HotPotQA) while optionally retrieving memory, using search tools, seeking human advice, and reflecting to update memory.",
            "memory_used": true,
            "memory_type": "External long-term memory with embedding-indexed retrieval; memory stores historical QA pairs and distilled knowledge entries (i.e., episodic/knowledge memory combined).",
            "memory_representation": "Raw long-answer/short-answer QA texts and distilled 'knowledge entries' (text), with embeddings (all-MiniLM-L6-v2) used for retrieval indexing.",
            "memory_update_mechanism": "On-demand via function tokens: [UpdateMemory] invoked by the LLM (often after [Reflection]) writes selected segments of the context (human answer and reflected distilled knowledge) into memory; memory can be initialized (e.g., MedMCQA) or empty (ProductQA).",
            "memory_retrieval_mechanism": "Embedding-based similarity search (all-MiniLM-L6-v2); [RetrieveMemory] fetches the most relevant historical QA pair and knowledge entry (ProductQA typically retrieves top 1; MedMCQA retrieves top 5 in training data generation) and appends them to LLM context.",
            "performance_with_memory": "ProductQA (agile-vic13b-ppo, averaged over six test groups): advice rate 0.233, short-answer accuracy 0.854, total score 0.784 (seeking advice cost c=0.3). MedMCQA (agile-mek7b-ppo): advice rate 0.316, accuracy 0.852, total score 0.726. HotPotQA (agile-vic13b-ppo): advice rate 0.156, exact-match accuracy 0.675, GPT-4-eval accuracy 0.858, total score 0.628.",
            "performance_without_memory": "ProductQA ablation (w/o Memory): advice rate 0.407, accuracy 0.876, total score 0.754 (average; -4.0% relative total score vs with-memory baseline). MedMCQA ablation (agile-mek7b-w/o Memory): advice rate 0.506, accuracy 0.741, total score 0.539 (large drop vs with-memory 0.726). HotPotQA not reported explicitly for w/o Memory in main tables.",
            "has_performance_comparison": true,
            "ablation_or_analysis": "Comprehensive ablation: disabling memory increases advice-seeking and harms total reward (ProductQA: advice rate ↑ from 0.233 to 0.407; total score ↓ 4.0% relative), and substantially harms MedMCQA performance (total score dropped from 0.726 to 0.539). Paper also reports that removing reflection or tools increases advice-seeking; RL training reduces advice-seeking and improves total score. The authors report per-task ablations in Appendix/Tables 5 and 15.",
            "challenges_or_limitations": "Long-range dependencies across sessions make trajectory-level RL expensive; deciding what to store in memory and estimating the future value of stored knowledge is nontrivial; token-probability-based confidence is unreliable for deciding when to seek advice; privacy constraints (memory access restricted per-product in ProductQA) complicate retrieval; embedding retrieval and memory scope must be carefully designed to avoid leaking sensitive data; sometimes ablations showed mixed effects on immediate accuracy (e.g., ProductQA w/o Memory had higher accuracy but far higher advice rate and lower total reward).",
            "best_practices_or_recommendations": "Use embedding-based retrieval to fetch relevant QA and distilled knowledge; employ a Reflection step to distill human/expert answers before writing to memory; update memory on-demand (via explicit [UpdateMemory]) rather than indiscriminately; initialize memory appropriately per domain (e.g., preload MedMCQA with training QA pairs); represent the value of memory additions via a session-level advantage term (Ai) and optimize with session-level PPO to handle long trajectories; tune the human-advice cost parameter c to control the accuracy/cost trade-off; restrict memory retrieval scope for privacy-sensitive settings; train decision policies end-to-end with RL (PPO) so the agent learns when to use memory and when to seek advice.",
            "notes": "The paper frames memory use and advice-seeking as explicit actions the LLM can choose (function-token-based), and presents implementation details (executor behavior, embedding model choice) and specific heuristics for proxy rewards quantifying expected future utility of stored knowledge.",
            "uuid": "e4674.0",
            "source_info": {
                "paper_title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting) framework",
            "brief_description": "A prompting-based approach combining chain-of-thought style reasoning traces with interleaved external actions (tool calls) to enable LLM agents to plan and act interactively.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "agent_name": "ReAct agent (baseline)",
            "agent_description": "Prompted LLM baseline that interleaves reasoning traces and actions (e.g., [Search]) to perform multi-step QA and tool use; used as a baseline for HotPotQA in the paper.",
            "llm_model_name": "GPT-4 (used in ReAct-gpt4-prompt baseline in experiments)",
            "game_or_benchmark_name": "HotPotQA (used as evaluation benchmark)",
            "task_description": "Multi-hop question answering using interleaved reasoning and search actions.",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "ReAct was reimplemented as a baseline and considered suboptimal in its original public implementation; the authors reproduced improved ReAct results with GPT-4 for comparison, but do not analyze ReAct memory strategies in depth.",
            "challenges_or_limitations": "Paper notes ReAct baseline lacks an explicit long-term memory accumulation component; when compared to AGILE, ReAct-based baselines are outperformed on HotPotQA.",
            "best_practices_or_recommendations": "No explicit recommendations about memory from ReAct in this paper; AGILE authors highlight the advantage of adding memory + reflection + RL over pure ReAct prompting.",
            "uuid": "e4674.1",
            "source_info": {
                "paper_title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ALFWorld",
            "name_full": "ALFWorld",
            "brief_description": "An interactive text/world environment for evaluating planning and navigation skills of agents in text-based simulated environments.",
            "citation_title": "ALFWorld: Aligning text and embodied environments for interactive learning",
            "mention_or_use": "mention",
            "agent_name": "ALFWorld agents (in related work)",
            "agent_description": "Text-environment agents (related work) used for planning/navigation benchmarks in prior research; cited to motivate need for long-term memory and interactive agent evaluation.",
            "llm_model_name": null,
            "game_or_benchmark_name": "ALFWorld (text/embodied simulator)",
            "task_description": "Planning and navigation tasks in text-to-action embodied environments.",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Mentioned as an example of an interactive text environment in related work; no experiments or memory analysis in this paper.",
            "challenges_or_limitations": "Not analyzed in this paper; cited to emphasize that existing benchmarks do not comprehensively evaluate long-term memory accumulation together with human interaction.",
            "best_practices_or_recommendations": "No specific recommendations; AGILE positions its memory+RL approach as complementary to such benchmarks.",
            "uuid": "e4674.2",
            "source_info": {
                "paper_title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ScienceWorld",
            "name_full": "ScienceWorld",
            "brief_description": "A simulated interactive environment for evaluating agents on science-oriented tasks in text-driven worlds.",
            "citation_title": "ScienceWorld: Is your agent smarter than a 5th grader?",
            "mention_or_use": "mention",
            "agent_name": "ScienceWorld agents (in related work)",
            "agent_description": "Benchmarks in related work for evaluating interactive text-based agent capabilities (science tasks); cited among environments motivating AGILE's features.",
            "llm_model_name": null,
            "game_or_benchmark_name": "ScienceWorld (text-based interactive environment)",
            "task_description": "Science reasoning, planning and interaction in a text-simulated environment.",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Mentioned in related work as an interactive benchmark; no direct experiments or memory-focused analysis here.",
            "challenges_or_limitations": "Not analyzed in this paper.",
            "best_practices_or_recommendations": "None specific in this paper.",
            "uuid": "e4674.3",
            "source_info": {
                "paper_title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Webshop",
            "name_full": "Webshop",
            "brief_description": "A web-environment benchmark for grounded language agents that must plan and use web tools to complete shopping tasks.",
            "citation_title": "Webshop: Towards scalable real-world web interaction with grounded language agents",
            "mention_or_use": "mention",
            "agent_name": "Webshop agents (in related work)",
            "agent_description": "Related benchmark for web-tool usage and planning in e-commerce-like simulations; cited to position ProductQA and AGILE's tool+memory focus.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Webshop (simulated web interaction benchmark)",
            "task_description": "Tool usage and planning for web-based shopping tasks.",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Cited as prior work focusing on tool usage and planning; AGILE claims ProductQA extends coverage to long-term memory and human interaction.",
            "challenges_or_limitations": "Not directly analyzed in this paper.",
            "uuid": "e4674.4",
            "source_info": {
                "paper_title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "ALFWorld: Aligning text and embodied environments for interactive learning",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "ScienceWorld: Is your agent smarter than a 5th grader?",
            "rating": 2,
            "sanitized_title": "scienceworld_is_your_agent_smarter_than_a_5th_grader"
        },
        {
            "paper_title": "Webshop: Towards scalable real-world web interaction with grounded language agents",
            "rating": 2,
            "sanitized_title": "webshop_towards_scalable_realworld_web_interaction_with_grounded_language_agents"
        },
        {
            "paper_title": "Mind2Web: Towards a generalist agent for the web",
            "rating": 1,
            "sanitized_title": "mind2web_towards_a_generalist_agent_for_the_web"
        }
    ],
    "cost": 0.016847749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AGILE: A Novel Reinforcement Learning Framework of LLM Agents
5 Nov 2024</p>
<p>Peiyuan Feng 
ByteDance Research</p>
<p>Yichen He 
ByteDance Research</p>
<p>Guanhua Huang guanhuahuang@mail.ustc.edu.cn 
University of Science and Technology of China</p>
<p>Yuan Lin linyuan.0@bytedance.com 
ByteDance Research</p>
<p>Hanchong Zhang zhanghanchong@sjtu.edu.cn 
Shanghai Jiao Tong University</p>
<p>Yuchen Zhang zhangyuchen.zyc@bytedance.com 
ByteDance Research</p>
<p>Hang Li 
ByteDance Research</p>
<p>AGILE: A Novel Reinforcement Learning Framework of LLM Agents
5 Nov 2024836E00EA4BC42A72DD3365AEC276C517arXiv:2405.14751v2[cs.LG]
We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts.The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation.We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model.We fine-tune the LLM using labeled data of actions and the PPO algorithm.We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping.Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents.Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance.Datasets and code are available at https://github.com/bytarnish/AGILE.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have exhibited remarkable capabilities such as instruction following, reasoning, and zero-shot learning [3,45,20,26], which have greatly catalyzed the development of autonomous agents based on LLMs [28,30,2], also known as LLM agents.Recent works propose several essential components or workflows to enhance the abilities of LLM agents, such as planning [45,51,39], reflection [21,40], tool-use [29,36,48] and life-long learning [42].However, it remains unclear how to integrate all components into a unified framework and optimize them end-to-end.</p>
<p>In this paper, we introduce a novel reinforcement learning framework for LLM agents to unify various components and streamline their learning and operation processes.As shown in Figure 1(a), the architecture of the agent system, named AGILE, comprises four modules: LLM, memory, tools, and executor.Furthermore, the agent can interact with both users and experts.The LLM, functioning as the predictor of all actions, generates instructions and processes responses.The executor, working as the controller of all actions, interprets the LLM instructions to activate the corresponding modules and collects their responses for the LLM.For example, the executor can fetch a text from the memory and append it to the context of LLM, or extract an excerpt from the context and append it to the memory.</p>
<p>Figure 1: (a) Architecture of our agent system, including LLM, memory, tools, and executor.(b) A running example of AGILE in a customer service QA environment.The tokens (actions) generated by the LLM are in orange color and the tokens appended by the executor are in blue color.</p>
<p>The executor can also follow instructions of the LLM to utilize a search tool.In addition to skills such as reasoning, planning, and reflection, we propose a new ability called seeking advice, which means that the agent proactively consults human experts when it encounters a problem unsolvable.The agent can reflect on the expert feedback and memorize it for future use.Furthermore, we propose a training method based on reinforcement learning (RL), which simultaneously trains the policy of invoking different modules and the reasoning, planning, reflection, and seeking advice abilities of the LLM agent in an end-to-end fashion.</p>
<p>While the proposed agent framework is general, in this paper, we evaluate it in complex question answering (QA).It is a task an LLM agent has the potential of outperforming existing solutions such as the use of an LLM alone.However, existing QA benchmarks [12,49,11,27] are designed for specific subsets of capabilities (e.g., reflection, memory retrieve, etc.) which cannot simultaneously investigate the ability to combine all modules and capabilities of the agent.</p>
<p>To address this, we have developed a new benchmark called ProductQA.ProductQA comprises 88,229 question-answer pairs in customer service divided into 26 QA tasks, each corresponding to a distinct Amazon product category.This benchmark is based on real Amazon user queries and includes fact-based questions, reasoning questions, and product recommendation queries.It comprehensively evaluates agents' abilities to handle historical information and accumulated knowledge, leverage tools, interact with humans, perform self-evaluation, and conduct reflection.Additionally, the training and testing tasks are made disjoint to assess the agent's ability to adapt to new product categories.</p>
<p>We evaluate our agent framework on three tasks, ProductQA, MedMCQA [27] and HotPotQA [49].For ProductQA, we use a two-stage training method based on Vicuna-13b [6].In the first stage, imitation learning is employed to create agile-vic13b-sft.In the second stage, the policy gradient algorithm of PPO [37] produces agile-vic13b-ppo.Experimental results show that agile-vic13b-ppo improves the relative total performance score by 9.2% over GPT-4 and by 90.8% over GPT-3.5.Ablation studies confirm that all modules in Figure 1 are indispensable.Specifically, removing tools or memory usage negatively impacts the agent's performance, leading to a 25.9% or 17.4% increase in seeking advice, respectively, or a 9.3% or 4.0% relative decrease in the total score, respectively.Disabling the seeking advice function results in a 10.7% decrease in accuracy.Finally, agile-vic13b-ppo achieves a 2.3% relative increase in total score compared to agile-vic13b-sft, demonstrating the necessity of PPO training.On MedMCQA, we train an agile-mek7b-ppo agent, initialized from Meerkat-7b [17], following the same two-stage procedure.</p>
<p>Our agent improves the base LLM's accuracy from 53.4% to 85.2% by seeking advice on 31.6% instances.This accuracy surpasses the SOTA accuracy of 79.1% by GPT4-MedPrompt [25].When all agents are able to seek advice, our agent also outperforms the GPT-4 agent in terms of the total score.For HotPotQA, we use the same two-stage method to train agile-vic13b-ppo from Vicuna-13b.Our agent achieves 67.5% accuracy, surpassing the strongest baseline of 48.2%, by seeking advice on 15.6% of instances.When advice-seeking is enabled for all agents, our agent outperforms GPT-4 by 10.8% in total score.</p>
<p>The main contributions of this paper are summarized as follows:</p>
<p>• We propose a novel reinforcement learning framework of LLM agents.It facilitates end-toend learning of agents.Notably, this framework enables the agent to proactively seek advice from human experts, providing two advantages: 1) It ensures high-level accuracy when dealing with complex and challenging questions, and 2) it fosters learning from humans, thereby enhancing its abilities to adapt to new tasks.• We develop a benchmark, ProductQA, to comprehensively evaluate the agent's capabilities in complex question answering.• We perform experiments on multiple tasks to verify our framework and show that AGILE agents based on 13B and 7B LLMs trained with PPO can surpass GPT-4 agents.</p>
<p>2 Methods</p>
<p>RL formulation of agent</p>
<p>Our agent framework comprises four elements: LLM, memory, tools, and executor, see Figure 1(a).</p>
<p>The LLM possesses a context, defined as the sequence of tokens it utilizes to generate the next token.</p>
<p>In RL terminology, the agent conducts a token-level Markov decision process (MDP).The action space A corresponds to the LLM's vocabulary, with each token representing an action.Hence, the LLM serves as the policy model.The agent's state consists of the (context, memory) pair.Upon predicting a new action a t (i.e., a new token), the LLM transfers control to the executor.The executor applies predefined logic to transition from the current state s t to the next state s t+1 , implementing the state transition function S × A → S in RL, and then returns control to the LLM to predict the next action.Concurrently, the environment issues a reward r(s t , a t ).</p>
<p>Let us examine the state transition more closely.For each action, the executor's first operation is to append the token to the context, preparing the LLM for generating the next token.Then, the executor checks a registered list of functions.Each function is designed to execute a set of operations, including memory I/O, tool usage, and interaction with the environment.If the action (i.e., the token) matches a function name, the executor will execute the associated function implementation, further mutating the agent state.For instance, if the token is [GetQuestion], the executor will prompt the user for a new question and append it to the context; if the token is [UpdateMemory], the executor will write a specific segment of the context into the memory; if the token is [ClearContext], the executor will reset the context to [BOS].In summary, the LLM interacts with the memory and tools by predicting function names, relying on the executor to execute these functions.See Table 1 for a full list of functions defined for a QA agent and see Figure 1(b) for a running example.</p>
<p>Policy learning</p>
<p>We frame the policy learning problem as a task of training a language model.Consider an agent trajectory τ = (s 1 , a 1 , ..., s n , a n ), we derive a training sequence denoted as (e 1 , ..., e n ), where e i represents the tokens that the executor appends to the context at step i.If a i is a function name token, then e i is the concatenation of a i and extra tokens appended by the function execution; otherwise, e i = a i .In this sequence, {a 1 , ..., a n } (the first token of each e i ) are referred to as action tokens.</p>
<p>The LLM context at step i, denoted by c i , is a subsequence of the prefix (e 1 , ..., e i−1 ); c i may be shorter than (e 1 , ..., e i−1 ) because the executor can delete context tokens.</p>
<p>In Imitation Learning (IL), we generate trajectories by observing human experts or more proficient agents, then we derive the training sequences to fine-tune the LLM.It is important to point out that (1) the loss is calculated on the action tokens only, and (2) c i should serve as the attention mask for tokens in e i , as it reflects the true context perceived by the LLM at the time of action prediction.In</p>
<p>Function name Function implementation</p>
<p>[GetQuestion] Prompt the user for a question and append it to the context.</p>
<p>[RetrieveMemory]</p>
<p>Retrieve relevant entries from the memory and append them to the context.</p>
<p>[SeekAdvice]</p>
<p>Ask human experts for advice and append it to the context.
[Reflection] ∅ [UpdateMemory]
Write a specific segment of the context into the memory.</p>
<p>[SearchProduct]</p>
<p>Extract a search query from the context, then invoke the search tool and append results to the context.
[PredictAnswer] ∅ [SubmitAnswer]
Extract a predicted answer from the context and submit it to the user.</p>
<p>[ClearContext]</p>
<p>Reset the context to a single token [BOS].</p>
<p>reinforcement learning (RL), we treat the LLM as the policy model, from which training sequences can be sampled and individual action tokens are assigned rewards.Consequently, the LLM can be optimized using policy gradient methods, such as PPO [37].Analogous to the IL setup, we apply policy gradient updates exclusively to the action tokens and employ c i as the attention mask.</p>
<p>In some situations, an agent may produce very long trajectories, potentially yielding training sequences that span millions of tokens and are impractical for training.We can leverage the structure of the trajectory to partition it into smaller segments.For instance, if the agent resets its LLM context at the beginning of every QA session, then we can partition by the session boundary.Nevertheless, these sessions are not entirely independent; actions taken in earlier sessions can influence memory, creating lasting effects on subsequent sessions.To tackle this challenge of long-range dependencies, we propose a training algorithm detailed in Appendix A.</p>
<p>Interaction with human experts</p>
<p>Our agent framework enables the agent to proactively seek advice from human experts.For example, the agent can invoke a [SeekAdvice] function to request expert advice.This approach helps in two ways.Firstly, the agent can request the correct answer when its confidence is low, ensuring sufficient accuracy for the application.Secondly, the agent can use [Reflection] to distill general knowledge from the expert advice before storing it in memory.This accumulation of knowledge allows the agent to adapt to new tasks that it has not encountered during training.</p>
<p>Seeking advice involves complex decision-making.The agent must estimate its own confidence in the current session, predict the potential value of the advice for future sessions, and consider the cost of human resources.The optimal trade-off is difficult to annotate manually but aligns well with our RL framework.Specifically, the present risk, future value, and cost of action can all be represented as RL rewards, allowing this skill to be trained as part of the policy model on an end-to-end basis.</p>
<p>The ProductQA dataset</p>
<p>We believe that product question answering in a real online shopping environment offers a comprehensive challenge for evaluating LLM agents.First, it demands expert knowledge about millions of products, including their technical specifications, usage in particular scenarios, and compatibility with other products.Second, answering some questions requires the use of tools, such as a product search tool.Third, the continuous emergence of new products necessitates the adaptability of the agent.This has motivated the creation of the ProductQA dataset.Unlike existing online shopping QA datasets [38,8], which primarily focus on questions about product metadata or page information, ProductQA features more complex queries involving reasoning, expert knowledge, and tool usage (e.g., SQL), providing a comprehensive assessment of an agent's capabilities.</p>
<p>The ProductQA dataset consists of 26 QA tasks, each representing a distinct group of products within a specific category.Each group encompasses 17-20 products.We collected 20 groups for training and 6 for testing, allowing for assessing the agent's adaptability to new tasks.We collected an average of 3,393 question-answer pairs for each product group.The questions within the same  12.</p>
<p>The dataset is annotated by 20 professional annotators, each with at least a college degree, employed by a commercial data annotation company.We pay the company at market rates for professional annotation.See annotation guidelines in Appendix F.2.In addition, we will release the code for the data pre-processing before human annotation.</p>
<p>Product collection</p>
<p>We gather products from the Amazon Review Data [23], which includes product metadata as well as reviews.We initially filter the Amazon Review Data to retain only popular products with at least 100 reviews, then cluster them by category tags.From these clusters, we select 26 based on the size of the cluster, each defined as a product group.Subsequently, we sample products from each product group.See Appendix F.1 for more details about product group and product selection.</p>
<p>After the products are collected, annotators compile an information table for each product group.An example of such a table is presented in Table 2. To enhance the efficiency of the annotation process, we employ GPT-4 to extract as many product features as possible from the reviews.These features, together with the product metadata, are provided to the annotators for table creation.</p>
<p>QA collection</p>
<p>We identify three predominant types of questions in online shopping contexts: 1) Fact-QA: questions concerning specific product details; 2) Search-QA: searches for product recommendations tailored to user preferences; 3) Reasoning-QA: questions whose answers require domain-specific reasoning, such as the implications of a product feature.Accordingly, we annotate question-answer pairs for these types.Each question is annotated with both a detailed paragraph-long answer and a concise short answer.The long answer should resemble a response from human customer service, while the short answer consists of a few words.We train the model to predict both answer types.The accuracy of the long answers is evaluated using GPT-4 (see Appendix J for the prompt); the short answers are assessed by exact match and are used for defining rewards for RL training.</p>
<p>Fact-QA Fact-QAs are constructed from product reviews.For each product, we provide GPT-4 with a batch of 30 reviews, prompting it to generate 20 questions and their corresponding answers before moving on to the next batch.We encourage GPT-4 to create diverse questions.The results are then given to annotators to refine and finalize the question-answer pairs.</p>
<p>Search-QA Starting with an information table for a given product group, we generate random SQL expressions using a set of predefined rules.These expressions are then translated into natural language questions by GPT-4.The answers are obtained by executing the SQL queries.Subsequently, human annotators thoroughly revise the QA pairs.</p>
<p>Reasoning-QA As the first step, we collect professional knowledge for each product group.To enhance efficiency, we utilize GPT-4 to generate candidate knowledge entries based on the technical specifications from the information table.These entries are then curated and refined by human annotators.Here is an example of a knowledge entry: Motherboards with the ATX form factor are ideally suited for high-performance computing tasks and gaming, due to their ample expansion slots for graphics cards and other peripherals that boost computing capabilities.Finally, annotators develop question-answer pairs from these knowledge entries.</p>
<p>4 Experiments</p>
<p>Experimental setting</p>
<p>Dataset We evaluate our agent on three complex QA tasks: ProductQA, MedMCQA and HotPotQA.</p>
<p>MedMCQA [27] is a dataset for multiple-choice QA.It consists of questions from medical school entrance examinations.HotPotQA [49] features natural, multi-hop questions, which challenge an agent's ability to perform reasoning and utilize search tools.For both MedMCQA and HotPotQA, we report results on their respective full dev sets.</p>
<p>Agent definition Our agent can invoke functions defined in Table 1.In a typical workflow, the agent prompts the user for a new question at the session start.It can then retrieve memory to get relevant information.The memory can be initialized as empty (ProdcutQA) or with domain knowledge (QA pairs from MedMCQA training dataset).The agent has the option to use external tools, such as product search in ProductQA and article search in HotPotQA), to gather more information.At last, the agent decides whether to predict an answer directly or seek human advice.If the agent seeks advice, it obtains a human answer (ground-truth answer in our setting).The agent can then optionally use a reflection round to extract general knowledge from the human answer, writing both the human answer and the reflected knowledge to its memory.Finally, the agent submits an answer to the user.In our setting, submitting a correct answer incurs a +1 reward, while submitting a wrong answer incurs a 0 reward.Seeking human advice has a fixed −c reward, where c represents seeking advice cost.Assuming the human advice always contains a correct answer, then the possible total rewards are {0, 1, 1 − c}.</p>
<p>Training The training consists of two stages.First, we construct trajectories from the training data and employ imitation learning to train the agent.Then we apply Algorithm 1 for further optimization by reinforcement learning.See Appendix B for implementation details.For ProductQA and HotPotQA, the agent's LLM is initialized from Vicuna-13b-1.5.For MedMCQA, we use Meerkat-7b [17], a medical LLM trained with high-quality CoT reasoning paths from 18 medical textbooks and diverse instruction-following datasets.We fine-tune the model for 2 epochs with a learning rate of 1e-5 and a batch size of 64.We implement PPO for 1 epoch with a learning rate of 1e-6 and a batch size of 64.The training runs on NVIDIA-H800.Training times and the number of GPUs for each experiment are reported in Table 13.The LLM is fully trained without using LoRA.</p>
<p>Evaluation and baselines We report three metrics for the agent: (a) Advice rate: the rate of seeking human advice; (b) Accuracy: the rate of predicting the correct answer; (c) Total score: the average reward across all sessions, taking the advice rate and the accuracy both into account.</p>
<p>We compare our agent against two types of baselines: 1) Prompting GPT-3.5 (gpt-3.5-turbo-0301)and GPT-4 (gpt-4-0613) [26] to directly answer the question, without working in an agent manner, noted as gpt3.5-promptand gpt4-prompt.2) Prompting GPT-3.5 and GPT-4 within the AGILE framework, noted as agile-gpt3.5-promptand agile-gpt4-prompt.We carefully designed prompts for all baselines and they are shown in Appendix J.</p>
<p>Results on ProductQA</p>
<p>As Table 4 shows, our AGILE agent outperforms all baselines on ProductQA.Notably, the average total score of agile-vic13b-ppo across six test groups shows a relative improvement of 9.2% in short answers and 5.0% in long answers to agile-gpt4-prompt where the seeking advice cost is added into the prompt.Concretely, agile-vic13b-ppo uses a comparable number of seeking advice to achieve 7.4% higher accuracy in short answers than agile-gpt4-prompt, and as Figure 2 shows, this accuracy improvement is consistent across the whole trajectory.Our agile-vic7b-ppo agent also outperforms agile-gpt4-prompt in average total scores.Note that the GPT-4 agent knows the seeking advice cost from its prompt (see Figure 7).</p>
<p>We investigate the impact of varying the seeking advice cost.As shown in Figure 3, when the cost decreases, both the advice rate and the accuracy increase, indicating greater utilization of human assistance.Specifically, with a high cost of 0.5, the advice rate is close to 0, and at a low cost of 0.1, the accuracy is close to 1.This result demonstrates that by adjusting the cost and through RL training, we can effectively manage the trade-off between accuracy and human cost.For instance, the agent can achieve 94.1% accuracy on the Motherboards task with a seeking advice cost of c = 0.1 (refer to Table 16).This capability is especially important in realistic scenarios that demand high accuracy levels.In most experiments, we set the cost at a medium level with c = 0.3.To validate the accuracy of GPT-4 evaluator in assessing the long answer results, we randomly select 100 triplets (questions, reference long answer, model-predicted long answer) and manually labeled the correctness.The results show a 94% agreement rate between the GPT-4 evaluator and the author.Trend of advice rate Figure 4 demonstrates a consistent decrease in the advice rate of agile-vic13b-ppo as more sessions are added to the trajectory.This decline can be attributed to the agent progressively accumulating knowledge and becoming more independent.Additionally, the figure illustrates that disabling RL training or reflection leads to a significant increase in the advice rate, underscoring the importance of RL training and reflection in reducing human costs.Our agile-mek7b-ppo agent, based on the smaller Meerkat-7b [17] model, reaches an accuracy of 85.2% with an advice rate of 31.6%.As Table 6 shows, this represents a 31.8%accuracy increase over the base model Meerkat-7b-prompt and a 6.1% increase over the state-of-the-art gpt4-Medprompt [25].Table 6 also shows that the ability to seek advice alone contributes a 23.2% accuracy gain, meaning that each instance of seeking advice corrects an average of 0.73 prediction errors.This indicates that PPO training effectively helps the agent identify its mistakes.For a fair comparison, we also evaluate agile-gpt3.5-prompt and agile-gpt4-prompt, which incorporate GPT-3.5</p>
<p>Ablation study We present ablation studies in</p>
<p>Results on MedMCQA</p>
<p>and GPT-4 within our AGILE framework.These agents also leverage advice-seeking to enhance accuracy, but without RL training, their total scores are lower than agile-mek7b-ppo.Finally, through ablation studies, we confirmed the essential roles of memory, reflection, seeking advice, and RL training in achieving high performance.Removing these components leads to a significant drop in total scores, detailed in Table 6.</p>
<p>Results on HotPotQA</p>
<p>We compare our method against several baselines.Specifically, we found the original ReAct baseline implementation in [51] to be suboptimal.By reproducing their results with GPT-4 (ReAct-gpt4-prompt), we observed improved performance.As shown in Table 7, our agile agent outperforms all baselines in accuracy, achieving a 40.0%relative improvement over ReAct-gpt4prompt, which is the strongest baseline.Additionally, compared to agile-gpt4-prompt, the trained agile-vic13b-ppo demonstrates both higher accuracy and a lower advice rate, leading to a 10.8% relative increase in total score.Ablation studies confirm that removing either seeking-advice or PPO training results in a significant decrease in the total score.PaLM-540b Reflexion [40] GPT-3/3.5/4ChatDev [30] ChatGPT-turbo-16k RAP [14] LLaMA-33b AutoAct [32] LLaMA2-70b TPTU [35] ChatGPT/InternLM AGILE (Ours) Vicuna-13b/Meerkat-7b</p>
<p>LLM agents Large Language Models (LLMs) have demonstrated substantial capabilities in following instructions, reasoning, and planning.Numerous research works, as shown in Table 8, utilizing prompt engineering, have constructed remarkable LLM agents capable of autonomously resolving complex tasks across various environments [28,44,2,30,4].Furthermore, extensive works identify key components in the design of LLM agents, including planning [22,39,10,32,51,35], tooluse [19,29,48,36], and reflection [40,21].In this work, we enable the agent to utilize memory, tools and proactively learn from the environment.We then formulate the entire process within an RL framework so that all agent skills can be jointly optimized end-to-end.</p>
<p>Human-agent interaction Although LLMs face practical challenges, such as hallucination [53] and a lack of long-tail knowledge [16], consulting human experts can help mitigate these issues.</p>
<p>Several studies [52,46] have incorporated human experts into agent workflows relying on passive feedback or predefined rules.However, these approaches do not involve proactively seeking advice, which requires more complex decision-making.While [5,31] train models to ask questions using behavior cloning, they ignore the fact that the decision to seek advice must be based on the LLM's own knowledge and capabilities [55,18,13].[34] use a calibrated version of an LLM's token probabilities as a confidence measure, yet token probabilities tend to be overconfident [47], and existing calibration methods don't generalize well to our agent setting when the LLM makes multiple decisions in sequence.Ultimately, the challenge of seeking advice is tied to the LLM's self-evaluation, which is difficult to ground truth or optimize through SFT.In our RL framework, the value and cost of seeking advice can be directly represented as RL rewards, enabling the proactive skill of seeking advice to be optimized as part of the policy model on end-to-end RL training.</p>
<p>LLM agent benchmarks Several benchmarks have been designed to assess the capabilities of agents.For instance, the Webshop [50] and Mind2Web [7] datasets evaluate agents' tool usage and planning abilities within a web environment.HotPotQA [49] and TriviaQA [12] focus on agents' reasoning and tool usage for question answering.ALFWorld [41] examines planning and navigation skills, while ScienceWorld [43] provides an interactive text-based environment to evaluate agents' scientific aptitude.As illustrated in Table 9, despite these existing benchmarks, none comprehensively addresses all the core challenges of real-world agent applications, such as handling long-tail knowledge, human-agent interaction, long-term memory usage, tool usage, self-evaluation, and reflection.This motivated us to develop ProductQA.AGILE is a general agent framework and we can certainly consider multiple extensions of it.An agent can be equipped with more tools, such as multimodal perception, manipulations in physical environments, logical reasoning, among others.We posit that AGILE's activities can be categorized into two distinct types: utilizing its LLM alone, and integrating the LLM with other tools.These two approaches conceptually align with the human cognitive processes known as System 1 and System 2 [15,1].Furthermore, AGILE's memory serves as a repository for the accumulation of experiences and knowledge, which is crucial for self-improvement.Consequently, AGILE offers an architecture for an very powerful agent that has the potential to attain human-level intelligence.</p>
<p>AGILE also includes interactions between the agent and external human experts.The framework can be extended to allow interactions with humans or machine agents in various roles such as students or teachers, and in different formats such as debates or coordination.Furthermore, AGILE can be employed in multi-agent systems.</p>
<p>A Session-level optimization algorithm</p>
<p>Assume that the entire trajectory τ can be partitioned into sub-trajectories (τ 1 , τ 2 , • • • , τ n ), each referred to as a session.For session i, let S i denote its initial state, where c i is the LLM context before the session starts, and m i is the memory before the session starts.In this section, we will explain how to transform a trajectory-level RL optimization algorithm into a session-level RL optimization algorithm.</p>
<p>Let r(τ ) represent the total reward of trajectory τ , and let π θ be a policy parameterized by θ.The optimization objective is to maximize the following expectation:
R(θ) = E τ ∼π θ [r(τ )].(1)
For an arbitrary session index i, the trajectory τ ∼ π θ can be sampled in three stages: τ 1:i−1 , τ i , and τ i+1:n .These stages represent the sub-trajectory from session 1 to i − 1, the sub-trajectory for session i, and the sub-trajectory from session i + 1 to n, respectively.Accordingly, we have
R(θ) = E τ1:i−1∼π θ E τi∼π θ (•|Si) E τi+1:n∼π θ (•|Si+1) [r(τ 1:i−1 ) + r(τ i ) + r(τ i+1:n )] = E τ1:i−1∼π θ r(τ 1:i−1 ) + E τi∼π θ (•|Si) [r(τ i ) + V π θ (S i+1 )] .(2)
Here, S i and S i+1 denote the initial states of sessions i and i + 1 respectively.The term r(τ 1:i−1 ) represents the total reward accumulated from session 1 to i − 1, while r(τ i ) is the reward obtained in session i.Additionally, V π θ (S i+1 ) represents the value function at state S i+1 with respect to policy π θ , indicating the expected total reward the agent expects to receive in the future.Averaging over all session indices, Eq. ( 2) gives:
R(θ) = 1 n n i=1 E τ1:i−1∼π θ r(τ 1:i−1 ) + E τi∼π θ (•|Si) [r(τ i ) + V π θ (S i+1 )] .(3)
In Eq. ( 3), the parameter θ appears in three places -two expectations and a value function -making optimization challenging.To simplify the problem, we assume a base policy θ k and define a proximal objective R(θ|θ k ), where θ only appears in the session-level expectation:
R(θ|θ k ) = 1 n n i=1 E τ1:i−1∼π θ r(τ 1:i−1 ) + E τi∼π θ (•|Si) r(τ i ) + V π θ k (S i+1 ) .(4)
R(θ|θ k ) is an approximation to R(θ) in the neighborhood of θ k .If we employ an iterative optimization procedure:</p>
<ol>
<li>Initialize θ 0 from a reference policy (obtained through SFT).</li>
</ol>
<p>For
k = 0, 1, 2, • • • , compute θ k+1 ← arg max θ R(θ|θ k ).
Then θ will converge to an (at least locally) optimal policy.Now we are ready to illustrate why the optimization of R(θ|θ k ) can be solved at the session level.Notice that
R(θ|θ k ) = 1 n n i=1 E τ1:i−1∼π θ k E τi∼π θ (•|Si) [r(τ i ) + V π θ k (S i+1 ) − V π θ k (S i )] + E τ1:i−1∼π θ k [r(τ 1:i−1 ) + V π θ k (S i )] = 1 n n i=1 E τ1:i−1∼π θ k E τi∼π θ (•|Si) [r(τ i ) + V π θ k (S i+1 ) − V π θ k (S i )] + E τi∼π θ k [r(τ )]
On the right-hand side, the first term involves two sampling steps.The first step samples τ 1:i−1 ∼ π θ k .The inner terms inside the expectation only depends on S i , thus we can replace it by S i ∼ π θ k .The second term on the right-hand side is a constant independent of θ.As a result, if we define a proxy reward:
rk (τ i ) := r(τ i ) + (V π θ k (S i+1 ) − V π θ k (S i )).(5)
Algorithm 1 Session-level optimization 1: Initialize θ 0 from a reference policy (obtained through SFT).</p>
<p>2:
for k ← 0, 1, 2, • • • do 3:
Sample a set of trajectories from π θ k , denote the set by T .</p>
<p>4:</p>
<p>Define or fit a state advantage function from T .</p>
<p>5:</p>
<p>for each τ ∈ T do 6:</p>
<p>Partition it into sessions (τ 1 , τ 2 , • • • , τ n ).</p>
<p>7:</p>
<p>for each τ i do 8:</p>
<p>Evaluate rk (τ i ) by Eq. ( 5) with the above state advantage function.Treat all sessions as independent, then employ an optimization algorithm (such as PPO) to obtain a new policy θ k+1 by maximizing Eq. ( 6).12: end for Then, we have
R(θ|θ k ) = 1 n n i=1 E Si∼π θ k E τi∼π θ (•|Si) [r k (τ i )] + constant. (6)
By Eq. ( 6), R(θ|θ k ) can be optimized by maximizing the average expected proxy reward for each session.The term
A i := V π θ k (S i+1 ) − V π θ k (S i
) measures the advantage of state S i+1 over state S i with respect to a policy; thus, we call it the state advantage function.This function can be either defined by heuristics or fitted by a neural network.In the latter case, one needs to sample trajectories from π θ k , evaluate their rewards, and then use the (state, reward-to-go) pairs to train an estimator for the value function V π θ k .</p>
<p>Finally, we present the session-level optimization algorithm as Algorithm 1.In this algorithm, the state advantage function is the only component that concerns inter-session correlation.While the algorithm is iterative, we anticipate that in practice, the outer loop will require only a few iterations to converge.</p>
<p>B Implementation details of AGILE B.1 ProductQA</p>
<p>Implementation of [GetQuestion] This function prompts the user for a new question and appends it to the LLM context.Every question is raised for a specific product, thus it has an associated product ID.Based on this ID, the function also appends the product information table's schema and the product metadata to the context.</p>
<p>Implementation of [RetrieveMemory]</p>
<p>This function employs the provided question as a query to retrieve the most relevant historical QA pair and the most relevant knowledge entry from the agent's memory.To safeguard sensitive data from sellers, the agent is restricted to accessing QA records exclusively for the queried product from historical interactions.However, it is permitted to retrieve general knowledge from the whole trajectory since this information is not seller-specific.We utilize an embedding-based retrieval method, specifically employing the all-MiniLM-L6-v2 model [33] as the embedding model.</p>
<p>Implementation of [SearchProdcut]</p>
<p>This function utilizes the LLM to predict a SQL query based on the context, and then invoke a MySQL execution engine.It appends the result to the LLM context.If there is an execution error, then the error is appended to the context too.</p>
<p>Implementation of [SeekAdvice]</p>
<p>This requests for human expert advice and append it to the LLM context.In our implementation, the human expert simply returns the ground truth long answer from the ProductQA dataset.</p>
<p>Implementation of [PredictAnswer]</p>
<p>This function passes control to the LLM to continue generating a long answer and a short answer.In this way, we constructed 55,772 session-level trajectories in total, from 6 training tasks in Pro-ductQA.This data is used for imitation learning.In PPO training, we reuse the initial memory data, while the session-level trajectories are generated by the model itself.</p>
<p>Implementation of [Reflection]</p>
<p>B.2 MedMCQA</p>
<p>For MedMCQA, the memory is initialized with all QA pairs from the training set, simulating that the agent has processed the training set before reaching the test set.We also add a knowledge entry for each QA pair, obtained through GPT-4 reflection (see Figure 12 for the prompt).</p>
<p>Training data generation We sample a subset of training data from MedMCQA to construct session-level trajectories.Each trajectory begins with [GetUserQuestion] and [RetrieveMemory].The [RetrieveMemory] function retrieves the five most relevant QA pairs and pieces of knowledge from the initial memory, using the same embedding similarity search method employed in ProductQA.Then, we prompt GPT-4 to predict an answer with chain-of-thought reasoning.If the GPT-4 answer is correct, we append a [PredictAnswer] call, the GPT-4 chainof-thought, and the ground-truth answer to the trajectory.If the GPT-4 answer is wrong, which suggests that the question is hard, we append a [SeekAdvice] call with the ground-truth answer, followed by a [Reflection] call with the reflection result generated by GPT-4.Then the reflection result is appended to the memory via [UpdateMemory].Finally, the trajectory is concluded by [SubmitAnswer].In this way, we obtain 23,015 session-level trajectories in total.</p>
<p>B.3 HotPotQA</p>
<p>In the HotPotQA task, the agent has the option to select either [Search], [SeekAdvice] or [PredictAnswer] in each round.Following ReAct [51], the agent first generates reasoning first and then selects an action.</p>
<p>Implementation of [Search]</p>
<p>B.4 Defining proxy reward for RL</p>
<p>In the question-answering tasks, sessions are not independent.Actions taken in earlier sessions can influence memory, creating lasting effects on subsequent sessions.As illustrated in Equation ( 5), the term
A i := V π θ k (S i+1 ) − V π θ k (S i
) measures the advantage of state S i+1 over state S i (note that S i here represents the initial state of session i).In our experiment setting, if the agent predicts [SeekAdvice], it will receive expert advice, extract some knowledge by reflection, and write that knowledge to the memory.Intuitively, A i should increase if the new knowledge is useful in subsequent sessions, and it should decrease if there is already a lot of similar knowledge in the memory at the start of session i.Hence, we use the following heuristic definition,
A i = β I(N i+1:n (q i ) &gt; 0) M 1:i−1 (q i ) + 1 ,(7)
where q i represents the user question in session i; N i+1:n (q i ) represents the number of user questions in session i + 1 to session n that are similar enough to q i ; M 0:i−1 (q i ) represents the number of user questions in session 1 to session i − 1 that are both similar enough to q i and added to the memory; I(•) is the indicator function.β is a hyperparameter, we set β = 0.1 by default.</p>
<p>C Supplementary experimental results on RL training</p>
<p>In this section, we present detailed experimental results for RL training on ProductQA.</p>
<p>C.1 Training curve</p>
<p>In Figure 5, we provide training curves, indicating that RL training converged after 500 steps.X-prompt represents directly prompting the model X;</p>
<p>D Tables</p>
<p>agile-X-Y represents incorporating the model X within the AGILE framework, while Y represents prompting or PPO training.The Short and Long stand for the results evaluated on short answers and long answers, respectively.The seeking advice cost is c = 0.3.</p>
<p>The best total scores are highlighted in bold.Certainly, the backpack straps on this camera case not only simplify the process of carrying but also evenly distribute the weight, making it an ideal option for those who are frequently on the move and prefer a more comfortable and balanced carrying solution.The product groups and the corresponding products are collected by the following steps.</p>
<ol>
<li>
<p>Filter the Amazon Review Data to retain only products with at least 100 reviews, then cluster them by category tags.</p>
</li>
<li>
<p>Sort the clusters by size, from largest to smallest.Manually review each cluster in order: we keep product clusters that involve diverse technical details and long-tail domain knowledge, such as electronics, from which we can potentially construct a diverse set of user questions.</p>
</li>
</ol>
<p>The manual review ends when we have collected 26 clusters.Each cluster is referred to as a product group.</p>
<ol>
<li>For each product group, we remove the top 10% of products with the highest number of reviews.We exclude these most popular products from the datasets to prevent data leakage, as information about them is likely included in the pre-training set of LLMs.From the remaining items, we randomly select up to 20 products to form the final product set.</li>
</ol>
<p>F.2 Annotation guidelines</p>
<p>There are two annotation tasks, product table creation and QA collection.We provide the annotation guidelines in this Section.</p>
<p>Task 1: Product table creation For each product group, we provide a series of features and their corresponding values for each product in the group.This information is obtained by prompting GPT-4 to extract data from the reviews of each product.The task of annotators is to construct a product table containing only the metadata.Please follow these steps:</p>
<ol>
<li>
<p>Select up to 15 common features relevant to the product group.These features must include product ID, product title, brand, and price.Choose additional features based on their commonality and necessity within the product group.</p>
</li>
<li>
<p>For each product in the product group, verify the feature values for each selected feature.</p>
</li>
</ol>
<p>Finally, the product tables are reviewed and refined by the authors.</p>
<p>Task 2: QA collection Annotators are required to fill out a table as shown in Table 19.Each row contains a triplet consisting of a question, a long answer, and a short answer, all generated by GPT-4.Annotators should fill the following columns: Is question reasonable, Is long answer correct, Refined long answer, Is short answer correct and Refined short answer.Please follow these steps:</p>
<ol>
<li>Evaluate the question: Verify if the question resembles a typical query found in real-world product conversations in online shopping.Select 'yes' or 'no' in the Is question reasonable column.Any question containing harmful information is considered unreasonable and should be labeled as 'no'.If 'no' is selected, the row will be dropped, and you do not need to proceed with the subsequent steps for that row.</li>
</ol>
<p>2.</p>
<p>Assess the long answer: Check if the long answer correctly responds to the question.Select 'yes', 'no' or 'I do not know' in the Is long answer correct column.Consider the following special cases:</p>
<p>• If the long answer is ambiguous (e.g., 'The product is designed to be waterproof, while some users do not think so.'), mark it as incorrect.• For numerical questions, an answer is considered correct if it fits the real-world scenario and the conclusion is clear.Specific values or ranges (e.g., 5cm, 5cm-10cm, several months) are acceptable if they correspond to the real-world scenario.• If the long answer contains a specific piece of knowledge, verify its accuracy.• If the long answer is incorrect or does not address the question, and you do not know the correct answer (even after checking the product information table, looking up the product URL, and searching online), select 'I do not know'.</p>
<p>powerful agents, especially in planning and reasoning.Expanding AGILE to larger LLMs is our future work.</p>
<p>(2) Our ProductQA dataset includes QA pairs from 20 product groups in the training set.Due to resource constraints, we randomly selected 6 of the 20 groups for training our AGILE agent.Despite using a subset of training data, our agile-vic13b-ppo shows significant improvements over GPT-4 agent in accuracy and total score.Future work could enhance the agent's capabilities by training on a larger and more diverse dataset, potentially further improving performance and effectiveness.</p>
<p>I Ethical considerations</p>
<p>ProductQA is constructed based on the Amazon Review Dataset.We only use the review data for each product without any user personal information, such as the identity of the reviewers.</p>
<p>All data in ProductQA are annotated by human annotators, as described in Appendix F.2. Any data containing harmful information is removed during the annotation process.</p>
<p>The annotation team has 20 annotators, each holding at least a college degree, and employed by a commercial data annotation company.We have contracted this company and paid them for the annotation work at a market price.</p>
<p>J Prompt templates</p>
<p>Prompt templates for ProductQA Figure 6 shows the prompt template for gpt3.5-prompt,gpt4-prompt.</p>
<p>Figure 7 provides the prompt template for agile-vic13b-prompt, agile-gpt3.5-prompt, and agile-gpt4-prompt.We leave the "{knowledge} and "{history}" empty when evaluate gpt3.5-prompt and gpt4-prompt.</p>
<p>The prompt template for reflection is shown in Figure 8.</p>
<p>The prompt template for long answer evaluation is shown in Figure 9.</p>
<p>Prompt templates for MedMCQA Figure 10 provides the prompt template for Meerkat-7b-prompt. Figure 11 illustrates the prompt template for agile-gpt3.5-prompt,agile-gpt4-prompt.We leave the "{related_question} and "{related_knowledge}" empty when evaluate gpt3.5-prompt and gpt4-prompt.The prompt template for reflection is shown in Figure 12.</p>
<p>Prompt templates for HotPotQA Figure 13 provides the prompt template for ReAct-gpt4-prompt. Figure 14 illustrates the prompt template for agile-gpt4-prompt.The prompt template for answer evaluation is shown in Figure 15.</p>
<p>[Prompt for long_answer evaluation] Based on the provided question and reference answer, please determine if the response is correct or incorrect.Begin by articulating your rationale, and conclude with a single word judgment: 'Yes' for correct or 'No' for incorrect.question: {question} reference answer: {reference} response: {response}   [prompt]</p>
<p>The following is a multiple-choice question about medical knowledge and some related questions and knowledge references.[prompt]</p>
<p>Based on the provided question and reference answer, please determine if the response is correct or incorrect.Begin by articulating your rationale, and conclude with a single word judgment: 'Yes' for correct or 'No' for incorrect.question: {question} reference answer: {reference} response: {response} Figure 15: The prompt for answer evaluation on HotPotQA.</p>
<p>Figure 2 :
2
Figure 2: Accuracy and advice rate over the following 200 sessions (c = 0.3).</p>
<p>Figure 3 :
3
Figure 3: Advice rate, accuracy along with seeking advice cost c on ProductQA.</p>
<p>Figure 4 :
4
Figure 4: Advice rate over the following 200 sessions on ProductQA (c = 0.3).</p>
<p>Figure 5 :
5
Figure 5: Reward and value function loss curves during the PPO training process on ProductQA.</p>
<p>0.154 0.359 0.359 0.200 0.200 0.201 0.201 0.251 0.251 Accuracy ↑ 0.181 0.306 0.503 0.594 0.154 0.267 0.497 0.646 0.766 0.790 0.837 0.834 0.876 00.214 0.303 0.303 0.225 0.225 0.162 0.162 0.235 0.235 Accuracy ↑ 0.253 0.431 0.511 0.637 0.215 0.337 0.525 0.686 0.815 0.855 0.835 0.831 0.877 00.197 0.293 0.293 0.198 0.198 0.167 0.167 0.216 0.216 Accuracy ↑ 0.187 0.306 0.463 0.587 0.197 0.313 0.502 0.657 0.770 0.806 0.802 0.760 0.828 00.174 0.323 0.323 0.208 0.208 0.179 0.179 0.233 0.233 Accuracy ↑ 0.202 0.322 0.464 0.571 0.174 0.294 0.508 0.644 0.780 0.809 0.818 0.800 0.854 00.242 0.411 0.547 0.718 0.747 0.764 0.746 0.784 0.784</p>
<p>Figure 9 :
9
Figure 9: The prompt for long answer evaluation on ProductQA.</p>
<p>the multiple-choice question about medical knowledge.[question]{question}(A){Option A} (B){Option B} (C){Option C} (D){Option D}[Answer]</p>
<p>Figure 10 :
10
Figure 10: The prompt for Meerkat-7b-prompt on MedMCQA.</p>
<p>[</p>
<p>Related Question] {related_question} [Releted Knowledge] {related_knowledge} [Question] {question} Solve this in a step-by-step fashion, starting by summarizing the available information.-If the question can be answered directly, output <code>[PredictAnswer]</code> and conclude your response with the phrase <code>the answer is ([option_id])-If the information available is insufficient, necessitating seek advice from a human, output</code><a href="If you choose [PredictAnswer], you will receive 1 point for doing right and no point for doing wrong.If you choose [SeekAdvice], you will directly receive 0.6 points.">SeekAdvice</a></p>
<p>Figure 11 :
11
Figure 11: The prompt for agile-gpt3.5-promptand agile-gpt4-prompt on MedMCQA.</p>
<p>{medical} [Question] and[Answer].Your task is to extract any [Information] that is typically applicable to issues related to the medical field.Summarize and generalize the extracted knowledge into one sentence.If the question-answer pair offers no relevant medical insights, simply respond with '[no information]'.[Question]:{question} [Answer]: {answer} [Information]:</p>
<p>Figure 12 :
12
Figure 12: The prompt for reflection on MedMCQA.</p>
<p>intelligent agent with the ability to search knowledge.Please answer the following questions.You can analyze the solution steps based on the problem and known information.For missing information, you can use search tools by output <code>[Search] ([entity])</code>.Please note that the [entity] should be a noun word/phrase rather than a sentence.If there is enough information, you can answer directly by output <code>[PredictAnser] ([answer])</code>.Please note that the answer must be the span in the observation sentences.[Question]:What U.S Highway gives access to Zilpo Road, and is also known as Midland Trail?Thought1: I need to search Zilpo Road, and find the U.S Highway gives access to it.Action: [Search] (Zilpo Road) Observation1: Search Result -Zilpo Road The nine mile byway starts south of Morehead, Kentucky and can be accessed by U.S. Highway 60.Thought2: Zilpo Road is located in the eastern Kentucky, United States.So I need to search Kentucky and find the Highway gives access to Zilpo Road.Action: [Search] (Kentucky) Observation2: Search Result -Morehead, Kentucky Morehead is a home rule-class city located along US 60 (the historic Midland Trail) and Interstate 64 in Rowan County, Kentucky, in the United States.Thought3: US 60 also named as Midland Trail, so the answer is US 60.Action: [PredictAnswer] (US 60) [Question]: {question} Thought1:</p>
<p>Figure 13 :
13
Figure 13: The prompt for ReAct-gpt4-prompt on HotPotQA.</p>
<p>intelligent agent with the ability to search knowledge.Please answer the following questions.You can analyze the solution steps based on the problem and known information.For missing information, you can use search tools by output <code>[Search] ([entity])</code>.If there is enough information, you can output <code>[PredictAnser] ([answer])</code> to answer the question directly or output <code>[Seekadvice] ()</code> if you are not sure and need to seek advice.Please note that the answer must be the span in the observation sentences.[Question]: What U.S Highway gives access to Zilpo Road, and is also known as Midland Trail?Thought1: I need to search Zilpo Road, and find the U.S Highway gives access to it.Action: [Search] (Zilpo Road) Observation1: Search Result -Zilpo Road (Summary version) The nine mile byway starts south of Morehead, Kentucky and can be accessed by U.S. Highway 60.Thought2: Zilpo Road is located in the eastern Kentucky, United States.So I need to search Kentucky and find the Highway gives access to Zilpo Road.Action: [Search] (Kentucky) Observation2: Search Result -Morehead, Kentucky (Summary version) Morehead is a home rule-class city located along US 60 (the historic Midland Trail) and Interstate 64 in Rowan County, Kentucky, in the United States.Thought3: US 60 also named as Midland Trail, so the answer is US 60.Action: [PredictAnswer] (US 60) [Question]: {question} Thought1:</p>
<p>Figure 14 :
14
Figure 14: The prompt for agile-gpt4-prompt on HotPotQA.</p>
<p>Table 1 :
1
Functions for an exemplary customer service QA agent.Among them, [Reflection] and [PredictAnswer] are trivial functions, as the executor passes control immediately back to the LLM to start generating result tokens.</p>
<p>Table 2 :
2
An example of an information table for the headphones group.
Product IDTitlePriceBrandHeadphone Cable Type TypeAudio Transmission Output Mode Audio...B00WSLZFTK Sennheiser RS 170 $11.03 Sennheiserover-earbluetoothkleerstereo...B003AIL2HE JVC HAEB75B$9.99JVCearbud3.5mm Jackanalogbass boost...B01C22IJV0 Phaiser BHS-530$6.04Phaiserearbudbluetoothbluetoothstereo...B0013OWPV4 JVC HARX700$2.00JVCover-ear3.5mm Jackanalogstereo..............................</p>
<p>Table 3 :
3
Examples of Fact-QA, Search-QA and Reasoning-QA in ProductQA.
TypeQuestionLong AnswerShort AnswerWhat is the size of theThe JVC HA-EB75 headphones contain a 13.5Fact-QAneodymium driver used in themm neodymium driver in each earpiece, which13.5 mmJVC HA-EB75 headphones?contributes to the enhanced sound quality.I'm an audiophile always on themove, so I need my music non-I found a product that matches your criteria.Search-QAstop. Tell me, what's the head-'ABCShopUSA Wireless Earbuds True' with asin:B00LJT2EPKphone with the longest playtimeB00LJT2EPKyou have, either on-ear or in-ear?Reasoning-QAWill these headphones deliver comparable sound quality to editing music? wired alternatives when I amNo, these headphones may not suit your needs for music editing since they are wireless and can intro-crucial for professional audio editing tasks. issues can impact the precise listening experience duce audio compression and slight latency. Suchnogroup are correlated, as knowledge from one answer may aid in addressing other questions. Thedataset statistics are presented in Table</p>
<p>Table 4 :
4
Results on ProductQA.Here, X-prompt represents directly prompting model X; agile-X-Y incorporates model X within the AGILE framework, while Y represents prompting or PPO training.We report results on short and long answers, respectively.The seeking advice cost is c = 0.3.Results are averaged over six test tasks.See Table14for individual product group performance.
MethodAdvice Rate↓Accuracy ↑ Short Long Short Long Total Score ↑gpt3.5-prompt-0.202 0.322--gpt4-prompt-0.464 0.571--agile-vic13b-prompt0.1740.174 0.294 0.122 0.242agile-gpt3.5-prompt0.3230.508 0.644 0.411 0.547agile-gpt4-prompt0.2080.780 0.809 0.718 0.747agile-vic7b-ppo (ours)0.1790.818 0.800 0.764 0.746agile-vic13b-ppo (ours)0.2330.854 0.854 0.784 0.784</p>
<p>Table 5
5
to assess the contributions of individual agent components and the effects of RL training.The table indicates that disabling the option to seek advice (w/o Advice) leads to a 10.7% drop in accuracy and a 5.0% relative reduction in total score.Forcing the agent to seek advice at the initial part of the trajectory (Non-adapt Advice) causes a 4.2% decrease in accuracy, underscoring the value of adaptive decision-making.Removing reflection and memory capabilities (w/o Memory and w/o Reflection) both increase the frequency of advice-seeking, as the agent struggles to accumulate or leverage valuable knowledge, consequently decreasing the</p>
<p>Table 5 :
5
Ablation studies for disabling reflection, memory, seeking advice, tool use, or RL training.Here, non-adapt-advice means that seeking advice is invoked for the first K sessions of the trajectory, where K equals to the number of [SeekAdvice] performed by agile-vic13b-ppo.See Table15for ablation results on individual product groups.Tool-Use) causes a substantial 25.9% increase in the advice-seeking rate because the agent's capabilities are diminished, making it more reliant on external advice.Lastly, RL training improves the relative total score by 2.3%, lowers the advice-seeking rate, and boosts accuracy, demonstrating that RL training effectively optimizes the policy.Additional results on RL training can be found in Appendix C.
MethodAdvice Rate ↓ Accuracy ↑ Total Score ↑w/o Reflection0.2700.8520.771(-1.7%)w/o Memory0.4070.8760.754(-4.0%)w/o Advice0.0000.7470.747(-5.0%)non-adapt-advice0.2330.8120.742(-5.7%)w/o Tool-Use0.4920.8640.717(-9.3%)w/o RL0.2560.8430.766(-2.3%)agile-vic13b-ppo (ours)0.2330.8540.784total score. Furthermore, disabling tool use (w/o
In Appendix E, we present detailed examples of agile-vic13b-ppo illustrating how memory, tools, seeking advice, and reflection enhance the agent workflow.</p>
<p>Table 6 :
6
Results
Meerkat-7b-prompt-0.534-gpt3.5-prompt[24]-0.501-gpt4-prompt[24]-0.695-gpt4-Medprompt[25]-0.791-agile-gpt3.5-prompt0.1940.6970.619agile-gpt4-prompt0.4210.8840.721agile-mek7b-w/o Reflection0.3680.7900.643agile-mek7b-w/o Memory0.5060.7410.539agile-mek7b-w/o Advice0.0000.6200.620agile-mek7b-w/o RL0.3220.8370.708agile-mek7b-ppo (ours)0.3160.8520.726
on the MedMCQA dev dataset.X-prompt represents directly prompting the model X; agile-X-Y represents incorporating the model X within the AGILE framework, while Y represents prompting, ablation studies or standard PPO training.The seeking advice cost is c = 0.4.MethodAdvice Rate ↓ Accuracy ↑ Total Score ↑</p>
<p>Table 7 :
7
Results on the HotPotQA full dev dataset.X-prompt represents directly prompting the model X; agile-X-Y represents incorporating the model X within the AGILE framework, while Y represents prompting, ablation studies or standard PPO training.The seeking advice cost is c = 0.3.
MethodAdvice Rate ↓Accuracy ↑ (Exact Match)Accuracy ↑ (GPT-4 Evaluator)Total Score ↑ (Exact Match)ReAct [51]-0.351--ReAct-gpt4-prompt-0.482--CRITIC [9]-0.443--Expel [54]-0.390--AutoAct [32]-0.384--agile-gpt4-prompt0.1940.6640.8420.567agile-vic13b-w/o Advice0.0000.5530.7510.553agile-vic13b-w/o RL0.1710.6680.8570.617agile-vic13b-ppo (ours)0.1560.6750.8580.6285 Related work</p>
<p>Table 8 :
8
Related work on LLM agents.AGILE stands out as the pioneering work that trains the entire agent using reinforcement learning, incorporating proactive human advice-seeking.
ProactiveLLM AgentLLMSFT RL Memory Tools ReflectionHuman-agentInteractionWebGPT [22] GPT-3ReAct [51]</p>
<p>Table 9 :
9
Benchmarks for evaluating LLM agents.ProductQA features long trajectories, tool use, long-term knowledge accumulation, and cross-task capabilities.In this work, we introduce a novel reinforcement learning framework of LLM agents, called AGILE.First, the whole system of AGILE is trained end-to-end by reinforcement learning.Second, AGILE has the ability of seeking advice from external human experts.In addition, we develop a challenging dataset of complex QA, ProductQA, for comprehensive evaluation of an agent's capabilities.Extensive experiments demonstrate that within our framework, an agent based on a smaller model after RL training can outperform GPT-4.
DatasetsTypeFieldsSizeLong TrajectoryTool UsageLong-term KnowledgeCross TaskWebshop [50]SimulatorWeb12,087Mind2Web [7]SimulatorWeb2,350ALFWorld [41]SimulatorNavigation3,827ScienceWorld [43] SimulatorScience7,207HotPotQA [49]QAWikipedia112,779TriviaQA [12]QAWeb95,956ProductQA (ours)QAE-commerce 88,2296 Conclusion and future work</p>
<p>ProductQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B.2 MedMCQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B.3 HotPotQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B.4 Defining proxy reward for RL . . . . . . . . . . . . . . . . . . . . . . . . . . . .
AppendixA Session-level optimization algorithmB Implementation details of AGILEB.1 H LimitationsI Ethical considerationsJ Prompt templates
C Supplementary experimental results on RL training C.1 Training curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.2 Training Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .C.3 Impact of PPO training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .D Tables E Case study F Development of the ProductQA dataset F.1 Product collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .F.2 Annotation guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .G Broader impact G.1 Positive broader impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .G.2 Negative broader impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p>
<p>This function passes control to the LLM to continue generating a reflection result.Training Data Generation We generate training data on a session-by-session basis, where each session consists of a QA pair.A session begins with an initial memory, consisting of historical QA pairs and knowledge entries accumulated from previous sessions.Recall that the [RetrieveMemory] function retrieves only the most relevant QA pair and knowledge entry per session.Thus, in constructing training memories, it suffices to put the retrieved QA pair and the retrieved knowledge entry into the memory.We select them in the following stochastic way: the retrieved QA pair can be the most relevant QA pair from the training set, or a random QA pair, or omitted entirely; similarly for the retrieved knowledge entry.Based on the initial memory, we generate trajectories by following the agent workflow detailed in Section 4.1.Each trajectory begins with [GetUserQuestion] and [RetrieveMemory].For QAs classified as Search-QA, a [SearchProduct] function is appended, followed by the corresponding SQL query and its execution result.For other QA types, if an associated knowledge entry exists and is successfully retrieved, the trajectory will extend with a [PredictAnswer] call with the ground truth answer as its result.If the knowledge entry is not retrieved or is absent, we use GPT-4 to evaluate whether the question can be answered with the available context.If affirmative, a [PredictAnswer] with the ground truth answer is appended.Otherwise, the trajectory extends with a [SeekAdvice] call with the ground truth answer as the advice, and a [Reflection] call, where the reflection result is the knowledge entry if it exists, or "no information" if not.Then the reflection result is appended to the memory via [UpdateMemory].Finally, the trajectory is concluded by[SubmitAnswer].</p>
<p>Training data generation We use the HotPotQA training set to construct session-level trajectories.Each trajectory begins with the [GetUserQuestion] prompt.We then repeatedly prompt GPT-4 to predict actions between [Search] and [PredictAnswer].If GPT-4 predicts [Search], we prompt it to generate a search query and append the corresponding search results to the trajectory, continuing this cycle.This process continues until GPT-4 predicts[PredictAnswer].If the answer is correct (as evaluated by the GPT-4 evaluator), we replace the predicted answer with the ground-truth answer; otherwise, the data is discarded.Additionally, if GPT-4 predicts [Search] five times in a session, we terminate and discard the data.Next, for each trajectory, where there are k rounds, we prompt GPT-3.5 using the first k − 1 rounds as context to decide the final round's action: [PredictAnswer] or [SeekAdvice].If GPT-3.5 selects [SeekAdvice], we replace the final step with [SeekAdvice] and the corresponding thoughts from GPT-3.5.Otherwise, the original trajectory remains unchanged.This process results in 10,240 session-level trajectories for the imitation learning stage.For the reinforcement learning stage, we directly use the original HotPotQA training set, consisting of 90,447 samples.</p>
<p>This function uses the LLM to generate a search query and invokes a search API.The first result not already present in the LLM context is selected and appended to the existing context.</p>
<p>Table 12 :
12
Statistics of the ProductQA dataset.# Products indicates the number of products within each group.# Fact-QA, # Search-QA and # Reasoning-QA display the respective numbers of QA pairs categorized as Fact-QA, Search-QA, and Reasoning-QA.
Groups# Products # Fact-QA # Search-QA # Reasoning-QA TotalBlades202,1477696313,547Headlight Bulbs201,7676444632,874Cell Phones201,6367613742,771Portable Power Banks203,3446735004,517Dresses202,2877382633,288Everyday Bras201,9426843362,962Wrist Watches202,1697573893,315Blu-ray Players201,6306885722,890Camera Lenses201,8597691,0253,653Headphones205,4327665836,781TrainMice205,6534902946,437Point &amp; Shoot Digital Cameras201,6967225652,983Coffee Machines204,1846816385,503Digital Scales202,7243916823,797Space Heaters202,2836744983,455Printers201,4317604892,680Litter201,8607535073,120Grips201,7717134132,897Gun Holsters201,679941,3623,135Handheld Flashlights202,0097684823,259Total40049,50313,29511,06673,864Leggings209697435272,239Camera Cases209757068982,579Motherboards209897368262,551TestAll Pans209737472751,995Rollerball Pens209677606032,330Rifle Scopes179797149782,671Total1175,8524,4064,10714,365</p>
<p>Table 13 :
13
Training statistics for each experiment.
TaskNumber of H800 GPU SFT Training Time RL Training TimeProductQA83.6 hours5.5 hoursMedMCQA80.9 hours2.0 hoursHotPotQA87.9 hours27.5 hours</p>
<p>Table 14 :
14
Detail performance of our methods and other baselines on six test product groups of ProductQA.</p>
<p>Table 15 :
15
Ablation study on ProductQA test tasks.w/o Reflection represents removing the reflection function.w/o Memory represents prohibiting memory component.w/o Advice represents removing the seeking advice function.Non-adapt advice represents seeking advice in the same number with agile-vic13b-ppo at the beginning of trajectory.w/o Tool-Use represents removing the search product function.w/o RL represents the agile-vic13b-sft.The best scores are highlighted in bold.
Groupw/o Reflection Memory w/ow/o AdviceNon-adapt Advicew/o Tool-Usew/o RLagile-vic-13b-ppoCamera CasesAdvice Rate ↓ Accuracy ↑ Total Score ↑0.335 0.851 0.750(-4.1%) 0.731(-6.8%) 0.735(-6.3%) 0.748(-4.4%) 0.459 0.000 0.263 0.869 0.735 0.8270.452 0.870 0.734(-6.4%) 0.760(-2.8%) 0.295 0.8490.263 0.860 0.781Advice Rate ↓0.2760.4370.0000.2510.5290.2900.251LeggingsAccuracy ↑0.8740.9020.7620.8280.8800.8670.876Total Score ↑0.791(-1.3%) 0.771(-3.9%) 0.762(-5.1%) 0.753(-6.4%) 0.721(-11.1%) 0.780(-2.7%)0.801Advice Rate ↓0.2630.4130.0000.2200.5500.2250.220All PansAccuracy ↑0.8670.9000.7590.8180.8770.8550.866Total Score ↑0.788(-1.5%) 0.776(-3.1%) 0.759(-5.4%) 0.752(-6.4%) 0.712(-12.4%) 0.788(-1.5%)0.800Rollerball PensAdvice Rate ↓ Accuracy ↑ Total Score ↑0.237 0.818 0.747(-0.7%) 0.730(-3.0%) 0.727(-3.4%) 0.721(-4.3%) 0.378 0.000 0.212 0.843 0.727 0.7850.501 0.868 0.718(-4.7%) 0.746(-0.8%) 0.220 0.8120.212 0.816 0.752Mother-boardsAdvice Rate ↓ Accuracy ↑ Total Score ↑0.270 0.878 0.797(-1.1%) 0.776(-3.9%) 0.766(-5.2%) 0.758(-6.3%) 0.728(-10.7%) 0.786(-2.5%) 0.368 0.000 0.235 0.483 0.285 0.886 0.766 0.829 0.873 0.8710.235 0.877 0.806Rifle ScopesAdvice Rate ↓ Accuracy ↑ Total Score ↑0.237 0.824 0.753(-1.3%) 0.742(-2.8%) 0.733(-4.1%) 0.718(-6.3%) 0.692(-10.3%) 0.739(-3.2%) 0.385 0.000 0.216 0.440 0.221 0.858 0.733 0.783 0.824 0.8050.216 0.828 0.763Advice Rate ↓0.2700.4070.0000.2330.4920.2560.233AverageAccuracy ↑0.8520.8760.7470.8120.8650.8430.854Total Score ↑0.771(-1.7%) 0.754(-4.0%) 0.747(-5.0%) 0.742(-5.7%)0.717(-9.3%) 0.766(-2.3%)0.784</p>
<p>Table 16 :
16
Performance of the model (agile-vic13b-ppo) trained on different seeking advice cost settings.Case #1, illustrated in Table 17, provides a specific example demonstrating how agile-vic13b-ppo proactively seeks advice from a human expert for questions it cannot answer.Furthermore, it leverages reflection to extract general knowledge from the expert's responses, which can then be applied in future QA sessions.
Seeking Advice CostGroup0.50.40.30.20.1Camera CasesAdvice Rate 0.108 0.189 0.263 0.339 0.458 Accuracy 0.806 0.829 0.860 0.885 0.929LeggingsAdvice Rate 0.098 0.188 0.251 0.317 0.464 Accuracy 0.824 0.844 0.876 0.877 0.921All PansAdvice Rate 0.094 0.163 0.220 0.262 0.384 Accuracy 0.813 0.845 0.866 0.889 0.926Rollerball PensAdvice Rate 0.100 0.163 0.212 0.264 0.406 Accuracy 0.780 0.799 0.816 0.829 0.891MotherboardsAdvice Rate 0.103 0.162 0.235 0.307 0.443 Accuracy 0.825 0.839 0.877 0.901 0.941Rifle ScopesAdvice Rate 0.087 0.144 0.216 0.257 0.385 Accuracy 0.780 0.797 0.828 0.845 0.897AverageAdvice Rate 0.098 0.168 0.233 0.291 0.423 Accuracy 0.805 0.825 0.854 0.871 0.918</p>
<p>Table 17 :
17
Case #1 from agile-vic13b-ppo on the test task, camera cases group, in ProductQA.representscorrect responses.Our agent proactively seeks advice from human experts at session 313 and summarizes a piece of general knowledge by reflection.In the following sessions, the agent leverages this knowledge to successfully answer three questions for different products at sessions 721, 1265, and 1364, respectively.It demonstrates the effectiveness of seeking advice and reflection.Camera cases with backpack straps feature ergonomic designs that help distribute the weight of the camera equipment across the user's back ... 3.[History] ... Camera cases with backpack straps feature ergonomic designs that help distribute the weight of the camera equipment across the user's back ... 3.[History] ... Camera cases with backpack straps feature ergonomic designs that help distribute the weight of the camera equipment across the user's back ... 3.[History] ...
Reflection / AnswerHuman Expert: Certainly, the backpackstraps on the camera case are designed for com-fort even when carrying heavier loads. Thesestraps enable ergonomic weight distribution,making the case ideal for prolonged travel.Reflection: Camera cases with backpackstraps feature ergonomic designs that helpdistribute the weight of the camera equipmentacross the user's back, reducing strain andincreasing comfort during transport.Answer: The case is designed to be adjustableand has padded straps which makes it suitablefor long-term use with heavy equipment.Answer: Certainly, a camera case with back-pack straps is engineered to evenly distributethe weight across your shoulders, improvingergonomics and comfort. This is particularlybeneficial for reducing fatigue and strain dur-ing prolonged activities like treks.Answer:ActionSeekAdvicePredictAnswerPredictAnswerPredictAnswerContextAnswer users' [Question] about product B00PV0IEDY based on the following threetypes of information:1. [Product Metadata] ... 'Carrying Options': 'backpack straps' ...2. [Memory] Existing knowledge in your memory about camera_cases.camera cases with no tripod mount are not suitable for scenarios where a stable camerasetup is crucial, such as long-exposure photography or macro work where camera shakecan significantly impact image quality. 3.[History] ......Question: Is the camera case designed for prolonged travel and capable of withstandingheavy loads?Answer users' [Question] about product B000WH86BQ ...1. [Product Metadata] ... 'Carrying Options': 'backpack straps' ...2. [Memory] Existing knowledge in your memory about camera_cases.Question: Is the camera case suitable for extended journeys and potentially heavy loads?Answer users' [Question] about product B005J09OXU ...1. [Product Metadata] ... 'Carrying Options': 'backpack straps' ...2. [Memory] Existing knowledge in your memory about camera_cases.Question: Will this camera case enhance the comfort of transporting my gear on treks?Answer users' [Question] about product B000YA33DC ...1. [Product Metadata] ... 'Carrying Options': 'backpack straps' ...2. [Memory] Existing knowledge in your memory about camera_cases.Question: Does this camera case facilitate easier transportation when I frequently travel?Session31372112651364</p>
<p>Table 18 :
18
Case #2 from agile-vic13b-ppo on the ProductQA test set.represents correct responses.By leveraging SQL searching tools, agent successfully find the correct product to recommend to users.
OutputSQL: SELECT product_id FROM"leggings" WHERE brand ="QZUnique" LIMIT 1;Answer: The product that meetsyour requirements is as follows.'QZUnique Women's Classic RetroPrinted Casual Pattern AnkleLength Elastic Tights Leggings'with asin: B012IKGF62SQL: SELECT product_id FROM"motherboards" WHERE brand ="ASRock" AND memory_support&gt;= 32 LIMIT 1;Answer: The product thatwould suit your preferences is: 'ASRock LGA1155 DDR3 SATA3USB3.0 Quad CrossFireX andQuad SLI A E ATX MotherboardZ77 EXTREME4' with asin:B007KTY4A6ActionSearchProductSearchProductContextAnswer users' [Question] about product B00FUHWE5M based on the following threetypes of information:1. [Product Metadata] provides metadata about product B00FUHWE5Mfrom the leggings database. The schema of the leggings database is outlined below(in the format field<a href="value1, value2, ...">unit</a>).price[dollar]brand(Jescakoo, QZUnique, ...)warmth(hot,moderate,cool) ...2. [Memory] ...3. [History] ......Question: I've heard good things about QZUnique. Spit out the first leggings you havefrom them. ...Answer users' [Question] about product B012AQGKXC based on the following three typesof information:1. [Product Metadata] provides metadata about product B012AQGKXCfrom the motherboards database. The schema of the motherboards database is outlined below(in the format field<a href="value1, value2, ...">unit</a>).price[dollar]brand(ASRock, Gigabyte, ...)memory_support[GB] ...2. [Memory] ...3. [History] ......Question: I'm looking for an ASRock motherboard with at least 32 GB of memory support.Don't keep me waiting, hustle up and find it for me. ...Session9181298
AcknowledgementsThe authors thank anonymous reviewers for their helpful suggestions.C.2 Training RobustnessWe conduct multiple independent trials of PPO training to study the variation of the result, as shown in Table10.On average, RL training improves the total score by 2.6%, with a standard deviation of 0.3%, demonstrating the significance of RL improvements.improvement in this setting.Interestingly, the performance of agile-vic13b-ppo-random is better than that of agile-vic13b-ppo.We conjecture that random seeking-advice is a better initial policy because it enables exploration in all directions.In the second experiment, we lowered the advice cost to 0. • Any long answer containing harmful information should be labeled as 'I do not know'.If you select 'I do not know', the row will be dropped, and you do not need to perform the subsequent steps for that row.3.Refine the long answer: If you select 'no' in step 2, provide a correct long answer in the Refined long answer column.4.Assess the short answer: Determine whether the short answer is correct.A short answer must be 'yes', 'no', or an entity.Choose 'yes' or 'no' in the Is short answer correct column.Consider the following special cases:• If the question is a choice and the short answer is 'yes' or 'no', it is incorrect.• If the question pertains to degrees (e.g.'How durable ... ?') and the short answer is 'yes' or 'no', it is incorrect.• If the short answer does not align with the long answer, it is incorrect.5.Refine the short answer: If you select 'no' in step 4, provide a correct short answer in the Refined short answer column.The authors will review the annotation in batches.Specifically, 5% of each batch will be checked.If the accuracy rate of the checked annotation is below 98%, the entire batch will be relabeled.The JVC HA-EB75 headphones contain a 13.5 mm neodymium driver in each earpiece, which contributes to the enhanced sound quality.[To fill] [To fill] 13.5 mm [To fill] [To fill]G Broader impact G.1 Positive broader impact(1) We created ProductQA, a dataset of 88,229 QA pairs across 26 product groups.This dataset provides a comprehensive evaluation environment for LLM agents, addressing real-world challenges such as managing historical information and accumulated knowledge, using tools, interacting with humans, performing self-evaluation, conducting reflection, and adapting to new tasks.We believe that ProductQA can advance the research in LLM agents.(2) AGILE serves as a general framework that supports a wide range of extensions.Agents within the framework can use more tools, perform complex reasoning using LLMs alone or in combination with other tools, and self-improve by accumulating experiences and knowledge.AGILE provides an architecture for creating powerful agents with the potential to achieve human-level intelligence.(3) AGILE supports proactive seeking advice from human experts, ensuring a high level of accuracy for applications, even when dealing with challenging questions.Within this framework, we can manage the trade-off between accuracy and human cost.These features enable AGILE agents to be applied in real-world scenarios.G.2 Negative broader impactIn practical applications, LLM agents exhibit superior capabilities compared to standalone LLMs.Our research validates that the AGILE framework is a highly effective approach for optimizing LLM agents.However, this improvement also increases the potential risks of harmful applications.Therefore, it is crucial to intensify research on the safety and responsible use of LLM agents.H Limitations(1) Due to resource constraints, our experiments primarily utilize LLMs with 7B or 13B parameters within AGILE.We expect that applying AGILE framework to larger models will result in moreAnswer users' [Question] about product {asin} based on the following three types of information:1. [Product Metadata] provides metadata about product {asin} from the {product_category} database.The schema of the {product_category} database is outlined below (in the format field<a href="value1, value2, ...">unit</a>).{schema} {metadata} In addition to the provided metadata about product {asin}, you have the option to access the full {product_category} database by executing SQL commands.[Memory]Existing knowledge in your memory about {product_category}.{knowledge}[History]Previous question-answer pairs related to product {asin}.{history}Evaluate whether the question can be answered based solely on the information available.[Question]: {question} -If the question can be answered directly, output <code>[PredictAnswer]\n[Answer]: -If an SQL search is required, output</code>[SearchProduct]\n[SQL]: SELECT product_id FROM {product_category} Figure6: The prompt for gpt3.5-prompt and gpt4-prompt on ProductQA.[prompt]Answer users' [Question] about product {asin} based on the following three types of information:1  The prompt for agile-vic13b-prompt, agile-gpt3.5-prompt, and agile-gpt4-prompt on ProductQA.[prompt]Analyze the question and answer that pertain to an online shopping scenario involving products in the "{product_category}" category.Your task is to extract any information that are generally applicable to the entire {product_category} category.Focus should be on the broader category characteristics rather than on details specific to a specific product.
Deep learning for ai. Yoshua Bengio, Yann Lecun, Geoffrey Hinton, Communications of the ACM. 6472021</p>
<p>Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew D Baldassari, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Autoagents: A framework for automatic agent generation. Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Jie Börje F Karlsson, Yemin Fu, Shi, arXiv:2309.172882023arXiv preprint</p>
<p>Asking before action: Gather information in embodied decision making with language models. Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, Jianyu Chen, arXiv:2305.156952023arXiv preprint</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, Yu Su, Advances in Neural Information Processing Systems. 202436</p>
<p>Product question answering in ecommerce: A survey. Yang Deng, Wenxuan Zhang, Qian Yu, Wai Lam, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, Peter Szolovits, Applied Sciences. 111464212021</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, arXiv:1705.035512017arXiv preprint</p>
<p>Language models (mostly) know what they know. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, arXiv:2207.052212022arXiv preprint</p>
<p>Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, Yang You, arXiv:2402.036102024arXiv preprint</p>
<p>Maps of bounded rationality: Psychology for behavioral economics. American economic review. Daniel Kahneman, 200393</p>
<p>Large language models struggle to learn long-tail knowledge. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel, International Conference on Machine Learning. PMLR2023</p>
<p>Small language models learn enhanced reasoning skills from medical textbooks. Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee, Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, Jaewoo Kang, arXiv:2404.003762024arXiv preprint</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.091102022arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>Justifying recommendations using distantlylabeled reviews and fine-grained aspects. Jianmo Ni, Jiacheng Li, Julian Mcauley, Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)2019</p>
<p>Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, arXiv:2303.13375Capabilities of gpt-4 on medical challenge problems. 2023arXiv preprint</p>
<p>Tao Qin, Naoto Usuyama, Chris White, and Eric Horvitz. Can generalist foundation models outcompete special. Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer Mckinney, Robert Osazuwa Ness, Hoifung Poon, 2023purpose tuning? case study in medicine</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu, Conference on health, inference, and learning. PMLR2022</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Tianjun Shishir G Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Tell me more! towards implicit user intention understanding of language model driven agents. Bingxiang Cheng Qian, Zhong He, Jia Zhuang, Yujia Deng, Xin Qin, Yankai Cong, Zhong Lin, Zhiyuan Zhang, Maosong Liu, Sun, arXiv:2402.092052024arXiv preprint</p>
<p>Autoact: Automatic agent learning from scratch via self-planning. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen, arXiv:2401.052682024arXiv preprint</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, 2019</p>
<p>Anushri Allen Z Ren, Alexandra Dixit, Sumeet Bodrova, Stephen Singh, Noah Tu, Peng Brown, Leila Xu, Fei Takayama, Jake Xia, Varley, arXiv:2307.01928Robots that ask for help: Uncertainty alignment for large language model planners. 2023arXiv preprint</p>
<p>Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, Rui Zhao, arXiv:2308.03427Tptu: Task planning and tool usage of large language model-based ai agents. 2023arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 202436</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Bill Byrne, and Adrià de Gispert. xpqa: Cross-lingual product question answering across 12 languages. Xiaoyu Shen, Akari Asai, arXiv:2305.092492023arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Advances in Neural Information Processing Systems. 202436</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, International Conference on Learning Representations. 2020</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Science-World: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.015602023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Llm a<em>: Human in the loop large language models enabled a</em> search for robotics. Hengjia Xiao, Peng Wang, arXiv:2312.017972023arXiv preprint</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, arXiv:2306.130632023arXiv preprint</p>
<p>Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, arXiv:2303.11381Mm-react: Prompting chatgpt for multimodal reasoning and action. 2023arXiv preprint</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.09600Hotpotqa: A dataset for diverse, explainable multi-hop question answering. 2018arXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202235</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Ask an expert: Leveraging language models to improve strategic reasoning in goal-oriented dialogue models. Qiang Zhang, Jason Naradowsky, Yusuke Miyao, arXiv:2305.178782023arXiv preprint</p>
<p>Siren's song in the ai ocean: a survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, arXiv:2309.012192023arXiv preprint</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Navigating the grey area: Expressions of overconfidence and uncertainty in language models. Kaitlyn Zhou, Dan Jurafsky, Tatsunori Hashimoto, arXiv-23022023arXiv e-prints</p>            </div>
        </div>

    </div>
</body>
</html>
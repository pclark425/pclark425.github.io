<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2592 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2592</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2592</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-85664bb73d09a20f4d3f7aae81143c0f25060fbb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/85664bb73d09a20f4d3f7aae81143c0f25060fbb" target="_blank">Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75% in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1. See \url{https://github.com/baoguangsheng/fast-detect-gpt} for code, data, and results.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2592.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2592.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fast-DetectGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fast-DetectGPT: Efficient Zero-Shot Detection via Conditional Probability Curvature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot automated detector that classifies passages as machine-generated or human-written by computing token-level conditional probability curvature; designed to replace expensive perturbation steps with efficient conditional sampling/analytic evaluation to achieve large speedups and improved accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fast-DetectGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Fast-DetectGPT is an automated zero-shot detection system that (1) samples alternative token choices conditioned on a fixed passage, (2) computes the conditional log-probabilities p_theta(tilde{x} | x) for those alternatives (either by sampling many independent token alternatives or by an analytic summation over the vocabulary), (3) computes a standardized curvature score d = (log p_theta(x|x) - mean(log p_theta(tilde{x}|x))) / std(log p_theta(tilde{x}|x)), and (4) thresholds this score to decide machine vs human authorship. Key capabilities: single or very few model calls (sampling + scoring can be merged), analytical exact computation of expectations token-wise, normalization by variance, ability to operate in white-box (use source model) or black-box (use surrogate models) settings, and robust evaluation across datasets/decoding strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Detection System / Zero-shot AI detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Natural language processing — detection of machine-generated text (AI-authorship attribution / content provenance)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Binary classification of passages as machine-generated versus human-authored in a zero-shot setting (no finetuning on generated text). Evaluated both in white-box (scoring model = source model) and black-box (use surrogate scoring/sampling models) across multiple datasets (news summarization XSum, Wikipedia contexts SQuAD, WritingPrompts stories, WMT16 translation, PubMedQA technical Q&A).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional discrete-sequence problem: variable-length token sequences, large vocabulary (tens of thousands of tokens), sequential dependency (autoregressive probabilities) and domain/model variability (multiple source models from 1.3B to 1800B parameters). Multi-source, cross-domain generalization required; search/uncertainty captured via token-wise distribution enumeration/sampling. Complexity quantified by vocabulary size (implicit), sequence lengths up to ~150 words used in experiments, and requirement to evaluate many alternative tokens (sampling defaults: 10,000 token-level samples or analytic enumeration over vocabulary).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses standard pre-existing NLP datasets: XSum, SQuAD, WritingPrompts, WMT16 (EN/DE), PubMedQA; per-dataset sample sizes used in experiments: 150–500 examples per dataset per source model for positive/negative samples. Data quality: real human-written passages sampled from these corpora; machine-generated samples produced by prompting source models with 30-token prefixes (or question prompts for PubMedQA). No additional costly data collection required.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Low relative to prior perturbation-based methods: experiments report a 340x speedup versus DetectGPT on Tesla A100 (DetectGPT ~79,113s for the five-model XSum runs vs Fast-DetectGPT ~233s). Default sampling approximation uses 10,000 categorical samples per token using a single sampling+scoring forward pass; analytic exact computation (enumeration over vocabulary per token) further reduces runtime (additional ~10% faster than sampling). Memory: depends on model size used for scoring/sampling (they run locally for models <= ~20B on A100 with fp16 for >6B) and storing 10k samples; overall compute vastly reduced compared to 100 perturbation scoring loops.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined binary classification with clear evaluation metrics (AUROC, recall at specified false positive rates). Discrete, stochastic generation process (autoregressive LM) with token-level conditional distributions; both white-box (known source model) and black-box (unknown source, surrogate models) variants. Domain knowledge useful for prompt design and surrogate model selection but not required for zero-shot use.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>AUROC (primary), recall / true positive rate at fixed false-positive rates, relative improvement metric ((new-old)/(1-old)), and runtime speedup factor.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported high performance: white-box average AUROC across five source models = 0.9887 (Table 2, avg over GPT-2..NeoX), black-box average AUROC (surrogate Neo-2.7) = 0.9677; on ChatGPT generations (black-box) avg AUROC = 0.9615 (Table 3); on GPT-4 (black-box) avg AUROC = 0.9061. Speed: ~340x faster than DetectGPT for XSum five-model runs. Recall examples: for ChatGPT, recall = 87% at 1% false-positive rate (paper also reports 'flags 80% of ChatGPT-crafted content while misidentifying 1% of human compositions' and 98% recall at 10% FPR).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performance degrades in black-box settings relative to white-box (authors report ~65% better in white-box than black-box), sensitivity to surrogate scoring model selection (domain/corpus mismatch can reduce detection), shorter passages reduce statistical signal leading to lower accuracy, potential elevated false positives on non-native English text (inherits LLM biases), and adversarial paraphrasing / decoherence attacks can reduce effectiveness (authors analyze robustness and note Fast-DetectGPT is more robust than baselines but not invulnerable).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Token-level conditional probability curvature is informative (machines choose higher-probability tokens within context); normalization by sample std (sigma) improves comparability and performance; ability to compute expectations analytically token-wise or to sample independently (conditional independent sampling) yields efficiency; using a strong sampling model (e.g., GPT-J) can boost detection; single (or few) forward-pass design avoids repeated expensive re-scoring of entire sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared to DetectGPT baseline: relative AUROC improvement ≈ 74.7% (white-box average) and ≈ 74.5% (black-box average) across multiple source models; absolute AUROC increases depend on source model (e.g., DetectGPT avg 0.9554 vs Fast-DetectGPT 0.9887 in white-box). Speed: Fast-DetectGPT is ~340x faster on XSum five-model runs. Compared to supervised detectors (RoBERTa-based) and GPTZero, Fast-DetectGPT achieves higher or competitive AUROCs across domains; GPTZero performs well on news but worse on stories and technical writing, indicating domain biases in supervised systems.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Supervised detectors (RoBERTa-base/large) used as human-engineered baselines; example: RoBERTa-base avg AUROC on ChatGPT generations = 0.7474 (Table 3) vs Fast-DetectGPT 0.9615 (black-box). GPTZero (commercial) performed at 0.9348 avg on ChatGPT but worse on other domains; no direct human-rater performance given, but authors highlight that even experts can be fooled per prior work (Ippolito et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2592.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2592.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DetectGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DetectGPT: Zero-shot machine-generated text detection using probability curvature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot detector that estimates probability curvature by generating textual perturbations of the input (via a perturbation model) and comparing the source model log-probability of the original to the mean log-probability of the perturbations; positive curvature indicates likely machine generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Detectgpt: Zero-shot machine-generated text detection using probability curvature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DetectGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DetectGPT computes a curvature score d(x,p_theta,q_phi) = log p_theta(x) - E_{tilde{x} ~ q_phi(.|x)}[log p_theta(tilde{x})] by (1) perturbing input x via a perturbation model (e.g., mask language model T5) to generate ~100 rewrites, (2) scoring each rewrite using the scoring/source model p_theta to get log-probabilities, and (3) estimating curvature and thresholding. It relies on many separate model calls: one perturbation generation per call and one scoring pass per perturbation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Detection System / Zero-shot AI detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>NLP — detection of machine-generated text</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Zero-shot detection of machine-generated passages by estimating how the source model probability of the passage compares to nearby perturbations; evaluated on same datasets as Fast-DetectGPT (XSum, SQuAD, WritingPrompts, WMT16, PubMedQA).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same high-dimensional sequence classification problem; requires evaluating entire autoregressive probability over perturbed sequences, meaning small token changes require recomputation of the full sequence probability (Markov chain), increasing computational cost. Typical practical setting uses ~100 perturbations per passage.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses standard NLP datasets (same as Fast-DetectGPT) with 150–500 examples per dataset in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>High: authors note DetectGPT typically executes ~100 model calls per passage (one per perturbation) for scoring; experimental runtime reported ~79,113 seconds (~22 hours) across five runs on a Tesla A100 for the XSum five-model experiment described, significantly larger than Fast-DetectGPT. Memory depends on model sizes used for scoring and perturbation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined binary detection problem with clear evaluation metrics (AUROC). Method treats the passage as an entire chain and requires multiple end-to-end sequence evaluations for perturbed samples.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>AUROC primarily.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported baseline performance: white-box AUROC averaged across five source models = 0.9554 (Table 1 / Table 2). On ChatGPT/GPT-4 detection in black-box, DetectGPT (T5-11B/Neo-2.7) avg AUROC = 0.8223 (ChatGPT) and 0.6228 (GPT-4) in Table 3 entries depending on configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Very high computational cost due to repeated full-sequence scoring; sensitivity to the choice of perturbation model q_phi; reduced practicality for large-scale or real-time use. Also less robust in black-box surrogate scenarios compared to Fast-DetectGPT and may degrade on domain mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Probability curvature principle — machine-generated text tends to be at local maxima of model probability relative to perturbations — yields good detection when computational resources permit many perturbations and scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Outperformed by Fast-DetectGPT in both accuracy and speed (Fast-DetectGPT relative AUROC improvements of ~74.7% white-box and ~74.5% black-box; 340x speedup). DetectGPT often outperforms simple likelihood baselines but at much higher compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Compared to supervised detectors and Fast-DetectGPT as above; DetectGPT white-box generally stronger than many baselines but weaker than Fast-DetectGPT in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2592.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2592.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NPR (DetectLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NPR / DetectLLM: Normalized Perturbation-based Rank/Normalized Log-Rank Perturbation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A perturbation/normalized log-rank based zero-shot detection variant that normalizes log-rank perturbation signals; used as a stronger variant of DetectGPT and a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NPR / DetectLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NPR (referred to as DetectLLM in cited work) leverages normalized log-rank perturbation metrics: it generates perturbations and uses normalized differences in token log-rank statistics to detect machine authorship. Like DetectGPT, it requires multiple perturbations and repeated scoring, but applies normalization strategies to improve detection.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Detection System / Zero-shot AI detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>NLP — machine-generated text detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Zero-shot detection using normalized perturbation-based log-rank signals; used as a competitive baseline in the same cross-domain detection tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same sequence-level detection complexity; requires generation and scoring of many perturbations (computationally expensive).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Evaluated on the same datasets (XSum, SQuAD, WritingPrompts, others) with 150–500 examples per dataset in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>High — methods marked with diamond in tables indicate they invoke models multiple times (hundreds of calls) and thus incur significant computational overhead; NPR in experiments used T5-11B for perturbations and Neo-2.7 for scoring in black-box variants.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined binary detection problem with evaluation by AUROC; uses sampling/perturbation paradigm similar to DetectGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>AUROC.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Competitive: NPR reported very high AUROCs on some configurations (e.g., Table 5 shows NPR achieving AUROCs near or above 0.98 on several source models/datasets), but performance and compute are configuration-dependent. Average NPR in some white-box settings reported around 0.9645 (Table 2 column 'NPR' avg for five models).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High compute cost due to perturbation/scoring loop; sensitivity to choice of perturbation/statistic normalization; potential degradation in black-box scenarios with surrogate models.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Normalization of log-rank perturbations increases signal robustness; multiple perturbations help reveal curvature/log-rank differences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Generally stronger than older baselines like basic likelihood or entropy but still outperformed by Fast-DetectGPT in combined efficiency and accuracy in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Benchmarked against supervised detectors and other zero-shot baselines; NPR often close to DetectGPT in performance but more expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2592.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2592.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DNA-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DNA-GPT: Divergent N-gram Analysis for GPT-generated text detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-free detector based on divergence in n-gram statistics between multiple completions of a truncated passage; used as a baseline zero-shot detector.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>dna-gpt: divergent n-gram analysis for training-free detection of gpt-generated text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DNA-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DNA-GPT detects machine-generated text by analyzing divergence patterns in n-gram distributions across multiple completions (divergent n-gram analysis). It is training-free and relies on comparing multiple generated continuations to find signals indicative of model generation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Detection System / Zero-shot AI detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>NLP — machine-generated text detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Zero-shot detection using completion-divergence-based n-gram statistics across multiple model-generated continuations of truncated passages.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Requires generating multiple completions per prefix (10 completions used in experiments) and computing n-gram divergence metrics; complexity grows with number of completions and n-gram sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Evaluated on standard datasets (XSum, SQuAD, WritingPrompts) with similar sample sizes as other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Moderate to high depending on number of completions (authors used 10 prefix completions per passage in experiments) and scoring models used; less expensive than 100-perturbation DetectGPT but more than single-pass likelihood baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined detection with explicit multiple-completion sampling; stochastic due to different completions and decoding strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>AUROC.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Competitive but generally below the best-performing DetectGPT/NPR/Fast-DetectGPT in the experiments; Table 5 reports DNA-GPT average AUROC ~0.8513 for five source models (white-box table), varying by dataset and model.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performance depends on number and diversity of completions; may be sensitive to decoding strategy and truncated-prefix length.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Divergence signals capture generation-specific n-gram regularities; training-free and simple to compute.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Generally outperformed by DetectGPT/NPR and by Fast-DetectGPT on average AUROC, though performance varies by source model and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Benchmarked against supervised and other zero-shot baselines; not as strong as top methods in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2592.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2592.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Likelihood / Entropy / LogRank / LRR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Statistical baselines: Likelihood, Entropy, LogRank, LRR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of established zero-shot statistical baseline detectors: Likelihood (average log-probability), Entropy (mean token entropy), LogRank (average log of token ranks), and LRR (combined log-probability and log-rank).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Statistical baselines (Likelihood, Entropy, LogRank, LRR)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>These are simple zero-shot detectors computed from a pre-trained LM: Likelihood uses mean token log-probability (or sequence likelihood), Entropy computes mean predictive token entropy, LogRank computes average token rank statistics, and LRR combines log-probability and log-rank. They require a single forward pass for scoring and therefore are computationally cheap.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Baseline Statistical Detection Methods</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>NLP — machine-generated text detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Single-pass statistical metrics from language model predictive distributions used to separate machine-generated and human-written text in a zero-shot fashion.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Low per-sample computational complexity (single forward pass), but detecting subtle signals across domains/models remains challenging; performance depends on model calibration and domain match.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Same datasets as other entries; evaluated across multiple source models.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Minimal: one scoring pass per passage using a pre-trained LM; very low runtime relative to perturbation-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Deterministic scoring functions with clear evaluation metrics; well-defined but limited statistical signal for short passages or model-matched domains.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>AUROC and related detection metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Varied: Likelihood is often a strong baseline — e.g., white-box Likelihood averaged 0.8683 (Table 2) and for ChatGPT detection Likelihood (Neo-2.7) avg = 0.9364 (Table 3). Entropy alone performs poorly in many cases (AUROCs often near 0.5). LRR and LogRank often improve upon basic likelihood for some settings (e.g., LRR avg 0.9299 for white-box five models in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Simple statistical metrics can fail under domain/model mismatch or with instruction-tuned models (e.g., GPT-4) and often degrade on very short passages; Entropy alone is weak. They lack robustness to decoding strategy variations and adversarial paraphrasing compared to curvature-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Extremely low compute cost and reasonable performance in many settings; likelihood especially works well when scoring model aligns with source model or domain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Likelihood often strong baseline and in some black-box experiments was the best simple baseline. Still, Fast-DetectGPT and DetectGPT generally outperform these baselines in AUROC when sufficient computation or curvature estimation is used.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Compared to supervised detectors and Fast-DetectGPT; statistical baselines sometimes approach supervised performance on particular datasets but are generally outperformed by best-curvature methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2592.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2592.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTZero / RoBERTa detectors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPTZero (commercial) and RoBERTa-based supervised detectors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised detectors trained to discriminate machine-generated from human text (RoBERTa-base/large trained by OpenAI and GPTZero commercial detector); used as supervised baselines in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gptzero: Towards detection of ai-generated text using zero-shot and supervised methods</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Supervised detectors (RoBERTa-base/large, GPTZero)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Supervised classifiers (RoBERTa fine-tuned, GPTZero proprietary) trained on labeled machine vs human text to predict AI authorship. They learn discriminative patterns from training data (bag-of-words or neural features) and are evaluated on held-out and out-of-distribution data to assess generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Supervised Detection System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>NLP — machine-generated text detection (supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Trained binary classifiers to detect AI-generated text across datasets; tested on ChatGPT, GPT-3, GPT-4 generations and human texts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Supervised learning on high-dimensional text representations; complexity depends on dataset diversity and label quality; prone to overfitting to training distribution and source models.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires labeled datasets of human and machine-generated text for training; public GPT-2 output dataset referenced for detector training; authors use 150–500 test examples per dataset for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Training cost variable (fine-tuning RoBERTa large requires GPU resources); inference cost modest (single forward pass per sample).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Supervised classification: deterministic inference given model weights; performance hinges on training data representativeness and domain overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>AUROC and accuracy in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Varies by domain: e.g., RoBERTa-base AUROC on ChatGPT (avg across datasets) = 0.7474 and RoBERTa-large = 0.6906 (Table 3), while GPTZero achieved very high AUROC on ChatGPT XSum (0.9952) but lower on other domains (avg 0.9348 on ChatGPT). Supervised detectors performed poorly on some out-of-distribution datasets compared to zero-shot methods.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Overfitting to particular domains or source models; poor generalization to out-of-distribution domains and unseen generation models; inconsistent performance across datasets (e.g., GPTZero strong on news but weak on stories/technical writing).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large and representative labeled training sets improve supervised detector performance in-domain; feature-rich models (RoBERTa-large) may help if training data matches test distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>In many black-box, cross-domain settings Fast-DetectGPT outperforms supervised detectors; supervised models can beat zero-shot methods in matched conditions but degrade strongly under domain/model shift.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Supervised detectors represent human-engineered baseline models; direct human rater performance not provided in paper but prior work indicates human experts can be fooled and supervised detectors are not fully reliable cross-domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Detectgpt: Zero-shot machine-generated text detection using probability curvature <em>(Rating: 2)</em></li>
                <li>Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text <em>(Rating: 2)</em></li>
                <li>dna-gpt: divergent n-gram analysis for training-free detection of gpt-generated text <em>(Rating: 2)</em></li>
                <li>Gltr: Statistical detection and visualization of generated text <em>(Rating: 1)</em></li>
                <li>Detecting fake content with relative entropy scoring <em>(Rating: 1)</em></li>
                <li>Can ai-generated text be reliably detected? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2592",
    "paper_id": "paper-85664bb73d09a20f4d3f7aae81143c0f25060fbb",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "Fast-DetectGPT",
            "name_full": "Fast-DetectGPT: Efficient Zero-Shot Detection via Conditional Probability Curvature",
            "brief_description": "A zero-shot automated detector that classifies passages as machine-generated or human-written by computing token-level conditional probability curvature; designed to replace expensive perturbation steps with efficient conditional sampling/analytic evaluation to achieve large speedups and improved accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Fast-DetectGPT",
            "system_description": "Fast-DetectGPT is an automated zero-shot detection system that (1) samples alternative token choices conditioned on a fixed passage, (2) computes the conditional log-probabilities p_theta(tilde{x} | x) for those alternatives (either by sampling many independent token alternatives or by an analytic summation over the vocabulary), (3) computes a standardized curvature score d = (log p_theta(x|x) - mean(log p_theta(tilde{x}|x))) / std(log p_theta(tilde{x}|x)), and (4) thresholds this score to decide machine vs human authorship. Key capabilities: single or very few model calls (sampling + scoring can be merged), analytical exact computation of expectations token-wise, normalization by variance, ability to operate in white-box (use source model) or black-box (use surrogate models) settings, and robust evaluation across datasets/decoding strategies.",
            "system_type": "Automated Detection System / Zero-shot AI detection",
            "problem_domain": "Natural language processing — detection of machine-generated text (AI-authorship attribution / content provenance)",
            "problem_description": "Binary classification of passages as machine-generated versus human-authored in a zero-shot setting (no finetuning on generated text). Evaluated both in white-box (scoring model = source model) and black-box (use surrogate scoring/sampling models) across multiple datasets (news summarization XSum, Wikipedia contexts SQuAD, WritingPrompts stories, WMT16 translation, PubMedQA technical Q&A).",
            "problem_complexity": "High-dimensional discrete-sequence problem: variable-length token sequences, large vocabulary (tens of thousands of tokens), sequential dependency (autoregressive probabilities) and domain/model variability (multiple source models from 1.3B to 1800B parameters). Multi-source, cross-domain generalization required; search/uncertainty captured via token-wise distribution enumeration/sampling. Complexity quantified by vocabulary size (implicit), sequence lengths up to ~150 words used in experiments, and requirement to evaluate many alternative tokens (sampling defaults: 10,000 token-level samples or analytic enumeration over vocabulary).",
            "data_availability": "Uses standard pre-existing NLP datasets: XSum, SQuAD, WritingPrompts, WMT16 (EN/DE), PubMedQA; per-dataset sample sizes used in experiments: 150–500 examples per dataset per source model for positive/negative samples. Data quality: real human-written passages sampled from these corpora; machine-generated samples produced by prompting source models with 30-token prefixes (or question prompts for PubMedQA). No additional costly data collection required.",
            "computational_requirements": "Low relative to prior perturbation-based methods: experiments report a 340x speedup versus DetectGPT on Tesla A100 (DetectGPT ~79,113s for the five-model XSum runs vs Fast-DetectGPT ~233s). Default sampling approximation uses 10,000 categorical samples per token using a single sampling+scoring forward pass; analytic exact computation (enumeration over vocabulary per token) further reduces runtime (additional ~10% faster than sampling). Memory: depends on model size used for scoring/sampling (they run locally for models &lt;= ~20B on A100 with fp16 for &gt;6B) and storing 10k samples; overall compute vastly reduced compared to 100 perturbation scoring loops.",
            "problem_structure": "Well-defined binary classification with clear evaluation metrics (AUROC, recall at specified false positive rates). Discrete, stochastic generation process (autoregressive LM) with token-level conditional distributions; both white-box (known source model) and black-box (unknown source, surrogate models) variants. Domain knowledge useful for prompt design and surrogate model selection but not required for zero-shot use.",
            "success_metric": "AUROC (primary), recall / true positive rate at fixed false-positive rates, relative improvement metric ((new-old)/(1-old)), and runtime speedup factor.",
            "success_rate": "Reported high performance: white-box average AUROC across five source models = 0.9887 (Table 2, avg over GPT-2..NeoX), black-box average AUROC (surrogate Neo-2.7) = 0.9677; on ChatGPT generations (black-box) avg AUROC = 0.9615 (Table 3); on GPT-4 (black-box) avg AUROC = 0.9061. Speed: ~340x faster than DetectGPT for XSum five-model runs. Recall examples: for ChatGPT, recall = 87% at 1% false-positive rate (paper also reports 'flags 80% of ChatGPT-crafted content while misidentifying 1% of human compositions' and 98% recall at 10% FPR).",
            "failure_modes": "Performance degrades in black-box settings relative to white-box (authors report ~65% better in white-box than black-box), sensitivity to surrogate scoring model selection (domain/corpus mismatch can reduce detection), shorter passages reduce statistical signal leading to lower accuracy, potential elevated false positives on non-native English text (inherits LLM biases), and adversarial paraphrasing / decoherence attacks can reduce effectiveness (authors analyze robustness and note Fast-DetectGPT is more robust than baselines but not invulnerable).",
            "success_factors": "Token-level conditional probability curvature is informative (machines choose higher-probability tokens within context); normalization by sample std (sigma) improves comparability and performance; ability to compute expectations analytically token-wise or to sample independently (conditional independent sampling) yields efficiency; using a strong sampling model (e.g., GPT-J) can boost detection; single (or few) forward-pass design avoids repeated expensive re-scoring of entire sequences.",
            "comparative_results": "Compared to DetectGPT baseline: relative AUROC improvement ≈ 74.7% (white-box average) and ≈ 74.5% (black-box average) across multiple source models; absolute AUROC increases depend on source model (e.g., DetectGPT avg 0.9554 vs Fast-DetectGPT 0.9887 in white-box). Speed: Fast-DetectGPT is ~340x faster on XSum five-model runs. Compared to supervised detectors (RoBERTa-based) and GPTZero, Fast-DetectGPT achieves higher or competitive AUROCs across domains; GPTZero performs well on news but worse on stories and technical writing, indicating domain biases in supervised systems.",
            "human_baseline": "Supervised detectors (RoBERTa-base/large) used as human-engineered baselines; example: RoBERTa-base avg AUROC on ChatGPT generations = 0.7474 (Table 3) vs Fast-DetectGPT 0.9615 (black-box). GPTZero (commercial) performed at 0.9348 avg on ChatGPT but worse on other domains; no direct human-rater performance given, but authors highlight that even experts can be fooled per prior work (Ippolito et al., 2020).",
            "uuid": "e2592.0",
            "source_info": {
                "paper_title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DetectGPT",
            "name_full": "DetectGPT: Zero-shot machine-generated text detection using probability curvature",
            "brief_description": "A zero-shot detector that estimates probability curvature by generating textual perturbations of the input (via a perturbation model) and comparing the source model log-probability of the original to the mean log-probability of the perturbations; positive curvature indicates likely machine generation.",
            "citation_title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "mention_or_use": "use",
            "system_name": "DetectGPT",
            "system_description": "DetectGPT computes a curvature score d(x,p_theta,q_phi) = log p_theta(x) - E_{tilde{x} ~ q_phi(.|x)}[log p_theta(tilde{x})] by (1) perturbing input x via a perturbation model (e.g., mask language model T5) to generate ~100 rewrites, (2) scoring each rewrite using the scoring/source model p_theta to get log-probabilities, and (3) estimating curvature and thresholding. It relies on many separate model calls: one perturbation generation per call and one scoring pass per perturbation.",
            "system_type": "Automated Detection System / Zero-shot AI detection",
            "problem_domain": "NLP — detection of machine-generated text",
            "problem_description": "Zero-shot detection of machine-generated passages by estimating how the source model probability of the passage compares to nearby perturbations; evaluated on same datasets as Fast-DetectGPT (XSum, SQuAD, WritingPrompts, WMT16, PubMedQA).",
            "problem_complexity": "Same high-dimensional sequence classification problem; requires evaluating entire autoregressive probability over perturbed sequences, meaning small token changes require recomputation of the full sequence probability (Markov chain), increasing computational cost. Typical practical setting uses ~100 perturbations per passage.",
            "data_availability": "Uses standard NLP datasets (same as Fast-DetectGPT) with 150–500 examples per dataset in experiments.",
            "computational_requirements": "High: authors note DetectGPT typically executes ~100 model calls per passage (one per perturbation) for scoring; experimental runtime reported ~79,113 seconds (~22 hours) across five runs on a Tesla A100 for the XSum five-model experiment described, significantly larger than Fast-DetectGPT. Memory depends on model sizes used for scoring and perturbation generation.",
            "problem_structure": "Well-defined binary detection problem with clear evaluation metrics (AUROC). Method treats the passage as an entire chain and requires multiple end-to-end sequence evaluations for perturbed samples.",
            "success_metric": "AUROC primarily.",
            "success_rate": "Reported baseline performance: white-box AUROC averaged across five source models = 0.9554 (Table 1 / Table 2). On ChatGPT/GPT-4 detection in black-box, DetectGPT (T5-11B/Neo-2.7) avg AUROC = 0.8223 (ChatGPT) and 0.6228 (GPT-4) in Table 3 entries depending on configuration.",
            "failure_modes": "Very high computational cost due to repeated full-sequence scoring; sensitivity to the choice of perturbation model q_phi; reduced practicality for large-scale or real-time use. Also less robust in black-box surrogate scenarios compared to Fast-DetectGPT and may degrade on domain mismatch.",
            "success_factors": "Probability curvature principle — machine-generated text tends to be at local maxima of model probability relative to perturbations — yields good detection when computational resources permit many perturbations and scoring.",
            "comparative_results": "Outperformed by Fast-DetectGPT in both accuracy and speed (Fast-DetectGPT relative AUROC improvements of ~74.7% white-box and ~74.5% black-box; 340x speedup). DetectGPT often outperforms simple likelihood baselines but at much higher compute cost.",
            "human_baseline": "Compared to supervised detectors and Fast-DetectGPT as above; DetectGPT white-box generally stronger than many baselines but weaker than Fast-DetectGPT in experiments.",
            "uuid": "e2592.1",
            "source_info": {
                "paper_title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "NPR (DetectLLM)",
            "name_full": "NPR / DetectLLM: Normalized Perturbation-based Rank/Normalized Log-Rank Perturbation",
            "brief_description": "A perturbation/normalized log-rank based zero-shot detection variant that normalizes log-rank perturbation signals; used as a stronger variant of DetectGPT and a baseline in this paper.",
            "citation_title": "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
            "mention_or_use": "use",
            "system_name": "NPR / DetectLLM",
            "system_description": "NPR (referred to as DetectLLM in cited work) leverages normalized log-rank perturbation metrics: it generates perturbations and uses normalized differences in token log-rank statistics to detect machine authorship. Like DetectGPT, it requires multiple perturbations and repeated scoring, but applies normalization strategies to improve detection.",
            "system_type": "Automated Detection System / Zero-shot AI detection",
            "problem_domain": "NLP — machine-generated text detection",
            "problem_description": "Zero-shot detection using normalized perturbation-based log-rank signals; used as a competitive baseline in the same cross-domain detection tasks.",
            "problem_complexity": "Same sequence-level detection complexity; requires generation and scoring of many perturbations (computationally expensive).",
            "data_availability": "Evaluated on the same datasets (XSum, SQuAD, WritingPrompts, others) with 150–500 examples per dataset in the experiments.",
            "computational_requirements": "High — methods marked with diamond in tables indicate they invoke models multiple times (hundreds of calls) and thus incur significant computational overhead; NPR in experiments used T5-11B for perturbations and Neo-2.7 for scoring in black-box variants.",
            "problem_structure": "Well-defined binary detection problem with evaluation by AUROC; uses sampling/perturbation paradigm similar to DetectGPT.",
            "success_metric": "AUROC.",
            "success_rate": "Competitive: NPR reported very high AUROCs on some configurations (e.g., Table 5 shows NPR achieving AUROCs near or above 0.98 on several source models/datasets), but performance and compute are configuration-dependent. Average NPR in some white-box settings reported around 0.9645 (Table 2 column 'NPR' avg for five models).",
            "failure_modes": "High compute cost due to perturbation/scoring loop; sensitivity to choice of perturbation/statistic normalization; potential degradation in black-box scenarios with surrogate models.",
            "success_factors": "Normalization of log-rank perturbations increases signal robustness; multiple perturbations help reveal curvature/log-rank differences.",
            "comparative_results": "Generally stronger than older baselines like basic likelihood or entropy but still outperformed by Fast-DetectGPT in combined efficiency and accuracy in the authors' experiments.",
            "human_baseline": "Benchmarked against supervised detectors and other zero-shot baselines; NPR often close to DetectGPT in performance but more expensive.",
            "uuid": "e2592.2",
            "source_info": {
                "paper_title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DNA-GPT",
            "name_full": "DNA-GPT: Divergent N-gram Analysis for GPT-generated text detection",
            "brief_description": "A training-free detector based on divergence in n-gram statistics between multiple completions of a truncated passage; used as a baseline zero-shot detector.",
            "citation_title": "dna-gpt: divergent n-gram analysis for training-free detection of gpt-generated text",
            "mention_or_use": "use",
            "system_name": "DNA-GPT",
            "system_description": "DNA-GPT detects machine-generated text by analyzing divergence patterns in n-gram distributions across multiple completions (divergent n-gram analysis). It is training-free and relies on comparing multiple generated continuations to find signals indicative of model generation.",
            "system_type": "Automated Detection System / Zero-shot AI detection",
            "problem_domain": "NLP — machine-generated text detection",
            "problem_description": "Zero-shot detection using completion-divergence-based n-gram statistics across multiple model-generated continuations of truncated passages.",
            "problem_complexity": "Requires generating multiple completions per prefix (10 completions used in experiments) and computing n-gram divergence metrics; complexity grows with number of completions and n-gram sizes.",
            "data_availability": "Evaluated on standard datasets (XSum, SQuAD, WritingPrompts) with similar sample sizes as other baselines.",
            "computational_requirements": "Moderate to high depending on number of completions (authors used 10 prefix completions per passage in experiments) and scoring models used; less expensive than 100-perturbation DetectGPT but more than single-pass likelihood baselines.",
            "problem_structure": "Well-defined detection with explicit multiple-completion sampling; stochastic due to different completions and decoding strategies.",
            "success_metric": "AUROC.",
            "success_rate": "Competitive but generally below the best-performing DetectGPT/NPR/Fast-DetectGPT in the experiments; Table 5 reports DNA-GPT average AUROC ~0.8513 for five source models (white-box table), varying by dataset and model.",
            "failure_modes": "Performance depends on number and diversity of completions; may be sensitive to decoding strategy and truncated-prefix length.",
            "success_factors": "Divergence signals capture generation-specific n-gram regularities; training-free and simple to compute.",
            "comparative_results": "Generally outperformed by DetectGPT/NPR and by Fast-DetectGPT on average AUROC, though performance varies by source model and dataset.",
            "human_baseline": "Benchmarked against supervised and other zero-shot baselines; not as strong as top methods in these experiments.",
            "uuid": "e2592.3",
            "source_info": {
                "paper_title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Likelihood / Entropy / LogRank / LRR",
            "name_full": "Statistical baselines: Likelihood, Entropy, LogRank, LRR",
            "brief_description": "A set of established zero-shot statistical baseline detectors: Likelihood (average log-probability), Entropy (mean token entropy), LogRank (average log of token ranks), and LRR (combined log-probability and log-rank).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Statistical baselines (Likelihood, Entropy, LogRank, LRR)",
            "system_description": "These are simple zero-shot detectors computed from a pre-trained LM: Likelihood uses mean token log-probability (or sequence likelihood), Entropy computes mean predictive token entropy, LogRank computes average token rank statistics, and LRR combines log-probability and log-rank. They require a single forward pass for scoring and therefore are computationally cheap.",
            "system_type": "Baseline Statistical Detection Methods",
            "problem_domain": "NLP — machine-generated text detection",
            "problem_description": "Single-pass statistical metrics from language model predictive distributions used to separate machine-generated and human-written text in a zero-shot fashion.",
            "problem_complexity": "Low per-sample computational complexity (single forward pass), but detecting subtle signals across domains/models remains challenging; performance depends on model calibration and domain match.",
            "data_availability": "Same datasets as other entries; evaluated across multiple source models.",
            "computational_requirements": "Minimal: one scoring pass per passage using a pre-trained LM; very low runtime relative to perturbation-based methods.",
            "problem_structure": "Deterministic scoring functions with clear evaluation metrics; well-defined but limited statistical signal for short passages or model-matched domains.",
            "success_metric": "AUROC and related detection metrics.",
            "success_rate": "Varied: Likelihood is often a strong baseline — e.g., white-box Likelihood averaged 0.8683 (Table 2) and for ChatGPT detection Likelihood (Neo-2.7) avg = 0.9364 (Table 3). Entropy alone performs poorly in many cases (AUROCs often near 0.5). LRR and LogRank often improve upon basic likelihood for some settings (e.g., LRR avg 0.9299 for white-box five models in Table 2).",
            "failure_modes": "Simple statistical metrics can fail under domain/model mismatch or with instruction-tuned models (e.g., GPT-4) and often degrade on very short passages; Entropy alone is weak. They lack robustness to decoding strategy variations and adversarial paraphrasing compared to curvature-based methods.",
            "success_factors": "Extremely low compute cost and reasonable performance in many settings; likelihood especially works well when scoring model aligns with source model or domain.",
            "comparative_results": "Likelihood often strong baseline and in some black-box experiments was the best simple baseline. Still, Fast-DetectGPT and DetectGPT generally outperform these baselines in AUROC when sufficient computation or curvature estimation is used.",
            "human_baseline": "Compared to supervised detectors and Fast-DetectGPT; statistical baselines sometimes approach supervised performance on particular datasets but are generally outperformed by best-curvature methods.",
            "uuid": "e2592.4",
            "source_info": {
                "paper_title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPTZero / RoBERTa detectors",
            "name_full": "GPTZero (commercial) and RoBERTa-based supervised detectors",
            "brief_description": "Supervised detectors trained to discriminate machine-generated from human text (RoBERTa-base/large trained by OpenAI and GPTZero commercial detector); used as supervised baselines in experiments.",
            "citation_title": "Gptzero: Towards detection of ai-generated text using zero-shot and supervised methods",
            "mention_or_use": "use",
            "system_name": "Supervised detectors (RoBERTa-base/large, GPTZero)",
            "system_description": "Supervised classifiers (RoBERTa fine-tuned, GPTZero proprietary) trained on labeled machine vs human text to predict AI authorship. They learn discriminative patterns from training data (bag-of-words or neural features) and are evaluated on held-out and out-of-distribution data to assess generalization.",
            "system_type": "Supervised Detection System",
            "problem_domain": "NLP — machine-generated text detection (supervised)",
            "problem_description": "Trained binary classifiers to detect AI-generated text across datasets; tested on ChatGPT, GPT-3, GPT-4 generations and human texts.",
            "problem_complexity": "Supervised learning on high-dimensional text representations; complexity depends on dataset diversity and label quality; prone to overfitting to training distribution and source models.",
            "data_availability": "Requires labeled datasets of human and machine-generated text for training; public GPT-2 output dataset referenced for detector training; authors use 150–500 test examples per dataset for evaluation.",
            "computational_requirements": "Training cost variable (fine-tuning RoBERTa large requires GPU resources); inference cost modest (single forward pass per sample).",
            "problem_structure": "Supervised classification: deterministic inference given model weights; performance hinges on training data representativeness and domain overlap.",
            "success_metric": "AUROC and accuracy in experiments.",
            "success_rate": "Varies by domain: e.g., RoBERTa-base AUROC on ChatGPT (avg across datasets) = 0.7474 and RoBERTa-large = 0.6906 (Table 3), while GPTZero achieved very high AUROC on ChatGPT XSum (0.9952) but lower on other domains (avg 0.9348 on ChatGPT). Supervised detectors performed poorly on some out-of-distribution datasets compared to zero-shot methods.",
            "failure_modes": "Overfitting to particular domains or source models; poor generalization to out-of-distribution domains and unseen generation models; inconsistent performance across datasets (e.g., GPTZero strong on news but weak on stories/technical writing).",
            "success_factors": "Large and representative labeled training sets improve supervised detector performance in-domain; feature-rich models (RoBERTa-large) may help if training data matches test distribution.",
            "comparative_results": "In many black-box, cross-domain settings Fast-DetectGPT outperforms supervised detectors; supervised models can beat zero-shot methods in matched conditions but degrade strongly under domain/model shift.",
            "human_baseline": "Supervised detectors represent human-engineered baseline models; direct human rater performance not provided in paper but prior work indicates human experts can be fooled and supervised detectors are not fully reliable cross-domain.",
            "uuid": "e2592.5",
            "source_info": {
                "paper_title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "rating": 2
        },
        {
            "paper_title": "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
            "rating": 2
        },
        {
            "paper_title": "dna-gpt: divergent n-gram analysis for training-free detection of gpt-generated text",
            "rating": 2
        },
        {
            "paper_title": "Gltr: Statistical detection and visualization of generated text",
            "rating": 1
        },
        {
            "paper_title": "Detecting fake content with relative entropy scoring",
            "rating": 1
        },
        {
            "paper_title": "Can ai-generated text be reliably detected?",
            "rating": 1
        }
    ],
    "cost": 0.020742249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>FAST-DETECTGPT: EFFICIENT ZERO-SHOT DETECTION OF MACHINE-GENERATED TEXT VIA CONDITIONAL PROBABILITY CURVATURE</h1>
<p>Guangsheng Bao<br>Zhejiang University<br>School of Engineering, Westlake University<br>baoguangsheng@westlake.edu.cn</p>
<p>Yanbin Zhao<br>School of Mathematics, Physics and Statistics, Shanghai Polytechnic University<br>zhaoyb553@nenu.edu.cn</p>
<h2>Zhiyang Teng</h2>
<p>Nanyang Technological University
zhiyang.teng@ntu.edu.sg</p>
<h2>Linyi Yang, Yue Zhang</h2>
<p>School of Engineering, Westlake University
Institute of Advanced Technology, Westlake Institute for Advanced Study
${$ yanglinyi, zhangyue}@westlake.edu.cn</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machinegenerated and human-authored content. The leading zero-shot detector, DetectGPT (Mitchell et al., 2023), showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present Fast-DetectGPT ${ }^{1}$, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around $75 \%$ in both the white-box and black-box settings but also accelerates the detection process by a factor of 340 , as detailed in Table 1.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">5-Model Generations $\uparrow$</th>
<th style="text-align: center;">ChatGPT/GPT-4 Generations $\uparrow$</th>
<th style="text-align: center;">Speedup $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DetectGPT</td>
<td style="text-align: center;">0.9554</td>
<td style="text-align: center;">0.7225</td>
<td style="text-align: center;">1 x</td>
</tr>
<tr>
<td style="text-align: left;">Fast-DetectGPT</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 8 7}$ <br> (relative $\uparrow \mathbf{7 4 . 7 \%}$ )</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 3 8}$ <br> (relative $\uparrow \mathbf{7 6 . 1 \%}$ )</td>
<td style="text-align: center;">$\mathbf{3 4 0 x}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Detection accuracy (measured in AUROC) and computational speedup for machinegenerated text detection. The white-box setting (directly using the source model) is applied to the methods detecting generations produced by five source models (5-model), whereas the blackbox setting (utilizing surrogate models) targets ChatGPT and GPT-4 generations. Results are averaged from data in Table 2 for the 5-model generations and Table 3 for ChatGPT/GPT-4, where the 'relative $\uparrow$ ' is calculated by $(\text { new }-o l d) /(1.0-\text { old })$, representing how much improvement has been made relative to the maximum possible improvement. Speedup assessments were conducted using the XSum news dataset, with computations on a Tesla A100 GPU.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Distribution of conditional probability curvatures of the original human-written passages and the machine-generated passages by four source models on 30-token prefix from XSum.</p>
<h1>1 INTRODUCTION</h1>
<p>Large language models (LLMs) like ChatGPT (OpenAI, 2022), PaLM (Chowdhery et al., 2022), and GPT-4 (OpenAI, 2023) have dramatically influenced both industrial and academic landscapes. These models have transformed productivity in diverse fields such as news reporting, story writing, and academic research (M Alshater, 2022; Yuan et al., 2022; Christian, 2023). However, their misuse also introduces concerns-especially regarding fake news (Ahmed et al., 2021), malicious product reviews (Adelani et al., 2020), and plagiarism (Lee et al., 2023). The sheer fluency and coherence of content generated by these models make it challenging, even for experts, to determine its human or machine origin (Ippolito et al., 2020; Shahid et al., 2022). Addressing this issue necessitates reliable machine-generated text detection methods (Kaur et al., 2022; Chen \&amp; Shu, 2023).</p>
<p>Existing detectors can be grouped into two main categories: supervised classifiers (Solaiman et al., 2019; Fagni et al., 2021; Mitrović et al., 2023) and zero-shot classifiers (Gehrmann et al., 2019; Mitchell et al., 2023; Su et al., 2023). While supervised classifiers excel within their specific training domains, they falter when confronted with text from diverse domains or unfamiliar models (Bakhtin et al., 2019; Uchendu et al., 2020; Pu et al., 2023). Zero-shot classifiers, using a pre-trained language model directly without finetuning, are immune to domain-specific degradation and are on par with supervised classifiers on detection accuracy. This stems from their need for "universal features" that can function across multiple domains and languages (Gehrmann et al., 2019; Mitchell et al., 2023).</p>
<p>A typical zero-shot classifier, DetectGPT (Mitchell et al., 2023), works under the assumption that machine-generated text variations typically have lower model probability than the original, while human-written ones could go either way. Despite its effectiveness, employing probability curvature demands the execution of around one hundred model calls or interactions with services such as the OpenAI API to create the perturbation texts, leading to prohibitive computational costs.</p>
<p>In this paper, we posit a new hypothesis for detecting machine-generated text. By viewing text generation as a sequential decision-making process on tokens, our core assertion is that humans and machines exhibit discernible differences in token choice given a context. More specifically, machines lean towards tokens with higher statistical probability due to their pre-training on large-scale human-written corpus, while humans individually exhibit no such bias because they craft sentences based on underlying meanings, intentions, and contexts rather than data statistics. As a consequence, the conditional probability function $p(\tilde{x} \mid x)$ reaches its maximum point at a machine-generated $x$ (evidenced by a positive curvature at that point). Our empirical observation supports this hypothesis across diverse datasets and models, as Figure 1 illustrates. Specifically, the conditional probability curvature of machine-generated texts typically hovers around 3, whereas human-generated texts exhibit curvatures close to 0 .</p>
<p>According to the above observation, we present Fast-DetectGPT, aiming to classify if a passage was produced by a particular source model, as outlined in Figure 2. In contrast to DetectGPT, our approach begins by sampling alternative word choices at each token (step 1). Subsequently, we assess the conditional probabilities of these generated samples (step 2) and combine them to arrive at a detection decision (step 3). Our empirical evaluation demonstrates the superior detection accuracy of Fast-DetectGPT over DetectGPT, showcasing a noteworthy relative boost of about 75\% in both white-box and black-box settings. Intriguingly, in the black-box setting, Fast-DetectGPT even trumps DetectGPT's white-box performance by an average of $28 \%$. Moreover, it aptly flags $80 \%$ of ChatGPT-crafted content, while only misidentifying $1 \%$ of human compositions.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Fast-DetectGPT v.s. DetectGPT (Mitchell et al., 2023). Fast-DetectGPT uses a conditional probability function $p(\tilde{x} \mid x)$ as defined in Eq. 2. Notably, Fast-DetectGPT invokes the sampling GPT once to generate all samples and similarly calls the scoring GPT once to evaluate all samples, while DetectGPT interacts with the perturbation model T5 to produce one perturbation per call, and summons the scoring model GPT for each perturbation assessment. The threshold $\epsilon$ should be chosen to balance the false and true positive rates in practice.</p>
<p>Our main contributions are threefold: 1) unveiling and validation a new hypothesis that human and machine select words differently given a context, 2) proposing conditional probability curvature as a new feature to detect machine-generated text, reducing the detection cost by two orders of magnitude, and 3) achieving the best average detection accuracy in both white-box and black-box settings compare to existing zero-shot text detectors.</p>
<h1>2 Method</h1>
<h3>2.1 Task and Settings</h3>
<p>Our objective is the zero-shot detection of machine-generated text, treating the challenge as a binary classification problem (detailed in Appendix A). Given a passage $x$, which may be human-authored or produced by a source model, the goal is to discern whether it is machine-generated.</p>
<p>In the white-box setting, we have the privilege of accessing the possible source model that a passage is either written by a human or generated by this source model. We use the source model to aid in scoring the candidate passage to inform the classification decision in the setting. Conversely, in the black-box setting, we operate without access to the source model. Instead, we rely on surrogate models to score the passage. Underpinning this approach is the assumption that language models, due to their training on vast human-authored corpora, inherently share characteristic features.</p>
<h3>2.2 DETECTGPT BASELINE</h3>
<p>Formally, given an input passage $x$ and the possible source model $p_{\theta}$, DetectGPT uses the source model for scoring (the white-box setting). Together with a predefined perturbation model $q_{\varphi}$, DetectGPT encapsulates the probability curvature as:</p>
<p>$$
\mathbf{d}\left(x, p_{\theta}, q_{\varphi}\right)=\log p_{\theta}(x)-\mathbb{E}<em _varphi="\varphi">{\tilde{x} \sim q</em>)\right]
$$}(\cdot \mid x)}\left[\log p_{\theta}(\tilde{x</p>
<p>where $\tilde{x}$ is a perturbation produced by the masked language model $q_{\varphi}(\cdot \mid x)$. When $x$ emerges from sampling from the source model $p_{\theta}, \mathbf{d}\left(x, p_{\theta}, q_{\varphi}\right)$ tends to be positive, while for passage $x$ written by human, $\mathbf{d}\left(x, p_{\theta}, q_{\varphi}\right)$ tends to be zero. $p_{\theta}$ is also called the scoring model in this method, which is used to score the log probabilities.</p>
<p>The Detection Process. To estimate the expectation $\mathbb{E}<em _varphi="\varphi">{\tilde{x} \sim q</em>)$, DetectGPT employs a sampling approach. Typically, it generates around a hundred variations of the input text $x$ and then computes the average of the log probabilities associated with these variations. The detection process}(\cdot \mid x)} \log p_{\theta}(\tilde{x</p>
<p>is summarized as Figure 2a, where DetectGPT advocates a three-step detection process, which include: 1) Perturb - generating slight rewrites of the original text using a pre-trained mask language model; 2) Score - evaluating the probability of the text and its rewrites using a pre-trained GPT language model; 3) Compare - estimating the probability curvature and making the final decision accordingly.</p>
<p>The Challenge. The probability function $p_{\theta}(\tilde{x})$ models $\tilde{x}$ in a Markov chain. Even if disparities between $\tilde{x}$ and $x$ are slight, amounting to changes in merely about $15 \%$ of the tokens, the entire Markov chain demands reevaluation for accurate probability estimation. This slight variation within the Markov chain mandates invoking the scoring model afresh for each variation, as the Score step in Figure 2a denotes. In this paper, we deviate from assessing the probability function across the entire Markov chain. Instead, we focus on evaluating the conditional probability function for each individual token, thereby eliminating the need for repetitive scoring.</p>
<h1>2.3 FAST-DETECTGPT</h1>
<p>Fast-DetectGPT operates on the premise that humans and machines tend to select different words during the text-generation process, with machines exhibiting a propensity for choosing words with higher model probabilities. The hypothesis is rooted in the fact that LLMs, pre-trained on the largescale corpus, mirror human collective writing behaviors instead of human individual writing behavior, resulting in a discrepancy in their word choices given a context.</p>
<p>The hypothesis is also substantiated to some extent by prior observations in the literature (Gehrmann et al., 2019; Hashimoto et al., 2019; Solaiman et al., 2019; Mitrović et al., 2023; Mitchell et al., 2023), which have indicated that machine-generated text typically boasts a higher average log probability (or lower perplexity) than human-written text. However, instead of solely relying on the assumption of a higher average log probability for machine-generated text, our approach posits the presence of a positive curvature within the conditional probability function specifically for machinegenerated text.</p>
<p>Given a passage $x$ and a model $p_{\theta}$, we define the conditional probability function as</p>
<p>$$
p_{\theta}(\tilde{x} \mid x)=\prod_{j} p_{\theta}\left(\tilde{x}<em _j="&lt;j">{j} \mid x</em>\right)
$$</p>
<p>where the tokens $\tilde{x}<em _theta="\theta">{j}$ are independently predicted given $x$. As a special case, $p</em>(x)$.
Specifically, we replace the probability function $p_{\theta}(\tilde{x})$ in DetectGPT with the conditional probability function $p_{\theta}(\tilde{x} \mid x)$. We estimate the curvature at the point $x$ by comparing the value of $p_{\theta}(x \mid x)$ with the values of alternative token choices $p_{\theta}(\tilde{x} \mid x)$. If $p_{\theta}(x \mid x)$ has a bigger value than $p_{\theta}(\tilde{x} \mid x)$, the function has a positive curvature at the point $x$, indicating that $x$ is more likely machine-generated. Otherwise, the function has a close-to-zero curvature at the point $x$, suggesting that $x$ is more likely human-written. We demonstrate the curvature distributions of human-written and machinegenerated texts in Figure 1, where we can see that human-written texts are concentrated around the zero curvature.}(x \mid x)$ equals to $p_{\theta</p>
<p>Formally, given an input passage $x$ and the possible source model $p_{\theta}$ (the white-box setting), we quantify the conditional probability curvature as</p>
<p>$$
\mathbf{d}\left(x, p_{\theta}, q_{\varphi}\right)=\frac{\log p_{\theta}(x \mid x)-\tilde{\mu}}{\tilde{\sigma}}
$$</p>
<p>where</p>
<p>$$
\tilde{\mu}=\mathbb{E}<em _varphi="\varphi">{\tilde{x} \sim q</em>}(\tilde{x} \mid x)}\left[\log p_{\theta}(\tilde{x} \mid x)\right] \quad \text { and } \quad \tilde{\sigma}^{2}=\mathbb{E<em _varphi="\varphi">{\tilde{x} \sim q</em>\right]
$$}(\tilde{x} \mid x)}\left[\left(\log p_{\theta}(\tilde{x} \mid x)-\tilde{\mu}\right)^{2</p>
<p>$\tilde{\mu}$ denotes the expected score of samples $\tilde{x}$ generated by the sampling model $q_{\varphi}(\cdot \mid x)$, and $\tilde{\sigma}^{2}$ the expected variance of the scores. We approximate $\tilde{\mu}$ using the average log probability of the random samples, and $\tilde{\sigma}^{2}$ using the mean of sample variances.
Conditional Independent Sampling. The independent sampling of alternative tokens is the key to the efficiency of Fast-DetectGPT. Specifically, we sample each token $\tilde{x}<em _varphi="\varphi">{j}$ from $q</em>}\left(\tilde{x<em _j="&lt;j">{j} \mid x</em>\right)$ given the fixed passage $x$ without depending on other sampled tokens. In practice, we can simply generate 10,000 samples (our default setting) by one line of PyTorch code: samples $=$</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Fast</span><span class="o">-</span><span class="n">DetectGPT</span><span class="w"> </span><span class="n">machine</span><span class="o">-</span><span class="n">generated</span><span class="w"> </span><span class="n">text</span><span class="w"> </span><span class="n">detection</span><span class="o">.</span>
<span class="w">    </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="n">passage</span><span class="w"> </span>\<span class="p">(</span><span class="n">x</span>\<span class="p">),</span><span class="w"> </span><span class="n">sampling</span><span class="w"> </span><span class="n">model</span><span class="w"> </span>\<span class="p">(</span><span class="n">q_</span><span class="p">{</span>\<span class="n">varphi</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">scoring</span><span class="w"> </span><span class="n">model</span><span class="w"> </span>\<span class="p">(</span><span class="n">p_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">decision</span><span class="w"> </span><span class="n">threshold</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">epsilon</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Output</span><span class="p">:</span><span class="w"> </span><span class="n">True</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">probably</span><span class="w"> </span><span class="n">machine</span><span class="o">-</span><span class="n">generated</span><span class="p">,</span><span class="w"> </span><span class="n">False</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">probably</span><span class="w"> </span><span class="n">human</span><span class="o">-</span><span class="n">written</span><span class="o">.</span>
<span class="w">    </span><span class="n">function</span><span class="w"> </span><span class="n">FastDetectGPT</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">q_</span><span class="p">{</span>\<span class="n">varphi</span><span class="p">},</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">tilde</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="n">sim</span><span class="w"> </span><span class="n">q_</span><span class="p">{</span>\<span class="n">varphi</span><span class="p">}(</span>\<span class="n">tilde</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span><span class="n">x</span><span class="p">),</span><span class="w"> </span><span class="n">i</span><span class="w"> </span>\<span class="ow">in</span><span class="p">[</span><span class="mi">1</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="n">N</span><span class="p">]</span>\<span class="p">)</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">tilde</span><span class="p">{</span>\<span class="n">mu</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="n">N</span><span class="p">}</span><span class="w"> </span>\<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="nb">log</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">tilde</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span><span class="n">x</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">tilde</span><span class="p">{</span>\<span class="n">sigma</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span>\<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="nb">log</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">tilde</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span><span class="n">x</span>\<span class="n">right</span><span class="p">)</span><span class="o">-</span>\<span class="n">tilde</span><span class="p">{</span>\<span class="n">mu</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">tilde</span><span class="p">{</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">d</span><span class="p">}}</span><span class="n">_</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="n">left</span><span class="p">(</span>\<span class="nb">log</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span>\<span class="n">tilde</span><span class="p">{</span>\<span class="n">mu</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span>\<span class="n">tilde</span><span class="p">{</span>\<span class="n">sigma</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">tilde</span><span class="p">{</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">c</span><span class="p">}}</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="n">triangleright</span>\<span class="p">)</span><span class="w"> </span><span class="n">Estimate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">mean</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">tilde</span><span class="p">{</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">d</span><span class="p">}}</span><span class="n">_</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="o">&gt;</span>\<span class="n">epsilon</span>\<span class="p">)</span>
</code></pre></div>

<p>torch.distributions.categorical.Categorical(logits=lprobs).sample([10000]), where the lprobs is the $\log$ probability distribution of $q_{\varphi}\left(\tilde{x}<em _j="&lt;j">{j} \mid x</em>\right)$ for $j$ from 0 to the length of $x$.</p>
<p>The sampling process plays a pivotal role in guiding us toward the solution. To discern whether a token within a given context is machine-generated or human-authored, it is essential to compare it against a range of alternative tokens in the same context. By sampling a substantial number of alternatives (say 10,000), we can effectively map out the distribution of their $\log p_{\theta}\left(\tilde{x}<em _j="&lt;j">{j} \mid x</em>\right)$ value of the passage token within this distribution provides a clear view of its relative position, enabling us to ascertain whether it is an outlier or a more typical selection. This fundamental insight forms the core rationale behind the development of Fast-DetectGPT.}\right)$ values. Placing the $\log p_{\theta}\left(x_{j} \mid x_{&lt;j</p>
<p>The Detection Process. As Figure 2b shows, Fast-DetectGPT proposes a new three-step detection process, including 1) Sample - we introduce a sampling model to generate alternative samples $\tilde{x}$ given the condition $x$, 2) Conditional Score - the conditional probability can be easily obtained by a single forward pass of the scoring model taking $x$ as the input. All the samples can be evaluated in the same predictive distribution, so we do not need multiple model calls, and 3) Compare conditional probabilities of the passage and samples are compared to calculate the curvature. More implementation details are described in Algorithm 1.</p>
<p>We find that the "Sample" and "Conditional Score" steps can be merged and have an analytical solution instead of sampling approximation, as described in Appendix B. Furthermore, when we use the same model for sampling and scoring, the conditional probability curvature has a close connection to the simple Likelihood and Entropy baselines as follows.</p>
<p>Connection to Likelihood and Entropy. Utilizing a singular model for both sampling and scoring enables the combination of these processes into a single step, necessitating only one model call. Given this, the conditional probability curvature can be succinctly expressed as</p>
<p>$$
\mathbf{d}\left(x, p_{\theta}\right)=\frac{\log p_{\theta}(x \mid x)-\tilde{\mu}}{\tilde{\sigma}}
$$</p>
<p>where $\tilde{\mu}=\mathbb{E}<em _theta="\theta">{\tilde{x} \sim p</em>}(\tilde{x} \mid x)}\left[\log p_{\theta}(\tilde{x} \mid x)\right]$ and $\tilde{\sigma}^{2}=\mathbb{E<em _theta="\theta">{\tilde{x} \sim p</em>\right]$.
Intriguingly, the curvature's numerator reveals itself to be the sum of the baseline methods: Likelihood $\left(\log p_{\theta}(x)\right)$ and Entropy $(-\tilde{\mu})$. While Likelihood and Entropy have been established as foundational baselines for zero-shot machine-generated text detection over the years (Lavergne et al., 2008; Gehrmann et al., 2019; Hashimoto et al., 2019; Mitchell et al., 2023), the discovery that their elementary combination can yield competitive detection accuracy was unforeseen.}(\tilde{x} \mid x)}\left[\left(\log p_{\theta}(\tilde{x} \mid x)-\tilde{\mu}\right)^{2</p>
<h1>3 EXPERIMENTS</h1>
<h3>3.1 SETTINGS</h3>
<p>Datasets. We follow DetectGPT using six datasets to cover various domains and languages, including XSum for news articles (Narayan et al., 2018), SQuAD for Wikipedia contexts (Rajpurkar et al., 2016), WritingPrompts for story writing (Fan et al., 2018), WMT16 English and German for different languages (Bojar et al., 2016), and PubMedQA for biomedical research question answering (Jin et al., 2019). We randomly sample 150 to 500 human-written examples per dataset as negative samples and produce equal numbers of positive samples by prompting the source model with the first 30 tokens of the human-written text (for PubMedQA, we only use the question as the prompt).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">GPT-2</th>
<th style="text-align: center;">OPT-2.7</th>
<th style="text-align: center;">Neo-2.7</th>
<th style="text-align: center;">GPT-J</th>
<th style="text-align: center;">NeoX</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">The White-Box Setting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Likelihood</td>
<td style="text-align: center;">0.9125</td>
<td style="text-align: center;">0.8963</td>
<td style="text-align: center;">0.8900</td>
<td style="text-align: center;">0.8480</td>
<td style="text-align: center;">0.7946</td>
<td style="text-align: center;">0.8683</td>
</tr>
<tr>
<td style="text-align: left;">Entropy</td>
<td style="text-align: center;">0.5174</td>
<td style="text-align: center;">0.4830</td>
<td style="text-align: center;">0.4898</td>
<td style="text-align: center;">0.5005</td>
<td style="text-align: center;">0.5333</td>
<td style="text-align: center;">0.5048</td>
</tr>
<tr>
<td style="text-align: left;">LogRank</td>
<td style="text-align: center;">0.9385</td>
<td style="text-align: center;">0.9223</td>
<td style="text-align: center;">0.9226</td>
<td style="text-align: center;">0.8818</td>
<td style="text-align: center;">0.8313</td>
<td style="text-align: center;">0.8993</td>
</tr>
<tr>
<td style="text-align: left;">LRR</td>
<td style="text-align: center;">0.9601</td>
<td style="text-align: center;">0.9401</td>
<td style="text-align: center;">0.9522</td>
<td style="text-align: center;">0.9179</td>
<td style="text-align: center;">0.8793</td>
<td style="text-align: center;">0.9299</td>
</tr>
<tr>
<td style="text-align: left;">DNA-GPT $\diamond$</td>
<td style="text-align: center;">0.9024</td>
<td style="text-align: center;">0.8797</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.8227</td>
<td style="text-align: center;">0.7826</td>
<td style="text-align: center;">0.8513</td>
</tr>
<tr>
<td style="text-align: left;">NPR $\diamond$</td>
<td style="text-align: center;">$0.9948 \dagger$</td>
<td style="text-align: center;">$0.9832 \dagger$</td>
<td style="text-align: center;">0.9883</td>
<td style="text-align: center;">0.9500</td>
<td style="text-align: center;">0.9065</td>
<td style="text-align: center;">0.9645</td>
</tr>
<tr>
<td style="text-align: left;">DetectGPT (T5-3B/*) $\diamond$</td>
<td style="text-align: center;">0.9917</td>
<td style="text-align: center;">0.9758</td>
<td style="text-align: center;">0.9797</td>
<td style="text-align: center;">0.9353</td>
<td style="text-align: center;">0.8943</td>
<td style="text-align: center;">0.9554</td>
</tr>
<tr>
<td style="text-align: left;">Fast-DetectGPT (<em>/</em>)</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 6 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 0 8}$</td>
<td style="text-align: center;">$0.9940 \dagger$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 6 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 7 5 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 8 7}$</td>
</tr>
<tr>
<td style="text-align: left;">(Relative $\dagger$ )</td>
<td style="text-align: center;">$60.2 \%$</td>
<td style="text-align: center;">$62.0 \%$</td>
<td style="text-align: center;">$70.4 \%$</td>
<td style="text-align: center;">$79.3 \%$</td>
<td style="text-align: center;">$76.7 \%$</td>
<td style="text-align: center;">$74.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">The Black-Box Setting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DetectGPT (T5-3B/Neo-2.7) $\diamond$</td>
<td style="text-align: center;">0.8517</td>
<td style="text-align: center;">0.8390</td>
<td style="text-align: center;">0.9797</td>
<td style="text-align: center;">0.8575</td>
<td style="text-align: center;">0.8400</td>
<td style="text-align: center;">0.8736</td>
</tr>
<tr>
<td style="text-align: left;">Fast-DetectGPT (GPT-J/Neo-2.7)</td>
<td style="text-align: center;">0.9834</td>
<td style="text-align: center;">0.9572</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 8 4}$</td>
<td style="text-align: center;">$0.9592 \dagger$</td>
<td style="text-align: center;">$0.9404 \dagger$</td>
<td style="text-align: center;">$0.9677 \dagger$</td>
</tr>
<tr>
<td style="text-align: left;">(Relative $\dagger$ )</td>
<td style="text-align: center;">$88.8 \%$</td>
<td style="text-align: center;">$73.4 \%$</td>
<td style="text-align: center;">$92.1 \%$</td>
<td style="text-align: center;">$71.4 \%$</td>
<td style="text-align: center;">$62.8 \%$</td>
<td style="text-align: center;">$74.5 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Zero-shot detection on passages from five source models, averaging AUROCs across XSum, SQuAD, and WritingPrompts from detailed Table 5 in Appendix D.1. Typically, the source model is employed for scoring, except that DetectGPT (T5-3B/Neo-2.7) and Fast-DetectGPT (GPT-J/Neo2.7) in a black-box setting utilize a surrogate Neo-2.7 model for scoring. While DetectGPT leverages T5-3B for perturbation generation, Fast-DetectGPT either resorts to the source model or a surrogate GPT-J for sample generation. $\dagger$ represents the second-best score. $\diamond$ indicates methods that invoke models multiple times, thereby significantly increasing computational demands.</p>
<p>We evaluate the methods on machine-generated text produced by different source models from 1.3B to 1,800B, described in Appendix C.1. We call most of the models locally except GPT-3, ChatGPT, and GPT-4 through OpenAI API.</p>
<p>Baselines. For zero-shot classifiers, we mainly compare Fast-DetectGPT with DetectGPT (Mitchell et al., 2023), as well as its enhanced variant, NPR (Su et al., 2023) and DNA-GPT (Yang et al., 2023). These represent the baseline methodologies we aim to expedite. Furthermore, we juxtapose Fast-DetectGPT with established zero-shot techniques, such as Likelihood (mean log probabilities), LogRank (average log of ranks in descending order by probabilities), Entropy (mean token entropy of the predictive distribution) (Gehrmann et al., 2019; Solaiman et al., 2019; Ippolito et al., 2020), and LRR (an amalgamation of log probability and log-rank) (Su et al., 2023). Regarding supervised classifiers, Fast-DetectGPT is benchmarked against existing supervised classifiers, including GPT-2 detectors based on RoBERTa-base/large (Liu et al., 2019) crafted by OpenAI ${ }^{2}$ and GPTZero (Tian $\&amp;$ Cui, 2023).</p>
<h1>3.2 MAIN RESULTS</h1>
<p>We generate 500 samples per dataset among XSum, SQuAD, and WritingPrompts for the following experiments, measuring the detection accuracy in AUROC (see Appendix A).</p>
<p>Inference Speedup. We compare the inference time (excluding the time for initializing the model) of Fast-DetectGPT and DetectGPT on a Tesla A100 GPU using XSum generations from the five models in Table 2. Despite DetectGPT's use of GPU batch processing, splitting 100 perturbations into 10 batches, it still requires substantial computational resources. It totals 79,113 seconds (approximately 22 hours) over five runs. In contrast, Fast-DetectGPT completes the task in only 233 seconds (about 4 minutes), achieving a remarkable speedup factor of approximately 340x, highlighting its significant performance improvement.</p>
<p>White-Box Zero-Shot Machine-Generated Text Detection. We evaluate zero-shot methods using each source model for scoring and typically Fast-DetectGPT using the source model also for sampling. The averaged scores are shown in Table 2 with more detailed results per dataset reported in Table 5 in Appendix D.1. Fast-DetectGPT achieves the best average AUROC on the three datasets, outperforming DetectGPT by a relative $74.7 \%$ and its enhanced variant, NPR, by $68.2 \%$. Notably, larger source models amplify this relative improvement, demonstrating the advantage of Fast-DetectGPT in detecting text produced by larger models.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">XSum</td>
<td style="text-align: center;">Writing</td>
<td style="text-align: center;">PubMed</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">XSum</td>
<td style="text-align: center;">Writing</td>
<td style="text-align: center;">PubMed</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-base</td>
<td style="text-align: center;">0.9150</td>
<td style="text-align: center;">0.7084</td>
<td style="text-align: center;">0.6188</td>
<td style="text-align: center;">0.7474</td>
<td style="text-align: center;">0.6778</td>
<td style="text-align: center;">0.5068</td>
<td style="text-align: center;">0.5309</td>
<td style="text-align: center;">0.5718</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-large</td>
<td style="text-align: center;">0.8507</td>
<td style="text-align: center;">0.5480</td>
<td style="text-align: center;">0.6731</td>
<td style="text-align: center;">0.6906</td>
<td style="text-align: center;">0.6879</td>
<td style="text-align: center;">0.3821</td>
<td style="text-align: center;">0.6067</td>
<td style="text-align: center;">0.5589</td>
</tr>
<tr>
<td style="text-align: left;">GPTZero</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 5 2}$</td>
<td style="text-align: center;">0.9292</td>
<td style="text-align: center;">0.8799</td>
<td style="text-align: center;">0.9348</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 1 5}$</td>
<td style="text-align: center;">0.8262</td>
<td style="text-align: center;">0.8482</td>
<td style="text-align: center;">0.8853</td>
</tr>
<tr>
<td style="text-align: left;">Likelihood (Neo-2.7)</td>
<td style="text-align: center;">0.9578</td>
<td style="text-align: center;">0.9740</td>
<td style="text-align: center;">0.8775</td>
<td style="text-align: center;">0.9364</td>
<td style="text-align: center;">0.7980</td>
<td style="text-align: center;">0.8553</td>
<td style="text-align: center;">0.8104</td>
<td style="text-align: center;">0.8212</td>
</tr>
<tr>
<td style="text-align: left;">Entropy (Neo-2.7)</td>
<td style="text-align: center;">0.3305</td>
<td style="text-align: center;">0.1902</td>
<td style="text-align: center;">0.2767</td>
<td style="text-align: center;">0.2658</td>
<td style="text-align: center;">0.4360</td>
<td style="text-align: center;">0.3702</td>
<td style="text-align: center;">0.3295</td>
<td style="text-align: center;">0.3786</td>
</tr>
<tr>
<td style="text-align: left;">LogRank(Neo-2.7)</td>
<td style="text-align: center;">0.9582</td>
<td style="text-align: center;">0.9656</td>
<td style="text-align: center;">0.8687</td>
<td style="text-align: center;">0.9308</td>
<td style="text-align: center;">0.7975</td>
<td style="text-align: center;">0.8286</td>
<td style="text-align: center;">0.8003</td>
<td style="text-align: center;">0.8088</td>
</tr>
<tr>
<td style="text-align: left;">LRR (Neo-2.7)</td>
<td style="text-align: center;">0.9162</td>
<td style="text-align: center;">0.8958</td>
<td style="text-align: center;">0.7433</td>
<td style="text-align: center;">0.8518</td>
<td style="text-align: center;">0.7447</td>
<td style="text-align: center;">0.7028</td>
<td style="text-align: center;">0.6814</td>
<td style="text-align: center;">0.7096</td>
</tr>
<tr>
<td style="text-align: left;">DNA-GPT (Neo-2.7)</td>
<td style="text-align: center;">0.9124</td>
<td style="text-align: center;">0.9425</td>
<td style="text-align: center;">0.7959</td>
<td style="text-align: center;">0.8836</td>
<td style="text-align: center;">0.7347</td>
<td style="text-align: center;">0.8032</td>
<td style="text-align: center;">0.7565</td>
<td style="text-align: center;">0.7648</td>
</tr>
<tr>
<td style="text-align: left;">NPR (T5-11B/Neo-2.7)</td>
<td style="text-align: center;">0.7899</td>
<td style="text-align: center;">0.8924</td>
<td style="text-align: center;">0.6784</td>
<td style="text-align: center;">0.7869</td>
<td style="text-align: center;">0.5280</td>
<td style="text-align: center;">0.6122</td>
<td style="text-align: center;">0.6328</td>
<td style="text-align: center;">0.5910</td>
</tr>
<tr>
<td style="text-align: left;">DetectGPT (T5-11B/Neo-2.7)</td>
<td style="text-align: center;">0.8416</td>
<td style="text-align: center;">0.8811</td>
<td style="text-align: center;">0.7444</td>
<td style="text-align: center;">0.8223</td>
<td style="text-align: center;">0.5660</td>
<td style="text-align: center;">0.6217</td>
<td style="text-align: center;">0.6805</td>
<td style="text-align: center;">0.6228</td>
</tr>
<tr>
<td style="text-align: left;">Fast-Detect (GPT-J/Neo-2.7)</td>
<td style="text-align: center;">0.9907</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 1 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 2 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 1 5}$</td>
<td style="text-align: center;">0.9067</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 1 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 5 0 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 6 1}$</td>
</tr>
<tr>
<td style="text-align: left;">(Relative $\uparrow$ )</td>
<td style="text-align: center;">$94.1 \%$</td>
<td style="text-align: center;">$92.9 \%$</td>
<td style="text-align: center;">$61.7 \%$</td>
<td style="text-align: center;">$78.3 \%$</td>
<td style="text-align: center;">$78.5 \%$</td>
<td style="text-align: center;">$89.7 \%$</td>
<td style="text-align: center;">$53.1 \%$</td>
<td style="text-align: center;">$75.1 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Detection of ChatGPT and GPT-4 generations. The black-box settings are used for all zero-shot methods, where the Likelihood provides the strongest baseline. A comparison of GPT-3 generation detection is provided in Appendix D.2, where we observe a similar hierarchy in accuracy.</p>
<p>Black-Box Zero-Shot Machine-Generated Text Detection. Table 2 further contrasts FastDetectGPT and DetectGPT in a black-box setting, employing a surrogate model, Neo-2.7 (empirically superior among GPT-2, Neo-2.7, and GPT-J) for scoring. Fast-DetectGPT (GPT-J/Neo-2.7) achieves a relative AUROC enhancement of $74.5 \%$ over DetectGPT (T5-3B/Neo-2.7) across the datasets. Specifically, the boost in Wikipedia context (SQuAD) averages at 0.1682 AUROC (detailed in Table 5 in Appendix D.1). Moreover, Fast-DetectGPT (GPT-J/Neo-2.7) outperforms DetectGPT (T5-3B/*) by relatively $27.6 \%$ on average. Such outcomes solidify Fast-DetectGPT's potential in the black-box setting as a versatile text detector across diverse domains and source models.</p>
<p>Ablation Study. We study the impact of the choice of the sampling model $q_{s^{\prime}}$ and the impact of the normalization $\hat{\sigma}$ in Eq. 3 on the detection accuracy. Experiments show that a properly selected sampling model can further enhance the performance of Fast-DetectGPT in the white-box setting by relatively about $27 \%$, while the normalization enhances the performance by $10 \%$. More details are described in Appendix E.</p>
<h1>3.3 ReSults in ReAL-WORld Scenarios</h1>
<p>We further assess Fast-DetectGPT in a black-box setting using passages generated by GPT-3, ChatGPT, and GPT-4 to simulate real-world scenarios. Using parameters and prompts delineated in Appendix C.2, we produced 150 samples per dataset and source model. As evidenced in Table 3, Fast-DetectGPT consistently exhibits superior detection proficiency. It surpasses DetectGPT by relative AUROC margins of $78.3 \%$ for ChatGPT and $75.1 \%$ for GPT-4. When compared to the supervised detectors RoBERTa-base/large and GPTZero, Fast-DetectGPT achieves overall higher accuracy. Collectively, these outcomes underscore the potential of Fast-DetectGPT working in realworld scenarios.</p>
<p>Interestingly, the commercial model GPTZero performs the best on news (XSum) but worse on stories (WritingPrompts) and technical writings (PubMedQA), indicating that the model may mainly be trained on news generations. The Likelihood detector emerges as the strongest baseline, which diverges from the results on open source models presented in Table 2, where Likelihood trails DetectGPT and NPR. A consistent trend is evident with GPT-3 generations (Appendix D.2). In comparison, Fast-DetectGPT offers more robust and consistent performance.</p>
<h3>3.4 UsAbility Analysis</h3>
<p>Interpretation of AUROC. In real-world scenarios, our concern extends beyond overall detection accuracy; we are particularly interested in the recall (the true positive rate) while minimizing the likelihood of making type-I errors (achieving a low false positive rate). As depicted in Figure 3, when applied to ChatGPT-generated content, Fast-DetectGPT achieves a recall of $87 \%$ for machinegenerated texts with only $1 \%$ misclassification of human-written text as machine-generated. When the tolerance for false positives increases to $10 \%$, the recall reaches $98 \%$. When applied to GPT-4</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: ROC curves in log scale evaluated on stories (WritingPrompts), where the dash lines denote the random classifier.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Detection accuracy (AUROC) on story passages (WritingPrompts) truncated to target number of words.
generations, the task becomes significantly more challenging but Fast-DetectGPT still achieves a recall of $89 \%$ on the condition of a false positive rate less than $10 \%$. These outcomes underscore the potential of Fast-DetectGPT in effectively detecting texts generated by state-of-the-art large language models.</p>
<p>Robustness on Different Passage Lengths. Zero-shot detectors are supposed to perform worse on shorter passages due to their statistical nature. We conduct evaluations by truncating the passages to various target lengths. As Figure 4 illustrates, the detectors show consistent trends on passages produced by ChatGPT, where the detection accuracy generally increases for longer passages. In contrast, on passages from GPT-4, the detectors show inconsistent trends. Specifically, the supervised detectors show a performance decline when the passage length increases, while DetectGPT experiences an increase at the beginning followed by an unexpected decrease when the passage length exceeds 90 words. We speculate the non-monotonic trends of the supervised detectors and DetectGPT are rooted in their perspective on handling the passages as a whole chain of tokens, which does not generalize to different lengths. In contrast, Fast-DetectGPT exhibits a consistent, monotonic increase in accuracy as passage length grows, indicating the robust performance of Fast-DetectGPT across passages of varying lengths.</p>
<p>Robustness across Domains and Languages. Detectors are expected to generalize to different domains and languages for higher usability. We compare Fast-DetectGPT against supervised detectors on both in-distribution and out-distribution datasets. Figure 5 reveals that Fast-DetectGPT achieves competitive detection accuracy on the in-distribution datasets XSum and WMT16-English. However, it significantly outperforms supervised detectors on the outdistribution datasets PubMedQA and WMT16German. Moreover, it is noteworthy that FastDetectGPT consistently outperforms DetectGPT across all four datasets. These results underscore the robustness of Fast-DetectGPT across diverse domains and languages.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Compare with supervised detectors, evaluated in AUROC. We generate 200 test samples for each dataset and source model.</p>
<p>Robustness against Decoding Strategies. Machine systems employ various decoding strategies, including top- $k$ sampling, top- $p$ (Nucleus) sampling, and temperature sampling with a constant $T$. Our experiments evaluate these strategies using the five models and three datasets mentioned in Table 2 by setting $k=40, p=0.96$, and $T=0.8$ for all cases. In the white-box setting, FastDetectGPT consistently emerged superior across all sampling strategies. It surpassed DetectGPT with relative margins of $95 \%$ in Top- $p$ sampling, $81 \%$ in Top- $k$ sampling, and a striking $99 \%$ in temperature sampling, as elaborated in Table 9 in Appendix F.1. Similar relative improvements are also achieved in the black-box setting. These results underscore the consistent performance of Fast-DetectGPT in detecting text generated through diverse decoding strategies.</p>
<p>Robustness under Paraphrasing Attack. We analyze the robustness under paraphrasing attack and propose decoherence attack, finding that Fast-DetectGPT consistently outperforms both zeroshot and trained detectors, as illustrated in Appendix F.2.</p>
<h1>4 DISCUSSION</h1>
<p>Fast-DetectGPT performs about $65 \%$ better in white-box settings than black-box ones. Industrial services could leverage the white-box setting to enhance the content authorship for proprietary LLMs like ChatGPT and GPT-4 by exposing conditional probability curvature in the service API, without significant extra cost on the computation of the feature.</p>
<p>From the research perspective, the black-box setting may have unforeseen potential. The best model for this setting remains unclear, and may depend on factors like model size, training corpus breadth, and training process convergence. These factors warrant further investigation to provide clarity and guidance in the development of black-box detection methods.</p>
<p>We further discuss the broader implications of Fast-DetectGPT in Appendix G, covering text authorship and watermarking.</p>
<p>Limitations and Future Work. Our initial research impetus centered on accelerating the detection process of DetectGPT. However, Fast-DetectGPT not only accelerates this process but also brings about notable enhancements in detection accuracy. In this paper, our focus is predominantly on these empirical advancements, with theoretical explorations earmarked for future endeavors.</p>
<p>Furthermore, Fast-DetectGPT's design leans on pre-trained models to span a multitude of domains and languages. This presents a challenge in a black-box setting, as no single model can seamlessly span all linguistic territories and domains. This is due to the intrinsic nature of pre-trained models being tailored to specific domains and languages.</p>
<h2>5 Related Work</h2>
<p>Current research primarily concentrates on supervised methods, involving the training of classifiers to distinguish between machine-generated and human-written text (Jawahar et al., 2020). These classifiers are typically trained using bag-of-words (Solaiman et al., 2019; Fagni et al., 2021) or neural representations (Bakhtin et al., 2019; Solaiman et al., 2019; Uchendu et al., 2020; Ippolito et al., 2020; Fagni et al., 2021; Yan et al., 2023; Pu et al., 2023; Mitrović et al., 2023; Li et al., 2023). It has been observed that these trained classifiers often exhibit overfitting tendencies, adapting too closely to the specific distribution of text domains and source models during training (Bakhtin et al., 2019; Uchendu et al., 2020), which consequently leads to limited generalization capabilities when exposed to out-of-distribution data (Pu et al., 2023). To address this challenge, our research focuses on zero-shot detection, aiming to identify "universal features" that can be applied to different domains, source models, and languages.</p>
<p>Existing zero-shot detectors primarily rely on statistical features, leveraging pre-trained large language models to gather them. These features encompass a range of measures, including relative entropy and perplexity (Lavergne et al., 2008), bag-of-words, average probability, and top-K buckets (Gehrmann et al., 2019), likelihood (Hashimoto et al., 2019; Solaiman et al., 2019), probability curvature (DetectGPT) (Mitchell et al., 2023), normalized log-rank perturbation (NPR) (Su et al., 2023), and divergence between multiple completions of a truncated passage (DNA-GPT) (Yang et al., 2023). Due to their statistical nature, zero-shot methods generally exhibit higher detection accuracy on longer passages (Lavergne et al., 2008). In this paper, we introduce a novel "conditional probability curvature" for machine-generated text detection. Differing from previous probability curvature or completion divergence approaches that require numerous model calls (variating from 10 to 100), our new feature only necessitates a single model forward pass, significantly expediting the detection process. Importantly, this new feature markedly enhances detection accuracy.</p>
<h2>6 CONCLUSION</h2>
<p>Our investigation reveals that conditional probability curvature on the token level serves as a more fundamental indicator of machine-generated texts, validating our proposed hypothesis concerning the distinction between machine and human text generation processes. Building upon this new hypothesis, our detector Fast-DetectGPT accelerates upon DetectGPT by two orders of magnitude. Experimental results further demonstrate that Fast-DetectGPT also significantly improves detection accuracy by approximately $75 \%$ in both white-box and black-box settings.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>We would like to thank the anonymous reviewers for their valuable feedback. This work is funded by the National Natural Science Foundation of China Key Program (Grant No. 62336006) and the Pioneer and "Leading Goose" R\&amp;D Program of Zhejiang (Grant No. 2022SDXHDX0003). Yanbin Zhao is supported by the National Natural Science Foundation of China (Grant No. 12201158).</p>
<h2>ETHICAL CONSIDERATIONS AND BROADER IMPACT</h2>
<p>Fast-DetectGPT, serving as a highly efficient detector for machine-generated text, holds promise in enhancing the integrity of AI systems by combating issues like fake news, disinformation, and academic plagiarism. However, akin to other methods reliant on Large Language Models (LLMs), it is susceptible to inheriting biases present in the training data. Notably, as emphasized by Liang et al. (2023), LLM-based detection systems may exhibit an elevated false-positive rate when confronted with text from non-native English speakers. Given the widespread and diverse utilization of such technologies, this presents a notable concern.</p>
<p>An immediate suggestion is to substitute the underlying LLMs in Fast-DetectGPT with alternative models trained on more varied and representative corpora. Additionally, we advocate for community involvement in the ongoing efforts to develop more inclusive LLMs, a development that would benefit not only Fast-DetectGPT but also similar systems at large.</p>
<h2>REFERENCES</h2>
<p>Sahar Abdelnabi and Mario Fritz. Adversarial watermarking transformer: Towards tracing text provenance with data hiding. In 2021 IEEE Symposium on Security and Privacy (SP), pp. 121140. IEEE, 2021.</p>
<p>David Ifeoluwa Adelani, Haotian Mai, Fuming Fang, Huy H Nguyen, Junichi Yamagishi, and Isao Echizen. Generating sentiment-preserving fake online reviews using neural language models and their human-and machine-based detection. In Advanced Information Networking and Applications: Proceedings of the 34th International Conference on Advanced Information Networking and Applications (AINA-2020), pp. 1341-1354. Springer, 2020.</p>
<p>Alim Al Ayub Ahmed, Ayman Aljabouh, Praveen Kumar Donepudi, and Myung Suh Choi. Detecting fake news using machine learning: A systematic literature review. arXiv preprint arXiv:2102.04458, 2021.</p>
<p>Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc'Aurelio Ranzato, and Arthur Szlam. Real or fake? learning to discriminate machine from human generated text. arXiv preprint arXiv:1906.03351, 2019.</p>
<p>Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021.</p>
<p>Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. In Proceedings of BigScience Episode# 5-Workshop on Challenges \&amp; Perspectives in Creating Large Language Models, pp. 95-136, 2022.</p>
<p>Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, et al. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pp. 131-198, 2016.</p>
<p>Elliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy Liang. Stanford CRFM Introduces PubMedGPT 2.7B. https://hai.stanford.edu/news/ stanford-crfm-introduces-pubmedgpt-27b, December 2022.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Canyu Chen and Kai Shu. Combating misinformation in the age of llms: Opportunities and challenges. arXiv preprint arXiv: 2311.05656, 2023.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Jon Christian. Cnet secretly used ai on articles that didn't disclose that fact, staff say. Futurusm, January, 2023.</p>
<p>Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi. Tweepfake: About detecting deepfake tweets. Plos one, 16(5):e0251415, 2021.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2018.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.</p>
<p>Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. Gltr: Statistical detection and visualization of generated text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 111-116, 2019.</p>
<p>Chenxi Gu, Chengsong Huang, Xiaoqing Zheng, Kai-Wei Chang, and Cho-Jui Hsieh. Watermarking pre-trained language models with backdooring. arXiv preprint arXiv:2210.07543, 2022.</p>
<p>Tatsunori B Hashimoto, Hugh Zhang, and Percy Liang. Unifying human and statistical evaluation for natural language generation. arXiv preprint arXiv:1904.02792, 2019.</p>
<p>Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 1808-1822, 2020.</p>
<p>Zunera Jalil and Anwar M Mirza. A review of digital watermarking techniques for text documents. In 2009 International Conference on Information and Multimedia Technology, pp. 230234. IEEE, 2009.</p>
<p>Ganesh Jawahar, Muhammad Abdul-Mageed, and VS Laks Lakshmanan. Automatic detection of machine generated text: A critical survey. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 2296-2309, 2020.</p>
<p>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2567-2577, 2019.</p>
<p>Nurul Shamimi Kamaruddin, Amirrudin Kamsin, Lip Yee Por, and Hameedur Rahman. A review of text watermarking: theory, methods, and applications. IEEE Access, 6:8011-8028, 2018.</p>
<p>Davinder Kaur, Suleyman Uslu, Kaley J Rittichier, and Arjan Durresi. Trustworthy artificial intelligence: a review. ACM Computing Surveys (CSUR), 55(2):1-38, 2022.</p>
<p>John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. arXiv preprint arXiv:2301.10226, 2023.</p>
<p>Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems, 35:31809-31826, 2022.</p>
<p>Thomas Lavergne, Tanguy Urvoy, and François Yvon. Detecting fake content with relative entropy scoring. In Proceedings of the 2008 International Conference on Uncovering Plagiarism, Authorship and Social Software Misuse-Volume 377, pp. 27-31, 2008.</p>
<p>Jooyoung Lee, Thai Le, Jinghui Chen, and Dongwon Lee. Do language models plagiarize? In Proceedings of the ACM Web Conference 2023, pp. 3637-3647, 2023.</p>
<p>Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang, Shuming Shi, and Yue Zhang. Deepfake text detection in the wild. arXiv preprint arXiv:2305.13242, 2023.</p>
<p>Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou. Gpt detectors are biased against non-native english writers. arXiv preprint arXiv:2304.02819, 2023.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Muneer M Alshater. Exploring the role of artificial intelligence in enhancing academic performance: A case study of chatgpt. Available at SSRN, 2022.</p>
<p>Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. arXiv preprint arXiv:2301.11305, 2023.</p>
<p>Sandra Mitrović, Davide Andreoletti, and Omran Ayoub. Chatgpt or human? detect and explain. explaining decisions of machine learning model for detecting short chatgpt-generated text. arXiv preprint arXiv:2301.13852, 2023.</p>
<p>Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1797-1807, 2018.</p>
<p>OpenAI. ChatGPT. https://chat.openai.com/, December 2022.
OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744, 2022.</p>
<p>Jiameng Pu, Zain Sarwar, Sifat Muhammad Abdullah, Abdullah Rehman, Yoonjin Kim, Parantapa Bhattacharya, Mobin Javed, and Bimal Viswanath. Deepfake text detection: Limitations and opportunities. In 2023 IEEE Symposium on Security and Privacy (SP), pp. 1613-1630. IEEE, 2023.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392, 2016.</p>
<p>Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? arXiv preprint arXiv:2303.11156, 2023.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.</p>
<p>Wajiha Shahid, Yiran Li, Dakota Staples, Gulshan Amin, Saqib Hakak, and Ali Ghorbani. Are you a cyborg, bot or human?-a survey on detecting fake news spreaders. IEEE Access, 10: $27069-27083,2022$.</p>
<p>Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina. mgpt: Few-shot learners go multilingual. arXiv preprint arXiv:2204.07580, 2022.</p>
<p>Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203, 2019.</p>
<p>Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text. arXiv preprint arXiv:2306.05540, 2023.</p>
<p>Edward Tian and Alexander Cui. Gptzero: Towards detection of ai-generated text using zero-shot and supervised methods, 2023. URL https://gptzero.me.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. Authorship attribution for neural text generation. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pp. 8384-8395, 2020.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Duanli Yan, Michael Fauss, Jiangang Hao, and Wenju Cui. Detection of ai-generated essays in writing assessment. Psychological Testing and Assessment Modeling, 65(2):125-144, 2023.</p>
<p>Xianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen. Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text. arXiv preprint arXiv:2305.17359, 2023.</p>
<p>Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. Wordcraft: story writing with large language models. In 27th International Conference on Intelligent User Interfaces, pp. 841-852, 2022.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<h1>A Zero-Shot Detection Task and Settings</h1>
<p>Our research centers on zero-shot detection of machine-generated text, under the premise that our model has not undergone training using any machine-generated text. This distinguishes our approach from conventional supervised methods, which commonly employ discriminative training strategies to acquire specific syntactic or semantic attributes customized for machine-generated text. In contrast, our zero-shot methodology capitalizes on the inherent capabilities of large language models to identify anomalies that function as markers of machine-generated content.</p>
<p>The White-box Setting. Conventional zero-shot methodologies often operate under the assumption that the source model responsible for generating machine-generated text is accessible. We refer to this context as the white-box setting, where the primary goal is to distinguish machine-generated texts produced by the source model from those generated by humans. In this white-box setting, our detection decisions are dependent on the source model, but it is not mandatory to possess detailed knowledge of the source model's architecture and parameters. For instance, within the white-box framework, a system like DetectGPT utilizes the OpenAI API to identify text generated by GPT-3, without requiring extensive knowledge of the inner workings of GPT-3.</p>
<p>The Black-box Setting. In real-world situations, there could be instances where we lack knowledge about the specific source models employed for content generation. This necessitates the development of a versatile detector capable of identifying texts generated by a variety of automated systems. We term this scenario the black-box setting, where the objective is to differentiate between machinegenerated texts produced by diverse, unidentified models and those composed by humans. In this context, the term "black box" signifies that we lack access to information about the source model or any details pertaining to it.</p>
<p>Evaluation Metric (AUROC). Instead of measuring the detection accuracy with a specific threshold ( $\epsilon$ in Figure 2), we measure the detection accuracy in the area under the receiver operating characteristic (AUROC), profiling the detectors on the whole spectrum of the thresholds. AUROC ranges from 0.0 to 1.0 , mathematically denoting the probability of a random machine-generated text having a higher predicted probability of being machine-generated than a random human-written text. Typically, an AUROC of 0.5 indicates a random classifier and an AUROC of 1.0 indicates a perfect classifier. We also report the relative improvement calculated by $(\text { new }-o l d) /(1.0-\text { old })$, which represents how much improvement has been made relative to the maximum possible improvement.</p>
<h2>B Analytical Solution of Conditional Probability Curvature</h2>
<p>The sample mean $\tilde{\mu}$ in Eq. 4 represents the cross-entropy of distribution $q_{\varphi}(\tilde{x} \mid x)$ and $p_{\theta}(\tilde{x} \mid x)$. By leveraging the conditional independence of each token, we can calculate the expectation analytically as</p>
<p>$$
\begin{aligned}
\tilde{\mu} &amp; =\mathbb{E}<em _varphi="\varphi">{\tilde{x} \sim q</em> \mid x) \
&amp; =\sum_{j} \sum_{\tilde{x}}(\tilde{x} \mid x)}\left[\log p_{\theta}(\tilde{x} \mid x)\right]=\sum_{\tilde{x}} q_{\varphi}(\tilde{x} \mid x) \log p_{\theta}(\tilde{x<em _varphi="\varphi">{j}} q</em>}\left(\tilde{x<em _j="&lt;j">{j} \mid x</em>}\right) \log p_{\theta}\left(\tilde{x<em _j="&lt;j">{j} \mid x</em>
\end{aligned}
$$}\right)=\sum_{j} \tilde{\mu}_{j</p>
<p>where $\tilde{\mu}<em j="j">{j}$ denotes the sample mean on the $j$-th token. The summation over $\tilde{x}</em>$ is computed by enumerating the tokens in the vocabulary, which can be exactly calculated.
The sample variance $\tilde{\sigma}^{2}$ in Eq. 4 can also be calculated analytically</p>
<p>$$
\begin{aligned}
\tilde{\sigma}^{2} &amp; =\mathbb{E}<em _varphi="\varphi">{\tilde{x} \sim q</em> \
&amp; =\sum_{j}\left(\sum_{\tilde{x}}(\tilde{x} \mid x)}\left[\left(\log p_{\theta}(\tilde{x} \mid x)-\tilde{\mu}\right)^{2}\right]=\sum_{\tilde{x}} q_{\varphi}(\tilde{x} \mid x)\left(\log p_{\theta}(\tilde{x} \mid x)-\tilde{\mu}\right)^{2<em _varphi="\varphi">{j}} q</em>}\left(\tilde{x<em _j="&lt;j">{j} \mid x</em>}\right) \log ^{2} p_{\theta}\left(\tilde{x<em _j="&lt;j">{j} \mid x</em>\right)
\end{aligned}
$$}\right)-\tilde{\mu}_{j}^{2</p>
<p>where the summation over $\tilde{x}_{j}$ can also be calculated exactly by enumerating the tokens in the vocabulary. Empirically, the analytical solution achieves a detection accuracy almost identical to the sampling approximation with 10,000 samples (our default setting) but further accelerates the detection process by about $10 \%$.</p>
<h1>C EXPERIMENTAL SETTINGS</h1>
<h2>C. 1 OPEN-SOURCE AND PROPRIETARY MODELS</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Model File/Service</th>
<th style="text-align: center;">Parameters</th>
<th style="text-align: center;">Training Corpus</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">mGPT (Shliazhko et al., 2022)</td>
<td style="text-align: center;">sberbank-ai/mGPT</td>
<td style="text-align: center;">1.3B</td>
<td style="text-align: center;">Wikipedia and Colossal Clean Crawled corpus.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2 (Radford et al., 2019)</td>
<td style="text-align: center;">gpt2-xl</td>
<td style="text-align: center;">1.5B</td>
<td style="text-align: center;">English WebText without Wikipedia.</td>
</tr>
<tr>
<td style="text-align: center;">PubMedGPT (Bolton et al., 2022)</td>
<td style="text-align: center;">stanford-crfm/pubmedgpt</td>
<td style="text-align: center;">2.7B</td>
<td style="text-align: center;">Biomedical abstracts and papers from the Pile.</td>
</tr>
<tr>
<td style="text-align: center;">OPT-2.7 (Zhang et al., 2022)</td>
<td style="text-align: center;">facebook/opt-2.7b</td>
<td style="text-align: center;">2.7B</td>
<td style="text-align: center;">A larger dataset including the Pile.</td>
</tr>
<tr>
<td style="text-align: center;">Neo-2.7 (Black et al., 2021)</td>
<td style="text-align: center;">EleutherAI/gpt-neo-2.7B</td>
<td style="text-align: center;">2.7B</td>
<td style="text-align: center;">The Pile (Gao et al., 2020).</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J (Wang \&amp; Komatsuzaki, 2021)</td>
<td style="text-align: center;">EleutherAI/gpt-j-6B</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">The Pile (Gao et al., 2020).</td>
</tr>
<tr>
<td style="text-align: center;">BLOOM-7.1 (Scao et al., 2022)</td>
<td style="text-align: center;">bigscience/bloom-7b1</td>
<td style="text-align: center;">7.1B</td>
<td style="text-align: center;">ROOTS corpus (Laurençon et al., 2022).</td>
</tr>
<tr>
<td style="text-align: center;">OPT-13 (Zhang et al., 2022)</td>
<td style="text-align: center;">facebook/opt-13b</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">A larger dataset including the Pile.</td>
</tr>
<tr>
<td style="text-align: center;">Llama-13 (Touvron et al., 2023a)</td>
<td style="text-align: center;">huggyllama/llama-13b</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">CommonCrawl, C4, Github, Wikipedia, Books, ...</td>
</tr>
<tr>
<td style="text-align: center;">Llama2-13 (Touvron et al., 2023b)</td>
<td style="text-align: center;">TheBloke/Llama-2-13B-fp16</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">A new mix of publicly available online data.</td>
</tr>
<tr>
<td style="text-align: center;">NeoX (Black et al., 2022)</td>
<td style="text-align: center;">EleutherAI/gpt-neox-20b</td>
<td style="text-align: center;">20B</td>
<td style="text-align: center;">The Pile (Gao et al., 2020).</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 (Brown et al., 2020)</td>
<td style="text-align: center;">OpenAI/davinci</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">CommonCrawl, WebText, English Wikipedia, ...</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (OpenAI, 2022)</td>
<td style="text-align: center;">OpenAI/gpt-3.5-turbo</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">CommonCrawl, WebText, English Wikipedia, ...</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (OpenAI, 2023)</td>
<td style="text-align: center;">OpenAI/gpt-4</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
</tbody>
</table>
<p>Table 4: Details of the source models that is used to produce machine-generated text.</p>
<p>We assess the performance of our methodologies using text generations sourced from various models, as outlined in Table 4. These models are arranged in order of their parameter count, with those having fewer than 20 billion parameters being run locally on a Tesla A100 GPU (80G). For models with over 6 billion parameters, we employ half-precision (float16), otherwise, we use full-precision (float32).</p>
<p>In the case of larger models like GPT-3, ChatGPT, and GPT-4, we utilize the OpenAI API for the evaluations. Additionally, we provide information about the training corpus associated with each model, which we believe is pertinent for understanding the detection accuracy of different sampling and scoring models when applied to text generations originating from diverse source models, domains, and languages.</p>
<h2>C. 2 SETTINGS FOR GPT-3, CHATGPT, AND GPT-4</h2>
<p>To compile our test set, we generate 150 samples for each dataset (among XSum, WritingPrompts, and PubMedQA) and each source model by calling the OpenAI service ${ }^{3}$. Specifically, for GPT-3, we requested text completions for a 30 -token prefix, while for ChatGPT and GPT-4, we request chat completions with predefined instructions as follows.</p>
<p>ChatGPT and GPT-4 function in a conversational manner, serving as assistants to execute user instructions. In the context of our experiment, we task these models with adopting the roles of professional News, Fiction, and Technical writers for the generation of News articles, stories, and answers, respectively. To encourage the production of content that is both unpredictable and creatively diverse, we utilize a temperature setting of 0.8 .</p>
<p>Specifically, we initiate the generation process by sending the following messages to the service.
Message for XSum:</p>
<div class="codehilite"><pre><span></span><code>{
    {&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a News writer.&#39;},
    {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Please write an article with about 150
        \hookrightarrow words starting exactly with: &lt;prefix&gt;&#39;},
</code></pre></div>

<p>The $&lt;$ prefix $&gt;$ could be like "Maj Richard Scott, 40, is accused of driving at speeds of up to 95 mph $(153 \mathrm{~km} / \mathrm{h})$ in bad weather", and the response is supposed to start with it.</p>
<h2>Message for WritingPrompts:</h2>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code>[
    {&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a Fiction writer.&#39;},
    {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Please write an article with about 150
        \hookrightarrow \text { words starting exactly with: &lt;prefix&gt;&#39;},
</code></pre></div>

<p>The $&lt;$ prefix $&gt;$ could be like "A man invents time travel in order to find a cure for his sick wife and succeeds, only to find out", and the response is supposed to start with it.</p>
<h1>Message for PubMedQA:</h1>
<p>1</p>
<div class="codehilite"><pre><span></span><code>    {&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;You are a Technical writer.&#39;},
    {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Please answer the question in about 50
        \hookrightarrow \text { words. &lt;prefix&gt;&#39;},
</code></pre></div>

<p>The $&lt;$ prefix $&gt;$ could be like "Question: Is an advance care planning model feasible in community palliative care? Answer:" and the response is supposed to answer the question directly.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Source Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">OPT-2.7</td>
<td style="text-align: center;">Neo-2.7</td>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">NeoX</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: center;">XSum</td>
<td style="text-align: center;">Likelihood</td>
<td style="text-align: center;">0.8638</td>
<td style="text-align: center;">0.8600</td>
<td style="text-align: center;">0.8609</td>
<td style="text-align: center;">0.8101</td>
<td style="text-align: center;">0.7604</td>
<td style="text-align: center;">0.8310</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">0.5835</td>
<td style="text-align: center;">0.5071</td>
<td style="text-align: center;">0.5712</td>
<td style="text-align: center;">0.5705</td>
<td style="text-align: center;">0.6035</td>
<td style="text-align: center;">0.5671</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LogRank</td>
<td style="text-align: center;">0.8918</td>
<td style="text-align: center;">0.8839</td>
<td style="text-align: center;">0.8949</td>
<td style="text-align: center;">0.8407</td>
<td style="text-align: center;">0.7939</td>
<td style="text-align: center;">0.8610</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LRR</td>
<td style="text-align: center;">0.9179</td>
<td style="text-align: center;">0.8867</td>
<td style="text-align: center;">0.9190</td>
<td style="text-align: center;">0.8592</td>
<td style="text-align: center;">0.8205</td>
<td style="text-align: center;">0.8807</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DNA-GPT $\diamond$</td>
<td style="text-align: center;">0.8548</td>
<td style="text-align: center;">0.8168</td>
<td style="text-align: center;">0.8197</td>
<td style="text-align: center;">0.7586</td>
<td style="text-align: center;">0.7167</td>
<td style="text-align: center;">0.7933</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NPR $\diamond$</td>
<td style="text-align: center;">$0.9891^{*}$</td>
<td style="text-align: center;">$0.9681^{*}$</td>
<td style="text-align: center;">$0.9929^{*}$</td>
<td style="text-align: center;">0.9566</td>
<td style="text-align: center;">0.9311</td>
<td style="text-align: center;">0.9676</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT $\diamond$</td>
<td style="text-align: center;">0.9875</td>
<td style="text-align: center;">0.9621</td>
<td style="text-align: center;">0.9914</td>
<td style="text-align: center;">$0.9632^{*}$</td>
<td style="text-align: center;">$0.9398^{*}$</td>
<td style="text-align: center;">$0.9688^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT</td>
<td style="text-align: center;">0.9930</td>
<td style="text-align: center;">0.9803</td>
<td style="text-align: center;">0.9885</td>
<td style="text-align: center;">0.9771</td>
<td style="text-align: center;">0.9703</td>
<td style="text-align: center;">0.9818</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.0055</td>
<td style="text-align: center;">0.0182</td>
<td style="text-align: center;">$-0.0029$</td>
<td style="text-align: center;">0.0139</td>
<td style="text-align: center;">0.0305</td>
<td style="text-align: center;">0.0130</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT (fixed) $\diamond$</td>
<td style="text-align: center;">0.9180</td>
<td style="text-align: center;">0.8868</td>
<td style="text-align: center;">0.9914</td>
<td style="text-align: center;">0.8830</td>
<td style="text-align: center;">0.8682</td>
<td style="text-align: center;">0.9095</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT (fixed)</td>
<td style="text-align: center;">0.9742</td>
<td style="text-align: center;">0.9444</td>
<td style="text-align: center;">0.9965</td>
<td style="text-align: center;">0.9335</td>
<td style="text-align: center;">0.9033</td>
<td style="text-align: center;">0.9504</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.0563</td>
<td style="text-align: center;">0.0576</td>
<td style="text-align: center;">0.0051</td>
<td style="text-align: center;">0.0505</td>
<td style="text-align: center;">0.0351</td>
<td style="text-align: center;">0.0409</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD</td>
<td style="text-align: center;">Likelihood</td>
<td style="text-align: center;">0.9077</td>
<td style="text-align: center;">0.8839</td>
<td style="text-align: center;">0.8585</td>
<td style="text-align: center;">0.7943</td>
<td style="text-align: center;">0.6977</td>
<td style="text-align: center;">0.8284</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">0.5791</td>
<td style="text-align: center;">0.5119</td>
<td style="text-align: center;">0.5581</td>
<td style="text-align: center;">0.5643</td>
<td style="text-align: center;">0.6056</td>
<td style="text-align: center;">0.5638</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LogRank</td>
<td style="text-align: center;">0.9454</td>
<td style="text-align: center;">0.9203</td>
<td style="text-align: center;">0.9054</td>
<td style="text-align: center;">0.8471</td>
<td style="text-align: center;">0.7545</td>
<td style="text-align: center;">0.8745</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LRR</td>
<td style="text-align: center;">0.9773</td>
<td style="text-align: center;">0.9597</td>
<td style="text-align: center;">0.9610</td>
<td style="text-align: center;">0.9244</td>
<td style="text-align: center;">0.8600</td>
<td style="text-align: center;">0.9365</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DNA-GPT $\diamond$</td>
<td style="text-align: center;">0.9094</td>
<td style="text-align: center;">0.8934</td>
<td style="text-align: center;">0.8589</td>
<td style="text-align: center;">0.8069</td>
<td style="text-align: center;">0.7525</td>
<td style="text-align: center;">0.8442</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NPR $\diamond$</td>
<td style="text-align: center;">$0.9965^{*}$</td>
<td style="text-align: center;">$0.9853^{*}$</td>
<td style="text-align: center;">0.9789</td>
<td style="text-align: center;">0.9108</td>
<td style="text-align: center;">0.8175</td>
<td style="text-align: center;">0.9378</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT $\diamond$</td>
<td style="text-align: center;">0.9914</td>
<td style="text-align: center;">0.9763</td>
<td style="text-align: center;">0.9625</td>
<td style="text-align: center;">0.8738</td>
<td style="text-align: center;">0.7916</td>
<td style="text-align: center;">0.9191</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT</td>
<td style="text-align: center;">0.9990</td>
<td style="text-align: center;">0.9949</td>
<td style="text-align: center;">$0.9956^{*}$</td>
<td style="text-align: center;">0.9853</td>
<td style="text-align: center;">0.9617</td>
<td style="text-align: center;">0.9873</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.0076</td>
<td style="text-align: center;">0.0186</td>
<td style="text-align: center;">0.0331</td>
<td style="text-align: center;">0.1116</td>
<td style="text-align: center;">0.1702</td>
<td style="text-align: center;">0.0682</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT (fixed) $\diamond$</td>
<td style="text-align: center;">0.7382</td>
<td style="text-align: center;">0.7530</td>
<td style="text-align: center;">0.9625</td>
<td style="text-align: center;">0.7882</td>
<td style="text-align: center;">0.7709</td>
<td style="text-align: center;">0.8026</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT (fixed)</td>
<td style="text-align: center;">0.9824</td>
<td style="text-align: center;">0.9762</td>
<td style="text-align: center;">0.9990</td>
<td style="text-align: center;">$0.9584^{*}$</td>
<td style="text-align: center;">$0.9379^{*}$</td>
<td style="text-align: center;">$0.9708^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.2442</td>
<td style="text-align: center;">0.2232</td>
<td style="text-align: center;">0.0365</td>
<td style="text-align: center;">0.1703</td>
<td style="text-align: center;">0.1669</td>
<td style="text-align: center;">0.1682</td>
</tr>
<tr>
<td style="text-align: center;">Writing Prompts</td>
<td style="text-align: center;">Likelihood</td>
<td style="text-align: center;">0.9661</td>
<td style="text-align: center;">0.9451</td>
<td style="text-align: center;">0.9505</td>
<td style="text-align: center;">0.9396</td>
<td style="text-align: center;">0.9256</td>
<td style="text-align: center;">0.9454</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">0.3895</td>
<td style="text-align: center;">0.4299</td>
<td style="text-align: center;">0.3400</td>
<td style="text-align: center;">0.3668</td>
<td style="text-align: center;">0.3908</td>
<td style="text-align: center;">0.3834</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LogRank</td>
<td style="text-align: center;">0.9782</td>
<td style="text-align: center;">0.9628</td>
<td style="text-align: center;">0.9675</td>
<td style="text-align: center;">0.9577</td>
<td style="text-align: center;">0.9454</td>
<td style="text-align: center;">0.9623</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LRR</td>
<td style="text-align: center;">0.9850</td>
<td style="text-align: center;">0.9740</td>
<td style="text-align: center;">0.9766</td>
<td style="text-align: center;">0.9702</td>
<td style="text-align: center;">0.9573</td>
<td style="text-align: center;">0.9726</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DNA-GPT $\diamond$</td>
<td style="text-align: center;">0.9431</td>
<td style="text-align: center;">0.9288</td>
<td style="text-align: center;">0.9283</td>
<td style="text-align: center;">0.9026</td>
<td style="text-align: center;">0.8786</td>
<td style="text-align: center;">0.9163</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NPR $\diamond$</td>
<td style="text-align: center;">0.9987</td>
<td style="text-align: center;">$0.9962^{*}$</td>
<td style="text-align: center;">0.9930</td>
<td style="text-align: center;">0.9825</td>
<td style="text-align: center;">0.9708</td>
<td style="text-align: center;">0.9882</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT $\diamond$</td>
<td style="text-align: center;">0.9962</td>
<td style="text-align: center;">0.9891</td>
<td style="text-align: center;">0.9852</td>
<td style="text-align: center;">0.9688</td>
<td style="text-align: center;">0.9516</td>
<td style="text-align: center;">0.9782</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT</td>
<td style="text-align: center;">$0.9982^{*}$</td>
<td style="text-align: center;">0.9972</td>
<td style="text-align: center;">$0.9980^{*}$</td>
<td style="text-align: center;">0.9974</td>
<td style="text-align: center;">0.9941</td>
<td style="text-align: center;">0.9970</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.0020</td>
<td style="text-align: center;">0.0081</td>
<td style="text-align: center;">0.0129</td>
<td style="text-align: center;">0.0285</td>
<td style="text-align: center;">0.0424</td>
<td style="text-align: center;">0.0188</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT (fixed) $\diamond$</td>
<td style="text-align: center;">0.8989</td>
<td style="text-align: center;">0.8772</td>
<td style="text-align: center;">0.9852</td>
<td style="text-align: center;">0.9014</td>
<td style="text-align: center;">0.8809</td>
<td style="text-align: center;">0.9087</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT (fixed)</td>
<td style="text-align: center;">0.9937</td>
<td style="text-align: center;">0.9509</td>
<td style="text-align: center;">0.9996</td>
<td style="text-align: center;">$0.9858^{*}$</td>
<td style="text-align: center;">$0.9801^{*}$</td>
<td style="text-align: center;">$0.9820^{*}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.0948</td>
<td style="text-align: center;">0.0738</td>
<td style="text-align: center;">0.0145</td>
<td style="text-align: center;">0.0844</td>
<td style="text-align: center;">0.0992</td>
<td style="text-align: center;">0.0733</td>
</tr>
</tbody>
</table>
<p>Table 5: Details of the main results in Table 2: Zero-shot detection on five source models, where Fast-DetectGPT consistently outperforms DetectGPT in terms of detection accuracy (in AUROC). We run DetectGPT and NPR with default 100 perturbations and run DNA-GPT with a truncate-ratio of 0.5 and 10 prefix completions per passage. Methods marked with "(fixed)" use the fixed models to detect texts from different sources (the black-box setting), where DetectGPT uses T5-3B/Neo-2.7 as the perturbation/scoring models and Fast-DetectGPT uses GPT-J/Neo-2.7 as the sampling/scoring models. The " $(D i f f)$ " rows indicate the AUROC improvement upon DetectGPT baselines. "*" denotes the second-best AUROC. $\diamond-$ Methods call models a hundred times, thus consuming much higher computational resources.</p>
<h1>D Additional Results</h1>
<h2>D. 1 Zero-Shot Detection on Additional Open-Source Models</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Source Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLOOM-7.1</td>
<td style="text-align: center;">OPT-13</td>
<td style="text-align: center;">Llama-13</td>
<td style="text-align: center;">Llama2-13</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: center;">XSum</td>
<td style="text-align: center;">Likelihood</td>
<td style="text-align: center;">0.8500</td>
<td style="text-align: center;">0.8105</td>
<td style="text-align: center;">0.6121</td>
<td style="text-align: center;">0.6550</td>
<td style="text-align: center;">0.7319</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">0.6642</td>
<td style="text-align: center;">0.5251</td>
<td style="text-align: center;">0.6731</td>
<td style="text-align: center;">0.6029</td>
<td style="text-align: center;">0.6163</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LogRank</td>
<td style="text-align: center;">0.9018</td>
<td style="text-align: center;">0.8369</td>
<td style="text-align: center;">0.6542</td>
<td style="text-align: center;">0.6911</td>
<td style="text-align: center;">0.7710</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LRR</td>
<td style="text-align: center;">0.9412</td>
<td style="text-align: center;">0.8344</td>
<td style="text-align: center;">0.7327</td>
<td style="text-align: center;">0.7351</td>
<td style="text-align: center;">0.8109</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NPR</td>
<td style="text-align: center;">0.9931</td>
<td style="text-align: center;">0.9283*</td>
<td style="text-align: center;">0.8031</td>
<td style="text-align: center;">0.8212*</td>
<td style="text-align: center;">0.8864*</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT</td>
<td style="text-align: center;">0.9912*</td>
<td style="text-align: center;">0.9268</td>
<td style="text-align: center;">0.8147*</td>
<td style="text-align: center;">0.8106</td>
<td style="text-align: center;">0.8858</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT</td>
<td style="text-align: center;">0.9890</td>
<td style="text-align: center;">0.9721</td>
<td style="text-align: center;">0.9473</td>
<td style="text-align: center;">0.9346</td>
<td style="text-align: center;">0.9607</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">-0.0021</td>
<td style="text-align: center;">0.0452</td>
<td style="text-align: center;">0.1325</td>
<td style="text-align: center;">0.1240</td>
<td style="text-align: center;">0.0749</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT (fixed)</td>
<td style="text-align: center;">0.7365</td>
<td style="text-align: center;">0.8713</td>
<td style="text-align: center;">0.6869</td>
<td style="text-align: center;">0.6848</td>
<td style="text-align: center;">0.7449</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT (fixed)</td>
<td style="text-align: center;">0.8886</td>
<td style="text-align: center;">0.9207</td>
<td style="text-align: center;">0.7306</td>
<td style="text-align: center;">0.6968</td>
<td style="text-align: center;">0.8092</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.1521</td>
<td style="text-align: center;">0.0495</td>
<td style="text-align: center;">0.0436</td>
<td style="text-align: center;">0.0121</td>
<td style="text-align: center;">0.0643</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD</td>
<td style="text-align: center;">Likelihood</td>
<td style="text-align: center;">0.8619</td>
<td style="text-align: center;">0.8220</td>
<td style="text-align: center;">0.5113</td>
<td style="text-align: center;">0.5000</td>
<td style="text-align: center;">0.6738</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">0.6199</td>
<td style="text-align: center;">0.5517</td>
<td style="text-align: center;">0.6534</td>
<td style="text-align: center;">0.6636</td>
<td style="text-align: center;">0.6221</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LogRank</td>
<td style="text-align: center;">0.9157</td>
<td style="text-align: center;">0.8645</td>
<td style="text-align: center;">0.5589</td>
<td style="text-align: center;">0.5457</td>
<td style="text-align: center;">0.7212</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LRR</td>
<td style="text-align: center;">0.9678</td>
<td style="text-align: center;">0.9228</td>
<td style="text-align: center;">0.7262</td>
<td style="text-align: center;">0.7036</td>
<td style="text-align: center;">0.8301</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NPR</td>
<td style="text-align: center;">0.9730*</td>
<td style="text-align: center;">0.9351</td>
<td style="text-align: center;">0.5332</td>
<td style="text-align: center;">0.5448</td>
<td style="text-align: center;">0.7465</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT</td>
<td style="text-align: center;">0.9510</td>
<td style="text-align: center;">0.9110</td>
<td style="text-align: center;">0.5204</td>
<td style="text-align: center;">0.5507</td>
<td style="text-align: center;">0.7333</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT</td>
<td style="text-align: center;">0.9953</td>
<td style="text-align: center;">0.9893</td>
<td style="text-align: center;">0.8717</td>
<td style="text-align: center;">0.8772</td>
<td style="text-align: center;">0.9333</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.0443</td>
<td style="text-align: center;">0.0782</td>
<td style="text-align: center;">0.3513</td>
<td style="text-align: center;">0.3265</td>
<td style="text-align: center;">0.2001</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT (fixed)</td>
<td style="text-align: center;">0.6359</td>
<td style="text-align: center;">0.7596</td>
<td style="text-align: center;">0.5588</td>
<td style="text-align: center;">0.5488</td>
<td style="text-align: center;">0.6258</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT (fixed)</td>
<td style="text-align: center;">0.9588</td>
<td style="text-align: center;">0.9590*</td>
<td style="text-align: center;">0.8028*</td>
<td style="text-align: center;">0.7627*</td>
<td style="text-align: center;">0.8708*</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.3228</td>
<td style="text-align: center;">0.1994</td>
<td style="text-align: center;">0.2440</td>
<td style="text-align: center;">0.2138</td>
<td style="text-align: center;">0.2450</td>
</tr>
<tr>
<td style="text-align: center;">WritingPrompts</td>
<td style="text-align: center;">Likelihood</td>
<td style="text-align: center;">0.9368</td>
<td style="text-align: center;">0.9400</td>
<td style="text-align: center;">0.8692</td>
<td style="text-align: center;">0.8737</td>
<td style="text-align: center;">0.9049</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">0.4876</td>
<td style="text-align: center;">0.4096</td>
<td style="text-align: center;">0.4831</td>
<td style="text-align: center;">0.4702</td>
<td style="text-align: center;">0.4626</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LogRank</td>
<td style="text-align: center;">0.9612</td>
<td style="text-align: center;">0.9578</td>
<td style="text-align: center;">0.9047</td>
<td style="text-align: center;">0.9069</td>
<td style="text-align: center;">0.9327</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LRR</td>
<td style="text-align: center;">0.9811</td>
<td style="text-align: center;">0.9650</td>
<td style="text-align: center;">0.9326*</td>
<td style="text-align: center;">0.9306*</td>
<td style="text-align: center;">0.9523*</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NPR</td>
<td style="text-align: center;">0.9909*</td>
<td style="text-align: center;">0.9850*</td>
<td style="text-align: center;">0.9184</td>
<td style="text-align: center;">0.9003</td>
<td style="text-align: center;">0.9487</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT</td>
<td style="text-align: center;">0.9829</td>
<td style="text-align: center;">0.9701</td>
<td style="text-align: center;">0.8596</td>
<td style="text-align: center;">0.8396</td>
<td style="text-align: center;">0.9130</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT</td>
<td style="text-align: center;">0.9983</td>
<td style="text-align: center;">0.9953</td>
<td style="text-align: center;">0.9892</td>
<td style="text-align: center;">0.9939</td>
<td style="text-align: center;">0.9942</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.0154</td>
<td style="text-align: center;">0.0253</td>
<td style="text-align: center;">0.1296</td>
<td style="text-align: center;">0.1543</td>
<td style="text-align: center;">0.0811</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DetectGPT (fixed)</td>
<td style="text-align: center;">0.7933</td>
<td style="text-align: center;">0.8695</td>
<td style="text-align: center;">0.7455</td>
<td style="text-align: center;">0.7532</td>
<td style="text-align: center;">0.7904</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fast-DetectGPT (fixed)</td>
<td style="text-align: center;">0.9779</td>
<td style="text-align: center;">0.9367</td>
<td style="text-align: center;">0.8925</td>
<td style="text-align: center;">0.9085</td>
<td style="text-align: center;">0.9289</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.1846</td>
<td style="text-align: center;">0.0672</td>
<td style="text-align: center;">0.1471</td>
<td style="text-align: center;">0.1553</td>
<td style="text-align: center;">0.1385</td>
</tr>
</tbody>
</table>
<p>Table 6: Addition to the main results in Table 5: Zero-shot detection on more source models.</p>
<p>We extend our evaluation to include several popular open-source LLMs, including BLOOM 7.1B, OPT 13B, Llama 13B, and Llama2 13B. In the white-box setting, Fast-DetectGPT exhibits an average relative improvement of $76.1 \%$ when compared to DetectGPT. This outcome aligns with the $74.7 \%$ average relative improvement observed across the five models presented in Table 5, underscoring the consistent performance of Fast-DetectGPT across diverse source models.</p>
<p>However, in the black-box setting, Fast-DetectGPT demonstrates an average relative improvement of $53.4 \%$ compared to DetectGPT. This figure is lower than the $74.5 \%$ average relative improvement seen across the five models in the main table. We suspect that the reduced improvement observed in these source models relate to the potential mismatch between the scoring model Neo-2.7 and the source models. It is conceivable that the training corpus used by the former may have limited overlap with the training corpus utilized by the latter according to Table 4. These findings underscore the challenges associated with identifying universally effective scoring models in the black-box setting.</p>
<p>D. 2 Zero-Shot Detection on GPT-3 Generations</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">XSum</th>
<th style="text-align: center;">WritingPrompts</th>
<th style="text-align: center;">PubMedQA</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa-base</td>
<td style="text-align: center;">0.8986</td>
<td style="text-align: center;">0.9282</td>
<td style="text-align: center;">0.6370</td>
<td style="text-align: center;">0.8212</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-large</td>
<td style="text-align: center;">0.9325</td>
<td style="text-align: center;">0.9113</td>
<td style="text-align: center;">0.6894</td>
<td style="text-align: center;">0.8444</td>
</tr>
<tr>
<td style="text-align: left;">GPTZero</td>
<td style="text-align: center;">0.4860</td>
<td style="text-align: center;">0.6009</td>
<td style="text-align: center;">0.4246</td>
<td style="text-align: center;">0.5038</td>
</tr>
<tr>
<td style="text-align: left;">Likelihood (GPT-3) $\diamond$</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: left;">DetectGPT (T5-11B/GPT-3) $\diamond$</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">$\mathbf{0 . 8 4}$</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: left;">Likelihood (Neo-2.7)</td>
<td style="text-align: center;">0.8307</td>
<td style="text-align: center;">0.8496</td>
<td style="text-align: center;">0.5668</td>
<td style="text-align: center;">0.7490</td>
</tr>
<tr>
<td style="text-align: left;">Entropy (Neo-2.7)</td>
<td style="text-align: center;">0.3923</td>
<td style="text-align: center;">0.3049</td>
<td style="text-align: center;">0.5358</td>
<td style="text-align: center;">0.4110</td>
</tr>
<tr>
<td style="text-align: left;">LogRank(Neo-2.7)</td>
<td style="text-align: center;">0.8096</td>
<td style="text-align: center;">0.8320</td>
<td style="text-align: center;">0.5527</td>
<td style="text-align: center;">0.7314</td>
</tr>
<tr>
<td style="text-align: left;">LRR (Neo-2.7)</td>
<td style="text-align: center;">0.6687</td>
<td style="text-align: center;">0.7410</td>
<td style="text-align: center;">0.4917</td>
<td style="text-align: center;">0.6338</td>
</tr>
<tr>
<td style="text-align: left;">DNA-GPT (Neo-2.7)</td>
<td style="text-align: center;">0.8209</td>
<td style="text-align: center;">0.8354</td>
<td style="text-align: center;">0.5761</td>
<td style="text-align: center;">0.7441</td>
</tr>
<tr>
<td style="text-align: left;">NPR (T5-11B/Neo-2.7)</td>
<td style="text-align: center;">0.8032</td>
<td style="text-align: center;">0.7847</td>
<td style="text-align: center;">0.6342</td>
<td style="text-align: center;">0.7407</td>
</tr>
<tr>
<td style="text-align: left;">DetectGPT (T5-11B/GPT-2)</td>
<td style="text-align: center;">0.8043</td>
<td style="text-align: center;">0.7899</td>
<td style="text-align: center;">0.6915</td>
<td style="text-align: center;">0.7552</td>
</tr>
<tr>
<td style="text-align: left;">DetectGPT (T5-11B/Neo-2.7)</td>
<td style="text-align: center;">0.8455</td>
<td style="text-align: center;">0.7818</td>
<td style="text-align: center;">0.6977</td>
<td style="text-align: center;">0.7750</td>
</tr>
<tr>
<td style="text-align: left;">DetectGPT (T5-11B/GPT-J)</td>
<td style="text-align: center;">0.8261</td>
<td style="text-align: center;">0.7666</td>
<td style="text-align: center;">0.6644</td>
<td style="text-align: center;">0.7524</td>
</tr>
<tr>
<td style="text-align: left;">Fast-DetectGPT (GPT-J/GPT-2)</td>
<td style="text-align: center;">0.9137</td>
<td style="text-align: center;">0.9533*</td>
<td style="text-align: center;">0.7589*</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 5 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Fast-DetectGPT (GPT-J/Neo-2.7)</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 9 6}$</td>
<td style="text-align: center;">0.9492</td>
<td style="text-align: center;">0.7225</td>
<td style="text-align: center;">$0.8704^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">Fast-DetectGPT (GPT-J/GPT-J)</td>
<td style="text-align: center;">$0.9329^{*}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 5 6 8}$</td>
<td style="text-align: center;">0.6664</td>
<td style="text-align: center;">0.8520</td>
</tr>
</tbody>
</table>
<p>Table 7: Detection of GPT-3 generations, evaluated in AUROC. Fast-DetectGPT in the black-box settings (using local models) significantly outperforms DetectGPT in both the black-box setting and the white-box setting (using GPT-3) on News (XSum) and story (WritingPrompts). Fast-DetectGPT uses 6B GPT-J to generate samples and models from 1.5B GPT-2 to 6B GPT-J to score samples, while DetectGPT uses 11B T5 to generate perturbations and models from 1.5B GPT-2 to 6B GPT-J, and GPT-3 service to score them. $\diamond$ - we report the official scores from Mitchell et al. (2023) instead of rerunning the experiments after confirming the consistency on RoBERTa-base/large.</p>
<p>Table 7 presents a comparison between Fast-DetectGPT, zero-shot DetectGPT, and supervised RoBERTa-based classifiers for the detection of GPT-3 generations. In contrast to DetectGPT, which employs the OpenAI API to assess perturbations, we utilize delegate models (specifically, GPT-2, Neo-2.7, and GPT-J) to identify GPT-3 generations.
Fast-DetectGPT outperforms supervised RoBERTa-base, RoBERTa-large, and GPTZero classifiers, achieving higher detection accuracy across the three datasets. On average, it improves the AUROC by 0.0310 AUROC (a relative increase of 20\%). Conversely, DetectGPT in the white-box setting (using T5-11B/GPT-3) achieves superior detection accuracy on PubMedQA but lags behind on XSum and WritingPrompt compared to RoBERTa-large. In the black-box setting (T5-11B/Neo-2.7), DetectGPT experiences a significant reduction in detection accuracy, averaging 0.0750 AUROC less. These findings emphasize that Fast-DetectGPT, operating in the black-box setting, offers a competitive alternative to supervised detectors and DetectGPT in the white-box setting.</p>
<p>When comparing the results on GPT-3 and ChatGPT, it becomes apparent that Fast-DetectGPT performs significantly better on ChatGPT. We speculate that this discrepancy may relate to factors such as instruction-tuning (Wei et al., 2021) and human-feedback reinforcement learning (HFRL) (Ouyang et al., 2022), which are employed in fine-tuning large language models and may skew the model toward high-probability tokens.</p>
<h1>E Ablation Study</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">XSum</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SQuAD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WritingPrompts</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Neo-2.7</td>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Neo-2.7</td>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Neo-2.7</td>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Fast-DetectGPT ( ${ }^{<em>}\left({ }^{</em>}\right)$</td>
<td style="text-align: center;">0.9930</td>
<td style="text-align: center;">0.9885</td>
<td style="text-align: center;">0.9771</td>
<td style="text-align: center;">0.9990</td>
<td style="text-align: center;">0.9956</td>
<td style="text-align: center;">0.9853</td>
<td style="text-align: center;">0.9982</td>
<td style="text-align: center;">0.9980</td>
<td style="text-align: center;">0.9974</td>
<td style="text-align: center;">0.9925</td>
</tr>
<tr>
<td style="text-align: center;">Fast-DetectGPT (GPT-2/*)</td>
<td style="text-align: center;">0.9930</td>
<td style="text-align: center;">0.9918</td>
<td style="text-align: center;">0.9534</td>
<td style="text-align: center;">0.9990</td>
<td style="text-align: center;">0.9728</td>
<td style="text-align: center;">0.8785</td>
<td style="text-align: center;">0.9982</td>
<td style="text-align: center;">0.9954</td>
<td style="text-align: center;">0.9868</td>
<td style="text-align: center;">0.9743</td>
</tr>
<tr>
<td style="text-align: center;">Fast-DetectGPT (Neo-2.7/*)</td>
<td style="text-align: center;">0.9778</td>
<td style="text-align: center;">0.9885</td>
<td style="text-align: center;">0.9153</td>
<td style="text-align: center;">0.9977</td>
<td style="text-align: center;">0.9956</td>
<td style="text-align: center;">0.9212</td>
<td style="text-align: center;">0.9994</td>
<td style="text-align: center;">0.9980</td>
<td style="text-align: center;">0.9861</td>
<td style="text-align: center;">0.9755</td>
</tr>
<tr>
<td style="text-align: center;">Fast-DetectGPT (GPT-J/*)</td>
<td style="text-align: center;">0.9960</td>
<td style="text-align: center;">0.9965</td>
<td style="text-align: center;">0.9771</td>
<td style="text-align: center;">1.0000</td>
<td style="text-align: center;">0.9990</td>
<td style="text-align: center;">0.9853</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">0.9996</td>
<td style="text-align: center;">0.9974</td>
<td style="text-align: center;">0.9945</td>
</tr>
</tbody>
</table>
<p>Table 8: Impact of reference model, where "<em>/</em>" indicates that we use the source model to generate reference samples and score the log probability, while "GPT-J/*" indicates that we use GPT-J to generate the samples and the source model to score.</p>
<p>Sampling Model Ablation. We investigate the influence of the choice of the sampling model, as summarized in Table 8. Remarkably, when GPT-J is employed as the sampling model, FastDetectGPT attains the highest average detection accuracy. In comparison to Fast-DetectGPT utilizing the source model for sampling, the utilization of GPT-J enhances accuracy by an average of 0.0020 AUROC, representing a relative improvement of $27 \%$ across all three datasets and the three models under consideration. These findings indicate that employing a more robust sampling model has the potential to further augment the performance of Fast-DetectGPT in the white-box setting.
Normalization Ablation. Normalization based on the standard deviation has previously been proposed as an additional technique within DetectGPT. In our study, we integrate this normalization as a default component of the conditional probability curvature metric for two principal reasons. Firstly, normalization exerts a significant influence on detection accuracy, resulting in an average AUROC improvement of 0.0172 , equivalent to a relative increase of $36 \%$ for DetectGPT. In the case of Fast-DetectGPT, normalization enhances the average AUROC by 0.0020 , corresponding to a $10 \%$ relative improvement. Secondly, after normalization, the distributions of the curvatures for different models across various datasets become more directly comparable. Without normalization, these distributions exhibit variations in width, ranging from narrow to wide, depending on the variance of the generated samples.
Entropy Ablation. Among the total $75 \%$ relative improvement, $10 \%$ is attributed to normalization by $\hat{\sigma}$, while the remaining $65 \%$ stems from the contribution of the numerator $\log p_{\theta}(x \mid x)-\hat{\mu}$. The entropy $-\hat{\mu}$ plays a crucial role in achieving high detection accuracy in Fast-DetectGPT.
An intuitive elucidation of the significance of entropy lies in the substantial variance observed in the $\log p_{\theta}\left(x_{j} \mid x_{&lt;j}\right)$ values for different tokens $x_{j}$ across diverse contexts $x_{&lt;j}$. This variability introduces instability in the statistical measures employed for detection. Conversely, the expectation $\hat{\mu}<em j="j">{j}$ establishes a dynamic probability baseline for each token. Consequently, the subtraction of $\log p \theta\left(x</em>$ serves to mitigate the variance of the log-likelihood, yielding a more stable statistic that proves resilient to token or context fluctuations.
In a specific experiment involving ChatGPT generations for XSum, the average standard deviation of token-level log-likelihood is 2.1893 , while the average standard deviation of token-level entropy is 1.6090 . Conversely, the average standard deviation resulting from their addition is 1.6342 , representing a significant reduction from the initial 2.1893 .} \mid x_{&lt;j}\right)$ and $\hat{\mu}_{j</p>
<h1>F Additional Analysis</h1>
<h2>F. 1 Robustness against Decoding Strategies</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Top- $p(p=0.96)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Top- $k(k=40)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Temperature $(T=0.8)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">XSum</td>
<td style="text-align: center;">SQuAD</td>
<td style="text-align: center;">WritingP</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">XSum</td>
<td style="text-align: center;">SQuAD</td>
<td style="text-align: center;">WritingP</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">XSum</td>
<td style="text-align: center;">SQuAD</td>
<td style="text-align: center;">WritingP</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: center;">Likelihood</td>
<td style="text-align: center;">0.9126</td>
<td style="text-align: center;">0.9045</td>
<td style="text-align: center;">0.9781</td>
<td style="text-align: center;">0.9317</td>
<td style="text-align: center;">0.8624</td>
<td style="text-align: center;">0.8612</td>
<td style="text-align: center;">0.9608</td>
<td style="text-align: center;">0.8948</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.9647</td>
<td style="text-align: center;">0.9962</td>
<td style="text-align: center;">0.9780</td>
</tr>
<tr>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">0.5287</td>
<td style="text-align: center;">0.5273</td>
<td style="text-align: center;">0.3255</td>
<td style="text-align: center;">0.4605</td>
<td style="text-align: center;">0.5538</td>
<td style="text-align: center;">0.5523</td>
<td style="text-align: center;">0.3632</td>
<td style="text-align: center;">0.4898</td>
<td style="text-align: center;">0.4854</td>
<td style="text-align: center;">0.4942</td>
<td style="text-align: center;">0.2522</td>
<td style="text-align: center;">0.4106</td>
</tr>
<tr>
<td style="text-align: center;">LogRank</td>
<td style="text-align: center;">0.9293</td>
<td style="text-align: center;">0.9324</td>
<td style="text-align: center;">0.9853</td>
<td style="text-align: center;">0.9490</td>
<td style="text-align: center;">0.8946</td>
<td style="text-align: center;">0.9047</td>
<td style="text-align: center;">0.9757</td>
<td style="text-align: center;">0.9250</td>
<td style="text-align: center;">0.9844</td>
<td style="text-align: center;">0.9821</td>
<td style="text-align: center;">0.9978</td>
<td style="text-align: center;">0.9881</td>
</tr>
<tr>
<td style="text-align: center;">LRR</td>
<td style="text-align: center;">0.9223</td>
<td style="text-align: center;">0.9600</td>
<td style="text-align: center;">0.9836</td>
<td style="text-align: center;">0.9553</td>
<td style="text-align: center;">0.9173</td>
<td style="text-align: center;">0.9566</td>
<td style="text-align: center;">0.9827</td>
<td style="text-align: center;">0.9522</td>
<td style="text-align: center;">0.9826</td>
<td style="text-align: center;">0.9923</td>
<td style="text-align: center;">0.9903</td>
<td style="text-align: center;">0.9884</td>
</tr>
<tr>
<td style="text-align: center;">NPR</td>
<td style="text-align: center;">0.9789</td>
<td style="text-align: center;">0.9511</td>
<td style="text-align: center;">0.9901</td>
<td style="text-align: center;">0.9734</td>
<td style="text-align: center;">0.9726</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.9912</td>
<td style="text-align: center;">0.9696</td>
<td style="text-align: center;">0.9892</td>
<td style="text-align: center;">0.9710</td>
<td style="text-align: center;">0.9897</td>
<td style="text-align: center;">0.9833</td>
</tr>
<tr>
<td style="text-align: center;">DetectGPT</td>
<td style="text-align: center;">0.9778</td>
<td style="text-align: center;">0.9359</td>
<td style="text-align: center;">0.9807</td>
<td style="text-align: center;">0.9648</td>
<td style="text-align: center;">0.9708</td>
<td style="text-align: center;">0.9247</td>
<td style="text-align: center;">0.9797</td>
<td style="text-align: center;">0.9584</td>
<td style="text-align: center;">0.9830</td>
<td style="text-align: center;">0.9362</td>
<td style="text-align: center;">0.9745</td>
<td style="text-align: center;">0.9646</td>
</tr>
<tr>
<td style="text-align: center;">Fast-Detect</td>
<td style="text-align: center;">0.9975</td>
<td style="text-align: center;">0.9976</td>
<td style="text-align: center;">0.9994</td>
<td style="text-align: center;">0.9982</td>
<td style="text-align: center;">0.9871</td>
<td style="text-align: center;">0.9914</td>
<td style="text-align: center;">0.9977</td>
<td style="text-align: center;">0.9921</td>
<td style="text-align: center;">0.9998</td>
<td style="text-align: center;">0.9996</td>
<td style="text-align: center;">0.9992</td>
<td style="text-align: center;">0.9995</td>
</tr>
<tr>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.0197</td>
<td style="text-align: center;">0.0617</td>
<td style="text-align: center;">0.0188</td>
<td style="text-align: center;">0.0334</td>
<td style="text-align: center;">0.0164</td>
<td style="text-align: center;">0.0667</td>
<td style="text-align: center;">0.0180</td>
<td style="text-align: center;">0.0337</td>
<td style="text-align: center;">0.0168</td>
<td style="text-align: center;">0.0634</td>
<td style="text-align: center;">0.0247</td>
<td style="text-align: center;">0.0350</td>
</tr>
<tr>
<td style="text-align: center;">DetectGPT (fixed)</td>
<td style="text-align: center;">0.9476</td>
<td style="text-align: center;">0.8506</td>
<td style="text-align: center;">0.9377</td>
<td style="text-align: center;">0.9120</td>
<td style="text-align: center;">0.9158</td>
<td style="text-align: center;">0.8202</td>
<td style="text-align: center;">0.9181</td>
<td style="text-align: center;">0.8847</td>
<td style="text-align: center;">0.9717</td>
<td style="text-align: center;">0.9026</td>
<td style="text-align: center;">0.9522</td>
<td style="text-align: center;">0.9422</td>
</tr>
<tr>
<td style="text-align: center;">Fast-Detect (fixed)</td>
<td style="text-align: center;">0.9889</td>
<td style="text-align: center;">0.9942</td>
<td style="text-align: center;">0.9945</td>
<td style="text-align: center;">0.9925</td>
<td style="text-align: center;">0.9642</td>
<td style="text-align: center;">0.9790</td>
<td style="text-align: center;">0.9864</td>
<td style="text-align: center;">0.9765</td>
<td style="text-align: center;">0.9988</td>
<td style="text-align: center;">0.9989</td>
<td style="text-align: center;">0.9984</td>
<td style="text-align: center;">0.9987</td>
</tr>
<tr>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">0.0413</td>
<td style="text-align: center;">0.1436</td>
<td style="text-align: center;">0.0568</td>
<td style="text-align: center;">0.0806</td>
<td style="text-align: center;">0.0484</td>
<td style="text-align: center;">0.1587</td>
<td style="text-align: center;">0.0683</td>
<td style="text-align: center;">0.0918</td>
<td style="text-align: center;">0.0271</td>
<td style="text-align: center;">0.0963</td>
<td style="text-align: center;">0.0462</td>
<td style="text-align: center;">0.0565</td>
</tr>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Top- $p(p=0.90)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Top- $k(k=30)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Temperature $(T=0.6)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Likelihood</td>
<td style="text-align: center;">0.9592</td>
<td style="text-align: center;">0.9495</td>
<td style="text-align: center;">0.9924</td>
<td style="text-align: center;">0.9670</td>
<td style="text-align: center;">0.9610</td>
<td style="text-align: center;">0.8922</td>
<td style="text-align: center;">0.9754</td>
<td style="text-align: center;">0.9229</td>
<td style="text-align: center;">0.9964</td>
<td style="text-align: center;">0.9954</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">0.9972</td>
</tr>
<tr>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">0.4985</td>
<td style="text-align: center;">0.4990</td>
<td style="text-align: center;">0.2881</td>
<td style="text-align: center;">0.4285</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.5374</td>
<td style="text-align: center;">0.3359</td>
<td style="text-align: center;">0.4684</td>
<td style="text-align: center;">0.4171</td>
<td style="text-align: center;">0.3718</td>
<td style="text-align: center;">0.1146</td>
<td style="text-align: center;">0.3012</td>
</tr>
<tr>
<td style="text-align: center;">LogRank</td>
<td style="text-align: center;">0.9701</td>
<td style="text-align: center;">0.9684</td>
<td style="text-align: center;">0.9951</td>
<td style="text-align: center;">0.9779</td>
<td style="text-align: center;">0.9309</td>
<td style="text-align: center;">0.9338</td>
<td style="text-align: center;">0.9863</td>
<td style="text-align: center;">0.9503</td>
<td style="text-align: center;">0.9990</td>
<td style="text-align: center;">0.9987</td>
<td style="text-align: center;">1.0000</td>
<td style="text-align: center;">0.9992</td>
</tr>
<tr>
<td style="text-align: center;">Fast-Detect</td>
<td style="text-align: center;">0.9997</td>
<td style="text-align: center;">0.9998</td>
<td style="text-align: center;">0.9996</td>
<td style="text-align: center;">0.9997</td>
<td style="text-align: center;">0.9941</td>
<td style="text-align: center;">0.9955</td>
<td style="text-align: center;">0.9988</td>
<td style="text-align: center;">0.9961</td>
<td style="text-align: center;">0.9998</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: center;">0.9991</td>
<td style="text-align: center;">0.9996</td>
</tr>
</tbody>
</table>
<p>Table 9: Impact of decoding strategies, where the machine-generated texts are produced by sampling with top- $p$, top- $k$, and temperature. The report AUROC is the average over the five models in Table 10 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Top- $p(p=0.96)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Top- $k(k=40)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Temperature $(T=0.8)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">OPT2.7</td>
<td style="text-align: center;">Neo2.7</td>
<td style="text-align: center;">GPTJ</td>
<td style="text-align: center;">NeoX</td>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">OPT2.7</td>
<td style="text-align: center;">Neo2.7</td>
<td style="text-align: center;">GPTJ</td>
<td style="text-align: center;">NeoX</td>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">OPT2.7</td>
</tr>
<tr>
<td style="text-align: center;">XSum</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Likelihood</td>
<td style="text-align: center;">9234</td>
<td style="text-align: center;">9308</td>
<td style="text-align: center;">931</td>
<td style="text-align: center;">9042</td>
<td style="text-align: center;">8753</td>
<td style="text-align: center;">8815</td>
<td style="text-align: center;">8918</td>
<td style="text-align: center;">8873</td>
<td style="text-align: center;">8401</td>
<td style="text-align: center;">8114</td>
<td style="text-align: center;">9781</td>
<td style="text-align: center;">982</td>
</tr>
<tr>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">.5541</td>
<td style="text-align: center;">.4665</td>
<td style="text-align: center;">.5303</td>
<td style="text-align: center;">.531</td>
<td style="text-align: center;">.5614</td>
<td style="text-align: center;">.5743</td>
<td style="text-align: center;">.4843</td>
<td style="text-align: center;">.5601</td>
<td style="text-align: center;">.5588</td>
<td style="text-align: center;">.5917</td>
<td style="text-align: center;">.5022</td>
<td style="text-align: center;">.4289</td>
</tr>
<tr>
<td style="text-align: center;">LogRank</td>
<td style="text-align: center;">.9414</td>
<td style="text-align: center;">.9422</td>
<td style="text-align: center;">.9502</td>
<td style="text-align: center;">.922</td>
<td style="text-align: center;">.8906</td>
<td style="text-align: center;">.9122</td>
<td style="text-align: center;">.9173</td>
<td style="text-align: center;">.9227</td>
<td style="text-align: center;">.8756</td>
<td style="text-align: center;">.8454</td>
<td style="text-align: center;">.9883</td>
<td style="text-align: center;">.9885</td>
</tr>
<tr>
<td style="text-align: center;">LRR</td>
<td style="text-align: center;">.9509</td>
<td style="text-align: center;">.9252</td>
<td style="text-align: center;">.9478</td>
<td style="text-align: center;">.9203</td>
<td style="text-align: center;">.8672</td>
<td style="text-align: center;">.9432</td>
<td style="text-align: center;">.9254</td>
<td style="text-align: center;">.9452</td>
<td style="text-align: center;">.9045</td>
<td style="text-align: center;">.8683</td>
<td style="text-align: center;">.9889</td>
<td style="text-align: center;">.9823</td>
</tr>
<tr>
<td style="text-align: center;">NPR</td>
<td style="text-align: center;">.9909</td>
<td style="text-align: center;">.9841</td>
<td style="text-align: center;">.9982</td>
<td style="text-align: center;">.973</td>
<td style="text-align: center;">.9486</td>
<td style="text-align: center;">.987</td>
<td style="text-align: center;">.9801</td>
<td style="text-align: center;">.9938</td>
<td style="text-align: center;">.9606</td>
<td style="text-align: center;">.9416</td>
<td style="text-align: center;">.9955</td>
<td style="text-align: center;">.9916</td>
</tr>
<tr>
<td style="text-align: center;">DetectGPT</td>
<td style="text-align: center;">9875</td>
<td style="text-align: center;">9793</td>
<td style="text-align: center;">9961</td>
<td style="text-align: center;">9762</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">9869</td>
<td style="text-align: center;">9707</td>
<td style="text-align: center;">9919</td>
<td style="text-align: center;">9619</td>
<td style="text-align: center;">9424</td>
<td style="text-align: center;">9928</td>
<td style="text-align: center;">9856</td>
</tr>
<tr>
<td style="text-align: center;">Fast-DetectGPT</td>
<td style="text-align: center;">.9994</td>
<td style="text-align: center;">.9965</td>
<td style="text-align: center;">.9988</td>
<td style="text-align: center;">.997</td>
<td style="text-align: center;">.9958</td>
<td style="text-align: center;">.9954</td>
<td style="text-align: center;">.9867</td>
<td style="text-align: center;">.9938</td>
<td style="text-align: center;">.9826</td>
<td style="text-align: center;">.9773</td>
<td style="text-align: center;">.9999</td>
<td style="text-align: center;">.9999</td>
</tr>
<tr>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">.0119</td>
<td style="text-align: center;">.0172</td>
<td style="text-align: center;">.0028</td>
<td style="text-align: center;">.0208</td>
<td style="text-align: center;">.0458</td>
<td style="text-align: center;">.0085</td>
<td style="text-align: center;">.0160</td>
<td style="text-align: center;">.0018</td>
<td style="text-align: center;">.0207</td>
<td style="text-align: center;">.0349</td>
<td style="text-align: center;">.0071</td>
<td style="text-align: center;">.0143</td>
</tr>
<tr>
<td style="text-align: center;">DetectGPT(fixed)</td>
<td style="text-align: center;">9599</td>
<td style="text-align: center;">9458</td>
<td style="text-align: center;">9963</td>
<td style="text-align: center;">9567</td>
<td style="text-align: center;">9214</td>
<td style="text-align: center;">9145</td>
<td style="text-align: center;">9026</td>
<td style="text-align: center;">9919</td>
<td style="text-align: center;">8846</td>
<td style="text-align: center;">8854</td>
<td style="text-align: center;">9722</td>
<td style="text-align: center;">9635</td>
</tr>
<tr>
<td style="text-align: center;">Fast-Detect(fixed)</td>
<td style="text-align: center;">.9953</td>
<td style="text-align: center;">.9861</td>
<td style="text-align: center;">.9997</td>
<td style="text-align: center;">.9856</td>
<td style="text-align: center;">.9779</td>
<td style="text-align: center;">.9805</td>
<td style="text-align: center;">.9613</td>
<td style="text-align: center;">.9979</td>
<td style="text-align: center;">.9499</td>
<td style="text-align: center;">.9311</td>
<td style="text-align: center;">.9978</td>
<td style="text-align: center;">.999</td>
</tr>
<tr>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">.0554</td>
<td style="text-align: center;">.0423</td>
<td style="text-align: center;">.0036</td>
<td style="text-align: center;">.0489</td>
<td style="text-align: center;">.0565</td>
<td style="text-align: center;">.0662</td>
<td style="text-align: center;">.0587</td>
<td style="text-align: center;">.0060</td>
<td style="text-align: center;">.0654</td>
<td style="text-align: center;">.0457</td>
<td style="text-align: center;">.0256</td>
<td style="text-align: center;">.0355</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Likelihood</td>
<td style="text-align: center;">961</td>
<td style="text-align: center;">944</td>
<td style="text-align: center;">9214</td>
<td style="text-align: center;">8838</td>
<td style="text-align: center;">8122</td>
<td style="text-align: center;">9395</td>
<td style="text-align: center;">9072</td>
<td style="text-align: center;">8926</td>
<td style="text-align: center;">8351</td>
<td style="text-align: center;">7317</td>
<td style="text-align: center;">9966</td>
<td style="text-align: center;">987</td>
</tr>
<tr>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">.5369</td>
<td style="text-align: center;">.4736</td>
<td style="text-align: center;">.539</td>
<td style="text-align: center;">.5277</td>
<td style="text-align: center;">.5593</td>
<td style="text-align: center;">.552</td>
<td style="text-align: center;">.5203</td>
<td style="text-align: center;">.5457</td>
<td style="text-align: center;">.5441</td>
<td style="text-align: center;">.5992</td>
<td style="text-align: center;">.5132</td>
<td style="text-align: center;">.4508</td>
</tr>
<tr>
<td style="text-align: center;">LogRank</td>
<td style="text-align: center;">.9792</td>
<td style="text-align: center;">.9657</td>
<td style="text-align: center;">.9535</td>
<td style="text-align: center;">.9156</td>
<td style="text-align: center;">.8482</td>
<td style="text-align: center;">.9692</td>
<td style="text-align: center;">.9423</td>
<td style="text-align: center;">.9385</td>
<td style="text-align: center;">.8842</td>
<td style="text-align: center;">.7895</td>
<td style="text-align: center;">.9972</td>
<td style="text-align: center;">.9959</td>
</tr>
<tr>
<td style="text-align: center;">LRR</td>
<td style="text-align: center;">.9865</td>
<td style="text-align: center;">.981</td>
<td style="text-align: center;">.981</td>
<td style="text-align: center;">.9507</td>
<td style="text-align: center;">.9009</td>
<td style="text-align: center;">.9898</td>
<td style="text-align: center;">.9782</td>
<td style="text-align: center;">.9815</td>
<td style="text-align: center;">.945</td>
<td style="text-align: center;">.8886</td>
<td style="text-align: center;">.9991</td>
<td style="text-align: center;">.9973</td>
</tr>
<tr>
<td style="text-align: center;">NPR</td>
<td style="text-align: center;">.9955</td>
<td style="text-align: center;">.9934</td>
<td style="text-align: center;">.9897</td>
<td style="text-align: center;">.9335</td>
<td style="text-align: center;">.8436</td>
<td style="text-align: center;">.9962</td>
<td style="text-align: center;">.99</td>
<td style="text-align: center;">.9877</td>
<td style="text-align: center;">.9251</td>
<td style="text-align: center;">.826</td>
<td style="text-align: center;">.9988</td>
<td style="text-align: center;">.9963</td>
</tr>
<tr>
<td style="text-align: center;">DetectGPT</td>
<td style="text-align: center;">994</td>
<td style="text-align: center;">9838</td>
<td style="text-align: center;">9781</td>
<td style="text-align: center;">9054</td>
<td style="text-align: center;">8182</td>
<td style="text-align: center;">9928</td>
<td style="text-align: center;">9604</td>
<td style="text-align: center;">9709</td>
<td style="text-align: center;">8819</td>
<td style="text-align: center;">7977</td>
<td style="text-align: center;">9959</td>
<td style="text-align: center;">9819</td>
</tr>
<tr>
<td style="text-align: center;">Fast-DetectGPT</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">.9997</td>
<td style="text-align: center;">.9998</td>
<td style="text-align: center;">.9981</td>
<td style="text-align: center;">.9903</td>
<td style="text-align: center;">.9986</td>
<td style="text-align: center;">.9973</td>
<td style="text-align: center;">.998</td>
<td style="text-align: center;">.9928</td>
<td style="text-align: center;">.9706</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">.0060</td>
<td style="text-align: center;">.0159</td>
<td style="text-align: center;">.0218</td>
<td style="text-align: center;">.0927</td>
<td style="text-align: center;">.1722</td>
<td style="text-align: center;">.0057</td>
<td style="text-align: center;">.0169</td>
<td style="text-align: center;">.0271</td>
<td style="text-align: center;">.1109</td>
<td style="text-align: center;">.1729</td>
<td style="text-align: center;">.0041</td>
<td style="text-align: center;">.0180</td>
</tr>
<tr>
<td style="text-align: center;">DetectGPT(fixed)</td>
<td style="text-align: center;">8066</td>
<td style="text-align: center;">7857</td>
<td style="text-align: center;">9781</td>
<td style="text-align: center;">8462</td>
<td style="text-align: center;">8365</td>
<td style="text-align: center;">7815</td>
<td style="text-align: center;">763</td>
<td style="text-align: center;">9709</td>
<td style="text-align: center;">8128</td>
<td style="text-align: center;">7729</td>
<td style="text-align: center;">8936</td>
<td style="text-align: center;">862</td>
</tr>
<tr>
<td style="text-align: center;">Fast-Detect(fixed)</td>
<td style="text-align: center;">.9984</td>
<td style="text-align: center;">.995</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">.9933</td>
<td style="text-align: center;">.9845</td>
<td style="text-align: center;">.9893</td>
<td style="text-align: center;">.984</td>
<td style="text-align: center;">.9997</td>
<td style="text-align: center;">.9709</td>
<td style="text-align: center;">.9509</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">.9999</td>
</tr>
<tr>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">.1918</td>
<td style="text-align: center;">.2093</td>
<td style="text-align: center;">.0219</td>
<td style="text-align: center;">.1471</td>
<td style="text-align: center;">.1480</td>
<td style="text-align: center;">.2078</td>
<td style="text-align: center;">.2210</td>
<td style="text-align: center;">.0288</td>
<td style="text-align: center;">.1581</td>
<td style="text-align: center;">.1780</td>
<td style="text-align: center;">.1064</td>
<td style="text-align: center;">.1379</td>
</tr>
<tr>
<td style="text-align: center;">WritingPrompts</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Likelihood</td>
<td style="text-align: center;">9889</td>
<td style="text-align: center;">9756</td>
<td style="text-align: center;">9806</td>
<td style="text-align: center;">9764</td>
<td style="text-align: center;">9711</td>
<td style="text-align: center;">9777</td>
<td style="text-align: center;">9555</td>
<td style="text-align: center;">9677</td>
<td style="text-align: center;">9566</td>
<td style="text-align: center;">9463</td>
<td style="text-align: center;">9983</td>
<td style="text-align: center;">9949</td>
</tr>
<tr>
<td style="text-align: center;">Entropy</td>
<td style="text-align: center;">.3236</td>
<td style="text-align: center;">.3789</td>
<td style="text-align: center;">.2798</td>
<td style="text-align: center;">.3212</td>
<td style="text-align: center;">.3238</td>
<td style="text-align: center;">.3601</td>
<td style="text-align: center;">.4149</td>
<td style="text-align: center;">.3183</td>
<td style="text-align: center;">.3529</td>
<td style="text-align: center;">.3696</td>
<td style="text-align: center;">.2511</td>
<td style="text-align: center;">.3033</td>
</tr>
<tr>
<td style="text-align: center;">LogRank</td>
<td style="text-align: center;">.9931</td>
<td style="text-align: center;">.9827</td>
<td style="text-align: center;">.987</td>
<td style="text-align: center;">.9853</td>
<td style="text-align: center;">.9785</td>
<td style="text-align: center;">.9867</td>
<td style="text-align: center;">.9719</td>
<td style="text-align: center;">.9807</td>
<td style="text-align: center;">.9735</td>
<td style="text-align: center;">.9656</td>
<td style="text-align: center;">.9991</td>
<td style="text-align: center;">.9972</td>
</tr>
<tr>
<td style="text-align: center;">LRR</td>
<td style="text-align: center;">.9906</td>
<td style="text-align: center;">.9854</td>
<td style="text-align: center;">.9857</td>
<td style="text-align: center;">.9839</td>
<td style="text-align: center;">.9724</td>
<td style="text-align: center;">.9901</td>
<td style="text-align: center;">.9835</td>
<td style="text-align: center;">.9858</td>
<td style="text-align: center;">.9834</td>
<td style="text-align: center;">.9709</td>
<td style="text-align: center;">.9961</td>
<td style="text-align: center;">.9917</td>
</tr>
<tr>
<td style="text-align: center;">NPR</td>
<td style="text-align: center;">.9984</td>
<td style="text-align: center;">.9964</td>
<td style="text-align: center;">.9944</td>
<td style="text-align: center;">.9855</td>
<td style="text-align: center;">.9758</td>
<td style="text-align: center;">.9988</td>
<td style="text-align: center;">.9955</td>
<td style="text-align: center;">.9971</td>
<td style="text-align: center;">.9888</td>
<td style="text-align: center;">.9759</td>
<td style="text-align: center;">.9985</td>
<td style="text-align: center;">.9953</td>
</tr>
<tr>
<td style="text-align: center;">DetectGPT</td>
<td style="text-align: center;">9969</td>
<td style="text-align: center;">9898</td>
<td style="text-align: center;">9926</td>
<td style="text-align: center;">9714</td>
<td style="text-align: center;">9526</td>
<td style="text-align: center;">9965</td>
<td style="text-align: center;">9868</td>
<td style="text-align: center;">992</td>
<td style="text-align: center;">971</td>
<td style="text-align: center;">9521</td>
<td style="text-align: center;">9932</td>
<td style="text-align: center;">985</td>
</tr>
<tr>
<td style="text-align: center;">Fast-DetectGPT</td>
<td style="text-align: center;">.9997</td>
<td style="text-align: center;">.9999</td>
<td style="text-align: center;">.9998</td>
<td style="text-align: center;">.9998</td>
<td style="text-align: center;">.9981</td>
<td style="text-align: center;">.9978</td>
<td style="text-align: center;">.9973</td>
<td style="text-align: center;">.999</td>
<td style="text-align: center;">.9987</td>
<td style="text-align: center;">.9957</td>
<td style="text-align: center;">.9997</td>
<td style="text-align: center;">.9999</td>
</tr>
<tr>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">.0028</td>
<td style="text-align: center;">.0102</td>
<td style="text-align: center;">.0071</td>
<td style="text-align: center;">.0284</td>
<td style="text-align: center;">.0455</td>
<td style="text-align: center;">.0012</td>
<td style="text-align: center;">.0105</td>
<td style="text-align: center;">.0070</td>
<td style="text-align: center;">.0277</td>
<td style="text-align: center;">.0436</td>
<td style="text-align: center;">.0065</td>
<td style="text-align: center;">.0149</td>
</tr>
<tr>
<td style="text-align: center;">DetectGPT(fixed)</td>
<td style="text-align: center;">9297</td>
<td style="text-align: center;">9177</td>
<td style="text-align: center;">9926</td>
<td style="text-align: center;">9299</td>
<td style="text-align: center;">9187</td>
<td style="text-align: center;">9115</td>
<td style="text-align: center;">8906</td>
<td style="text-align: center;">992</td>
<td style="text-align: center;">9052</td>
<td style="text-align: center;">8914</td>
<td style="text-align: center;">9385</td>
<td style="text-align: center;">9451</td>
</tr>
<tr>
<td style="text-align: center;">Fast-Detect(fixed)</td>
<td style="text-align: center;">.9982</td>
<td style="text-align: center;">.9825</td>
<td style="text-align: center;">.9999</td>
<td style="text-align: center;">.9974</td>
<td style="text-align: center;">.9945</td>
<td style="text-align: center;">.9947</td>
<td style="text-align: center;">.9616</td>
<td style="text-align: center;">.9998</td>
<td style="text-align: center;">.9901</td>
<td style="text-align: center;">.986</td>
<td style="text-align: center;">.9995</td>
<td style="text-align: center;">.9959</td>
</tr>
<tr>
<td style="text-align: center;">(Diff)</td>
<td style="text-align: center;">.0685</td>
<td style="text-align: center;">.0648</td>
<td style="text-align: center;">.0073</td>
<td style="text-align: center;">.0676</td>
<td style="text-align: center;">.0758</td>
<td style="text-align: center;">.0832</td>
<td style="text-align: center;">.0710</td>
<td style="text-align: center;">.0079</td>
<td style="text-align: center;">.0849</td>
<td style="text-align: center;">.0946</td>
<td style="text-align: center;">.0610</td>
<td style="text-align: center;">.0508</td>
</tr>
</tbody>
</table>
<p>Table 10: Detailed results on decoding strategies.</p>
<p>Machine systems may use different decoding strategies such as top- $k$ sampling, top- $p$ (Nucleus) sampling, and sampling with a temperature $T$. We experiment on the five models and three datasets</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3} \mathrm{https}: / /$ openai.com/blog/openai-api&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
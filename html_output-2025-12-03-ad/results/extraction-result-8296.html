<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8296 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8296</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8296</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-270045732</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.15687v1.pdf" target="_blank">Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models</a></p>
                <p><strong>Paper Abstract:</strong> Conventional demographic inference methods have predominantly operated under the supervision of accurately labeled data, yet struggle to adapt to shifting social landscapes and diverse cultural contexts, leading to narrow specialization and limited accuracy in applications. Recently, the emergence of large multimodal models (LMMs) has shown transformative potential across various research tasks, such as visual comprehension and description. In this study, we explore the application of LMMs to demographic inference and introduce a benchmark for both quantitative and qualitative evaluation. Our findings indicate that LMMs possess advantages in zero-shot learning, interpretability, and handling uncurated ’in-the-wild’ inputs, albeit with a propensity for off-target predictions. To enhance LMM performance and achieve comparability with supervised learning baselines, we propose a Chain-of-Thought augmented prompting approach, which effectively mitigates the off-target prediction issue.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8296.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8296.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA (Large Language and Vision Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source large multimodal model combining a vision encoder with a 13B LLM backbone to answer visual prompts; evaluated in this paper both with native zero-shot prompting and with Chain-of-Thought (CoT) augmented prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA (13B, evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LMM using a visual encoder to produce embeddings fed to a 13B language model (LLaVA 1.5, 13B weights); used with native zero-shot prompting and with the paper's Chain-of-Thought augmented prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['naive direct zero-shot prompting (single-step direct answer)', 'Chain-of-Thought (CoT) augmented prompting (multi-step intermediate attribute generation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Naive prompting: give the task prompt and image and request a direct demographic label. CoT prompting (as implemented here): three-step textual pipeline — (1) generate a Facial Feature Collection (FFC) describing fine-grained visual attributes for age/gender/ethnicity; (2) predict a name based on the image and combine it with FFC to form a demographic description D; (3) append D to the prompt and request final demographic subcategory predictions (age/gender/race). This CoT pipeline is purely text-based and zero-shot (no fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparison between naive (single-step) zero-shot prompting and the CoT-augmented multi-step prompting applied to LLaVA (i.e., LLaVA vs. LLaVA w/ COT) across three demographic datasets (UTKFace, FairFace, CACD). Measurements include off-target rate, accuracy/Kappa for discrete attributes, and MAE/RMSE/R^2/MAPE for age regression.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Demographic inference benchmark assembled by authors using UTKFace, FairFace, and CACD datasets — tasks: age estimation (regression), gender classification, and ethnicity classification (seven classes on FairFace).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>LLaVA w/ COT yields large practical gains over naive LLaVA: on CACD the paper reports LLaVA baseline MAE = 6.65 -> LLaVA w/ COT MAE = 5.75 (units: years), RMSE improves from 8.73 -> 8.17, R^2 improves from 0.428 -> 0.500, and MAPE improves from 22.51% -> 15.99%. Across UTKFace and FairFace the authors report LLaVA w/ COT achieves 0.00% off-target rate and higher Kappa/accuracy scores compared to naive LLaVA (exact per-dataset classification numbers given in the paper's tables).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT makes LLaVA outputs more interpretable (exposes intermediate facial attribute descriptions) and substantially reduces off-target predictions; authors also note occasional failure modes where the CoT chain can be misled by non-facial cues (e.g., clothing) resulting in incorrect demographic inference despite richer reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Applying Chain-of-Thought prompting to LLaVA transforms it from a flexible but often off-target zero-shot predictor into a precise, interpretable model competitive with supervised baselines; COT eliminates off-target outputs for LLaVA in the reported experiments and improves age estimation metrics (MAE, RMSE, R^2, MAPE).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8296.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8296.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniGPTv2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniGPT-v2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified interface LMM that uses a small LLM backbone (Llama-2 7B in this study) connected to a visual encoder; evaluated with and without CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniGPTv2 (Llama-2 7B backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision–language model built on a 7B LLM (Llama-2 7B) front-end; used here in zero-shot demographic inference and with the paper's CoT augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['naive direct zero-shot prompting (single-step direct answer)', 'Chain-of-Thought (CoT) augmented prompting (multi-step intermediate attribute generation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same CoT pipeline as applied to other LMMs: generate facial features (FFC), propose a name, combine into demographic description D, and use D as context to produce final demographic labels; compared against direct prompting without intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablative-style comparison: MiniGPTv2 evaluated in native zero-shot mode and in CoT-augmented mode on FairFace/UTKFace/CACD; off-target rate and classification/regression metrics compared.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same demographic inference benchmark (UTKFace, FairFace, CACD) covering age regression and gender/ethnicity classification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>CoT reduces off-target errors: reported FairFace off-target rate drops from 18.4% (MiniGPTv2 baseline) to 6.8% (MiniGPTv2 w/ COT). Numerical improvements in accuracy/Kappa/age metrics are reported in the paper's tables but are presented mostly as aggregate improvements; the off-target reduction is the clearest quantified effect shown for MiniGPTv2.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT substantially reduces the frequency of off-target or out-of-category textual outputs and increases correct labels; however, naive MiniGPTv2 without CoT exhibits high off-target rates and lower sensitivity to fine-grained visual cues.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Applying CoT to MiniGPTv2 meaningfully reduces off-target predictions and improves demographic inference reliability; CoT is an effective zero-shot prompting strategy across LMM architectures including MiniGPTv2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8296.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8296.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructBLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructBLIP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned vision-language model based on Vicuna 13B used here for demographic inference, evaluated with native prompting and with Chain-of-Thought augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructBLIP (Vicuna 13B-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-tuned LMM combining a vision encoder and a 13B language model (Vicuna-based), applied to zero-shot demographic inference tasks and with the paper's CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['naive direct zero-shot prompting (single-step direct answer)', 'Chain-of-Thought (CoT) augmented prompting (multi-step intermediate attribute generation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>InstructBLIP evaluated both with direct response prompting and with the three-step CoT procedure (FFC -> name -> combined demographic description -> final prediction) to improve interpretability and reduce off-target outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct head-to-head comparison of native InstructBLIP responses vs. InstructBLIP w/ COT on FairFace and other datasets; primary metrics include off-target rate and classification/regression performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Demographic inference on UTKFace, FairFace, and CACD (age, gender, ethnicity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported FairFace off-target rate decreases from 17.1% (InstructBLIP baseline) to 5.5% with CoT. Other accuracy/Kappa improvements are summarized in the paper's tables and show consistent improvements when CoT is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT reduces irrelevant or out-of-category textual outputs and yields more specific, attribute-grounded justifications for predictions; naive InstructBLIP shows higher off-target tendencies without CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Chain-of-Thought prompting significantly mitigates InstructBLIP's off-target predictions and improves demographic inference performance and interpretability in a zero-shot setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8296.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8296.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternLM (vision-language variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An InternLM-based LMM (7B variant used in this study) evaluated for demographic inference with and without CoT prompting; shown to have low off-target rates that drop to zero with CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM (7B, evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A vision-language model built upon the InternLM family (7B weights used) that maps visual inputs into the LLM embedding space and produces textual demographic predictions; tested in zero-shot and CoT-augmented modes.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['naive direct zero-shot prompting (single-step direct answer)', 'Chain-of-Thought (CoT) augmented prompting (multi-step intermediate attribute generation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>InternLM was evaluated with straightforward direct prompting and with the paper's CoT pipeline (FFC + name prediction + combined demographic description used as prompt context) to improve outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Comparison of InternLM baseline vs. InternLM w/ COT on FairFace/UTKFace/CACD, measuring off-target rate and standard classification/regression metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Demographic inference tasks on UTKFace, FairFace, and CACD datasets (age/gender/ethnicity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>InternLM baseline already shows a low off-target rate (reported ~0.1% in the paper) which the authors report as reduced to 0.0% with CoT. Performance on accuracy/Kappa and age metrics also improve with CoT as summarized in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Even models with low baseline off-target tendencies benefit from CoT by becoming fully on-target and more interpretable; CoT supplies explicit intermediate attributes that can be inspected to understand model decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Chain-of-Thought prompting can eliminate the remaining off-target predictions even for InternLM, further improving reliability and interpretability in demographic inference without any fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8296.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8296.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought augmented prompting (this work's implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specific, zero-shot Chain-of-Thought prompting pipeline introduced/used in this paper to improve demographic inference from images by eliciting intermediate attribute descriptions (FFC) and name suggestions, then using them as context for final predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought augmented prompting (applied to LMMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A three-step, purely textual CoT pipeline: (1) generate Facial Feature Collection (FFC) of fine-grained visual attributes; (2) generate a plausible name for the person based on the image; (3) combine FFC and name into a demographic description used to prompt final subcategory predictions (age/gender/race). No fine-tuning or few-shot examples required.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['decompositional intermediate attribute generation (FFC)', 'auxiliary attribute inference (name prediction)', 'contextualized final decision-making using assembled demographic description']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Decompositional intermediate attribute generation: elicit a structured list of facial attributes (age/gender/ethnicity cues). Name prediction: use LMM's captioning/naming ability to suggest a plausible first/last name as an auxiliary ethnicity cue. Contextualized final decision: feed the assembled demographic description into the final prompt for each demographic subtask. This CoT is applied zero-shot as a prompt engineering technique.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (explicit multi-step reasoning pipeline as compared to single-step direct prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Applied CoT vs. direct prompting across multiple LMMs (LLaVA, MiniGPTv2, InstructBLIP, InternLM) and datasets (UTKFace, FairFace, CACD); measured off-target rates and standard accuracy/regression metrics to quantify differences.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Demographic inference on UTKFace, FairFace, and CACD datasets (age regression; gender and ethnicity classification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>CoT consistently reduces off-target rates across models (examples: MiniGPTv2 18.4% -> 6.8%; InstructBLIP 17.1% -> 5.5%; InternLM ~0.1% -> 0.0%; LLaVA baseline 0.0% -> remains 0.0% or eliminated where present). CoT improves age estimation metrics for LLaVA on CACD (MAE 6.65 -> 5.75 years; RMSE 8.73 -> 8.17; R^2 0.428 -> 0.500; MAPE 22.51% -> 15.99%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT yields attribute-rich intermediate outputs that increase interpretability and reduce off-target/out-of-distribution textual answers; authors note CoT can occasionally introduce biases by over-weighting contextual or non-facial cues (e.g., clothing) when only a single image is available.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>The explicitly multi-step CoT prompting pipeline substantially improves zero-shot demographic inference by reducing off-target outputs, increasing accuracy/Kappa and regression performance, and providing interpretable intermediate explanations — enabling LMM performance competitive with supervised baselines without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cocot: Contrastive chain-of-thought prompting for large multimodal models with multiple image inputs <em>(Rating: 2)</em></li>
                <li>Instructblip: Towards general-purpose vision-language models with instruction tuning <em>(Rating: 1)</em></li>
                <li>Visual instruction tuning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8296",
    "paper_id": "paper-270045732",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "LLaVA",
            "name_full": "LLaVA (Large Language and Vision Assistant)",
            "brief_description": "An open-source large multimodal model combining a vision encoder with a 13B LLM backbone to answer visual prompts; evaluated in this paper both with native zero-shot prompting and with Chain-of-Thought (CoT) augmented prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA (13B, evaluated)",
            "model_description": "LMM using a visual encoder to produce embeddings fed to a 13B language model (LLaVA 1.5, 13B weights); used with native zero-shot prompting and with the paper's Chain-of-Thought augmented prompting.",
            "reasoning_methods": [
                "naive direct zero-shot prompting (single-step direct answer)",
                "Chain-of-Thought (CoT) augmented prompting (multi-step intermediate attribute generation)"
            ],
            "reasoning_methods_description": "Naive prompting: give the task prompt and image and request a direct demographic label. CoT prompting (as implemented here): three-step textual pipeline — (1) generate a Facial Feature Collection (FFC) describing fine-grained visual attributes for age/gender/ethnicity; (2) predict a name based on the image and combine it with FFC to form a demographic description D; (3) append D to the prompt and request final demographic subcategory predictions (age/gender/race). This CoT pipeline is purely text-based and zero-shot (no fine-tuning).",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Direct comparison between naive (single-step) zero-shot prompting and the CoT-augmented multi-step prompting applied to LLaVA (i.e., LLaVA vs. LLaVA w/ COT) across three demographic datasets (UTKFace, FairFace, CACD). Measurements include off-target rate, accuracy/Kappa for discrete attributes, and MAE/RMSE/R^2/MAPE for age regression.",
            "task_or_benchmark": "Demographic inference benchmark assembled by authors using UTKFace, FairFace, and CACD datasets — tasks: age estimation (regression), gender classification, and ethnicity classification (seven classes on FairFace).",
            "performance_results": "LLaVA w/ COT yields large practical gains over naive LLaVA: on CACD the paper reports LLaVA baseline MAE = 6.65 -&gt; LLaVA w/ COT MAE = 5.75 (units: years), RMSE improves from 8.73 -&gt; 8.17, R^2 improves from 0.428 -&gt; 0.500, and MAPE improves from 22.51% -&gt; 15.99%. Across UTKFace and FairFace the authors report LLaVA w/ COT achieves 0.00% off-target rate and higher Kappa/accuracy scores compared to naive LLaVA (exact per-dataset classification numbers given in the paper's tables).",
            "qualitative_findings": "CoT makes LLaVA outputs more interpretable (exposes intermediate facial attribute descriptions) and substantially reduces off-target predictions; authors also note occasional failure modes where the CoT chain can be misled by non-facial cues (e.g., clothing) resulting in incorrect demographic inference despite richer reasoning.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Applying Chain-of-Thought prompting to LLaVA transforms it from a flexible but often off-target zero-shot predictor into a precise, interpretable model competitive with supervised baselines; COT eliminates off-target outputs for LLaVA in the reported experiments and improves age estimation metrics (MAE, RMSE, R^2, MAPE).",
            "uuid": "e8296.0",
            "source_info": {
                "paper_title": "Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "MiniGPTv2",
            "name_full": "MiniGPT-v2",
            "brief_description": "A unified interface LMM that uses a small LLM backbone (Llama-2 7B in this study) connected to a visual encoder; evaluated with and without CoT prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MiniGPTv2 (Llama-2 7B backbone)",
            "model_description": "Vision–language model built on a 7B LLM (Llama-2 7B) front-end; used here in zero-shot demographic inference and with the paper's CoT augmentation.",
            "reasoning_methods": [
                "naive direct zero-shot prompting (single-step direct answer)",
                "Chain-of-Thought (CoT) augmented prompting (multi-step intermediate attribute generation)"
            ],
            "reasoning_methods_description": "Same CoT pipeline as applied to other LMMs: generate facial features (FFC), propose a name, combine into demographic description D, and use D as context to produce final demographic labels; compared against direct prompting without intermediate steps.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Ablative-style comparison: MiniGPTv2 evaluated in native zero-shot mode and in CoT-augmented mode on FairFace/UTKFace/CACD; off-target rate and classification/regression metrics compared.",
            "task_or_benchmark": "Same demographic inference benchmark (UTKFace, FairFace, CACD) covering age regression and gender/ethnicity classification.",
            "performance_results": "CoT reduces off-target errors: reported FairFace off-target rate drops from 18.4% (MiniGPTv2 baseline) to 6.8% (MiniGPTv2 w/ COT). Numerical improvements in accuracy/Kappa/age metrics are reported in the paper's tables but are presented mostly as aggregate improvements; the off-target reduction is the clearest quantified effect shown for MiniGPTv2.",
            "qualitative_findings": "CoT substantially reduces the frequency of off-target or out-of-category textual outputs and increases correct labels; however, naive MiniGPTv2 without CoT exhibits high off-target rates and lower sensitivity to fine-grained visual cues.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Applying CoT to MiniGPTv2 meaningfully reduces off-target predictions and improves demographic inference reliability; CoT is an effective zero-shot prompting strategy across LMM architectures including MiniGPTv2.",
            "uuid": "e8296.1",
            "source_info": {
                "paper_title": "Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "InstructBLIP",
            "name_full": "InstructBLIP",
            "brief_description": "An instruction-tuned vision-language model based on Vicuna 13B used here for demographic inference, evaluated with native prompting and with Chain-of-Thought augmentation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InstructBLIP (Vicuna 13B-based)",
            "model_description": "An instruction-tuned LMM combining a vision encoder and a 13B language model (Vicuna-based), applied to zero-shot demographic inference tasks and with the paper's CoT prompting.",
            "reasoning_methods": [
                "naive direct zero-shot prompting (single-step direct answer)",
                "Chain-of-Thought (CoT) augmented prompting (multi-step intermediate attribute generation)"
            ],
            "reasoning_methods_description": "InstructBLIP evaluated both with direct response prompting and with the three-step CoT procedure (FFC -&gt; name -&gt; combined demographic description -&gt; final prediction) to improve interpretability and reduce off-target outputs.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Direct head-to-head comparison of native InstructBLIP responses vs. InstructBLIP w/ COT on FairFace and other datasets; primary metrics include off-target rate and classification/regression performance.",
            "task_or_benchmark": "Demographic inference on UTKFace, FairFace, and CACD (age, gender, ethnicity).",
            "performance_results": "Reported FairFace off-target rate decreases from 17.1% (InstructBLIP baseline) to 5.5% with CoT. Other accuracy/Kappa improvements are summarized in the paper's tables and show consistent improvements when CoT is applied.",
            "qualitative_findings": "CoT reduces irrelevant or out-of-category textual outputs and yields more specific, attribute-grounded justifications for predictions; naive InstructBLIP shows higher off-target tendencies without CoT.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Chain-of-Thought prompting significantly mitigates InstructBLIP's off-target predictions and improves demographic inference performance and interpretability in a zero-shot setting.",
            "uuid": "e8296.2",
            "source_info": {
                "paper_title": "Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "InternLM",
            "name_full": "InternLM (vision-language variant)",
            "brief_description": "An InternLM-based LMM (7B variant used in this study) evaluated for demographic inference with and without CoT prompting; shown to have low off-target rates that drop to zero with CoT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InternLM (7B, evaluated)",
            "model_description": "A vision-language model built upon the InternLM family (7B weights used) that maps visual inputs into the LLM embedding space and produces textual demographic predictions; tested in zero-shot and CoT-augmented modes.",
            "reasoning_methods": [
                "naive direct zero-shot prompting (single-step direct answer)",
                "Chain-of-Thought (CoT) augmented prompting (multi-step intermediate attribute generation)"
            ],
            "reasoning_methods_description": "InternLM was evaluated with straightforward direct prompting and with the paper's CoT pipeline (FFC + name prediction + combined demographic description used as prompt context) to improve outputs.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Comparison of InternLM baseline vs. InternLM w/ COT on FairFace/UTKFace/CACD, measuring off-target rate and standard classification/regression metrics.",
            "task_or_benchmark": "Demographic inference tasks on UTKFace, FairFace, and CACD datasets (age/gender/ethnicity).",
            "performance_results": "InternLM baseline already shows a low off-target rate (reported ~0.1% in the paper) which the authors report as reduced to 0.0% with CoT. Performance on accuracy/Kappa and age metrics also improve with CoT as summarized in the paper.",
            "qualitative_findings": "Even models with low baseline off-target tendencies benefit from CoT by becoming fully on-target and more interpretable; CoT supplies explicit intermediate attributes that can be inspected to understand model decisions.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Chain-of-Thought prompting can eliminate the remaining off-target predictions even for InternLM, further improving reliability and interpretability in demographic inference without any fine-tuning.",
            "uuid": "e8296.3",
            "source_info": {
                "paper_title": "Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "CoT (general)",
            "name_full": "Chain-of-Thought augmented prompting (this work's implementation)",
            "brief_description": "A specific, zero-shot Chain-of-Thought prompting pipeline introduced/used in this paper to improve demographic inference from images by eliciting intermediate attribute descriptions (FFC) and name suggestions, then using them as context for final predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought augmented prompting (applied to LMMs)",
            "model_description": "A three-step, purely textual CoT pipeline: (1) generate Facial Feature Collection (FFC) of fine-grained visual attributes; (2) generate a plausible name for the person based on the image; (3) combine FFC and name into a demographic description used to prompt final subcategory predictions (age/gender/race). No fine-tuning or few-shot examples required.",
            "reasoning_methods": [
                "decompositional intermediate attribute generation (FFC)",
                "auxiliary attribute inference (name prediction)",
                "contextualized final decision-making using assembled demographic description"
            ],
            "reasoning_methods_description": "Decompositional intermediate attribute generation: elicit a structured list of facial attributes (age/gender/ethnicity cues). Name prediction: use LMM's captioning/naming ability to suggest a plausible first/last name as an auxiliary ethnicity cue. Contextualized final decision: feed the assembled demographic description into the final prompt for each demographic subtask. This CoT is applied zero-shot as a prompt engineering technique.",
            "reasoning_diversity": "diverse (explicit multi-step reasoning pipeline as compared to single-step direct prompting)",
            "reasoning_diversity_experimental_setup": "Applied CoT vs. direct prompting across multiple LMMs (LLaVA, MiniGPTv2, InstructBLIP, InternLM) and datasets (UTKFace, FairFace, CACD); measured off-target rates and standard accuracy/regression metrics to quantify differences.",
            "task_or_benchmark": "Demographic inference on UTKFace, FairFace, and CACD datasets (age regression; gender and ethnicity classification).",
            "performance_results": "CoT consistently reduces off-target rates across models (examples: MiniGPTv2 18.4% -&gt; 6.8%; InstructBLIP 17.1% -&gt; 5.5%; InternLM ~0.1% -&gt; 0.0%; LLaVA baseline 0.0% -&gt; remains 0.0% or eliminated where present). CoT improves age estimation metrics for LLaVA on CACD (MAE 6.65 -&gt; 5.75 years; RMSE 8.73 -&gt; 8.17; R^2 0.428 -&gt; 0.500; MAPE 22.51% -&gt; 15.99%).",
            "qualitative_findings": "CoT yields attribute-rich intermediate outputs that increase interpretability and reduce off-target/out-of-distribution textual answers; authors note CoT can occasionally introduce biases by over-weighting contextual or non-facial cues (e.g., clothing) when only a single image is available.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "The explicitly multi-step CoT prompting pipeline substantially improves zero-shot demographic inference by reducing off-target outputs, increasing accuracy/Kappa and regression performance, and providing interpretable intermediate explanations — enabling LMM performance competitive with supervised baselines without fine-tuning.",
            "uuid": "e8296.4",
            "source_info": {
                "paper_title": "Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cocot: Contrastive chain-of-thought prompting for large multimodal models with multiple image inputs",
            "rating": 2,
            "sanitized_title": "cocot_contrastive_chainofthought_prompting_for_large_multimodal_models_with_multiple_image_inputs"
        },
        {
            "paper_title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "rating": 1,
            "sanitized_title": "instructblip_towards_generalpurpose_visionlanguage_models_with_instruction_tuning"
        },
        {
            "paper_title": "Visual instruction tuning",
            "rating": 1,
            "sanitized_title": "visual_instruction_tuning"
        }
    ],
    "cost": 0.01389875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CHAIN-OF-THOUGHT PROMPTING FOR DEMOGRAPHIC INFERENCE WITH LARGE MULTIMODAL MODELS
24 May 2024</p>
<p>Yongsheng Yu 
Department of Computer Science
University of Rochester</p>
<p>Jiebo Luo jluo@cs.rochester.edu 
Department of Computer Science
University of Rochester</p>
<p>CHAIN-OF-THOUGHT PROMPTING FOR DEMOGRAPHIC INFERENCE WITH LARGE MULTIMODAL MODELS
24 May 2024BF1C554803E1BB1009F0221FCA62B617arXiv:2405.15687v1[cs.CV]demographic inferenceChain-of-Thought promptinglarge vision-language modelsand multimodal understanding
Conventional demographic inference methods have predominantly operated under the supervision of accurately labeled data, yet struggle to adapt to shifting social landscapes and diverse cultural contexts, leading to narrow specialization and limited accuracy in applications.Recently, the emergence of large multimodal models (LMMs) has shown transformative potential across various research tasks, such as visual comprehension and description.In this study, we explore the application of LMMs to demographic inference and introduce a benchmark for both quantitative and qualitative evaluation.Our findings indicate that LMMs possess advantages in zeroshot learning, interpretability, and handling uncurated 'inthe-wild' inputs, albeit with a propensity for off-target predictions.To enhance LMM performance and achieve comparability with supervised learning baselines, we propose a Chain-of-Thought augmented prompting approach, which effectively mitigates the off-target prediction issue.</p>
<p>INTRODUCTION</p>
<p>Demographic inference [1] involves analyzing population data based on characteristics like age [2,3], gender [4], and ethnicity [5,6].Essential in fields such as sociology, marketing, and public health, it helps identify societal trends, understand consumer behavior, and inform public policies.It is crucial for addressing issues such as aging populations and population migration, significantly impacting societal and economic strategies.</p>
<p>Despite these advancements, current traditional artificial intelligence approaches to demographical inference typically rely on domain-specific, real-world labeled paired data.These methods are often limited in scope, lacking a comprehensive understanding of the Knowledge of Demographic Data, diversity among individuals and groups, cultural backgrounds, and macro socio-economic contexts.As a result, they fail to accurately predict demographic information in in-  Please tell me the age of the person in the photo.the-wild data and do not offer an interpretable process or suggestions for demographic inference.</p>
<p>Recently, the emergence of AI foundational models, led by Large Language Models (LLMs) [7], has provided a new paradigm.Characterized by their massive parameter count, training on broad and diverse data, and exceptional versatility, these models can adapt to a wide range of tasks through further training, such as evolving into large multimodal models (LMMs) [8,9] with the training of visual encoding models.This approach, capable of understanding and processing both image and text inputs, introduces a new paradigm in methodological design across various research fields.</p>
<p>In this study, we propose an integrated demographic inference benchmark and evaluate it on a series of popular opensource LMMs, namely LLaVA [8], MiniGPTv2 [9], Instruct-BLIP [10], and internLM [11].These LMMs have already demonstrated impressive capabilities in understanding visual content and answering questions with a broad common-sense Fig. 2: Conceptual workflow of our Chain-of-Thought prompting approach for demographic inference.The process begins with task prompts guiding LMM to articulate the facial features of the individual in the image, followed by name suggestions.Subsequently, the LMM employs these attributes as demographic descriptions to deduce age, race, and gender, and provides post-hoc explanations for its conclusions.</p>
<p>understanding and high natural language proficiency.Our investigation confirms that LMMs possess three main advantages over traditional demographic inference methods:</p>
<ol>
<li>
<p>Interpretability.While traditional deep learning approaches yield results conforming to the format of training data labels, LMMs can easily allow the model to explain its predictions through post-hoc questioning.</p>
</li>
<li>
<p>Zero-shot prediction.In our study, LMMs do not require any real-world labeled data for downstream task fine-tuning or few-shot data for context-providing instruction and can be directly applied to visual-based demographic inference test datasets.</p>
</li>
<li>
<p>Proficiency in handling out-of-domain data.Traditional methods, trained on domain-specific data like cropped standard facial images, struggle with visual inputs like half-body portraits.In contrast, LMMs can accurately predict demographics in in-the-wild images, see Figure 1(b) and (c).</p>
</li>
</ol>
<p>Nevertheless, in in-domain test datasets, we have observed a significant gap between the performance of naive LMMs in a zero-shot setting and traditional supervised learning approaches.Additionally, the high degree of response flexibility in the language model of LMMs can lead to offtarget predictions (see Fig. 1(a)), a common issue when replacing traditional supervised models with LMMs for predictions.To address this, we also propose a Chain-of-Thought approach, enhancing LMMs' prompting with a two-step intermediary questioning process to obtain demographic feature descriptions.Our main contributions are as follows:</p>
<p>• We are the first, to the best of our knowledge, to incorporate LMMs for demographic inference.</p>
<p>• We propose an integrated benchmark to assess the performance of models in demographic inference and study the effectiveness of LMMs on in-the-wild data.</p>
<p>• We introduce a Chain-of-Thought strategy to improve the performance of LMMs on demographic inference tasks while reducing the rate of off-target predictions.</p>
<p>RELATED WORKS</p>
<p>Demographic Inference</p>
<p>In the evolving landscape of demographic inference, particularly in race, age, and gender prediction, recent advancements have been marked by the integration of deep learning.Gender prediction has similarly benefited from deep learning models [4] applied to ocular and real-world images.In age prediction, the use of discriminant subspace learning [2] extracts aging patterns from facial images, presenting them as distinct manifold structures, thereby enhancing both age estimation and our visual understanding of the aging process.Deep learning systems, especially those trained on time-variant features [4], have shown remarkable results in age prediction from text and images.In this study, our primary focus is on the analysis and interpretation of three key demographic attributes: gender, age, and race.</p>
<p>Large Multimodal Models</p>
<p>Large Multimodal Models (LMMs) [8,10] represent a leap in AI technology, synthesizing the nuanced reasoning capabilities of Large Language Models (LLMs) [7] with the perceptual insights of vision-language (VL) models.The transformative aspect of LMMs lies in their ability to perform sophis-Macro Question: What are the characteristics of the person in this picture?</p>
<p>Demographic Description Generation</p>
<p>Request: Can you roughly suggest a name for him/her based on their facial features?</p>
<p>Request: To analyze the provided image and answer the above question, generating a fine-grained facial image description containing:</p>
<p>1.A series of ethnicity features (such as skin color, eye shape, nose bridge height, hair color) to answer the question.2. A series of age features (such as cheekbones, eye size, beard, chin) to answer the question.</p>
<ol>
<li>A series of gender features (such as crow feet, facial fat, hair under-eye bags, age spots, eyelid drooping) to answer the question.</li>
</ol>
<p>Demographic Reasoning</p>
<p>Given the facial image and its description.Please tell me the race of the person in the image, choosing one from the following options: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino.Respond only with the chosen race and do not include any other characters.</p>
<p>[ ] [ ]</p>
<p>LMM</p>
<p>Given the facial image and its description.ticated tasks that require concurrent understanding and generation of multi-modal data, such as in visual question answering [12] and social media understanding [13].Our research diverges from traditional LMM applications, pioneering the use of these models for demographic inference.Unlike prior works which primarily leveraged LMMs for tasks like visual question answering or object recognition, our approach explores the untapped potential of LMMs in deciphering demographic information.</p>
<p>Chain-of-Thought in LLMs</p>
<p>The advent of Chain-of-Thought (CoT) prompting in Large Language Models (LLMs) has emerged as a significant development in AI, enhancing model reasoning capabilities.</p>
<p>CoT enables models to process and articulate intermediate steps or 'thoughts' during problem-solving, akin to a humanlike reasoning process.This method contrasts with traditional direct answer generation, offering a more transparent and interpretable approach.CoT, along with its extensions like self-consistency, Tree-of-Thought, and Graph-of-Thought, enriches the interaction between users and AI models, particularly in complex tasks requiring detailed explanations or step-by-step reasoning.The implementation of CoT in multimodal contexts further extends its utility, allowing for intricate inference across both textual and visual inputs.This approach demonstrates the evolving sophistication of AI models in mirroring human cognitive processes and their application in diverse problem-solving scenarios.</p>
<p>METHODOLOGY</p>
<p>Problem Formulation</p>
<p>Our task can be conceptualized as a text generation problem under multimodal prompting.The core input to our model is a single-person photo featuring a face, upon which our demographical models infer demographic information such as gender, age, and ethnicity implicitly from visual features, based on textual task instructions.We assess the method's performance using distance metrics for paired data that are either continuous or discrete, aligning with the dataset's label data type.For dataset and experimental settings, please refer to the subsequent sections.</p>
<p>Large Multimodal Models</p>
<p>Based on text dialogues in large language models f (•) parameterized by θ, LMMs are multimodal models that incorporate both visual and linguistic modalities as inputs.Typically, they receive a collection of images I and a related textual task prompt P , and the large language model, based on the task prompt and visual input, can infer and respond in text form.More specifically, to process visual inputs, language models first map each modality to a shared representational space using pre-trained encoding models, i.e., the visual encoding model embeds I into the textual space, resulting in ϕ(I), which is then combined with tokenized language embeddings τ (P ) and fed into the large language model for textual response R. R = f (ϕ(I), τ (P ); θ).</p>
<p>The submodules mentioned above may vary in network architecture, pretraining methods, and parameters across dif-  'Others' [16] in race categories includes those not White, Black, Asian, or Indian.Zoom in for a better view.</p>
<p>ferent LMMs, such as LLaVA [8] and MiniGPTv2 [9], but generally follow the outlined pipeline.</p>
<p>Chain-of-thought Augmented Prompting</p>
<p>We find that prompting LMMs with native task instructions does not achieve performance comparable to supervised methods, as shown in Table 1.This is attributed to the excessive versatility of LMMs in handling tasks without being fine-tuned for demographic prediction, where straightforward task instructions fail to fully leverage the prior of LMM in a zero-shot manner.</p>
<p>To maximize the inherent inferential potential of LMMs, we propose enhancing textual prompts with a Chain-of-Thought approach.Our first step is to have LMMs directly interpret the input image, generating a detailed Facial Feature Collection (FFC) P facial , thus avoiding the need for groundtruth caption data.As illustrated in Figure 3, P facial systematically constructs key attributes about age, gender, and ethnicity, e.g., gender-related attributes might include crow's feet, facial fat, hair color, under-eye bags, and age spots.R (1) = f (ϕ(I), τ (P marco ), τ (P facial ); θ).</p>
<p>(
)2
Ethnicity is a more challenging category to discern, as predictions rely not only on facial features, especially for an attribute like ethnicity with strong multicultural aspects and common knowledge dependencies (like birth geographical location, and nationality).In the second step, inspired by [1], we consider the person's name as a key feature for discerning ethnicity.Using the naming (captioning) capability of LMMs, we predict the last and first names based on the input image.</p>
<p>We combine the estimated name P name with facial feature collection P facial to form a demographic description D, as seen in Figure 3, which is used to enhance the original textual prompt.R (2) = f (ϕ(I), τ (P marco ), τ (P name ); θ).</p>
<p>(
)3
It is noteworthy that in the demographic description, the roles of attributes are not entirely orthogonal; for instance, gender-related attributes might also aid in age prediction.Therefore, in the third step, we integrate the entire demographic description into the prompt for each demographic subcategory prediction as contextual supplementation.Thus, we utilize the generated demographic description as intermediate Chain-of-Thought inference steps, explicitly enhancing the overall input prompt for response generation, as follows:
P (3) in = [I, D(R (2) , R (2) ), P age|gender|race ].(4)
Under Chain-of-Thought enhancement, we modify the previous formula 1, and the final response text R generated by LMMs is as follows:
R (3) = f (P (3) in ; ϕ, τ, θ).
(
)5
This leverages the capability LMM of to represent images in high-dimensional spaces and their descriptive power in language modeling.Notably, our enhancement method requires no fine-tuning or few-shot in-context prompting, remaining entirely text-based and zero-shot, and does not require any annotated facial image captions.</p>
<p>EXPERIMENTS</p>
<p>This paper integrates a benchmark for evaluating LMMs in demographic inference, incorporating three facial image datasets: CACD [18], FairFace [5], and UTKFace [16].</p>
<p>• CACD comprises over 160,000 images of 2,000 celebrities, offering a broad spectrum in terms of age and appearance diversity.The ground truth age labels span from 14 to 54 years.</p>
<p>• UTKFace includes more than 20,000 facial images with age, gender, and ethnicity annotations.The age range is extensive, from 0 to 116 years.It encompass a wide array of subjects across different ethnicities and a balance of male female participants.</p>
<p>• Fairface consists of 108,501 images, each labeled with age, gender, and race.It represents seven racial groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino.</p>
<p>Since there is no need to use these datasets' training sets to fine-tune LMMs for any downstream tasks, we select images from the official test splits of each dataset to serve as our benchmark for evaluating LMMs.</p>
<p>Models</p>
<p>On our benchmark, we evaluate four popular LMMs: LLaVA [8], InstructBLIP [10], InternLM [11], and MiniG-PTv2 [9].To implement these LMMs, we utilize their official code and pre-trained weights.Specifically, we select the 13B weights version of LLaVA 1.5 [15], the InstructBLIP based on vicuna 13B weights, MiniGPTv2 based on the llama-2 7B [7] language model, and the 7B version of InternLM.</p>
<p>We apply our Chain-of-Thought enhancement to LLaVA.It's noteworthy that commercial models like GPT4V [19] and Gemini, which are not open-sourced and may update versions frequently, are not the focus of this paper regarding performance state-of-the-art (SOTA).Therefore, we only use the aforementioned open-source LMMs, ensuring experimental stability and reproducibility.</p>
<p>Quantitative analysis</p>
<p>We apply our proposed COT strategy to LLaVA, referred to as "LLaVA w/ COT".For the continuous age annotations in the UTKFace and CACD datasets, we utilize metrics such as MAE, MAPE, and RMSE for evaluation.For other discrete attributes, performance is measured using Accuracy and Kappa.As shown in Figure 2(a) and Figure 1, "LLaVA w/ COT" exhibits top performance on both UTKFace and Fair-Face datasets, particularly in achieving 0.00% off-target rate and high Kappa scores, indicating precise and reliable predictions across age, gender, and ethnicity categories, and significantly outperforming naive LMMs.As shown in Figure 3, "LLaVA w/ COT" variant outperforms LMM baselines in the quantitative analysis on the CACD dataset, exhibiting the lowest Mean Absolute Error (MAE) at 5.75 and the highest R 2 value of 0.500, indicating its strong predictive accuracy and its ability to explain half of the variance in the dataset.Beyond LMMs, our method exhibits comparable performance to traditional supervised learning approaches.In Table 2(a), "LLaVA w/ COT" outperforms state-of-the-art supervised learning methods in gender prediction accuracy.</p>
<p>Qualitative analysis</p>
<p>In a qualitative comparison, CoT-augmented LMMs show a improvement in demographic prediction accuracy over naive LMMs.As shown in Figure 4, it is evident in the consistent green (correct) labels across the augmented models, indicating precise age, gender, and race identification.The naive LMMs, however, frequently mislabel, particularly in the 'Others' race category, highlighting the difficulty of handling diverse demographic attributes without augmentation.Enhanced models display a higher degree of specificity and sensitivity to visual cues.An interesting observation is that utilizing CoT-augmented prompting for inference may occasionally be counterproductive when dependent on a singe image.For instance, in the top-left case of Figure 4, the CoTaugmented prediction identifies the subject as female, potentially due to the interpretation of clothing details, such as the cute bunny-style outfit, which might be stereotypically associated with female infants.However, the ground truth is male.</p>
<p>Off-target prediction</p>
<p>Due to the high flexibility of the language model of LMMs, texts generated by them do not necessarily fit the ground truth categories, even with explicit prompting (as shown in Figure 3 with instruction "not to output any other characters").This off-target phenomenon occasionally enhances the readability of responses or provides some interpretable information, which is acceptable in contexts that do not require quantitative evaluation.However, it also results in ambiguous or irrelevant answers, as seen in Figure 1(a).Therefore, we consider offtarget prediction in demographic inference as an issue with LMMs.To fairly assess LMMs, for each demographic inference, if the output is off-target, the inference is repeated until an on-target prediction occurs or N iterations have passed.As shown in Table 2(b), some LMMs still exhibit a high offtarget rate despite this procedure.After N repetitions, if the prediction remains off-target, we use the CLIP [20] to encode prediction and measure the similarity between prediction and ground truth categories, selecting the highest similarity as a replacement.We set N to 5 in our experiments.Importantly, we find that the proposed COT method effectively mitigates off-target prediction issues.As depicted in Figure 2(b), MiniGPTv2 with COT sees a reduction in off-target rate from 18.4% to 6.8%, and InstructBLIP with COT drops from 17.1% to 5.5%.Both InternLM and LLaVA achieve a 0.0% off-target rate when combined with COT, underscoring the efficacy of our COT in enhancing model precision.</p>
<p>CONCLUSION</p>
<p>Our study presents a comprehensive exploration of LMMs for demographic inference, establishing a benchmark for the field.We demonstrate that LMMs, when enhanced with a Chain-of-Thought prompting strategy, not only provide interpretable, zero-shot predictions that excel in uncurated scenarios but also show promising results in adapting to the diverse and dynamic nature of demographic data.The introduction of this Chain-of-Thought approach significantly reduces offtarget predictions, aligning LMM performance closely with that of traditional supervised learning methods, as evidenced by our rigorous quantitative and qualitative evaluations across multiple datasets.Our findings highlight the potential of LMMs to revolutionize demographic inference, offering a flexible and robust alternative to existing AI models.We believe that the integration of such AI models can greatly benefit societal-scale applications, from policy formulation to personalized services, by providing nuanced and accurate demographic analyses.</p>
<p>Please tell me the race of the person in the image.</p>
<p>(</p>
<p>such as Hispanic, Latino, Middle Eastern) White Please tell me the age of the person in the photo.25.7 years old.70-80 years old.This estimation is based on the visible signs such as pronounced wrinkles, fragile appearance of the skin and prominent veins.</p>
<p>Fig. 1 :
1
Fig. 1: Analysis of traditional Supervised Learning (SL) methods and naive LMMs in demographic inference task.</p>
<p>2 .
2
tell me the age/race/gender of the person in the image.The person in the image is White.3. The person in the image is a male.</p>
<p>Fig. 3 :
3
Fig. 3: Full prompt example of the Chain-of-Thought augmented prompting for demographic inference.</p>
<p>Fig. 4 :
4
Fig. 4: Qualitative comparison of naive LMMs and COT-augmented LMMs.Red answers are incorrect, green ones are correct.'Others'[16] in race categories includes those not White, Black, Asian, or Indian.Zoom in for a better view.</p>
<p>Table 1 :
1
Quantitative experimental results on the UTKFace dataset.Gray row indicates supervised learning baselines.
AgeGenderEthnicityOff-targetMSERMSE MAE R 2MAPEAccuracy Kappa Accuracy KappaRateFLAC [6]-------0.9200-0.0%MWR [14]--4.37------0.0%MiniGPTv2 [9]132.25 11.507.280.6600 103.36% 0.95400.9071 0.59200.46567.9%InstructBLIP [10] 241.39 15.548.350.3794 232.51% 0.89800.7914 0.61400.413920.2%LLaVA [15]60.147.755.130.8454 24.03%0.96200.9236 0.85600.80050.2%LLaVA w/ COT55.047.424.800.8585 15.35%0.97500.9496 0.88100.83580.0%</p>
<p>Table 2 :
2
Comparative performance and off-target prediction mitigation on the FairFace dataset.(a)Quantitative results on FairFace.Gray row indicates supervised learning baselines.
(b) Mitigation of off-target prediction on FairFace via COT.AgeGenderEthnicityOff-target Rate AccuracyAccuracy Kappa Accuracy Kappa Accuracy KappaMiniGPTv218.4%0.316FairFace [5]0.597-0.942-0.937-MiniGPTv2 w/ COT6.8%0.473MiVOLO [3]0.611-0.957---InternLM0.1%0.500MiniGPTv2 [9]0.3160.1750.9250.8490.4720.373InternLM w/ COT0.0%0.568InternLM [11]0.5000.3990.9550.9100.4620.351InstructBLIP17.1%0.291InstructBLIP [10] 0.2910.1530.8740.7440.5390.449InstructBLIP w/ COT5.5%0.434LLaVA [15]0.4990.3980.9560.9120.6180.546LLaVA0.0%0.499LLaVA w/ COT0.5770.4900.9580.9160.6920.634LLaVA w/ COT0.0%0.577</p>
<p>Table 3 :
3
Quantitative experimental results on the CACD dataset.Gray row indicates supervised learning baselines.
MSE RMSE MAER 2MAPECORAL [17]-7.485.25--MWR [14]--4.37--MiniGPTv2 [9]92.099.607.18 0.309 25.24%InternLM [11]83.449.137.39 0.374 25.18%InstructBLIP [10] 78.448.866.27 0.412 21.85%LLaVA [15]76.198.736.65 0.428 22.51%LLaVA w/ COT66.698.175.75 0.500 15.99%</p>
<p>Face off: Polarized public opinions on personal face mask usage during the covid-19 pandemic. Neil Yeung, Jonathan Lai, Jiebo Luo, Big Data. IEEE. 2020</p>
<p>Human age estimation with regression on discriminative aging manifold. Yun Fu, Thomas S Huang, IEEE Transactions on Multimedia. 2008</p>
<p>Mivolo: Multi-input transformer for age and gender estimation. Maksim Kuprashevich, Irina Tolstykh, arXiv:2307.046162023arXiv preprint</p>
<p>Hand image-based human age estimation using a time distributed cnn-gru. Mohamed Ait Abderrahmane, Ibrahim Guelzim, Abdelkaher Ait, Abdelouahad , 2020ICDABI. IEEE</p>
<p>Multitask learning of cascaded cnn for facial attribute classification. Ni Zhuang, Yan Yan, Si Chen, Hanzi Wang, ICPR. 2018</p>
<p>Flac: Fairness-aware representation learning by suppressing attribute-class associations. Ioannis Sarridis, Christos Koutlis, arXiv:2304.142522023arXiv preprintSymeon Papadopoulos, and Christos Diou</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, abs/2307.09288CoRR. 2023</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, NeurIPS. 2023</p>
<p>MINIGPT-V2: Large language model as a unified interface for vision-language multi-task learning. Jun Chen, Deyao Li, Zhu Xiaoqian, Shen Xiang, arXiv:2310.094782023arXiv preprint</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, 2023</p>
<p>Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. Pan Zhang, Xiaoyi Dong, Bin Wang, abs/2309.15112CoRR. 2023</p>
<p>Cocot: Contrastive chain-of-thought prompting for large multimodal models with multiple image inputs. Daoan Zhang, Junming Yang, Hanjia Lyu, Zijian Jin, Yuan Yao, Mingkai Chen, Jiebo Luo, vol. abs/2401.025822024</p>
<p>Gpt-4v(ision) as A social media analysis engine. Hanjia Lyu, Jinfa Huang, Daoan Zhang, Yongsheng Yu, Xinyi Mou, Jinsheng Pan, Zhengyuan Yang, Zhongyu Wei, Jiebo Luo, abs/2311.075472023</p>
<p>Moving window regression: A novel approach to ordinal regression. Nyeong-Ho Shin, Seon-Ho Lee, Chang-Su Kim, CVPR. 2022</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, arXiv:2310.037442023arXiv preprint</p>
<p>Age progression/regression by conditional adversarial autoencoder. Zhifei Zhang, Yang Song, Hairong Qi, CVPR. 2017</p>
<p>Rank consistent ordinal regression for neural networks with application to age estimation. Wenzhi Cao, Vahid Mirjalili, Sebastian Raschka, 2020PR</p>
<p>Cross-age reference coding for age-invariant face recognition and retrieval. Chu-Song Bor-Chun Chen, Winston H Chen, Hsu, ECCV. 2014</p>
<p>Gpt-4v(ision) system card. Openai, 2023</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris , ICML. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
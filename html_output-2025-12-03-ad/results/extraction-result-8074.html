<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8074 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8074</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8074</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-82e440220e29d6c2c5866f9cb40e522ca0c8a22d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/82e440220e29d6c2c5866f9cb40e522ca0c8a22d" target="_blank">Human-like Summarization Evaluation with ChatGPT</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> ChatGPT was able to complete annotations relatively smoothly using Likert scale scoring, pairwise comparison, Pyramid, and binary factuality evaluation, and it outperformed commonly used automatic evaluation metrics on some datasets.</p>
                <p><strong>Paper Abstract:</strong> Evaluating text summarization is a challenging problem, and existing evaluation metrics are far from satisfactory. In this study, we explored ChatGPT's ability to perform human-like summarization evaluation using four human evaluation methods on five datasets. We found that ChatGPT was able to complete annotations relatively smoothly using Likert scale scoring, pairwise comparison, Pyramid, and binary factuality evaluation. Additionally, it outperformed commonly used automatic evaluation metrics on some datasets. Furthermore, we discussed the impact of different prompts, compared its performance with that of human evaluation, and analyzed the generated explanations and invalid responses.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8074.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8074.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT vs Humans (SummEval Likert)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301) compared to expert human annotators on SummEval using Likert-scale scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct comparison of ChatGPT's Likert-scale ratings (relevance, faithfulness/consistency, fluency, coherence) against three expert human annotators on the SummEval dataset, reported with Spearman's rho at sample/system/dataset levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI gpt-3.5-turbo-0301 via ChatGPT API; temperature=0; max_tokens=256; default other parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three expert annotators (paper computes correlation between each annotator and the average of the three)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho (system-level, per-dimension reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.889</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Overall gap to human experts on most correlation measures; sensitivity to prompt wording; occasional invalid responses (~<=1%); explanations may be self-consistent but not always correct</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT achieves high system-level correlation on several dimensions (e.g., fluency system-level 0.889) and in one case (system-level fluency) exceeds a single human annotator (annotator_? 0.843), but overall correlations to the average human are lower than individual expert-to-average correlations; ChatGPT's scores are more reproducible and less variable than human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Lower cost (authors estimate ~10-20x cheaper for SummEval), high reproducibility/control (deterministic when temperature set to 0), produces explanations alongside scores which aid interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompted with Likert template (Figure 1/5), temperature=0, ChatGPT returns 1-5 ratings on four dimensions; correlations computed at sample, system, and dataset levels against human expert average.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-like Summarization Evaluation with ChatGPT', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8074.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8074.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT vs Humans (Newsroom Likert)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301) compared to dataset human annotations on Newsroom using Likert-scale scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of ChatGPT Likert-scale ratings (coherence, fluency, informativeness, relevance) against human judgments on the Newsroom dataset, reported with Spearman's rho at multiple levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Newsroom</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI gpt-3.5-turbo-0301 via ChatGPT API; temperature=0; max_tokens=256.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human annotations (original Newsroom human evaluation labels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho (system-level, per-dimension reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.821</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Does not consistently outperform top automatic metrics (e.g., some BARTScore variants outperform on Newsroom); sensitivity to prompt design; occasional invalid responses</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT attains competitive correlations with human judgments on Newsroom (e.g., system-level coherence rho=0.821) but is not uniformly superior to state-of-the-art automatic metrics; performance highly prompt-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Flexible evaluation format; reproducibility and lower variance compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Likert-scale prompt adapted to Newsroom dimensions; temperature=0; correlations computed at sample, system, and dataset levels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-like Summarization Evaluation with ChatGPT', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8074.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8074.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT Pairwise (TLDR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301) used as pairwise comparator vs human pairwise judgments on TLDR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of ChatGPT's pairwise comparison judgments (choose better of two summaries) against human pairwise labels on the TLDR dataset, reported as classification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization (pairwise comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TLDR</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI gpt-3.5-turbo-0301; temperature=0; instructed to answer 'Summary 0' or 'Summary 1' (Figure 2 template).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided pairwise human judgments (from TLDR dataset annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy (agreement with human pairwise choices)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.6178</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Overall gap vs human experts (accuracy not perfect); limited advantage except on some datasets; sensitivity to prompt phrasing</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT achieves moderate accuracy (61.78%) on TLDR pairwise comparisons, comparable to or slightly better than many automatic metrics but not perfect; authors note no large advantage except on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Can perform pairwise comparisons directly without training specialized metrics; reproducible and low-cost.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Pairwise prompt (Figure 2), no explanations requested; accuracy computed against human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-like Summarization Evaluation with ChatGPT', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8074.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8074.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT Pyramid (REALSumm)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301) performing Pyramid-style SCU detection vs human SCU annotations on REALSumm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT was prompted to mark whether each semantic content unit (SCU) is present in a generated summary (Pyramid method) and compared to human annotations; reported accuracy of binary determinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization (Pyramid SCU detection)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>REALSumm</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI gpt-3.5-turbo-0301; temperature=0; prompted with SCU list and asked Yes/No for each (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>REALSumm dataset human SCU annotations (used as ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy (binary SCU detection)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.6436</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Does not reach perfect agreement with humans; may misclassify some SCUs; sensitivity to prompt design</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT attains moderate accuracy (64.36%) on SCU detection, outperforming some baselines and comparable to other automatic metrics; overall performance is reasonable but not equivalent to human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Can follow Pyramid-style multi-SCU prompts directly; scalable compared to manual SCU annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Pyramid prompt template (Figure 3) with up to 16 SCUs per sample; ChatGPT returns Yes/No per SCU; accuracy computed against human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-like Summarization Evaluation with ChatGPT', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8074.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8074.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT Binary Factuality (QAGS_CNN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301) binary factuality classification vs human labels on QAGS (CNN subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Binary factuality judgments (is sentence supported by article?) by ChatGPT compared to human annotations on the QAGS CNN subset, reported as accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization (binary factuality per sentence)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>QAGS_CNN</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI gpt-3.5-turbo-0301; temperature=0; prompted with Article+Sentence binary template (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>QAGS dataset human factuality annotations</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy (binary factuality determination)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.8488</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Less reliable on some domains (see QAGS_XSUM lower accuracy); occasional incorrect explanations and small fraction of invalid responses</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT achieves high accuracy (84.88%) on QAGS_CNN factuality judgments, outperforming some automatic classifiers and showing strong factual-detection ability on this subset.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Strong factuality detection on some domains; can produce explanations in addition to labels.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Binary factuality prompt (Figure 4), sentence-level judgments extracted and compared to human binary labels; accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-like Summarization Evaluation with ChatGPT', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8074.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8074.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT Binary Factuality (QAGS_XSUM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301) binary factuality classification vs human labels on QAGS (XSUM subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Binary factuality judgments by ChatGPT compared to human annotations on the QAGS XSUM subset, reported as accuracy; lower performance than on CNN subset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization (binary factuality per sentence)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>QAGS_XSUM</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI gpt-3.5-turbo-0301; temperature=0; binary factuality prompt (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>QAGS dataset human factuality annotations</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Accuracy (binary factuality determination)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.7573</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Lower accuracy on XSUM domain than on CNN; domain sensitivity leading to reduced agreement with humans</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT performs worse on the XSUM subset (75.73% accuracy) than on CNN (84.88%), indicating domain-dependent limitations in factuality detection.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Still outperforms some classical factuality classifiers on XSUM and provides explanations; scalable and reproducible.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Binary factuality prompt applied to QAGS XSUM; sentence-level Yes/No judgments compared to human labels; accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-like Summarization Evaluation with ChatGPT', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8074.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8074.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT Comparative Summary (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate qualitative comparison of ChatGPT-as-judge vs human evaluation across datasets and methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper-level synthesis of strengths, limitations, failure modes, and practical advantages observed when using ChatGPT as an automated human-like judge for summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Human-like Summarization Evaluation with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval, Newsroom, TLDR, REALSumm, QAGS (as reported across experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI gpt-3.5-turbo-0301 via API; experiments used temperature=0 and prompt templates mirroring human evaluation protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Dataset-provided human annotations and three expert annotators on SummEval</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho (Likert) and Accuracy (pairwise/Pyramid/binary), reported per dataset/method</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Overall gap to human experts on many measures; sensitivity to prompt formulation; domain-dependent failures (e.g., lower QAGS_XSUM accuracy); small rate of invalid/ refusal responses (~<=1%); explanations may be plausible but not guaranteed correct</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT can reproduce many human evaluation protocols, often outperforming traditional automatic metrics on some datasets; it is highly prompt-sensitive and exhibits domain-dependent failure modes; explanations tend to be self-consistent and align with its scores but require validation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Much lower cost (authors estimate ~10-20x cheaper for SummEval), reproducibility and controllability (deterministic outputs with low randomness), flexibility to mimic multiple human evaluation methods, and ability to provide explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Multiple prompt templates mirroring human evaluation methods (Likert, pairwise, Pyramid, binary factuality). Majority of experiments used a single ChatGPT run per sample (temperature=0), with post-processing to extract labels/scores; authors measured sample/system/dataset-level Spearman correlations and accuracy against human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-like Summarization Evaluation with ChatGPT', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is chatgpt a good nlg evaluator? a preliminary study <em>(Rating: 2)</em></li>
                <li>Chatgpt as a factual inconsistency evaluator for abstractive text summarization <em>(Rating: 2)</em></li>
                <li>Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences <em>(Rating: 2)</em></li>
                <li>Gpteval: Nlg evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8074",
    "paper_id": "paper-82e440220e29d6c2c5866f9cb40e522ca0c8a22d",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "ChatGPT vs Humans (SummEval Likert)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0301) compared to expert human annotators on SummEval using Likert-scale scoring",
            "brief_description": "Direct comparison of ChatGPT's Likert-scale ratings (relevance, faithfulness/consistency, fluency, coherence) against three expert human annotators on the SummEval dataset, reported with Spearman's rho at sample/system/dataset levels.",
            "citation_title": "Human-like Summarization Evaluation with ChatGPT",
            "mention_or_use": "use",
            "paper_title": "Human-like Summarization Evaluation with ChatGPT",
            "evaluation_task": "summarization",
            "dataset_name": "SummEval",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "OpenAI gpt-3.5-turbo-0301 via ChatGPT API; temperature=0; max_tokens=256; default other parameters.",
            "human_evaluator_type": "Three expert annotators (paper computes correlation between each annotator and the average of the three)",
            "agreement_metric": "Spearman's rho (system-level, per-dimension reported)",
            "agreement_score": 0.889,
            "reported_loss_aspects": "Overall gap to human experts on most correlation measures; sensitivity to prompt wording; occasional invalid responses (~&lt;=1%); explanations may be self-consistent but not always correct",
            "qualitative_findings": "ChatGPT achieves high system-level correlation on several dimensions (e.g., fluency system-level 0.889) and in one case (system-level fluency) exceeds a single human annotator (annotator_? 0.843), but overall correlations to the average human are lower than individual expert-to-average correlations; ChatGPT's scores are more reproducible and less variable than human annotators.",
            "advantages_of_llm_judge": "Lower cost (authors estimate ~10-20x cheaper for SummEval), high reproducibility/control (deterministic when temperature set to 0), produces explanations alongside scores which aid interpretability.",
            "experimental_setting": "Prompted with Likert template (Figure 1/5), temperature=0, ChatGPT returns 1-5 ratings on four dimensions; correlations computed at sample, system, and dataset levels against human expert average.",
            "uuid": "e8074.0",
            "source_info": {
                "paper_title": "Human-like Summarization Evaluation with ChatGPT",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ChatGPT vs Humans (Newsroom Likert)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0301) compared to dataset human annotations on Newsroom using Likert-scale scoring",
            "brief_description": "Comparison of ChatGPT Likert-scale ratings (coherence, fluency, informativeness, relevance) against human judgments on the Newsroom dataset, reported with Spearman's rho at multiple levels.",
            "citation_title": "Human-like Summarization Evaluation with ChatGPT",
            "mention_or_use": "use",
            "paper_title": "Human-like Summarization Evaluation with ChatGPT",
            "evaluation_task": "summarization",
            "dataset_name": "Newsroom",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "OpenAI gpt-3.5-turbo-0301 via ChatGPT API; temperature=0; max_tokens=256.",
            "human_evaluator_type": "Dataset-provided human annotations (original Newsroom human evaluation labels)",
            "agreement_metric": "Spearman's rho (system-level, per-dimension reported)",
            "agreement_score": 0.821,
            "reported_loss_aspects": "Does not consistently outperform top automatic metrics (e.g., some BARTScore variants outperform on Newsroom); sensitivity to prompt design; occasional invalid responses",
            "qualitative_findings": "ChatGPT attains competitive correlations with human judgments on Newsroom (e.g., system-level coherence rho=0.821) but is not uniformly superior to state-of-the-art automatic metrics; performance highly prompt-dependent.",
            "advantages_of_llm_judge": "Flexible evaluation format; reproducibility and lower variance compared to humans.",
            "experimental_setting": "Likert-scale prompt adapted to Newsroom dimensions; temperature=0; correlations computed at sample, system, and dataset levels.",
            "uuid": "e8074.1",
            "source_info": {
                "paper_title": "Human-like Summarization Evaluation with ChatGPT",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ChatGPT Pairwise (TLDR)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0301) used as pairwise comparator vs human pairwise judgments on TLDR",
            "brief_description": "Evaluation of ChatGPT's pairwise comparison judgments (choose better of two summaries) against human pairwise labels on the TLDR dataset, reported as classification accuracy.",
            "citation_title": "Human-like Summarization Evaluation with ChatGPT",
            "mention_or_use": "use",
            "paper_title": "Human-like Summarization Evaluation with ChatGPT",
            "evaluation_task": "summarization (pairwise comparison)",
            "dataset_name": "TLDR",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "OpenAI gpt-3.5-turbo-0301; temperature=0; instructed to answer 'Summary 0' or 'Summary 1' (Figure 2 template).",
            "human_evaluator_type": "Dataset-provided pairwise human judgments (from TLDR dataset annotations)",
            "agreement_metric": "Accuracy (agreement with human pairwise choices)",
            "agreement_score": 0.6178,
            "reported_loss_aspects": "Overall gap vs human experts (accuracy not perfect); limited advantage except on some datasets; sensitivity to prompt phrasing",
            "qualitative_findings": "ChatGPT achieves moderate accuracy (61.78%) on TLDR pairwise comparisons, comparable to or slightly better than many automatic metrics but not perfect; authors note no large advantage except on some datasets.",
            "advantages_of_llm_judge": "Can perform pairwise comparisons directly without training specialized metrics; reproducible and low-cost.",
            "experimental_setting": "Pairwise prompt (Figure 2), no explanations requested; accuracy computed against human labels.",
            "uuid": "e8074.2",
            "source_info": {
                "paper_title": "Human-like Summarization Evaluation with ChatGPT",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ChatGPT Pyramid (REALSumm)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0301) performing Pyramid-style SCU detection vs human SCU annotations on REALSumm",
            "brief_description": "ChatGPT was prompted to mark whether each semantic content unit (SCU) is present in a generated summary (Pyramid method) and compared to human annotations; reported accuracy of binary determinations.",
            "citation_title": "Human-like Summarization Evaluation with ChatGPT",
            "mention_or_use": "use",
            "paper_title": "Human-like Summarization Evaluation with ChatGPT",
            "evaluation_task": "summarization (Pyramid SCU detection)",
            "dataset_name": "REALSumm",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "OpenAI gpt-3.5-turbo-0301; temperature=0; prompted with SCU list and asked Yes/No for each (Figure 3).",
            "human_evaluator_type": "REALSumm dataset human SCU annotations (used as ground truth)",
            "agreement_metric": "Accuracy (binary SCU detection)",
            "agreement_score": 0.6436,
            "reported_loss_aspects": "Does not reach perfect agreement with humans; may misclassify some SCUs; sensitivity to prompt design",
            "qualitative_findings": "ChatGPT attains moderate accuracy (64.36%) on SCU detection, outperforming some baselines and comparable to other automatic metrics; overall performance is reasonable but not equivalent to human annotators.",
            "advantages_of_llm_judge": "Can follow Pyramid-style multi-SCU prompts directly; scalable compared to manual SCU annotation.",
            "experimental_setting": "Pyramid prompt template (Figure 3) with up to 16 SCUs per sample; ChatGPT returns Yes/No per SCU; accuracy computed against human labels.",
            "uuid": "e8074.3",
            "source_info": {
                "paper_title": "Human-like Summarization Evaluation with ChatGPT",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ChatGPT Binary Factuality (QAGS_CNN)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0301) binary factuality classification vs human labels on QAGS (CNN subset)",
            "brief_description": "Binary factuality judgments (is sentence supported by article?) by ChatGPT compared to human annotations on the QAGS CNN subset, reported as accuracy.",
            "citation_title": "Human-like Summarization Evaluation with ChatGPT",
            "mention_or_use": "use",
            "paper_title": "Human-like Summarization Evaluation with ChatGPT",
            "evaluation_task": "summarization (binary factuality per sentence)",
            "dataset_name": "QAGS_CNN",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "OpenAI gpt-3.5-turbo-0301; temperature=0; prompted with Article+Sentence binary template (Figure 4).",
            "human_evaluator_type": "QAGS dataset human factuality annotations",
            "agreement_metric": "Accuracy (binary factuality determination)",
            "agreement_score": 0.8488,
            "reported_loss_aspects": "Less reliable on some domains (see QAGS_XSUM lower accuracy); occasional incorrect explanations and small fraction of invalid responses",
            "qualitative_findings": "ChatGPT achieves high accuracy (84.88%) on QAGS_CNN factuality judgments, outperforming some automatic classifiers and showing strong factual-detection ability on this subset.",
            "advantages_of_llm_judge": "Strong factuality detection on some domains; can produce explanations in addition to labels.",
            "experimental_setting": "Binary factuality prompt (Figure 4), sentence-level judgments extracted and compared to human binary labels; accuracy reported.",
            "uuid": "e8074.4",
            "source_info": {
                "paper_title": "Human-like Summarization Evaluation with ChatGPT",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ChatGPT Binary Factuality (QAGS_XSUM)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0301) binary factuality classification vs human labels on QAGS (XSUM subset)",
            "brief_description": "Binary factuality judgments by ChatGPT compared to human annotations on the QAGS XSUM subset, reported as accuracy; lower performance than on CNN subset.",
            "citation_title": "Human-like Summarization Evaluation with ChatGPT",
            "mention_or_use": "use",
            "paper_title": "Human-like Summarization Evaluation with ChatGPT",
            "evaluation_task": "summarization (binary factuality per sentence)",
            "dataset_name": "QAGS_XSUM",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "OpenAI gpt-3.5-turbo-0301; temperature=0; binary factuality prompt (Figure 4).",
            "human_evaluator_type": "QAGS dataset human factuality annotations",
            "agreement_metric": "Accuracy (binary factuality determination)",
            "agreement_score": 0.7573,
            "reported_loss_aspects": "Lower accuracy on XSUM domain than on CNN; domain sensitivity leading to reduced agreement with humans",
            "qualitative_findings": "ChatGPT performs worse on the XSUM subset (75.73% accuracy) than on CNN (84.88%), indicating domain-dependent limitations in factuality detection.",
            "advantages_of_llm_judge": "Still outperforms some classical factuality classifiers on XSUM and provides explanations; scalable and reproducible.",
            "experimental_setting": "Binary factuality prompt applied to QAGS XSUM; sentence-level Yes/No judgments compared to human labels; accuracy reported.",
            "uuid": "e8074.5",
            "source_info": {
                "paper_title": "Human-like Summarization Evaluation with ChatGPT",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ChatGPT Comparative Summary (qualitative)",
            "name_full": "Aggregate qualitative comparison of ChatGPT-as-judge vs human evaluation across datasets and methods",
            "brief_description": "Paper-level synthesis of strengths, limitations, failure modes, and practical advantages observed when using ChatGPT as an automated human-like judge for summarization.",
            "citation_title": "Human-like Summarization Evaluation with ChatGPT",
            "mention_or_use": "use",
            "paper_title": "Human-like Summarization Evaluation with ChatGPT",
            "evaluation_task": "summarization",
            "dataset_name": "SummEval, Newsroom, TLDR, REALSumm, QAGS (as reported across experiments)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "judge_model_details": "OpenAI gpt-3.5-turbo-0301 via API; experiments used temperature=0 and prompt templates mirroring human evaluation protocols.",
            "human_evaluator_type": "Dataset-provided human annotations and three expert annotators on SummEval",
            "agreement_metric": "Spearman's rho (Likert) and Accuracy (pairwise/Pyramid/binary), reported per dataset/method",
            "agreement_score": null,
            "reported_loss_aspects": "Overall gap to human experts on many measures; sensitivity to prompt formulation; domain-dependent failures (e.g., lower QAGS_XSUM accuracy); small rate of invalid/ refusal responses (~&lt;=1%); explanations may be plausible but not guaranteed correct",
            "qualitative_findings": "ChatGPT can reproduce many human evaluation protocols, often outperforming traditional automatic metrics on some datasets; it is highly prompt-sensitive and exhibits domain-dependent failure modes; explanations tend to be self-consistent and align with its scores but require validation.",
            "advantages_of_llm_judge": "Much lower cost (authors estimate ~10-20x cheaper for SummEval), reproducibility and controllability (deterministic outputs with low randomness), flexibility to mimic multiple human evaluation methods, and ability to provide explanations.",
            "experimental_setting": "Multiple prompt templates mirroring human evaluation methods (Likert, pairwise, Pyramid, binary factuality). Majority of experiments used a single ChatGPT run per sample (temperature=0), with post-processing to extract labels/scores; authors measured sample/system/dataset-level Spearman correlations and accuracy against human annotations.",
            "uuid": "e8074.6",
            "source_info": {
                "paper_title": "Human-like Summarization Evaluation with ChatGPT",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "rating": 2
        },
        {
            "paper_title": "Chatgpt as a factual inconsistency evaluator for abstractive text summarization",
            "rating": 2
        },
        {
            "paper_title": "Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences",
            "rating": 2
        },
        {
            "paper_title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 1
        }
    ],
    "cost": 0.01336575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Human-like Summarization Evaluation with ChatGPT</h1>
<p>Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, Xiaojun Wan<br>Wangxuan Institute of Computer Technology, Peking University<br>{gaomingqi, xjyin, wanxiaojun}@pku.edu.cn<br>{ruanjie, sunrenliang}@stu.pku.edu.cn<br>yangshiping@bupt.edu.cn</p>
<h4>Abstract</h4>
<p>Evaluating text summarization is a challenging problem, and existing evaluation metrics are far from satisfactory. In this study, we explored ChatGPT's ability to perform humanlike summarization evaluation using four human evaluation methods on five datasets. We found that ChatGPT was able to complete annotations relatively smoothly using Likert scale scoring, pairwise comparison, Pyramid, and binary factuality evaluation. Additionally, it outperformed commonly used automatic evaluation metrics on some datasets. Furthermore, we discussed the impact of different prompts, compared its performance with that of human evaluation, and analyzed the generated explanations and invalid responses.</p>
<h2>1 Introduction</h2>
<p>Text summarization is a task that involves generating a condensed version of one or multiple documents. Thanks to the advancements in deep learning-based techniques, automatic summarization has made significant strides. Specifically, the emergence of large language models such as InstructGPT has resulted in comparable performance to reference summaries written by humans, even in zero-shot settings (Zhang et al., 2023).</p>
<p>Evaluating text summarization, like other text generation tasks, is a challenging problem. While human evaluation is considered the gold standard, it is expensive and time-consuming. As a result, automatic evaluation metrics play a crucial role. ROUGE (Lin, 2004) and its variants, which are based on reference summaries and n-gram matching, are widely accepted and used in various types of summarization. However, surface-level word matching cannot accurately reflect the quality of the summary. Additionally, it is challenging to evaluate the factual accuracy of the summary without utilizing the source document. Recently, evaluation metrics based on pre-trained models such as</p>
<p>BERTScore (Zhang et al., 2020) and BARTScore (Yuan et al., 2021) have achieved better correlation with human judgments. Factuality evaluation methods based on entailment classification, such as FactCC (Kryscinski et al., 2020), and question answering, such as FEQA (Durmus et al., 2020), have also been used to evaluate the factual consistency of summaries. Despite the existence of advanced automatic evaluation metrics, their performance, usability, and interpretability are still far from satisfactory.</p>
<p>Large language models (LLMs) offer completely different possibilities for the automatic evaluation of summarization. GPT-3 (Brown et al., 2020) has the ability of in-context learning, and instruction tuning allows LLMs to align with human evaluation (Ouyang et al., 2022). These two abilities make it possible for LLMs to mimic the behavior of human evaluators, who generally evaluate summaries by understanding examples and instructions. We refer to this automatic evaluation method that views large models as human evaluators as human-like automatic evaluation. The most prominent feature of this evaluation method is its flexibility, which unifies all types of automatic evaluation in form and can simulate many of the practices of human evaluators. Unlike previous automatic evaluation metrics that give one or more numerical values as evaluation results, the evaluation results of this human-like automatic evaluation are fully reflected in the generated responses, which may include scoring, comparison, labels, and explanations.</p>
<p>We conducted an evaluation of the evaluation ability of ChatGPT, a recently popular LLM, using four commonly used human evaluation methods for summarization. The methods include Likert scale scoring, pairwise comparison, Pyramid (Nenkova and Passonneau, 2004), and binary factuality evaluation. Our findings indicate that ChatGPT is capable of completing annotations relatively smoothly using these methods. In addition,</p>
<p>our results demonstrate that ChatGPT outperforms commonly used automatic evaluation metrics on some datasets. Furthermore, we analyzed the impact of different prompts, compared the performance of ChatGPT with human evaluation, and examined the quality of the generated explanations and invalid responses.</p>
<h2>2 Preliminary</h2>
<h3>2.1 Automatic Evaluation Metrics</h3>
<p>We select several evaluation metrics that are commonly used in summarization:</p>
<p>ROUGE (Lin, 2004), which is the dominant automatic evaluation metric in summarization, is widely used by researchers. The most commonly used ROUGE measures are ROUGE-1, ROUGE-2, and ROUGE-L, which evaluate the similarity between two texts based on the overlap of unigrams, bigrams, and the longest common sequence.</p>
<p>BERTScore (Zhang et al., 2020) assesses the similarity between two texts at the token level by measuring the soft overlap using contextual embeddings from BERT. Similarly, MoverScore (Zhao et al., 2019) uses n-gram embeddings that are pooled from BERT to compute the semantic distance between two texts at the n-gram level.</p>
<p>BARTScore (Yuan et al., 2021) ${ }^{1}$ views evaluation as a natural language generation task and considers that when the quality of the generated text is higher, BART is more likely to generate it from the source text or the reference, or to generate the reference from it. BARTScore can be flexibly applied to evaluate text from various perspectives.</p>
<p>FactCC $^{2}$ and DAE $^{3}$ are two factuality metrics based on classification. When evaluating a summary, we use NLTK ${ }^{4}$ to split it into individual sentences and classify each one as factually correct or not. The factual score of the summary is then calculated as the ratio of sentences that are factually correct.</p>
<h3>2.2 Human Evaluation Methods</h3>
<p>There are several commonly used methods for human evaluation, including the Likert scale scoring and pairwise comparison for general text generation, as well as Pyramid and binary factuality</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>evaluation specifically designed for summarization. After introducing each method, we will list the datasets we used that were annotated in this way.</p>
<p>Likert scale scoring is the most common method for human evaluation. Specifically, given a source document and a generated summary, annotators rate the summary on several dimensions. Typically, this is an absolute evaluation, meaning each summary is evaluated individually without explicit comparison to other summaries. Dimensions usually include factual consistency, informativeness, fluency, etc. The rating scale is usually 1 (worst) to 5 (best). We used SummEval (Fabbri et al., 2021) and Newsroom datasets (Grusky et al., 2018).</p>
<p>Pairwise comparison is a relative human evaluation method. Given a source document and two generated summaries, annotators choose the one that is of higher quality. This method is used in reinforcement learning based human feedback for summarization. We used the TLDR dataset (Stiennon et al., 2022).</p>
<p>Pyramid (Nenkova and Passonneau, 2004) is a human evaluation method designed for summarization that is based on reference summaries. Prior to human annotation, several semantic content units (SCUs) are extracted from the reference summary. For each SCU, annotators judge whether it presents in the generated summary. For single-document summarization, the final score of the summary is the proportion of SCUs it contains. We used the REALSumm dataset (Bhandari et al., 2020).</p>
<p>Binary factuality evaluation is a method for evaluating the factual correctness of summaries. Given a source document and a sentence in the generated summary, annotators judge whether the sentence is faithful to the source document. We used the QAGS dataset (Wang et al., 2020).</p>
<h2>3 Experiments</h2>
<h3>3.1 Model and Parameters</h3>
<p>We used the ChatGPT API (gpt-3.5-turbo-0301) provided by OpenAI for our experiments. To reduce randomness, we set temperature to 0 . In addition, we set max_tokens to 256. We kept the default values for other parameters.</p>
<h3>3.2 Prompt Design</h3>
<p>When designing prompts, we made it as identical as possible to the original instructions of human evaluations.</p>
<p>Evaluate the quality of summaries written for a news article. Rate each summary on four dimensions: {Dimension_1}, {Dimension_2}, {Dimension_3}, and {Dimension_4}. You should rate on a scale from 1 (worst) to 5 (best).</p>
<p>Article: ${$ Article $}$
Summary: ${$ Summary $}$
Figure 1: The template for Likert scale scoring.</p>
<p>Figure 1 shows the template for Likert scale scoring. ChatGPT is asked to rate four dimensions at a time. For SummEval, the four dimensions are relevance, faithfulness ${ }^{5}$, fluency, and coherence. For Newsroom, the four dimensions are relevance, informativeness, fluency, and coherence. Figure 2 shows the template for pairwise comparison.</p>
<p>Given a new article, which summary is better? Answer "Summary 0" or "Summary 1". You do not need to explain the reason.</p>
<p>Article: ${$ Article $}$
Summary 0: ${$ Summary_0 $}$
Summary 1: ${$ Summary_1 $}$
Figure 2: The template for pairwise comparison.
Figure 3 shows the template for Pyramid. The number of SCUs depends on the content of the reference summary, up to 16 .</p>
<p>You are given a summary and some semantic content units. For each semantic unit, mark "Yes" if it can be inferred from the summary, otherwise mark "No".</p>
<p>Summary: {Summary }
Semantic content units:</p>
<ol>
<li>${$ SCU_1 $}$</li>
<li>${$ SCU_2 $}$
$\qquad$
n. ${$ SCU_n $}$</li>
</ol>
<p>Figure 3: The template for Pyramid.
Figure 4 shows the template for binary factuality</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>evaluation. The sentences are from the generated summaries.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Is the sentence supported by the article?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Answer "Yes" or "No".</td>
</tr>
<tr>
<td style="text-align: left;">Article: {Article }</td>
</tr>
<tr>
<td style="text-align: left;">Sentence: {Sentence}</td>
</tr>
</tbody>
</table>
<p>Figure 4: The template for binary factuality evaluation.</p>
<h3>3.3 Post-processing of Results</h3>
<p>The vast majority of ChatGPT responses contained annotation results, which can be extracted by some simple rules. For invalid responses, we considered them as failing to complete the tagging successfully and marked them as NAN (not a number).</p>
<h3>3.4 Evaluation</h3>
<p>For Likert scale scoring, we computed samplelevel, system-level, and dataset-level correlation with human judgments. For the other human evaluation methods, we calculated the accuracy of the responses generated by ChatGPT using human annotation as the answer.</p>
<h3>3.5 Results</h3>
<p>Tables 1 and 2 show that ChatGPT has a good ability to evaluate summaries with Likert scale scoring. On SummEval, it performs substantially better than the existing evaluation metrics. On Newsroom, it performs second only to BARTScore_s_h and BARTScore_cnn_s_h.</p>
<p>Tables 3, 4 and 5 illustrate that ChatGPT can also perform relatively smoothly on pairwise comparisons, Pyramid, and binary factuality evaluation. Nevertheless, from the current experimental results, ChatGPT has not yet shown a very large advantage except on QAGS_XSUM.</p>
<h2>4 Analysis and Discussion</h2>
<h3>4.1 Impact of different prompts</h3>
<p>We tried several different prompts on SummEval. As shown in Figure 5, more detailed step instructions and dimension definitions are added. These instructions and definitions are from the original human evaluation. In addition, we consider setting the system prompt as "You are a human annotator that rates the quality of summaries." when using ChatGPT API.</p>
<table>
<thead>
<tr>
<th></th>
<th>consistency</th>
<th></th>
<th></th>
<th>relevance</th>
<th></th>
<th></th>
<th>fluency</th>
<th></th>
<th></th>
<th>coherence</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Metric Name</td>
<td>sample</td>
<td>system</td>
<td>dataset</td>
<td>sample</td>
<td>system</td>
<td>dataset</td>
<td>sample</td>
<td>system</td>
<td>dataset</td>
<td>sample</td>
<td>system</td>
<td>dataset</td>
</tr>
<tr>
<td>ROUGE-1</td>
<td>0.153</td>
<td>0.744</td>
<td>0.137</td>
<td>0.326</td>
<td>0.744</td>
<td>0.302</td>
<td>0.113</td>
<td>0.730</td>
<td>0.080</td>
<td>0.167</td>
<td>0.506</td>
<td>0.184</td>
</tr>
<tr>
<td>ROUGE-2</td>
<td>0.179</td>
<td>0.779</td>
<td>0.129</td>
<td>0.290</td>
<td>0.621</td>
<td>0.245</td>
<td>0.156</td>
<td>0.690</td>
<td>0.062</td>
<td>0.184</td>
<td>0.335</td>
<td>0.145</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>0.111</td>
<td>0.112</td>
<td>0.109</td>
<td>0.311</td>
<td>0.362</td>
<td>0.284</td>
<td>0.103</td>
<td>0.306</td>
<td>0.079</td>
<td>0.128</td>
<td>0.138</td>
<td>0.141</td>
</tr>
<tr>
<td>BERTScore</td>
<td>0.105</td>
<td>-0.077</td>
<td>0.118</td>
<td>0.312</td>
<td>0.324</td>
<td>0.362</td>
<td>0.189</td>
<td>0.246</td>
<td>0.150</td>
<td>0.284</td>
<td>0.477</td>
<td>0.317</td>
</tr>
<tr>
<td>MoverScore</td>
<td>0.151</td>
<td>0.679</td>
<td>0.150</td>
<td>0.318</td>
<td>0.724</td>
<td>0.294</td>
<td>0.126</td>
<td>0.687</td>
<td>0.119</td>
<td>0.159</td>
<td>0.474</td>
<td>0.178</td>
</tr>
<tr>
<td>BARTScore_s_h</td>
<td>0.299</td>
<td>0.800</td>
<td>0.269</td>
<td>0.264</td>
<td>0.524</td>
<td>0.363</td>
<td>0.243</td>
<td>0.614</td>
<td>0.187</td>
<td>0.322</td>
<td>0.477</td>
<td>0.335</td>
</tr>
<tr>
<td>BARTScore_h_r</td>
<td>0.097</td>
<td>0.606</td>
<td>0.101</td>
<td>0.178</td>
<td>0.147</td>
<td>0.246</td>
<td>0.002</td>
<td>0.261</td>
<td>0.000</td>
<td>0.017</td>
<td>-0.115</td>
<td>0.064</td>
</tr>
<tr>
<td>BARTScore_r_h</td>
<td>-0.075</td>
<td>-0.556</td>
<td>-0.090</td>
<td>-0.081</td>
<td>-0.112</td>
<td>-0.136</td>
<td>0.013</td>
<td>-0.212</td>
<td>0.019</td>
<td>0.044</td>
<td>0.165</td>
<td>-0.010</td>
</tr>
<tr>
<td>BARTScore_cnn_s_h</td>
<td>0.367</td>
<td>0.435</td>
<td>0.334</td>
<td>0.356</td>
<td>0.765</td>
<td>0.394</td>
<td>0.349</td>
<td>0.746</td>
<td>0.285</td>
<td>0.448</td>
<td>0.700</td>
<td>0.408</td>
</tr>
<tr>
<td>BARTScore_cnn_h_r</td>
<td>0.171</td>
<td>0.771</td>
<td>0.106</td>
<td>0.320</td>
<td>0.456</td>
<td>0.244</td>
<td>0.111</td>
<td>0.561</td>
<td>0.066</td>
<td>0.153</td>
<td>0.174</td>
<td>0.130</td>
</tr>
<tr>
<td>BARTScore_cnn_r_h</td>
<td>0.001</td>
<td>-0.079</td>
<td>-0.004</td>
<td>0.146</td>
<td>0.312</td>
<td>0.221</td>
<td>0.107</td>
<td>0.297</td>
<td>0.145</td>
<td>0.228</td>
<td>0.506</td>
<td>0.236</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>0.435</td>
<td>0.833</td>
<td>0.425</td>
<td>0.433</td>
<td>0.901</td>
<td>0.445</td>
<td>0.419</td>
<td>0.889</td>
<td>0.410</td>
<td>0.561</td>
<td>0.832</td>
<td>0.557</td>
</tr>
</tbody>
</table>
<p>Table 1: Spearman's $\rho$ of sample level, system level, and dataset level on SummEval.</p>
<table>
<thead>
<tr>
<th></th>
<th>coherence</th>
<th></th>
<th></th>
<th>fluency</th>
<th></th>
<th></th>
<th>informativeness</th>
<th></th>
<th></th>
<th>relevance</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Metric Name</td>
<td>sample</td>
<td>system</td>
<td>dataset</td>
<td>sample</td>
<td>system</td>
<td>dataset</td>
<td>sample</td>
<td>system</td>
<td>dataset</td>
<td>sample</td>
<td>system</td>
<td>dataset</td>
</tr>
<tr>
<td>ROUGE-1</td>
<td>0.095</td>
<td>0.429</td>
<td>0.100</td>
<td>0.104</td>
<td>0.429</td>
<td>0.064</td>
<td>0.130</td>
<td>0.286</td>
<td>0.149</td>
<td>0.147</td>
<td>0.357</td>
<td>0.122</td>
</tr>
<tr>
<td>ROUGE-2</td>
<td>0.025</td>
<td>0.321</td>
<td>0.080</td>
<td>0.047</td>
<td>0.321</td>
<td>0.045</td>
<td>0.078</td>
<td>0.250</td>
<td>0.158</td>
<td>0.090</td>
<td>0.357</td>
<td>0.124</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>0.064</td>
<td>0.357</td>
<td>0.079</td>
<td>0.072</td>
<td>0.357</td>
<td>0.045</td>
<td>0.089</td>
<td>0.214</td>
<td>0.137</td>
<td>0.106</td>
<td>0.321</td>
<td>0.101</td>
</tr>
<tr>
<td>BERTScore</td>
<td>0.148</td>
<td>0.429</td>
<td>0.169</td>
<td>0.170</td>
<td>0.429</td>
<td>0.154</td>
<td>0.131</td>
<td>0.286</td>
<td>0.196</td>
<td>0.163</td>
<td>0.357</td>
<td>0.176</td>
</tr>
<tr>
<td>MoverScore</td>
<td>0.162</td>
<td>0.429</td>
<td>0.173</td>
<td>0.120</td>
<td>0.429</td>
<td>0.112</td>
<td>0.188</td>
<td>0.286</td>
<td>0.232</td>
<td>0.195</td>
<td>0.357</td>
<td>0.192</td>
</tr>
<tr>
<td>BARTScore_s_h</td>
<td>0.679</td>
<td>0.964</td>
<td>0.656</td>
<td>0.670</td>
<td>0.964</td>
<td>0.615</td>
<td>0.646</td>
<td>0.821</td>
<td>0.645</td>
<td>0.604</td>
<td>0.893</td>
<td>0.588</td>
</tr>
<tr>
<td>BARTScore_h_r</td>
<td>0.329</td>
<td>0.286</td>
<td>0.302</td>
<td>0.292</td>
<td>0.286</td>
<td>0.261</td>
<td>0.419</td>
<td>0.429</td>
<td>0.386</td>
<td>0.363</td>
<td>0.357</td>
<td>0.386</td>
</tr>
<tr>
<td>BARTScore_r_h</td>
<td>-0.311</td>
<td>-0.571</td>
<td>-0.249</td>
<td>-0.215</td>
<td>-0.571</td>
<td>-0.232</td>
<td>-0.423</td>
<td>-0.750</td>
<td>-0.346</td>
<td>-0.334</td>
<td>-0.607</td>
<td>-0.305</td>
</tr>
<tr>
<td>BARTScore_cnn_s_h</td>
<td>0.653</td>
<td>0.893</td>
<td>0.623</td>
<td>0.640</td>
<td>0.893</td>
<td>0.596</td>
<td>0.616</td>
<td>0.750</td>
<td>0.592</td>
<td>0.567</td>
<td>0.786</td>
<td>0.557</td>
</tr>
<tr>
<td>BARTScore_cnn_h_r</td>
<td>0.239</td>
<td>0.429</td>
<td>0.215</td>
<td>0.235</td>
<td>0.429</td>
<td>0.165</td>
<td>0.284</td>
<td>0.429</td>
<td>0.239</td>
<td>0.267</td>
<td>0.464</td>
<td>0.221</td>
</tr>
<tr>
<td>BARTScore_cnn_r_h</td>
<td>0.316</td>
<td>0.429</td>
<td>0.333</td>
<td>0.353</td>
<td>0.429</td>
<td>0.330</td>
<td>0.242</td>
<td>0.286</td>
<td>0.289</td>
<td>0.245</td>
<td>0.357</td>
<td>0.292</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>0.484</td>
<td>0.821</td>
<td>0.476</td>
<td>0.480</td>
<td>0.607</td>
<td>0.471</td>
<td>0.521</td>
<td>0.607</td>
<td>0.508</td>
<td>0.524</td>
<td>0.714</td>
<td>0.521</td>
</tr>
</tbody>
</table>
<p>Table 2: Spearman's $\rho$ of sample level, system level, and dataset level on Newsroom.</p>
<table>
<thead>
<tr>
<th>Metric Name</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>ROUGE-1</td>
<td>0.5869</td>
</tr>
<tr>
<td>ROUGE-2_f</td>
<td>0.4997</td>
</tr>
<tr>
<td>ROUGE-L_f</td>
<td>0.5647</td>
</tr>
<tr>
<td>BARTScore</td>
<td>0.5674</td>
</tr>
<tr>
<td>MoverScore</td>
<td>0.5864</td>
</tr>
<tr>
<td>BARTScore_s_h</td>
<td>0.5858</td>
</tr>
<tr>
<td>BARTScore_h_r</td>
<td>0.6151</td>
</tr>
<tr>
<td>BARTScore_r_h</td>
<td>0.5317</td>
</tr>
<tr>
<td>BARTScore_cnn_s_h</td>
<td>0.5880</td>
</tr>
<tr>
<td>BARTScore_cnn_h_r</td>
<td>0.5934</td>
</tr>
<tr>
<td>BARTScore_cnn_r_h</td>
<td>0.5089</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>$\mathbf{0 . 6 1 7 8}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy of pairwise comparison on TLDR.</p>
<table>
<thead>
<tr>
<th>Metric Name</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>DAE</td>
<td>0.6304</td>
</tr>
<tr>
<td>FactCC</td>
<td>0.5362</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>$\mathbf{0 . 6 4 3 6}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy of the binary determination of SCUs on REALSumm.</p>
<table>
<thead>
<tr>
<th></th>
<th>QAGS_CNN</th>
<th>QAGS_XSUM</th>
</tr>
</thead>
<tbody>
<tr>
<td>DAE</td>
<td>0.8459</td>
<td>0.6360</td>
</tr>
<tr>
<td>FactCC</td>
<td>0.7731</td>
<td>0.4937</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>$\mathbf{0 . 8 4 8 8}$</td>
<td>$\mathbf{0 . 7 5 7 3}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy of binary factuality evaluation on QAGS.</p>
<p>Table 6 shows that changing prompts result in a significant change in the performance of the humanlike automatic evaluation using ChatGPT, especially in terms of system-level correlations. From the current results, these changes do not make it to achieve higher correlations with human judgments, except for a modest improvement in a few cases by adding dimension definitions alone.</p>
<h3>4.2 Comparison with human evaluation</h3>
<p>In terms of accuracy, there is still an overall gap between the current automatic human-like evaluations using ChatGPT compared to human experts. Table 6 illustrates that in most cases, the correlation between scores given by a human expert and the average of scores given by human experts is substantially better than ChatGPT at all levels. However, the correlation between ChatGPT and human evaluations ( 0.889 ) is already higher than that of a particular human expert ( 0.843 ) in terms of system-level correlation of fluency.</p>
<p>For variance and reproducibility, automatic human-like evaluations using ChatGPT are more controllable. It is easy to know from Table 6 that the scores of the same samples will not be identical between different human annotators. Belz et al. (2021) pointed out that reproducing the manual evaluation was difficult. In contrast, we can make the human-like manual evaluation based on</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sample</td>
<td style="text-align: center;">system</td>
<td style="text-align: center;">dataset</td>
<td style="text-align: center;">sample</td>
<td style="text-align: center;">system</td>
<td style="text-align: center;">dataset</td>
<td style="text-align: center;">sample</td>
<td style="text-align: center;">system</td>
<td style="text-align: center;">dataset</td>
<td style="text-align: center;">sample</td>
<td style="text-align: center;">system</td>
<td style="text-align: center;">dataset</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">0.832</td>
<td style="text-align: center;">0.557</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT+def</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.786</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.453</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.570</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT+def+ins</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">$-0.149$</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.396</td>
<td style="text-align: center;">$-0.079$</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.016</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.494</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT+sys_prompt</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.268</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.470</td>
</tr>
<tr>
<td style="text-align: center;">Annotator_0</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.884</td>
</tr>
<tr>
<td style="text-align: center;">Annotator_1</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">0.913</td>
</tr>
<tr>
<td style="text-align: center;">Annotator_2</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.743</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.747</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.820</td>
</tr>
</tbody>
</table>
<p>Table 6: Spearman's $\rho$ of sample level, system level, and dataset level on SummEval. Annotator_0, Annotator_1, Annotator_2 refer to the three expert annotators. We compute the correlation coefficient between the score given by a particular annotator and the average score of the three. "+def" means adding dimension definitions in the prompt. "+ins" means adding step instructions in the prompt. Please see the example in Figure 5 for dimension definitions and step instructions. "+sys_prompt" denotes setting system prompt.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Summary</th>
<th style="text-align: center;">Paul Merson was brought on with only seven minutes remaining in his team's 0-0 draw with Burnley. Andros Townsend scored the Tottenham midfielder in the 89th minute. Paul Merson had another dig at Andros Townsend after his appearance. The midfielder had been brought on to the England squad last week. Click here for all the latest arsenal news."</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Expert annotator</td>
<td style="text-align: center;">Relevance: $1.67(2,1,2)$ <br> Consistency (Faithfulness): $1.00(1,1,1)$ <br> Fluency: $3.00(4,2,3)$ <br> Coherence: $1.33(2,1,1)$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">Relevance: 2 <br> Faithfulness: 2 <br> Fluency: 2 <br> Coherence: 2 <br> Overall score: 2 <br> This summary is not very relevant to the article as it only mentions a few details and leaves out important information. It is not very faithful as it misrepresents the article by suggesting that Andros Townsend scored in the Tottenham game, when in fact he was only brought on as a substitute. The fluency and coherence are also poor, with awkward phrasing and lack of logical flow.</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT+def</td>
<td style="text-align: center;">Relevance: 2 <br> Consistency: 1 <br> Fluency: 2 <br> Coherence: 1 <br> Explanation: <br> The summary is not relevant as it does not capture the key points of the article. It is inconsistent with the facts in the original article, as it incorrectly states that Andros Townsend scored in the 89th minute, when in fact he was brought on in the 83rd minute. The fluency of the summary is poor, with grammatical errors and incomplete sentences. The coherence of the summary is also poor, as it does not flow well and does not make sense as a whole.</td>
</tr>
</tbody>
</table>
<p>Table 7: Examples of ratings and explanations generated by ChatGPT. The red parts show the factual errors of the summary. The blue parts show the explanations for fluency and coherence. The long source document is not shown. "Expert Annotator" shows the average of scores given by three expert annotators, with each person's score in parentheses.</p>
<p>Imagine you are a human annotator now. You will evaluate the quality of summaries written for a news article. Please follow these steps:</p>
<ol>
<li>Carefully read the news article, and be aware of the information it contains.</li>
<li>Read the proposed summary.</li>
<li>Rate the summary on four dimensions: relevance, consistency, fluency, and coherence. You should rate on a scale from 1 (worst) to 5 (best).</li>
</ol>
<p>Definitions are as follows:
Relevance: The rating measures how well the summary captures the key points of the article. Consider whether all and only the important aspects are contained in the summary.
Consistency: The rating measures whether the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up untrue information.
Fluency: This rating measures the quality of individual sentences, whether they are well-written and grammatically correct. Consider the quality of individual sentences.
Coherence: The rating measures the quality of all sentences collectively, to fit together and sound natural. Consider the quality of the summary as a whole.</p>
<p>The article and the summary are given below:
Article: ${$ Article $}$
Summary: ${$ Summary $}$
Figure 5: The template for Likert scale scoring with step instructions (in red) and dimension definitions (in orange) on SummEval.</p>
<p>ChatGPT reproducible by setting randomness parameters (e.g., temperature) at decoding time.</p>
<p>In terms of cost, it is cheaper to perform the human-like automatic evaluation. Taking SummEval as an example, in our experiments, the assessment of one summary consumed about 1000 tokens, and it took about $0.002 \times 1600=3.2$ USD ${ }^{6}$ to finish the evaluation on the whole dataset. Assuming that a single annotator spends 5 hours annotating the whole dataset. It costs $12 \times 5=60$ USD. It is estimated that the cost of human evaluation is about 10 to 20 times higher than human-like automatic evaluation using ChatGPT.</p>
<h3>4.3 The quality of generated explanations</h3>
<p>We sampled and examined the responses generated by ChatGPT on SummEval, and found the following characteristics of the explanations given by ChatGPT:</p>
<p>ChatGPT sometimes provides scores or labels followed by an explanation, even if it is not explicitly asked to provide the explanation in the prompt. Of course, it is possible to add a request such as "You do not need to explain." to the prompt so that</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>it does not generate an explanation, but the impact of this on the evaluation scores is unknown.</p>
<p>The explanations generated by ChatGPT are generally self-consistent but not necessarily correct. The generated explanations generally coincide with its scoring. For example, Table 7 shows that ChatGPT and ChatGPT+def both scored low for the faithfulness of the summary, and they both pointed out factual errors in the summary. However, the correctness of these explanations still needs further testing.</p>
<p>The combination of ChatGPT's explanations and scoring can better confirm whether it understands the requirements of the evaluation, for example, the dimension definitions. Without providing dimension definitions (see Figure 5), ChatGPT's understanding of fluency and coherence converged. After examining multiple samples we found that its explanations of the scoring of these two dimensions are placed together and the dataset-level correlation between the scoring of these two dimensions is 0.960 . ChatGPT is better able to distinguish between these two dimensions when dimension definitions are provided. Its explanations of the scoring of the two dimensions are separated and the dataset-</p>
<table>
<thead>
<tr>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Refuse to evaluate</td>
<td>I'm sorry, but the proposed summary is not acceptable. It contains inaccurate and irrelevant information, and it is not coherent or fluent. Please provide a new summary for evaluation.</td>
</tr>
<tr>
<td>Not evaluate as required</td>
<td>As an AI language model, I cannot evaluate the quality of summaries as it requires subjective judgment. However, I can provide feedback on the summary's coherence, fluency, and relevance. The proposed summary is poorly written and [......]</td>
</tr>
<tr>
<td>Write a new summary</td>
<td>As an AI language model, I cannot evaluate the quality of summaries as it requires subjective human judgment. However, I can provide a summary of the article that is more accurate and relevant:</td>
</tr>
<tr>
<td></td>
<td>Referee Mark Clattenburg has been [......]</td>
</tr>
<tr>
<td>Continue to write the summary</td>
<td>land movement overnight. The mass, which has risen some 50ft above sea level, measures roughly 1,000-1,640ft long, and 100ft wide. [......]</td>
</tr>
</tbody>
</table>
<p>Table 8: Examples of invalid responses generated by ChatGPT on SummEval.
level correlation between the two dimensions drops to 0.843 .</p>
<h3>4.4 Invalid responses</h3>
<p>ChatGPT sometimes generates invalid responses, but this fraction is only about $1 \%$ at most (see Table 9). As shown in Table 8, common invalid responses were refusing to evaluate, not evaluating as required, writing a new summary, and continuing to write the summary. The reason why invalid responses are generated needs to be further explored.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Invalid responses</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: right;">0.0000</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT+def</td>
<td style="text-align: right;">0.0003</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT+def+ins</td>
<td style="text-align: right;">0.0106</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT+sys_prompt</td>
<td style="text-align: right;">0.0013</td>
</tr>
</tbody>
</table>
<p>Table 9: Porportions of invalid responses generated by ChatGPT on SummEval.</p>
<h2>5 Related Work</h2>
<p>There are some concurrent studies using LLMs for human-like NLG evaluation. According to Kocmi and Federmann (2023), LLMs are currently the most advanced evaluators of translation quality. Wang et al. (2023) tested ChatGPT's ability to be an evaluator on three NLG meta-evaluation datasets. Ji et al. (2023) explored the effectiveness of ChatGPT in ranking model-generated content. Luo et al. (2023) investigated ChatGPT's ability to evaluate
factual consistency in summarization.. Liu et al. (2023) utilized ChatGPT and GPT-4 to assess the quality of NLG outputs with chain-of-thoughts.</p>
<h2>6 Conclusion</h2>
<p>From the above experiments using ChatGPT for human-like summarization evaluation, the key findings are as follows:</p>
<ul>
<li>ChatGPT has the ability to perform summarization evaluation using various human evaluation methods. In some instances, it attains a higher correlation with human judgments than existing evaluation metrics.</li>
<li>The performance of ChatGPT on summarization evaluation is highly dependent on prompt design.</li>
<li>Human-like evaluation with ChatGPT is more cost-effective and reproducible than human evaluation.</li>
<li>The explanation generated by ChatGPT is consistent with its scoring.</li>
</ul>
<h2>References</h2>
<p>Anya Belz, Anastasia Shimorina, Shubham Agarwal, and Ehud Reiter. 2021. The ReproGen shared task on reproducibility of human evaluations in NLG:</p>
<p>Overview and results. In Proceedings of the 14th International Conference on Natural Language Generation, pages 249-258, Aberdeen, Scotland, UK. Association for Computational Linguistics.</p>
<p>Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Reevaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9347-9359, Online. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are fewshot learners. Computing Research Repository, arXiv:2005.14165. Version 4.</p>
<p>Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 50555070, Online. Association for Computational Linguistics.</p>
<p>Alexander R. Fabbri, Wojciech Kryciski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.</p>
<p>Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 708-719, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Yunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun, Dongyu Pan, Baochang Ma, and Xiangang Li. 2023. Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences. Computing Research Repository, arXiv:2303.07610.</p>
<p>Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. Computing Research Repository, arXiv:2302.14520.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In</p>
<p>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg evaluation using gpt-4 with better human alignment.</p>
<p>Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for abstractive text summarization. Computing Research Repository, arXiv:2303.15621.</p>
<p>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 145-152, Boston, Massachusetts, USA. Association for Computational Linguistics.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Computing Research Repository, arXiv:2203.02155.</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2022. Learning to summarize from human feedback. Computing Research Repository, arXiv:2009.01325. Version 3.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is chatgpt a good nlg evaluator? a preliminary study. Computing Research Repository, arXiv:2303.04048.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Computing Research Repository, arXiv:2106.11520. Version 2.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. Computing Research Repository, arXiv:1904.09675. Version 3.</p>
<p>Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. 2023. Benchmarking large language models for news summarization. Computing Research Repository, arXiv:2301.13848.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Linguistics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://openai.com/pricing&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4533 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4533</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4533</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-8c5aab75826620559d33e99652f4cac9f6efd2fc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8c5aab75826620559d33e99652f4cac9f6efd2fc" target="_blank">CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation</a></p>
                <p><strong>Paper Venue:</strong> 2024 IEEE LLM Aided Design Workshop (LAD)</p>
                <p><strong>Paper TL;DR:</strong> CreativEval is presented, a framework for evaluating the creativity of LLMs within the context of generating hardware designs that quantifies four creative sub-components, fluency, flexibility, originality, and elaboration, and results indicate GPT-3.5 as the most creative model in generating hardware designs.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have proved effective and efficient in generating code, leading to their utilization within the hardware design process. Prior works evaluating LLMs’ abilities for register transfer level code generation solely focus on functional correctness. However, the creativity associated with these LLMs, or the ability to generate novel and unique solutions, is a metric not as well understood, in part due to the challenge of quantifying this quality.To address this research gap, we present CreativEval, a framework for evaluating the creativity of LLMs within the context of generating hardware designs. We quantify four creative sub-components, fluency, flexibility, originality, and elaboration, through various prompting and post-processing techniques. We then evaluate multiple popular LLMs (including GPT models, CodeLlama, and VeriGen) upon this creativity metric, with results indicating GPT-3.5 as the most creative model in generating hardware designs.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4533.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4533.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CreativEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CreativEval creativity evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework introduced in this paper to quantify the creativity of LLMs in hardware (Verilog) code generation by combining four cognitive subcomponents—fluency, flexibility, originality, and elaboration—into a single creativity score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>CreativEval</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>CreativEval evaluates LLM creativity in hardware code generation by measuring four subcomponents (fluency, flexibility, originality, elaboration) computed from multiple generated responses per prompt, functionality checks against testbenches, structural similarity scoring (via GNN4IP) against a golden reference, and simple aggregation: C = 0.25*F + 0.25*X + 0.25*O + 0.25*E. It uses t responses per prompt, filters functional outputs, computes uniqueness and similarity measures per prompt, and averages across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Fluency (quantity of unique functional solutions), Flexibility (ability to produce alternative implementations given an example), Originality (degree of dissimilarity to typical/ golden solution), Elaboration (ability to compose/instantiate provided smaller modules to build larger designs), plus baseline functionality (pass@10).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated across CodeLlama-7B, CodeLlama-13B, VeriGen-6B, VeriGen-16B, GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>CodeLlama: 7B/13B; VeriGen: 6B/16B; GPT-3.5/GPT-4 sizes not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Hardware design / digital integrated circuit (Verilog RTL) generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Engineering design solutions / implementation strategies (Verilog modules); not evaluating scientific theories per se</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Table I (selected): overall Creativity scores: GPT-3.5=0.2201, GPT-4=0.2107, CodeLlama-13B=0.2056, VeriGen-6B=0.2026, VeriGen-16B=0.1962, CodeLlama-7B=0.1658. Functionality (pass@10) values: GPT-4=0.3750, VeriGen-6B=0.3667, VeriGen-16B=0.3250, CodeLlama-13B=0.3167, GPT-3.5=0.3083, CodeLlama-7B=0.2417. Component highlights: fluency highest for GPT-4 (0.1644), flexibility highest for GPT-3.5 (0.1600), originality higher for CodeLlama models (~0.30), elaboration equal across several models at 0.3333 (dataset small).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: functionality checked programmatically via Icarus Verilog testbenches; structural similarity computed automatically with GNN4IP; metrics computed from these automated signals. No human/expert ratings used for the core creativity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation of outputs done by running generated Verilog against provided HDLBits testbenches (Icarus Verilog) for functional correctness; structural similarity validated using GNN4IP pre-existing model; no human-judgment correlation or inter-rater reliability reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Framework is tailored to hardware code generation (Verilog), so applicability to scientific theories/hypotheses is indirect; elaboration dataset is small (9 multimodule prompts) limiting statistical power; reliance on GNN4IP similarity imposes constraints (graph similarity may not capture semantic novelty beyond structural differences); thresholding for flexibility (s<0) is a simple heuristic; no human expert evaluation to validate creativity judgments; results may be biased by dataset (HDLBits) and prompt engineering; GPT model sizes unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>HDLBits-derived dataset: 111 single-module prompts (with golden solutions and testbenches) for functionality/fluency/originality and 9 multimodule prompts for elaboration; total pass@10 measured on 120 prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4533.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4533.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fluency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fluency (CreativEval subcomponent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantifies the average number of unique, functional Verilog implementations an LLM produces per prompt by generating multiple responses, filtering for functionality, and counting distinct implementations using graph-similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Fluency (CreativEval)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each prompt the LLM generates t responses. Each response is simulated against the prompt's testbench; functional responses are compared to the golden reference using GNN4IP to produce similarity scores. The count of unique similarity values among functional responses is normalized by t and averaged across prompts: F = (1/n) * sum_i (|S(R_i)| / t), where S(R_i) is the set of similarity scores for functional responses to prompt i.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Number of distinct functional implementations per prompt (distinctness derived from GNN4IP similarity), normalized by number of attempts; functional correctness required.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-7B/13B, VeriGen-6B/16B, GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>CodeLlama:7B/13B; VeriGen:6B/16B; GPT sizes not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Hardware design / Verilog</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Alternative engineering implementations/solutions</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Fluency scores (Table I): GPT-4=0.1644 (highest), CodeLlama-13B=0.1611, GPT-3.5=0.1343, CodeLlama-7B=0.1483, VeriGen-6B=0.1244, VeriGen-16B=0.1189.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Fully automated: functionality via Icarus Verilog; uniqueness via automated GNN4IP similarity scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Functional validation via testbenches; uniqueness via GNN4IP similarity values. No human adjudication of uniqueness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>GNN4IP similarity thresholds and score discretization determine what counts as 'unique'—may conflate structural equivalence with semantic novelty; dependent on number of samples t and randomness of generation; only valid for code-generation tasks where testbenches exist.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>111 single-module HDLBits prompts (t responses per prompt, t specified in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4533.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4533.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flexibility</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flexibility (CreativEval subcomponent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measures the LLM's ability to produce an alternative (different and functional) implementation when provided with an existing correct implementation (golden response).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Flexibility (CreativEval)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompts include a correct Verilog implementation; the LLM is asked to provide a different implementation for the same module. For each prompt t responses are generated and functional responses compared to the golden reference with GNN4IP. If the minimum similarity s across functional responses is below a threshold (paper uses s < 0 on GNN4IP's [-1,1] scale), that prompt counts as demonstrating flexibility. Flexibility X = (1/n) * sum_i T[min S(R_i)] where T(s)=1 if s<0 else 0.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Presence of at least one functional response per prompt whose similarity to the provided golden solution is below threshold (s < 0), indicating an alternative implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-7B/13B, VeriGen-6B/16B, GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>CodeLlama:7B/13B; VeriGen:6B/16B; GPT sizes not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Hardware design / Verilog</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Alternative implementations (rewrites) of the same functional specification</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Flexibility scores (Table I): GPT-3.5=0.1600 (highest), VeriGen-6B=0.1000, CodeLlama-13B=0.0260, VeriGen-16B=0.0556, GPT-4=0.0795, CodeLlama-7B=0.0000 (often copied the provided implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: functionality via testbenches and alternative-ness via GNN4IP similarity thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Functional validation with testbenches; GNN4IP similarity used to validate difference from golden solution; no human verification of 'different' designs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Binary threshold (s<0) is heuristic; GNN4IP similarity may not capture semantic differences beyond structural graphs; some models simply copied the provided code leading to low flexibility—this could reflect prompt-following rather than lack of creative alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Subset of HDLBits prompts where golden implementation is provided; same 111 prompts used for these tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4533.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4533.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Originality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Originality (CreativEval subcomponent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measures how dissimilar a functional generated solution is from the typical/golden implementation using GNN4IP similarity scores renormalized to a [0,1] originality scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Originality (CreativEval)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each prompt the minimum GNN4IP similarity value (min S(R_i)) among functional responses is taken; similarity s in [-1,1] is renormalized to [0,1] with 1 indicating maximal originality: O = (1/n) * sum_i ((-min S(R_i) + 1)/2).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Degree of dissimilarity to golden implementation as measured by GNN4IP graph similarity; higher renormalized values indicate more original (less similar) solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-7B/13B, VeriGen-6B/16B, GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>CodeLlama:7B/13B; VeriGen:6B/16B; GPT sizes not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Hardware design / Verilog</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Novelty of implementation relative to canonical solution</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Originality scores (Table I): CodeLlama-13B=0.3021, CodeLlama-7B=0.2926 (highest), VeriGen-16B=0.2771, VeriGen-6B=0.2527, GPT-3.5=0.2526, GPT-4=0.2657. GPT models tended to be closer to golden solutions, producing less original variants on average.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated via GNN4IP similarity renormalization; no human scoring of originality.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Functional correctness plus automated graph-similarity based measurement; no correlation to human judgments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Originality depends wholly on GNN4IP's notion of similarity, which may favor or penalize certain coding styles; a single minimum-similarity selection per prompt may miss other original but less extreme variations; no human-grounded originality validation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>111 single-module HDLBits prompts (functional responses compared to golden references).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4533.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4533.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elaboration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elaboration (CreativEval subcomponent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assesses whether an LLM can compose larger modules by reusing/invoking provided smaller modules (multi-modular design), counting prompts where any functional generated response instantiates the smaller modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Elaboration (CreativEval)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompts include multiple smaller Verilog modules and ask the LLM to build a larger module using them. For p prompts, n prompts have at least one functional response that uses the provided smaller modules; elaboration E = n / p. Functional correctness is required and usage of provided modules is detected programmatically.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Whether generated functional solution instantiates/uses the provided smaller modules (yes/no) and passes testbench, aggregated as fraction of prompts demonstrating such usage.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-7B/13B, VeriGen-6B/16B, GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>CodeLlama:7B/13B; VeriGen:6B/16B; GPT sizes not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Hardware design / Verilog</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Compositional multi-module design solutions</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Elaboration scores (Table I): all reported models show 0.3333 (based on the small multimodule set), indicating 1/3 of the 9 elaboration prompts had at least one functional response that used provided modules.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: functional checks via testbenches and automated detection of module instantiation/use in generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Functional testing and programmatic parsing to detect instantiation of smaller modules; no human verification of design quality/compositional correctness beyond testbench pass.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Very small sample (9 prompts) reduces reliability; detecting 'use' of modules is syntactic and may miss semantic reuse; difficulty for LLMs to synthesize larger modules even if they could in principle.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>9 multimodule HDLBits prompts drawn from AutoChip/HDLBits dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4533.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4533.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN4IP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GNN4IP (Graph neural network for IP piracy detection) similarity tool</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An existing graph-neural-network-based tool that converts Verilog modules into data-flow graphs and outputs a similarity score in [-1, 1] between two modules; used here to quantify structural similarity between generated and golden Verilog implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GNN4IP: Graph neural network for hardware intellectual property piracy detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>GNN4IP-based structural similarity</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Verilog modules are represented as data-flow graphs (DFGs), then GNN4IP computes a similarity score in [-1,1] where higher values indicate greater similarity. CreativEval uses these scores to determine uniqueness (fluency), alternative-ness (flexibility thresholding), and originality (renormalization of min similarity to [0,1]).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Graph-structural similarity between LLM-generated modules and golden/reference implementations (continuous score in [-1,1]).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GNN4IP is a tool/model used to score similarity; LLMs evaluated with its outputs: CodeLlama, VeriGen, GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Hardware design / program-structure similarity</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Structural similarity assessment for implementation artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GNN4IP similarity scores were the basis for all fluency/flexibility/originality computations; numeric examples are reported by the paper only after processing (component scores above reflect aggregation of GNN4IP-based measures).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated similarity metric (graph neural network model) integrated into the automated evaluation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Paper references and uses the existing GNN4IP tool; no additional validation against human judgments performed in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>GNN4IP captures structural/graph similarity and may not reflect higher-level semantic differences or design-intent novelty; score thresholds (e.g., 0 for flexibility) are heuristic; reliance on one similarity model may bias originality/uniqueness judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>GNN4IP applied to comparisons between generated responses and golden implementations from the HDLBits-derived prompt set (111 single-module prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Assessing and understanding creativity in large language models <em>(Rating: 2)</em></li>
                <li>Putting gpt-3's creativity to the (alternative uses) test <em>(Rating: 2)</em></li>
                <li>GNN4IP: Graph neural network for hardware intellectual property piracy detection <em>(Rating: 2)</em></li>
                <li>Verilogeval: Evaluating large language models for verilog code generation <em>(Rating: 2)</em></li>
                <li>Verigen: A large language model for verilog code generation <em>(Rating: 2)</em></li>
                <li>Autochip: Automating hdl generation using llm feedback <em>(Rating: 1)</em></li>
                <li>Torrance tests of creative thinking <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4533",
    "paper_id": "paper-8c5aab75826620559d33e99652f4cac9f6efd2fc",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "CreativEval",
            "name_full": "CreativEval creativity evaluation framework",
            "brief_description": "A framework introduced in this paper to quantify the creativity of LLMs in hardware (Verilog) code generation by combining four cognitive subcomponents—fluency, flexibility, originality, and elaboration—into a single creativity score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "CreativEval",
            "evaluation_method_description": "CreativEval evaluates LLM creativity in hardware code generation by measuring four subcomponents (fluency, flexibility, originality, elaboration) computed from multiple generated responses per prompt, functionality checks against testbenches, structural similarity scoring (via GNN4IP) against a golden reference, and simple aggregation: C = 0.25*F + 0.25*X + 0.25*O + 0.25*E. It uses t responses per prompt, filters functional outputs, computes uniqueness and similarity measures per prompt, and averages across prompts.",
            "evaluation_criteria": "Fluency (quantity of unique functional solutions), Flexibility (ability to produce alternative implementations given an example), Originality (degree of dissimilarity to typical/ golden solution), Elaboration (ability to compose/instantiate provided smaller modules to build larger designs), plus baseline functionality (pass@10).",
            "model_name": "Evaluated across CodeLlama-7B, CodeLlama-13B, VeriGen-6B, VeriGen-16B, GPT-3.5, GPT-4",
            "model_size": "CodeLlama: 7B/13B; VeriGen: 6B/16B; GPT-3.5/GPT-4 sizes not specified in paper",
            "scientific_domain": "Hardware design / digital integrated circuit (Verilog RTL) generation",
            "theory_type": "Engineering design solutions / implementation strategies (Verilog modules); not evaluating scientific theories per se",
            "human_comparison": false,
            "evaluation_results": "Table I (selected): overall Creativity scores: GPT-3.5=0.2201, GPT-4=0.2107, CodeLlama-13B=0.2056, VeriGen-6B=0.2026, VeriGen-16B=0.1962, CodeLlama-7B=0.1658. Functionality (pass@10) values: GPT-4=0.3750, VeriGen-6B=0.3667, VeriGen-16B=0.3250, CodeLlama-13B=0.3167, GPT-3.5=0.3083, CodeLlama-7B=0.2417. Component highlights: fluency highest for GPT-4 (0.1644), flexibility highest for GPT-3.5 (0.1600), originality higher for CodeLlama models (~0.30), elaboration equal across several models at 0.3333 (dataset small).",
            "automated_vs_human_evaluation": "Automated: functionality checked programmatically via Icarus Verilog testbenches; structural similarity computed automatically with GNN4IP; metrics computed from these automated signals. No human/expert ratings used for the core creativity metrics.",
            "validation_method": "Validation of outputs done by running generated Verilog against provided HDLBits testbenches (Icarus Verilog) for functional correctness; structural similarity validated using GNN4IP pre-existing model; no human-judgment correlation or inter-rater reliability reported.",
            "limitations_challenges": "Framework is tailored to hardware code generation (Verilog), so applicability to scientific theories/hypotheses is indirect; elaboration dataset is small (9 multimodule prompts) limiting statistical power; reliance on GNN4IP similarity imposes constraints (graph similarity may not capture semantic novelty beyond structural differences); thresholding for flexibility (s&lt;0) is a simple heuristic; no human expert evaluation to validate creativity judgments; results may be biased by dataset (HDLBits) and prompt engineering; GPT model sizes unspecified.",
            "benchmark_dataset": "HDLBits-derived dataset: 111 single-module prompts (with golden solutions and testbenches) for functionality/fluency/originality and 9 multimodule prompts for elaboration; total pass@10 measured on 120 prompts.",
            "uuid": "e4533.0",
            "source_info": {
                "paper_title": "CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Fluency",
            "name_full": "Fluency (CreativEval subcomponent)",
            "brief_description": "Quantifies the average number of unique, functional Verilog implementations an LLM produces per prompt by generating multiple responses, filtering for functionality, and counting distinct implementations using graph-similarity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Fluency (CreativEval)",
            "evaluation_method_description": "For each prompt the LLM generates t responses. Each response is simulated against the prompt's testbench; functional responses are compared to the golden reference using GNN4IP to produce similarity scores. The count of unique similarity values among functional responses is normalized by t and averaged across prompts: F = (1/n) * sum_i (|S(R_i)| / t), where S(R_i) is the set of similarity scores for functional responses to prompt i.",
            "evaluation_criteria": "Number of distinct functional implementations per prompt (distinctness derived from GNN4IP similarity), normalized by number of attempts; functional correctness required.",
            "model_name": "CodeLlama-7B/13B, VeriGen-6B/16B, GPT-3.5, GPT-4",
            "model_size": "CodeLlama:7B/13B; VeriGen:6B/16B; GPT sizes not specified",
            "scientific_domain": "Hardware design / Verilog",
            "theory_type": "Alternative engineering implementations/solutions",
            "human_comparison": false,
            "evaluation_results": "Fluency scores (Table I): GPT-4=0.1644 (highest), CodeLlama-13B=0.1611, GPT-3.5=0.1343, CodeLlama-7B=0.1483, VeriGen-6B=0.1244, VeriGen-16B=0.1189.",
            "automated_vs_human_evaluation": "Fully automated: functionality via Icarus Verilog; uniqueness via automated GNN4IP similarity scoring.",
            "validation_method": "Functional validation via testbenches; uniqueness via GNN4IP similarity values. No human adjudication of uniqueness.",
            "limitations_challenges": "GNN4IP similarity thresholds and score discretization determine what counts as 'unique'—may conflate structural equivalence with semantic novelty; dependent on number of samples t and randomness of generation; only valid for code-generation tasks where testbenches exist.",
            "benchmark_dataset": "111 single-module HDLBits prompts (t responses per prompt, t specified in experiments).",
            "uuid": "e4533.1",
            "source_info": {
                "paper_title": "CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Flexibility",
            "name_full": "Flexibility (CreativEval subcomponent)",
            "brief_description": "Measures the LLM's ability to produce an alternative (different and functional) implementation when provided with an existing correct implementation (golden response).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Flexibility (CreativEval)",
            "evaluation_method_description": "Prompts include a correct Verilog implementation; the LLM is asked to provide a different implementation for the same module. For each prompt t responses are generated and functional responses compared to the golden reference with GNN4IP. If the minimum similarity s across functional responses is below a threshold (paper uses s &lt; 0 on GNN4IP's [-1,1] scale), that prompt counts as demonstrating flexibility. Flexibility X = (1/n) * sum_i T[min S(R_i)] where T(s)=1 if s&lt;0 else 0.",
            "evaluation_criteria": "Presence of at least one functional response per prompt whose similarity to the provided golden solution is below threshold (s &lt; 0), indicating an alternative implementation.",
            "model_name": "CodeLlama-7B/13B, VeriGen-6B/16B, GPT-3.5, GPT-4",
            "model_size": "CodeLlama:7B/13B; VeriGen:6B/16B; GPT sizes not specified",
            "scientific_domain": "Hardware design / Verilog",
            "theory_type": "Alternative implementations (rewrites) of the same functional specification",
            "human_comparison": false,
            "evaluation_results": "Flexibility scores (Table I): GPT-3.5=0.1600 (highest), VeriGen-6B=0.1000, CodeLlama-13B=0.0260, VeriGen-16B=0.0556, GPT-4=0.0795, CodeLlama-7B=0.0000 (often copied the provided implementation).",
            "automated_vs_human_evaluation": "Automated: functionality via testbenches and alternative-ness via GNN4IP similarity thresholding.",
            "validation_method": "Functional validation with testbenches; GNN4IP similarity used to validate difference from golden solution; no human verification of 'different' designs.",
            "limitations_challenges": "Binary threshold (s&lt;0) is heuristic; GNN4IP similarity may not capture semantic differences beyond structural graphs; some models simply copied the provided code leading to low flexibility—this could reflect prompt-following rather than lack of creative alternatives.",
            "benchmark_dataset": "Subset of HDLBits prompts where golden implementation is provided; same 111 prompts used for these tests.",
            "uuid": "e4533.2",
            "source_info": {
                "paper_title": "CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Originality",
            "name_full": "Originality (CreativEval subcomponent)",
            "brief_description": "Measures how dissimilar a functional generated solution is from the typical/golden implementation using GNN4IP similarity scores renormalized to a [0,1] originality scale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Originality (CreativEval)",
            "evaluation_method_description": "For each prompt the minimum GNN4IP similarity value (min S(R_i)) among functional responses is taken; similarity s in [-1,1] is renormalized to [0,1] with 1 indicating maximal originality: O = (1/n) * sum_i ((-min S(R_i) + 1)/2).",
            "evaluation_criteria": "Degree of dissimilarity to golden implementation as measured by GNN4IP graph similarity; higher renormalized values indicate more original (less similar) solutions.",
            "model_name": "CodeLlama-7B/13B, VeriGen-6B/16B, GPT-3.5, GPT-4",
            "model_size": "CodeLlama:7B/13B; VeriGen:6B/16B; GPT sizes not specified",
            "scientific_domain": "Hardware design / Verilog",
            "theory_type": "Novelty of implementation relative to canonical solution",
            "human_comparison": false,
            "evaluation_results": "Originality scores (Table I): CodeLlama-13B=0.3021, CodeLlama-7B=0.2926 (highest), VeriGen-16B=0.2771, VeriGen-6B=0.2527, GPT-3.5=0.2526, GPT-4=0.2657. GPT models tended to be closer to golden solutions, producing less original variants on average.",
            "automated_vs_human_evaluation": "Automated via GNN4IP similarity renormalization; no human scoring of originality.",
            "validation_method": "Functional correctness plus automated graph-similarity based measurement; no correlation to human judgments reported.",
            "limitations_challenges": "Originality depends wholly on GNN4IP's notion of similarity, which may favor or penalize certain coding styles; a single minimum-similarity selection per prompt may miss other original but less extreme variations; no human-grounded originality validation.",
            "benchmark_dataset": "111 single-module HDLBits prompts (functional responses compared to golden references).",
            "uuid": "e4533.3",
            "source_info": {
                "paper_title": "CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Elaboration",
            "name_full": "Elaboration (CreativEval subcomponent)",
            "brief_description": "Assesses whether an LLM can compose larger modules by reusing/invoking provided smaller modules (multi-modular design), counting prompts where any functional generated response instantiates the smaller modules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Elaboration (CreativEval)",
            "evaluation_method_description": "Prompts include multiple smaller Verilog modules and ask the LLM to build a larger module using them. For p prompts, n prompts have at least one functional response that uses the provided smaller modules; elaboration E = n / p. Functional correctness is required and usage of provided modules is detected programmatically.",
            "evaluation_criteria": "Whether generated functional solution instantiates/uses the provided smaller modules (yes/no) and passes testbench, aggregated as fraction of prompts demonstrating such usage.",
            "model_name": "CodeLlama-7B/13B, VeriGen-6B/16B, GPT-3.5, GPT-4",
            "model_size": "CodeLlama:7B/13B; VeriGen:6B/16B; GPT sizes not specified",
            "scientific_domain": "Hardware design / Verilog",
            "theory_type": "Compositional multi-module design solutions",
            "human_comparison": false,
            "evaluation_results": "Elaboration scores (Table I): all reported models show 0.3333 (based on the small multimodule set), indicating 1/3 of the 9 elaboration prompts had at least one functional response that used provided modules.",
            "automated_vs_human_evaluation": "Automated: functional checks via testbenches and automated detection of module instantiation/use in generated code.",
            "validation_method": "Functional testing and programmatic parsing to detect instantiation of smaller modules; no human verification of design quality/compositional correctness beyond testbench pass.",
            "limitations_challenges": "Very small sample (9 prompts) reduces reliability; detecting 'use' of modules is syntactic and may miss semantic reuse; difficulty for LLMs to synthesize larger modules even if they could in principle.",
            "benchmark_dataset": "9 multimodule HDLBits prompts drawn from AutoChip/HDLBits dataset.",
            "uuid": "e4533.4",
            "source_info": {
                "paper_title": "CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GNN4IP",
            "name_full": "GNN4IP (Graph neural network for IP piracy detection) similarity tool",
            "brief_description": "An existing graph-neural-network-based tool that converts Verilog modules into data-flow graphs and outputs a similarity score in [-1, 1] between two modules; used here to quantify structural similarity between generated and golden Verilog implementations.",
            "citation_title": "GNN4IP: Graph neural network for hardware intellectual property piracy detection",
            "mention_or_use": "use",
            "evaluation_method_name": "GNN4IP-based structural similarity",
            "evaluation_method_description": "Verilog modules are represented as data-flow graphs (DFGs), then GNN4IP computes a similarity score in [-1,1] where higher values indicate greater similarity. CreativEval uses these scores to determine uniqueness (fluency), alternative-ness (flexibility thresholding), and originality (renormalization of min similarity to [0,1]).",
            "evaluation_criteria": "Graph-structural similarity between LLM-generated modules and golden/reference implementations (continuous score in [-1,1]).",
            "model_name": "GNN4IP is a tool/model used to score similarity; LLMs evaluated with its outputs: CodeLlama, VeriGen, GPT-3.5, GPT-4",
            "model_size": null,
            "scientific_domain": "Hardware design / program-structure similarity",
            "theory_type": "Structural similarity assessment for implementation artifacts",
            "human_comparison": false,
            "evaluation_results": "GNN4IP similarity scores were the basis for all fluency/flexibility/originality computations; numeric examples are reported by the paper only after processing (component scores above reflect aggregation of GNN4IP-based measures).",
            "automated_vs_human_evaluation": "Automated similarity metric (graph neural network model) integrated into the automated evaluation pipeline.",
            "validation_method": "Paper references and uses the existing GNN4IP tool; no additional validation against human judgments performed in this work.",
            "limitations_challenges": "GNN4IP captures structural/graph similarity and may not reflect higher-level semantic differences or design-intent novelty; score thresholds (e.g., 0 for flexibility) are heuristic; reliance on one similarity model may bias originality/uniqueness judgments.",
            "benchmark_dataset": "GNN4IP applied to comparisons between generated responses and golden implementations from the HDLBits-derived prompt set (111 single-module prompts).",
            "uuid": "e4533.5",
            "source_info": {
                "paper_title": "CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Assessing and understanding creativity in large language models",
            "rating": 2
        },
        {
            "paper_title": "Putting gpt-3's creativity to the (alternative uses) test",
            "rating": 2
        },
        {
            "paper_title": "GNN4IP: Graph neural network for hardware intellectual property piracy detection",
            "rating": 2
        },
        {
            "paper_title": "Verilogeval: Evaluating large language models for verilog code generation",
            "rating": 2
        },
        {
            "paper_title": "Verigen: A large language model for verilog code generation",
            "rating": 2
        },
        {
            "paper_title": "Autochip: Automating hdl generation using llm feedback",
            "rating": 1
        },
        {
            "paper_title": "Torrance tests of creative thinking",
            "rating": 1
        }
    ],
    "cost": 0.013170249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation</h1>
<p>Matthew DeLorenzo, Vasudev Gohil, Jeyavijayan Rajendran<br>Texas A\&amp;M University, USA<br>{matthewdelorenzo, gohil.vasudev, jv.rajendran}@tamu.edu</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have proved effective and efficient in generating code, leading to their utilization within the hardware design process. Prior works evaluating LLMs' abilities for register transfer level code generation solely focus on functional correctness. However, the creativity associated with these LLMs, or the ability to generate novel and unique solutions, is a metric not as well understood, in part due to the challenge of quantifying this quality.</p>
<p>To address this research gap, we present CreativEval, a framework for evaluating the creativity of LLMs within the context of generating hardware designs. We quantify four creative sub-components, fluency, flexibility, originality, and elaboration, through various prompting and post-processing techniques. We then evaluate multiple popular LLMs (including GPT models, CodeLlama, and VeriGen) upon this creativity metric, with results indicating GPT-3.5 as the most creative model in generating hardware designs.</p>
<p>Index Terms—Hardware Design, LLM, Creativity</p>
<h2>I. INTRODUCTION</h2>
<p>Recent advancements within artificial intelligence, machine learning, and computing performance have resulted in the development of LLMs, which have quickly proven to be a widely applicable and successful solution when applied to a variety of text-based tasks [1]. After extensive training on large quantities of text data, these transformer-based models [2] have demonstrated the ability to not only successfully interpret the contextual nuances of a provided text (or prompt), but also generate effective responses to a near human-like degree [3]. This can take the form of summarizing a document, answering and elaborating upon questions, and even generating code. The effectiveness and versatility of LLMs regarding textual understanding have resulted in their adoption within various applications, such as language translation [4], customer service chat-bots [5], and programming assistants [1].</p>
<p>Furthermore, the potential of LLM code generation has recently been explored within the integrated circuit (IC) design process [6], such as within the logic design stage. With chip designs continually growing in scale and complexity, efforts to increase the automation of this task through LLMs have been explored. This includes the evaluation of LLMs' ability to generate hardware design codes from English prompts, leading to promising initial results within various frameworks [7]-[10].</p>
<p>With the goal of further optimizing these LLMs to the level of an experienced hardware designer, many research efforts have focused on improving performance within the metric of code functionality. This includes testing various LLM finetuning strategies and prompting methods for domain-optimized
performance, such as register transfer level (RTL) code generation.</p>
<p>However, another dimension to consider when evaluating the ability of a designer, absent from previous evaluations, is creativity. This term refers to the capacity to think innovatively-the ability to formulate new solutions or connections that are effective and unconventional [11]. When applied to hardware code generation, this can take the form of writing programs that are not only correct, but also novel, surprising, or valuable when compared to typical design approaches. This quality is essential to understanding the greater potential of LLMs as a tool for deriving new approaches to hardware design challenges, rather than simply a method to accelerate existing design practices. With a quantitative method of measuring this concept of creativity within LLM hardware generation, valuable insights could be derived, such as how performance could be further improved, or how LLMs can be best utilized within the hardware design process.</p>
<p>To address this absence within the analysis of LLM-based RTL code generation, we propose a comparative evaluation framework in which the creativity of LLMs can be effectively measured. This assessment is composed of four cognitive subcategories of creativity (fluency, flexibility, originality, and elaboration), which are quantified and evaluated within the context of generating functional Verilog modules. Furthermore, this approach utilizes various prompting structures, generation strategies, and post-processing methods, from which the quality and variations of responses are utilized to generate a metric for creativity. This work presents the following contributions:</p>
<ul>
<li>To the best of our knowledge, we propose the first framework from which a metric for creativity is defined for LLMs within the context of hardware design and code generation.</li>
<li>We provide a comparative evaluation between state-of-theart LLMs upon our creativity metric and its components, with GPT-3.5 achieving the highest result.</li>
<li>To enable future research, we will open-source our framework codebase and datasets here: https://github.com/matthewdelorenzo/CreativEval/</li>
</ul>
<h2>II. BACKGROUND AND RELATED WORK</h2>
<h2>A. LLMs for Code Generation and Hardware Design</h2>
<p>Many state-of-the-art LLMs have demonstrated remarkable success in generating code when provided only with a natural</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Experimental Framework - calculating creativity of LLMs in Verilog code generation.</p>
<p>language description, such as GPT-3.5/4 [12], BERT [13], and Claude [14], revolutionizing the software development process. These models demonstrate promising performance in code functionality, such as GPT-4 generating correct code for 67% of programming tasks in the HumanEval benchmark in a single response (pass@1) [15]–[17].</p>
<p>Therefore, the applications of LLMs within hardware design through RTL code generation are explored within various studies, such as DAVE [18] which utilized GPT-2 for this task. VeriGen [7] then demonstrated that fine-tuning smaller models (CodeGen) upon a curated RTL dataset can outperform larger models in RTL tests. VerilogEval [19] presents enhanced LLM hardware generation through supervised fine-tuning, and provides an RTL benchmark for evaluating functionality in RTL generation. ChipNeMo [9] applied fine-tuning upon open-source models (Llama2 7B/13B) for various hardware design tasks. RTLCoder [20] presents an automated method for expanding the RTL dataset used for fine-tuning, resulting in a 7B-parameter model that outperforms GPT-3.5 on RTL benchmarks. Other works, including RTLLM [21] and ChipChat [8], explore prompt engineering strategies to enhance the quality and scale of LLM-generated designs. Although there is a plethora of work on LLM-based RTL generation, none of these prior works assess the creative component of LLMs in the hardware design process. We address this shortcoming in this work.</p>
<h3>B. Evaluating Creativity</h3>
<p>Prior cognitive science studies [22]–[25] have explored methods in which creative thinking can be effectively measured. A widely accepted creativity model [24] defines four primary cognitive dimensions from which divergent thinking, or the ability to generate creative ideas through exploring multiple possible solutions [26], can be measured—fluency, flexibility, originality, and elaboration.</p>
<ul>
<li><strong>Fluency.</strong> The quantity of relevant and separate ideas able to be derived in response to a single given question.</li>
<li><strong>Flexibility.</strong> The ability to formulate alternative solutions to a given problem or example across a variety of categories.</li>
<li><strong>Originality.</strong> A measure of how unique or novel a given idea is, differing from typical responses or solutions.</li>
<li><strong>Elaboration.</strong> The ability to expand upon or refine a given idea. This can include the ability to construct complex solutions utilizing provided, basic concepts.</li>
</ul>
<p>These subcategories have been widely in evaluating human creativity within educational research, including various studies of students [27]–[29] as a metric for effective learning. Furthermore, recent works explore the intersection between cognitive science and LLMs [30]–[32], in which the creativity of LLMs are evaluated within the context of natural language, demonstrating near-human-like performance in many cases [31]. In particular, [33] utilizes the four creative subcategories to evaluate LLMs across multiple language-based cognitive tasks. However, this framework has not been adapted to LLMs within the context of generating hardware code. To this end, we devise our creativity evaluation framework for LLM-based hardware code generation.</p>
<h3>III. CREATIVEVAL FRAMEWORK</h3>
<p>Given a target LLM, our CreativeEval framework, as shown in Fig. 1, seeks to evaluate the creativity associated with LLMs in hardware code generation. CreativeEval evaluates the previously defined subcategories of creativity—fluency, flexibility, originality, and elaboration. To this end, we query the target LLM with different Verilog-based prompts, and analyze the responses through various methods of postprocessing to calculate the desired metrics, as explained below.</p>
<h4>A. Fluency</h4>
<p>To capture the quantity of relevant and separate ideas in our context, we define fluency as the average number of unique Verilog solutions generated by the target LLM in response to a given prompt. Our prompts contain a brief English description of the module and the module's declaration, as shown in Listing 1. Each prompt is provided as input to the LLM, with the response intended to be the completed implementation of the module. As the inference process of LLMs contains variations in the generated responses, we generate t responses for each prompt to estimate the average performance.</p>
<p>Upon generating all responses, each response is then tested for functionality against the module's associated testbench. If all test cases pass, the module is considered functional. Then, for each prompt, the functional responses (if any) are collected and compared to identify if they are unique implementations.</p>
<p>This is done through GNN4IP [34], a tool utilized to assess the similarities between circuits. By representing two Verilog</p>
<p>1 //Create a full adder.
2 //A full adder adds three bits (including carry-in) and produces a sum and carry-out.
3
4module top_module {
5 input a, b, cin,
6 output cout, sum };
Listing 1: Fluency/Originality prompt example</p>
<p>modules as a data-flow graph (DFG), GNN4IP generates a similarity score within [-1,1], with larger values indicating a higher similarity. Each correct generated solution from the LLM is input into GNN4IP, and compared to its ideal solution, or “golden response”. Upon the generation of each similarity value for a given prompt, these results are then compared to determine how many unique values are in the response set, indicating the number of distinct solutions.</p>
<p>Given that there are a set of $p$ total prompts in the dataset, the LLM generates $t$ responses for each. After evaluating the functionality of these results, there is then a subset $n$ prompts that contain at least one success (functional module generation). For each of these $n$ prompts, there is a sub-total of the $t$ responses that are functional, defined as $m$. Each of these $m$ functional responses, $r$, are defined as the set $R=\left{r_{1n},...,r_{mn}\right}$. The GNN4IP similarity value is then found for each response in $R$, represented as the function $S$. The number of unique similarity values is then determined within the set, and normalized to total $t$ responses. This process is repeated for all $n$ successful prompts and averaged to define the fluency metric $F$ below:</p>
<p>$F=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{\left|S(R_{i})\right|}{t}\right)$ (1)</p>
<h3>III-B Flexibility</h3>
<p>Flexibility is quantified as the ability of the LLM to generate an alternative implementation of a Verilog module when provided with a solution. The prompts for this metric are constructed for a set of Verilog modules in which a correct solution (the golden response) is included (Listing 2). The LLM then rewrites the Verilog module, ideally resulting in a functional and unique implementation.</p>
<p>As before, $t$ responses are generated for each of the $p$ total prompts. After all responses are checked for functionality, $n$ prompts have at least one functional response, each with $m$ functional responses. These functional responses are compared directly with the golden response (through GNN4IP) to identify their similarity value. If the similarity value $s$ is lower than a given threshold on the scale [-1,1], the response is considered an alternative solution, shown in Equation 2. For each successful prompt, the response with minimum similarity is found and evaluated against the threshold. Then, the total amount of $n$ prompts with a response less than the threshold is determined, and normalized by the total prompts $n$. The final flexibility metric $X$ is then defined below:</p>
<p>1 // You are a professional hardware designer that writes correct, fully functional Verilog modules.
2 // Given the fully implemented example of the Verilog module below:
3
4module true_module(
5 input a, b, cin,
6 output cout, sum );
7 assign sum = a ^ b ^ cin;
8 assign cout = a &amp; b | a &amp; cin | b &amp; cin;
9endmodule
10
11 // Finish writing a different and unique implementation of the provided true_module in the module below, top_module.
12module top_module (
13 input a, b, cin,
14 output cout, sum );</p>
<p>Listing 2: Flexibility prompt example</p>
<p>[ T(s)=\left{\begin{array} { l } 
{ 1 \text{ if } s &lt; 0 } \
{ 0 \text{ if } s \geq 0 } \
{ X = \frac{1}{n} \sum_{i=1}^{n} (T[\min S(R_{i})]) }
\end{array} \right. \tag{2} ]</p>
<h3>III-C Originality</h3>
<p>The originality metric is defined as the variance (uniqueness) of an LLM-generated Verilog module in comparison to a typical, fully functional implementation. This metric is derived from the similarity value (generated through GNN4IP) between successful generations and their golden response.</p>
<p>The originality experiment follows the same prompt structure and procedure as described in III-A. For each prompt, the response with the minimum similarity value is found. Then, the similarity values [-1, 1] are re-normalized to be on scale of [0, 1] with 1 indicating the least similarity (i.e. most original). These results are averaged over all $n$ prompts, with the final originality metric $O$ is described below:</p>
<p>$O=\frac{1}{n}\sum_{i=1}^{n}\frac{(-\min S(R_{i})+1)}{2}$</p>
<h3>III-D Elaboration</h3>
<p>To measure an LLM’s capacity for elaboration, the LLM is provided with multiple smaller Verilog modules in a prompt, and tasked with utilizing them to implement a larger, more complex module. As this metric requires multi-modular designs, a separate set of Verilog modules is utilized in constructing the prompts, as shown in Listing 3.</p>
<p>Multiple LLM responses are generated for each module, which are all then checked for functionality. For all given functional solutions, the responses are checked to see if the solution utilizes the smaller modules (as opposed to a single modular solution). If any of the responses for a given prompt are both functional and utilize the smaller modules, it is</p>
<p>TABLE I: COMPARISON OF DIFFERENT LLMS IN TERMS OF CREATIVITY AND ITS SUBCATEGORIES</p>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Functionality</th>
<th>Fluency</th>
<th>Flexibility</th>
<th>Originality</th>
<th>Elaboration</th>
<th>Creativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>CodeLlama-7B [35]</td>
<td>0.2417</td>
<td>0.1483</td>
<td>0.0000</td>
<td>0.2926</td>
<td>0.2222</td>
<td>0.1658</td>
</tr>
<tr>
<td>CodeLlama-13B [36]</td>
<td>0.3167</td>
<td>0.1611</td>
<td>0.0260</td>
<td>0.3021</td>
<td>0.3333</td>
<td>0.2056</td>
</tr>
<tr>
<td>VeriGen-6B [37]</td>
<td>0.3667</td>
<td>0.1244</td>
<td>0.1000</td>
<td>0.2527</td>
<td>0.3333</td>
<td>0.2026</td>
</tr>
<tr>
<td>VeriGen-16B [38]</td>
<td>0.3250</td>
<td>0.1189</td>
<td>0.0556</td>
<td>0.2771</td>
<td>0.3333</td>
<td>0.1962</td>
</tr>
<tr>
<td>GPT-3.5 [39]</td>
<td>0.3083</td>
<td>0.1343</td>
<td>0.1600</td>
<td>0.2526</td>
<td>0.3333</td>
<td>0.2201</td>
</tr>
<tr>
<td>GPT-4 [40]</td>
<td>0.3750</td>
<td>0.1644</td>
<td>0.0795</td>
<td>0.2657</td>
<td>0.3333</td>
<td>0.2107</td>
</tr>
</tbody>
</table>
<p>1 // You are given a module add16 that performs a 16-bit addition.
2 //Instantiate two of them to create a 32-bit adder.
3
4module add16 (input[15:0] a, input[15:0] b, input cin, output[15:0] sum, output cout );
5
6module top_module (
7 input [11:0] a,
8 input [11:0] b,
9 output [11:0] sum
10);
Listing 3: Elaboration prompt example
considered a positive instance of elaboration. Given $p$ total Verilog prompts, of which $n$ have at least one response that demonstrates elaboration, the metric is specified as:</p>
<p>$$
E=\left(\frac{n}{p}\right)
$$</p>
<h2>E. Creativity: Putting It All Together</h2>
<p>Given each of the subcategories associated with creativity defined above, the metrics are then combined to define the overall creativity of a given LLM in Verilog hardware design.</p>
<p>$$
C=(0.25) F+(0.25) X+(0.25) O+(0.25) E
$$</p>
<h2>IV. EXPERIMENTAL EVALUATION</h2>
<h2>A. Experimental Setup</h2>
<p>We evaluate multiple LLMs using the CreativEval framework, including CodeLlama 7B [35] and 13B [36] parameter models, VeriGen 6B [37] and 16B [38] (16B model loaded in 8-bit quantization due to memory constraints), GPT3.5 [39], and GPT-4 [40]. The inference process of the VeriGen and CodeLlama models was performed locally on an NVIDIA A100 GPU with 80 GB RAM, while GPT-3.5/4 were queried through the OpenAI Python API. All scripts are written in Python 3.10, with Icarus Verilog 10.3 as the simulator for evaluating functionality checks. The open-source GNN4IP repository was adapted to this framework to generate the similarity scores. The prompt dataset utilized for functionality, fluency, and originality consists of 111 single-module HDLBits [41] prompts sourced through AutoChip [42], each containing a
correctly implemented solution and testbench. The smaller prompt set used for elaboration contains 9 separate multimodule prompts from the same source. The base functionality metric (pass@10) is measured on all 120 prompts.</p>
<p>When generating LLM responses in all experiments, the LLMs were all set to the following inference hyperparameters: temperature=0.3; max_tokens=1024; top_k=10; top_p=0.95. All responses were trimmed to the first generated instance of "endmodule" for effective functionality evaluation.</p>
<h2>B. Results</h2>
<p>Table I summarizes the results for all LLMs for all subcategories of creativity. In evaluating fluency, GPT-4 had the highest quantity of separate and correct Verilog solutions to a module (with respect to the modules that have at least one correct solution), with CodeLlama-13B achieving similar results. The VeriGen models comparatively struggled in this metric, partly due to repeated generations of similar implementations instead of different implementations.</p>
<p>Regarding flexibility, GPT-3.5 had the highest rate of generating alternative solutions to provided modules across most models. The models that struggled (e.g., CodeLlama) produced results that were often direct copies of the provided module, indicating the ability to understand the prompt's natural language description as an important factor that determined flexibility.</p>
<p>As for originality, the GPT models had slightly worse performance than the others, with CodeLlama performing best. This means that the successful solutions provided with the GPT models were, on average, closer to the ideal solution. This could be due to its large size and training dataset, resulting in a more direct retrieval of existing solutions or coding practices.</p>
<p>Elaboration was largely similar for all modules, as the HDLBits dataset for this metric is comparatively small ( 9 modules). The models primarily excelled in correctly connecting the input and output parameters between separate modules, while struggling to generate the larger module solution.</p>
<p>Overall, the GPT models were the most creative, with GPT3.5 as the best, and CodeLlama-7B was the least creative. Creativity is shown to slightly drop for the larger model sizes of GPT and VeriGen.</p>
<h2>V. CONCLUSION</h2>
<p>Recent studies on LLMs regarding their applications to hardware design have effectively demonstrated their poten-</p>
<p>tial, applying many optimization strategies to increase the performance in terms of functional correctness. However, these studies do not investigate the creativity associated with LLMs in their ability to generate solutions, largely due to the lack of an effective metric. Within this work, we propose CreativEval, a framework to evaluate the creativity of LLMs in generating hardware code. By evaluating multiple popular LLMs within this framework, we perform a comparative analysis, concluding that GPT-3.5 had the greatest creativity. Future research in this direction can further evaluate more LLMs and on larger prompt sets.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>The authors acknowledge the support from the Purdue Center for Secure Microelectronics Ecosystem - CSME#210205. This work was also partially supported by the National Science Foundation (NSF CNS-1822848 and NSF DGE-2039610).</p>
<h2>REFERENCES</h2>
<p>[1] Tim Keary, "12 Practical Large Language Model (LLM) Applications," https://www.techopedia.com/12-practical-large-language-model-llm-a pplications, 2023, [Online; last accessed 21-Nov-2023].
[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and J. Polosukhin, "Attention is all you need," 2023.
[3] Z. G. Cai, X. Duan, D. A. Haslett, S. Wang, and M. J. Pickering, "Do large language models resemble humans in language use?" 2024.
[4] T. Kocmi and C. Federmann, "Large language models are state-of-the-art evaluators of translation quality," 2023.
[5] K. Pandya and M. Holia, "Automating customer service using langchain: Building custom open-source gpt chatbot for organizations," 2023.
[6] R. Zhong, X. Du, S. Kai, Z. Tang, S. Xu, H.-L. Zhen, J. Hao, Q. Xu, M. Yuan, and J. Yan, "Llm4eda: Emerging progress in large language models for electronic design automation," arXiv preprint arXiv:2401.12224, 2023.
[7] S. Thakur, B. Ahmad, H. Pearce, B. Tan, B. Dolan-Gavitt, R. Karri, and S. Garg, "Verigen: A large language model for verilog code generation," 2023.
[8] J. Blocklove, S. Garg, R. Karri, and H. Pearce, "Chip-chat: Challenges and opportunities in conversational hardware design," in 2023 ACM/IEEE 5th Workshop on Machine Learning for CAD (MLCAD). IEEE, Sep. 2023. [Online]. Available: http: //dx.doi.org/10.1109/MLCAD58807.2023.10299874
[9] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang et al., "Chipnemo: Domain-adapted llms for chip design," 2024.
[10] M. DeLorenzo, A. B. Chowdhury, V. Gohil, S. Thakur, R. Karri, S. Garg, and J. Rajendran, "Make every move count: Llm-based high-quality rtl code generation using mcts," 2024.
[11] M. Runco and G. Jaeger, "The standard definition of creativity," Creativity Research Journal - CREATIVITY RES J, vol. 24, pp. 92-96, 01 2012.
[12] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman et al., "Gpt-4 technical report," 2024.
[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding," in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171-4186. [Online]. Available: https://aclanthology.org/N19-1423
[14] [Online]. Available: https://www.anthropic.com/news/clasde-3-haiku
[15] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang, "Wizardcoder: Empowering code large language models with evol-instruct," 2023.
[16] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan et al., "Evaluating large language models trained on code," 2021.
[17] Y. Wang, H. Le, A. D. Gotmare, N. D. Q. Bui, J. Li, and S. C. H. Hoi, "Codet5+: Open code large language models for code understanding and generation," 2023.
[18] H. Pearce, B. Tan, and R. Karri, "Dave: Deriving automatically verilog from english," in Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD, ser. MLCAD '20. New York, NY, USA: Association for Computing Machinery, 2020, p. 27-32. [Online]. Available: https://doi.org/10.1145/3380446.3430634
[19] M. Liu, N. Pinckney, B. Khailany, and H. Ren, "Verilogeval: Evaluating large language models for verilog code generation," 2023.
[20] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, and Z. Xie, "Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution," 2024.
[21] Y. Lu, S. Liu, Q. Zhang, and Z. Xie, "Rtllm: An open-source benchmark for design rtl generation with large language model," 2023.
[22] L. S. Almeida, L. P. Prieto, M. Ferrando, E. Oliveira, and C. Ferrándiz, "Torrance test of creative thinking: The question of its construct validity," Thinking Skills and Creativity, vol. 3, no. 1, pp. 53-58, 2008. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ S1871187108000072
[23] S. L. Doerr, "Conjugate lateral eye movement, cerebral dominance, and the figural creativity factors of fluency, flexibility, originality, and elaboration," Studies in Art Education, vol. 21, no. 3, pp. 5-11, 1980. [Online]. Available: http://www.jstor.org/stable/1319788
[24] J. P. Guilford, The nature of human intelligence. McGraw-Hill, 1971.
[25] E. P. Torrance, "Torrance tests of creative thinking," Educational and psychological measurement, 1966.
[26] M. Arefi, "Comparation of creativity dimensions (fluency, flexibility, elaboration, originality) between bilingual elementary students (azari language-kurdish language) in urmia city iran - the iafor research archive," Dec 2018. [Online]. Available: https://papers.iafor.org/submi ssion22045/
[27] S. A. Handayani, Y. S. Rahayu, and R. Agustini, "Students' creative thinking skills in biology learning: fluency, flexibility, originality, and elaboration," Journal of Physics: Conference Series, vol. 1747, no. 1, p. 012040, feb 2021. [Online]. Available: https://dx.doi.org/10.1088/1742-6596/1747/1/012040
[28] F. Alacapinar, "Grade level and creativity," Eurasian Journal of Educational Research (EJER), vol. 13, pp. 247-266, 012012.
[29] M. Arefi and N. Jalali, "Comparation of creativity dimensions (fluency, flexibility, elaboration, originality) between bilingual elementary students (azari language-kurdish language) in urmia city-iran," in The IAFOR International Conference on Language Learning, 2016.
[30] R. Shiffrin and M. Mitchell, Mar 2023. [Online]. Available: https://www.pnas.org/doi/abs/10.1073/pnas.2300963120
[31] C. Stevenson, I. Smal, M. Baas, R. Grusman, and H. van der Maas, "Putting gpt-3's creativity to the (alternative uses) test," 2022.
[32] M. Binz and E. Schulz, "Using cognitive psychology to understand gpt-3," Proceedings of the National Academy of Sciences, vol. 120, no. 6, Feb. 2023. [Online]. Available: http://dx.doi.org/10.1073/pnas. 22 18523120
[33] Y. Zhao, R. Zhang, W. Li, D. Huang, J. Guo, S. Peng, Y. Hao, Y. Wen, X. Hu, Z. Du, Q. Guo, L. Li, and Y. Chen, "Assessing and understanding creativity in large language models," 2024.
[34] R. Yasaei, S.-Y. Yu, E. K. Naeini, and M. A. A. Faruque, "Gnn4ip: Graph neural network for hardware intellectual property piracy detection," 2021.
[35] "Hugging face." [Online]. Available: https://huggingface.co/codellama /CodeLlama-7b-hf
[36] "Hugging face." [Online]. Available: https://huggingface.co/codellama /CodeLlama-13b-hf
[37] "Hugging face." [Online]. Available: https://huggingface.co/shailja/fin e-tuned-codegen-6B-Verilog
[38] "Hugging face." [Online]. Available: https://huggingface.co/shailja/fin e-tuned-codegen-16B-Verilog
[39] "fine-tuning and api updates." [Online]. Available: https://openai.com/b log/gpt-3-5-turbo-fine-tuning-and-api-updates
[40] "fine-tuning and api updates." [Online]. Available: https://openai.com/r esearch/gpt-4
[41] [Online]. Available: https://hdlbits.01xz.net/wiki/Main_Page
[42] S. Thakur, J. Blocklove, H. Pearce, B. Tan, S. Garg, and R. Karri, "Autochip: Automating hdl generation using llm feedback," 2023.</p>            </div>
        </div>

    </div>
</body>
</html>
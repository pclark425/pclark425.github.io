<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8735 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8735</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8735</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-279465189</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.15894v1.pdf" target="_blank">Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated impressive mathematical reasoning capabilities, yet their performance remains brittle to minor variations in problem description and prompting strategy. Furthermore, reasoning is vulnerable to sampling-induced errors which autoregressive models must primarily address using self-correction via additionally-generated tokens. To better understand self-correction capabilities of recent models, we conduct experiments measuring models'ability to self-correct synthetic perturbations introduced into their Chain of Thought (CoT) reasoning. We observe robust single-utterance intrinsic self-correction behavior across a range of open-weight models and datasets, ranging from subtle, implicit corrections to explicit acknowledgments and corrections of errors. Our findings suggest that LLMs, including those not finetuned for long CoT, may possess stronger intrinsic self-correction capabilities than commonly shown in the literature. The presence of this ability suggests that recent"reasoning"model work involves amplification of traits already meaningfully present in models.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8735.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8735.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-Utterance Intrinsic Self-Correction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-Utterance Intrinsic Self-Correction of Perturbed Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test-time behavior where a language model, within a single uninterrupted generation, detects an introduced error in its chain-of-thought, acknowledges it (explicitly or implicitly) and produces a corrected final answer without an external verifier or a separate critique turn.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various evaluated models (Command R7B, Nemo 12B, Gemma 2 27B, QwQ 32B Preview, LLaMA 3.3 70B, Qwen 2.5 72B, R1 671B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Set of modern LLMs evaluated in this paper spanning ~7B to 671B parameters; precisions and providers vary (e.g., BF16, FP8) as reported in Table A2. Included both models advertised as 'reasoning' (QwQ, R1) and general-purpose models (Gemma, Nemo, Command).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-utterance intrinsic self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model is given a partially-complete reasoning 'stub' that has been synthetically perturbed; the model continues generation in one uninterrupted utterance, and may (a) proceed with the incorrect trajectory, (b) implicitly correct intermediate steps, or (c) explicitly acknowledge an error and emit corrected computation and final answer — all inside the same generated output. No external verifier or multi-turn critique is used during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math reasoning tasks (GSM8K, GSM-Symbolic, MATH-500, GSM8K Matched)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard math word-problem benchmarks: GSM8K (grade-school math problems), GSM-Symbolic (template variants of GSM8K), MATH-500 (subset of competition-level math problems).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative and aggregate metrics reported: models showed varying degrees of recovery when completing perturbed reasoning stubs in a single utterance. Aggregate statistics include an average absolute drop (in on-policy perturbed scenario) of 61.6% across evaluated models excluding R1; smaller models (<30B) showed a larger average absolute drop of 78.1%, larger models (>30B) 41.7%. Some large models (Qwen 2.5 72B, LLaMA 3.3 70B) showed relatively robust recovery on MATH-500 and GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline 'Direct Solutions' (unperturbed pass@1) were used as reference (numerical values are given in Table 1 but not enumerated in the main text). The paper reports direct/unperturbed pass@1 as the baseline against which perturbed completion success rates are compared.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineering / intrinsic generation behavior: the experiment forces continuation from a perturbed reasoning stub and observes the model's natural completion. The mechanism is wholly model-internal (no external verifier during generation); verification is performed post hoc by a grader model (LLaMA 3.1 405B) that checks final answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: observed recoveries from perturbed stubs where some models re-derived correct answers in a single generation; aggregate drop/recovery statistics (see 'performance_with_reflection'). Qualitative: examples in Appendix D and Figure 1 show explicit mid-generation corrections and use of pivot tokens (e.g., 'Wait,' 'However,' 'Hold on') coinciding with corrected computations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: models still commonly fail to detect simple introduced errors; off-policy perturbations (perturbations produced by a held-out model) may be artificially easy to detect; inability to observe token-level probabilities (API restrictions) limits realism of perturbations; dataset coverage limited; perturbation types biased toward low-level arithmetic/operator corruptions rather than high-level reasoning-backtracking; style-capability coupling (e.g. QwQ performs worse when continuing off-policy stubs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Paper contrasts single-utterance intrinsic self-correction with multi-turn generate-critique-correct pipelines and extrinsic-feedback approaches, noting prior mixed findings (e.g., some literature reports multi-turn self-critique helps, while Huang et al. 2024 found it can worsen performance in weakly-specified critique prompts). The paper positions single-utterance intrinsic recovery as complementary and sometimes naturally present in base models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8735.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8735.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generate-Critique-Correct (Self-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Three-turn generate-critique-correct iterative refinement (Self-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A popular multi-turn prompt-based self-correction technique where a model first generates an answer, then is prompted to critique its own output, and finally produces a corrected answer (can be iterated).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prompted multi-turn self-correction (literature technique)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Technique commonly applied with large language models via crafted prompts that request a critique and a correction; may be iterated multiple times in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generate-then-critique-then-correct (multi-turn self-refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A three-turn cycle: (1) model generates an initial answer, (2) model (or separate critic) generates a critique of that answer, and (3) the model produces a corrected answer, sometimes repeating the critique-and-correct loop.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General QA and reasoning tasks (as used in cited literature)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applied broadly across reasoning and QA benchmarks in prior work to improve final-answer correctness through iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Cited prior work (Madaan et al., 2023) reports improvements in some settings; however, Huang et al. (2024) is cited as showing multi-turn self-correction can sometimes degrade performance when critique prompts lack helpful criteria. This paper does not reproduce specific numeric comparisons for this method.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline single-shot or chain-of-thought generation; exact numbers depend on cited studies and are not presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineering: explicit separate critique prompt and correction prompt; sometimes uses rubrics or external critic models.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mention of literature demonstrating iterative refinement benefits (Madaan et al., 2023) alongside literature showing failure modes (Huang et al., 2024). The present paper cites these to motivate contrast with single-utterance intrinsic correction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prior work shows that in some settings multi-turn self-correction can worsen outcomes if critique prompts are uninformative; dependence on prompt quality and external verifier availability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared in-text with single-utterance intrinsic self-correction (this work) and with extrinsic verifier / tool-augmented approaches (Appendix E.1).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8735.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8735.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-rewarding / self-critique-as-reward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-rewarding language models (generator-as-judge using rubric to assign scalar reward)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative strategy where the generator judges its own outputs using a rubric and assigns scalar rewards to guide selection or training, enabling self-improvement without an external reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-rewarding language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Technique described in Yuan et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach where the model evaluates its own generations using a rubric and yields scalar scores used to choose or refine outputs; presented as an iterative self-critique/selection mechanism in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generator-as-judge self-critique (rubric + scalar reward)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model acts as its own judge, assigning scalar rewards to candidate generations according to a rubric, which can steer decoding/selection or training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General generation/evaluation tasks (as in Yuan et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to improve generation quality by ranking or weighting candidate outputs based on the model's own rubric-based assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Cited as an effective iterative strategy in prior work; no numeric performance figures from the current paper (mention-only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Model-internal evaluation producing scalar rewards (rubric-based); can be combined with decoding/search.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mention of Yuan et al. (2024) applying this iterative strategy; current paper references it as part of the broader literature on self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in detail in this paper (only referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned alongside prompt-based self-refinement and RL-based elicitation approaches; not directly compared experimentally in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8735.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8735.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R1 (DeepSeek)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R1 (DeepSeek) reasoning model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large reasoning-focused model (DeepSeek's R1) advertised for strong intrinsic self-evaluating behavior; evaluated in this paper and observed to be robust to synthetic perturbations relative to most other evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>R1 (DeepSeek)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Advertised 'reasoning' model from DeepSeek; reported parameterization in the paper as 671B and evaluated in FP8 precision (Table A2). Described in related work as having self-evaluating behavior that can be distilled into smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Intrinsic self-evaluation elicited via reinforcement learning finetuning (advertised)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>R1 is described (in the paper and related work) as exhibiting a characteristic self-evaluating generation style likely elicited by RL-based objectives; in experiments R1 was evaluated by completing perturbed reasoning stubs in a single utterance and showed little meaningful degradation compared to other models.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, GSM-Symbolic, MATH-500 (same benchmarks used in main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math reasoning benchmarks used to probe model robustness to perturbed chain-of-thought stubs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper states that 'all models other than R1 experience meaningful performance degradations' under perturbation — R1 maintained high accuracy across scenarios where others fell sharply (paper includes high accuracy entries for R1 in figures/tables, but full numeric per-dataset pass@1 values are in Table 1/figures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct (unperturbed) pass@1 baseline used; exact numeric baseline per model is reported in Table 1 but not enumerated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Model-internal: RL finetuning and model generation style reportedly produce self-evaluation signals; in the current paper R1's behavior is observed via single-utterance completion of perturbed stubs (no external verifier used during generation).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical: R1 did not suffer the large absolute drops in success rate observed for most other models when presented with perturbed reasoning stubs (aggregate comparisons discussed in Results; R1's high accuracy shown in figures/tables).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes provider reliability issues for R1 (some absent responses); inability to evaluate some open models; also the experimental caveat that perturbations were produced off-policy by LLaMA 3.1 405B and may not represent the hardest, high-probability errors R1 would encounter in production.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared experimentally to other evaluated models in the single-utterance perturbed-stub protocol; R1 stands out as much more robust relative to most other models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8735.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8735.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ 32B (Qwen family)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwQ 32B Preview (Qwen Team)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Qwen-family 'reasoning' model (32B) evaluated in the paper; it demonstrates sensitivity to the style/origin of reasoning stubs when attempting to re-initiate its self-evaluative reasoning style.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ 32B Preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 32B-parameter model from the Qwen family, described/advertised as a reasoning-focused model; evaluated in BF16 precision (Table A2) via an inference provider.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reinforcement-learned self-evaluating generation style / single-utterance recovery</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>QwQ exhibits a characteristic self-evaluating generation style (likely elicited via RL finetuning), but its ability to initiate that style depends on continuing from stubs with a familiar style; when forced to continue from off-policy stubs produced by another model, QwQ's off-policy completion performance degrades noticeably.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, GSM-Symbolic, MATH-500</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math reasoning benchmarks used in paper's perturbed-stub experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>QwQ showed robust recovery in some on-policy contexts, but 'exhibited degraded performance' when completing perturbed off-policy stubs; the off-policy drop suggests its self-evaluative style is coupled to stub style. Exact numeric performance is reported in Table 1/figures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct/unperturbed pass@1 baseline used (numbers in Table 1 but not quoted in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Model-internal generation style induced by RL finetuning; in experiments elicited by continuing generation from a perturbed stub (prompt-based, single-utterance).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical: QwQ matched or outperformed some models on certain datasets in on-policy settings (e.g., MATH-500), but showed a performance drop when completing off-policy stubs, indicating that self-correction behavior depends on stub style.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Off-policy fragility: QwQ's reasoning/self-evaluation capability appears coupled to familiar generation styles and can degrade when forced to continue from unfamiliar stubs. Provider/inference reliability issues affected data collection for QwQ in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with R1 and non-'reasoning' models which sometimes displayed implicit or explicit single-utterance corrections regardless of stub origin; suggests RL-finetuning can couple style with capability.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Self-rewarding language models <em>(Rating: 2)</em></li>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 2)</em></li>
                <li>Huang et al. (2024) (critique of self-correction experiments) <em>(Rating: 2)</em></li>
                <li>DeepSeek (2025) R1 technical report / R1-Zero <em>(Rating: 2)</em></li>
                <li>Qwen2.5 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8735",
    "paper_id": "paper-279465189",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Single-Utterance Intrinsic Self-Correction",
            "name_full": "Single-Utterance Intrinsic Self-Correction of Perturbed Reasoning",
            "brief_description": "A test-time behavior where a language model, within a single uninterrupted generation, detects an introduced error in its chain-of-thought, acknowledges it (explicitly or implicitly) and produces a corrected final answer without an external verifier or a separate critique turn.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various evaluated models (Command R7B, Nemo 12B, Gemma 2 27B, QwQ 32B Preview, LLaMA 3.3 70B, Qwen 2.5 72B, R1 671B)",
            "model_description": "Set of modern LLMs evaluated in this paper spanning ~7B to 671B parameters; precisions and providers vary (e.g., BF16, FP8) as reported in Table A2. Included both models advertised as 'reasoning' (QwQ, R1) and general-purpose models (Gemma, Nemo, Command).",
            "reflection_method_name": "Single-utterance intrinsic self-correction",
            "reflection_method_description": "The model is given a partially-complete reasoning 'stub' that has been synthetically perturbed; the model continues generation in one uninterrupted utterance, and may (a) proceed with the incorrect trajectory, (b) implicitly correct intermediate steps, or (c) explicitly acknowledge an error and emit corrected computation and final answer — all inside the same generated output. No external verifier or multi-turn critique is used during generation.",
            "task_name": "Math reasoning tasks (GSM8K, GSM-Symbolic, MATH-500, GSM8K Matched)",
            "task_description": "Standard math word-problem benchmarks: GSM8K (grade-school math problems), GSM-Symbolic (template variants of GSM8K), MATH-500 (subset of competition-level math problems).",
            "performance_with_reflection": "Qualitative and aggregate metrics reported: models showed varying degrees of recovery when completing perturbed reasoning stubs in a single utterance. Aggregate statistics include an average absolute drop (in on-policy perturbed scenario) of 61.6% across evaluated models excluding R1; smaller models (&lt;30B) showed a larger average absolute drop of 78.1%, larger models (&gt;30B) 41.7%. Some large models (Qwen 2.5 72B, LLaMA 3.3 70B) showed relatively robust recovery on MATH-500 and GSM8K.",
            "performance_without_reflection": "Baseline 'Direct Solutions' (unperturbed pass@1) were used as reference (numerical values are given in Table 1 but not enumerated in the main text). The paper reports direct/unperturbed pass@1 as the baseline against which perturbed completion success rates are compared.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineering / intrinsic generation behavior: the experiment forces continuation from a perturbed reasoning stub and observes the model's natural completion. The mechanism is wholly model-internal (no external verifier during generation); verification is performed post hoc by a grader model (LLaMA 3.1 405B) that checks final answer correctness.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative: observed recoveries from perturbed stubs where some models re-derived correct answers in a single generation; aggregate drop/recovery statistics (see 'performance_with_reflection'). Qualitative: examples in Appendix D and Figure 1 show explicit mid-generation corrections and use of pivot tokens (e.g., 'Wait,' 'However,' 'Hold on') coinciding with corrected computations.",
            "limitations_or_failure_cases": "Reported limitations include: models still commonly fail to detect simple introduced errors; off-policy perturbations (perturbations produced by a held-out model) may be artificially easy to detect; inability to observe token-level probabilities (API restrictions) limits realism of perturbations; dataset coverage limited; perturbation types biased toward low-level arithmetic/operator corruptions rather than high-level reasoning-backtracking; style-capability coupling (e.g. QwQ performs worse when continuing off-policy stubs).",
            "comparison_to_other_methods": "Paper contrasts single-utterance intrinsic self-correction with multi-turn generate-critique-correct pipelines and extrinsic-feedback approaches, noting prior mixed findings (e.g., some literature reports multi-turn self-critique helps, while Huang et al. 2024 found it can worsen performance in weakly-specified critique prompts). The paper positions single-utterance intrinsic recovery as complementary and sometimes naturally present in base models.",
            "ablation_study_results": null,
            "uuid": "e8735.0",
            "source_info": {
                "paper_title": "Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Generate-Critique-Correct (Self-refine)",
            "name_full": "Three-turn generate-critique-correct iterative refinement (Self-refine)",
            "brief_description": "A popular multi-turn prompt-based self-correction technique where a model first generates an answer, then is prompted to critique its own output, and finally produces a corrected answer (can be iterated).",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "Prompted multi-turn self-correction (literature technique)",
            "model_description": "Technique commonly applied with large language models via crafted prompts that request a critique and a correction; may be iterated multiple times in practice.",
            "reflection_method_name": "Generate-then-critique-then-correct (multi-turn self-refinement)",
            "reflection_method_description": "A three-turn cycle: (1) model generates an initial answer, (2) model (or separate critic) generates a critique of that answer, and (3) the model produces a corrected answer, sometimes repeating the critique-and-correct loop.",
            "task_name": "General QA and reasoning tasks (as used in cited literature)",
            "task_description": "Applied broadly across reasoning and QA benchmarks in prior work to improve final-answer correctness through iterative refinement.",
            "performance_with_reflection": "Cited prior work (Madaan et al., 2023) reports improvements in some settings; however, Huang et al. (2024) is cited as showing multi-turn self-correction can sometimes degrade performance when critique prompts lack helpful criteria. This paper does not reproduce specific numeric comparisons for this method.",
            "performance_without_reflection": "Baseline single-shot or chain-of-thought generation; exact numbers depend on cited studies and are not presented here.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt-engineering: explicit separate critique prompt and correction prompt; sometimes uses rubrics or external critic models.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Mention of literature demonstrating iterative refinement benefits (Madaan et al., 2023) alongside literature showing failure modes (Huang et al., 2024). The present paper cites these to motivate contrast with single-utterance intrinsic correction.",
            "limitations_or_failure_cases": "Prior work shows that in some settings multi-turn self-correction can worsen outcomes if critique prompts are uninformative; dependence on prompt quality and external verifier availability.",
            "comparison_to_other_methods": "Compared in-text with single-utterance intrinsic self-correction (this work) and with extrinsic verifier / tool-augmented approaches (Appendix E.1).",
            "ablation_study_results": null,
            "uuid": "e8735.1",
            "source_info": {
                "paper_title": "Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-rewarding / self-critique-as-reward",
            "name_full": "Self-rewarding language models (generator-as-judge using rubric to assign scalar reward)",
            "brief_description": "An iterative strategy where the generator judges its own outputs using a rubric and assigns scalar rewards to guide selection or training, enabling self-improvement without an external reward model.",
            "citation_title": "Self-rewarding language models",
            "mention_or_use": "mention",
            "model_name": "Technique described in Yuan et al. (2024)",
            "model_description": "Approach where the model evaluates its own generations using a rubric and yields scalar scores used to choose or refine outputs; presented as an iterative self-critique/selection mechanism in the literature.",
            "reflection_method_name": "Generator-as-judge self-critique (rubric + scalar reward)",
            "reflection_method_description": "The model acts as its own judge, assigning scalar rewards to candidate generations according to a rubric, which can steer decoding/selection or training.",
            "task_name": "General generation/evaluation tasks (as in Yuan et al., 2024)",
            "task_description": "Used to improve generation quality by ranking or weighting candidate outputs based on the model's own rubric-based assessment.",
            "performance_with_reflection": "Cited as an effective iterative strategy in prior work; no numeric performance figures from the current paper (mention-only).",
            "performance_without_reflection": "Not reported in this paper.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Model-internal evaluation producing scalar rewards (rubric-based); can be combined with decoding/search.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mention of Yuan et al. (2024) applying this iterative strategy; current paper references it as part of the broader literature on self-correction.",
            "limitations_or_failure_cases": "Not discussed in detail in this paper (only referenced).",
            "comparison_to_other_methods": "Mentioned alongside prompt-based self-refinement and RL-based elicitation approaches; not directly compared experimentally in this work.",
            "ablation_study_results": null,
            "uuid": "e8735.2",
            "source_info": {
                "paper_title": "Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "R1 (DeepSeek)",
            "name_full": "R1 (DeepSeek) reasoning model",
            "brief_description": "A large reasoning-focused model (DeepSeek's R1) advertised for strong intrinsic self-evaluating behavior; evaluated in this paper and observed to be robust to synthetic perturbations relative to most other evaluated models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "R1 (DeepSeek)",
            "model_description": "Advertised 'reasoning' model from DeepSeek; reported parameterization in the paper as 671B and evaluated in FP8 precision (Table A2). Described in related work as having self-evaluating behavior that can be distilled into smaller models.",
            "reflection_method_name": "Intrinsic self-evaluation elicited via reinforcement learning finetuning (advertised)",
            "reflection_method_description": "R1 is described (in the paper and related work) as exhibiting a characteristic self-evaluating generation style likely elicited by RL-based objectives; in experiments R1 was evaluated by completing perturbed reasoning stubs in a single utterance and showed little meaningful degradation compared to other models.",
            "task_name": "GSM8K, GSM-Symbolic, MATH-500 (same benchmarks used in main experiments)",
            "task_description": "Math reasoning benchmarks used to probe model robustness to perturbed chain-of-thought stubs.",
            "performance_with_reflection": "Paper states that 'all models other than R1 experience meaningful performance degradations' under perturbation — R1 maintained high accuracy across scenarios where others fell sharply (paper includes high accuracy entries for R1 in figures/tables, but full numeric per-dataset pass@1 values are in Table 1/figures).",
            "performance_without_reflection": "Direct (unperturbed) pass@1 baseline used; exact numeric baseline per model is reported in Table 1 but not enumerated in the main text.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Model-internal: RL finetuning and model generation style reportedly produce self-evaluation signals; in the current paper R1's behavior is observed via single-utterance completion of perturbed stubs (no external verifier used during generation).",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Empirical: R1 did not suffer the large absolute drops in success rate observed for most other models when presented with perturbed reasoning stubs (aggregate comparisons discussed in Results; R1's high accuracy shown in figures/tables).",
            "limitations_or_failure_cases": "Paper notes provider reliability issues for R1 (some absent responses); inability to evaluate some open models; also the experimental caveat that perturbations were produced off-policy by LLaMA 3.1 405B and may not represent the hardest, high-probability errors R1 would encounter in production.",
            "comparison_to_other_methods": "Compared experimentally to other evaluated models in the single-utterance perturbed-stub protocol; R1 stands out as much more robust relative to most other models.",
            "ablation_study_results": null,
            "uuid": "e8735.3",
            "source_info": {
                "paper_title": "Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "QwQ 32B (Qwen family)",
            "name_full": "QwQ 32B Preview (Qwen Team)",
            "brief_description": "A Qwen-family 'reasoning' model (32B) evaluated in the paper; it demonstrates sensitivity to the style/origin of reasoning stubs when attempting to re-initiate its self-evaluative reasoning style.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QwQ 32B Preview",
            "model_description": "A 32B-parameter model from the Qwen family, described/advertised as a reasoning-focused model; evaluated in BF16 precision (Table A2) via an inference provider.",
            "reflection_method_name": "Reinforcement-learned self-evaluating generation style / single-utterance recovery",
            "reflection_method_description": "QwQ exhibits a characteristic self-evaluating generation style (likely elicited via RL finetuning), but its ability to initiate that style depends on continuing from stubs with a familiar style; when forced to continue from off-policy stubs produced by another model, QwQ's off-policy completion performance degrades noticeably.",
            "task_name": "GSM8K, GSM-Symbolic, MATH-500",
            "task_description": "Math reasoning benchmarks used in paper's perturbed-stub experiments.",
            "performance_with_reflection": "QwQ showed robust recovery in some on-policy contexts, but 'exhibited degraded performance' when completing perturbed off-policy stubs; the off-policy drop suggests its self-evaluative style is coupled to stub style. Exact numeric performance is reported in Table 1/figures.",
            "performance_without_reflection": "Direct/unperturbed pass@1 baseline used (numbers in Table 1 but not quoted in main text).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Model-internal generation style induced by RL finetuning; in experiments elicited by continuing generation from a perturbed stub (prompt-based, single-utterance).",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Empirical: QwQ matched or outperformed some models on certain datasets in on-policy settings (e.g., MATH-500), but showed a performance drop when completing off-policy stubs, indicating that self-correction behavior depends on stub style.",
            "limitations_or_failure_cases": "Off-policy fragility: QwQ's reasoning/self-evaluation capability appears coupled to familiar generation styles and can degrade when forced to continue from unfamiliar stubs. Provider/inference reliability issues affected data collection for QwQ in some cases.",
            "comparison_to_other_methods": "Contrasted with R1 and non-'reasoning' models which sometimes displayed implicit or explicit single-utterance corrections regardless of stub origin; suggests RL-finetuning can couple style with capability.",
            "ablation_study_results": null,
            "uuid": "e8735.4",
            "source_info": {
                "paper_title": "Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Self-rewarding language models",
            "rating": 2,
            "sanitized_title": "selfrewarding_language_models"
        },
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 2,
            "sanitized_title": "selfcritiquing_models_for_assisting_human_evaluators"
        },
        {
            "paper_title": "Huang et al. (2024) (critique of self-correction experiments)",
            "rating": 2,
            "sanitized_title": "huang_et_al_2024_critique_of_selfcorrection_experiments"
        },
        {
            "paper_title": "DeepSeek (2025) R1 technical report / R1-Zero",
            "rating": 2,
            "sanitized_title": "deepseek_2025_r1_technical_report_r1zero"
        },
        {
            "paper_title": "Qwen2.5 technical report",
            "rating": 1,
            "sanitized_title": "qwen25_technical_report"
        }
    ],
    "cost": 0.01712575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning
18 Jun 2025</p>
<p>Sam Silver 
Ivan Zhang 
Sara Hooker 
Eddie Kim Cohere 
Yuntao Bai 
Saurav Kadavath 
Sandipan Kundu 
Amanda Askell 
Jackson Kernion 
Andy Jones 
Anna Chen 
Anna Goldie 
Azalia Mirhoseini 
Cameron Mckinnon 
Carol Chen 
Catherine Olsson 
Christo- Pher Olah 
Danny Hernandez 
Dawn Drain 
Deep Ganguli 
Dustin Li 
Eli Tran-Johnson 
Ethan Perez 
Jamie Kerr 
Jared Mueller 
Jeffrey Ladish 
Joshua Landau 
Kamal Ndousse 
Kamile Lukosuite 
Liane Lovitt 
Michael Sellitto 
Nelson Elhage 
Nicholas Schiefer 
Noemi Mercado 
Nova Dassarma 
Robert Lasenby 
Robin Larson 
Sam Ringer 
Scott John- Ston 
SheerShauna Kravec 
El Showk 
Stanislav Fort 
Tamera Lanham 
Timothy Telleen-Lawton 
Tom Henighan 
Tristan Hume 
Samuel R Bow- Man 
Zac Hatfield-Dodds 
Ben Mann 
Dario Amodei 
Nicholas Joseph 
Sam Mccandlish 
Tom Brown 
Jared 2022 Kaplan 
Constitutional 
Xinyun Chen 
Ryan A Chi 
Xuezhi Wang 
Denny 2024 Zhou 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Mark Chen 
Heewoo Jun 
Lukasz Kaiser 
Matthias Plappert 
Jerry Tworek 
Jacob Hilton 
Reiichiro Nakano 
Christopher Hesse 
Pei Ke 
Bosi Wen 
Andrew Feng 
Xiao Liu 
Xuanyu Lei 
Jiale Cheng 
Shengyuan Wang 
Aviral Kumar 
Vincent Zhuang 
Rishabh Agarwal 
Yi Su 
John D Co-Reyes 
Avi Singh 
Kate Baumli 
Shariq Iqbal 
Colton Bishop 
Rebecca Roelofs 
Lei M Zhang 
Kay Mckinney 
Disha Shrivastava 
Cosmin Paduraru 
George Tucker 
Doina Precup 
Feryal Behbahani 
Aleksandra 2024 Faust 
Nathan Lambert 
Jacob Morrison 
Valentina Pyatkin 
Aitor Lewkowycz 
Anders Andreassen 
David Dohan 
Ethan Dyer 
Henryk Michalewski 
Vinay Ramasesh 
Ambrose Slone 
Cem Anil 
Imanol Schlag 
Theo Gutman-Solo 
Yuhuai Wu 
Behnam Neyshabur 
Guy Gur-Ari 
Vedant 2022 Misra 
Jia Li 
Edward Beeching 
Lewis Tunstall 
Ben Lipkin 
Roman Soletskyi 
Costa Shengyi 
Kashif Huang 
Longhui Rasul 
Albert Yu 
Ziju Jiang 
Zihan Shen 
Bin Qin 
Li Dong 
Yann Zhou 
Guillaume Fleureau 
Stanislas Lample 
Polu 
Numinamath </p>
<p>UC Santa Cruz Jimin Sun Cohere
CMU</p>
<p>Aohan Zeng
Hongning WangYuxiao Dong</p>
<p>Jie Tang, and Minlie Huang
2024</p>
<p>Faeze Brahman
Shengyi Huang
Hamish Ivison</p>
<p>Lester James V. Miranda
Alisa Liu</p>
<p>Nouha Dziri
Shane Lyu, Yuling Gu, Saumya Malik, Jena D. Hwang, Jiangjiang YangVictoria Graf</p>
<p>Ronan Le Bras
Oyvind Tafjord, Chris WilhelmLuca Soldaini</p>
<p>Noah A. Smith
Yizhong Wang, Pradeep Dasigi</p>
<p>Hannaneh Hajishirzi
2024</p>
<p>Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning
18 Jun 20258950495461FE0F945B22DABB9F87CEEAarXiv:2506.15894v1[cs.CL]
Large Language Models (LLMs) have demonstrated impressive mathematical reasoning capabilities, yet their performance remains brittle to minor variations in problem description and prompting strategy.Furthermore, reasoning is vulnerable to sampling-induced errors which autoregressive models must primarily address using self-correction via additionallygenerated tokens.To better understand selfcorrection capabilities of recent models, we conduct experiments measuring models' ability to self-correct synthetic perturbations introduced into their Chain of Thought (CoT) reasoning.We observe robust single-utterance intrinsic self-correction behavior across a range of open-weight models and datasets, ranging from subtle, implicit corrections to explicit acknowledgments and corrections of errors.Our findings suggest that LLMs, including those not finetuned for long CoT, may possess stronger intrinsic self-correction capabilities than commonly shown in the literature.The presence of this ability suggests that recent "reasoning" model work involves amplification of traits already meaningfully present in models.Mistral AI. 2024.Mistral NeMo: A state-of-the-art 12b model.Blog post.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have shown progressively impressive performance in mathematical domains (Cobbe et al., 2021;Hendrycks et al., 2021;Lewkowycz et al., 2022;Yang et al., 2024), owing largely to improvements in data curation and post-training techniques.</p>
<p>At inference time, researchers have found that performance can be substantially improved by encouraging models to generate natural language rationales that allow for an adaptive amount of computation for each subproblem (Nye et al., 2022;Wei et al., 2022;Zhou et al., 2023;Zheng et al., 2024, inter alia).</p>
<p>However, despite the apparent sophistication of LLM reasoning capabilities, recent work has documented a variety of reasoning failure modes.For example, models have a tendency to fall into poorly performing reasoning patterns when presented with familiar but subtly modified problems (Mirzadeh et al., 2024), can be easily distracted with irrelevant context (Shi et al., 2023), and are brittle to changes in premise ordering (Chen et al., 2024).Critically, LLMs struggle to identify their own errors and contradictions, making it difficult to trust outputs without external verification.</p>
<p>While a three-turn generate-critique-correct process with optimized prompting is popular in selfcorrection literature (Madaan et al., 2023), recent trends in frontier language model releases (Ope-nAI, 2024; Qwen Team, 2024;Pichai et al., 2024;DeepSeek, 2025) point to a growing interest in models' ability to perform self-evaluation intrinsically at test-time in a single-utterance, without aid from external verifiers.A critical component of this behavior is intrinsic self-correction, when models recognize an error in their reasoning, acknowledge the mistake, and output a corrected generation.</p>
<p>To better understand current capabilities around single-utterance intrinsic self-correction, we introduce a novel experimental framework focused on evaluating how LLMs recover from perturbations in their reasoning chains.Our results reveal that language models, even those not trained as "reasoning" models, can successfully recover from introduced reasoning perturbations, exhibiting both implicit and explicit self-correction behavior.</p>
<p>Related Work</p>
<p>In contrast to approaches that rely on external feedback (See Appendix E.1), recent work has explored methods to enable LLM self-correction using only their own parametric knowledge.</p>
<p>Prompt-based self-correction techniques involve models reviewing and revising their own outputs, checking for potential errors, inconsistencies, or misalignment (Bai et al., 2022;Saunders et al., 2022).These self-refinement processes can be iter- ated, allowing for rounds of reflection and refactoring to improve responses (Madaan et al., 2023;Ye et al., 2023).Yuan et al. (2024) applies a similar iterative strategy where self-critique takes the form of the generator itself acting as a judge of its own responses, using a rubric and its own judgment to assign a scalar reward to generations.</p>
<p>Other approaches aim to develop models that robustly recognize and correct their own errors at a level beyond that offered by simple prompting by incorporating self-correction training into model training (Kumar et al., 2024).</p>
<p>The most recent and emerging advances in intrinsic self-evaluation focus on single-utterance techniques in which models continuously monitor, assess, and refine their generation trajectories (OpenAI, 2024;Qwen Team, 2024).Lambert et al. (2024), DeepSeek (2025), and Kimi Team (2025) have offered concrete insights into how simple reinforcement learning (RL) against verifiable outcomes effectively elicits improved reasoning performance and qualitatively similar generation styles.In particular, DeepSeek's R1-Zero highlights that self-evaluating behavior can be elicited directly from high-quality base models, and that this behavior can be distilled into models as small as 1.5B parameters (DeepSeek, 2025).</p>
<p>Still, there exists criticism of self-correction experiments as commonly-performed in literature: Huang et al. ( 2024) present perhaps the most direct challenge to the optimism surrounding self-correction capabilities, finding that language models, in a three-turn generate-critique-correct pipeline, not only struggle to reliably correct their own reasoning, but often perform worse after attempting intrinsic self-correction in a setting in which helpful information and criteria are not imparted into the critique prompt.</p>
<p>In contrast to either the extrinsic feedback approaches or multi-turn prompt-based intrinsic correction approaches, we examine models' ability to perform single-utterance intrinsic self-correction of introduced perturbations.</p>
<p>Experiments</p>
<p>To better understand self-correction capabilities in language models, we designed an experimental framework to measure the elicitation of intrinsic self-correction under synthetically perturbed reasoning trajectories.We evaluate a variety of models' recovery performance in the context of popular math reasoning datasets (See Appendix E.2).</p>
<p>For each model, our approach involves four phases detailed in Figure B1 and Appendix C: First, each evaluated model is prompted with a reasoning problem and generates a 100-token solution "stub."These stubs empirically contain enough progress to enable effective perturbation, but not so much as to leave no headroom for recovery.</p>
<p>Next, a held-out model (LLaMA 3.1 405B) applies a reasoning perturbation to the solution stub.Perturbations include changing decimal places (e.g. from "1.5" to "15"), switching operators (e.g. from × to ÷), altering a key phrase (e.g. from "60% of $5" to "60% more than $5"), or a number of other perturbations (shown in Figure C3) similar to those used in Sun et al. (2024).</p>
<p>Then, the model under evaluation completes the generation stemming from the perturbed reasoning stub to finish the candidate solution.This stub generation and completion are seen from the model's perspective a single, uninterrupted utterance.Finally, a grader model (LLaMA 3.1 405B) with access to the ground-truth solution determines candidate solution correctness.</p>
<p>Models and Datasets</p>
<p>We evaluate seven modern language models of various size and origin, aiming to cover a range of sizes across diverse model families.Models include Command R7B (Cohere, 2024), Nemo 12B (AI, 2024), Gemma 2 27B (Team, 2024), QwQ 32B Preview (Qwen Team, 2024), LLaMA 3.3 70B (et al., 2024), Qwen 2.5 72B (Team et al., 2024), andR1 (DeepSeek, 2025).QwQ and R1 are advertised as "reasoning" models.For perturbation generation and solution verification, we employ LLaMA 3.1 405B.See Table A2 for more information on inference providers and model precision.</p>
<p>These models are evaluated on three popular math reasoning datasets: GSM8K (Cobbe et al., 2021), GSM-Symbolic (Mirzadeh et al., 2024), and MATH-500, a subset of the popular competition math dataset (Hendrycks et al., 2021) as employed in Lightman et al. (2023).We additionally re-evaluated on the subset of the GSM8K dataset from which our GSM-Symbolic template-swapped sample was derived, which we refer to as "GSM8K Matched" (See Appendix E.2).</p>
<p>Evaluation Scenarios</p>
<p>We evaluated models across three scenarios:</p>
<p>Direct Solutions: We evaluate each model's natural, unperturbed "pass@1" rate, giving a single opportunity to correctly solve each problem as determined by a grader language model with access to the ground-truth solution.</p>
<p>Perturbed On-Policy Reasoning Stub: We perform the four-phase workflow described in Section 3 using our evaluated model to generate a reasoning stub which is then perturbed and completed.</p>
<p>Perturbed Off-Policy Reasoning Stub: We perform a similar set of experiments using a heldout language model to produce common reasoning stubs in the same four-phase workflow described in Section 3.This controlled set of perturbed stubs are individually completed by all evaluated models for an apples-to-apples comparison.</p>
<p>Model correction performance is evaluated using a simple accuracy metric, with the success rate S computed as S a = c N , where c is the number of correct solutions as determined by our grader language model and N is the number of problems in the dataset.Models are accessed via OpenRouter or Cohere APIs in their original precision.Solution generation uses Top-P sampling (Holtzman et al., 2020) with P=0.8 and T=0.2, while perturbation and verification use greedy decoding.</p>
<p>Results</p>
<p>Our results demonstrate that self-correction capabilities are found across all evaluated models in the context of synthetic reasoning perturbations, even those not explicitly advertised as having been trained in single-utterance self-correction.Table 1 shows accuracy performance across all  datasets, scenarios, and models.Several key findings emerged from our experiment: First, all models other than R1 experience meaningful performance degradations when errors are synthetically introduced into their reasoning process.We observe an average absolute drop in success rates (excluding R1) of 61.6% in the on-policy scenario, with the smallest models suffering the largest relative performance drops.</p>
<p>We observe that smaller models (&lt; 30B) experience a larger average drop of 78.1% in absolute success rates, while larger models (&gt;30B) experience a more modest drop of 41.7% (ignoring R1).Qwen 2.5 72B and LLaMA 3.3 70B show surprisingly robust self-correction capabilities relative to QwQ 32B Preview and R1's impressive "reasoning" model performances, approximately matching the recovery performance of QwQ on MATH-500.</p>
<p>Recovery performance on GSM-Symbolic and MATH-500 is consistent with GSM8K results, suggesting that observed self-correction capabilities are not dataset-specific but rather indicative of general model ability.Interestingly, Figure 2 shows that higher relative recovery rates were observed in the more difficult MATH dataset than in GSM8K.</p>
<p>QwQ exhibited degraded performance in the scenario involving completion of a perturbed offpolicy reasoning stub.Examination of these completions indicates that QwQ's ability to initiate its characteristic self-evaluating style of generation is contingent on the style of the off-policy stub that it continues generation from.This drop in off-policy performance suggests that the reasoning capability induced by QwQ's reinforcement learning finetuning may couple style with capability, with performance degrading when generating outside a familiar format distribution.In contrast, we observe R1 to be much more capable of re-initiating effective reasoning regardless of the reasoning stub's origin.</p>
<p>Finally, we observe a diversity of styles of self-correction behaviors on display from non-"reasoning" models, ranging from implicit correction behavior to explicit, well-aligned corrections as seen in Appendix D. Explicit self-correction examples from non-"reasoning" models include "Wait a minute, let me double-check that because I think I might have made a mistake" and "However, the problem states that the discount is 30%, not 50%.Let's correct this and recalculate."We observe common use of critical "pivot tokens" (e.g."Wait," "However," "Hold on") during generation in a manner reminiscent of the meta-cognitive "aha moment" highlighted in DeepSeek's (2025) R1 technical report.</p>
<p>The meaningful presence of these behaviors in our limited experiment suggests that strong models inherently possess latent self-correction capabilities, helping to explain why recent RL techniques have been particularly effective in promoting and amplifying these patterns.</p>
<p>Conclusion</p>
<p>Our work reveals that current language models may exhibit intrinsic self-correction capabilities more frequently than commonly believed, demonstrating that models can, in a single utterance, detect and recover from errors in their own reasoning chains without explicit prompting or external verification.</p>
<p>However, important limitations remain, as models commonly fail to detect simple introduced errors.Looking ahead, we believe that a better understanding of self-correction capabilities, more investigation into the coupling of style and reasoning in recent models, and improved methods for eliciting such behavior are crucial to developing reliable and trustworthy systems.</p>
<p>There are a variety of limitations of our current analysis that could be explored in future work.</p>
<p>Off-Policy Perturbations: We use a language model to apply perturbations to the reasoning of a model under evaluation.These errors that are introduced are likely to be significantly off-policy with respect to the models under evaluation, potentially making the recovery task artificially simple.Our reliance on API-based inference results in an inability to observe token-level probabilities at critical decision points in the reasoning process.If models were self-hosted, we could generate more realistic perturbations by selecting high-probability but incorrect continuations, creating a more natural experiment.</p>
<p>Dataset Coverage: While we evaluated model recovery on multiple math datasets of varying difficulty, our analysis could benefit from the inclusion of even more challenging math and reasoning benchmarks.An earlier incarnation of our experiment tested recovery on the NuminaMath-CoT (LI et al., 2024) and ZebraLogic (Lin et al., 2025) datasets through a different method of error introduction, but the results were inconclusive.Our current experimental setup could similarly be used to evaluate the robustness of model alignment by introducing misaligned perturbations to assistant responses or to evaluate model instruction-following abilities by violating stated constraints.</p>
<p>Perturbation Abstraction: Our perturbation methodology introduces errors that may be relatively easy for models to detect and correct.Rather than applying low-level perturbations like the corruption of arithmetic operations, future work could consider higher-level perturbations that significantly effect the problem-solving trajectory of the model under evaluation.Such perturbations would be useful in evaluating a model's ability to perform reasoning backtracking.</p>
<p>Assistant Prefill: While we used a battery of heuristic prefill-completion tests to select Open-Router model/provider combinations that seem to support the assistant prefill feature, OpenRouter and downstream inference provider documentation and support for this uncommonly-used feature is lacking, and we cannot guarantee with certainty that the assistant-prefill feature functions as advertised for each model/provider combination.</p>
<p>Model Availability: Several promising openweight models including DeepSeek 2.5 and the recently-released Deepseek V3 could not be evaluated due to the lack of inference provider support for the assistant prefill feature required for models to complete assistant turns prefixed by perturbed reasoning stubs.Similarly, many frontier closedsource models do not expose this feature.</p>
<p>Provider Reliability: We encountered reliability issues with certain model-provider combinations, particularly with QwQ 32B Preview and R1, leading to a small number of absent responses for cases in which 20 retries failed to yield 2XX responses.Although these data collection gaps are relatively small and do not meaningfully effect the results, they highlight the challenges of conducting large-scale evaluations using third-party inference providers.</p>
<p>Scale Effects: Our study does not systematically explore how self-correction capability varies with model scale within the same model family.While we observe positive correlation between model size and self-correction performance, a more controlled study across model scales within model families would be needed to draw stronger conclusions about whether explicit self-correction is emergent with model scale.</p>
<p>Taxonomies: We have not developed a comprehensive taxonomy of either perturbation types or observed correction strategies.Analysis powered by a more detailed categorization of both the kinds of errors introduced and the methods models use to recover could provide insights to improve model robustness.</p>
<p>Ethics Statements</p>
<p>Our research on self-correction capabilities in language models touches on several important ethical considerations.</p>
<p>Reliability and Trust: Understanding how language models detect and correct their own errors is crucial for developing more reliable AI systems, and especially critical in high-stakes applications where unchecked errors in reasoning could have serious social consequences.</p>
<p>Dual Use Considerations: Our work aims to improve our understanding of model self-correction capabilities, but enhancements in self-correction capabilities could improve LM-powered systems designed with nefarious intentions.We acknowledge the dual-use nature of AI systems and their potential for misuse.Evaluated models generate complete solutions without intermediate stubbing or perturbation.An unperturbed pass@1 reference performance to which Off-Policy and On-Policy performance can be compared.Off-Policy Completion LLaMA 3.1 405B generates initial reasoning stub and its perturbation; evaluated models complete generation stemming from a common perturbed reasoning stub.On-Policy Completion Evaluates models generate initial reasoning stub; LLaMA 3.1 405B generates a unique perturbed version of each reasoning stub, and the evaluate model completes generation stemming from its own perturbed reasoning stub.</p>
<p>B Experiment Diagram</p>
<p>C Prompts</p>
<p>Solve the following math or reasoning problem, clearly presenting your reasoning and final answer.</p>
<p>Your input is as follows: <problem> {problem} </problem> You are given a math problem, its ground-truth solution, and a candidate solution to that problem, and your goal is to verify that the candidate solution is correct.</p>
<p>You will be given the following information:</p>
<p>-The problem -The answer to the problem -The candidate solution, which includes reasoning and a final answer You will output a single word, "correct" or "incorrect", to indicate if the candidate solution is a valid solution to the problem.You should not mark a candidate solution as incorrect because of any error in its reasoning.You should only care about whether the final answer produced matches the answer.</p>
<p>Your input is as follows: <problem> {problem} </problem> <answer> {answer} </answer> <candidate_solution> {candidate_solution} </candidate_solution> Now, evaluate the candidate solution by outputting either "correct" or "incorrect", considering the final answer produced.Do not output any other text than "correct" or "incorrect".Do not output any form of reasoning or explanation.Only output "correct" or "incorrect", this is absolutely critical.You will be given a reasoning stub from a math word problem solution.Your task is to perturb this stub by introducing a single mathematical or reasoning error.</p>
<p>Your goal is to modify this stub in a way that will change the results of future reasoning and lead to an incorrect answer if followed.Follow these guidelines:</p>
<ol>
<li>Make minimal changes to the stub.Do not add additional reasoning or try to solve for the right answer.2. Focus on changing numbers, operators, or key words/phrases that will affect the result of the reasoning if it were played out.3. Do not change the overall structure or length of the stub.4. Do not apply any additional formatting (e.g.bolding, italicizing, etc.) your perturbation.</li>
</ol>
<p>Good perturbations may include:</p>
<p>-Changing a number (e.g., from 40 hours to 45 hours) -Switching an operator (e.g., from multiplication to division, or addition to subtraction) -Altering a key phrase (e.g., from "60% of $5" to "60% more than $5") -Reversing the order of operations (e.g., from "(2 + 3) * 4" to "2 + (3 * 4)") -Changing units of measurement (e.g., from "meters" to "feet" without converting) -Swapping comparison operators (e.g., from "greater than (&gt;)" to "less than (&lt;)") -Modifying time relationships (e.g., from "per hour" to "per minute") -Inverting fractions (e.g., from "3/4" to "4/3") -Changing decimal places (e.g., from "1.5" to "15.0") Bad perturbations include: -Adding new steps or explanations -Removing large portions of the stub -Changing the reasoning stub entirely.First, calculate the cost of the first glass and the second glass.\n-The first glass costs $5.\n -The second glass costs 60% more than $5, which is $5 1.60 = $8.\n\n2.Determine the pattern for the cost of the glasses.\n-Every odd-numbered glass (1st, 3rd, 5th" Explanation: The perturbation changed "60% of $5" to "60% more than $5" and consequently updated the calculation from "$5 0.60 = $3" to "$5 1.60 = $8".It will lead to incorrect subsequent calculations.</p>
<p>{{Additional 3 exemplars omitted for brevity}}</p>
<p></examples></p>
<p>Here is the reasoning stub that I want you to perturb:
<reasoning_stub> {stub} </reasoning_stub>
Apply a single perturbation to the reasoning stub.</p>
<p>First, briefly reason about the specific perturbation you will apply.</p>
<p>Then, provide your perturbed version of the reasoning stub as <perturbed_stub>[answer]</perturbed_stub>, where [answer] is your perturbed version of the reasoning stub.Make sure to close your perturbed tags with </perturbed_stub>.Ensure that your perturbation is impactful and that it maintains the original structure and length of the stub as closely as possible.</p>
<p>D Example GSM8K Corrections</p>
<p>E Supplementary Content E.1 Extrinsic Feedback Approaches</p>
<p>A significant line of work has focused on augmenting language models with external verification components.</p>
<p>Tool augmentation approaches enhance language model capabilities by providing access to external tools that can verify outputs or assist in error-prone computation.These approaches can provide reliable verification in specific domains, but are somewhat limited to tasks where appropriate tools exist (Gou et al., 2024;Qiao et al., 2024).</p>
<p>Other approaches use a separate model trained specifically to detect errors or verify the output of a primary language model.These learned verifiers and critics are often instantiated from trained language models and further trained using human feedback to develop more specialized capabilities (Wang et al., 2023;Ke et al., 2024;Li et al., 2024;Cui et al., 2024;Welleck et al., 2023).Unlike tool-based approaches, learned verifiers and critics can potentially operate across a broader range of domains, though their effectiveness depends on the quality and coverage of their training data.Still other techniques use external reward models that offer scalar rewards to generations rather than textual critiques.These scalar rewards are combined with search-inspired decoding strategies at test time to generate higher-quality trajectories (Uesato et al., 2022).</p>
<p>In contrast, multi-agent debate frameworks leverage multiple instances of language models trained and/or prompted to critique and refine eachothers' outputs through structured dialogue.Models take on specialized roles in the debate, such as proposer, critic, and judge, working together to identify and correct errors through iterative refinement (Du et al., 2023;Liang et al., 2024).2021) developed a high-quality dataset of human-authored grade school-level math word problems centered around real-world scenarios.Problems are designed to require 2-8 steps of basic arithmetic operations to solve.We evaluate model performance against the 1,319-problem test split.GSM8K data was originally collected using freelance contractors on Upwork, then scaled with Surge AI, an NLP data labeling platform.License: MIT.</p>
<p>GSM-Symbolic: Mirzadeh et al. ( 2024) introduced a programmatically-generated benchmark derived from the GSM8K dataset, employing symbolic templates that enable the generation of diverse variants of familiar grade-school math problems while preserving their underlying reasoning structure and correctness.We use a 100-problem subset derived from 100 unique GSM8K problems.License: cc-by-nc-nd-4.0.</p>
<p>MATH: Hendrycks et al. ( 2021) contributed a math reasoning benchmark drawn from high school math competitions covering a range of problem difficulties across seven diverse subject areas.For cost and expediency, we evaluate models on the 500-problem MATH-500 subset of the test split as seen in Lightman et al. (2023).License: MIT.</p>
<p>GSM8K Matched: To better understand how recovery performance of models is affected by the template-based substitutions of the 100-record sample of GSM-Symbolic used in our experiments, we included results for the GSM8K Matched dataset, which is simply GSM8K filtered to the same 100 problems that were used to derive our specific GSM-Symbolic sample.Comparing the recovery performance between GSM8K Matched and GSM-Symbolic is a way to assess whether dataset familiarity played a significant role in the self-correction behavior of models under evaluation.</p>
<p>Figure 1 :
1
Figure 1: Truncated excerpt of a candidate solution (brown) showing LLaMA 3.3 70B explicitly self-correcting (green) mid-generation during single-utterance completion of a perturbed (red) on-policy reasoning stub.</p>
<p>.0 41.8 93.0 45.0 46.0 95.0 43.0 39.0 85.1 53.8 54.0 R1 (671B) 96.4 87.0 89.3 98.4 76.8 80.0 95.0 88.0 90.0 91.9 90.0 90.0</p>
<p>Figure A1 :
A1
Figure A1: Accuracy results across all models, datasets, and scenarios (Direct, On-Policy Stub, Off-Policy Stub)</p>
<p>Figure B1 :
B1
Figure B1: Diagram of experiment structure, showing the flow of data between language model calls</p>
<p>Figure C1 :
C1
Figure C1: Prompt used by models under evaluation for generation of initial reasoning stubs and stub completions.</p>
<p>Figure C2 :
C2
Figure C2: Prompt used for verification of candidate solutions via LLaMA 3.1 405B.</p>
<p>... (Continued in the next figure) ...</p>
<p>Figure C3 :
C3
Figure C3: Prompt used for perturbation of reasoning by LLaMA 3.1 405B (Part 1/2).</p>
<p>Figure C4 :
C4
Figure C4: Prompt used for perturbation of reasoning stubs by LLaMA 3.1 405B (Part 2/2).</p>
<p>Figure D1 :
D1
Figure D1: Gemma 27B Implicit Self-Correction</p>
<p>E. 2
2
Dataset Descriptions GSM8K: Cobbe et al. (</p>
<p>Table 1 :
1
Completion recovery success rate shown across datasets.Direct, On, and Off refer to our Direction Solution, perturbed on-policy reasoning stub, and perturbed off-policy reasoning stub scenarios, respectively.Models range in size from 7B to 671B and are ordered by parameter count, ascending.</p>
<p>Table A1 :
A1
Evaluation scenarios for testing self-correction capabilities
ModelPrecision Inference ProviderCommand R7B?CohereNemo 12BBF16DeepInfraGemma 2 27BBF16DeepInfraQwQ 32B Preview BF16DeepInfraLLaMA 3.3 70BBF16NovitaQwen 2.5 72BBF16DeepInfraR1 (671B)FP8Together</p>
<p>Table A2 :
A2
Provider and precision details of models evaluated in our experiments.Cohere R7B is not open-weight, but likely provided by Cohere in its original precision.</p>
<p>CritiqueLLM: Towards an informative critique generation model for evaluation of large language model generation.In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13034-13054, Bangkok, Thailand.Association for Computational Linguistics.Kimi Team.2025.Kimi k1.5: Scaling reinforcement learning with LLMs.
Generative judge for evaluating alignment. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu, International Conference on Learning Representations. 2024</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu, 2024</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. Bowen Baker, Teddy LeeJan</p>
<p>Zebralogic: On the scaling limits of llms for logical reasoning. Ronan Bill Yuchen Lin, Kyle Le Bras, Ashish Richardson, Radha Sabharwal, Peter Poovendran, Yejin Clark, Choi, 2025</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar, 2024</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, International Conference on Learning Representations. 2022</p>
<p>Learning to Reason with LLMs. 2024Ope-nAI BlogOpenAI</p>
<p>Introducing Gemini 2.0: Our new AI model for the agentic era. Demis Sundar Pichai, Koray Hassabis, Kavukcuoglu, 2024The Keyword (Google Blog</p>
<p>Making language models better tool learners with execution feedback. Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, Ningyu Zhang, Annual Meeting of the Association for Computational Linguistics. 2024</p>
<p>Qwq: Reflect deeply on the boundaries of the unknown. Qwen Team, 2024Blog post</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jan Leike. 2022Jonathan Ward</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, Denny Zhou, International Conference on Machine Learning. 2023</p>
<p>Tools fail: Detecting silent errors in faulty tools. Jimin Sun, So Yeon Min, Yingshan Chang, Yonatan Bisk, 2024</p>
<p>Gemma 2: Improving open language models at a practical size. Gemma Team, 2024</p>
<p>. Qwen Team, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Xingzhang Ren, Xuancheng Ren,and Zihan Qiu. 2024. Qwen2.5 technical report</p>
<p>Solving math word problems with process-and outcomebased feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, 2022</p>
<p>Tianlu Wang, Ping Yu, Ellen Xiaoqing, Sean O Tan, Ramakanth 'brien, Jane Pasunuru, Olga Dwivedi-Yu, Luke Golovneva, Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023. Shepherd: A critic for language model generation. </p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Generating sequences by learning to selfcorrect. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, International Conference on Learning Representations. 2023</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. </p>
<p>SelFee: Iterative self-revising LLM empowered by self-feedback generation. Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Minjoon Seo, 2023Blog post</p>
<p>Self-rewarding language models. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, International Conference on Machine Learning. 2024</p>
<p>Take a step back: Evoking reasoning via abstraction in large language models. Swaroop Huaixiu Steven Zheng, Xinyun Mishra, Heng-Tze Chen, Ed H Cheng, Chi, Denny Quoc V Le, Zhou, International Conference on Learning Representations. 2024</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, International Conference on Learning Representations. Quoc Le, Ed Chi, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
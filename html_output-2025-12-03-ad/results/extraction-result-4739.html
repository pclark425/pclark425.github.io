<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4739 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4739</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4739</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-258352268</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.13911v2.pdf" target="_blank">Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering</a></p>
                <p><strong>Paper Abstract:</strong> We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that our proposed methods can significantly enhance question accuracy by fully exploring the synonymous nature of the questions and the consistency of the answers.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4739.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4739.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fed-SP-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Federated Synonymous-Questions with Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that federates multiple rephrasings (synonymous questions with identical parameters) and applies self-consistency/majority voting over the LLM answers to produce a single consistent answer for the federated question set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI text-davinci-002 (GPT-3, ~175B) and text-davinci-003 (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-family generative models used via API: text-davinci-002 (GPT-3 class, ~175B parameters) and text-davinci-003 (GPT-3.5 variant / improved decoder). The paper uses these via zero-shot prompting (including 'Let's think step by step').</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency on Federated Synonymous Questions (Fed-SP-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate multiple synonymous rephrasings of the same problem, obtain an answer for each (using CoT/zero-shot CoT prompting), then aggregate the multiple reasoning paths/answers via majority voting (self-consistency) to select the most-voted final answer. Diversity arises from distinct phrasings and sampled reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and SVAMP (arithmetic / grade-school math reasoning benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: grade-school math word problems requiring multi-step arithmetic reasoning; SVAMP: simple arithmetic word problems with textual variations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K: Zero-Shot-CoT baseline 52.5% -> Fed-SP-SC (GPT-3 Gen.) 62.7%, Fed-SP-SC (GPT-3.5 Gen.) 70.6%. SVAMP: Zero-Shot-CoT baseline 77.2% -> Fed-SP-SC (GPT-3 Gen.) 86.3%, Fed-SP-SC (GPT-3.5 Gen.) 91.1%. (Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Fed-DP-CoT (GPT-3 Gen.) GSM8K 59.2%, SVAMP 82.4%; Fed-DP-CoT (GPT-3.5 Gen.) GSM8K 62.5%, SVAMP 85.7% (Table 4); Zero-Shot-CoT shown above.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using diverse reasoning paths induced by multiple synonymous phrasings plus self-consistency voting yields sizable gains over single-path zero-shot CoT: roughly +17.5% absolute improvement on GSM8K and +14% on SVAMP (relative to the reported baselines). Fed-SP-SC outperforms the Fed-DP-CoT approach by about ~5% in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Ablation shows accuracy increases as number of sampled reasoning paths grows up to ~5, but adding more synonymous/rephrased questions beyond that can reduce accuracy (noise/semantic drift from poor paraphrases). Quality of paraphrases matters (GPT-3.5 paraphrases gave larger gains than GPT-3 paraphrases).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4739.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4739.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fed-DP-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Federated Different-Parameters Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that retrieves previously answered synonymous questions with different parameters, concatenates those question+answer pairs (as pseudo-labelled CoTs) with a disclaimer, and prepends them to the new query to improve final LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI text-davinci-002 (GPT-3) and text-davinci-003 (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same OpenAI decoders as above; used via API. The method reuses pseudo-label answers (from Fed-SP-SC) as Chain-of-Thought examples in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Federated DP Chain-of-Thought (Fed-DP-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Construct a single composite CoT prompt by concatenating retrieved DP question-answer pairs (pseudo-labels) as worked examples, add a short disclaimer ('The examples given above may contain errors, please think more carefully.'), and append the user's query; the LLM then produces a single reasoning trace/answer. This is a similar / exemplar-based CoT rather than ensembling diverse outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and SVAMP (arithmetic / grade-school math benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above: multi-step arithmetic reasoning datasets used to measure improvements from CoT-style prompting using pseudo-labelled DP examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 4: Compared to Zero-Shot-CoT, Fed-DP-CoT (GPT-3 Gen.) achieved GSM8K 59.2% (vs Zero-Shot-CoT 48.3% in that table) and SVAMP 82.4% (vs Zero-Shot-CoT 76.5%); Fed-DP-CoT (GPT-3.5 Gen.) achieved GSM8K 62.5% and SVAMP 85.7%. Reported improvements over Zero-Shot-CoT of roughly 6-14% depending on model/dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Fed-SP-SC (GPT-3 Gen.) GSM8K 62.7% / SVAMP 86.3%; Fed-SP-SC (GPT-3.5 Gen.) GSM8K 70.6% / SVAMP 91.1% (Fed-SP-SC outperforms Fed-DP-CoT by ~5% in reported experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using previously generated (pseudo-labeled) CoTs as prompt exemplars improves performance over single-shot Zero-Shot-CoT by ~10-15% in some settings; adding a disclaimer that the examples may contain errors yields a modest extra improvement (~2%). However, Fed-DP-CoT is consistently reported as somewhat weaker than the Fed-SP-SC self-consistency ensembled approach.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Because DP examples have different parameters, their pseudo-labels can be incorrect; incorrect CoTs can harm performance if unchecked. The paper reports that adding many (noisy) examples harms accuracy and that a disclaimer mitigates but does not eliminate errors. Fed-DP-CoT performed worse than Fed-SP-SC in all reported comparisons (~5% lower).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4739.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4739.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-Shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Chain-of-Thought (baseline 'Let's think step by step')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline prompting technique (from prior work) that elicits a single chain-of-thought by appending a short trigger phrase such as 'Let's think step by step' to the question to induce multi-step reasoning in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI text-davinci-002 / text-davinci-003 (GPT-3 / GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as the baseline prompt style for the experiments; underlying LLMs are the same OpenAI decoders used in the other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-Shot-CoT (single-path CoT prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>A single prompting strategy that requests the model to produce a step-by-step reasoning trace by adding a fixed instruction (e.g., 'Let's think step by step') to each question; produces one primary reasoning path (no ensemble/diverse sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and SVAMP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same arithmetic reasoning datasets used as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baselines vary across tables: Table 3 reports Zero-Shot-CoT GSM8K 52.5% and SVAMP 77.2%; Table 4 reports Zero-Shot-CoT GSM8K 48.3% and SVAMP 76.5% (differences reflect setup/model selection described in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Fed-SP-SC and Fed-DP-CoT improvements reported above (Fed-SP-SC yields largest gains).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A single-path CoT baseline is significantly improved by aggregating diverse reasoning paths (Fed-SP-SC) or by augmenting prompts with federated CoT examples (Fed-DP-CoT). Self-consistency over diverse paraphrases gives stronger gains than a single CoT exemplar approach in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The baseline single-path CoT is outperformed by both federated approaches in these experiments; the paper also notes variability in baseline numbers depending on model choice and prompt generation procedure.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4739",
    "paper_id": "paper-258352268",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "Fed-SP-SC",
            "name_full": "Federated Synonymous-Questions with Self-Consistency",
            "brief_description": "A method introduced in this paper that federates multiple rephrasings (synonymous questions with identical parameters) and applies self-consistency/majority voting over the LLM answers to produce a single consistent answer for the federated question set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI text-davinci-002 (GPT-3, ~175B) and text-davinci-003 (GPT-3.5)",
            "model_description": "OpenAI GPT-family generative models used via API: text-davinci-002 (GPT-3 class, ~175B parameters) and text-davinci-003 (GPT-3.5 variant / improved decoder). The paper uses these via zero-shot prompting (including 'Let's think step by step').",
            "reasoning_method_name": "Self-Consistency on Federated Synonymous Questions (Fed-SP-SC)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate multiple synonymous rephrasings of the same problem, obtain an answer for each (using CoT/zero-shot CoT prompting), then aggregate the multiple reasoning paths/answers via majority voting (self-consistency) to select the most-voted final answer. Diversity arises from distinct phrasings and sampled reasoning paths.",
            "task_name": "GSM8K and SVAMP (arithmetic / grade-school math reasoning benchmarks)",
            "task_description": "GSM8K: grade-school math word problems requiring multi-step arithmetic reasoning; SVAMP: simple arithmetic word problems with textual variations.",
            "performance": "GSM8K: Zero-Shot-CoT baseline 52.5% -&gt; Fed-SP-SC (GPT-3 Gen.) 62.7%, Fed-SP-SC (GPT-3.5 Gen.) 70.6%. SVAMP: Zero-Shot-CoT baseline 77.2% -&gt; Fed-SP-SC (GPT-3 Gen.) 86.3%, Fed-SP-SC (GPT-3.5 Gen.) 91.1%. (Table 3)",
            "comparison_with_other_method": true,
            "performance_other_method": "Fed-DP-CoT (GPT-3 Gen.) GSM8K 59.2%, SVAMP 82.4%; Fed-DP-CoT (GPT-3.5 Gen.) GSM8K 62.5%, SVAMP 85.7% (Table 4); Zero-Shot-CoT shown above.",
            "key_findings": "Using diverse reasoning paths induced by multiple synonymous phrasings plus self-consistency voting yields sizable gains over single-path zero-shot CoT: roughly +17.5% absolute improvement on GSM8K and +14% on SVAMP (relative to the reported baselines). Fed-SP-SC outperforms the Fed-DP-CoT approach by about ~5% in reported experiments.",
            "counter_examples_or_negative_results": "Ablation shows accuracy increases as number of sampled reasoning paths grows up to ~5, but adding more synonymous/rephrased questions beyond that can reduce accuracy (noise/semantic drift from poor paraphrases). Quality of paraphrases matters (GPT-3.5 paraphrases gave larger gains than GPT-3 paraphrases).",
            "uuid": "e4739.0"
        },
        {
            "name_short": "Fed-DP-CoT",
            "name_full": "Federated Different-Parameters Chain-of-Thought",
            "brief_description": "A method introduced in this paper that retrieves previously answered synonymous questions with different parameters, concatenates those question+answer pairs (as pseudo-labelled CoTs) with a disclaimer, and prepends them to the new query to improve final LLM reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI text-davinci-002 (GPT-3) and text-davinci-003 (GPT-3.5)",
            "model_description": "Same OpenAI decoders as above; used via API. The method reuses pseudo-label answers (from Fed-SP-SC) as Chain-of-Thought examples in the prompt.",
            "reasoning_method_name": "Federated DP Chain-of-Thought (Fed-DP-CoT)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Construct a single composite CoT prompt by concatenating retrieved DP question-answer pairs (pseudo-labels) as worked examples, add a short disclaimer ('The examples given above may contain errors, please think more carefully.'), and append the user's query; the LLM then produces a single reasoning trace/answer. This is a similar / exemplar-based CoT rather than ensembling diverse outputs.",
            "task_name": "GSM8K and SVAMP (arithmetic / grade-school math benchmarks)",
            "task_description": "Same as above: multi-step arithmetic reasoning datasets used to measure improvements from CoT-style prompting using pseudo-labelled DP examples.",
            "performance": "Table 4: Compared to Zero-Shot-CoT, Fed-DP-CoT (GPT-3 Gen.) achieved GSM8K 59.2% (vs Zero-Shot-CoT 48.3% in that table) and SVAMP 82.4% (vs Zero-Shot-CoT 76.5%); Fed-DP-CoT (GPT-3.5 Gen.) achieved GSM8K 62.5% and SVAMP 85.7%. Reported improvements over Zero-Shot-CoT of roughly 6-14% depending on model/dataset.",
            "comparison_with_other_method": true,
            "performance_other_method": "Fed-SP-SC (GPT-3 Gen.) GSM8K 62.7% / SVAMP 86.3%; Fed-SP-SC (GPT-3.5 Gen.) GSM8K 70.6% / SVAMP 91.1% (Fed-SP-SC outperforms Fed-DP-CoT by ~5% in reported experiments).",
            "key_findings": "Using previously generated (pseudo-labeled) CoTs as prompt exemplars improves performance over single-shot Zero-Shot-CoT by ~10-15% in some settings; adding a disclaimer that the examples may contain errors yields a modest extra improvement (~2%). However, Fed-DP-CoT is consistently reported as somewhat weaker than the Fed-SP-SC self-consistency ensembled approach.",
            "counter_examples_or_negative_results": "Because DP examples have different parameters, their pseudo-labels can be incorrect; incorrect CoTs can harm performance if unchecked. The paper reports that adding many (noisy) examples harms accuracy and that a disclaimer mitigates but does not eliminate errors. Fed-DP-CoT performed worse than Fed-SP-SC in all reported comparisons (~5% lower).",
            "uuid": "e4739.1"
        },
        {
            "name_short": "Zero-Shot-CoT",
            "name_full": "Zero-Shot Chain-of-Thought (baseline 'Let's think step by step')",
            "brief_description": "A baseline prompting technique (from prior work) that elicits a single chain-of-thought by appending a short trigger phrase such as 'Let's think step by step' to the question to induce multi-step reasoning in LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI text-davinci-002 / text-davinci-003 (GPT-3 / GPT-3.5)",
            "model_description": "Used as the baseline prompt style for the experiments; underlying LLMs are the same OpenAI decoders used in the other methods.",
            "reasoning_method_name": "Zero-Shot-CoT (single-path CoT prompting)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "A single prompting strategy that requests the model to produce a step-by-step reasoning trace by adding a fixed instruction (e.g., 'Let's think step by step') to each question; produces one primary reasoning path (no ensemble/diverse sampling).",
            "task_name": "GSM8K and SVAMP",
            "task_description": "Same arithmetic reasoning datasets used as baselines.",
            "performance": "Reported baselines vary across tables: Table 3 reports Zero-Shot-CoT GSM8K 52.5% and SVAMP 77.2%; Table 4 reports Zero-Shot-CoT GSM8K 48.3% and SVAMP 76.5% (differences reflect setup/model selection described in the paper).",
            "comparison_with_other_method": true,
            "performance_other_method": "Fed-SP-SC and Fed-DP-CoT improvements reported above (Fed-SP-SC yields largest gains).",
            "key_findings": "A single-path CoT baseline is significantly improved by aggregating diverse reasoning paths (Fed-SP-SC) or by augmenting prompts with federated CoT examples (Fed-DP-CoT). Self-consistency over diverse paraphrases gives stronger gains than a single CoT exemplar approach in this study.",
            "counter_examples_or_negative_results": "The baseline single-path CoT is outperformed by both federated approaches in these experiments; the paper also notes variability in baseline numbers depending on model choice and prompt generation procedure.",
            "uuid": "e4739.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        }
    ],
    "cost": 0.011637999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering</p>
<p>Xiangyang Liu 
South China Normal University
GuangdongChina</p>
<p>Tianqi Pang 
South China Normal University
GuangdongChina</p>
<p>Chenyou Fan fanchenyou@scnu.edu.cn 
South China Normal University
GuangdongChina</p>
<p>Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering
Synonymous Question-answering · Federated Learning · Large Language Model · Prompt Learning · Chain-of-Thought
We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that our proposed methods can significantly enhance question accuracy by fully exploring the synonymous nature of the questions and the consistency of the answers.</p>
<p>Introduction</p>
<p>Recently, Large Language Models (LLMs) such as PaLM [2] and GPT family [1,13] have revolutionized the methodology of tackling natural language processing (NLP) tasks such as sentiment analysis [20], question answering [24], text summarization [23], and reasoning on arithmetic and common sense questions [25].</p>
<p>Large Language Models (LLMs) are highly over-parameterized with millions or billions of parameters, e.g., the GPT-3 model has about 175 Billion parameters. Due to this redundancy in model design, LLMs can represent language in a highly flexible and expressive manner by capturing the complex and structured patterns in human languages. In addition, LLMs can generate remarkably natural dialogues and accurate answers with contextual understanding, sometimes even surpassing human experts in certain tasks. For example, in arithmetic reasoning, GPT-4 achieved an accuracy rate of 92% on the GSM8K dataset [12]; in common sense reasoning, KEAR achieved an accuracy rate of 89.4% on the CSQA dataset [22]. Q1:Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? Q2:The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Q1:Roger initially had 5 tennis balls.he bought 2 more cans of tennis balls. Each can contains 3 tennis balls. What is the total number of tennis balls he has now? Q2:The cafeteria had 23 apples, but they used 20 of them for lunch and bought 6 more. How many apples do they have now? Q1:If the cafeteria used 20 out of its 23 apples for lunch and then purchased 6 more, how many apples does it have in total? Q2:After purchasing 2 cans of tennis balls, each containing 3 tennis balls, how many tennis balls does Roger have in total if he originally had 5?  Fig. 1: A general overview of our approach to dealing with federated synonymous question-answering. Our approach is categorized into two user scenarios: synonymous questions that share the same parameters, and those that have different parameters. When the parameters are the same, we utilize self-consistency to select the most commonly voted answer as the consistent response. However, for cases where the parameters are different, we amalgamate each question's consistent answer to create a Chain-of-Thought, which makes it easier for the LLM to respond to new queries.</p>
<p>Synonymous question</p>
<p>We consider a practical user scenario in which a large number of users can access a cloud-deployed LLM for solving personal tasks from all places over the world. For example, more and more primary school students and their parents rely on the capability of LLMs for solving mathematical problems. The users often access a LLM and ask common realistic questions. For example, primary school children might ask "Chickens and rabbits are in the same cage, a total of 35 heads, 94 feet, how many chickens and rabbits are there?", while computer science students often ask "How to write a QuickSort in Python?".</p>
<p>Due to the complexity in task understanding and reasoning, the LLMs often return the wrong answers even given seemingly simple questions. For example, on the GSM8K dataset, the fine-tuned GPT-3 (175B) with verifier only achieves an accuracy of 55.0%. Meanwhile, the PaLM-540B (Few-shot-CoT) only achieves an accuracy of 58.1%. [8] How to improve the question answering accuracy has become a serious challenge which decides whether LLMs can be accepted as a robust and reliable part in realistic applications.</p>
<p>One commonsense is that can we crowd-source many questions and aggregate those questions to better understand some common questions. A common question might be asked frequently as its variants in the concrete parameters or rephrased formulations. For example, the Chickens-and-rabbits questions can be asked with different number of heads and feet. Now we want to ask Can we fully utilize those similar questions to improve the question answering of the LLMs without tuning the model parameters or infringing user privacy?</p>
<p>Recent progressives in federated learning (FL) [10] have proved that utilizing distributed data sources can both preserve data privacy and enhance model training. In the FL paradigm, each client trains a local learning model with own data, while a central server regularly communicates with all agents to generate a better global model through the aggregation of the local models.</p>
<p>In this study, we consider improving the reasoning capacity of LLMs by better understanding crowd-sourced similar questions, from which we can explore the contextual information and improve the LLM answers substantially. Inspired by FL, we propose two typical scenarios when a user sends a QA request to the LLM and the LLM tries to answer with a collected question database.</p>
<p>-Synonymous Questions with Same Parameters (SP-questions). The cloud-deployed system retrieves from the database and finds several synonymous but rephrased questions with exactly the same parameters. For example, Q1:"If a farmer has a certain number of chickens and rabbits in a barn, and there are a total of 32 heads and 100 feet, how many chickens and how many rabbits does the farmer have?" Q2:"In a barn, there are a certain number of chickens and rabbits that have a total of 32 heads and 100 feet. how many of each animal are in the barn?" -Synonymous Questions with Different Parameters (DP-questions).</p>
<p>This situation is harder as the question parameters mined in the database are different from each other. For example, Q1:"If a farmer has a certain number of chickens and rabbits in a barn, and there are a total of 32 heads and 100 feet, how many chickens and how many rabbits does the farmer have?" Q2:"A farmer has a total of 20 chickens and rabbits in his barn. If the total number of legs in the barn is 56, how many chickens and how many rabbits are in the barn?"</p>
<p>For SP-questions, we propose to leverage LLMs to directly generate answers first. Then we federate the answers and apply the self-consistency [19] technique to obtain the most voted answer for all synonymous questions in the federation. We call this method Fed-SP-SC (Fed-SP with Self-Consistency).</p>
<p>For DP-questions, we propose to leverage LLMs to generate consistent answers for each DP-questions first. Different from procedures of dealing SPquestions, we cannot directly agglomerate the answers since they are for different parameters. Instead, we federate them by forming the Chain-of-Thought (CoT) to provide hints to the LLMs. We append the original query to the CoT as the full prompt to obtain improved final answer. We call this technique Fed-DP-CoT.</p>
<p>Once the LLM has finished generating the answer using either Fed-SP-SC or Fed-DP-CoT, the system will store both the questions and answers into the database. This enables the system to collect all records and leverage past records to produce re-fined answers to new queries. For questions that have been asked before with wrong answers, the system can evolve itself by correcting the answers with self-consistency mechanism or with more comprehensive CoT prompts.</p>
<p>We extensively evaluate our methods on the GSM8K and SVAMP datasets and demonstrate that the Fed-SP-SC method achieves a notable improvement in accuracy of 14-18% over the standalone LLMs with Zero-Shot-CoT ("Let's think step by step"). Additionally, our Fed-DP-CoT method delivers an impressive increase of 10-15% over the standalone LLMs with Zero-Shot-CoT.</p>
<p>We summarize our contributions in this study as follows.</p>
<ol>
<li>We consider a practical but under-studied scenario, which is the cloud-based LLMs are frequently asked similar and even synonymous common questions from large number distributed users. 2. We abstract two main user scenarios: distributed users are querying synonymous questions that share the same parameters (SP-questions), and those that have different parameters (DP-questions). 3. We design the system to firstly federate those SP-and DP-questions first by retrieving the database. Then we propose to utilize self-consistency methodology to select the most commonly voted answer to improve SP-question answering. All consistent answers and CoTs will be stored back into database for further reuse. 4. We also amalgamate consistent answers to create a chain-of-thought prompt that significantly improves DP-questions answering quality. We also design a simple disclaimer to handle noisy CoT generated from LLM answers better. 5. Inherited from Federated Learning, our Fed-SP-SC and Fed-DP-COT methods can collaboratively enhance the question-answering process of the LLM while preserving their anonymity. There would be no data exchange or leakage among distributed users.</li>
</ol>
<p>Related Work</p>
<p>Pre-trained Language Models (PLMs). Recent studies in Transformerbased language models such as ELMo [15] and BERT [4] have shown their capabilities in scaling up model sizes with pre-training methodology such as Masked Language Modeling [4]. Shortly after, several Large Language Models (LLMs), e.g., the GPT family [1,13], PaLM [2], Jurassic-X [9], Megatron-Turing [16] , LaMDA [17],LLaMA [18], have been emerging with huge amount of parameters of up to 100B-5000B parameters. They have shown great advantages in language modeling tasks, such as arithmetic reasoning, commonsense reasoning, symbolic reasoning and natural language inference. However, PLMs are still like black boxes which lack of explanation. Some recent studies made efforts towards unveiling the power of those LLMs. The proposal of the concept of the Chain-of-Thought (CoT) [21] indicates that incorporating intermediate reasoning steps can lead to a significant improvement in the performance of large language models on reasoning tasks. The proposal of the Self-consistency [19] suggest that aggregating multiple reasoning paths, rather than relying on greedy decoding, can lead to further improvements in the accuracy of models on reasoning tasks. LMSI [7] provides a demonstration of how large language models can achieve self-improvement by utilizing only unlabelled datasets.</p>
<p>However, it is unknown how to apply proper pre-training to distributed learning scenarios, due to substantial differences between centralized large model deployment and distributed query demands. In this study, we adopt the recent popular distributed machine learning methodology called Federated Learning [5,6,10,26] (FL) to fully explore the potentiality of Large Language Models to tackle frequently asked questions while preserving data privacy for the users. The FL provides a way of learning models over a collection of distributed devices while keeping data locality. However, classical FL studies assumed the agents in FL can own copies of local models while receiving updates from centralized model. In contrast, we focus on a practical scenario that the clients can only query answers from centralized Large Language Models without owning any local model, due to the practical situations that Large Language Models are simply too large and computational extensive to be deployed locally.</p>
<p>Scenarios and Approaches</p>
<p>In this section, we describe the federated scenarios that distributed users query the LLMs with similar (but not exact the same) questions. We identify two types of questions and discuss them in details.</p>
<p>Basic Concepts</p>
<p>Chain-of-Thought (CoT) [21] is a series of generated intermediate reasoning texts that can be added to the original prompts. CoT is proposed for enhancing the capability of language models to perform various reasoning tasks by allowing LLMs to decompose complex problems into intermediate steps that could be solved well step-by-step. Chain-of-thought prompting, i.e. prompting LLMs with CoT, is a simple and practical method for improving the reasoning tasks readily with no additional efforts of tuning the original LLMs. CoT prompting has shown improved reasoning results on arithmetic, commonsense, and symbolic reasoning tasks.</p>
<p>Self-Consistency (SC) [19] is a decoding strategy that enhances language model reasoning with voting ensemble. SC first samples a diverse set of answers as reasoning paths of a question, rather than only the greedy path. By exploring multiple paths, SC is capable of identifying the most consist answer as the final answer by majority voting, i.e., the most voted answer of the LLM is taken as the final answer. Compared with a single-path reasoning, SC ensembles answers to improve accuracy and filters out noises or outliers. SC has also been widely explored in reasoning and QA tasks [19].</p>
<p>Majority voting(MV) [11] is a commonly used method in statistical decision theory that involves aggregating the opinions or decisions of multiple individuals or models, typically by selecting the option with the highest frequency of agreement among the voters.</p>
<p>Synonymous Questions with Same Parameters (SP-questions)</p>
<p>We consider a cloud-based LLM system which accepts queries from distributed users. The first practical user scenario that we consider is as follows. Given a user query, we can retrieve from the cloud database several synonymous questions with same parameters (SP-questions). Table 1: Examples of synonymous SP-questions and answers.</p>
<p>Example1:</p>
<p>Q1: "If a farmer has a certain number of chickens and rabbits in a barn, and there are a total of 32 heads and 100 feet, how many chickens and how many rabbits does the farmer have?" A1: "The farmer has 24 chickens and 8 rabbits." (wrong)</p>
<p>Q2: "In a barn, there are a certain number of chickens and rabbits that have a total of 32 heads and 100 feet. how many of each animal are in the barn?" A2: "Let x = the number of chickens and y = the number of rabbits. We can set up the following system of equations:x + y = 32(heads), 2x + 4y = 100 (feet), Solving this system of equations, we get x = 20 and y = 12.Therefore, there are 20 chickens and 12 rabbits in the barn." (wrong) Example2:</p>
<p>Q1: "James writes a 3-page letter to 2 different friends twice a week.How many pages does he write a year?" A1: "James writes 3 pages to 2 different friends twice a week, which is 24 pages a month and 288 pages a year." (wrong) Q2: "If James writes a 3-page letter to two different friends twice per week,what is the total number of pages he produces every year?" A2: "James writes two 3-page letters twice per week. There are 52 weeks in a year. Therefore, James produces a total of 312 pages every year(2 * 3 * 52 = 312)." (wrong)</p>
<p>For complex reasoning tasks, the LLMs may provide unreliable answers to the questions. We provide two failure cases in Table. 1. We found that in both examples, Q1 and Q2 are synonymous while each of them gets a wrong answer from LLM. We summarize the difficulties of tackling the SP-questions as follows:</p>
<ol>
<li>Most LLMs have unsatisfying accuracy in solving reasoning problems in zeroshot way, i.e., prompting the LLMs with questions directly without giving other information. 2. LLMs tend to under-perform when understanding complex problems involving multiple reasoning steps, such as the arithmetic problems given above.</li>
</ol>
<p>Thus, our task is to fully explore the SP-questions as a federation which can enhance the answer quality together, instead of dealing them separately. To this end, we propose a technique named Fed-SP-SC (Federated SP-questions with Self-Consistency) for answering the questions with the self-consistency technique mentioned above. Fed-SP-SC can improve the zero-shot accuracy of the answers by eliciting answers from multiple synonymous questions and make a majority vote to disclose the most likely answer.</p>
<p>Concretely, we query the database using the user's prompt to match SPquestions in the database. Note that here we assume we can retrieve the SPquestions which are just rephrased with synonymous and same parameters.</p>
<p>Next, we generate the answers with LLMs by zero-shot prompting. For SPquestions, these answers are presumably same. Assuming that we have generated a total of n answers of synonymous questions during the Fed-SP-SC process, we can ensure the consistency with SC procedure, i.e., we make a majority vote and select the most voted answer A SC as the final answer of all SP-questions, as below.</p>
<p>A SC ← arg max
A∈A 1[A == A i ], ∀i = 1, ..., n .(1)
Intuitively, the majority voting filters out outliers and noisy rephrased questions. In addition, the most voted answer is the agreement of multiple reasoning paths from multiple rephrased SP-questions, thus is more likely to be better than a single prompted answer decoded from a single reasoning path.</p>
<p>In our experiments, we demonstrate that Fed-SP-SC achieves a 17.5% improvement in accuracy on the GSM8K dataset and a 14% improvement on the SVAMP dataset in Table 3. In a practical system, we can further store these user prompts and the SC-selected answer back into the database.</p>
<p>Synonymous questions with Different Parameters (DP-questions)</p>
<p>We now describe the second scenario which is named synonymous questions with different parameters (DP-questions), which is broader and more practical. Based on the user query question, the cloud-deployed system searches and retrieves from the database for questions with same meanings but may have different parameters. DP-questions are more practical yet harder than SP-questions as the question parameters retrieved from the database are different. We show two exemplary questions Q1 and Q2 below which have the same meaning yet with different parameters heads and feet in Table 2.</p>
<p>Note that tackling DP-questions would face all the difficulties of SP-questions, and would have additional obstacles as summarized below:</p>
<ol>
<li>There is no uniform ground-truth for DP-questions in the database, since each one has different parameters. 2. Similarly, we cannot apply self-consistency (SC) directly to improve accuracy due to different parameters. 3. If we apply Chain-of-Thought (CoT) together to the original questions as enhanced prompts, we cannot guarantee the correctness of the CoT. Incorrect CoT may even harm the answering accuracy.  The most voted answer is returned to the user as the best answer. The database could store the query and answer pair back to the database, caching for later retrieval. This procedure can grow the database quickly by gathering distributed user queries. The specific description of Q1 and Q2:</li>
</ol>
<p>Q1: "If a farmer has a certain number of chickens and rabbits in a barn and, there are a total of 32 heads and 100 feet, how many chickens and how many rabbits does the farmer have?" Q2: "A farmer has a total of 20 chickens and rabbits in his barn. If the total number of legs in the barn is 56, how many chickens and how many rabbits are in the barn?"</p>
<p>To tackle the above challenges, we propose the Federated questions of Different Parameters with Chain-of-Thought (Fed-DP-CoT) technique to leverage existing answers of DP-questions in CoT forms to improve new query answering. LLM Q:Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet? A:Betty has 50% of the cost, which means she has $50. Then, her parents gave her $15, so she now has $65. Her grandparents gave her twice as much as her parents, so they gave her $30. Therefore, Betty has $95. The answer is $5.</p>
<p>Q:Betty wants to buy a new wallet that costs $100. She has 50% of the total cost, and her parents gave her $15. Her grandparents gave her twice as much as her parents. How much more does she need to save?</p>
<p>A:Betty has $50 for the wallet (50% of $100). Her parents gave her $15, so her grandparents gave her $30 ($15 * 2).Altogether, Betty has $95 ($50 + $15 + $30).To reach the cost of the wallet ($100), she still needs to save $5.</p>
<p>Q:If Betty has half of the money needed to buy a $100 wallet, and her parents gave her $15, while her grandparents gave her twice as much as her parents, how much more does she need to buy the wallet? A:Betty has half of the money needed to buy a $100 wallet, so she has $50. Her parents gave her $15, so she now has $65. Her grandparents gave her twice as much as her parents, so they gave her $30. She now has $95. So she needs $5 more to buy the wallet.</p>
<p>Query</p>
<p>Retrieved DP-question</p>
<p>Fed-DP-CoT prompt When a user starts querying the cloud-based LLM service, the cloud system performs query-question retrieval first. The system matches several questions with the highest similarity in the database. Generally, these the retrieved questions are of different parameters (DP-questions). We design the system to perform Fed-DP-CoT for understanding DP-questions.</p>
<p>We consider a practical case that these DP-questions have pseudo-labels generated by self-consistency majority voting in the Fed-SP-SC processes. We call these labels "pseudo-labels" as they are not actual ground-truth labels.</p>
<p>Then we utilize these DP-questions with pseudo-labels together as CoT for the original query-question. To be specific, we concatenate DP-questions with their answers as a single prompt, followed by the error disclaimer "The examples given above may contain errors , please think more carefully. " at the end of this prompt as the complete prompt. The error disclaimer reminds the LLMs that the answers in CoT are pseudo-labels and could be incorrect. We found this simple practice can boost performance by approximately 2%. Finally, we use the entire disclaimed CoT as a prefix to the user's query prompt for the LLMs to provide the final answer.</p>
<p>Experiment</p>
<p>We evaluate our proposed Fed-SP-SC and Fed-DP-CoT methods on benchmark datasets with simulated user scenarios such that SP-and DP-questions are retrieved to improve over standalone question answering.</p>
<p>We compare our methods with Zero-Shot-CoT [1], which refers to adding "Let's think step by step." to prompt as a composite prompt, such as "[Question] A:Let's think step by step."</p>
<p>Datasets</p>
<p>Grade School Math (GSM8K) is a math dataset with 7,473 training and 1,319 testing examples of grade-school-level word problems [3]. These math problems typically require two to eight calculation steps to arrive at a final answer, as shown in Fig.2. GSM8K is widely used as a benchmark dataset for testing the arithmetic and commonsense reasoning capacities of LLMs [7,8].</p>
<p>Simple Variations on Arithmetic Math word Problems (SVAMP) is a dataset of simple arithmetic math word problems [14] with around 6,000 samples. Each data instance has a short story and a question about unknown quantities. SVAMP provides a benchmark test set for comparing the textual understanding and reasoning abilities of LLMs, which is widely compared in recent studies [14,19,21].</p>
<p>In practice, we utilized the OpenAI's API 1 text-davinci-002 and text-davinci-003. We selected text-davinci-003 for the GSM8K dataset as text-davinci-002 performed very poorly. Similarly, we used text-davinci-002 for the SVAMP dataset as text-davinci-003 had an overly high accuracy rate on this dataset.</p>
<p>Results of Fed-SP-SC</p>
<p>As a kind reminder, in the following discussions, SP-questions stand for a set of rephrased synonymous questions with same parameters. Differently, DPquestions stand for a set of rephrased synonymous questions with different parameters. We now describe the experiment procedures shown in Fig. 4.</p>
<p>[SP-questions generation]. We first generate four SP-questions for each of the original question with both OpenAI GPT-3 [1] and GPT-3.5 [13], respectively. Concretely, we add each original question a same prompt prefix "Rephrase in 4 ways: [ORIGINAL QUESTION]", then we collect the generated answers.</p>
<p>[SP-questions answering]. We query the cloud-deployed LLM for answering rephrased questions generated as above. Specifically, we obtain the improved Zero-Shot-CoT answering with the magic sentence "Let's think step by step" as the prompt.</p>
<p>We first examine the performance of our proposed Fed-SP-SC which deals with SP-questions with the Self-consistency technique. We conducted experiments on GSM8K and SVAMP and report the results below.  We show accuracy of self-consistency after obtaining results from different phrasings of the synonymous question on GSM8K and SVAMP in Table 3. We have the following observations.</p>
<p>-Fed-SP-SC can improve answering accuracy of LLMs by federating multiple SP-questions through self-consistency. -Fed-SP-SC(GPT-3.5 Gen.) performs best on the GSM8K and SVAMP datasets, improved the performance by 17.5% and 14% on the GSM8K and SVAMP datasets, respectively. -The quality of the synonymous questions can affect the accuracy significantly, as seen in the larger improvement from the synonymous questions generated by GPT-3.5 compared to GPT-3.</p>
<p>Results of Fed-DP-CoT</p>
<p>Load benchmark</p>
<p>Add disclaimer Chat with LLMs using Fed-CoT prmopt "The example given above may contain errors , please think more carefully."</p>
<p>Extract Self-consistency answers in Fed-SC experiment We report results of Fed-DP-CoT on GSM8K and SVAMP in Table 4, and compare with the baseline Zero-Shot-CoT.</p>
<p>-Fed-DP-CoT can improve the performance. Compared to Zero-Shot-CoT, CoT Prompt(GPT-3 Gen.) and CoT Prompt(GPT-3.5 Gen.) improve by approximately 10.9%-14.2% and 6.6%-10% respectively on the datasets GSM8K and SVAMP. -Fed-SP-SC performs better than Fed-DP-CoT. The results of Fed-SP-SC (GPT-3 Gen.) and Fed-SP-SC (GPT-3.5 Gen.) on the GSM8K and SVAMP datasets are both higher than Fed-DP-CoT (GPT-3 Gen.) and CoT Prompt (GPT-3.5 Gen.), with an approximate improvement of 5%. -Less significant performance difference between GPT-3 Gen. and GPT-3.5</p>
<p>Gen. compared to Fed-SP-SC experiment. The reason for this is the disparity in parameters employed, coupled with the lack of emphasis on synonym usage in the CoT prompt. The number of reasoning paths for self-consistency. We study the effect of using different number of sampled reasoning paths for Fed-SP-SC (Sec. 4.2) to apply self-consistency. We conduct hyper-parameter search with a subset of the data for this ablation study due to the limits of accesses of the OpenAI API.</p>
<p>Ablation studies</p>
<p>We vary the number of sampled reasoning paths of synonymous questions from one to nine. Figure 6 shows that increasing the number of sampled reasoning paths of the synonymous questions does not always improve the accuracy of the model.</p>
<p>In the line chart, as the number of sampled reasoning paths increases from one to five, the accuracy rate gradually increases. However, when the number of synonymous questions exceeds five, the accuracy of the model starts to decrease.</p>
<p>We speculate that this is because introducing synonymous questions also introduces noisy phrases, causing a deviation in the semantic meaning of the original questions. This deviation is particularly evident in synonymous questions generated by GPT-3 (blue lines), while the generation results of GPT-3.5 (orange lines) exhibit stronger robustness. Disclaimer We investigate whether the disclaimer is effective of correcting noisy CoTs in this ablation experiment. As Zero-Shot-CoT does not employ pseudo-labels, we do not conduct disclaimer ablation on it. Table 5 compares the DP-questions answering accuracy with disclaimer or without disclaimer. We observe that the addition of a disclaimer in the questions and answers generated by GPT-3 resulted in an increase in accuracy from 57.7% to 59.2% for the Fed-DP-CoT task. Similarly, in the case of questions and answers generated by GPT-3.5, the accuracy increase from 60% to 62.5%. These results indicate that the use of a simple disclaimer can potentially improve the accuracy of LLMs by approximately 2% for the Fed-DP-CoT task. We postulate that the improvement in accuracy may be attributed to the fact that the disclaimer prompts LLMs to be careful of the pseudo-labels and self-examine the reasoning steps.</p>
<p>Conclusion</p>
<p>We investigate the potential benefits of employing synonymous queries from distributed users to enhance question-answering beyond what is achievable by a single user. Specifically, we explore the use of such queries in a federated manner by extracting two common user scenarios whereby the cloud database retrieves either SP-or DP-questions. To address these scenarios, we propose the application of self-consistency to identify the most consistent answers for SP-questions and utilize them as CoT to improve the answers provided for DP-questions. Our experimental results demonstrate that this approach yields a significant boost in performance compared to standalone zero-shot QA.</p>
<p>Moving forward, future research may investigate the implementation of more realistic systems that can efficiently retrieve federated questions while also improving CoT correctness to further advance reasoning capabilities. In this study, we assumed the DP-questions have already been stored with their answers generated by LLMs, and the consistent answers have been generated. Future work can further extend to scenarios that part of the DP-questions have no answers or pseudo-answers.</p>
<p>Fig. 2 :
2The illustration of performing Fed-SP-SC for answering synonymous SP-questions. (A → B): When receiving the user's query, the LLM retrieves synonymous SP-questions from the centralized database. (B → C): The LLM generates the answers with zero-shot prompting for the query and combines the retrieved SP-questions' answers from the database for a majority vote to ensure self-consistency. (C → D):</p>
<p>Fig. 3 :
3The illustration of performing Fed-DP-CoT. (A → B): DP-questions are firstly retrieved from the centralized user query database. (B → C): The system selects SP-questions with consistent answers after applying Fed-SP-SC as DP-questions and retrieves consistent answers as "pseudo-labels". (C → D): The system concatenates questions and pseudo-labels, adds a pseudo-label disclaimer such as "The examples may have errors." after, and finally appends the original user query and Zero-Shot-CoT to form the complete CoT prompt.</p>
<p>Fig. 4 :
4The experiment of Fed-SP-SC contains five steps: (1) Load the GSM8K and SVAMP datasets as our benchmark and extract the questions and answers in the dataset; (2) Add each original question a same prompt prefix "Rephrase in 4 ways: [QUESTION]" to generate SP-questions; (3) Prompt both the original and rewritten questions to the LLMs to obtain their respective answers; (4) Use the majority vote for self-consistency; (5) Get the answer generated by Fed-SP-SC and compare it with the answer in the dataset to determine the accuracy rate.</p>
<p>Fig. 5 :
5The experiment of Fed-DP-CoT contains five steps:(1) Form a new test set by rephrasing the questions using different parameters in the benchmark manually and providing answers; (2) Extract consistent questions and answers in Fed-SP-SC experiment; (3) Add a disclaimer to form the CoT prompt; (4) Add the rephrased questions after the CoT prompt; (5) Prompt LLMs with entire CoT prompt and compared the answers with the rephrased answers for evaluation.</p>
<p>Fig. 6 :
6Ablation study of choice of sampled reasoning paths.</p>
<p>Table 2 :
2Two examples of DP-questions. Q1 and Q2 are synonymous but with different question parameters.</p>
<p>Q :
QBetty is saving money for a new wallet which costs $200. Betty has only half of the money ... A:Betty has 50% of the cost, which means she has $50. Then, her parents gave her $15, so ... Q:Betty wants to buy a new wallet that costs $100. She has 50% of the total cost, and her parents gave her $15... Betty has half of the money needed to buy a $100 wallet, so... The example given above may have some error. Q:Betty is saving money for a new wallet which costs $200. Betty has only half of the money she needs. Her parents decided to give her $30for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet? A:Let's think step by step. Q:Betty is saving money for a new wallet which costs $200. Betty has only half of the money she needs. Her parents decided to give her $30 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?A:Betty has $50 for the wallet (50% of 
$100). Her parents gave her $15, so 
her grandparents gave her $30 ($15 * 
2).Altogether, Betty has $95 ($50 + 
$15 + $30)... </p>
<p>Q:If Betty has half of the money 
needed to buy a $100 wallet, and her 
p a r e n t s g a v e h e r $ 1 5 , w h i l e h e r 
grandparents gave her twice as much 
as her parents, how much more does 
she need to buy the wallet? 
A:</p>
<p>Table 3 :
3Fed-SP-SC resultsData\Method Zero-Shot-CoT 
Fed-SP-SC 
(GPT-3 Gen.) </p>
<p>Fed-SP-SC 
(GPT-3.5 Gen.) </p>
<p>GSM8K 
52.5% 
62.7% 
70.6% 
SVAMP 
77.2% 
86.3% 
91.1% </p>
<p>Table 4 :
4Fed-DP-CoT results.Setting\Method Zero-Shot-CoT 
Fed-DP-CoT 
(GPT-3 Gen.) </p>
<p>Fed-DP-CoT 
(GPT-3.5 Gen.) </p>
<p>GSM8K 
48.3% 
59.2% 
62.5% 
SVAMP 
76.5% 
82.4% 
85.7% </p>
<p>Table 5 :
5GSM8K disclaimer ablation.Method\Setting 
Zero-shot 
-CoT </p>
<p>Fed-DP-CoT 
(GPT-3 Gen.) </p>
<p>Fed-DP-CoT 
(GPT-3.5 Gen.) </p>
<p>w/o disclaimer 48.3% 
57.7% 
60% 
w/ disclaimer 
NA 
59.2% 
62.5% </p>
<p>https://platform.openai.com/docs/models</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 33Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee- lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877-1901 (2020)</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. arXiv preprintChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022)</p>
<p>K Cobbe, V Kosaraju, arXiv:2110.14168Training verifiers to solve math word problems. arXiv preprintCobbe, K., Kosaraju, V., et al.: Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021)</p>
<p>J Devlin, M W Chang, K Lee, K Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi- rectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)</p>
<p>Private semi-supervised federated learning. C Fan, J Hu, J Huang, Fan, C., Hu, J., Huang, J.: Private semi-supervised federated learning. In: IJCAI. pp. 2009-2015 (2022)</p>
<p>Federated few-shot learning with adversarial learning. C Fan, J Huang, 2021 19th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt). IEEEFan, C., Huang, J.: Federated few-shot learning with adversarial learning. In: 2021 19th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt). pp. 1-8. IEEE (2021)</p>
<p>Large language models can self-improve. J Huang, S S Gu, L Hou, Y Wu, X Wang, H Yu, J Han, arXiv:2210.11610arXiv preprintHuang, J., Gu, S.S., Hou, L., Wu, Y., Wang, X., Yu, H., Han, J.: Large language models can self-improve. arXiv preprint arXiv:2210.11610 (2022)</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, arXiv:2205.11916arXiv preprintKojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (2023)</p>
<p>Y Levine, I Dalmedigos, O Ram, Y Zeldes, D Jannai, D Muhlgay, Y Osin, O Lieber, B Lenz, S Shalev-Shwartz, arXiv:2204.10019Standing on the shoulders of giant frozen language models. arXiv preprintLevine, Y., Dalmedigos, I., Ram, O., Zeldes, Y., Jannai, D., Muhlgay, D., Osin, Y., Lieber, O., Lenz, B., Shalev-Shwartz, S., et al.: Standing on the shoulders of giant frozen language models. arXiv preprint arXiv:2204.10019 (2022)</p>
<p>Communication-efficient learning of deep networks from decentralized data. H B Mcmahan, E Moore, D Ramage, S Hampson, B A Arcas, AISTATSMcMahan, H.B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.: Communication-efficient learning of deep networks from decentralized data. In: AISTATS (2017)</p>
<p>In search of an understandable consensus algorithm. D Ongaro, J Ousterhout, Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference. the 2014 USENIX Conference on USENIX Annual Technical ConferenceOngaro, D., Ousterhout, J.: In search of an understandable consensus algorithm. In: Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference. p. 305-320 (2014)</p>
<p>arXiv:2303.08774OpenAI: Gpt-4 technical report. arXiv preprintOpenAI: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)</p>
<p>L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, arXiv:2203.02155Training language models to follow instructions with human feedback. arXiv preprintOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 (2022)</p>
<p>Are NLP models really able to solve simple math word problems?. A Patel, S Bhattamishra, N Goyal, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesPatel, A., Bhattamishra, S., Goyal, N.: Are NLP models really able to solve simple math word problems? In: Proceedings of the 2021 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2080-2094 (2021)</p>
<p>M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, 10.18653/v1/N18-1202Deep contextualized word representations. In: ACL. pp. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.: Deep contextualized word representations. In: ACL. pp. 2227-2237 (2018). https://doi.org/10.18653/v1/N18-1202</p>
<p>Megatron-lm: Training multi-billion parameter language models using model parallelism. M Shoeybi, M Patwary, R Puri, P Legresley, J Casper, B Catanzaro, arXiv:1909.08053arXiv preprintShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., Catanzaro, B.: Megatron-lm: Training multi-billion parameter language models using model paral- lelism. arXiv preprint arXiv:1909.08053 (2019)</p>
<p>R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.08239Lamda: Language models for dialog applications. arXiv preprintThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.T., Jin, A., Bos, T., Baker, L., Du, Y., et al.: Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022)</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. arXiv preprintTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., Zhou, D.: Self-consistency improves chain of thought reasoning in language models (2023)</p>
<p>A survey on sentiment analysis methods, applications, and challenges. M Wankhade, A C S Rao, C Kulkarni, Artificial Intelligence Review. 557Wankhade, M., Rao, A.C.S., Kulkarni, C.: A survey on sentiment analysis methods, applications, and challenges. Artificial Intelligence Review 55(7), 5731-5780 (2022)</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. arXiv preprintWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., Zhou, D.: Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022)</p>
<p>Human parity on commonsenseQA: Augmenting self-attention with external attention. Y Xu, C Zhu, arXiv:2112.03254arXiv preprintXu, Y., Zhu, C., et al.: Human parity on commonsenseQA: Augmenting self-attention with external attention. arXiv preprint arXiv:2112.03254 (2022)</p>
<p>D Yadav, J Desai, A K Yadav, arXiv:2204.01849Automatic text summarization methods: A comprehensive review. arXiv preprintYadav, D., Desai, J., Yadav, A.K.: Automatic text summarization methods: A comprehensive review. arXiv preprint arXiv:2204.01849 (2022)</p>
<p>Conversational question answering: A survey. M Zaib, W E Zhang, Q Z Sheng, A Mahmood, Y Zhang, Knowledge and Information Systems. 6412Zaib, M., Zhang, W.E., Sheng, Q.Z., Mahmood, A., Zhang, Y.: Conversational question answering: A survey. Knowledge and Information Systems 64(12), 3151- 3195 (2022)</p>
<p>W X Zhao, K Zhou, arXiv:2303.18223A survey of large language models. arXiv preprintZhao, W.X., Zhou, K., et al.: A survey of large language models. arXiv preprint arXiv:2303.18223 (2023)</p>
<p>Y Zhao, M Li, L Lai, N Suda, D Civin, V Chandra, arXiv:1806.00582Federated learning with non-iid data. arXiv preprintZhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V.: Federated learning with non-iid data. arXiv preprint arXiv:1806.00582 (2018)</p>            </div>
        </div>

    </div>
</body>
</html>
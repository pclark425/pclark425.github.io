<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8792 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8792</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8792</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-279250211</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.07334v1.pdf" target="_blank">Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Modern large language models (LLMs) are inherently auto-regressive, requiring input to be serialized into flat sequences regardless of their structural dependencies. This serialization hinders the model's ability to leverage structural inductive biases, especially in tasks such as retrieval-augmented generation (RAG) and reasoning on data with native graph structures, where inter-segment dependencies are crucial. We introduce Graph-KV with the potential to overcome this limitation. Graph-KV leverages the KV-cache of text segments as condensed representations and governs their interaction through structural inductive biases. In this framework, 'target' segments selectively attend only to the KV-caches of their designated 'source' segments, rather than all preceding segments in a serialized sequence. This approach induces a graph-structured block mask, sparsifying attention and enabling a message-passing-like step within the LLM. Furthermore, strategically allocated positional encodings for source and target segments reduce positional bias and context window consumption. We evaluate Graph-KV across three scenarios: (1) seven RAG benchmarks spanning direct inference, multi-hop reasoning, and long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with full-text scientific papers structured as citation ego-graphs; and (3) paper topic classification within a citation network. By effectively reducing positional bias and harnessing structural inductive biases, Graph-KV substantially outperforms baselines, including standard costly sequential encoding, across various settings. Code and the Graph-KV data are publicly available.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8792.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8792.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-KV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-KV (Graph-structured KV-cache attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that injects structural inductive biases into auto-regressive LLMs by treating each text chunk's KV-cache as a condensed node representation and updating target-node KV-caches via attention only over designated source-node KV-caches, combined with shared positional-encoding ranges for sources and targets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-structured KV-cache representation (Graph-KV)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each text chunk is independently prefixed and encoded into a KV pair in the model's KV-cache. Given a directed graph G=(V,E) over chunks, Graph-KV updates each target chunk u_j by computing attention from u_j's Query vector only to the stacked Keys/Values of its source neighbors N(j) (sparsified block attention). Source chunks share a common PE range [0,L) and target chunks share another PE range [L,2L), reducing positional bias and overall positional span. The update can be iterated (the paper uses a single round).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Directed text-attributed graphs (e.g., citation ego-graphs), implicit logical/temporal dependency graphs constructed for RAG (bipartite graphs over retrieved chunks), and general node-attribute graphs where nodes are text chunks</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No linear textual serialization is required; conversion is implemented by pre-filling independent KV-caches per node and imposing a graph-structured block attention mask so that a target node's KV is generated by attending only to its source nodes' KVs. Positional encoding is allocated in shared ranges for sources and targets rather than unique absolute positions.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) benchmarks (direct inference, multi-hop reasoning, long-document understanding), ARXIV-QA (multi-hop QA over citation ego-graphs with full texts), paper topic classification (node classification on citation graphs), and synthetic scalability/stress tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported improvements across tasks: Graph-KV-Top-3 consistently outperforms Block-RAG across RAG datasets; in multi-hop/long-document reasoning Graph-KV-Top-3 outperforms sequential encoding by ~2%–10%. On paper topic classification (Table 4) using the 8B-Block backbone, Graph-KV: Cora = 71.03% accuracy, Pubmed = 84.61% accuracy. Reported specific gains versus baselines: +4.65% (bridge on HotpotQA) and +6.93% (compose on 2Wiki) over Block-RAG in selected subtasks. Memory/latency: Graph-KV encodes >3× the number of neighbors compared to sequential encoding in the synthetic star-graph memory test and achieves significantly lower Time-To-First-Token (TTFT) due to KV-prefilling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Directly compared to sequential serialization, PCW, APE, and Block-RAG baselines. Graph-KV generally outperforms parallel-encoding baselines (PCW, APE) and the Block-RAG variant on most benchmarks; Graph-KV-Top-3 often beats or matches sequential encoding and surpasses sequential in many multi-hop/long-context scenarios. Graph-KV-Full (modeling all pairwise dependencies) typically outperforms sparsified Top-m variants.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Models structural dependencies explicitly (message-passing-like step inside LLM), reduces positional bias via shared PE ranges, is long-context friendly (reduces positional-span consumption), sparsifies attention for efficiency, supports parallel prefill of KVs for low TTFT, scales to many neighbors with lower peak memory than full sequential encoding, and shows robustness to distractors/order in ARXIV-QA.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Evaluated mainly for one-hop dependency injection in this paper (multi-round/iterative updates not fully explored), requires a backbone adapted/post-trained to independent block attention for best stability (distributional shift if using vanilla pretrained LLMs), sparse variants require choosing hyperparameter m (top-m) which affects performance, Graph-KV-Full increases computation relative to sparse variants.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Sparser variants (e.g., Top-1) can underperform compared to more connected variants (Top-3 or Full) and may be insufficient when multiple sources are necessary; Graph-KV did not consistently exceed sequential encoding on simple single-chunk inference tasks where serialization artificially provides the needed context; multi-round updates provided no significant gains on the evaluated tasks; the method depends on an attention-backbone fine-tuned for block-prefilling (tuning-free variants underperform).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8792.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8792.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sequential Encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential Encoding / Serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The standard auto-regressive approach that linearizes all text chunks into a single concatenated token sequence so that causal attention attends over all preceding tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearization / serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph or multi-document inputs are converted into a single flat token sequence by concatenating text chunks (e.g., documents, references) in some order; positional embeddings and causal attention define the effective relations during decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Applied to any structured input by flattening (graphs, citation networks, sets of documents become sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Simple concatenation/linearization of nodes' texts into one ordered prompt (Prefix + Text Chunks + Question) with token-level positional encodings following the concatenated order.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Baseline for RAG tasks, ARXIV-QA, paper topic classification, and other QA/classification tasks used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Varied by backbone and dataset. Examples from Table 4: with 8B-SFT backbone, Sequential: Cora 66.66% ±0.62, Pubmed 80.64% ±0.39; with 8B-RAG backbone, Sequential: Cora 70.35% ±0.17, Pubmed 82.06% ±0.16. Sequential sometimes attains peak performance in tasks when an optimal ordering happens to encode structural dependencies in sequence, but performance is highly order-sensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Sequential encoding is compared as the canonical baseline. It can model dependencies implicitly via ordering but suffers from positional bias and quadratic attention complexity versus parallel methods; Graph-KV often outperforms sequential on multi-hop/long-context tasks and is more robust to ordering and distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Straightforward to implement, can convey chain-of-thought / ordered dependencies through serialization, sometimes attains strong performance when ordering aligns with required reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Strong positional bias (sensitive to chunk order), quadratic attention complexity across chunks (O(n^2 L^2) encoding complexity), rapid consumption of positional index space (context window limits), and poor robustness when many distractors long contexts are present.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Severe performance degradation under long contexts with distractors and when relevant documents are positioned early in the sequence; exhibits recency/primacy biases; cannot scale to many neighbor documents due to memory constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8792.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8792.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parallel Text Encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel Text Encoding (independent KV-cache per chunk, shared PEs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that independently prefill KV-caches for each text chunk in parallel (often sharing PE ranges) and then concatenate or combine those KV-caches at generation time; includes PCW, APE, Block-RAG variants referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Independent-block KV encoding / Parallel encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each text chunk is encoded independently in parallel into KV pairs; chunks commonly share positional-encoding ranges to indicate lack of explicit order; during generation tokens attend to all encoded chunks' KVs. Methods differ in how they handle distributional shift (e.g., APE rescales attention; Block-RAG uses post-training).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Applied to sets of text chunks (unordered), not explicitly modeling graph edges unless augmented externally.</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No graph-to-text linearization; nodes are independently encoded and their KV caches concatenated. Some approaches sort chunks by similarity, or share PEs, or apply attention re-scaling to mitigate distributional shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>RAG benchmarks, ARXIV-QA, topic classification, and long-context generation tasks used as baselines in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mixed — APE typically outperforms PCW (tuning-free improvements), but a performance gap remains relative to fine-tuned block-attention or Graph-KV. Examples from Table 4 (paper topic classification): 8B-SFT PCW: Cora 68.63%, Pubmed 76.95%; 8B-SFT APE: Cora 66.92%, Pubmed 77.01%; 8B-Block Block-RAG: Cora 69.55%, Pubmed 83.24%. Parallel methods failed on ARXIV-QA to capture cross-document connections and showed much lower QA correctness than Graph-KV/optimal sequential.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared against Graph-KV and sequential: Parallel encoding is more efficient (O(n L^2) prefilling) and long-context friendly but cannot model inter-document dependencies; Block-RAG (post-trained block attention) mitigates some gaps but Graph-KV still outperforms by injecting structure-aware updates.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Efficiency (linear in number of chunks), lower TTFT when KV caches are prefilled, long-context friendliness (shared PEs reduce positional-span), and simplicity of independent encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>By default discards direct interactions among chunks and thus fails to capture relational/multi-hop dependencies; distributional shift from pretraining may require re-scaling heuristics (APE) or post-training (Block-RAG) to perform well.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fails to capture cross-document dependencies needed in multi-hop reasoning and ARXIV-QA (parallel baselines performed poorly on ARXIV-QA and multi-hop scenarios in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8792.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8792.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph serialization to NL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Serialization of graph-structured data into natural language (graph → text linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of prior approaches that convert graph nodes and edges into natural language sentences (linearized descriptions) for LLM input so that the model processes graph structure as text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-to-text linearization / natural-language serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Explicitly verbalize nodes and edges as natural-language sentences or templates and concatenate them into a sequence that an LLM can read; the graph structure is encoded by the order and content of these sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (knowledge graphs, dependency trees, scene graphs, text-attributed graphs) that are transformed into text descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Construct textual sentences describing node attributes and edges (e.g., 'Node A cites Node B', 'Node X has property Y') and linearize them into a prompt; often accompanied by templates or traversal-based ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used historically for structured-data reasoning, QA, and tasks requiring LLMs to read and reason about graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numerical metrics provided in this paper for these methods; the paper states scalability and quadratic attention limits prevent effective reasoning on moderately sized text-attributed graphs (tens of documents / 100k+ tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper contrasts this method with Graph-KV and adapter-based methods: serialization is straightforward but scales poorly and struggles to faithfully capture complex structural dependencies compared to Graph-KV's structural attention.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Human interpretable, simple to implement (no model modification), directly compatible with any LLM input pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Scalability issues due to long sequences and quadratic attention, difficulty accurately verbalizing and preserving complex structural relations, and poor performance for moderately large text-attributed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Ineffective on moderately sized text-attributed graphs (tens of documents / 100k+ tokens) because of long sequence length, quadratic attention, and inability to preserve structure faithfully.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8792.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8792.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adapter projection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adapter modules projecting graphs into token embedding space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses adapter layers to map graph-structured features into the LLM token embedding space so the model can consume graph signals without direct attention mask changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adapter-based graph→token projection</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Train small adapter modules that transform graph representations (e.g., node embeddings, aggregated neighbor features) into token-like embeddings consumable by the LLM; these adapters are attached to the model or run as a preprocessing step.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs and text-attributed graphs where graph features are summarized into dense vectors</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Train or fine-tune small adapter networks to take graph features and produce embeddings aligned with the LLM's token space; insert those embeddings into prompts or internal layers.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph learning tasks interfaced with LLMs, such as classification or generation where structural information is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No specific numerical metrics provided in this paper; the paper notes that adapter-based solutions often show limited generalization due to alignment difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared qualitatively to Graph-KV and graph→text serialization: adapters avoid long serializations but can suffer from poor alignment/generalization; Graph-KV takes a more foundational approach by changing attention rather than relying on adapters.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Modular (can be attached to pretrained LLMs), avoids long prompt serialization, potentially efficient if adapters are small.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Alignment challenges between adapter outputs and LLM token space can limit generalization; may require careful training and may not capture fine-grained structural interactions as directly as attention-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported limited generalization in prior works; may not reliably capture complex structural inductive biases without extensive adapter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8792.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8792.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-KV-Top-m (Bipartite RAG construction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-KV-Top-m (bipartite graph construction for RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical instantiation of Graph-KV for RAG: construct directed edges from the top-m retrieved chunks (by similarity) to each remaining chunk so that each target chunk attends to a small number (m) of most-relevant sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Bipartite/top-m source→target KV-graph representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each retrieved chunk, designate the top-m (m=1 or 3 tested) most semantically similar chunks as its sources. The KV caches of those top-m source chunks are concatenated and used as the attention keys/values when updating the target chunk's KV cache, producing a sparsified, task-derived graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Implicit bipartite graphs over retrieved document chunks constructed for RAG (task-defined, not native data graph)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Select top-m chunks by similarity (retriever scores) as sources per target; prefill KV caches independently; update each target's KV by attending to stacked KVs of its top-m sources (sparsified attention). Optionally Graph-KV-Full treats all chunks as both sources and targets.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>RAG benchmarks including direct inference, multi-hop reasoning, and long-document understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Graph-KV-Top-3 generally performs better than Top-1 and typically outperforms Block-RAG; in multi-hop and medium/long reasoning tasks Graph-KV-Top-3 can outperform sequential encoding by ~2%–10%. Top-3 consistently outperforms Block-RAG across tested cases; Top-1 outperforms Block-RAG except on single-document QA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Top-m variants trade off sparsity and computation: Top-3 > Top-1; Graph-KV-Full (all-pairs) often outperforms the sparsified Top-m but at higher compute. Compared to parallel encoding baselines, Top-m injects structural bias and closes large performance gaps on multi-hop tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Sparsified structure provides a cheap approximation of inter-document dependencies, improves multi-hop reasoning relative to parallel baselines, controls computation via m, and reduces positional-span via shared PEs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires selecting m (a hyperparameter) and depends on retriever similarity quality; too-sparse choices (m small) can miss necessary sources; denser (Full) raises compute/memory.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Top-1 sometimes underperforms compared to denser Top-3 or Full; performance sensitive to the quality of top-m selection when source information is distributed across many chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Let your graph do the talking: Encoding structured data for llms <em>(Rating: 2)</em></li>
                <li>APE: Faster and longer context-augmented generation via adaptive parallel encoding <em>(Rating: 2)</em></li>
                <li>Parallel context windows for large language models <em>(Rating: 2)</em></li>
                <li>Block-attention for efficient prefilling <em>(Rating: 2)</em></li>
                <li>Promptcache: Modular attention reuse for low-latency inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8792",
    "paper_id": "paper-279250211",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Graph-KV",
            "name_full": "Graph-KV (Graph-structured KV-cache attention)",
            "brief_description": "A method that injects structural inductive biases into auto-regressive LLMs by treating each text chunk's KV-cache as a condensed node representation and updating target-node KV-caches via attention only over designated source-node KV-caches, combined with shared positional-encoding ranges for sources and targets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph-structured KV-cache representation (Graph-KV)",
            "representation_description": "Each text chunk is independently prefixed and encoded into a KV pair in the model's KV-cache. Given a directed graph G=(V,E) over chunks, Graph-KV updates each target chunk u_j by computing attention from u_j's Query vector only to the stacked Keys/Values of its source neighbors N(j) (sparsified block attention). Source chunks share a common PE range [0,L) and target chunks share another PE range [L,2L), reducing positional bias and overall positional span. The update can be iterated (the paper uses a single round).",
            "graph_type": "Directed text-attributed graphs (e.g., citation ego-graphs), implicit logical/temporal dependency graphs constructed for RAG (bipartite graphs over retrieved chunks), and general node-attribute graphs where nodes are text chunks",
            "conversion_method": "No linear textual serialization is required; conversion is implemented by pre-filling independent KV-caches per node and imposing a graph-structured block attention mask so that a target node's KV is generated by attending only to its source nodes' KVs. Positional encoding is allocated in shared ranges for sources and targets rather than unique absolute positions.",
            "downstream_task": "Retrieval-Augmented Generation (RAG) benchmarks (direct inference, multi-hop reasoning, long-document understanding), ARXIV-QA (multi-hop QA over citation ego-graphs with full texts), paper topic classification (node classification on citation graphs), and synthetic scalability/stress tests.",
            "performance_metrics": "Reported improvements across tasks: Graph-KV-Top-3 consistently outperforms Block-RAG across RAG datasets; in multi-hop/long-document reasoning Graph-KV-Top-3 outperforms sequential encoding by ~2%–10%. On paper topic classification (Table 4) using the 8B-Block backbone, Graph-KV: Cora = 71.03% accuracy, Pubmed = 84.61% accuracy. Reported specific gains versus baselines: +4.65% (bridge on HotpotQA) and +6.93% (compose on 2Wiki) over Block-RAG in selected subtasks. Memory/latency: Graph-KV encodes &gt;3× the number of neighbors compared to sequential encoding in the synthetic star-graph memory test and achieves significantly lower Time-To-First-Token (TTFT) due to KV-prefilling.",
            "comparison_to_others": "Directly compared to sequential serialization, PCW, APE, and Block-RAG baselines. Graph-KV generally outperforms parallel-encoding baselines (PCW, APE) and the Block-RAG variant on most benchmarks; Graph-KV-Top-3 often beats or matches sequential encoding and surpasses sequential in many multi-hop/long-context scenarios. Graph-KV-Full (modeling all pairwise dependencies) typically outperforms sparsified Top-m variants.",
            "advantages": "Models structural dependencies explicitly (message-passing-like step inside LLM), reduces positional bias via shared PE ranges, is long-context friendly (reduces positional-span consumption), sparsifies attention for efficiency, supports parallel prefill of KVs for low TTFT, scales to many neighbors with lower peak memory than full sequential encoding, and shows robustness to distractors/order in ARXIV-QA.",
            "disadvantages": "Evaluated mainly for one-hop dependency injection in this paper (multi-round/iterative updates not fully explored), requires a backbone adapted/post-trained to independent block attention for best stability (distributional shift if using vanilla pretrained LLMs), sparse variants require choosing hyperparameter m (top-m) which affects performance, Graph-KV-Full increases computation relative to sparse variants.",
            "failure_cases": "Sparser variants (e.g., Top-1) can underperform compared to more connected variants (Top-3 or Full) and may be insufficient when multiple sources are necessary; Graph-KV did not consistently exceed sequential encoding on simple single-chunk inference tasks where serialization artificially provides the needed context; multi-round updates provided no significant gains on the evaluated tasks; the method depends on an attention-backbone fine-tuned for block-prefilling (tuning-free variants underperform).",
            "uuid": "e8792.0",
            "source_info": {
                "paper_title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Sequential Encoding",
            "name_full": "Sequential Encoding / Serialization",
            "brief_description": "The standard auto-regressive approach that linearizes all text chunks into a single concatenated token sequence so that causal attention attends over all preceding tokens.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Linearization / serialization",
            "representation_description": "Graph or multi-document inputs are converted into a single flat token sequence by concatenating text chunks (e.g., documents, references) in some order; positional embeddings and causal attention define the effective relations during decoding.",
            "graph_type": "Applied to any structured input by flattening (graphs, citation networks, sets of documents become sequences)",
            "conversion_method": "Simple concatenation/linearization of nodes' texts into one ordered prompt (Prefix + Text Chunks + Question) with token-level positional encodings following the concatenated order.",
            "downstream_task": "Baseline for RAG tasks, ARXIV-QA, paper topic classification, and other QA/classification tasks used in the paper.",
            "performance_metrics": "Varied by backbone and dataset. Examples from Table 4: with 8B-SFT backbone, Sequential: Cora 66.66% ±0.62, Pubmed 80.64% ±0.39; with 8B-RAG backbone, Sequential: Cora 70.35% ±0.17, Pubmed 82.06% ±0.16. Sequential sometimes attains peak performance in tasks when an optimal ordering happens to encode structural dependencies in sequence, but performance is highly order-sensitive.",
            "comparison_to_others": "Sequential encoding is compared as the canonical baseline. It can model dependencies implicitly via ordering but suffers from positional bias and quadratic attention complexity versus parallel methods; Graph-KV often outperforms sequential on multi-hop/long-context tasks and is more robust to ordering and distractors.",
            "advantages": "Straightforward to implement, can convey chain-of-thought / ordered dependencies through serialization, sometimes attains strong performance when ordering aligns with required reasoning chains.",
            "disadvantages": "Strong positional bias (sensitive to chunk order), quadratic attention complexity across chunks (O(n^2 L^2) encoding complexity), rapid consumption of positional index space (context window limits), and poor robustness when many distractors long contexts are present.",
            "failure_cases": "Severe performance degradation under long contexts with distractors and when relevant documents are positioned early in the sequence; exhibits recency/primacy biases; cannot scale to many neighbor documents due to memory constraints.",
            "uuid": "e8792.1",
            "source_info": {
                "paper_title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Parallel Text Encoding",
            "name_full": "Parallel Text Encoding (independent KV-cache per chunk, shared PEs)",
            "brief_description": "A family of methods that independently prefill KV-caches for each text chunk in parallel (often sharing PE ranges) and then concatenate or combine those KV-caches at generation time; includes PCW, APE, Block-RAG variants referenced in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Independent-block KV encoding / Parallel encoding",
            "representation_description": "Each text chunk is encoded independently in parallel into KV pairs; chunks commonly share positional-encoding ranges to indicate lack of explicit order; during generation tokens attend to all encoded chunks' KVs. Methods differ in how they handle distributional shift (e.g., APE rescales attention; Block-RAG uses post-training).",
            "graph_type": "Applied to sets of text chunks (unordered), not explicitly modeling graph edges unless augmented externally.",
            "conversion_method": "No graph-to-text linearization; nodes are independently encoded and their KV caches concatenated. Some approaches sort chunks by similarity, or share PEs, or apply attention re-scaling to mitigate distributional shifts.",
            "downstream_task": "RAG benchmarks, ARXIV-QA, topic classification, and long-context generation tasks used as baselines in the paper.",
            "performance_metrics": "Mixed — APE typically outperforms PCW (tuning-free improvements), but a performance gap remains relative to fine-tuned block-attention or Graph-KV. Examples from Table 4 (paper topic classification): 8B-SFT PCW: Cora 68.63%, Pubmed 76.95%; 8B-SFT APE: Cora 66.92%, Pubmed 77.01%; 8B-Block Block-RAG: Cora 69.55%, Pubmed 83.24%. Parallel methods failed on ARXIV-QA to capture cross-document connections and showed much lower QA correctness than Graph-KV/optimal sequential.",
            "comparison_to_others": "Compared against Graph-KV and sequential: Parallel encoding is more efficient (O(n L^2) prefilling) and long-context friendly but cannot model inter-document dependencies; Block-RAG (post-trained block attention) mitigates some gaps but Graph-KV still outperforms by injecting structure-aware updates.",
            "advantages": "Efficiency (linear in number of chunks), lower TTFT when KV caches are prefilled, long-context friendliness (shared PEs reduce positional-span), and simplicity of independent encoding.",
            "disadvantages": "By default discards direct interactions among chunks and thus fails to capture relational/multi-hop dependencies; distributional shift from pretraining may require re-scaling heuristics (APE) or post-training (Block-RAG) to perform well.",
            "failure_cases": "Fails to capture cross-document dependencies needed in multi-hop reasoning and ARXIV-QA (parallel baselines performed poorly on ARXIV-QA and multi-hop scenarios in the paper).",
            "uuid": "e8792.2",
            "source_info": {
                "paper_title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Graph serialization to NL",
            "name_full": "Serialization of graph-structured data into natural language (graph → text linearization)",
            "brief_description": "A class of prior approaches that convert graph nodes and edges into natural language sentences (linearized descriptions) for LLM input so that the model processes graph structure as text.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Graph-to-text linearization / natural-language serialization",
            "representation_description": "Explicitly verbalize nodes and edges as natural-language sentences or templates and concatenate them into a sequence that an LLM can read; the graph structure is encoded by the order and content of these sentences.",
            "graph_type": "General graphs (knowledge graphs, dependency trees, scene graphs, text-attributed graphs) that are transformed into text descriptions",
            "conversion_method": "Construct textual sentences describing node attributes and edges (e.g., 'Node A cites Node B', 'Node X has property Y') and linearize them into a prompt; often accompanied by templates or traversal-based ordering.",
            "downstream_task": "Used historically for structured-data reasoning, QA, and tasks requiring LLMs to read and reason about graphs.",
            "performance_metrics": "No numerical metrics provided in this paper for these methods; the paper states scalability and quadratic attention limits prevent effective reasoning on moderately sized text-attributed graphs (tens of documents / 100k+ tokens).",
            "comparison_to_others": "Paper contrasts this method with Graph-KV and adapter-based methods: serialization is straightforward but scales poorly and struggles to faithfully capture complex structural dependencies compared to Graph-KV's structural attention.",
            "advantages": "Human interpretable, simple to implement (no model modification), directly compatible with any LLM input pipeline.",
            "disadvantages": "Scalability issues due to long sequences and quadratic attention, difficulty accurately verbalizing and preserving complex structural relations, and poor performance for moderately large text-attributed graphs.",
            "failure_cases": "Ineffective on moderately sized text-attributed graphs (tens of documents / 100k+ tokens) because of long sequence length, quadratic attention, and inability to preserve structure faithfully.",
            "uuid": "e8792.3",
            "source_info": {
                "paper_title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Adapter projection",
            "name_full": "Adapter modules projecting graphs into token embedding space",
            "brief_description": "An approach that uses adapter layers to map graph-structured features into the LLM token embedding space so the model can consume graph signals without direct attention mask changes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Adapter-based graph→token projection",
            "representation_description": "Train small adapter modules that transform graph representations (e.g., node embeddings, aggregated neighbor features) into token-like embeddings consumable by the LLM; these adapters are attached to the model or run as a preprocessing step.",
            "graph_type": "General graphs and text-attributed graphs where graph features are summarized into dense vectors",
            "conversion_method": "Train or fine-tune small adapter networks to take graph features and produce embeddings aligned with the LLM's token space; insert those embeddings into prompts or internal layers.",
            "downstream_task": "Graph learning tasks interfaced with LLMs, such as classification or generation where structural information is needed.",
            "performance_metrics": "No specific numerical metrics provided in this paper; the paper notes that adapter-based solutions often show limited generalization due to alignment difficulties.",
            "comparison_to_others": "Compared qualitatively to Graph-KV and graph→text serialization: adapters avoid long serializations but can suffer from poor alignment/generalization; Graph-KV takes a more foundational approach by changing attention rather than relying on adapters.",
            "advantages": "Modular (can be attached to pretrained LLMs), avoids long prompt serialization, potentially efficient if adapters are small.",
            "disadvantages": "Alignment challenges between adapter outputs and LLM token space can limit generalization; may require careful training and may not capture fine-grained structural interactions as directly as attention-based methods.",
            "failure_cases": "Reported limited generalization in prior works; may not reliably capture complex structural inductive biases without extensive adapter tuning.",
            "uuid": "e8792.4",
            "source_info": {
                "paper_title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Graph-KV-Top-m (Bipartite RAG construction)",
            "name_full": "Graph-KV-Top-m (bipartite graph construction for RAG)",
            "brief_description": "A practical instantiation of Graph-KV for RAG: construct directed edges from the top-m retrieved chunks (by similarity) to each remaining chunk so that each target chunk attends to a small number (m) of most-relevant sources.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Bipartite/top-m source→target KV-graph representation",
            "representation_description": "For each retrieved chunk, designate the top-m (m=1 or 3 tested) most semantically similar chunks as its sources. The KV caches of those top-m source chunks are concatenated and used as the attention keys/values when updating the target chunk's KV cache, producing a sparsified, task-derived graph structure.",
            "graph_type": "Implicit bipartite graphs over retrieved document chunks constructed for RAG (task-defined, not native data graph)",
            "conversion_method": "Select top-m chunks by similarity (retriever scores) as sources per target; prefill KV caches independently; update each target's KV by attending to stacked KVs of its top-m sources (sparsified attention). Optionally Graph-KV-Full treats all chunks as both sources and targets.",
            "downstream_task": "RAG benchmarks including direct inference, multi-hop reasoning, and long-document understanding.",
            "performance_metrics": "Graph-KV-Top-3 generally performs better than Top-1 and typically outperforms Block-RAG; in multi-hop and medium/long reasoning tasks Graph-KV-Top-3 can outperform sequential encoding by ~2%–10%. Top-3 consistently outperforms Block-RAG across tested cases; Top-1 outperforms Block-RAG except on single-document QA.",
            "comparison_to_others": "Top-m variants trade off sparsity and computation: Top-3 &gt; Top-1; Graph-KV-Full (all-pairs) often outperforms the sparsified Top-m but at higher compute. Compared to parallel encoding baselines, Top-m injects structural bias and closes large performance gaps on multi-hop tasks.",
            "advantages": "Sparsified structure provides a cheap approximation of inter-document dependencies, improves multi-hop reasoning relative to parallel baselines, controls computation via m, and reduces positional-span via shared PEs.",
            "disadvantages": "Requires selecting m (a hyperparameter) and depends on retriever similarity quality; too-sparse choices (m small) can miss necessary sources; denser (Full) raises compute/memory.",
            "failure_cases": "Top-1 sometimes underperforms compared to denser Top-3 or Full; performance sensitive to the quality of top-m selection when source information is distributed across many chunks.",
            "uuid": "e8792.5",
            "source_info": {
                "paper_title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Let your graph do the talking: Encoding structured data for llms",
            "rating": 2,
            "sanitized_title": "let_your_graph_do_the_talking_encoding_structured_data_for_llms"
        },
        {
            "paper_title": "APE: Faster and longer context-augmented generation via adaptive parallel encoding",
            "rating": 2,
            "sanitized_title": "ape_faster_and_longer_contextaugmented_generation_via_adaptive_parallel_encoding"
        },
        {
            "paper_title": "Parallel context windows for large language models",
            "rating": 2,
            "sanitized_title": "parallel_context_windows_for_large_language_models"
        },
        {
            "paper_title": "Block-attention for efficient prefilling",
            "rating": 2,
            "sanitized_title": "blockattention_for_efficient_prefilling"
        },
        {
            "paper_title": "Promptcache: Modular attention reuse for low-latency inference",
            "rating": 1,
            "sanitized_title": "promptcache_modular_attention_reuse_for_lowlatency_inference"
        }
    ],
    "cost": 0.017696,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GRAPH-KV: BREAKING SEQUENCE VIA INJECTING STRUCTURAL BIASES INTO LARGE LANGUAGE MODELS
9 Jun 2025</p>
<p>Haoyu Wang haoyu.wang@gatech.edu 
Georgia Institute of Technology</p>
<p>Peihao Wang 
The University of Texas at Austin</p>
<p>Mufei Li 
Georgia Institute of Technology</p>
<p>Shikun Liu 
Georgia Institute of Technology</p>
<p>Siqi Miao 
Georgia Institute of Technology</p>
<p>Zhangyang Wang 
The University of Texas at Austin</p>
<p>Pan Li panli@gatech.edu 
Georgia Institute of Technology</p>
<p>GRAPH-KV: BREAKING SEQUENCE VIA INJECTING STRUCTURAL BIASES INTO LARGE LANGUAGE MODELS
9 Jun 2025A0CD5343FE1C253749D3C9C17A74EFC7arXiv:2506.07334v1[cs.LG]
Modern large language models (LLMs) are inherently auto-regressive, requiring input to be serialized into flat sequences regardless of their structural dependencies.This serialization hinders the model's ability to leverage structural inductive biases, especially in tasks such as retrieval-augmented generation (RAG) and reasoning on data with native graph structures, where inter-segment dependencies are crucial.We introduce Graph-KV with the potential to overcome this limitation.Graph-KV leverages the KV-cache of text segments as condensed representations and governs their interaction through structural inductive biases.In this framework, "target" segments selectively attend only to the KV-caches of their designated "source" segments, rather than all preceding segments in a serialized sequence.This approach induces a graph-structured block mask, sparsifying attention and enabling a message-passing-like step within the LLM.Furthermore, strategically allocated positional encodings for source and target segments reduce positional bias and context window consumption.We evaluate Graph-KV across three scenarios: (1) seven RAG benchmarks spanning direct inference, multi-hop reasoning, and long-document understanding; (2) ARXIV-QA, a novel academic paper QA task with full-text scientific papers structured as citation ego-graphs; and (3) paper topic classification within a citation network.By effectively reducing positional bias and harnessing structural inductive biases, Graph-KV substantially outperforms baselines, including standard costly sequential encoding, across various settings.Code and the ARXIV-QA data are publicly available at https://github.com/Graph-COM/GraphKV.</p>
<p>Introduction</p>
<p>Modern large language models (LLMs) [1,2,3], despite their notable successes, are fundamentally auto-regressive.This characteristic, as a consequence of their training approaches [4,5], necessitates the serialization of information for processing.Consequently, all input, regardless of its intrinsic structure or complex dependencies, such as orderinsensitivity, temporal or logical relationships, must be flattened into an ordered sequence.This forced serialization can be suboptimal and may introduce a sequential bias, potentially hindering the LLM's ability to fully leverage these internal relationships.For example, in retrieval-augmented generation (RAG) [6,7,8,9], retrieved text segments, which may lack a linear order or possess complex, non-linear interdependencies, must still be artificially serialized, which can limit effective multi-hop reasoning [10,11,12,13] and introduce positional biases [14,15,16].Similarly, processing data with native graph structures, such as citation networks [17,18] where citations signify knowledge dependencies, presents challenges.Serializing documents referenced by the same document, for instance, leads to drawbacks including: 1) positional biases that can obscure parallel citation relationships; 2) quadratic computational complexity when attending to all document pairs; and 3) context window limitations when dealing with numerous references (Fig. 1).</p>
<p>Therefore, a critical question arises: How can we align the structural inductive bias of the data with the mechanisms of auto-regressive LLMs, moving beyond simplistic serialization?</p>
<p>Existing literature has explored mitigating the negative effects of token-serialization, primarily by aiming to eliminate positional bias and enable LLMs to process text segments in a permutation-invariant manner.For instance, Wang (1) positional bias, where different serialization orders lead to varied outputs [19];(2) quadratic computational complexity from full attention between all document pairs; and (3) rapid context window consumption, as position indices quickly exceed limits.The bottom-right subfigure illustrates Graph-KV.Text chunks are independently encoded into KV caches, where Graph-KV arranges the text chunk of each target text after the KV of their source texts to update their respective KV caches.Notably, all source texts share same positional encoding (PE) range, while all target texts share another, with their position index immediately following that of the source nodes.This approach reduces the PE and context window usage.At query time, the query attends to both the source chunks and the target chunks to perform decoding.et al. [20] proposed to reorder documents based on attention scores computed without positional encodings (PEs); however, the requisite sorting and full attention computations in this method introduce significant computational overhead.Other works prioritize efficient inference by prefilling the key-value (KV) cache of documents independently in parallel [21,22,23,24].In these approaches, the encoding of documents within the KV-cache often relies on either shared PEs or sorting based on semantic similarity scores from external retrievers.Although these parallel encoding strategies enhance efficiency, they inherently cannot model inter-segment dependencies, let alone native inductive biases within the structured data (as summarized in Table 1).</p>
<p>Long-Context Friendly</p>
<p>Sparse Attention &amp; Efficiency</p>
<p>Free from Positional Biases Structural Inductive Biases Sequential Encoding ✗ ✗ ✗ ✗ Promptcache [23] ✗ ✓ ✗ ✗ PINE [20] ✗ ✗ ✓ ✗ PCW [22] ✓ ✓ ✓ ✗ APE [21] ✓ ✓ ✓ ✗ Block-Attention [24] ✗ ✓ ✗ ✗ Graph-KV (ours) ✓ ✓ ✓ ✓ To address these limitations, we introduce Graph-KV.The core principle of Graph-KV is to treat the KV cache of a given text segment as its condensed information representation and to control its generation using structural inductive biases.Specifically, after initially prefilling the independent KV caches of all text segments, a "target" segment's KV cache is generated by attending only to the KV caches of its "source" text segments, rather than to all segments that merely precede it in the token-serialization sequence.The determination of "source → target" relationships is guided by structural inductive biases tied to either the data or the specific tasks.From another perspective, this approach essentially introduces a graph-structured block mask (Fig. 1) that sparsifies attention computation during KV cache generation, effectively enabling a "message passing through graph" step within the LLM.Moreover, to mitigate inherent positional biases from the LLM, the attention computation imposes shared PEs across the source segments, with the target segment receiving PEs with position indices immediately following its sources.This design substantially reduces context window consumption through shared PEs while preserving structural alignment.</p>
<p>We evaluated Graph-KV across three diverse settings.First, Graph-KV was assessed on seven RAG benchmarks, covering direct inference [25,26], multi-hop reasoning [13,10,12,11], and long-document understanding [27].For these tasks, where native graph structures are absent, we introduced a bipartite graph to establish structural bias between text segments.Across all RAG benchmarks, Graph-KV significantly outperformed parallel text encoding baselines.Notably, in multi-hop reasoning tasks, Graph-KV surpassed even sequential reading while maintaining sparse computation.Second, we introduced ARXIV-QA, a novel and challenging task featuring real-world graph biases.In ARXIV-QA, questions are constructed from the full text of a central scientific paper and its linked references, sourced from the arXiv citation network [17].These questions require probing technical details and understanding both content and citation relationships.On ARXIV-QA, existing efficient parallel text encoding baselines performed poorly, and standard sequential encoding demonstrated severe positional biases.In contrast, Graph-KV exhibited significant robustness, achieving performance comparable to the peak results of sequential encoding (which necessitates optimal document positioning) without displaying such sensitivity.Third, Graph-KV was evaluated on paper topic classification tasks within citation networks, which possess inherent structural biases through citation links.In this setup, LLMs must classify a central paper by analyzing its title, abstract, and potentially hundreds of references.Graph-KV demonstrated significantly superior performance compared to both sequential encoding and parallel text encoding baselines.</p>
<p>Related Work</p>
<p>Positional Bias in LLMs.Large Language Models (LLMs) exhibit positional bias, wherein their performance is adversely affected by the sequential order of input data [19,28,29,30,31,16,32,15].This phenomenon is widely believed to stem from the interplay of Positional Embeddings (PEs) [33,14,34,35] and the inherent causal attention mechanism [36].Although some research indicates that removing PEs from the transformer architecture can enhance LLM generalization to longer context windows [37], the causal attention mechanism itself can still implicitly induce positional biases [34,38].Concrete examples of positional bias are evident in RAG, where models often favor information placed at the beginning or end of the context [32,39], and in in-context learning, where the order of examples significantly impacts outcomes [40,41].The tasks investigated in this work necessitate a more explicit capture of structural dependencies, and our findings reveal that naively serializing input exacerbates positional bias in such scenarios.</p>
<p>Parallel Encoding and Block Attention.Research has explored techniques to avoid quadratic computational complexity in RAG for generating KV caches for retrieved documents individually and in parallel [42,22,23,21,43,44,45,46]. PCW [22] initiated this line of study; however, its performance can degrade substantially in many cases due to distribution shifts in the new form of KV caches.APE [21] proposes a fine-tuning-free algorithm that mitigates distribution shifts with parallel encoding by re-scaling attention magnitudes.[43] further trains a small-LM as a scorer to refine the retrieved parallel contexts.Block-Attention [24] demonstrates further performance improvements over these methods, attributed to its more extensive post-training process.However, a common limitation of these parallel processing strategies is their failure to model inter-document dependencies.Graph-KV mitigates this limitation while preserving the efficiency of parallel encoding.</p>
<p>Modeling structured data with LLMs.LLMs predominantly process structured data via two main strategies.The first serializes structured information like graphs into natural language formats for model input [47,48,49].However, this method faces scalability issues from quadratic attention complexity and the inherent challenge of accurately verbalizing intricate structural dependencies.As a result, even reasoning over moderately sized text-attributed graphs (e.g., tens of documents, 100k+ tokens) can be problematic [50,51].The second strategy uses adapter modules to project graph data into the LLM's token embedding space [52,53,54,55,56].These adapter-based solutions often exhibit limited generalization, largely due to challenges in achieving robust adapter-mediated alignment [57,58,59,60].Graph-KV offers a distinct, more foundational approach by being the first to directly modify the LLM's attention mechanism for structured data integration.</p>
<p>The Challenge of Noisy Multi-Hop Reasoning.Multi-hop reasoning, which demands capturing arbitrary structural dependencies among multiple pieces of information, remains challenging for LLMs with standard sequential encoding.This difficulty is substantially amplified in real-world scenarios where information is non-contiguous and sparsely distributed within noisy, long contexts, leading to N-fold reductions in LLM performance [61,62,63].However, as our experiments demonstrate, with structural inductive biases, Graph-KV can significantly improve the reasoning capabilities of LLMs.</p>
<p>Methodology</p>
<p>In this section, we introduce Graph-KV.We assume structural inductive biases can be described by a graph connecting text chunks.Such biases might originate natively from the data or be defined based on the tasks.How these are specified for various tasks will be detailed in the experimental section.</p>
<p>Preliminaries.Let q be a natural language query and G = (V, E) be a directed graph representing input structured data.Each node u ∈ V corresponds to a text chunk with an average token length d, and each directed edge (u 1 , u 2 ) ∈ E represents some structural dependence from a source chunk u 1 to a target chunk u 2 .The objective is for an LLM f to generate an answer a = f (q, G) by encoding both q and G.This task requires the LLM to comprehend the individual textual content of nodes while also properly leveraging and reasoning over the structural inductive biases encoded in G.</p>
<p>Sequential Encoding is the default approach in modern LLMs [36,64,65].It involves processing an arbitrary linearized sequence of text chunks, denoted without loss of generality as [u 1 , u 2 , . . ., u n ], where each u i ∈ V is composed of a sequence of token embeddings, by using the concatenated sequence [u 1 , u 2 , . . ., u n , q] as input.Modern LLMs commonly employ causal attention, where each token attends to all preceding tokens.This mechanism results in a computational complexity of O(n 2 L 2 ), where L denotes the max chunk length.A consequence of sequential encoding is the sensitivity of the model output to input order, termed positional biases.Moreover, this default setting lacks inherent mechanisms for leveraging structural inductive biases, should they exist in the data.</p>
<p>Parallel Text Encoding is adopted by an alternative line of work [22,21,23,24] that treats text chunks as an unordered set, S = {u 1 , u 2 , . . ., u n }.Here, the LLM encodes each chunk u i ∈ S independently and in parallel, often with all chunks sharing the same PEs to signify their lack of explicit order.The computational complexity for this encoding process is O(nL 2 ), linear in n (the number of chunks).This method, however, discards the modeling of direct interactions among chunks, thereby sacrificing relational information critical for multi-hop reasoning in favor of efficiency.During answer generation, tokens attend to all encoded text chunks in S and the query q.</p>
<p>Graph-KV injects structural inductive bias using two main strategies: a structure-aware attention mechanism and appropriately assigned shared PEs.The Structure-aware Attention Mechanism.First, Graph-KVperforms offline parallel encoding of each text chunk u i to obtain its initial latent representation h (0)</p>
<p>Graph-KV</p>
<p>ui .These representations, {h (0) ui } ui∈V , are then used to form initial Key-Value (KV) pairs, denoted as {(k
(0) ui , v(0)
ui )} ui∈V , which can be loaded into the KV cache.Following the graph structure G = (V, E), Graph-KV updates the representation of a target chunk u j by modeling its relationships via the attention mechanism with its source chunks, denoted as N (j) = {u i | (u i , u j ) ∈ E}.This update is achieved by computing a sparsified attention: softmax
Qj K ⊤ N (j) √ d h V N (j)
where Q j is the Query vector associated with u j (the target chunk),
K N (j) = [k (0) ui ] ui∈N (j) and V N (j) = [v (0)
ui ] ui∈N (j) are matrices formed by stacking the key-values of its source chunks, respectively, and d h used for normalization denotes the dimension of QK values.This update procedure can be iterated for multiple rounds.However, for our experiments, we conduct a single round (i.e., t = 1).This serves as a proof-of-concept to model interactions extending beyond those addressed by existing parallel encoding methods, which typically process chunks as purely independent, while preserving low computational complexity.Iterating for multiple rounds does not yield significant performance gains on the current evaluation tasks.Nevertheless, we believe that with more complex tasks, or if the model were further fine-tuned to adapt to this new attention architecture, greater improvements could be anticipated.Finally, upon query, input query tokens and subsequently generated answer tokens attend to the representations of both source and target chunks.</p>
<p>The Allocation of Positional Encodings.Our allocation of PEs aims to reduce positional bias and improve context efficiency.We begin by considering the scenario without explicit directed edges, where graph-structured data effectively becomes a collection of independent text chunks.This configuration is analogous to those in many existing studies on parallel encoding [21,22,24].To mitigate positional bias for these independent chunks, all such text chunks are assigned positions within the shared range [0, L), where we assume a maximum chunk length of L. Furthermore, when structural dependencies are present in the data (e.g., via directed edges), target chunks are subsequently assigned positions from L to 2L, i.e., within another shared range [L, 2L) immediately following the first.Query tokens and any generated tokens are then allocated positions in a range beyond 2L, subsequent to those of the target chunks.</p>
<p>While this study does not investigate the iterative application of target chunks as source chunks in subsequent processing rounds, the proposed methodology permits such natural extension.For instance, target chunks in a subsequent round could be allocated positions within the range [2L, 3L), with query token positions adjusted correspondingly; this iterative pattern can be continued as needed.A key benefit of this procedure is its conservation of the context window: since numerous chunks share identical positional ranges, the overall required positional span is only about T L. Here, T denotes the number of iterations (a generally small constant), and L is often less than 10k.</p>
<p>To illustrate how latent representations of target chunks are formed by Graph-KV, please refer to Fig. 2. Suppose there are directed edges doc.2 → doc.1 and doc.3 → doc.1, Graph-KV can be understood as guiding the LLM to process two effective "documents": one formed by doc.2 followed by doc.1, and another by doc.3 followed by doc.1.The representations of the doc.1 portions obtained from both these effective documents, are then aggregated.Consequently, the representation of target chunk doc.1 contains the information reflecting its connections to source chunks doc.2 and doc.3.</p>
<p>Computational Complexity.The representations for all text chunks in the first round are computed with complexity O(|V |L 2 ), aligning with previous parallel encoding methods.Suppose T is a set of target chunks, updating target chunk representations has a complexity of O( uj ∈T |N (j)|L 2 ) = O(|E|L 2 ) as |E| is the total number of such dependencies.During query time, the attention complexity for each query or generated token over all source and target chunks is O(|V |L), similar to vanilla sequential and parallel encoding schemes at generation.</p>
<p>Remark: Attention Sink.APE [21] considers sharing the PE range but avoids using the first several positions to avoid the attention sink problem [66].We find that Graph-KV remains unaffected even though its chunks share the PE range from the first token.This is because we adopt the model that has been fine-tuned with independent attention blocks [24] to fit this change.</p>
<p>Experiments</p>
<p>Task Selection We design four tasks to evaluate Graph-KV, including three real-world applications and one stress test: Task 1: Retrieval-Augmented Generation (RAG), Task 2: ARXIV-QA, a new task of multi-hop paper understanding, Task 3: Paper topic classification, which is a classical graph learning task; Task 4: Stress test on scalability and efficiency over synthetic data.</p>
<p>Backbone for Graph-KV.Graph-KV necessitates the independent encoding of different text segments.This process introduces a distributional shift standard LLM backbones [22].Two primary solutions address this challenge.The first involves applying a tuning-free heuristic, such as APE [21], which alleviates the shift by adjusting the temperature and scaling of attention weights.The second approach is to post-train the model with attention masks composed of independent attention blocks for different text chunks, such as Block-RAG [24].Empirically, we found that the finetuned model exhibits more stable performance, particularly when employing Graph-KV, as demonstrated in subsequent experiments.Consequently, we default to using the llama-3.1-8B-block-ft(8B-Block) model as the backbone for Graph-KV.This model is based on the pre-trained llama-3.1-8Band is further post-trained with independent attention blocks [24] on the tulu-3 dataset [67] and 20k RAG training instances from 2Wiki [10] and TriviaQA [25].</p>
<p>Due to limited computational resources, our work focuses on the llama-3.1-8Bfamily, and we have not extended this specific tuning to other LLMs.However, our findings are, in principle, generalizable.Furthermore, we did not attempt to directly fine-tune the model with Graph-KV, although we believe such a step could further enhance its performance on many of the tasks discussed later.</p>
<p>Baselines for comparison. 1) Sequential Encoding: We consider two models which conducts serialized nexttoken prediction during post-training based on llama-3.1-8B.One is llama-3.1-8B-sft(8B-SFT) which is fully supervised tuned on the tulu-3 dataset.For fair comparison, we also take llama-3.1-8B-rag(8B-RAG), which is further tuned with the extra RAG data that is used for llama-3.1-8B-block-ft(8B-Block), which were also used for comparison in [24].The two models encode inputs in a standard serialized manner, serving as direct baselines for Graph-KV, particularly in assessing its ability to leverage structural inductive biases, eliminate positional bias, and reduce context window consumption.2) Parallel Context Window (PCW) [22], 3) Adaptive Parallel Encoding (APE) [21] and 4) Block-RAG [24] are methods with block attentions.Block-RAG serves as another direct baseline for Graph-KV as they adopt the same backbone LLM.Examples of the task scenarios in RAG are shown in Fig. 4, including direct inference, multi-hop reasoning, and long document understanding.We evaluate these scenarios using a total of 7 datasets, including NarrativeQA [26], TriviaQA [25], HotpotQA [11], 2Wiki [10], Multihop-RAG [13], MorehopQA [12] and LongBenchV2 [27].For all the datasets, 10 text chunks are provided, and accuracy is selected as the primary metric.We follow [24,32,31] to judge whether any correct answers appear in the predicted output.See more implementation details in Appendix.A.1.1.Others that require multi-hop reasoning include multi-hop reasoning (comparison, bridge and compositional (from 2Wiki [10],</p>
<p>Bridge-Comparison Compositional</p>
<p>HotpotQA [11]) and long-document understanding (from LongBench-v2 [27]).In these tasks, there exists implicit temporal or logical dependencies among the retrieved chunks.[13], MorehopQA [12] and single documentQA in LongBench-V2 [27].The best sequential encoding method is underlined, the best non-sequential approach is bolded.↑ refers that the best non-sequential approach outperforms the best sequential encoding.</p>
<p>Graph-KV for Structure Modeling in RAG.In RAG tasks, especially those involving multi-hop reasoning, text segments exhibit strong logical or temporal structural inductive biases.However, these dependencies are implicit and could not be cheaply modeled.We do not assume the dependencies are explicit, instead we construct them using a bipartite graph of text chunks.As illustrated in Fig. 3. Graph-KV tries to capture the structural dependencies among retrieved chunks without introducing too much complexity.Specifically, the retrieved chunks with the top-m (m=1,3 in our experiments) similarity scores are considered source chunks, the KV-cache of them is concatenated, and attended by each of the remaining text chunks to independently generate the corresponding KV values.We also consider all retrieved chunks as both source and target chunks (named as "Graph-KV-Full"), which can be viewed as modeling all potential pair-wise dependencies.</p>
<p>Result Analysis.Experiment results on RAG datasets are displayed in Tables. 2 3, with key insights as follows: Across all the tasks, results show a clear trend: Graph-KV-Full generally outperforms its sparsified variants.Specifically, Graph-KV-Top-3 generally achieves better performance than m=1.Among them, the Graph-KV-Top-1 outperforms the Block-RAG [24] baseline except on Single Document QA while Graph-KV-Top-3 consistently outperforms Block-RAG across all cases, which reveals that our injected sparse dependency effectively complements the lack of dependencies among text chunks in parallel encoding methods.Notably, in tasks that emphasize multi-hop reasoning, or medium and long reasoning from long documents (Table .2), Graph-KV-Top-3 significantly outperforms Sequential Encoding by about 2% − 10%.In the rest tasks (in Table .3), although the gaps are generally narrower Graph-KV-Top-3 still outperforms Block-RAG by 4.65% in bridge on HotpotQA, 6.93% in compose on 2Wiki).Graph-KV-Top-3 only achieves performance comparable to, but not exceeding, the sequential baseline.This is because, in many tasks in Table .3 such as inference and comparison (e.g.NarrativeQA [26], TriviaQA [25])), models can often directly infer from one text chunk.In the tasks that require two-hop reasoning, such as bridge or compose, sequential encoding may model structural dependencies that are originally held in the serialized order.</p>
<p>Regarding tuning-free approaches, although APE [21] consistently outperforms PCW [22] due to its temperature and scale adjustments, a substantial performance gap remains between the tuning-free approach APE and the fine-tuned Table 3: Performance on NarrativeQA [26], 2Wiki [10], TriviaQA [25] and HotpotQA [11].The best sequential encoding method is underlined, the best non-sequential approach is bolded.↑ refers that the best non-sequential approach outperforms the best sequential encoding.</p>
<p>Answer</p>
<p>Evidence in Reference Paper model Block-RAG.This reveals the need for fine-tuning to enable LLMs that were pretrained on full causal attention to effectively encode text chunks independently and decode properly from these text chunks.Experimental Setup.We randomly selected 100 academic papers from the Arxiv dataset [17], each with its corresponding reference papers.Following a data processing and cleaning phase, which focused on the relationships between the primary paper and its references (detailed in Appendix A.1.2),we curated a final dataset comprising 60 primary papers.Including their references, this dataset encompassed a total of 472 papers with full-text availability.We then formulated one technical question for each of the 60 papers.Answering these questions necessitates: 1) interpreting the main context of the paper, 2) comprehending the citation relationships, and 3) understanding the context of its references.An illustrative example is provided in Fig. 5 (note: the figure size has been significantly reduced due to space constraints; readers are encouraged to zoom in for detailed content).On average, addressing each question involves processing approximately 88k tokens of context, which approaches the 128k effective context window limit of the foundational models used.</p>
<p>To further elevate the difficulty of the question-answering task, we introduced distractors for each question.This was achieved by randomly grouping multiple papers along with their respective references.This expanded setup enables the evaluation of the model's positional bias (e.g., by varying the placement of the relevant paper and its references within the paper sequence) and rigorously tests the boundaries of extremely long contexts.For instance, grouping three papers and their references (i.e., one relevant paper plus two distractor papers, along with all their associated references) results in an average input length of 264.6k tokens.Fig. 7 provides a comparison.Result Analysis.Fig. 6 displays the QA accuracy.When queried solely with the paper and its references containing the answer (without distractors), sequential encoding demonstrates strong performance in this straightforward scenario.However, all parallel text encoding baselines (PCW, APE, and Block-RAG) fail to capture cross-document connections, resulting in significantly fewer correct answers.In the setup including distractors, sequential encoding exhibits severe positional bias.When the relevant paper is placed at the end of the sequence, performance remains comparable to Graph-KV, primarily due to the recency bias of pre-trained auto-regressive LLMs, which tend to focus more on later-positioned text chunks [14].Conversely, when relevant texts are positioned at the beginning of the sequence, the distractors and extended contexts lead to substantial performance degradation.For instance, all sequential encoding baselines fail to answer correctly when queried with two distractors.This demonstrates the limitations of sequential encoding in long-context, multi-source structured reasoning.Due to its PE sharing strategy and structural inductive bias injection, Graph-KV does not suffer from the positional bias and consistently achieves the best performance.We further evaluated Graph-KV on the paper topic classification task using the Cora [68] and Pubmed [69] citation graphs.This task, which originated from graph learning as 'node classification', requires LLMs to classify a central paper into one of several categories based on its abstract, title, and neighbors.See Appendix.A.1.3 for details.Each central paper may have hundreds of references (e.g. up to 130 in Pubmed), thus making the task challenging.To verify the effectiveness of including reference information, we added a 'Query-Only' baseline that only feeds the model the central paper.For all methods, the central paper was consistently placed at the end of the sequence.Since Sequential encoding and Block-RAG showed varying performance depending on the order of references placed before the central paper, we report their average performance across seeds 42 to 44.In contrast, Graph-KV is robust to the order of references.The results, presented in Table 4, show that by incorporating the dependency on references from the central paper, Graph-KV outperforms both sequential encoding and parallel text encoding baselines.It is important to note that existing works applying LLMs to this traditional graph learning task typically either perform classification in the text embedding latent space [58,70,57] or train an adapter to map sampled reference papers into the LLM's token space [54,56,55].Graph-KV is the first approach to address this task by adjusting the fundamental mechanism of LLMs.We conduct stress test on synthetic data with an Nvidia RTX6000 GPU (48GB) with AMD EPYC 7763 64-core processor, to compare Graph-KV with sequential encoding baseline on scalability and efficiency.For details, refer to Appendix.A.1.4. 1) We construct synthetic star graphs (similar to the ego graphs used in previous tasks) with fixed word number on each node of 500 and 1000.We then gradually increase the number of neighbors to assess GPU peak memory usage and report the remaining available memory.Results are presented on the left side of Fig. 8. Graph-KV is capable of encoding more than 3 times the number of neighbors compared with sequential encoding.2) To evaluate efficiency, we report the time-to-first-token (TTFT) latency on star graphs with a fixed 10 neighbors, varying the number of words per node from 100 to 800.The results are shown on the right side of Fig. 8.The benefit stems from pre-filling text chunks with injected structural biases, a capability not achievable with sequential encoding.</p>
<p>Paper Topic Classification
Backbone</p>
<p>Conclusion and Future Work</p>
<p>This paper introduces Graph-KV, a novel approach designed to overcome the limitations of auto-regressive LLMs in processing structured data.It achieves this by directly injecting structural inductive biases into the attention mechanism and employing strategic positional encoding, which in turn reduces positional bias and context window demands.Evaluations across diverse tasks, including RAG, a new academic QA benchmark (ARXIV-QA), and paper topic classification, demonstrate Graph-KV's substantial outperformance of sequential and parallel encoding baselines, particularly in multi-hop reasoning and long-context scenarios.Although this work currently evaluates Graph-KV on one-hop structural dependencies, the core idea of leveraging structural dependencies to improve LLM understanding of more intricate data topologies holds significant promise for broader research.</p>
<p>Please write a high-quality answer for the given question using only the provided search documents.</p>
<p>(If the answer is a date, format is as follows: YYYY-MM-DD (ISO standard).)After thinking step by step, give your final answer following 'Answer:' \n Question: {Question}</p>
<p>Implementation</p>
<p>• Sequential Encoding: Sequential method directly feeds the model with the sequence of Prefix + Text Chunks + Question.</p>
<p>• Parallel Encoding: independently encode the Prefix, each one of the Text Chunks, and Question, and then concatenate the KV cache together.The positional encoding setup follows the implementation used in the corresponding papers.</p>
<p>• Graph-KV: independently encode Prefix and Question, while inject the structural inductive biases as introduced in Section.4.1.</p>
<p>A.1.2Implementation Details for ARXIV-QA</p>
<p>Dataset Curation</p>
<p>We initially sample 100 central papers along with their references from the OGBN-ARXIV [17] citation network.Using the Arxiv API || , we download the PDF files for each paper and convert them to full text using the fitz library ** .During data cleaning, we ensure that the correct papers are downloaded and that each contains at least three valid references.For each reference, if it appears in a standalone sentence in the central paper-indicating that the central paper uses at least one sentence to compare or discuss the reference-we manually design a corresponding question.Through this process, 60 central papers with associated questions are selected to form the ARXIV-QA task, which is publicly available † † .</p>
<p>Distractors When adding distractors, we also employ randoms seeds 42 − 44 to randomly sample distractors paper citation ego-graphs, but we do not observe different outputs across the methods with different seeds.</p>
<p>Prompt: For ARXIV-QA, the input prompt can be divided into 3 parts, namely Prefix, Paper texts, Question:</p>
<p>• Prefix: To ensure fair comparison, all the methods adopt the same prefix as follows:</p>
<p>You are an intelligent AI assistant.You will first read the related works of a paper, then you will read the paper.Then answer the question.</p>
<p>• Paper texts: • Question: To ensure fair comparison, all the attention methods adopt the same question as follows:</p>
<p>Question: {Question}</p>
<p>Implementation</p>
<p>• Sequential Encoding: directly input the sequence of Prefix + Paper Texts + Question.When distractors are included, the Paper Texts are repeated accordingly, while ensuring that each citation ego-graph remains intact, as illustrated above.</p>
<p>• Parallel Encoding: independently encode the Prefix, the full text of each reference and each paper, Question, and then concatenate the KV cache together.The positional encoding setup follows the implementation used in the corresponding paper.</p>
<p>|| https://info.arxiv.org/help/api/index.html ** https://pymupdf.readthedocs.io/en/latest/tutorial.html † † See the zip file</p>
<p>• Graph-KV: independently encode Prefix and Question, while inject the structural inductive biases following the method introduced in Section.4.2.</p>
<p>Demo Example Below we display two examples of question of ARXIV-QA, and the corresponding answers generated by different attention methods.Full questions and ground truth answers could be found in ‡ ‡ .</p>
<p>Demo Example #1</p>
<p>Question: In paper 'xLED: Covert Data Exfiltration from Air-Gapped Networks via Router LEDs', the authors describe a method from another paper that introduces a type of malware aimed bridging the air-gap between computers and a nearby mobile phone by exploiting FM radio signals emanating from the video card.You've also read this paper, please provide the full name of the paper.</p>
<p>Ground Truth: Bridging the Air-Gap between Isolated Networks and Mobile Phones using Radio Frequencies Without Distractor 8B-SFT+SEQ: The paper that introduces a type of malware aimed bridging the air-gap between computers and a nearby mobile phone by exploiting FM radio signals emanating from the video card is titled AirHopper: Bridging the Air-Gap between Isolated Networks and Mobile Phones using Radio Frequencies.8B-RAG+SEQ: The paper that introduces a type of malware aimed bridging the air-gap between computers and a nearby mobile phone by exploiting FM radio signals emanating from the video card is titled AirHopper: Bridging the Air-Gap between Isolated Networks and Mobile Phones using Radio Frequencies 8B-Block-FT+SEQ: The paper that introduces a type of malware aimed at bridging the air-gap between computers and a nearby mobile phone by exploiting FM radio signals emanating from the video card is titled AirHopper: Bridging the Air-Gap between Isolated Networks and Mobile Phones using Radio Frequencies 8B-Block-FT+Block-RAG: The paper that introduces a type of malware aimed at bridging the air-gap between computers and a nearby mobile phone by exploiting FM radio signals emanating from the video card is titled COTTONMOUTH: An Asymmetric Subtle Software Attack by Mordechai Guri, Gabi Kedma, Assaf Kachlon, and Yuval Elovici.8B-Block-FT+Graph-KV: The paper that introduces a type of malware aimed bridging the air-gap between computers and a nearby mobile phone by exploiting FM radio signals emanating from the video card is titled: AirHopper: Bridging the Air-Gap between Isolated Networks and Mobile Phones using Radio Frequencies.Authors: Mordechai Guri, Gabi Kedma, Assaf Kachlon, Yuval Elovici.</p>
<p>With #1 Distractor 8B-SFT+SEQ-First: The full name of the paper is xLED: Covert Data Exfiltration from Air-Gapped Networks via Router LEDs.8B-SFT+SEQ-Last: AirHopper: Bridging the Air-Gap between Isolated Networks and Mobile Phones using Radio Frequencies 8B-RAG+SEQ-First: According to the provided information, the paper that introduces the malware that exploits FM radio signals emanating from the video card to bridge the air-gap between computers and a nearby mobile phone is xLED: Covert Data Exfiltration from Air-Gapped Networks via Router LEDs 8B-RAG+SEQ-Last: According to the provided information, the paper you're referring to is AirHopper: Bridging the Air-Gap between Isolated Networks and Mobile Phones using Radio Frequencies by Mordechai Guri, Yosef Solewicz, Andrey Daidakulov, and Yuval Elovici.8B-Block-FT+SEQ-First: The paper you are referring to is titled Software-Defined Radio and the RF Fingerprints of Computers by Mordechai Guri, Yuval Elovici, and Boris Zadov.8B-Block-FT+SEQ-Last: The paper you are referring to is ÄirHopper: Bridging the Air-Gap between Isolated Networks and Mobile Phones using Radio Frequencies by Mordechai Guri, K. Gabi, K. Assaf, and Yuval Elovici.8B-Block-FT+Block-RAG-First: <end of text> 8B-Block-FT+Block-RAG-Last: The authors of the 5.The 5. The 5. and the 5.The 5. The 5. and the 5.The 5. The 5. The 5. The 5. The 8B-Block-FT+Graph-KV: The paper that introduces a type of malware aimed bridging the air-gap between computers and a nearby mobile phone by exploiting FM radio signals emanating from the video card is titled AirHopper: Bridging the Air-Gap between Isolated Networks and Mobile Phones using Radio Frequencies by Mordechai Guri, Gabi Kedma, Assaf Kachlon, and Yuval Elovici.</p>
<p>With #2 Distractors 8B-SFT+SEQ-First: The full name of the paper is Learning to Bridge the Air-Gap between Isolated Networks and Mobile Devices using RF Modulation from the Video Card 8B-SFT+SEQ-Last: xLED: Covert Data Exfiltration from Air-Gapped Networks via Router LEDs 8B-RAG+SEQ-First: <end of text> 8B-RAG+SEQ-Last: <end of text> 8B-Block-FT+SEQ-First: <end of text> 8B-Block-FT+SEQ-Last: <end of text> 8B-Block-FT+Block-RAG-First: Null 8B-Block-FT+Block-RAG-Last: Null 8B-Block-FT+Graph-KV: The paper that introduces a type of malware aimed bridging the air-gap between computers and a nearby mobile phone by exploiting FM radio signals emanating from the video card is: AirHopper: Bridging the air-gap between isolated networks and mobile phones using radio frequencies by Mordechai Guri, Gabi Kedma, Assaf Kachlon, and Yuval Elovici.</p>
<p>Demo Example #2</p>
<p>Question: In paper 'Fast Multi-frame Stereo Scene Flow with Motion Segmentation', they directly adopt an edge detector, which originates from another paper that you've read.Provide the full name of that paper.</p>
<p>A.1.3 Implementation Details for Paper Topic Classification</p>
<p>Dataset Curation The dataset is originally from Cora [68] and Pubmed [69], we adopt the test set split adopted in [57].For each paper, the input text consists of the title and abstract.</p>
<p>Prompt: For Paper Topic Classification, the input prompt can be divided into 3 parts, namely Prefix, Paper Title &amp; Abstracts, Question:</p>
<p>• Prefix: To ensure fair comparison, all the methods adopt the same prefix as follows:</p>
<p>You are an intelligent AI assistant.You will first read a list of titles or abstracts of papers cited by a central paper.Then, you will read the title or abstract of the central paper itself.Finally, you will answer a question related to the central paper:</p>
<p>Implementation</p>
<p>For sequential encoding and Block-RAG [24], we report the average performance with seeds 42 − 44 to randomly shuffle the placement order of sequence.</p>
<p>• Sequential Encoding: directly input the sequence of Prefix + Paper Title &amp; Abstract + Question.When distractors are included, the Paper Texts are repeated accordingly, while ensuring that each citation ego-graph remains intact.</p>
<p>Figure 1 :
1
Figure 1: When processing data with inherent structure (bottom-left), modern LLMs encounter three challenges due to serialized input reading (top row): (1) positional bias, where different serialization orders lead to varied outputs [19];(2) quadratic computational complexity from full attention between all document pairs; and (3) rapid context window consumption, as position indices quickly exceed limits.The bottom-right subfigure illustrates Graph-KV.Text chunks are independently encoded into KV caches, whereGraph-KV arranges the text chunk of each target text after the KV of their source texts to update their respective KV caches.Notably, all source texts share same positional encoding (PE) range, while all target texts share another, with their position index immediately following that of the source nodes.This approach reduces the PE and context window usage.At query time, the query attends to both the source chunks and the target chunks to perform decoding.</p>
<p>Figure 2 :
2
Figure 2: PE-sharing mechanism in Graph-KV.As shown on the right side, source docs share one PE range, while targets share another.Attending Doc.1 to the KVs of its sources (Doc.2 and Doc.3), is functionally equivalent to the left side: reading Doc.2 followed by Doc.1, and Doc.3 followed by Doc.1, then merging the resulting representations of Doc.1.</p>
<ol>
<li>1 6 Bipartite
16
Task 1: Retrieval Augmented Generation (RAG) Graph Modeling in Graph-KV Top-m Chunks Top-m</li>
</ol>
<p>Figure 3 :
3
Figure 3: Graph-KV modeling for RAG.</p>
<p>Paragraph A: Versus (Versace) is the diffusion line of Italian . . ., a gift by the founder Gianni Versace to his sister, Donatella Versace. . . .Paragraph B: Gianni Versace . . .Versace was shot and killed outside . . .Query: Why did the founder of Versus die?Paragraph A: FAQ: Frequently Asked Questions is a feature-length dystopian movie, written and directed by Carlos Atanes and released ... Paragraph B: The Big Money . . .directed by John Paddy Carstairs . . .Paragraph C: Carlos Atanes is a Spanish film director . . .Paragraph D: John Paddy Carstairs was a prolific British film director . . .Query: Are both director of film FAQ: Frequently Asked Questions and director of film The Big Money from the same country?Chunks from Long Document Query: Please try to deduce the true story based on the evidence currently known.Who killed Evelyn Hardcastle in your deduction?who got the first nobel prize in physics?Paragraph B Paragraph A: receive a diploma, a medal and a document ... It is one of the five Nobel Prizes.Paragraph B: The first Nobel Prize in Physics was awarded to physicist Wilhelm Röntgen in... Query: Were Scott Derrickson and Ed Wood of the same nationality?Paragraph A: Scott Derrickson (born July 16, 1966) is an American director... Paragraph B: Edward Davis Wood Jr. (October 10, 1924 -December 10, 1978) was an American filmmaker...</p>
<p>Figure 4 :
4
Figure4: The reasoning settings in RAG tasks .Direct inference task requires identifying evidence chunks (from NarrativeQA[26]).</p>
<p>Figure 5 :
5
Figure 5: An example from the ARXIV-QA task.One needs first locate the central paper's introduction of the low row-weight generator matrix, and then compare the described methods with the content across all provided references (e.g., Theorems 1 and 3 in the ground-truth reference paper) to arrive at the correct answer.</p>
<ol>
<li>2
2
Task 2: ARXIV-QA -Multi-Hop Reasoning over Citation Network with Full Texts</li>
</ol>
<p>Figure 7 :
7
Figure 7: Average input sequence length (equivalent to position index range in sequential encoding) on ARXIV-QA with 0, 1, 2 distractors, compared to the position index range used in Graph-KV.</p>
<p>Figure 6 :
6
Figure 6: QA accuracy when querying with 0, 1, 2 distractors (up to down) on the 60 questions from ARXIV-QA.The number of correct answers is provided after ✓. 'SEQ' refers to sequential encoding.Green entry means correct answer and Red refers to wrong.When querying with distractors, 'Last' and 'First' refers to the position where the paper along with its references that are contain the answer are positioned (at the beginning or end of the sequence).Graph-KV for Structure-Aware Modeling in ARXIV-QA.Understanding technical details in a paper often involves reviewing its references, particularly when methods in the references are adapted to solving the problem in the current work.Graph-KV forms target-source pairs connecting the central paper with all its references.When distractors are included and text length surpasses 128k tokens, the shared PE strategy is vital for the LLM to properly digest the full context.</p>
<ol>
<li>4 Figure 8 :
48
Figure 8: Left: Memory left w.r.t.num of nodes to encode.Right: TTFT w.r.t.num words per node.</li>
</ol>
<p>{Full</p>
<p>Text of Reference a}, {Full Text of Reference b},..., {Full Text of Reference k} \n\n Now please read the paper: {Full Text of Central Paper #1} (if with distractors:) (Distractor #1){Full Text of Reference l}, {Full Text of Reference m},..., {Full Text of Reference s} \n\n Now please read the paper: {Full Text of Central Paper #2} (Distractor #2){Full Text of Reference t}, {Full Text of Reference u},..., {Full Text of Reference y} \n\n Now please read the paper: {Full Text of Central Paper #3}</p>
<p>Ground Truth: Fast Edge Detection Using Structured Forests Without Distractor 8B-SFT+SEQ: The edge detector adopted in the paper 'Fast Multi-frame Stereo Scene Flow with Motion Segmentation' originates from the paper titled Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.8B-RAG+SEQ: The paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation by Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato directly adopts an edge detector from the paper Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.8B-Block-FT+SEQ: According to paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation by Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato, they directly adopt an edge detector from the paper Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.8B-Block-FT+Block-RAG: The edge detector adopted in the paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation is the Census Transform, which is a robust and efficient edge detection method.The Census Transform is a non-parametric image matching technique that was first introduced by Zabih and Woodfill in their paper Non-parametric Local Transforms for Computing Visual Correspondence (Zabih and Woodfill, 1994).8B-Block-FT+Graph-KV: The paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation directly adopts an edge detector that originates from the paper Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.With #1 Distractor 8B-SFT+SEQ-First: The edge detector adopted in the paper 'Fast Multi-frame Stereo Scene Flow with Motion Segmentation' is Dollár and Zitnick's Fast Edge Detection Using Structured Forests (DOI: 10.1109/T-PAMI.2015.2467470).8B-SFT+SEQ-Last: The edge detector adopted in the paper 'Fast Multi-frame Stereo Scene Flow with Motion Segmentation' is the Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.8B-RAG+SEQ-First: According to the supplementary material of the paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation by Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato, they directly adopt an edge detector from the paper Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.8B-RAG+SEQ-Last: According to the paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation by Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato, the edge detector they adopted is from the paper Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.8B-Block-FT+SEQ-First: According to the supplementary material of the paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation by Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato, the edge detector used in their work originates from the paper Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.8B-Block-FT+SEQ-Last: The paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation directly adopts an edge detector from the paper Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.8B-Block-FT+Block-RAG-First: The paper that is directly adopted as an edge detector in 'Fast Multi-frame Stereo Scene Flow with Motion Segmentation' is Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.8B-Block-FT+Block-RAG-Last: The paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation by Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato does not directly adopt an edge detector from another paper.Instead, they use a fast edge detector proposed by Dollár and Zitnick (2017) in their work A Continuous Optimization Approach for Efficient and Accurate Scene Flow 8B-Block-FT+Graph-KV: The paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation directly adopts an edge detector from the paper Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.With #2 Distractors 8B-SFT+SEQ-First: The paper that the edge detector originates from is Edge-preserving Multi-scale Video Deblurring by Hu et al. 8B-SFT+SEQ-Last: The edge detector used in the paper 'Fast Multi-frame Stereo Scene Flow with Motion Segmentation' is the Fast Edge Detection Using Structured Forests.8B-RAG+SEQ-First: The paper that is referred to in the paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation is Edge-preserving Joint Image Segmentation and Estimation with Contextual and Spatial Priors by J. Shen, X. Yang, and M. Chen.8B-RAG+SEQ-Last: According to the provided information, the paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation adopts an edge detector from another paper.The edge detector mentioned in the paper is the Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.8B-Block-FT+SEQ-First: The paper that the authors of Fast Multi-frame Stereo Scene Flow with Motion Segmentation directly adopted an edge detector from is Edge-preserving Multi-modal Deblurring.8B-Block-FT+SEQ-Last: The edge detector mentioned in the paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation is the one proposed in the following paper: Fast Edge Detection Using Structured Forests by Piotr Dollár and C. Lawrence Zitnick.8B-Block-FT+Block-RAG-First: Null 8B-Block-FT+Block-RAG-Last: Null 8B-Block-FT+Graph-KV: The edge detector used in the paper Fast Multi-frame Stereo Scene Flow with Motion Segmentation is the Simplex detector, which is described in the paper: Piotr Dollár and C. Lawrence Zitnick.Fast Edge Detection Using Structured Forests.IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2015.</p>
<p>•</p>
<p>Paper Title &amp; Abstracts: {Title &amp; Abstract of Reference 1}, {Title &amp; Abstract of Reference 2},..., {Title &amp; Abstract of Reference k} \n\n They are all cited by the following central paper: : {Title &amp; Abstract of Central Paper}• Question: To ensure fair comparison, all the attention methods adopt the same question as follows:Classify the central paper into one of the following categories: {Classes}.Provide your answer following 'Answer:'</p>
<p>Table 1 :
1
Comparison among existing approaches."Long-Context Friendly" refers to avoiding of rapid context window consumption as the number of input text chunks increases."Free from positional bias" means model predictions remain stable irrespective of the input chunks' placement order.</p>
<p>Table 2 :
2
Performance on Multihop-RAG</p>
<p>Table 4 :
4
Performance
AttentionCoraPubmed8B-SFTSequential66.66±0.62 80.64±0.398B-SFTPCW68.6376.958B-SFTAPE66.9277.018B-RAGSequential70.35±0.17 82.06±0.168B-RAGPCW66.0576.528B-RAGAPE68.4676.498B-Block Query-Only57.3883.608B-BlockSequential67.09±0.17 79.79±0.168B-Block Block-RAG 69.55±0.30 83.24±1.168B-BlockGraph-KV71.0384.61
on paper topic classification.Sequential encoding and Block-RAG produce varied answer due to different placement order of references.'Query-Only' means only providing the central paper.</p>
<p>‡ ‡ zip file of supplementary material
A AppendixA.1 Implementation DetailsHardware and PlatformFor all the experiments involved in this study, the code is implemented using PyTorch[71], the HuggingFace Transformers library[72], and FlashAttention-2[73].As to hardware, for the task ARXIV-QA, the parallel text encoding baselines (Block-RAG, PCW, APE) and Graph-KV run on 4 NVIDIA A100 Tensor Core GPUs, while the sequential encoding baseline runs on 8 NVIDIA A100 Tensor Core GPUs, as it requires higher memory.For the other tasks, all the methods run on with NVIDIA RTX 6000 Ada GPUs.Model Weights For all the experiments, we adopt the open-source weight of llama-3.1-8B-sft<em> , llama-3.1-8B-TAG† and llama-3.1-8B-Block-FT‡ release by[24].We do not further fine-tune the llama-3.1-8Bwith Graph-KV due to limited computational resources, although we believe that doing so could further improve performance on the experiments.A.1.1 Implementation Details for RAG Data Process and Evaluation For 2Wiki[10], NarrativeQA[26], TriviaQA[25], MorehopQA[12]and HotpotQA[11], the data processing (the process to retrieve text chunk and the evaluation pipeline) strictly follows Block-RAG [24]  § .For Mulihop-RAG[13], the data processing and evaluation follows the original implementation ¶ .Across all benchmarks, the top 10 text chunks retrieved based on similarity scores are included in the input prompt in ascending order with respect to the scores[24].Following[24,32,31], we use accuracy as the metric, and evaluate whether the correct answer appears in the output.For all methods, the output is constrained to a maximum of 256 tokens.Prompt: For RAG tasks, the entire prompt input is divided into 3 parts, namely Prefix, Text Chunks, and Question, with each formatted as follows:• Prefix: To ensure fair comparison, all the methods adopt the same prefix as follows:You are an intelligent AI assistant.Please answer questions based on the user\'s instructions.Below are some reference documents that may help you in answering the user\'s question.• Text Chunks: The format for each text chunk (10 chunks in total for each question) is as follows: • Question: All the methods adopt the same question format as follows:For 2Wiki[10], HotpotQA[11], NarrativeQA[26]and TriviaQA[25], the question prompt follows those used in[24]:Please write a high-quality answer for the given question using only the provided search documents (some of which might be irrelevant).\n Question: {Question} For MultiHop-QA[13], we adopt the prompt from the original implementation, which is as follows:Please write a high-quality answer for the given question using only the provided search documents.The answer to the question is a word or entity.If the provided information is insufficient to answer the question, respond 'Insufficient Information'.Please finally give your answer started with: 'The answer is:'.\n Question: {Question}For MoreHop-QA[12], the prompt is also from the original implementation, which is:</em> https://huggingface.co/ldsjmdy/Tulu3-SFT † https://huggingface.co/ldsjmdy/Tulu3-RAG ‡ https://huggingface.co/ldsjmdy/Tulu3-Block-FT § https://github.com/TemporaryLoRA/Block-Attention¶ https://github.com/yixuantt/MultiHop-RAG• Parallel Encoding The parallel encoding baselines independently encode the Prefix, each paper title &amp; abstract, Question, and then concatenate the KV cache together.The positional encoding setup follows the implementation used in the corresponding paper.• Graph-KV: independently encode Prefix and Question, while inject the structural inductive biases with Paper Title &amp; Abstract following the modeling as introduced in Section.4.3.A.1.4 Implementation Details for Stress TestAs introduced in the main text, we employ an Nvidia RTX6000 GPU (48GB) with AMD EPYC 7763 64-core processor for stress test.Specifically, for attention implementation, all the methods use FlashAttention2[73].The raw text is extracted from the first test sample of the Cora[68]dataset.To meet the specified input length requirements-500 and 1000 words for the memory test, and 100, 200, 400, and 800 words for the generation latency evaluation-we either repeat or truncate the original text accordingly.• Memory test: To test the memory usage of each method, we gradually increase the number of neighbors of synthetic star graph, and use torch.cuda.reset_peak_memory_stats()and torch.cuda.max_memory_allocated()functions to monitor the peak GPU memory usage.• Time-To-First-Token (TTFT): Similar to other parallel encoding baselines, Graph-KV also benefits from KV-cache pre-filling.With pre-filled KV-cache, Graph-KV achieves significantly lower TTFT (Time-To-First-Token) compared to sequential encoding.To measure TTFT, we use the time.time()function to record the elapsed time.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in neural information processing systems. 202033</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, Haofen Wang, arXiv:2312.1099720232arXiv preprint</p>
<p>Retrieval-augmented generation for ai-generated content: A survey. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui, arXiv:2402.194732024arXiv preprint</p>
<p>A survey on retrieval-augmented text generation. Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu, arXiv:2202.011102022arXiv preprint</p>
<p>Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, arXiv:2011.010602020arXiv preprint</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.09600Hotpotqa: A dataset for diverse, explainable multi-hop question answering. 2018arXiv preprint</p>
<p>Julian Schnitzler, Xanh Ho, Jiahao Huang, Florian Boudin, Saku Sugawara, Akiko Aizawa, arXiv:2406.13397Morehopqa: More than multi-hop reasoning. 2024arXiv preprint</p>
<p>Yixuan Tang, Yi Yang, arXiv:2401.15391Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. 2024arXiv preprint</p>
<p>On the emergence of position bias in transformers. Xinyi Wu, Yifei Wang, Stefanie Jegelka, Ali Jadbabaie, arXiv:2502.019512025arXiv preprint</p>
<p>Xiaobo Guo, Soroush Vosoughi, arXiv:2406.15981Serial position effects of large language models. 2024arXiv preprint</p>
<p>Found in the middle: How language models use long contexts better via plug-and-play positional encoding. Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, Zhangyang Wang, arXiv:2403.047972024arXiv preprint</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in neural information processing systems. 202033</p>
<p>How citation distortions create unfounded authority: analysis of a citation network. A Steven, Greenberg, Bmj. 3392009</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202336</p>
<p>Ziqi Wang, Hanlin Zhang, Xiner Li, Kuan-Hao Huang, Chi Han, Shuiwang Ji, Hao Sham M Kakade, Heng Peng, Ji, arXiv:2407.01100Eliminating position bias of language models: A mechanistic approach. 2024arXiv preprint</p>
<p>Ape: Faster and longer context-augmented generation via adaptive parallel encoding. Xinyu Yang, Tianqi Chen, Beidi Chen, arXiv:2502.054312025arXiv preprint</p>
<p>Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, arXiv:2212.10947Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows for large language models. 2022arXiv preprint</p>
<p>Prompt cache: Modular attention reuse for low-latency inference. In Gim, Guojun Chen, Seung-Seob Lee, Nikhil Sarda, Anurag Khandelwal, Lin Zhong, Proceedings of Machine Learning and Systems. Machine Learning and Systems20246</p>
<p>Block-attention for efficient prefilling. Dongyang Ma, Yan Wang, Tian Lan, The Thirteenth International Conference on Learning Representations. 2024</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, arXiv:1705.035512017arXiv preprint</p>
<p>The narrativeqa reading comprehension challenge. Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, Edward Grefenstette, Transactions of the Association for Computational Linguistics. 62018</p>
<p>Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, arXiv:2412.15204Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. 2024arXiv preprint</p>
<p>Yiwei Wang, Yujun Cai, Muhao Chen, Yuxuan Liang, Bryan Hooi, arXiv:2310.13206Primacy effect of chatgpt. 2023arXiv preprint</p>
<p>Judgelm: Fine-tuned large language models are scalable judges. Lianghui Zhu, Xinggang Wang, Xinlong Wang, arXiv:2310.176312023arXiv preprint</p>
<p>Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms. Lin Shi, Chiyu Ma, Wenhua Liang, Weicheng Ma, Soroush Vosoughi, arXiv:2406.077912024arXiv preprint</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, European Conference on Information Retrieval. Springer2024</p>
<p>Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023arXiv preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, Yunfeng Liu, Neurocomputing. 5681270632024</p>
<p>The impact of positional encoding on length generalization in transformers. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, Siva Reddy, Advances in Neural Information Processing Systems. 202336</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. Ofir Press, Noah A Smith, Mike Lewis, arXiv:2108.124092021arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Length generalization of causal transformers without position encoding. Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang, Xiaoling Wang, arXiv:2404.122242024arXiv preprint</p>
<p>Transformer language models without positional encodings still learn positional information. Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Attention sorting combats recency bias in long context language models. Alexander Peysakhovich, Adam Lerer, arXiv:2310.014272023arXiv preprint</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International conference on machine learning. PMLR2021</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.087862021arXiv preprint</p>
<p>Leveraging passage retrieval with generative models for open domain question answering. Gautier Izacard, Edouard Grave, arXiv:2007.012822020arXiv preprint</p>
<p>Accelerating inference of retrieval-augmented generation via sparse context selection. Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, arXiv:2405.161782024arXiv preprint</p>
<p>Long-context language modeling with parallel context encoding. Howard Yen, 2024Princeton UniversityMaster's thesis</p>
<p>Sglang: Efficient execution of structured language model programs. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, 2024</p>
<p>Cacheblend: Fast large language model serving for rag with cached knowledge fusion. Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang, Proceedings of the Twentieth European Conference on Computer Systems. the Twentieth European Conference on Computer Systems2025</p>
<p>Graphwiz: An instruction-following language model for graph problems. Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li, arXiv:2402.160292024arXiv preprint</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, arXiv:2310.045602023arXiv preprint</p>
<p>Let your graph do the talking: Encoding structured data for llms. Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow, arXiv:2402.058622024arXiv preprint</p>
<p>Ruler: What's the real context size of your long-context language models?. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, Boris Ginsburg, arXiv:2404.066542024arXiv preprint</p>
<p>Model generalization on text attribute graphs: Principles with large language models. Haoyu Wang, Shikun Liu, Rongzhe Wei, Pan Li, arXiv:2502.118362025arXiv preprint</p>
<p>Gl-fusion: Rethinking the combination of graph neural network and large language model. Haotong Yang, Xiyuan Wang, Qian Tao, Shuxian Hu, Zhouchen Lin, Muhan Zhang, arXiv:2412.068492024arXiv preprint</p>
<p>Gofa: A generative one-for-all model for joint graph language modeling. Lecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang, arXiv:2407.097092024arXiv preprint</p>
<p>Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, Zhangyang Wang, arXiv:2402.08170Llaga: Large language and graph assistant. 2024arXiv preprint</p>
<p>Llms as zero-shot graph learners: Alignment of gnn representations with llm token embeddings. Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu, Advances in Neural Information Processing Systems. 202437</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Text-space graph foundation models: Comprehensive benchmarks and new insights. Zhikai Chen, Haitao Mao, Jingzhe Liu, Yu Song, Bingheng Li, Wei Jin, Bahare Fatemi, Anton Tsitsulin, Bryan Perozzi, Hui Liu, arXiv:2406.107272024arXiv preprint</p>
<p>Glbench: A comprehensive benchmark for graph with large language models. Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Victor W Chan, Jia Li, Advances in Neural Information Processing Systems. 202437</p>
<p>Zerog: Investigating cross-dataset zero-shot transferability in graphs. Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Graphclip: Enhancing transferability in graph foundation models for text-attributed graphs. Yun Zhu, Haizhou Shi, Xiaotang Wang, Yongchao Liu, Yaoke Wang, Boci Peng, Chuntao Hong, Siliang Tang, Proceedings of the ACM on Web Conference 2025. the ACM on Web Conference 20252025</p>
<p>LongBench: A bilingual, multitask benchmark for long context understanding. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Babilong: Testing the limits of llms with long context reasoning-in-a-haystack. Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev, Advances in Neural Information Processing Systems. 2024</p>
<p>Lv-eval: A balanced long-context benchmark with 5 length levels up to 256k. Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, Yu Wang, arXiv:2402.051362024arXiv preprint</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Efficient streaming language models with attention sinks. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis, arXiv:2309.174532023arXiv preprint</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, arXiv:2411.15124Pushing frontiers in open language model post-training. 20243arXiv preprint</p>
<p>Automating the construction of internet portals with machine learning. Andrew Kachites Mccallum, Kamal Nigam, Jason Rennie, Kristie Seymore, Information Retrieval. 32000</p>
<p>Collective classification in network data. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, Tina Eliassi-Rad, AI magazine. 2932008</p>
<p>Node feature extraction by self-supervised multi-scale neighborhood prediction. Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Jiong Zhang, Olgica Milenkovic, Inderjit S Dhillon, arXiv:2111.000642021arXiv preprint</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Paszke, arXiv:1912.017032019arXiv preprint</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.037712019arXiv preprint</p>
<p>Flashattention-2: Faster attention with better parallelism and work partitioning. Tri Dao, arXiv:2307.086912023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
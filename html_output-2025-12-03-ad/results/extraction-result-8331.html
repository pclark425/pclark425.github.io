<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8331 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8331</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8331</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-268889526</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.03622v2.pdf" target="_blank">Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind’s Eye , enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought ( VoT ) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate mental images to facilitate spatial reasoning resembles the mind’s eye process, suggesting its potential viability in MLLMs. Please find the dataset and codes in our project page.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8331.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8331.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (VoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with Visualization-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 LLM evaluated with the paper's proposed Visualization-of-Thought (VoT) zero-shot prompt that asks the model to 'Visualize the state after each reasoning step' to elicit spatial reasoning and visual state tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large language model (OpenAI). In experiments accessed via Azure OpenAI API (model version 1106-preview) with greedy decoding (temperature 0). Evaluated in zero-shot setting with VoT prompting to produce interleaved textual visualizations and reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual Navigation (2D grid)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based navigation task requiring multi-hop spatial planning on synthetic 2D maps (up/down/left/right moves, obstacles, start/destination).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Input: text description plus ASCII/emoji grid; tasks: route planning (generate full direction sequence) and next-step prediction; prompting variants compared: 'Let's think step by step' (CoT), 'Don't use visualization. Let's think step by step' (w/o Viz), and 'Visualize the state after each reasoning step.' (VoT). Zero-shot; outputs include intermediate textual reasoning steps interleaved with ASCII-art/grid visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Visualization-of-Thought (VoT) — interleaved generation of reasoning steps and textual/ASCII visualizations representing internal state (visuospatial sketchpad). Also chain-of-thought (CoT) baselines, and ablation 'w/o Viz'. Visual state tracking and self-refinement behaviors observed (models revise visualization/answers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As reported in Table 1, GPT-4 VoT achieved higher accuracy than GPT-4 CoT and GPT-4 w/o Viz across visual navigation metrics (next-step prediction ≈ 40.77% per table row); route-planning metrics (reported completing rate and success rate; success-rate reported ≈ 55.28% in table row); improvements are stated qualitatively and quantitatively vs baselines. VoT achieved notable gains in downstream tasks (see paper Table 1 and Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High partial/complete visual state tracking rates under VoT; qualitative examples where GPT-4 generates accurate grid visualizations and uses them to plan routes; extraction and evaluation of final visualizations (compliance and accuracy metrics reported in Table 2: Visual Navigation compliance 51.14%, visualization accuracy 26.48%, and when visualization is correct, spatial-understanding accuracy 65.16%). The paper reports improved next-step and route-planning performance under VoT and presents case studies (self-refine, diverse visualization formats).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against GPT-4 CoT (standard 'Let's think step by step'), GPT-4 w/o Viz (explicitly disabled visualization), and GPT-4V CoT (GPT-4 Vision multimodal model using image inputs). Paper reports GPT-4 VoT outperforms GPT-4 CoT, GPT-4 w/o Viz, and GPT-4V CoT on the tested visual navigation metrics (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Visualizations are often inaccurate: visualization accuracy is low (~24-26% in Table 2). Inconsistencies between text reasoning and generated visualizations observed. VoT is sensitive to prompt wording (removing the word 'reasoning' caused visualizations to be produced only after wrong answers). Route planning remains challenging (completing rates can be low) and overall performance degrades with task difficulty. VoT can underperform in tasks where algebraic/logical solutions suffice (ring navigation example where GPT-4 CoT outperformed VoT).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8331.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8331.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (CoT) / GPT-4 w/o Viz</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with Chain-of-Thought and GPT-4 with visualization disabled</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated under standard chain-of-thought prompting ('Let's think step by step') and an ablation where visualization is explicitly disabled ('Don't use visualization. Let's think step by step').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GPT-4 as above, evaluated via Azure OpenAI API (1106-preview), zero-shot CoT and a 'w/o Viz' prompt variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Natural-Language Navigation; Visual Navigation; Visual Tiling (polyomino tiling)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Natural-language navigation over graph/grid; grid-based visual navigation; polyomino tiling (2D shape arrangement).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot CoT prompt 'Let's think step by step' (CoT). 'w/o Viz' prompt explicitly disables visualization then asks to think step-by-step. Same dataset and metrics as VoT experiments (synthetic 2D grids, polyomino tilings, natural-language maps).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Chain-of-thought (CoT) verbal reasoning without interleaved visual state generation (or explicitly suppressed). Relies on textual reasoning and implicit internal state.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1: GPT-4 CoT and GPT-4 w/o Viz show lower performance compared to GPT-4 VoT across visual tasks. Example numbers from table rows: GPT-4 CoT next-step prediction ≈ 37.02%, visual tiling ≈ 54.15%, natural-language navigation ≈ 54.00%; GPT-4 w/o Viz numbers are lower on several metrics (e.g., natural-language navigation ≈ 35.50% per table row).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Paper observes that GPT-4 CoT sometimes exhibits spontaneous visual state tracking (tracking rate non-zero), indicating latent ability to simulate spatiotemporal states even without explicit VoT prompting, but overall less grounded than VoT. CoT can outperform VoT on tasks reducible to algebraic/logical computations (ring navigation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Used as baselines to compare improvements from VoT; CoT occasionally shows partial visual tracking but VoT yields higher complete-tracking and better task performance on most spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CoT and w/o Viz produce fewer or later visualizations; are less grounded and perform worse on visual tasks. w/o Viz especially suffers in natural-language navigation per reported numbers. CoT may rely on non-spatial/logical shortcuts and can fail when genuine spatial simulation is required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8331.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8331.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Vision with Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The multimodal GPT-4 Vision (GPT-4V) model evaluated using its image input capability and CoT prompting as an additional baseline for visual tasks (visual navigation, visual tiling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V (GPT-4 Vision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal variant of GPT-4 that accepts image inputs (accessed via Azure OpenAI 'vision-preview'); used with CoT prompt 'Let's think step by step' and with corresponding image inputs rendered from the ASCII/emoji grids.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual Navigation; Visual Tiling</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based visual navigation and polyomino tiling where the model receives rendered image counterparts of the textual grids.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>For visual tasks a counterpart image is provided (images generated by rendering the textual grids). Prompt: 'Let's think step by step' (CoT). Zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard multimodal perception of the rendered grid image followed by chain-of-thought textual reasoning; does not use VoT interleaved textual visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1: GPT-4V CoT underperforms GPT-4 VoT on the tested visual tasks in this paper. Example per table row: visual navigation next-step prediction ≈ 33.36%, visual tiling ≈ 46.59% (numbers taken from paper table rows).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Has access to explicit image rendering of the grid but in experiments GPT-4V CoT showed lower performance than GPT-4 VoT; suggests that internally generated mental visualizations (VoT) can be more effective than direct image grounding in these tasks, per the paper's results and analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to GPT-4 VoT and GPT-4 CoT; GPT-4 VoT outperformed GPT-4V CoT on the paper's visual navigation and tiling benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite receiving actual images, GPT-4V CoT performed worse than VoT in experiments; authors note that VoT (mental-image tracking) outperformed this SOTA MLLM on their synthetic 2D grid tasks. Limitations may include image interpretation or inability to maintain interleaved state-tracking as effectively as VoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8331.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8331.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 turbo (VoT / CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 turbo evaluated with VoT and CoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Earlier-generation OpenAI model (GPT-3.5 turbo) tested to measure VoT effects on less powerful LLMs; accessed via Azure OpenAI API with greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller/earlier OpenAI LLM (accessed via Azure OpenAI API version 1106-preview). Evaluated under zero-shot VoT and CoT prompts to probe whether VoT benefits accrue to less-capable models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual Navigation, Visual Tiling, Natural-Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Same synthetic grid and tiling tasks as used for GPT-4 experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot CoT and VoT prompts identical to GPT-4 experiments; greedy decoding (temperature 0).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Attempts to produce interleaved visualizations under VoT; compared to CoT baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports that less powerful models often rely on random guessing; GPT-3.5 CoT 'random guessing in nearly half of the instances' in route-planning, and GPT-3.5 VoT reduces random guesses. Exact numeric accuracies for GPT-3.5 are in Table 3 (paper), with modest improvements from VoT but lower absolute performance than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>VoT reduced random guesses for GPT-3.5 and produced some improvement at lower difficulty levels, but the model's visual state tracking and spatial reasoning are inconsistent and brittle compared to GPT-4. Paper notes unpredictable performance trajectories at higher difficulty for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared within the paper to GPT-4 and LLAMA3 models; smaller models show scaling behavior — gains with size and VoT more effective in larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High reliance on random guessing for complex tasks; token-exhausting speculative outputs in route-planning; VoT benefits are limited and variable — smaller models often fail to sustain reliable reasoning in multi-hop spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8331.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8331.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B / Llama3-70B (VoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 family (8B and 70B instruct-tuned) evaluated with VoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source Llama3 instruct models of different sizes tested to examine scaling effects of VoT prompting on non-OpenAI LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA3-8B-Instruct; LLAMA3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned Llama 3 family (8B and 70B parameter sizes) evaluated zero-shot with VoT and CoT prompts in the same spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B and 70B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual Navigation; Visual Tiling; Natural-Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based navigation and polyomino tiling tasks requiring spatial manipulation and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot VoT / CoT prompting identical to other experiments; models decode greedily (temperature 0).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>VoT prompting to produce textual visualizations of internal state interleaved with reasoning steps; scaling analysis across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports that performance improves with model size within same family; LLAMA3-70B VoT significantly outperforms LLAMA3 CoT baselines on most tasks (Table 3 and Table 6 referenced). For visual tiling LLAMA3-70B VoT aligns closely with GPT-4 results (paper statement). Exact per-task numeric values are presented in the paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>LLAMA3-70B shows consistent gains with VoT, indicating the VoT paradigm benefits scale with model capacity. However, smaller LLAMA3 model (8B) shows unreliable behavior and random-guess tendencies at higher difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Within-family comparisons (8B vs 70B) show scaling advantage; LLAMA3-70B with VoT approaches GPT-4-level performance on some tasks (visual tiling) per paper statements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less capable Llama3 models still rely on guessing; VoT effectiveness depends on model capacity and pretraining exposure to code/ascii-art. Visual state tracking and visualization accuracy remain imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8331.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8331.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual Tiling (polyomino tiling) task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Polyomino visual tiling (extended tetromino tiling) task used in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic polyomino tiling puzzle adapted to evaluate LLM spatial shape reasoning: select correct variant/orientation of a masked polyomino to fill a rectangle without overlaps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated across GPT-4, GPT-4V, GPT-3.5, LLAMA3 models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Task dataset: 5x4 rectangle with five tetromino pieces (two I, two T, one L); masked 2–3 pieces to form QA instances (total 796 QA instances). Rendered as text/emoji grids and as images for MLLM experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual Tiling (polyomino tiling)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based combinatorial tiling puzzle requiring 2D shape understanding, rotations/reflections, and spatial arrangement reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Input: rectangle rendering (emoji/text) and variants for each polyomino; query asks which variant of a queried polyomino fits. Prompt variants: VoT ('Visualize the state after each reasoning step') vs CoT. Zero-shot. Evaluation by substring/exact matching of answers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>VoT interleaved visualizations where LLMs place candidate pieces into ASCII grids, check overlaps/fit (compliance), and self-refine; algorithmic data generation used exact cover / dancing links to ensure solvable configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-4 VoT visual-tiling accuracy reported ~63.94% (per Table 1 row), GPT-4 CoT ~54.15%, GPT-4V CoT ~49.62%, GPT-4 w/o Viz ~46.98% (table-row values). Table 2 reports spatial visualization compliance ~52.01% and visualization accuracy ~24.25% for visual tiling; when visualization is correct, decision accuracy ~77.20%.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Quantitative compliance/accuracy metrics (Table 2) and qualitative cases: self-refine behaviors where model detects spatial hallucination, re-evaluates placements, and corrects final answer; examples of inconsistent visualization vs language also analyzed. VoT increases complete/partial visual-tracking rates which correlate with improved task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>VoT outperforms CoT and image-grounded GPT-4V CoT on this tiling task per reported metrics; LLAMA3-70B with VoT approaches GPT-4 performance for tiling (paper statement).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Visualization accuracy is low (~24%), meaning many generated mental images are incorrect. Models sometimes produce correct language answers despite incorrect visualizations or vice versa. VoT can cause spatial hallucination (overlap) that requires self-refinement; less-capable models fail more often. The task is restricted to 2D grids and tetromino shapes; generalization to more complex geometric/3D tasks is untested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating spatial understanding of large language models <em>(Rating: 2)</em></li>
                <li>Provable limitations of acquiring meaning from ungrounded form: What will future language models understand? <em>(Rating: 1)</em></li>
                <li>Mind's eye: Grounded language model reasoning through simulation <em>(Rating: 1)</em></li>
                <li>Tiling with polyominoes <em>(Rating: 1)</em></li>
                <li>Reasoning with language model is planning with world model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8331",
    "paper_id": "paper-268889526",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4 (VoT)",
            "name_full": "GPT-4 with Visualization-of-Thought prompting",
            "brief_description": "OpenAI's GPT-4 LLM evaluated with the paper's proposed Visualization-of-Thought (VoT) zero-shot prompt that asks the model to 'Visualize the state after each reasoning step' to elicit spatial reasoning and visual state tracking.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary large language model (OpenAI). In experiments accessed via Azure OpenAI API (model version 1106-preview) with greedy decoding (temperature 0). Evaluated in zero-shot setting with VoT prompting to produce interleaved textual visualizations and reasoning steps.",
            "model_size": null,
            "puzzle_name": "Visual Navigation (2D grid)",
            "puzzle_type": "Grid-based navigation task requiring multi-hop spatial planning on synthetic 2D maps (up/down/left/right moves, obstacles, start/destination).",
            "task_setup": "Input: text description plus ASCII/emoji grid; tasks: route planning (generate full direction sequence) and next-step prediction; prompting variants compared: 'Let's think step by step' (CoT), 'Don't use visualization. Let's think step by step' (w/o Viz), and 'Visualize the state after each reasoning step.' (VoT). Zero-shot; outputs include intermediate textual reasoning steps interleaved with ASCII-art/grid visualizations.",
            "mechanisms_or_strategies": "Visualization-of-Thought (VoT) — interleaved generation of reasoning steps and textual/ASCII visualizations representing internal state (visuospatial sketchpad). Also chain-of-thought (CoT) baselines, and ablation 'w/o Viz'. Visual state tracking and self-refinement behaviors observed (models revise visualization/answers).",
            "performance_metrics": "As reported in Table 1, GPT-4 VoT achieved higher accuracy than GPT-4 CoT and GPT-4 w/o Viz across visual navigation metrics (next-step prediction ≈ 40.77% per table row); route-planning metrics (reported completing rate and success rate; success-rate reported ≈ 55.28% in table row); improvements are stated qualitatively and quantitatively vs baselines. VoT achieved notable gains in downstream tasks (see paper Table 1 and Table 6).",
            "evidence_of_spatial_reasoning": "High partial/complete visual state tracking rates under VoT; qualitative examples where GPT-4 generates accurate grid visualizations and uses them to plan routes; extraction and evaluation of final visualizations (compliance and accuracy metrics reported in Table 2: Visual Navigation compliance 51.14%, visualization accuracy 26.48%, and when visualization is correct, spatial-understanding accuracy 65.16%). The paper reports improved next-step and route-planning performance under VoT and presents case studies (self-refine, diverse visualization formats).",
            "comparisons": "Compared against GPT-4 CoT (standard 'Let's think step by step'), GPT-4 w/o Viz (explicitly disabled visualization), and GPT-4V CoT (GPT-4 Vision multimodal model using image inputs). Paper reports GPT-4 VoT outperforms GPT-4 CoT, GPT-4 w/o Viz, and GPT-4V CoT on the tested visual navigation metrics (Table 1).",
            "limitations_or_failure_cases": "Visualizations are often inaccurate: visualization accuracy is low (~24-26% in Table 2). Inconsistencies between text reasoning and generated visualizations observed. VoT is sensitive to prompt wording (removing the word 'reasoning' caused visualizations to be produced only after wrong answers). Route planning remains challenging (completing rates can be low) and overall performance degrades with task difficulty. VoT can underperform in tasks where algebraic/logical solutions suffice (ring navigation example where GPT-4 CoT outperformed VoT).",
            "uuid": "e8331.0",
            "source_info": {
                "paper_title": "Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4 (CoT) / GPT-4 w/o Viz",
            "name_full": "GPT-4 with Chain-of-Thought and GPT-4 with visualization disabled",
            "brief_description": "GPT-4 evaluated under standard chain-of-thought prompting ('Let's think step by step') and an ablation where visualization is explicitly disabled ('Don't use visualization. Let's think step by step').",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Same GPT-4 as above, evaluated via Azure OpenAI API (1106-preview), zero-shot CoT and a 'w/o Viz' prompt variant.",
            "model_size": null,
            "puzzle_name": "Natural-Language Navigation; Visual Navigation; Visual Tiling (polyomino tiling)",
            "puzzle_type": "Natural-language navigation over graph/grid; grid-based visual navigation; polyomino tiling (2D shape arrangement).",
            "task_setup": "Zero-shot CoT prompt 'Let's think step by step' (CoT). 'w/o Viz' prompt explicitly disables visualization then asks to think step-by-step. Same dataset and metrics as VoT experiments (synthetic 2D grids, polyomino tilings, natural-language maps).",
            "mechanisms_or_strategies": "Chain-of-thought (CoT) verbal reasoning without interleaved visual state generation (or explicitly suppressed). Relies on textual reasoning and implicit internal state.",
            "performance_metrics": "Reported in Table 1: GPT-4 CoT and GPT-4 w/o Viz show lower performance compared to GPT-4 VoT across visual tasks. Example numbers from table rows: GPT-4 CoT next-step prediction ≈ 37.02%, visual tiling ≈ 54.15%, natural-language navigation ≈ 54.00%; GPT-4 w/o Viz numbers are lower on several metrics (e.g., natural-language navigation ≈ 35.50% per table row).",
            "evidence_of_spatial_reasoning": "Paper observes that GPT-4 CoT sometimes exhibits spontaneous visual state tracking (tracking rate non-zero), indicating latent ability to simulate spatiotemporal states even without explicit VoT prompting, but overall less grounded than VoT. CoT can outperform VoT on tasks reducible to algebraic/logical computations (ring navigation).",
            "comparisons": "Used as baselines to compare improvements from VoT; CoT occasionally shows partial visual tracking but VoT yields higher complete-tracking and better task performance on most spatial tasks.",
            "limitations_or_failure_cases": "CoT and w/o Viz produce fewer or later visualizations; are less grounded and perform worse on visual tasks. w/o Viz especially suffers in natural-language navigation per reported numbers. CoT may rely on non-spatial/logical shortcuts and can fail when genuine spatial simulation is required.",
            "uuid": "e8331.1",
            "source_info": {
                "paper_title": "Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4V (CoT)",
            "name_full": "GPT-4 Vision with Chain-of-Thought prompting",
            "brief_description": "The multimodal GPT-4 Vision (GPT-4V) model evaluated using its image input capability and CoT prompting as an additional baseline for visual tasks (visual navigation, visual tiling).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V (GPT-4 Vision)",
            "model_description": "Multimodal variant of GPT-4 that accepts image inputs (accessed via Azure OpenAI 'vision-preview'); used with CoT prompt 'Let's think step by step' and with corresponding image inputs rendered from the ASCII/emoji grids.",
            "model_size": null,
            "puzzle_name": "Visual Navigation; Visual Tiling",
            "puzzle_type": "Grid-based visual navigation and polyomino tiling where the model receives rendered image counterparts of the textual grids.",
            "task_setup": "For visual tasks a counterpart image is provided (images generated by rendering the textual grids). Prompt: 'Let's think step by step' (CoT). Zero-shot.",
            "mechanisms_or_strategies": "Standard multimodal perception of the rendered grid image followed by chain-of-thought textual reasoning; does not use VoT interleaved textual visualizations.",
            "performance_metrics": "Reported in Table 1: GPT-4V CoT underperforms GPT-4 VoT on the tested visual tasks in this paper. Example per table row: visual navigation next-step prediction ≈ 33.36%, visual tiling ≈ 46.59% (numbers taken from paper table rows).",
            "evidence_of_spatial_reasoning": "Has access to explicit image rendering of the grid but in experiments GPT-4V CoT showed lower performance than GPT-4 VoT; suggests that internally generated mental visualizations (VoT) can be more effective than direct image grounding in these tasks, per the paper's results and analysis.",
            "comparisons": "Directly compared to GPT-4 VoT and GPT-4 CoT; GPT-4 VoT outperformed GPT-4V CoT on the paper's visual navigation and tiling benchmarks.",
            "limitations_or_failure_cases": "Despite receiving actual images, GPT-4V CoT performed worse than VoT in experiments; authors note that VoT (mental-image tracking) outperformed this SOTA MLLM on their synthetic 2D grid tasks. Limitations may include image interpretation or inability to maintain interleaved state-tracking as effectively as VoT prompting.",
            "uuid": "e8331.2",
            "source_info": {
                "paper_title": "Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-3.5 turbo (VoT / CoT)",
            "name_full": "GPT-3.5 turbo evaluated with VoT and CoT prompting",
            "brief_description": "Earlier-generation OpenAI model (GPT-3.5 turbo) tested to measure VoT effects on less powerful LLMs; accessed via Azure OpenAI API with greedy decoding.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 turbo",
            "model_description": "Smaller/earlier OpenAI LLM (accessed via Azure OpenAI API version 1106-preview). Evaluated under zero-shot VoT and CoT prompts to probe whether VoT benefits accrue to less-capable models.",
            "model_size": null,
            "puzzle_name": "Visual Navigation, Visual Tiling, Natural-Language Navigation",
            "puzzle_type": "Same synthetic grid and tiling tasks as used for GPT-4 experiments.",
            "task_setup": "Zero-shot CoT and VoT prompts identical to GPT-4 experiments; greedy decoding (temperature 0).",
            "mechanisms_or_strategies": "Attempts to produce interleaved visualizations under VoT; compared to CoT baseline.",
            "performance_metrics": "Paper reports that less powerful models often rely on random guessing; GPT-3.5 CoT 'random guessing in nearly half of the instances' in route-planning, and GPT-3.5 VoT reduces random guesses. Exact numeric accuracies for GPT-3.5 are in Table 3 (paper), with modest improvements from VoT but lower absolute performance than GPT-4.",
            "evidence_of_spatial_reasoning": "VoT reduced random guesses for GPT-3.5 and produced some improvement at lower difficulty levels, but the model's visual state tracking and spatial reasoning are inconsistent and brittle compared to GPT-4. Paper notes unpredictable performance trajectories at higher difficulty for smaller models.",
            "comparisons": "Compared within the paper to GPT-4 and LLAMA3 models; smaller models show scaling behavior — gains with size and VoT more effective in larger models.",
            "limitations_or_failure_cases": "High reliance on random guessing for complex tasks; token-exhausting speculative outputs in route-planning; VoT benefits are limited and variable — smaller models often fail to sustain reliable reasoning in multi-hop spatial tasks.",
            "uuid": "e8331.3",
            "source_info": {
                "paper_title": "Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Llama3-8B / Llama3-70B (VoT)",
            "name_full": "Llama 3 family (8B and 70B instruct-tuned) evaluated with VoT",
            "brief_description": "Open-source Llama3 instruct models of different sizes tested to examine scaling effects of VoT prompting on non-OpenAI LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLAMA3-8B-Instruct; LLAMA3-70B-Instruct",
            "model_description": "Instruction-tuned Llama 3 family (8B and 70B parameter sizes) evaluated zero-shot with VoT and CoT prompts in the same spatial tasks.",
            "model_size": "8B and 70B",
            "puzzle_name": "Visual Navigation; Visual Tiling; Natural-Language Navigation",
            "puzzle_type": "Grid-based navigation and polyomino tiling tasks requiring spatial manipulation and planning.",
            "task_setup": "Zero-shot VoT / CoT prompting identical to other experiments; models decode greedily (temperature 0).",
            "mechanisms_or_strategies": "VoT prompting to produce textual visualizations of internal state interleaved with reasoning steps; scaling analysis across model sizes.",
            "performance_metrics": "Paper reports that performance improves with model size within same family; LLAMA3-70B VoT significantly outperforms LLAMA3 CoT baselines on most tasks (Table 3 and Table 6 referenced). For visual tiling LLAMA3-70B VoT aligns closely with GPT-4 results (paper statement). Exact per-task numeric values are presented in the paper's tables.",
            "evidence_of_spatial_reasoning": "LLAMA3-70B shows consistent gains with VoT, indicating the VoT paradigm benefits scale with model capacity. However, smaller LLAMA3 model (8B) shows unreliable behavior and random-guess tendencies at higher difficulty levels.",
            "comparisons": "Within-family comparisons (8B vs 70B) show scaling advantage; LLAMA3-70B with VoT approaches GPT-4-level performance on some tasks (visual tiling) per paper statements.",
            "limitations_or_failure_cases": "Less capable Llama3 models still rely on guessing; VoT effectiveness depends on model capacity and pretraining exposure to code/ascii-art. Visual state tracking and visualization accuracy remain imperfect.",
            "uuid": "e8331.4",
            "source_info": {
                "paper_title": "Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Visual Tiling (polyomino tiling) task",
            "name_full": "Polyomino visual tiling (extended tetromino tiling) task used in this paper",
            "brief_description": "A classic polyomino tiling puzzle adapted to evaluate LLM spatial shape reasoning: select correct variant/orientation of a masked polyomino to fill a rectangle without overlaps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluated across GPT-4, GPT-4V, GPT-3.5, LLAMA3 models",
            "model_description": "Task dataset: 5x4 rectangle with five tetromino pieces (two I, two T, one L); masked 2–3 pieces to form QA instances (total 796 QA instances). Rendered as text/emoji grids and as images for MLLM experiments.",
            "model_size": null,
            "puzzle_name": "Visual Tiling (polyomino tiling)",
            "puzzle_type": "Grid-based combinatorial tiling puzzle requiring 2D shape understanding, rotations/reflections, and spatial arrangement reasoning.",
            "task_setup": "Input: rectangle rendering (emoji/text) and variants for each polyomino; query asks which variant of a queried polyomino fits. Prompt variants: VoT ('Visualize the state after each reasoning step') vs CoT. Zero-shot. Evaluation by substring/exact matching of answers.",
            "mechanisms_or_strategies": "VoT interleaved visualizations where LLMs place candidate pieces into ASCII grids, check overlaps/fit (compliance), and self-refine; algorithmic data generation used exact cover / dancing links to ensure solvable configurations.",
            "performance_metrics": "GPT-4 VoT visual-tiling accuracy reported ~63.94% (per Table 1 row), GPT-4 CoT ~54.15%, GPT-4V CoT ~49.62%, GPT-4 w/o Viz ~46.98% (table-row values). Table 2 reports spatial visualization compliance ~52.01% and visualization accuracy ~24.25% for visual tiling; when visualization is correct, decision accuracy ~77.20%.",
            "evidence_of_spatial_reasoning": "Quantitative compliance/accuracy metrics (Table 2) and qualitative cases: self-refine behaviors where model detects spatial hallucination, re-evaluates placements, and corrects final answer; examples of inconsistent visualization vs language also analyzed. VoT increases complete/partial visual-tracking rates which correlate with improved task accuracy.",
            "comparisons": "VoT outperforms CoT and image-grounded GPT-4V CoT on this tiling task per reported metrics; LLAMA3-70B with VoT approaches GPT-4 performance for tiling (paper statement).",
            "limitations_or_failure_cases": "Visualization accuracy is low (~24%), meaning many generated mental images are incorrect. Models sometimes produce correct language answers despite incorrect visualizations or vice versa. VoT can cause spatial hallucination (overlap) that requires self-refinement; less-capable models fail more often. The task is restricted to 2D grids and tetromino shapes; generalization to more complex geometric/3D tasks is untested.",
            "uuid": "e8331.5",
            "source_info": {
                "paper_title": "Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating spatial understanding of large language models",
            "rating": 2,
            "sanitized_title": "evaluating_spatial_understanding_of_large_language_models"
        },
        {
            "paper_title": "Provable limitations of acquiring meaning from ungrounded form: What will future language models understand?",
            "rating": 1,
            "sanitized_title": "provable_limitations_of_acquiring_meaning_from_ungrounded_form_what_will_future_language_models_understand"
        },
        {
            "paper_title": "Mind's eye: Grounded language model reasoning through simulation",
            "rating": 1,
            "sanitized_title": "minds_eye_grounded_language_model_reasoning_through_simulation"
        },
        {
            "paper_title": "Tiling with polyominoes",
            "rating": 1,
            "sanitized_title": "tiling_with_polyominoes"
        },
        {
            "paper_title": "Reasoning with language model is planning with world model",
            "rating": 1,
            "sanitized_title": "reasoning_with_language_model_is_planning_with_world_model"
        }
    ],
    "cost": 0.01776625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models
23 Oct 2024</p>
<p>Wenshan Wu 
Microsoft Research ‡ East China Normal University</p>
<p>Shaoguang Mao 
Microsoft Research ‡ East China Normal University</p>
<p>Yadong Zhang 
Microsoft Research ‡ East China Normal University</p>
<p>Yan Xia 
Microsoft Research ‡ East China Normal University</p>
<p>Li Dong 
Microsoft Research ‡ East China Normal University</p>
<p>Lei Cui 
Microsoft Research ‡ East China Normal University</p>
<p>Furu Wei 
Microsoft Research ‡ East China Normal University</p>
<p>Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models
23 Oct 2024C6438C8D961D0615DEBBE263293136FBarXiv:2404.03622v3[cs.CL]
Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks.However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored.Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind's Eye, enabling the imagination of the unseen world.Inspired by this cognitive capacity, we propose Visualizationof-Thought (VoT) prompting.VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps.We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds.Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs.Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks.While VoT works surprisingly well on LLMs, the ability to generate mental images to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs.Please find the dataset and * Contribution during internship at Microsoft Research.38th Conference on Neural Information Processing Systems (NeurIPS 2024).</p>
<p>Introduction</p>
<p>Recently, large language models (LLMs) [BCE + 23, BMR + 20, TLI + 23, JSM + 23] have achieved remarkable performance on various language-related tasks.However, despite their success in math reasoning [KGR + 23], common sense reasoning [LKH + 22], and other reasoning tasks such as symbolic reasoning or logic reasoning [KGR + 23], their abilities in spatial reasoning still remain underexplored [RFD + 21, YBL + 23, MHV + 24].Spatial reasoning is an essential function of human cognition, allowing us to interact with the environment.It facilitates tasks that require understanding and reasoning about the spatial relationships between objects and their motions.The spatial reasoning of language models largely relies on language to reason about spatial information, whereas human cognitive capabilities extend far beyond verbal reasoning.Humans can not only create task-relevant abstract representations from visual perception [BK18,KC22], but also imagine unseen scenes through their mind's eye.It remains a research topic called mental image [She78] in domains of neuroscience, philosophy of mind, and cognitive science.Building upon this cognitive function, humans facilitate spatial reasoning by mental image manipulation, such as navigation [Tol48], mental rotation [SM71], mental paper folding [SF72], and mental simulation [MK09].Figure 1 illustrates the human process involved in a navigation task.Humans enhance their spatial awareness and inform their decisions by creating mental images of a route, utilizing various sensory inputs such as navigation instructions or a map image.Subsequently, they simulate route planning through the mind's eye.</p>
<p>Inspired by this cognitive mechanism, we conjecture that LLMs possess the ability to create and manipulate mental images in the mind's eye for spatial reasoning.As illustrated in Figure 1, LLMs could potentially process and understand spatial information in various formats.They might be capable of visualizing internal states and manipulating these mental images through their mind's eye, thereby guiding subsequent reasoning steps to enhance spatial reasoning.Therefore, we propose the Visualization-of-Thought (VoT) prompting to elicit this ability.This method leverage LLMs to visualize their reasoning steps and inform subsequent steps, implementing the concept of visuospatial sketchpad [Bad92].VoT adopts zero-shot prompting instead of relying on few-shot demonstrations or text-to-image visualization with CLIP [RKH + 21].This choice stems from LLMs' ability to acquire various mental images from text-based visual art [SB14,SMM21,Reg19].</p>
<p>To evaluate the effectiveness of VoT in spatial reasoning, we selected three tasks that require spatial awareness in LLMs, including natural-language navigation [YBL + 23], visual navigation, and visual tiling.These tasks require an understanding of space, direction, and geometric shape reasoning.To emulate human-like multisensory perception, we designed 2D grid worlds using special characters as enriched input formats for the LLMs in visual navigation and visual tiling tasks.We compared different models (GPT-4, GPT-4V) and prompting techniques across these three tasks.The findings reveal that the VoT prompting proposed in this paper consistently induces LLMs to visualize their reasoning steps and inform subsequent steps.Consequently, this approach achieved significant performance improvements on the corresponding tasks.</p>
<p>Spatial Reasoning</p>
<p>Spatial reasoning refers to the ability to comprehend and reason about the spatial relationships among objects, their movements, and interactions with the environment.This skill is vital for a wide range of real-world applications such as navigation, robotics, and autonomous driving.These fields necessitate action planning based on visual perception and a concrete understanding of spatial dimensions.</p>
<p>Although several tasks and datasets [WBC + 15, SZL22, MK22, LB18, RAB + 20] have been developed to probe the spatial semantics embedded in text, existing research efforts often focus on how spatial terms are linguistically structured.Recently, significant achievements and impressive results have been achieved in these benchmarks by converting spatial terms to logical forms through LLMs and adopting logic programming [YIL23].This implies that excelling in these tasks does not necessarily equate to a genuine understanding of spatial information by LLMs, nor does it provide an accurate measure of their spatial awareness.Spatial awareness involves understanding spatial relationships, directions, distances, and geometric shapes, all of which are essential for action planning in the physical world.To evaluate the spatial awareness and spatial reasoning abilities of LLMs, we have selected tasks that test navigation and geometric reasoning skills, including natural language navigation, visual navigation and visual tiling.</p>
<p>Natural Language Navigation</p>
<p>Natural language navigation task [YBL + 23] was inspired by prior research on human cognition [GDB17] presenting participants with sequential transitions sampled from a graph structure.</p>
<p>In this context, a square map is defined by a sequence of random walk instructions and associated objects at each location, denoted as W = {(l 1 , o 1 ), (l 2 , o 2 ), . . ., (l n , o n )}.Given a square map W , and sequence of navigation instructions I = {i 1 , . . ., i k }, the task for the model is to identify the associated object o ∈ W at the specified location l which is determined by the navigation instructions, as detailed in Equation 1 and exemplified in Appendix B.2.
o ∼ p(o ∈ W |W = {(l 1 , o 1 ), (l 2 , o 2 ), . . . , (l n , o n )}, I)(1)</p>
<p>Visual Navigation</p>
<p>Visual navigation task presents a synthetic 2D grid world to LLM, challenging it to navigate using visual cues.The model must generate navigation instructions to move in four directions (left, right, up, down) to reach the destination from the starting point while avoiding obstacles.This involves two sub-tasks: route planning and next step prediction, requiring multi-hop spatial reasoning, while the former is more complex.Task instructions are available in Figure 6 in appendix.</p>
<p>Formulation The model is presented with a grid map M consisting of k consecutive edges E = {e(s 0 , s 1 ), e(s 1 , s 2 ), • • • , e(s k−1 , s k )}, where the starting point and destination are s 0 and s k respectively, as shown in Figure 2. Route planning task is to generate a sequence of correct directions
D = {d(s 0 , s 1 ), d(s 1 , s 2 ), • • • , d(s k−1 , s k )},
as defined in Equation 2. Given M and t navigation instructions D t,0&lt;t&lt;k = {d(s 0 , s 1 ), • • • , d(s t−1 , s t )}, next step prediction task is to identify the correct direction d(s t , s t+1 ) of the next step, as defined in Equation 3.
D ∼ p({d(s 0 , s 1 ), d(s 1 , s 2 ), • • • , d(s k−1 , s k )} | M ) (2) (a) k=2 (b) k=3 (c) k=4 (d) k=5 (e) k=6 (f) k=7d ∼ p(d(s t , s t+1 ) | M, D t,0&lt;t&lt;k )(3)
Implementation The navigation map's underlying graph is semi-Eulerian, alternating between horizontal and vertical edges, with 2 k+1 possible spatial configurations for a k-hop navigation map.For each map and set of k navigation instructions, k − 1 question-and-answer (QA) instances,i.e."what is the next step?"are created.Further implementation details are in Appendix A.1.</p>
<p>Visual Tiling</p>
<p>Introduced by [Gol66], polyomino tiling is a classic spatial reasoning challenge.We extend this concept to test the LLM's ability to comprehend, organize, and reason with shapes in a confined area, thus enhancing the evaluation of spatial reasoning skills.As depicted in Figure 3, the task involves a rectangle with unfilled cells and various polyomino pieces, like the I-tetromino made of four aligned squares.The model must select the appropriate polyomino variant, such as choosing the orientation for the I-tetromino, to solve the QA puzzle.Task instructions are provided in Figure 7 in appendix.</p>
<p>Formulation The model is presented with a rectangle R masked with k unique polyominoes
M P = {mp 1 , • • • , mp k }, 2 corresponding variants of each polyomino v i&lt;=k = {v i1 , v i2
}, and a polyomino query q ∈ M P .Visual tiling task is to identify the correct variant of q, as defined in Equation 4. 3 Visualization-of-Thought Prompting
v ∼ p(v q | R, {mp 1 , • • • , mp k }, {v 11 , v 12 • • • , v k1 , v k2 }, q)(4
Considering the way humans process spatial information during tasks like navigation, it's common to create mental images , such as maps, to enhance spatial awareness or simulating movements to inform decision-making.Our objective is to elicit the spatial awareness of LLMs and ground their reasoning by visualizing the consequence of their intermediate reasoning steps.</p>
<p>We introduce Visualization-of-Thought (VoT) prompting: "Visualize the state after each reasoning step."This new paradigm for spatial reasoning aims to generate reasoning traces and visualizations in an interleaved manner.Qualitative results of this approach are presented in Figure 4.</p>
<p>We use p θ to denote a pre-trained LM with parameters θ, x, y, z to denote a language sequence, and v to denote a visualization sequence in text form.In a multi-hop spatial reasoning task with input x, CoT prompting generates a series of intermediate steps
z 1 , • • • , z n , each step z i ∼ p θ (z i | x, z 1•••i−1 )
is sampled sequentially, followed by the output y ∼ p θ (y|x, z 1•••n ).As shown in Figure 1, VoT prompting enhances this process by adding a visuospatial sketchpad to each intermediate step z i , then the subsequent step z i+1 is sampled conditioned on prior steps
z 1•••i and visualizations v 1•••i .
As defined in the Equation 5 and 6, it forms interleaved reasoning traces and visualizations.A qualitative comparison between outputs of VoT and CoT is provided in Figure 8a in appendix.
v i ∼ p θ (v i | prompt V oT , x, z 1•••i , v 1•••i−1 )(5)
Starting from , provide the steps to navigate to .</p>
<p>Provided: I T L To fit all the provided polyominoes into the empty squares, what's the correct variation of Tetromino T?</p>
<p>Visualize the state after each reasoning step.</p>
<p>Visual Navigation</p>
<p>Visual Tiling Natural Language Navigation</p>
<p>You have been given a 3 by 3 square grid.Initially, you are at the bottom-left corner…find a cassette player…go right…a wool, go right…a conch, go up…a moving van, go left…a confectionery store, go left…a pot pie, go up…a siamang, go right…a black-and-white colobus, go right…a minivan.Now you have all the information on the map.You start at where the cassette player is located, then you go right by one step, go right…go up…go left…go left…go up…go right…go down by one step.What will you find?</p>
<p>Visualize the state after each reasoning step.Visualize the state after each reasoning step.
z i+1 ∼ p θ (z i+1 | prompt V oT , x, z 1•••i , v 1•••i )(6)
This reasoning paradigm enables LLMs with visual state tracking.We introduce the concept of a state, denoted as
s i = [x, z 1•••i , v 1•••i−1 ]
representing a partial solution at step i with the input, the sequence of intermediate steps z 1•••i and the sequence of visualizations
v 1•••i−1 . v i ∼ p θ (v i | prompt V oT , x, z 1•••i , v 1•••i−1 ) ∼ p θ (v i | prompt V oT , s i )(7)
As shown in Equation 7, visual state tracking is implemented by generating the visualization v i as representation of the internal state s i after each reasoning step z i (e.g.v i could be a grid of the navigation map marked with path or a filled rectangle).Grounded by the visual state tracking sequence, the subsequent state is derived by s i+1 ∼ p θ (s i+1 | prompt V oT , x, s i , v i ).This mechanism allows for the derivation of subsequent states, reflecting spatiotemporal causality and enhancing the spatial reasoning capabilities of LLMs in a grounded context.</p>
<p>Experiment</p>
<p>Setup</p>
<p>For the visual tasks where a counterpart image exists for each text input, we conduct additional experiments with a multimodal model.Specifically, we adopt GPT-4 [OA + 23] and GPT-4 Vision [Ope23] via Azure OpenAI API as they're state of the art LLM and multimodal model respectively.API settings are temperature 0 as greedy decoding and top p 1, with model versions of 1106-preview and vision-preview.For all experiments we adopt zero-shot prompting.</p>
<p>Depending on whether the LLM is explicitly prompted to visualize intermediate steps, we experiment with three settings of GPT-4, including zero-shot CoT prompting(GPT-4 CoT), GPT-4 w/o Viz where visualization is explicitly disabled during reasoning, and VoT prompting (GPT-4 VoT).Additional setting of GPT-4 Vision with counterpart image input is GPT-4V CoT.Prompts are as following:</p>
<p>• GPT-4 CoT: Let's think step by step.</p>
<p>• GPT-4 w/o Viz: Don't use visualization.Let's think step by step.</p>
<p>• GPT-4V CoT: Let's think step by step.</p>
<p>• GPT-4 VoT: Visualize the state after each reasoning step.</p>
<p>Task instructions and examples could be found in Appendix B.</p>
<p>Dataset</p>
<p>Natural Language Navigation We generate 200 square maps of size 3x3 which is described by 9 landmarks in snake order traversal, and a set of navigation instructions.</p>
<p>Visual Navigation We generate 496 navigation maps and 2520 QA instances in total, covering various map sizes, up to 7×9 and 9×7.The data distribution is provided in Table 4 in appendix.</p>
<p>Visual Tiling We first generate multiple unique configurations to fill a 5 x 4 rectangle with 5 polyomino pieces including two I tetrominoes, two T tetrominoes and one L tetromino.Then we randomly masked two or three pieces of different types and generate QA instance for each masked pieces.The total number of QA instances is 796, and we show dataset details in Table 5 in appendix.</p>
<p>Metric</p>
<p>We extract the answer from model output by pattern matching.For tasks except for route planning, we calculate accuracy by Equation 8. We adopted sub-string matching † as f correct to determine correctness.
acc = n i f correct (extracted_answer, ground_truth)/n(8)
For the route planning task which predicts a sequence of navigation instructions, we reject any sequences exceeding 100 instructions, considering them to be random guesses.We then normalize the navigation instructions by executing each navigation instruction.Those instructions which violate navigation rules will be ignored.The length t of normalized instruction sequence is considered as the temporal distance against the starting point.Given the ground-truth of k navigation instructions, the completing rate of route planning is t/k.For the dataset of n maps, we report two metrics including:</p>
<ol>
<li>Average completing rate:
n i t i /k i /n.
Average completing rate among all instruction sequences, reflecting LLM's effectiveness of route planning.2. Success rate: n i (t i == k i )/n.This metric represents the proportion of instruction sequences with t = k, i.e., reaching the destination.</li>
</ol>
<p>Results</p>
<p>As illustrated in Table 1, GPT-4 VoT significantly outperforms other settings in all tasks across all metrics.The significant gap when comparing GPT-4 VoT with GPT-4V CoT and GPT-4 w/o Viz demonstrates that effectiveness of visual state tracking, which allows LLMs visually interpret their actions within an grounded world.And in the natural language navigation task, GPT-4 VoT outperforms GPT-4 w/o Viz by 23.5%.In the visual tasks, the noticeable performance gap between GPT-4 CoT and GPT-4V CoT indicates that LLM grounded with 2D grid could possibly outperform a MLLM in challenging spatial reasoning tasks.</p>
<p>On the other hand, performance of GPT-4 VoT is still far from perfect in all tasks, especially in the most challenging route planning task.Despite these tasks are relatively easy for humans, performance of LLMs drops significantly as task difficulty increases.Details on performance trends across difficulty levels are provided in figure 9  For each model output, we extract the sequence of visualizations sampled prior to generating the final answer and discard any visualizations generated thereafter.Then we compare the sequence length l v with the number of reasoning steps l s .We calculate Complete Tracking n i (l v == l s )/n when a visualization v i corresponds to each state s i .Similarly, we calculate the Partial Tracking metric as n i (l v &gt; 0)/n when at least one visualization is present before the final answer is generated.Figure 5 shows the significant differences between these settings.In the GPT-4 CoT setting, it demonstrated noticeable tracking rate across almost all tasks except route planning.This observation implies that LLMs inherently exhibit the capability of visual state tracking when spatiotemporal simulation is integral to reasoning.</p>
<p>On the other hand, the visual state tracking behavior is sensitive to prompts to varying degrees.As showcased in Figure 8 in appendix, after removing "reasoning" from the prompt of VoT, the visualizations are sampled after GPT-4 generates the wrong answer.Consequently, explicitly prompting LLMs to visualize their reasoning traces with VoT markedly improves the visual tracking rate, thereby enhancing overall performance.The potential contribution of code pre-training to this emergent capability is further explored in Appendix C.</p>
<p>How visualizations enhance final answers?</p>
<p>Ideally, VoT is supposed to generate an accurate visualization v i at each step, so that subsequent step z i+1 could be determined correctly.This relies on the spatial visualization and spatial understanding capability of LLMs.To evaluate these capabilities of LLMs in these tasks, we extract the final visualization from each model output under the setting GPT-4 VoT in visual navigation and polyomino tiling task.Specifically, for visual navigation task, we extract the visualized map where LLM completed all navigation instructions.For polyomino tiling, we extract the rectangle filled with corresponding polyomino piece.The spatial visualization capability is measured by two criteria:</p>
<p>(1) Compliance, indicating whether the manipulation of mental image satisfies requirements such as avoiding overlap and navigating around obstacles.(2) Accuracy, indicating whether the mental image aligns with the corresponding state.The spatial understanding capability is measured by the proportion of correct answers when the corresponding visualization is generated accurately.</p>
<p>As could be seen from</p>
<p>Can VoT benefit less powerful language models?</p>
<p>To evaluate the efficacy of VoT on less powerful language models, we conducted experiments across various model families [BMR + 20, OA + 23, TLI + 23] and model sizes, including GPT-3.5 turbo, LLAMA3-8B-Instruct and LLAMA3-70B-Instruct.We access GPT-3.5 via Azure OpenAI API with model version 1106-preview and apply greedy decoding to all models.</p>
<p>As shown in Table 3, within the same model family, performance improves across all tasks with increases in model size.LLAMA3-70B VoT significantly outperforms the baseline across all tasks except for visual tiling, where it aligns closely with results observed in GPT-4.This consistency suggests that VoT offers a scaling advantage when applied to more advanced models, markedly enhancing performance in larger models.In contrast, less capable models tend to rely on random guessing, especially in spatial reasoning tasks.For instance, in the route planning task, GPT-3.5</p>
<p>CoT often resorts to speculative responses, random guessing in nearly half of the instances, which leads to exhaustion of output tokens.While GPT-3.5 VoT effectively minimizes random guesses, such occurrences become increasingly rare with GPT-4 CoT as the model size expands.On the other hand, the reliance on random guessing introduces unpredictability in performance trends for less powerful models.It suggests their limitations in sustaining reliable reasoning processes across different difficulty levels.Details on performance trends are provided in Appendix D.</p>
<p>6 Related Works</p>
<p>Conclusion</p>
<p>This study introduces Visualization-of-Thought Prompting (VoT), inspired by the human cognitive function of visualizing and manipulating mental images through the mind's eye.We have demonstrated that VoT enables LLMs to exhibit the mechanism of "the mind's eye", as evidenced by their performance in multi-hop spatial reasoning tasks and our comprehensive analysis of the reasoning traces.Remarkably, VoT enable LLMs to outperform state-of-the-art multimodal large language models (MLLMs) in the tested visual tasks.While VoT demonstrates impressive efficacy in LLMs, this emergent capability to create mental images to enhance spatial reasoning resembles the mind's eye process, suggesting its promise in MLLMs.</p>
<p>Building on the success of experiments with GPT-4, we plan to investigate how VoT can futher elicit "the mind's eye" in MLLMs to enhance their spatial awareness.Additionally, our future efforts will focus on automatic data augmentation from real-world scenarios, aiming to identify effective methods for learning generalized internal representations of mental images.This will further improve the mind's eye of LLMs, ultimately contributing to the advancement of their cognitive and reasoning abilities.</p>
<p>Limitations</p>
<p>This work only scratches the surface of spatial reasoning of LLMs.Both mental images and visual state tracking rely on the emergent ability of advanced LLMs.Therefore, it might cause performance deterioration in less advanced language models or more challenging tasks.Besides, due to the limited data exposure and a lack of explicit instruction tuning, visual state tracking of current LLMs are sensitive to prompts.For example, when explicitly prompted with "use ascii-art", the tracking rate will significantly increase thereby boosting performance, while removing "reasoning" from the VoT prompt will cause a decrease of tracking rate.Moreover, the mental images tested in our work are limited to 2D grid.To strength the mind's eye of LLMs, more diverse and complicated representation should be explored in the future, such as complex geometric shapes and even 3D semantics shown in Figure 11 in appendix.</p>
<p>[SMM21] Harini Sampath, Alice Merrick, and Andrew Macvean.Accessibility of command line interfaces.In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-10, 2021.</p>
<p>[SZL22] Zhengxiang Shi, Qiang Zhang, and Aldo Lipani.Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts.AAAI Conference on Artificial Intelligence, 2022.</p>
<p>[TLI + 23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.Llama: Open and efficient foundation language models.arXiv.org,2023.</p>
<p>[Tol48] Edward C Tolman.Cognitive maps in rats and men.Psychological review, 55(4):189, 1948.</p>
<p>[WBC + 15] Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov.Towards ai-complete question answering: A set of prerequisite toy tasks, 2015.</p>
<p>[YBL + 23] Yutaro Yamada, Yihan Bao, Andrew K. Lampinen, Jungo Kasai, and Ilker Yildirim.</p>
<p>Evaluating spatial understanding of large language models, 2023.</p>
<p>[YIL23] Zhun Yang, Adam Ishay, and Joohyung Lee.Coupling large language models with logic programming for robust and general reasoning from text, 2023.</p>
<p>A Synthetic Data</p>
<p>A.1 Visual Navigation</p>
<p>As depicted in 1, given a specific k, the process of generating a 2D navigation map is composed of 3 steps, which are instruction generation, instruction simulation, map rendering.In instruction generation step, we enumerate all possible instruction sets navigating from the starting point to the destination (e.g move up, then move right).During this step, only the direction of each instruction is determined, while the moving distance is undetermined until next step.In instruction simulation step, simulation is applied in the 2D coordinate system with origin (0, 0) as the starting point.To guarantee an unique answer in each navigation map, the moving distance of each instruction is dynamically calculated to avoid overlapping.Each time when an overlapping is detected, the moving distance of previous instruction will be increased by 1 unit recursively until overlapping is resolved.As the distance is determined, those corresponding points are added to the navigating path.After all instructions are completed, the final point is marked as the destination.In the map rendering step, the bounding box of those points is adopted and normalized to a 2D square grid.The starting point and destination are marked with dedicated squares, and cells along the path are marked by empty squares, while other untouched cells are marked by obstacle squares.</p>
<p>Algorithm
C solution ← C solution ∪ s i C textual_map ← C textual_map ∪ t i C visual_map ← C visual_map ∪ v i end
Since the direction of each navigation instruction is alternating, there are 4 * 2 k−1 = 2 k+1 kinds of spatial configurations for a k-hop navigation map.During the implementation, we simplify the recursive implementation with an early quit when path overlapping could not be resolved within one iteration, the main consideration of which is the size of the map.So the number of generated map is slightly lower than 2 k+1 as the navigating step k increases.</p>
<p>A.2 Visual Tiling</p>
<p>The data generation process comprises 3 stages, including configuration generation, question generation and polyomino rendering.In the configuration generation stage, to generate valid spatial configurations of a rectangle and the corresponding polyomino set, we convert a tiling problem to existing formalized problems.One of the problems is an exact cover problem leveraging dancing link algorithm [Knu00], which could be described as: given a matrix of 0s and 1s, find a set of rows containing exactly one 1 in each column.The conversion is to construct a matrix of 0s and 1s, each row of which represents a possible arrangement of placing a specific polyomino in a rectangle.As illustrated in Equation 9, given k polyomino pieces, and a rectangle of n units to be filled, the first k columns compose an one-hot vector indicating the corresponding polyomino, and the last n columns are marked with 0 or 1 depending on whether the corresponding unit is filled by that polyomino.Then finding a set of polyomino arrangements in a rectangle equals to find a set of rows containing exactly one 1 in each column.Another adaptable problem is the boolean satisfiability problem (commonly known as SAT), for which efficient solvers exist [ES03,GN07].A tiling problem can be converted to SAT by introducing a boolean variable for each possible arrangement of each piece, and then adding clauses comprising of those boolean variables that ensure at least one arrangement of each piece is achieved, while avoiding conflicts between arrangements of one piece or two different pieces.</p>
<p>Given the size of a rectangle and polyominoes to be fit, multiple corresponding solutions are generated by applying those algorithms.Then in the question generation stage, we randomly mask several polyomino pieces in the rectangle, and generate a question answer(QA) pair for each masked polyomino.Finally the rectangle and each polyomino piece are rendered with emoji squares.
       C 1 C k C k+1 C n+k 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 P 1 P k       (9)</p>
<p>A.3 Visual Data Rendering</p>
<p>After gathering the textual dataset of 2D square grid, we generate the corresponding visual dataset by drawing text onto an image.Specifically we adopt color emojis for a fair comparison as they're more visual friendly to a multimodal model.</p>
<p>A.4 Dataset Details</p>
<p>Data distribution among various difficulty levels for visual navigation tasks and visual tiling tasks are provided in Table 4 and 5.It provides flexible difficulty control across different tasks.For visual tiling task, the difficulty is controlled by the number of masked polyomino pieces.As the number increases, the more spatial arrangements LLMs need to consider.Regarding the visual navigation task, as illustrated in figure2, we use the number of roads k to control difficulty, which is corresponding to the size of the map.</p>
<p>B Examples</p>
<p>For visual navigation and visual tiling tasks, the structured input template is comprised of task instruction, input parameters and prompt of specific setting.</p>
<p>B.1 Visual Tasks</p>
<p>Task instructions and responses of each visual task under setting GPT-4 VoT are provided as following:</p>
<p>• Route Planning Task instruction in Figure 6, response in Figure 12 .</p>
<p>where you will find a baseball player, then you go right, where you will find a harvestman, then you go right, where you will find a neck brace.Now you have all the information on the map.You start at the position where the torch is located, then you go right by one step, then you go right by one step, then you go up by one step, then you go up by one step, then you go left by one step, then you go down by one step, and then you go down by one step.What will you find?</p>
<p>Response Example See Figure 15.</p>
<p>C Visual State Tracking</p>
<p>As for where this emergent ability stems from, it might derive from tabular data, city grid navigation, maze exploration related coding problems [YBL + 23].These tasks involves understanding and manipulating objects in a 2D square grid.Besides, we conjecture the exposure of ascii-art comments [Reg19] during LLMs' code pre-training possibly enhances this generalized ability.As a fact to support this conjecture, the visual tiling task is different from navigation tasks because it requires shape understanding and spatial manipulation ability.While tabular data and square grid navigation data boost row-wise or column-wise attention, ascii-art supplements intricate spatial attention to understand and manipulate 2D shapes.Additionally, ascii-art in code comments is presented in various formats, one of which is interleaved ascii diagrams, natural language and programming language.It require LLMs to generate the interleaved mental images and text sequence, thereby enhancing spatial visualization ability and spatiotemporal causality.Interestingly in the natural language navigation task, when GPT-4 is prompted with "use ascii-art to visualize", the complete tracking rate increases to 98.5% (+78.5%),boosting task performance to 62.5% (+3.5%).</p>
<p>C.1 Ascii-art in Code Comments</p>
<p>Ascii-art is commonly used in code comments to represent data structure, diagram, geometry and so on, which could benefit LLMs' spatial understanding and visualization capability.Besides, it's also used to illustrate how an algorithm works or simulate an operation, where reasoning traces and corresponding visualization are presented in an interleaved manner.Below are several examples in open-source projects.</p>
<p>• Spatial Causality:Double-ended queue in Rust, Scrolling web pages and tree rotation present triplets of previous visual state, instruction, and updated state of instruction following.• Temporal Causality: Undo systems from emacs provides various temporal states of the undo system when undo operation happens in different timelines and corresponding visualizations in an interleaved manner.Each visualization reflects the temporal casuality of the system state.</p>
<p>This kind of interleaved sequence tracks the system state over time, thus reflecting spatiotemporal casuality.</p>
<p>D Performance Trends Across Levels</p>
<p>In this analysis, we examine performance trends across varying difficulty levels in the next-step prediction task for models utilizing either CoT or VoT methods.These trends are crucial for understanding the inherent unpredictability associated with random guessing.As k increases from 2 to 7 in a k-step navigation map, distinct performance patterns emerge among different models, as depicted in Figure 9. Larger language models such as GPT-4 and LLAMA3-70B demonstrate a more predictable decrease in accuracy with increasing k.This trend indicates a robust ability to handle progressively challenging tasks, despite the overall decrease in performance.Detailed statistics are provided in table 6.In contrast, less powerful models like GPT-3.5 and LLAMA3-8B exhibit an irregular performance trajectory.These models show variable accuracy, with significant fluctuations at higher difficulty levels, suggesting a reliance on random guessing, particularly under conditions of increased task difficulty.This behavior highlights their limitations in sustaining reliable reasoning processes through more complex scenarios.Furthermore, the VoT method seems to offer a modest improvement in performance for the less powerful models, particularly in scenarios of lower difficulty.</p>
<p>This observation suggests that VoT might be advantageous for enhancing reliable reasoning in simpler spatial reasoning tasks, potentially compensating for the inherent weaknesses of smaller language models.</p>
<p>Model</p>
<p>E Case study</p>
<p>We consider visual state tracking similar to spatiotemporal simulation.During the simulation in those tasks, we discovered several interesting behaviors of LLM.</p>
<ol>
<li>Diverse visualization formats for state tracking: Nearly 30 different symbols found in the navigation tasks to track the navigation progress, including marking the path, marking the current location.Among those diverse representations, LLM succeeded in some challenging cases where it used directional arrow emojis to indicate both the location and moving direction at each step.More examples could be found in Appendix E.1.</li>
</ol>
<p>2.</p>
<p>Inconsistency between language and visualization: This is commonly observed across all tasks.Due to the limited visualization capability, sometimes LLM generates accurate language instruction but inaccurate visualization.And in other cases, LLM generates wrong answers even the visualization is generated correctly, which reflects its limitation of spatial understanding as discussed in previous section.More examples could be found in Appendix E.2.</p>
<ol>
<li>Self-refine mechanism: We found several cases in visual tiling tasks where spatial hallucination happens due to the inconsistency or inaccurate visualization.Subsequently, LLM refined its reasoning, resulting in an accurate visualization and the correction of the final answer.More examples could be found in Appendix E.3.</li>
</ol>
<p>E.1 mental images for State Tracking</p>
<p>In the visual navigation task, LLM adopted various symbols and representations to track the state of navigation progress.As shown in Figure 10, there're several tracking styles.</p>
<p>• Mark the path: adopting an identical symbol to mark current location or part of the path.</p>
<p>• Mark path and direction: using directional arrows to mark current location and indicate the moving direction simultaneously, which is more challenging than simply marking the path.• Mark path with temporal steps: using numbers to demonstrate both temporal steps and current location.• Remove road: turning roads into obstacles to avoid turning back, instead of adopting additional symbols to mark the path.</p>
<p>E.2 Inconsistency between Language and Visualization</p>
<p>In the visual tiling task, two inconsistent steps are highlighted in Figure 16.One is the inconsistent visualization with the language instruction of "place Variation 6 of Tetromino L".Another is the wrong decision to chose "Variation 2 of Tetromino I" given the visualization of the valid state.</p>
<p>E.3 Self-refine Mechanism</p>
<p>We found visualization could enhance LLM's reasoning by self-grounding and refining subsequent reasoning steps in some cases.As shown in Figure 17, despite successfully identifying variation 1 of tetromino L as incorrect option, GPT-4 excluded the correct option of variation 6 even it's placed accurately due to spatial hallucination (overlapping with yellow pieces), which led to a impossible solution.Then it detected the mistake and re-evaluate the placement of variation 6. Finally it placed the correct piece into the top left corner and validated the answer by filling the remaining space.
`` 0 3 <em>-------</em> / /| <em>-------</em> | 4| | |7 | | * | |/ <em>-------</em> | | | | <em>-------</em> 1 2 ```
Here, the face with vertices 1, 2, 6, and 5 has been unfolded downwards.The vertical lines connecting the top face to the front face have now become horizontal, indicating that the front face is lying flat.The cube is now represented as a two-dimensional net, with the front face unfolded.Keep in mind that this is a simplified representation and that in an actual net, the faces would be connected at the edges without the gaps shown in the ASCII art.</p>
<p>The ASCII art you've provided represents a three-dimensional cube with labeled vertices from 0 to 7. To unfold the front face (which appears to be the face connecting vertices 1, 2, 6, and 5), we would lay it flat.Assuming that the front face is the one with vertices 1, 2, 6, and 5, and we unfold it downwards, the net might look something like this: `  We have two variations for Tetromino I ( ), which can either be placed vertically or horizontally.Given the configuration of the empty spaces, the only way to place the Tetromino I is horizontally at the top row, as the vertical placement would not fit anywhere.
<code>0 3 *-------* /| /| *-------* | 4| | | |7 | *-----|-* |/ |/ *-------* 1 2 ``` Ascii cube:</code>Ù nfold the front face 0 3 <em>-------</em> 1 / | 2 /| <em>-------</em> | | |4 | |7 | <em>-----|-</em> |/ | / <em>-------</em> 5 6
After placing Tetromino I ( ), the target rectangle looks like this: <code></code> Next, we have two variations for Tetromino L ( ).We can see that Variation 3 of Tetromino L can fit in the bottom left corner, while Variation 7 would not fit anywhere without overlapping or going outside the rectangle.Since neither Variation 1 nor Variation 6 of Tetromino L fits into the target rectangle without overlapping existing polyominoes or going outside the rectangle, the correct answer is: C. Neither However, there seems to be a mistake because the task guarantees that there always exists a solution.Let's re-evaluate the placement of Tetromino L.</p>
<p>We can see that the Tetromino L must fit in the top left corner because that's the only place with enough space for its shape.The Tetromino I fits perfectly in the remaining space using Variation 1 (vertical placement).Therefore, the correct variation of Tetromino L that fits into the target rectangle is:</p>
<p>A. 1</p>
<p>The initial attempt to place Variation 1 of Tetromino L was incorrect because it was not placed in the top left corner.Upon correcting the placement, we find that Variation 1 does indeed fit, and the Tetromino I can be placed afterward to complete the puzzle.</p>
<p>Figure 2 :
2
Figure 2: Examples of a navigation map under different settings of k, with emoji of house indicating the starting point, and emoji of office indicating the destination.</p>
<p>)</p>
<p>Figure 3: Example of visual tiling with masked polyomino pieces.Variants of those polyomino pieces including rotation and reflection are not shown in this figure.</p>
<p>Figure 4 :
4
Figure 4: Examples of VoT prompting in three tasks, where LLM generates 2D grids as text-form mental images.The generated reasoning traces and visualizations form an interleaved sequence to track the state over time.The 2D grids in the input and responses are composed of special characters.Full responses could be found in Appendix B.</p>
<p>Figure 5: tracking rate of different settings across all tasksk.</p>
<p>Figure 9 :
9
Figure 9: Performance Trends of CoT and VoT Models Across difficulty levels in next-step-prediction task.</p>
<p>Figure 10 :
10
Figure 10: Mental images for state tracking in visual navigation tasks.We found diverse visualization formats in LLMs to track the navigation progress, including marking the path, marking the current location, marking directional steps, and marking temporal steps.More details are available in Appendix E.1</p>
<p>Figure 11 :
11
Figure 11: Representation for 3D semantics and qualitative results of GPT-4 manipulating the mental image correctly.</p>
<p>Let's start by examining the empty spaces in the target rectangle and the provided polyominoes and their variations.We need to fit the Tetromino I, Tetromino L, and Tetromino T into the empty spaces without overlapping any existing polyominoes or going outside the rectangle.The target rectangle with 12 empty squares looks like this:</p>
<p>Figure 14 :Figure 15 :Figure 15 :
141515
Figure 14: Responses of Visual Tiling</p>
<p>Figure 16 :
16
Figure 16: Inconsistency between language and visualization</p>
<p>Figure 17 :
17
Figure 17: Example case of Self-refine</p>
<p>Figure 17 :
17
Figure 17: Example case of Self-refine</p>
<p>Table 1 :
1
and table 6 in appendix.Performance of different GPT-4/4V settings in all tasks.Underline denotes statistical significance with p &lt; 0.05 when comparing GPT-4 VoT against all baselines using two-sample z-test, while p &lt; 0.16 is observed compared with GPT-4 CoT in natural language navigation task.As explained in section 3, one of the core aspects of VoT lies in enabling LLMs with visual state tracking.During the experiments, it was observed that GPT-4 CoT occasionally exhibited this reason-
SettingsVisual Navigation Route PlanningNext StepVisual TilingNatural-Language NavigationCompleting Rate Succ RatePredictionGPT-4 CoT37.029.4848.6154.1554.00GPT-4 w/o Viz37.1710.2848.4946.9835.50GPT-4V CoT33.365.6546.5949.62/GPT-4 VoT40.7714.7255.2863.9459.005 Analysis
ing pattern across several tasks with exception of route planning.Besides, incorrect visualizations of VoT are commonly observed in model outputs.In this section, our analysis of VoT primarily focuses on three questions: (1) Do visual state tracking behaviors differ among prompting methods?(2) How visualizations enhance final answers?(3) Can VoT benefit less powerful language models?5.1 Do visual state tracking behaviors differ among prompting methods?</p>
<p>Table 2 ,
2
LLMs demonstrate promising potential in performing multi-hop visualization while adhering to spatial constraints, with compliance rates of approximately 51-52%.However, the relatively low accuracy of state visualization (around 24%-26%) indicates a need for significant improvements in this area.Despite this limitation, LLMs are able to make correct decisions in 65%-77% of the cases when accurate internal state visualizations are generated, which enhances groundedness and contributes to notable performance gains.Several case studies are provided in Appendix E for interested readers.
TaskSpatial VisualizationSpatial UnderstandingCompliance AccuracyAccuracyVisual Navigation51.1426.4865.16Visual Tilling52.0124.2577.20</p>
<p>Table 2 :
2
Spatial visualization/understanding evaluation in visual navigation and visual tiling task.</p>
<p>On the other hand, VoT prompting might underperform in those tasks where LLMs can leverage logical reasoning without visualizing internal states.We conducted experiments in natural language navigation within a ring [YBL + 23], where navigation instructions are either clockwise or counterclockwise movements.By normalizing each instruction to a signed number, GPT-4 converts this task to mathematical calculation of adding and modulus operation.For example, instructions of 15 steps clockwise and 3 steps counter-clockwise are normalized to (15 -3) % 12. Results show that GPT-4 CoT outperforms GPT-4 VoT with 52.5% VS 49.5% among 200 test instances with ring size of 12.</p>
<p>Table 3 :
3
[Cho19]ance of VoT in GPT-3.5 and LLAMA3 models.Underline denotes statistical significance with p &lt; 0.05 compared to corresponding CoT baseline using two-sample z-test.geometricpatterns[Cho19]and3D spatial information [AMKK21, HZC + 23].[FML + 22] investigated spatial reasoning capabilities of transformer-based models in the UI grounding setting.On the other hand, some works adopted in-context learning, leveraging LLMs for general purpose reasoning to convert spatial information to logic forms [YIL23], or as a general pattern machine for sequence transformation [MXF + 23].Recently, several works focused on evaluating spatial reasoning of LLMs as cognitive capability on navigation [YBL + 23] and planning tasks [MHV + 24] among various spatial structures.While most existing works rely on linguistic semantics and verbal reasoning, and might not always necessitate spatial awareness, we propose to elicit mind's eye of LLMs in spatial reasoning tasks with various formats from a cognitive perspective.The VoT prompting induces LLMs to create mental images for visualizing their internal states and inform subsequent reasoning step.
Spatial Reasoning over Text Spatial reasoning and spatial language understanding [KPM20] inNLP domain mainly focus on semantic representation [CBGG97, Bat10, HK11], spatial information
[NLW23],MK22]KFP19]OM11], learning and reasoning[KM15,SLYA17,KFP19].Recent advancements have further explored spatial reasoning within the context of large language models (LLMs).To improve multi-hop spatial reasoning skills of language models, several works[MFNK21,MK22]proposed to pretrain language models with synthetic datasets.An increasing number of dataset were then developed to covers various type of spatial relations in 2D visual scenes [WBC + 15, SZL22],World Models of LLMs While there have been many theoretical debates about whether LLMs can effectively learn an internal world model from ungrounded form alone [BHT + 20, MGSS21],[LeC22]advocated that world models should represent percepts and action plans at multiple levels of abstraction and multiple time scales, with the capability of planning, predicting, and reasoning.[LWG+22]proposedtoground LLM in the physical world by reasoning over the experimental results predicted by external simulation.[HGM+23]furtherleveraged LLMs as world models to predict the subsequent states by action simulation, given predefined states and actions per task.On the other hand, an increasing number of studies focus on investigating internal representations of LLMs.[PP22,AKH + 21] showed that by utilizing in-context learning, LLMs' learned representations can be mapped to grounded perceptual and conceptual structure in color and spatial domains.Moreover,[GT23]and[NLW23]discovered linear representations of space, time and game state in specifically trained LLMs, which are important for dynamic causal world models.Our work does not probe the internal representations of specialized LLMs, nor does it depend on external simulation engine or state definitions.We demonstrate LLMs' zero-shot capability of representing their precepts at an abstract level, predicting and tracking the internal states over time to generate action plans in multi-hop spatial reasoning tasks, which possibly mirrors the causal world model within LLMs.</p>
<p>Table 4 :
4
Data distribution of visual navigation dataset with the total navigating step of k indicating difficulty level.The reason why the number of generated map is slightly lower than 2 k+1 for k &gt; 5 is explained in Appendix A.1.
Mask count Total23Configuration248124376QA Instance489307796Table 5: Details of visual tilingdataset. Some QA instances arediscarded when multiple solutionsexist and all answers are correct.</p>
<p>Table 6 :
6
CoT and VoT performance of advanced models in next-step-prediction task across various difficulty levels.While performance drops as difficulty level increases, VoT method generally maintains a higher accuracy compared to CoT, highlighting its robustness in more challenging scenarios.
K-step Map Map Count CoT Accuracy (%) VoT Accuracy (%)2875.0075.0033268.7562.50GPT-449660.4268.75525650.7864.06664052.3455.167148845.3052.692862.5062.5033268.7565.63LLama3-70B49660.4262.50525656.2557.42664048.5954.847148846.7152.35
†  We use this term for simplicity. In natural language navigation tasks, LLMs often output additional words in the extracted answer besides the expected object name. For example, "Answer: You will find ...". In this case, sub-string matching is adopted without affecting the correctness. Otherwise, exact matching is adopted for multiple choice questions in visual tasks.
(a) Response of Natural Language Navigation -Page 1
• NextStep Prediction Task instruction in Figure6, response in Figure13.• Visual Tiling Task instruction in Figure7, response in Figure14.Navigation Task: for a provided map, is the home as starting point, is the office as the destination.means the road, means the obstacle.There exists one and only one viable route for each map.Each step you choose a direction and move to the end of the continuous road or the destination.map: <code>S tarting from , provide the steps to navigate to .Visualize the state after each reasoning step.</code>S tarting from , to navigate to , you made following movements: 1. Move right to the end of continuous road.----------------------------------------------------------------------------------------------Variations for Tetromino T: Variation 2 fitting into its bounding box: ```V ariation 3 fitting into its bounding box: ````------------------------B.2 Natural Language NavigationPrompt Example You have been given a 3 by 3 square grid.Starting from a vertex, you will move along the edges of the grid.Initially, you are positioned at the bottom-left corner of the grid, where you will find a torch, then you go right, where you will find an infant bed, then you go right, where you will find an American dipper.Then you go up, where you will find a jay, then you go left, where you will find a terrapin, then you go left, where you will find a microwave oven.Then you go up, First, let's look at the target rectangle and identify the empty squares ( ) where we need to fit the polyominoes:We have two polyominoes to fit into this rectangle: Tetromino I ( ) and Tetromino T ( ).The Tetromino I can only fit in the bottom row since it's the only place with four consecutive empty squares, either horizontally or vertically.Let's place the Tetromino I ( ) in the bottom row: …..We can see that Variation 2 of Tetromino T ( ) fits perfectly into the remaining empty squares, as it matches the T shape formed by the empty squares.Let's place Variation 2 of Tetromino T ( ) into the target rectangle:The correct variation of Tetromino T to fit into the empty squares is Variation 2. Answer: A. 2 First, let's look at the empty spaces in the target rectangle.We have a 4x1 space at the bottom and three single squares above it.The Tetromino I ( ) can only fit in the 4x1 space at the bottom, so we'll use Variation 2 of the Tetromino I to fill that space.Now, let's consider the Tetromino T ( ) variations.We have three single empty squares that are not in a straight line, so Variation 2 of the Tetromino T, which is a straight line with a protrusion, will not fit.However, Variation 3 of the Tetromino T has a configuration that can fit into the three remaining empty squares with one square protruding to the side, which matches the layout of the empty squares.Therefore, the correct variation of Tetromino T to fit into the remaining empty squares is:The final configuration would look like this:Visualize the state after each reasoning stepLet's think step by step.(a) Common behaviors of VoT and CoT prompting in visual tiling task, with the overall track rate of 87.1% and 57.4% respectively.To navigate from to , we need to find the path that avoids obstacles ( ) and follows the roads ( ).Here's the step-by-step navigation:1. Move down from to the end of the continuous road.After the move:2. Move left from the current position to the office .After the move:The has now reached the , completing the navigation task.Visualize the state after each reasoning step Visualize the state after each reasoning stepTo navigate from to on the provided map, follow these instructions:1. Move down to the end of the continuous road.2. Move right to the end of the continuous road.
Can language models encode perceptual structure without grounding? a case study in color. Akh + 21] Mostafa, Artur Abdou, Daniel Kulmizev, Stella Hershcovich, Ellie Frank, Anders Pavlick, Søgaard, arXiv:2109.061292021arXiv preprint</p>
<p>Scanqa: 3d question answering for spatial scene understanding. Daich Azuma, Taiki Miyanishi, Shuhei Kurita, Motoaki Kawanabe, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022. 2021</p>
<p>Working memory. Alan Baddeley, Science. 25550441992</p>
<p>Language and space: a two-level semantic approach based on principles of ontological engineering. John A Bateman, International Journal of Speech Technology. 132010</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, J Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Y Lee, Yuan-Fang Li, Scott M Lundberg, Harsha Nori, H Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv.org. 2023BCE + 23</p>
<p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, arXiv:2004.10151Aleksandr Nisnevich, et al. Experience grounds language. 2020arXiv preprintBHT + 20</p>
<p>Abstract shape representation in human visual perception. Nicholas Baker, Philip J Kellman, Journal of Experimental Psychology: General. 147912952018</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T Henighan, Rewon Child, A Ramesh, Daniel M Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, S Mateusz Litwin, Benjamin Gray, Jack Chess, Christopher Clark, Sam Berner, Alec Mccandlish, I Radford, Dario Sutskever, Amodei, Neural Information Processing Systems. 2020BMR + 20</p>
<p>Representing and reasoning with qualitative spatial relations about regions. Brandon Anthony G Cohn, John Bennett, Nicholas M Gooday, Gotts, Spatial and temporal reasoning. Springer1997</p>
<p>On the measure of intelligence. François Chollet, 2019</p>
<p>An extensible sat-solver. Niklas Eén, Niklas Sörensson, ; R Freedman, Joseph B Mueller, Jack Ladwig, Steven Johnston, David Mcdonald, H Wauck, Ruta Wheelock, Hayley Borck, arXiv.orgA symbolic representation of human posture for interpretable learning and reasoning. Springer2003. 2022International conference on theory and applications of satisfiability testing</p>
<p>A map of abstract relational knowledge in the human hippocampal-entorhinal cortex. Mona M Garvert, Raymond J Dolan, Timothy Ej Behrens, 20176e17086</p>
<p>Berkmin: A fast and robust sat-solver. Eugene Goldberg, Yakov Novikov, Discrete Applied Mathematics. 155122007</p>
<p>Tiling with polyominoes. Solomon W Golomb, Journal of Combinatorial Theory. 121966</p>
<p>Language models represent space and time. Wes Gurnee, Max Tegmark ; Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. 2023. 2023HGM + 23</p>
<p>Towards linguistically-grounded spatial logics. Joana Hois, Oliver Kutz, Dagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum fÃ1/4r Informatik. 2011</p>
<p>3d-llm: Injecting the 3d world into large language models. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan, ; Albert, Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023. 2023HZC + 23</p>
<p>Unveiling the abstract format of mnemonic representations. Yuna Kwak, Clayton E Curtis, Neuron. 110112022</p>
<p>Combining deep learning and qualitative spatial reasoning to learn complex structures from sparse examples with noise. Nikhil Krishnaswamy, Scott Friedman, James Pustejovsky, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Large language models are zero-shot reasoners. Shixiang Kgr + 23] Takeshi Kojima, Machel Shane Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023</p>
<p>Global machine learning for spatial ontology population. Parisa Kordjamshidi, Marie-Francine Moens, Journal of Web Semantics. 302015</p>
<p>Dancing links. Donald E Knuth, arXiv preprint cs/00110472000</p>
<p>Representation, learning and reasoning on spatial language for downstream nlp tasks. J Parisa Kordjamshidi, Marie-Francine Pustejovsky, Moens, Conference on Empirical Methods in Natural Language Processing. 2020</p>
<p>Spatial role labeling: Towards extraction of spatial relations from natural language. Parisa Kordjamshidi, Martijn Van Otterlo, Marie-Francine Moens, ACM Transactions on Speech and Language Processing. 832011TSLP)</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. M Brenden, Marco Lake, Baroni, 2018</p>
<p>A path towards autonomous machine intelligence version 0. Yann Lecun, Open Review. 912022</p>
<p>Cyprien de Masson d'Autume, Phil Blunsom, and Aida Nematzadeh. A systematic investigation of commonsense knowledge in large language models. Lorraine Xiang, Adhiguna Li, Jordan Kuncoro, Hoffmann, 2022LKH + 22</p>
<p>Mind's eye: Grounded language model reasoning through simulation. Lwg + 22] Ruibo, Jason Liu, Shixiang Wei, Te-Yen Shane Gu, Soroush Wu, Claire Vosoughi, Denny Cui, Andrew M Zhou, Dai, 2022</p>
<p>Spartqa: A textual question answering benchmark for spatial reasoning. Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning, Kordjmashidi, 2021the Association for Computational LinguisticsNorth American Chapter</p>
<p>Provable limitations of acquiring meaning from ungrounded form: What will future language models understand?. William Merrill, Yoav Goldberg, Roy Schwartz, Noah A Smith, Evaluating cognitive maps and planning in large language models with cogeval. Ida Momennejad, Hosein Hasanbeig, Felipe Vieira, Hiteshi Sharma, Robert Osazuwa Ness, Nebojsa Jojic, Hamid Palangi, Jonathan Larson, 2021. 20249MHV + 24</p>
<p>Imagining predictions: mental imagery as mental emulation. T Samuel, Stephen M Moulton, Kosslyn, Philosophical Transactions of the Royal Society B: Biological Sciences. 3641521. 2009</p>
<p>Transfer learning with synthetic corpora for spatial role labeling and reasoning. Roshanak Mirzaee, Parisa Kordjamshidi, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. Mxf + 23] Suvir, Fei Mirchandani, Pete Xia, Brian Florence, Danny Ichter, Driess, 2023</p>
<dl>
<dt>Emergent linear representations in world models of self-supervised sequence models. Neel Nanda, Andrew Lee, Martin Wattenberg, arXiv:2309.009412023arXiv preprint</dt>
<dd>
<p>Openai, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Hendrik Kim, Jamie Kirchner, Matt Kiros, Daniel Knight, Łukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, ; Mayne, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Vinnie Mishkin, Evan Monaco, Daniel Morikawa, Tong Mossing, Mira Mu, Oleg Murati, David Murk, Ashvin Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Ouyang, O' Cullen, Jakub Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Giambattista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Kyla Perelman, Toki Sheppard, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin Sohl, Yang Sokolowsky, Natalie Song, Staudacher, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Lauren Wong, Sherwin Workman, Jeff Wu, Michael Wu, Kai Wu, Tao Xiao, Sarah Xu, Kevin Yoo, Qiming Yu, Wojciech Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Tianhao Zhao, Juntang Zheng, William Zhuang, Barret Zhuk, Zoph, OA + 23. Felipe Petroski Such. Filipe De Avila Belbute Peres, Michael Petrov, Henrique Ponde De Oliveira, Pinto, Michael, Michelle Pokorny, Vitchyr Pokrass, Tolly Pong, Alethea Powell, Boris Power, Elizabeth Power, Raul Proehl, Alec Puri, Jack Radford, Aditya Rae, Cameron Ramesh, Francis Raymond, Kendra Real, Carl Rimbach, Bob Ross, Henri Rotsted, Nick Roussez, Mario Ryder, Ted Saltarelli, Shibani Sanders, Girish Santurkar, Heather Sastry, David Schmidt, John Schnurr, Daniel Schulman, Selsam, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok; Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak; Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek; Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason2023Madeleine Thompson, Phil Tillet, Amin Tootoonchian ; Juan Felipe Cerón Uribe, Andrea Vallone, Arun VijayvergiyaGpt-4 technical report</p>
</dd>
</dl>
<p>Gpt-4v(ision) system card. 2023OpenAI</p>
<p>Mapping language models to grounded conceptual spaces. Roma Patel, Ellie Pavlick, International Conference on Learning Representations. 2022</p>
<p>A benchmark for systematic generalization in grounded language understanding. Jacob Rab + 20] Laura Ruis, Marco Andreas, Diane Baroni, Brenden M Bouchacourt, Lake, Advances in neural information processing systems. 202033</p>
<p>Explaining code using ascii art. John Regehr, 2019</p>
<p>Grounding natural language instructions: Can large language models capture spatial information?. Julia Rozanova, Deborah Ferreira, Krishna Dubba, Weiwei Cheng, Dell Zhang, Andre Freitas, 2021RFD + 21</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, 2021RKH + 21</p>
<p>Visually guided spatial relation extraction from text. Taher Rahgooy, Umar Manzoor, Parisa Kordjamshidi, Proceedings of the 2018 Conference of the North American Chapter. Short Papers. the 2018 Conference of the North American ChapterHuman Language Technologies20182</p>
<p>A logical approach to building dungeons: Answer set programming for hierarchical procedural content generation in roguelike games. J Anthony, Joanna J Smith, Bryson, Proceedings of the 50th Anniversary Convention of the AISB. the 50th Anniversary Convention of the AISB2014</p>
<p>A chronometric study of mental paper folding. N Roger, Christine Shepard, Feng, Cognitive psychology. 321972</p>
<p>The mental image. Shepard Roger, American psychologist. 3321251978</p>
<p>A corpus of natural language for visual reasoning. Alane Suhr, Mike Lewis, James Yeh, Yoav Artzi, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Regina Barzilay, Min-Yen Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20172Short Papers)</p>
<p>Let's consider the variations for Tetromino L: Variation 1: <code></code> Variation 6: <code></code> We also have the variations for Tetromino I, but since it's a straight line, it can only fit in one way in the remaining space after placing Tetromino L. Roger N Shepard, Jacqueline Metzler, Science. 1711971Let's start by visualizing the target rectangle with the empty squares ( ) and the existing polyominoes: <code></code> We have two polyominoes to fit into this rectangle: Tetromino L ( ) and Tetromino I (. Let's try to fit Variation 1 of Tetromino L first: ``` (a) Example case of Self-refine -Page 1</p>            </div>
        </div>

    </div>
</body>
</html>
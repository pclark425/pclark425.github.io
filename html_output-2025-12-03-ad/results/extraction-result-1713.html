<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1713 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1713</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1713</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-273350802</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.10934v1.pdf" target="_blank">Agent-as-a-Judge: Evaluate Agents with Agents</a></p>
                <p><strong>Paper Abstract:</strong> Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1713.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1713.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent-as-a-Judge framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic evaluator that uses modular agent components (graph, locate, read, retrieve, ask, etc.) to produce per-requirement binary judgments (<SATISFIED> / <UNSATISFIED>) with evidence, applied to evaluate code-generating agents on the DevAI benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Agent-as-a-Judge (agentic evaluator producing binary per-requirement judgments with evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Single binary judgment per requirement with concise justification ("<SATISFIED>" / "<UNSATISFIED>") using modular context-gathering (read/locate/graph/etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>generated source code and workspace artifacts (files, saved models, reports)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>AI development tasks (multi-file Python-centric development workspaces)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>365 hierarchical, mostly-binary requirements (milestones) plus 125 softer preferences; per-requirement satisfied/unsatisfied judgments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Three expert evaluators independently assessed outputs, then held a discussion to reach a consensus used as the final human label</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>AI experts / software-development experts (authors' team members recruited as expert evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Alignment Rate (percentage agreement with Human-as-a-Judge consensus), Judge Shift (absolute deviation from consensus), PR curves (precision-recall) and pairwise disagreement rates among humans</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Overall alignment ~90% with human consensus; example for OpenHands: 92.07% (gray-box) and 90.44% (black-box). Example judge-shift as low as 0.27% on some requirements. Cost/time: Agent-as-a-Judge $30.58, 118.43 minutes vs Human $1297.50, 86.5 hours.</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Gray-box setting with access to execution trajectories; inclusion of read and locate modules (direct file content access and targeted file location); explicit binary, hierarchical requirements; lower/noisy evidence environments</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Black-box setting without trajectories; removal of locate/read modules; addition of noisy or low-precision retrieval/search; use of memory/planning that propagates prior errors; class imbalance (rare positives) without careful PR analysis</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Agent-as-a-Judge performed robustly across tasks and especially outperformed LLM-as-a-Judge on tasks with dependencies; search module was less useful (and sometimes harmful) in simple/small workspaces but is expected to become valuable as workspace complexity grows.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clear, explicit, binary hierarchical requirements improved alignment — the ablations show modules that enable precise evidence retrieval (read, locate) strongly increase Alignment Rate (e.g., adding read then locate raised alignment up to ~90%).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>55 tasks (DevAI), 365 requirements</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Pairwise human disagreement rates ranged ~10%–30%; after discussion consensus used as final label (majority vote reduced error to 6.01% vs consensus).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Agent-as-a-Judge aligned more closely with the human consensus (~90% Alignment Rate) than LLM-as-a-Judge (~60–70%) and in some cases outperformed individual human evaluators, aligning closest with majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No calibration/fine-tuning on human judgments reported for this proof-of-concept; Agent-as-a-Judge is a designed modular system rather than a trained judge.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Agent-as-a-Judge substantially outperforms LLM-as-a-Judge in alignment with human consensus (example: for OpenHands Agent ~90–92% vs LLM ~60–71%), shows very low judge-shift on some requirements (down to 0.27%), and can match or exceed single-human performance while reducing evaluation cost/time dramatically (~97% cost/time savings vs three human experts). Ablations show read and locate modules are the largest contributors to alignment; search can hurt in small/simple workspaces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Current proof-of-concept limited to relatively small workspaces (hundreds of LOC); memory and planning modules introduced error chains and were detrimental in experiments; gray-box evidence (trajectories) improves performance but may be impractical to obtain in all settings; class imbalance of satisfied vs unsatisfied requirements can mislead simple metrics so PR curves are necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1713.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1713.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline automated evaluator that uses a single LLM to judge artifacts (here used to evaluate per-requirement satisfaction), faster but lacking agentic context selection that Agent-as-a-Judge provides.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-Judge (single LLM-based judge producing requirement judgments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Single per-requirement judgment (binary) with justification; simpler context selection compared to Agent-as-a-Judge (no modular locate/read pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>generated source code and workspace artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>AI development tasks (Python-centric)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same DevAI hierarchical/ binary requirement set (365 requirements)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Compared against the same three-expert human consensus baseline</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>AI experts / software-development experts</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Alignment Rate (agreement with human consensus), Judge Shift, PR curves; also reported pairwise human disagreement to contextualize results</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Lower alignment than Agent-as-a-Judge: example OpenHands alignment reported as 70.76% (gray-box) and 60.38% (black-box). In some class-imbalanced scenarios (MetaGPT rarely meets requirements), LLM-as-a-Judge could trivially score high on negative detection (example: 84.15% in one black-box setting) but this is misleading without PR analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>When artifacts are clearly negative (class imbalance with many failures) or when evidence needed is trivially discriminative; gray-box access to trajectories helps but LLM-as-a-Judge lags due to poorer context selection.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Complex multi-step tasks with dependencies, ambiguous or subtle requirements, lack of intelligent context selection (no locate/read), noisy retrieved context, and black-box settings without trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Agreement degrades on tasks with dependencies and higher complexity; LLM-as-a-Judge underperforms strongly compared to Agent-as-a-Judge on dependency-aware evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clear binary criteria help but LLM-as-a-Judge is limited by context selection (cannot reliably locate relevant snippets/evidence), reducing effective alignment even for clear criteria if the right evidence is not presented.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>55 tasks (DevAI), 365 requirements</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>See human baseline: pairwise disagreement ~10%–30%; majority vote error 6.01% vs consensus</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>LLM-as-a-Judge aligns substantially worse with human consensus (~60–71% in examples) than Agent-as-a-Judge (~90%), and is less consistent than the human majority vote; faster but less reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-as-a-Judge is cheaper and faster (10.99 minutes reported) but achieves notably lower alignment with human consensus compared to Agent-as-a-Judge. It is prone to poor context selection and can be misled in class-imbalanced evaluation settings; PR-curve analysis is necessary to properly evaluate its performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Lacks intelligent context selection modules (locate/read), shows large judge-shift on dependency tasks, sensitive to how context is provided (gray-box vs black-box), and can be misleadingly high on simple negative-detection when positives are rare.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1713.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1713.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert consensus evaluation (Human-as-a-Judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual human evaluation baseline: three expert human evaluators independently assess whether each requirement is satisfied, then hold discussion to reach a final consensus used as the ground-truth comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Human expert manual evaluation (consensus)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>generated source code and workspace artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>AI development tasks (Python-centric)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>365 hierarchical binary requirements plus preferences; evaluators used templates and minimal initial instructions then discussed disagreements to reach consensus</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Three expert evaluators (231a, 38bb, cn90) independently labelled artifacts (first round), then met and debated to produce a consensus label (second round). Consensus was treated as final human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>AI/ML/software development experts (authors recruited as expert evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pairwise disagreement rate between human evaluators (percentage), majority-vote mismatch vs consensus (error rate), and total hours/cost for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Pairwise human disagreement ranged ~10%–30%; majority vote reduced overall error to 6.01% relative to the consensus labels. Total human evaluation time ~86.5 hours and estimated cost ~$1297.50 (at $15/hr proxy).</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>After discussion/debate (second round), consensus labels were more stable; majority voting across multiple independent human evaluators reduces individual errors; clear and explicit criteria reduce disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Single human evaluator without peer discussion is unreliable (high disagreement); ambiguous / subjective preferences increase disagreement; missing runtime/trajectory information or complex multi-step tasks increase errors.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Complex, multi-step AI development tasks lead to more human disagreement because of missing evidence, stepwise dependencies, and differing interpretations of ambiguous requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Explicit, binary, and dependency-structured requirements reduced ambiguity and helped produce more consistent human judgments; minimal instruction in the first round contributed to observed variance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>55 tasks (DevAI), 365 requirements</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Pairwise disagreement ~10%–30%; majority vote mismatch reduced to 6.01% relative to consensus</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Agent-as-a-Judge aligns with the human consensus at ~90% — closer than LLM-as-a-Judge (~60–71%); Agent-as-a-Judge sometimes matched or exceeded single-human agreement and aligned closely with majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human evaluation is reliable after debate/consensus but is time-consuming and costly; individual human evaluators show substantial disagreement (10–30%). Majority voting and discussion reduce error; humans remain the reference standard but ensemble/multi-round human procedures are needed to reduce error.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human evaluation is expensive and slow (~86.5 hours, ~$1.3k). Single-human judgments are noisy; majority voting with small panels still leaves ~5% error vs consensus; achieving high-quality consensus requires substantial discussion time.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks. <em>(Rating: 2)</em></li>
                <li>Chateval: Towards better llm-based evaluators through multi-agent debate. <em>(Rating: 2)</em></li>
                <li>Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1713",
    "paper_id": "paper-273350802",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "Agent-as-a-Judge",
            "name_full": "Agent-as-a-Judge framework",
            "brief_description": "An agentic evaluator that uses modular agent components (graph, locate, read, retrieve, ask, etc.) to produce per-requirement binary judgments (&lt;SATISFIED&gt; / &lt;UNSATISFIED&gt;) with evidence, applied to evaluate code-generating agents on the DevAI benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "Agent-as-a-Judge (agentic evaluator producing binary per-requirement judgments with evidence)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": "Single binary judgment per requirement with concise justification (\"&lt;SATISFIED&gt;\" / \"&lt;UNSATISFIED&gt;\") using modular context-gathering (read/locate/graph/etc.)",
            "artifact_type": "generated source code and workspace artifacts (files, saved models, reports)",
            "artifact_domain": "AI development tasks (multi-file Python-centric development workspaces)",
            "evaluation_criteria": "365 hierarchical, mostly-binary requirements (milestones) plus 125 softer preferences; per-requirement satisfied/unsatisfied judgments",
            "human_evaluation_setup": "Three expert evaluators independently assessed outputs, then held a discussion to reach a consensus used as the final human label",
            "human_expert_count": 3,
            "human_expert_expertise": "AI experts / software-development experts (authors' team members recruited as expert evaluators)",
            "agreement_metric": "Alignment Rate (percentage agreement with Human-as-a-Judge consensus), Judge Shift (absolute deviation from consensus), PR curves (precision-recall) and pairwise disagreement rates among humans",
            "agreement_score": "Overall alignment ~90% with human consensus; example for OpenHands: 92.07% (gray-box) and 90.44% (black-box). Example judge-shift as low as 0.27% on some requirements. Cost/time: Agent-as-a-Judge $30.58, 118.43 minutes vs Human $1297.50, 86.5 hours.",
            "high_agreement_conditions": "Gray-box setting with access to execution trajectories; inclusion of read and locate modules (direct file content access and targeted file location); explicit binary, hierarchical requirements; lower/noisy evidence environments",
            "low_agreement_conditions": "Black-box setting without trajectories; removal of locate/read modules; addition of noisy or low-precision retrieval/search; use of memory/planning that propagates prior errors; class imbalance (rare positives) without careful PR analysis",
            "artifact_complexity_effect": "Agent-as-a-Judge performed robustly across tasks and especially outperformed LLM-as-a-Judge on tasks with dependencies; search module was less useful (and sometimes harmful) in simple/small workspaces but is expected to become valuable as workspace complexity grows.",
            "criteria_clarity_effect": "Clear, explicit, binary hierarchical requirements improved alignment — the ablations show modules that enable precise evidence retrieval (read, locate) strongly increase Alignment Rate (e.g., adding read then locate raised alignment up to ~90%).",
            "sample_size": "55 tasks (DevAI), 365 requirements",
            "inter_human_agreement": "Pairwise human disagreement rates ranged ~10%–30%; after discussion consensus used as final label (majority vote reduced error to 6.01% vs consensus).",
            "proxy_vs_human_comparison": "Agent-as-a-Judge aligned more closely with the human consensus (~90% Alignment Rate) than LLM-as-a-Judge (~60–70%) and in some cases outperformed individual human evaluators, aligning closest with majority voting.",
            "calibration_or_training": "No calibration/fine-tuning on human judgments reported for this proof-of-concept; Agent-as-a-Judge is a designed modular system rather than a trained judge.",
            "key_findings": "Agent-as-a-Judge substantially outperforms LLM-as-a-Judge in alignment with human consensus (example: for OpenHands Agent ~90–92% vs LLM ~60–71%), shows very low judge-shift on some requirements (down to 0.27%), and can match or exceed single-human performance while reducing evaluation cost/time dramatically (~97% cost/time savings vs three human experts). Ablations show read and locate modules are the largest contributors to alignment; search can hurt in small/simple workspaces.",
            "limitations_noted": "Current proof-of-concept limited to relatively small workspaces (hundreds of LOC); memory and planning modules introduced error chains and were detrimental in experiments; gray-box evidence (trajectories) improves performance but may be impractical to obtain in all settings; class imbalance of satisfied vs unsatisfied requirements can mislead simple metrics so PR curves are necessary.",
            "uuid": "e1713.0",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "LLM-as-a-Judge baseline",
            "brief_description": "A baseline automated evaluator that uses a single LLM to judge artifacts (here used to evaluate per-requirement satisfaction), faster but lacking agentic context selection that Agent-as-a-Judge provides.",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-Judge (single LLM-based judge producing requirement judgments)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": "Single per-requirement judgment (binary) with justification; simpler context selection compared to Agent-as-a-Judge (no modular locate/read pipeline)",
            "artifact_type": "generated source code and workspace artifacts",
            "artifact_domain": "AI development tasks (Python-centric)",
            "evaluation_criteria": "Same DevAI hierarchical/ binary requirement set (365 requirements)",
            "human_evaluation_setup": "Compared against the same three-expert human consensus baseline",
            "human_expert_count": 3,
            "human_expert_expertise": "AI experts / software-development experts",
            "agreement_metric": "Alignment Rate (agreement with human consensus), Judge Shift, PR curves; also reported pairwise human disagreement to contextualize results",
            "agreement_score": "Lower alignment than Agent-as-a-Judge: example OpenHands alignment reported as 70.76% (gray-box) and 60.38% (black-box). In some class-imbalanced scenarios (MetaGPT rarely meets requirements), LLM-as-a-Judge could trivially score high on negative detection (example: 84.15% in one black-box setting) but this is misleading without PR analysis.",
            "high_agreement_conditions": "When artifacts are clearly negative (class imbalance with many failures) or when evidence needed is trivially discriminative; gray-box access to trajectories helps but LLM-as-a-Judge lags due to poorer context selection.",
            "low_agreement_conditions": "Complex multi-step tasks with dependencies, ambiguous or subtle requirements, lack of intelligent context selection (no locate/read), noisy retrieved context, and black-box settings without trajectories",
            "artifact_complexity_effect": "Agreement degrades on tasks with dependencies and higher complexity; LLM-as-a-Judge underperforms strongly compared to Agent-as-a-Judge on dependency-aware evaluations.",
            "criteria_clarity_effect": "Clear binary criteria help but LLM-as-a-Judge is limited by context selection (cannot reliably locate relevant snippets/evidence), reducing effective alignment even for clear criteria if the right evidence is not presented.",
            "sample_size": "55 tasks (DevAI), 365 requirements",
            "inter_human_agreement": "See human baseline: pairwise disagreement ~10%–30%; majority vote error 6.01% vs consensus",
            "proxy_vs_human_comparison": "LLM-as-a-Judge aligns substantially worse with human consensus (~60–71% in examples) than Agent-as-a-Judge (~90%), and is less consistent than the human majority vote; faster but less reliable.",
            "calibration_or_training": null,
            "key_findings": "LLM-as-a-Judge is cheaper and faster (10.99 minutes reported) but achieves notably lower alignment with human consensus compared to Agent-as-a-Judge. It is prone to poor context selection and can be misled in class-imbalanced evaluation settings; PR-curve analysis is necessary to properly evaluate its performance.",
            "limitations_noted": "Lacks intelligent context selection modules (locate/read), shows large judge-shift on dependency tasks, sensitive to how context is provided (gray-box vs black-box), and can be misleadingly high on simple negative-detection when positives are rare.",
            "uuid": "e1713.1",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Human-as-a-Judge",
            "name_full": "Human expert consensus evaluation (Human-as-a-Judge)",
            "brief_description": "Manual human evaluation baseline: three expert human evaluators independently assess whether each requirement is satisfied, then hold discussion to reach a final consensus used as the ground-truth comparator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "Human expert manual evaluation (consensus)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "generated source code and workspace artifacts",
            "artifact_domain": "AI development tasks (Python-centric)",
            "evaluation_criteria": "365 hierarchical binary requirements plus preferences; evaluators used templates and minimal initial instructions then discussed disagreements to reach consensus",
            "human_evaluation_setup": "Three expert evaluators (231a, 38bb, cn90) independently labelled artifacts (first round), then met and debated to produce a consensus label (second round). Consensus was treated as final human evaluation.",
            "human_expert_count": 3,
            "human_expert_expertise": "AI/ML/software development experts (authors recruited as expert evaluators)",
            "agreement_metric": "Pairwise disagreement rate between human evaluators (percentage), majority-vote mismatch vs consensus (error rate), and total hours/cost for evaluation",
            "agreement_score": "Pairwise human disagreement ranged ~10%–30%; majority vote reduced overall error to 6.01% relative to the consensus labels. Total human evaluation time ~86.5 hours and estimated cost ~$1297.50 (at $15/hr proxy).",
            "high_agreement_conditions": "After discussion/debate (second round), consensus labels were more stable; majority voting across multiple independent human evaluators reduces individual errors; clear and explicit criteria reduce disagreement.",
            "low_agreement_conditions": "Single human evaluator without peer discussion is unreliable (high disagreement); ambiguous / subjective preferences increase disagreement; missing runtime/trajectory information or complex multi-step tasks increase errors.",
            "artifact_complexity_effect": "Complex, multi-step AI development tasks lead to more human disagreement because of missing evidence, stepwise dependencies, and differing interpretations of ambiguous requirements.",
            "criteria_clarity_effect": "Explicit, binary, and dependency-structured requirements reduced ambiguity and helped produce more consistent human judgments; minimal instruction in the first round contributed to observed variance.",
            "sample_size": "55 tasks (DevAI), 365 requirements",
            "inter_human_agreement": "Pairwise disagreement ~10%–30%; majority vote mismatch reduced to 6.01% relative to consensus",
            "proxy_vs_human_comparison": "Agent-as-a-Judge aligns with the human consensus at ~90% — closer than LLM-as-a-Judge (~60–71%); Agent-as-a-Judge sometimes matched or exceeded single-human agreement and aligned closely with majority voting.",
            "calibration_or_training": null,
            "key_findings": "Human evaluation is reliable after debate/consensus but is time-consuming and costly; individual human evaluators show substantial disagreement (10–30%). Majority voting and discussion reduce error; humans remain the reference standard but ensemble/multi-round human procedures are needed to reduce error.",
            "limitations_noted": "Human evaluation is expensive and slow (~86.5 hours, ~$1.3k). Single-human judgments are noisy; majority voting with small panels still leaves ~5% error vs consensus; achieving high-quality consensus requires substantial discussion time.",
            "uuid": "e1713.2",
            "source_info": {
                "paper_title": "Agent-as-a-Judge: Evaluate Agents with Agents",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks.",
            "rating": 2,
            "sanitized_title": "llms_instead_of_human_judges_a_large_scale_empirical_study_across_20_nlp_evaluation_tasks"
        },
        {
            "paper_title": "Chateval: Towards better llm-based evaluators through multi-agent debate.",
            "rating": 2,
            "sanitized_title": "chateval_towards_better_llmbased_evaluators_through_multiagent_debate"
        },
        {
            "paper_title": "Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges.",
            "rating": 2,
            "sanitized_title": "judging_the_judges_evaluating_alignment_and_vulnerabilities_in_llmsasjudges"
        }
    ],
    "cost": 0.0195215,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Agent-as-a-Judge: Evaluate Agents with Agents
October 18, 2024</p>
<p>Mingchen Zhuge mingchen.zhuge@kaust.edu.sa 
Meta AI</p>
<p>10/10</p>
<p>Changsheng Zhao cszhao@meta.com 
Meta AI</p>
<p>Dylan R Ashley 
10/10</p>
<p>Wenyi Wang 
10/10</p>
<p>Dmitrii Khizbullin 
10/10</p>
<p>Yunyang Xiong 
Meta AI</p>
<p>Zechun Liu 
Meta AI</p>
<p>Ernie Chang 
Meta AI</p>
<p>Raghuraman Krishnamoorthi 
Meta AI</p>
<p>Yuandong Tian 
Meta AI</p>
<p>Yangyang Shi 
Meta AI</p>
<p>Vikas Chandra 
Meta AI</p>
<p>Jürgen Schmidhuber 
10/10</p>
<p>Agentic Task </p>
<p>10/10</p>
<p>Agent-as-a-Judge: Evaluate Agents with Agents
October 18, 202472D0AC832ED7673B0A4F74D6E3F7018EarXiv:2410.10934v2[cs.AI]
KAUSTContemporary evaluation techniques are inadequate for agentic systems.These approaches either focus exclusively on final outcomes-ignoring the step-by-step nature of agentic systems, or require excessive manual labour.To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems.This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process.We apply the Agent-as-a-Judge to the task of code generation.To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks.It includes rich manual annotations, like a total of 365 hierarchical user requirements.We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline.Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems-by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.</p>
<p>Introduction</p>
<p>Recent years have seen multimodal agentic systems move from occasionally being able to solve small toy problems to being regularly deployed for challenging real-world problems (the dream of most AI research).Yet, the current evaluation methods and the available benchmarks for agentic systems are struggling to keep up with these rapid advances, dramatically slowing true progress.</p>
<p>We believe that the current issue with evaluating agentic systems stems from the lack of feedback during the intermediate task-solving stages for these nontraditional systems.Agentic systems think more like humans, often act step-by-step (Wooldridge, 1999) and often host very human-like symbolic communications internally to solve problems (Zhuge et al., 2023).And thus agentic systems should be evaluated like a human, with rich evaluative feedback which looks at the full thought and action trajectory; evaluating an agentic system in the traditional way is like evaluating a student using multiple-choice testing-a comparatively unreliable estimator (Park, 2010).For example, while SWE-Bench (Yang et al., 2024a) is widespread, its evaluation method, which relies solely on the final resolve rate for long-term automated repair tasks, does not effectively pinpoint what is happening within agentic systems that affects the resolve rate.On the other hand, performing a better evaluation with a human is prohibitively expensive.We instead propose that agentic systems should be used to evaluate agentic systems.Inspired by LLM-as-a-Judge (Zheng et al., 2024;Fu et al., 2023;Chen et al., 2024b), which uses LLMs to evaluate LLMs, we call this framework Agent-as-a-Judge, of which it is</p>
<p>Upgrade</p>
<p>LLM-as-a-Judge</p>
<p>If the FED buys bonds in the secondary market (A) the money supply will increase.(B) the money supply will decrease.(C) the money supply will not be affected.(D) the discount rate would be affected.Output the answer only.</p>
<p>User</p>
<p>The Federal Reserve buys bonds in the secondary market to increase the money supply.</p>
<p>(A) the money supply will increase.</p>
<p>a key extension to the world of agentic systems (see Figure 1).It not only retains the cost-effectiveness of LLM-as-a-Judge but is also equipped with agentic features, allowing it to provide rich intermediate feedback throughout the entire process, as it acts as an agentic system.We apply the Agent-as-a-Judge systems to the problem of evaluating code generating systems-one of the areas where agentic systems have looked the most promising recently.</p>
<p>In code generation, the development of benchmarks has also lagged behind the rapid advancement of agentic systems.HumanEval (Chen et al., 2021), for example, focuses exclusively on algorithmic problems, while MBPP (Austin et al., 2021) deals with simple programming tasks.Although they are useful for evaluating the basic skills of foundation models, neither of these two reflects the most practical challenges developers face.As a step away from this, SWE-Bench (Jimenez et al., 2023) did introduce more realistic problems from GitHub, offering a fresh approach to evaluation, but still primarily focuses on automated repairs tasks development process.Concerningly, recent research shows that large language models (LLMs) can already solve over 27% of the tasks in SWE-Bench without needing of advanced agentic systems (Xia et al., 2024).Equally concerning, recent work has begun to introduce mechanisms designed specifically for the individual tasks in the SWE-Bench dataset, leading to a lack of real-world generalization and violating Goodhart's law: "When a measure becomes a target, it ceases to be a good measure" (Goodhart, 1976).</p>
<p>To address the aforementioned issues with the current benchmarks in code generation, we introduce DevAI: the AI Developer Dataset, which contains 55 real-world comprehensive AI app development tasks created by expert annotators.We apply three leading open-source code-generating agentic frameworks to the tasks in DevAI: MetaGPT (Hong et al., 2024b), GPT-Pilot (Pythagora.io, 2023), and OpenHands (Wang et al., 2024d).We evaluate their performance using human judges (a painstaking process), LLM-as-a-Judge (Zheng et al., 2024), and our Agent-as-a-Judge framework.</p>
<p>Through human evaluation, we found that GPT-Pilot and OpenHands were each able to satisfy about 29% of the task requirements in DevAI, but only one full task-showing that DevAI presents a good level of challenge to current systems.When comparing our human judges with our automatic Agent-as-a-Judge framework, we found that Agent-as-a-Judge aligns more closely with the consensus of our human judges (90%) as compared to LLM-as-a-Judge (70%) in all cases tested.In addition, we find that it aligns more closely with this ensemble than the individual human evaluators do, suggesting that-not only is it suitable as a replacement for a human evaluator-but it could in fact be more useful than an average lone human evaluator.</p>
<p>In addition, considering the evaluation cost, Agent-as-a-Judge saves 97.72% of the time and 97.64% of the cost compared to involving three human experts.</p>
<p>In summary, the principal contributions of this work are:</p>
<p>Motivation</p>
<p>Background The code generation domain is an area where agentic systems have seen significant industrial deployment over the past two years (e.g., Devin1 and Cursor2 ).However, in code generation, there isn't yet a benchmark that accurately reflects realistic user queries for developing complete AI systems.We believe this is because of the difficulty to evaluate such complex, real-world tasks.For example, while many companies advertise their systems based on its performance on benchmarks such as SWE-Bench (Yang et al., 2024a) (for automated repair) or HumanEval (Chen et al., 2021) (for algorithmic problems), these benchmarks cover only a small bit of an actual development process.Moreover, none of them accurately reflect the intermediate stages of development or provide sufficient reward signals for long-horizon development-similar issues are present in OpenAI's recent MLE-Bench (Chan et al., 2024).A benchmark that can evaluate the entire development process-ideally in a way that can help understand the degree to which current AI methods can reduce human labour-is missing.</p>
<p>Topic We chose automated AI development as our main topic.While AI and ML tasks are often more complex, they follow clear, standard procedures.For example, data processing typically comes first in an AI pipeline, and performance reporting goes at the end.We believe this topological nature can help better monitor the development process and provide useful signals to the agentic systems.</p>
<p>Goals An ideal benchmark should address critical issues in automated development by focusing on three key factors.First, it should reflect practical software scenarios, where tasks are often too complex for a single LLM, requiring human or agentic systems.Second, it should emphasize the development process, not just final outcomes (e.g., pass@1 rates offer limited feedback and fail to highlight intermediate problems).Lastly, the evaluation should be computationally cost-effective and efficient, avoiding long training times or excessive manual oversight.(2) The first 53 tasks in DevAI all have a one-paragraph query but of varying lengths (note that task 54 and 55 are excluded here as they are outliers, representing the longest and most complex tasks in the dataset).( 3) Each task has one or more tags.The prevalence of supervised learning here reflects the fact that it dominates many machine learning applications.(4) SVM classifiers (Cortes, 1995) and LSTM models (Hochreiter, 1997) are two of the most widely used architectures-a fact reflected by DevAI.</p>
<p>The DevAI Dataset</p>
<p>Motivated by the ideas outlined above, we propose the DevAI dataset.DevAI consists of a curated set of 55 tasks, each defined by (1) a plain text user query that describes an AI development task; (2) a set of plain text requirements (for a total of 365 requirements), each with a set of dependencies connecting them to other requirements; and (3) a set of preferences (for a total of 125 preferences) which represent softer requirements.</p>
<p>DevAI is structured so that an agentic system starts by receiving a user query to begin development.The system is then evaluated on how well it meets the requirements, with preferences serving as optional, softer criteria.An example of one of the DevAI tasks can be seen in Figure 3.</p>
<p>The tasks in DevAI are relatively small-scale but cover commonly used key development techniques.As shown in Figure 2, our tasks are tagged and cover a variety of key areas in AI: supervised learning, reinforcement learning, computer vision, natural language processing, generative models, and others.Each of the tasks is a real-world problem that could be given to a research engineer, while simultaneously being relatively inexpensive computationally to run so as to reduce the cost of evaluating a method on this benchmark.Details of the sample collection and human labeling process for DevAI are provided in Appendix E.</p>
<p>The requirements belonging to each task represent a milestone in the comprehensive development process and are arranged as a directed acyclic graph (similar to the work by He et al. (2021)), with requirements such as visualizing results depending on correct data loading and modeling.This allows for more comprehensive non-sparse feedback than a binary success metric.Furthermore, the inclusion of hierarchical requirements makes simple memorization an inadequate solution strategy, as completing the entire task requires agentic capabilities rather than relying solely on symbolic memorization, as is typical in foundation models.</p>
<p>Preliminary Benchmark</p>
<p>We first conduct experiments to collect development outcomes from different frameworks, which serve as baselines in the DevAI dataset.We test three of the most popular open-source frameworks (which we Task 51: Devin AI Software Engineer Plants Secret Messages in Images Query Hi! Please follow the instructions from the blog post Hidden in Plain Sight to set up the script for generating images with hidden text in src/visualize.py.Ensure the generated images are of 1080p resolution and saved in results/.Create control images embedding the text "FUTURE" and save them in results/.Please manually verify that the hidden text is embedded in the images.will refer to as "AI developers"): MetaGPT (Hong et al., 2024b), GPT-Pilot (Pythagora.io, 2023), and OpenHands (Wang et al., 2024d)-all selected for their strong community acceptance (each having over 30,000 stars on GitHub).</p>
<p>Requirements</p>
<p>Experiment Setup All of these three systems require a language model as a back-end engine, for which we use gpt-4o-2024-05-13, a state-of-the-art language model.These AI developers were given a time-limit of 1800 seconds to solve each task and were forcefully halted if they exceeded this time limit (we imposed this constraint, which was visible to the AI developers, as detailed in Appendix I).We capture the outputs generated during the automated development process, including code, files, and other artifacts.Additionally, we record key decisions and actions made by the agentic systems through some custom instrumentation code, resulting in a development trajectory for each of the agentic systems.</p>
<p>Analysis The basic statistics are shown in Table 1.MetaGPT is the most cost-efficient (1.19 USD), while OpenHands is the most expensive (6.38 USD).In terms of development time, OpenHands completes tasks Evaluations Note that the results in Table 1 are not directly indicative of performance but provide valuable insights into the practical utility of DevAI and the performance of AI developers.The generated workspaces (generated files, code, etc.) and trajectories are utilized in subsequent experiments to perform evaluations using Human-as-a-Judge (section 3), LLM-as-a-Judge, and Agent-as-a-Judge (section 4).</p>
<p>Human-as-a-Judge: Manual Evaluation on DevAI</p>
<p>To determine the pragmatic validity of DevAI and to accurately estimate the actual code-generating abilities of current state-of-the-art agentic systems, in this section, we run and then manually evaluate the application of three AI developer baselines to DevAI.In Section 4, we show how this evaluation can be automated.3.1 Benchmark Baselines by Human-as-a-Judge</p>
<p>Human Evaluation Setup After obtaining the baseline executions and conducting basic statistical analysis, we have three expert human evaluators (referred to here by their anonymous names: 231a, 38bb, and cn90) review the outputs of AI developer baselines to assess whether each requirement was satisfied.We have two rounds of human evaluations.To capture the bias inherent in typical human evaluation (this is desirable to capture here as it represents a likely scenario in deployment), in the first round, our evaluators first discussed the basic standards but were given minimal instructions.The templates the evaluators were given for the evaluation and their self-reported post-hoc descriptions of how they resolved ambiguities are reported in Figure 12 in Appendix H.</p>
<p>After the initial round of human evaluations (which totaled an estimated total of 58 human hours), we asked our evaluators to discuss and reach a consensus on their assessments (which took an estimated total of 28.5 additional human hours).This consensus, achieved after long sessions of debate, was used as the final human evaluation result for each method.</p>
<p>Performance Analysis The results of this experiment are shown in Table 2.We found that the two bestperforming methods (GPT-Pilot and OpenHands) could satisfy about 29% of the requirements (or around 44% if prerequisites are ignored) but only on one task could they meet all the requirements.This highlights that DevAI offers a considerable but appropriate level of challenge for current and future methods.Moreover, the fulfillment of intermediate requirements aligns with our expectations, as discussed in Section 2, that DevAI provides richer feedback by uncovering how agentic systems falter during the process instead of just focusing on a single performance metric at the end.</p>
<p>Judging Human-as-a-Judge</p>
<p>Figure 4 Between the three human evaluators, a large amount of disagreement was observed in their individual evaluations-highlighting the inherent unreliability of a single human evaluation.</p>
<p>Disagreement Analysis</p>
<p>To analyze the presence of inductive bias and the reliability of the Human-as-a-Judge paradigm here, we calculate the disagreement rate between individual evaluators (shown in Figure 4).The results indicate that the disagreement rates between pairs of evaluators range from around 10% to 30%.</p>
<p>Due to the complexity of a complete AI development task, which typically involves multiple steps with varying outcomes at each step, humans can easily make errors when critical information is missed, such as environment feedback indicating small but severe coding errors or bugs.Additionally, some disagreements are not necessarily incorrect but arise from differing perspectives on how ambiguity should be resolved.</p>
<p>Error Analysis As previously noted, the evaluators engaged in a round of debating after their initial evaluations until they reached a consensus on each requirement in each task (with the results of this consensus evaluation shown in Table 2).</p>
<p>In our Human-as-a-Judge pipeline, evaluators could be convinced by evidence from others and acknowledge their judgment errors, adjusting their answers accordingly.This can be used to approximate individual errors.</p>
<p>If the consensus evaluation more accurately predicts any extant ground truth, we would expect the majority vote from the individual evaluations to more closely approximate this than any single evaluation, due to the fundamental properties of ensemble classifiers (see Hastie et al. (2009)).</p>
<p>While the consensus evaluation may not represent the absolute ground truth (we acknowledge that some quantity of error likely would still exist even after this procedure), we expect the consensus evaluation to more accurately approximate any extant ground truth (Clemen, 1989).If this holds, the majority vote should align more closely with the consensus than with any individual evaluation.As shown in Figure 5, this is the case.</p>
<p>As seen in the results, although significant errors occur among all evaluators, the majority vote effectively corrects most of these errors.Notably, cn9o made the most errors (for example, 23.77% in evaluating GPT-Pilot).After applying the majority vote from all three evaluators, the overall error rate dropped to 6.01%, demonstrating the inherent benefits of majority voting.</p>
<p>Figure 5 Mismatch between the individual evaluations and the consensus evaluation.In particular, the majority vote classifier showed the smallest deviation from the consensus evaluation.</p>
<p>Conclusion</p>
<p>Human judgment errors are inevitable.To reduce them, we suggest two methods.First, like in this work, introduce a debate round after each judgment, where individuals present evidence and either persuade others or adjust their own opinions after discussion.This is particularly important when there are only a few evaluators, as majority voting with a small group can still lead to errors (around 5% compared to consensus evaluation, as shown in Figure 5).The second approach involves assembling a larger panel of experts (more is better when their accuracy exceeds 50% (Grofman et al., 1983)), with over 5 people recommended by Hastie and Kameda (2005); Larrick and Soll ( 2006), and relying on a majority vote.However, due to the high cost of engaging more experts and the fact that this is not always feasible in practice, we argue for the former.</p>
<p>4 Agent-as-a-Judge: Evaluating Agents with Agents</p>
<p>Human evaluation, while somewhat reliable, is time-consuming and requires substantial expertise.To address this, we propose the Agent-as-a-Judge framework.If such an agentic system could evaluate like a human, it would reduce the need for human involvement and eliminate the trade-off between thoroughness and effort.</p>
<p>Proof-of-Concept</p>
<p>Based on our prior experiences with agent design and by imitating the human evaluation process, we initially designed eight modular, interacting components that form the foundation of our Proof-of-Concept for the Agent-as-a-Judge.(1) The graph module constructs a graph that captures the entire structure of the project, including files, modules, and dependencies.It can also break down chunks of code into code snippets.(2) The locate module identifies the specific folder or file referred to by a requirement.( 3) The read module goes beyond simple file parsing, supporting the reading and understanding of multimodal data across 33 different formats, including code, images, videos and documents.This allows the agent to cross-reference various data streams and verify different kinds of requirement.(4) The search module provides a contextual understanding of code and can quickly retrieve highly relevant code snippets, as well as the nuances behind them (e.g., hidden dependencies).( 5) The retrieve module Table 3 AI Judges and Their Shift/Alignment with Human-as-a-Judge.We compare the results of LLM-as-a-Judge and Agent-as-a-Judge with Human-as-a-Judge.(I) represents performance on independent tasks, while (D) represents performance considering task dependencies.Note: gray-box settings use carefully manually collected trajectory data (which is nearly inaccessible in practical situations, see Appendix J).In contrast, black-box setting doesn't need to access to such data.The red scores represent the absolute judge shift compared with Human-as-a-Judge (e.g., 2.74%).</p>
<p>Metric</p>
<p>MetaGPT (Hong et al., 2024b extracts information from long texts, identifying relevant segments in trajectories.With context from the above, (6) the ask module determines whether a given requirement is satisfied.(7) The memory module stores historical judgment information, allowing the agent to build on past evaluations.Finally, (8) the planning module plans the following actions, allowing the agent to strategize and sequence tasks based on the current state and the project goals.</p>
<p>Our initial design of the Agent-as-a-Judge, including all its components, is shown in Figure 6, and the operational process of the Agent-as-a-Judge is illustrated in Figure 9.</p>
<p>After conducting comprehensive ablation studies, we found that the modular combination of (1), ( 2), ( 3), (5), and (6) achieved the highest performance (see Appendix K).A sample of the dynamic evidence collected by the Agent-as-a-Judge is shown in Appendix M. We hypothesize this is because Agent-as-a-Judge needs high-quality factual information and is sensitive to noise.For example, while our design of the planning module introduces promising decision-making for future actions, the procedure is unstable.Initially, we hoped that historical information from the memory module would help to assess current requirements.However, it proved detrimental, as any errors in previous judgments could lead to a chain of errors, negatively affecting current decisions.Besides, the current workspaces generated by developer agents, having only hundreds of lines of code, cannot fully benefit from the search module.The details of these findings are explained in Appendix K.Note that a perfect Agent-as-a-Judge is not the focus of this proof of concept, and thus, we leave the utilization of advanced agentic optimization methods for Agent-as-a-Judge, such as automated prompt optimization and workflow design (Zhuge et al., 2024;Hu et al., 2024), for future work.</p>
<p>4.2 Judging Agent-as-a-Judge and LLM-as-a-Judge Judge Shift Judge Shift measures deviation from the Human-as-a-Judge consensus results, with lower values indicating a closer alignment.As shown in table 3, Agent-as-a-Judge consistently outperforms LLM-as-a-Judge across tasks, particularly those with task dependencies.For example, in Requirement (I), Agent-as-a-Judge shows a Judge Shift as low as 0.27%, while LLM-as-a-Judge reaches 31.24% for OpenHands.This underscores Agent-as-a-Judge's stability and suitability for meeting task requirements.Furthermore, in the gray-box setting, both Agent-as-a-Judge and LLM-as-a-Judge show even better results than their performance in the black-box setting.Alignment Rate The Alignment Rate reflects how closely the AI Judges' evaluations align with human consensus across all 365 requirements.It is defined as the percentage of requirement evaluations that are the same as the Human-as-a-Judge consensus evaluation.Compared to LLM-as-a-Judge, Agentas-a-Judge consistently achieves a higher Alignment Rate, closely matching human judgments.For example, when evaluating OpenHands, Agent-as-a-Judge reaches 92.07%and 90.44%, surpassing LLMas-a-Judge's 70.76% and 60.38% in both gray-box and black-box settings.This shows that Agent-as-a-Judge produces more accurate and human-aligned evaluations, especially in complex scenarios.</p>
<p>PR Curves Judging developer agents is a classimbalanced task, where meeting requirements is much rarer than failing.Metrics like judge shift and alignment rate can be misleading.For example, since MetaGPT rarely meets requirements, LLM-as-a-Judge easily identifies most cases as negative (achieving 84.15% in the black-box setting).PR Curves offer a clearer performance measure by balancing precision and recall.Agent-as-a-Judge even outperforms any single human evaluator on OpenHands and aligns closest with majority voting.This shows that, in some cases, Agent-as-a-Judge can nearly replace human evaluators.</p>
<p>Ablations For Agent-as-a-Judge</p>
<p>We conduct ablations to evaluate the impact of adding different components on Agent-as-a-Judge's performance.</p>
<p>The components analyzed include ask, graph, read, locate, and retrieve.The component ablation study for Agent-as-a-Judge reveals key insights into the performance gains from adding specific functionalities.The introduction of read further improves the alignment rate to 82.24%, reflecting the value of direct access to the contents of the file.Incorporating locate brings a substantial boost to 90.44%, as the agent can efficiently target files relevant to the requirements.However, adding retrieve does not provide a significant benefit in this case.In contrast, as shown in Table 3, the judgment of MetaGPT and GPT-Pilot indicates that retrieve is useful, as the trajectory provides additional valuable information.</p>
<p>Cost Analysis</p>
<p>The Human-as-a-Judge took the three evaluators a self-reported total of 86.5 hours.With a 15 USD minimum wage (assuming this would buy a subject expert in AI), a full evaluation under DevAI would cost around 1297.50 USD.In comparison, Agent-as-a-Judge cost only 30.58 USD in API calls and took only 118.43 minutes-2.29% of the cost and 2.36% of the time of Human-as-a-Judge.LLM-as-a-Judge was faster at 10.99 minutes, but due to the absence of intelligent context selection by the Agent-as-a-Judge's modules, it still cost 29.63USD.</p>
<p>Related Work</p>
<p>Agentic systems and their applications are highly active research areas with numerous recent works having a relation to this work.This section details those works most relevant to ours.We provide a treatment of the less relevant related works in Appendix D.</p>
<p>AI Developers AI in software development is growing fast (Liu et al., 2024).AI-driven developers have been applied to directly imitate software companies (Hong et al., 2024b;Qian et al., 2024a), debug code (Yang et al., 2024a), run data science methods (Guo et al., 2024;Hong et al., 2024a;Li et al., 2024;Qiao et al., 2023), and even write academic papers (Lu et al., 2024a).</p>
<p>Benchmarks for AI developments Benchmarks like MLAgentBench (Huang et al., 2024), ML-Bench (Liu et al., 2023d), SUPER (Bogin et al., 2024), DS-bench (Jing et al., 2024), and MLE-Bench (Chan et al., 2024) all focus on benchmarking agentic systems using AI tasks.However, DevAI distinguishes itself from all of these by focusing on realistic user queries that target a complete development cycle.It further includes a more comprehensive evaluation with multiple hierarchical requirements and preferences for each task.Comparatively, MLAgentBench (Huang et al., 2024) for example, focuses on final performance for a limited set of well-known tasks, which risks overfitting and fails to assess a system's generalization or adaptability.</p>
<p>AI Judges Several works have looked at using AI systems as judges 3 .The work by Chan et al. (2023); Zhao et al. (2024), for example, extends LLM-as-a-Judge to have multiple LLMs in their evaluation process for conversational tasks.Unlike Agent-as-a-Judge, they employ a trivial agentic system and apply it only to evaluate LLMs under traditional evaluation setups.In contrast, (Lu et al., 2024b) uses a single LLM-based evaluator but, unlike LLM-as-a-Judge, applies this to multimodal tasks rather than just for evaluating LLMs.</p>
<p>Discussion and Conclusion</p>
<p>Outlook 1: Intermediate Feedback for Agentic Self-Improvement A key power of the Agent-as-a-Judge, though not fully exploited here but nonetheless clear, is that it provides intermediate feedback that is essential for effective and efficient optimization (Zhuge et al., 2024;Pan et al., 2024).For example, Agarwal et al. (2019) proposes to solve the sparse reward problem in reinforcement learning, by learning auxiliary reward functions that provide intermediate feedback.Perhaps the greatest strength of the Agent-as-a-Judge framework is that an agentic system can use it to identify and fix issues in its solutions to complex, multistage problems on the fly-something older, delayed-feedback methods did not permit.By introducing Agent-as-a-Judge, we create the opportunity to build a process-supervised reward model (PRM) for improving agentic systems (Lightman et al., 2023).</p>
<p>Outlook 2: Flywheel Effect Driven by Agent-as-a-Judge The cycle of mutual improvement between the Agent-as-a-Judge and the evaluated agents, where both evolve together through iterative feedback, presents a promising direction.We hypothesize that an agentic version of a self-play system (Zelikman et al., 2022;Chen et al., 2024e;Wang et al., 2024b), could viably emerge by using the Agent-as-a-Judge as a key mechanism.Furthermore, the ongoing interaction between the Agent-as-a-Judge and the evaluated agents has the potential to create a flywheel effect, where successive incremental improvements reinforce one another, leading to progressively greater optimization and enhanced performance over time (Wang et al., 2022).This iterative process may also serve as a valuable complement to LLM reasoning data, help embedding agentic capabilities into foundation models (Luo et al., 2024).</p>
<p>Conclusion</p>
<p>In this work, we introduced the Agent-as-a-Judge method to use agentic systems to evaluate agentic systems.We simultaneously released DevAI: a new benchmark that evaluates the code-generating ability of agentic systems on complete AI development tasks when used with Agent-as-a-Judge.We went on to show that Agent-as-a-Judge outperforms existing methods on this task and that it performs similarly to an ensemble of expert human evaluators.Altogether, we believe that the above opens the door for scaling up agentic far more than before.</p>
<p>Future Directions</p>
<p>■ Direction 1: Intermediate Feedback for Agentic Self-Improvement Description: Agent-as-a-Judge offers intermediate feedback, crucial for reinforcement learning, where rewards are sparse but vital for improvement.It also enables real-time issue identification and resolution in complex, multi-stage tasks, overcoming the limitations of delayed feedback.</p>
<p>■ Direction 2: Flywheel Effect Driven by Agent-as-a-Judge Description: The iterative feedback cycle between the Agent-as-a-Judge and evaluated agents (such as Developer Agents here) could create a flywheel effect, where mutual improvements lead to progressively greater optimization.This dynamic could drive an agentic self-play system and complement LLM reasoning data to embed agentic features into foundation models.</p>
<p>Figure 8 We Outline the Logical Flow of the Agent-as-a-Judge Framework.</p>
<p>D Extend Related Work</p>
<p>Our main paper includes mostly related works of AI developers, Benchmarks for AI developments, and AI judges.However, the following works contribute significantly to the community and also relate to this work.We record this work as additional related work.</p>
<p>LLM-based Autonomous Agents</p>
<p>Recent developments in LLM-based agents have expanded their capabilities beyond simple task execution to more autonomous problem-solving and decision-making.AutoGPT (Gravitas, 2023) and LangChain (Chase, 2022) provide frameworks for single-agent systems that leverage external tools for more complex tasks.Similarly, research such as CAMEL (Li et al., 2023), MetaGPT (Hong et al., 2024b), NLSOM (Zhuge et al., 2023), AutoGen (Wu et al., 2023)  Most recently, using graphs to build agents has gained prominence.Earlier work like GPTSwarm (Zhuge et al., 2024) and LangGraph (langchain ai, 2024) proposed using nodes to represent operations and edges to represent the connections between them.In GPTSwarm, multiple agents represented as subgraphs in a graph are connected by optimizable edges, and reinforcement learning is employed to optimize the edges.Following this approach, several agent frameworks have incorporated graphs into their designs (Hong et al., 2024a;Zhou et al., 2024;Qian et al., 2024b).Additionally, various optimization methods have been developed to enhance agent performance further (Wu et al., 2024a;Song et al., 2024;Hu et al., 2024).In practical applications, many studies focus on understanding and interacting with GUIs (Wang et al., 2024a;Chen et al., 2024c;Yang et al., 2023;Xu et al., 2024;Tan et al., 2024;Wu et al., 2024b).For code generation agents (Jin et al., 2024), current research mainly emphasizes automated repair (Yang et al., 2024a;Phan et al., 2024;Tao et al., 2024), computational modular design (Khattab et al., 2023;Cheng et al., 2024), and automated development (Tufano et al., 2024;Huang et al., 2023).Among these, open-sourced frameworks like OpenHands (Wang et al., 2024d) have gained popularity due to their strong user experience.Moreover, scientific discovery (Jansen et al., 2024;Lu et al., 2024a) and ML agents (Yang et al., 2024b) are also receiving increased attention.</p>
<p>LLM-as-a-Judge In the domain of AI evaluation and judgment, frameworks (Zheng et al., 2024;Fu et al., 2023) have pioneered the use of LLMs to assess conversational agents, demonstrating how LLMs can evaluate dialogue quality and consistency.LLM-as-a-judge has also expanded into the multimodal domain, providing clear visual-language feedback (Chen et al., 2024b;Xiong et al., 2024).ICE-Score (Zhuo, 2023) improves on metrics like CodeBERTScore (Zhou et al., 2023a) and G-EVAL Liu et al. (2023c) by better aligning with human preferences and functional correctness.Expanding beyond dialogue, LLMs like CodeR (Chen et al., 2024a) and MASAI (Arora et al., 2024) apply similar judging principles to the code validation process, where AI systems autonomously evaluate and verify computer programs.Our work builds on these advancements by exploring how LLMs can perform more nuanced judgment tasks, further investigating their potential in decision-making across various domains.Recent research also focuses on judging LLM-as-a-Judges (Chen et al., 2024d;Bavaresco et al., 2024;Thakur et al., 2024;Dong et al., 2024;Shi et al., 2024;Raina et al., 2024).</p>
<p>Coding Benchmarks Recent advances in code generation have led to the innovation of various benchmarks to evaluate model performance (Liu et al., 2024).Early benchmarks, such as MBPP (Austin et al., 2021), HumanEval (Chen et al., 2021), and MultiPL-E (Cassano et al., 2023), focus primarily on generating simple functions.While these benchmarks are useful for evaluating the correctness of generated code, they are limited in complexity and do not fully represent the challenges encountered in real-world software development.</p>
<p>As the field progressed, newer benchmarks began to focus on more complex and realistic tasks.APPS (Hendrycks et al., 2021), CodeContests (Li et al., 2022), and LiveCodeBench (Jain et al., 2024) moved toward competitive programming challenges that involve advanced algorithms and data structures.These tasks are more representative of problems encountered in coding competitions and help push models toward more sophisticated problem-solving.DS-1000 (Lai et al., 2023) was introduced to assess the skills of models with data science libraries, evaluating their ability to use APIs and execute complex data analysis workflows.</p>
<p>Meanwhile, AgentBench (Liu et al., 2023b) focuses on testing reasoning and decision-making abilities in interactive environments, highlighting differences in performance between commercial and open-source models.</p>
<p>To address real-world programming needs beyond code generation, specialized benchmarks have been created to evaluate tasks such as debugging, refactoring, and code navigation.BigCodeBench (Zhuo et al., 2024) has advanced the evaluation of LLMs by testing their ability to solve complex, real-world programming tasks beyond traditional algorithmic challenges.CANITEDIT (Cassano et al., 2024), DebugBench (Tian et al., 2024), andFixEval (Haque, 2023) evaluate the ability of a model to edit and improve existing code.Additionally, benchmarks such as SWE-Bench (Jimenez et al., 2023) focus on resolving issues in GitHub repositories, simulating practical software development scenarios.Finally, benchmarks such as RepoBench (Liu et al., 2023a) and RepoEval (Zhang et al., 2023) delve into the evaluation of models in large-scale, multifile codebases.These benchmarks measure the ability of language models to understand the structure of repositories and solve problems within more complex, collaborative environments.</p>
<p>There are also tasks that assess tool-use skills requiring programming knowledge, though not directly focused on programming.CRAB (Xu et al., 2024) sets a new standard for benchmarking autonomous agents by supporting cross-environment tasks and offering a more comprehensive evaluation framework.Similarly, WebArena-serious (Zhou et al., 2023b;Koh et al., 2024) introduces a realistic benchmark for evaluating multimodal agents on complex, real-world web tasks, addressing the limitations of text-only agents by integrating both visual and textual inputs across diverse domains.</p>
<p>E The Procedures of Creating DevAI Dataset E.1 Manually draft user queries</p>
<p>Given the execution cost of the developer agents, we collect small-scale AI tasks to ensure the practical applicability of our benchmark.Since these tasks are small-scale and well studied, which are easy to overfit in terms of task performance metrics, unlike previous benchmarks (e.g., (Huang et al., 2024)), we do not evaluate task performance as the development performance measure.Instead, we prioritize the step-by-step task-solving ability, which is essential for real-world development.Our quires are specifically designed to require the development agents to understand user intentions, solve the task in multiple steps, and adapt to unexpected step outcomes.This approach also makes our benchmark user-friendly, transparent, and better reflects real-world deployment situations.To enable effective evaluation, our queries present a specific development file structure for the developer agents to follow.To ensure that the developer agents save the files to be evaluated in the workspace, we develop constraint prompts added to the query to form an extended query.The constraint prompts guide the developer agents to save and execute the source codes, which are in line with the real-world development standard.See Appendix I for our constraint prompts.</p>
<p>E.2 Set Judging Criteria</p>
<p>To make the evaluation of the developer agent precise, we assign to each task query a list of requirements as task milestones.The requirements are chosen so that satisfying all the requirements is a necessary condition to consider the task to be solved successively.Since our tasks are AI-centric, our queries target essential elements of AI development, including data processing, AI method, presentation of evaluation metrics, visualization, and human-computer interaction, covering the key areas that matter most in real-world scenarios.On the other hand, breaking down tasks into individual requirements also reflects the multi-step nature of code development.</p>
<p>Importantly, to avoid ambiguity, we set the requirements to be explicit, binary, and straightforward to evaluate.</p>
<p>To include other human predispositions, we include a list of preferences per task that covers subjective, ambiguous, or non-explicitly stated characteristics.</p>
<p>E.3 Building Dependency Among Requirements</p>
<p>To enhance the realism of our benchmark, we analyzed the dependencies among requirements.Over the past decades, methodologies such as the KDD Process (Fayyad et al., 1996) and CRISP-DM (Wirth and Hipp, 2000) have guided ML/AI development, establishing foundational frameworks that have been further refined with the rise of AutoML (He et al., 2021).</p>
<p>Inspired by these methodologies, we identified a seven-step process for automated AI development tasks.This process includes critical stages such as data preprocessing, feature engineering, model selection, and hyperparameter tuning, along with essential post-development activities like metrics recording, report generation, and the development of interactive user applications.This structured approach allows us to evaluate the agent's ability to manage task dependencies and effectively navigate complex, real-world scenarios.</p>
<p>E.4 Refine the dataset</p>
<p>Manual refinements were necessary to ensure the accuracy and clarity of DevAI.We perform two rounds of comprehensive review and edits on DevAI, each round being done by a different participant.During these manual refinements, we focus on the logical consistency of our queries and requirements, the correctness and ambiguity of the language, and the applicability of the content to the task domain.We identified a moderate number of errors in our dataset during the review.</p>
<p>E.5 Analyse the dataset</p>
<p>We categorized each requirement based on its focus, whether it was related to the data set, the machine learning method, visualization, metrics, HCI, or data processing.While this approach provides a useful framework, it is important to recognize that these categories might overlap or miss certain nuances.Similarly, preferences were classified by how strongly they appeared in the query text, ranging from those inferred by common sense to those explicitly stated.Although this helps to organize preferences, it is worth noting that such classifications can be subjective and may not fully capture the importance of each preference in practical applications.By structuring the requirements and preferences this way, we aim to enhance the evaluation process, though flexibility and context awareness remain crucial for truly robust assessments.</p>
<p>E.6 Auxiliary Information</p>
<p>Some of the tasks require the download of a Kaggle data set, where a Kaggle credential is needed.Our constraint I requires an "is kaggle api needed" tag to determine whether the credential is needed to be included in the extended query.We further mark each task with some tags describing the AI sub-fields related to the task, including computer vision, supervised learning, reinforcement learning, natural language processing, etc., as well as two "is training needed" and "is web navigation needed" tags as auxiliary information.We also categorize each requirement into one of the following: (1) dataset or environment, (2) data preprocessing and postprocessing, (3) machine learning method, (4) save trained model, (4) performance metrics, (5) human computer interaction, (6) visualization, and (7) other, reflecting the nature of the requirement.</p>
<p>E.7 A Json Format of Our Sample</p>
<p>Here, we provide a sample of the DevAI with its json format.We also provide more samples in Appendix G.The project should load the dataset and perform robust audio preprocessing ( noise removal and normalization ) and MFCC feature extraction , implemented in <code>src / data_loader .py</code>.The CNN -LSTM model should be implemented in ' src / model .py '.Recognition accuracy should be saved in <code>results / metrics / r e cognition_accuracy .txt</code>, and a confusion matrix should be generated and saved as <code>results / figures / confusion_matrix .png</code>.Additionally , a userfriendly local API should be created using Flask to allow users to upload audio files and receive emotion recognition results , with the implementation included in <code>src / hci .py</code>. " , " tags " : [ " Audio Processing " , " Classification " ] , " requirements " : [ { " requirement_id " : 0 , " prerequisites " : [] ,</p>
<p>" criteria " : " The \ " RAVDESS \ " dataset is loaded in <code>src / data_loader .py</code>, which is downloaded from Kaggle or <a href="https : // huggingface .co / datasets / xbgoose / ravdess"> this Hugging Face link </a> ." , " category " : " Dataset or Environment " , " satisfied " : null } , { " requirement_id " : 1 , " prerequisites " : [ 0 ] , " criteria " : " Audio preprocessing , including noise removal and normalization , is implemented in <code>src / data_loader .py</code>. " , " category " : " Data preprocessing and postprocessing " , " satisfied " : null } , { " requirement_id " : 2 , " prerequisites " : [ 0 , 1 ] , " criteria " : " MFCC feature extraction is implemented in <code>src / data_loader .py</code>. " , " category " : " Data preprocessing and postprocessing " , " satisfied " : null } , { " requirement_id " : 3 , " prerequisites " : [] ,</p>
<p>" criteria " : " The \ " CNN -LSTM \ " model is implemented in ' src / model .py '. " , " category " : " Machine Learning Method " , " satisfied " : null } , { " requirement_id " : 4 , " prerequisites " : [ 2 , 3 ] , " criteria " : " Recognition accuracy is saved in <code>results / metrics / recognition_accuracy .txt</code>. " , " category " : " Performance Metrics " , " satisfied " : null } , { " requirement_id " : 5 , " prerequisites " : [ 2 , 3 , 4 ] , " criteria " : " The confusion matrix is generated and saved as <code>results / figures / confusion_matrix .png</code>. " , " category " : " Visualization " , " satisfied " : null } , { " requirement_id " : 6 , " prerequisites " : [ 2 , 3 ] , " criteria " : " A local API is created using \ " Flask \ " to allow users to upload audio files and receive emotion recognition results .The implementation should be included in <code>src / hci .py</code>. " , " category " : " Human Computer Interaction " , " satisfied " : null } ] , " preferences " : [ { " preference_id " : 0 , " criteria " : " The audio preprocessing step should be robust , effectively reducing noise while preserving the integrity of the speech signals ." , " satisfied " : null } , { " preference_id " : 1 , " criteria " : " The local API should be user -friendly , with clear instructions for uploading files and interpreting results ." , " satisfied " : null } ] , " i s _ kaggle_api_needed " : true , " is _t raining_needed " : true , " i s _ w e b _ n av i g a ti o n _ ne e d e d " : true }</p>
<p>F User experiences of code-generation agentic systems</p>
<p>OpenHands (Wang et al., 2024d) offers the most refined user experience, leveraging its highly interactive frontend to enable seamless user interaction and task execution.This interface allows users to engage directly with the system, resulting in a smoother and more intuitive workflow, which drives operational efficiency.</p>
<p>In contrast, MetaGPT (Hong et al., 2024b) excels in task decomposition through its use of Directed Acyclic Graphs (DAGs), a well-structured and scalable approach aligned with industry best practices in system modularization.This enhances its appeal for users focused on task clarity and modular breakdowns.However, in practical deployments, MetaGPT tends to be less aggressive in file management and preservation, potentially due to its core positioning as a data analysis tool, which does not prioritize persistent state management.Similarly, OpenDevin demonstrates a notable overconfidence in its code generation, frequently skipping the critical step of post-generation code execution, requiring users to intervene manually.</p>
<p>GPT-Pilot (Pythagora.io, 2023), praised for its detailed task delegation via over 20 specialized agents, suffers from reduced interactivity due to an overly granular division of responsibilities, resulting in a more fragmented user experience.These qualitative insights, although not fully captured by quantitative metrics, were evident through the DevAI dataset, providing key areas for improvement in user engagement and operational fluidity in future releases of these frameworks.Criteria: The model should be tuned to capture seasonal trends in the sales data for more accurate forecasting.Evaluator cn9o I evaluated everything based on whether the requirement was satisfied verbatim, using my own professional judgement when there was ambiguity.If there was a data folder and a look at the trajectory indicated it was real, I allowed it to be loaded directly.If there were results and no clear source code making them, I considered the task incomplete.I didn't consider a dummy data path to be correct (GPT-Pilot used a lot of these).To determine if something was executed correctly, I skimmed the end of the logs (e.g., trajectory) for any obvious signs of an error.If there wasn't any, I said it was correctly executed.I ignored prerequisites while evaluating and did not execute anything, instead just eyeballing the code for correctness.I was lenient in what I considered to be sufficient in terms of the more vague requirements (e.g., if the preprocessing had to include scaling and rotation, just those two would be sufficient to consider it done).</p>
<p>G More DevAI dataset samples</p>
<p>Figure 12 Each evaluator was given a full description of each task and the associated requirements and preferences in markdown format.They were then asked, for each workspace and trajectory generated by each of the agents on each of the task, whether (1) the agent successfully finished its execution cycle and (2) which of the requirements were satisfied.After the evaluation was complete, the evaluators were asked to self-report the nuances of their evaluation.</p>
<p>I Suggest Constraints</p>
<p>Below is a sample of constraints in JSON format that describes task-specific guidelines: { " generic " : " This is a task that requires you to write , execute , and save source code .You have a hard time limit of 3 0 minutes to produce your programmatic solution to the given task .This time limit includes execution time .The quality of your solution will be judged based on what you left in the working folder by the time 3 0 minutes expire .Additionally , the hardware you are running on is unknown , and the presence of a GPU is not guaranteed ." , " is _t ra ining_needed " : " Keep the time limit in mind when setting hyperparameters for training ." , " i s _ k a g gle_api_needed " : " You can use the Kaggle API credentials stored in kaggle .json `in your current working directory ." } To address automation and security concerns, we have written code to place the kaggle.jsonfile into the current workspace for each baseline during each run.However, dataset users are free to modify the solution to enable the Kaggle API for their specific developer agents.</p>
<p>J Collected Trajectories J.1 Schema</p>
<p>Below is the required JSON format for a trajectory in gray-box settings (where the trajectories can serve as input for LLM-as-a-Judge and Agent-as-a-Judge).</p>
<p>{</p>
<p>" type " : " array " , " items " : { " type " : " object " , " properties " : { " step " : { " type " : " integer " , " description " : " The step number in the trajectory , 0 -based ." } , " user_message " : { " type " : [ " string " , " null " ] , " description " : " The message from the external user to the agent .If null , no message was sent ." } , " agent " : { " type " : " object " , " properties " : { " thought " : { " type " : " string " , " description " : " The agent ' s thought at this step ." } , " action " : { " type " : [ " string " , " null " ] , " description " : " The agent ' s action sent to the environment .If null , the agent did not take any action , for example , when the agent has finished the task ." } , " agent_name " : { " type " : " string " , " description " : " The name of the agent that made the action ." } } , " required " : [ " thought " , " action " ] , " description " : " Everything related to the agent at this step ." } , " environment " : { " type " : [ " string " , " null " ] , " description " : " The environment ' s ( shell , python interpreter ) response to the action submitted by the agent .If null , the environment was not involved in this step ." } , " step_usage " : { " type " : " object " , " properties " : { " input_tokens " : { " type " : " integer " , " description " : " The number of input tokens passed as LLM context ." } , " output_tokens " : { " type " : " integer " , " description " : " The number of tokens produced by the LLM ." } , calculation \ n mAP = 0 .0 # This should be replaced with actual mAP calculation \ n \ n with open (\ " results / metrics / model_performance .txt \ " , \ " w \ " ) as f : \ n f .write ( f \ " Mean Inference Time : { mean_inference_time : .4 f } seconds \\ n \ " ) \ n f .write ( f \ " mAP : { mAP : .4 f }\\ n \ " ) \ n \ nif <strong>name</strong> == \ " <strong>main</strong> \ " : \ n evaluate_model () \ n \ " \ " \ " \ n \ ne dit_ file_ by_r eplac e ( ' src / evaluate .py ' , ' with open (\ " results / metrics / model_performance .txt \ " , \ " w \ " ) as f : ', '</p>
<p>with open (\ " results / metrics / model_performance .txt \ " , \ " w \ " ) f : ') " } , " environment " : null , " step_usage " : { " input_tokens " : 2 0 1 3 2 , " output_tokens " : 8 0 5 , " llm_inference_time " : 1 5 . 1 5 6 5 5 2 3 1 4 7 5 8 3 , " model " : " gpt -4o -2 0 2 4 -0 5 -1 3 " , " cost " : 0 . 1 1 2 7 3 5 0 0 0 0 0 0 0 0 0 0 2 , " step_execution_time " : 1 5 . 1 9 6 2 2 7 0 7 3 6 6 9 4 3 4 } , " accumulated_usage " : { " accumulated_cost " : 1 .2 9 1 9 7 5 0 0 0 0 0 0 0 0 0 3 , " accumulated_time " : 1 7 6 .9 9 7 7 2 3 1 0 2 5 6 9 5 8 } } K Ablations of Agent-as-a-Judge K.1 Component Abalations Analysis We designed 8 modular components for the Agent-as-a-Judge system.In the Table 5, components are added progressively from left to right.If the addition of a component led to a significant performance drop, we removed it from further iterations.Our experiments showed that adding the components ask, graph, read, and locate resulted in significant performance gains.However, when the search component was introduced, there was a noticeable decline in performance.</p>
<p>We hypothesize that the performance drop from search is due to its role in retrieving relevant code snippets (top-3) using BM25.The retrieval accuracy of BM25 (Robertson et al., 2009) might not have been high enough, potentially introducing noise.Moreover, as noted in Table 1, the DevAI tasks in our experiments did not generate a large amount of code.In fact, even when all code was fed into an LLM, the total content typically stayed within the maximum context length.Therefore, in simpler workspaces, search was less critical.However, we believe this component will become more important as the complexity of the workspace increases, making it more valuable in larger and more complex environments.</p>
<p>We also observed that the introduction of the planning mechanism did not bring a noticeable improvement in performance.This may be related to the nature of the Judge -it needs clean factual information.When planning is unstable, the evidence collected from different actions can become inconsistent, leading to a decline in performance.Finally, we experimented with a memory mechanism.Initially, we hypothesized that since DevAI tasks often involve interconnected requirements, memory could help track whether requirements were met.However, in practice, we saw no improvement.We suspect that the interconnected nature of the requirements may have caused biases: specifically, once a prior requirement was fulfilled, it might have overly influenced positive judgments on subsequent requirements, even if they were not fully met.</p>
<p>K.2 Search Algorithms in Search Module</p>
<p>We initially hypothesized that the performance drop was due to the low precision of the search component, particularly with BM2.5.To explore this, we replaced BM2.5 with Sentence-BERT (Reimers, 2019) as a more advanced alternative and tested Fuzzy Search (Levenshtein, 1966) as a less precise option.However, neither improved the performance of the Agent-as-a-Judge.hese results suggest that the performance issue is not due to BM2.5's poor search accuracy.Instead, the workspaces generated in our DevAI tasks are too simple for the search component to have a significant impact.In simpler workspaces, direct retrieval and evaluation are sufficient.Even though Sentence-BERT performed better than the other methods, its alignment rate (87.70%) still falls short of the configuration without the search component (90.44%).</p>
<p>As workspace complexity increases, the search component may become more valuable.</p>
<p>L Prompt Demos of Agent-as-a-Judge</p>
<p>Here, we present some prompts used by the Agent-as-a-Judge system.Each of these prompt demos plays a crucial role in guiding the agent's behavior.</p>
<p>L.1 System Prompt for Agent-as-a-Judge def g et_system_prompt ( language = " English " ) :</p>
<p>if language == " English " : return " " " You are an advanced AI system serving as an impartial judge for intelligent code generation outputs .Your primary role is to rigorously evaluate whether the agent ' s outputs satisfy the specified requirements by thoroughly analyzing the provided code , data , and other relevant materials .</p>
<p>You will systematically assess aspects such as datasets , model implementations , training procedures , and any task -specific criteria outlined in the requirements .Your evaluations must be objective , detailed , and based solely on the evidence provided .</p>
<p>For each requirement , deliver one of the following judgments :</p>
<p>1 .&lt; SATISFIED &gt; : Use this if the agent ' s output fully meets the requirement .Provide a brief and precise explanation demonstrating how the specific criteria are fulfilled .</p>
<p>2 .&lt; UNSATISFIED &gt; : Use this if the agent ' s output does not meet the requirement .Provide a concise explanation indicating the deficiencies or omissions .</p>
<p>Your assessment should reference specific elements such as code snippets , data samples , or output results where appropriate .Ensure that your justifications are clear , precise , and directly related to the criteria .</p>
<p>Respond with either &lt; SATISFIED &gt; or &lt; UNSATISFIED &gt; , followed by your concise justification ." " " else :</p>
<p>raise NotImplementedError ( f " The language '{ language } ' is not supported ." )</p>
<p>L.2 System Prompt for Locate Module def g e t _ l o c at e _ s ys t e m _p r o m pt ( language = " English " ) : if language == " English " : return " " " You are an advanced AI system specializing in understanding project structures and determining file locations based on provided criteria .Your task is to locate specific files in the workspace based on the user ' s criteria and workspace information .ution problems with the files mentioned in the criteria ." " " else :</p>
<p>raise NotImplementedError ( f " The language '{ language } ' is not supported ." )</p>
<p>L.3 System Prompt for Retrieve Module</p>
<p>def g e t _ r e t r i e v e _ s y s t e m _ p r o m p t ( language = " English " ) :</p>
<p>if language == " English " : return " " " You are an advanced AI system specializing in retrieving environmental feedback from project execution trajectories .Your task is to analyze the provided trajectory data and extract information about the most relevant files mentioned in the given criteria .</p>
<p>Focus on the following :</p>
<p>1 .Identify the ** most recent steps ** where the files directly related to the criteria were involved in execution , loading , or saving operations . 2 .Provide environmental feedback for these files , such as any errors , warnings , or issues encountered during their execution or processing .3 .Highlight whether any problems occurred that might affect the functionality or success of these files in the project .</p>
<p>Your output should be structured as follows :</p>
<p>-<strong> &lt; RELEVANT STEPS &gt;</strong> : List the specific steps involving the relevant files , including any environmental feedback such as error messages , execution results , or other issues encountered .Each step should concisely present the key information needed to assess the files ' execution status .</p>
<p>Avoid including details about file contents or existence , as this information is already available .Focus solely on the environmental feedback related to the execution of the most relevant files .</p>
<p>Your goal is to provide clear and concise information that helps determine if there were any execution problems with the files mentioned in the criteria ." " " else :</p>
<p>raise NotImplementedError ( f " The language '{ language } ' is not supported ." )</p>
<p>Figure 2
2
Figure 2 Distribution of DevAI Tasks (1) DevAI focuses on AI development tasks and so terms such as "dataset," "model," and "results" are particularly common in the queries.(2) The first 53 tasks in DevAI all have a one-paragraph query but of varying lengths (note that task 54 and 55 are excluded here as they are outliers, representing the longest and most complex tasks in the dataset).(3) Each task has one or more tags.The prevalence of supervised learning here reflects the fact that it dominates many machine learning applications.(4) SVM classifiers(Cortes, 1995) and LSTM models(Hochreiter, 1997) are two of the most widely used architectures-a fact reflected by DevAI.</p>
<p>Figure 6
6
Figure 6 Initial diagram of Agent-as-a-Judge.</p>
<p>Figure 7
7
Figure 7 PR Curves comparing judge Methods.</p>
<p>{ " name " : " 2 5 _ S p e e c h _ E m o t i o n _ R e c o g n i t i o n _ C N N _ L S T M _ R A V D E S S _ D L " , " query " : " I am seeking a speech emotion recognition project using a CNN -LSTM model with the RAVDESS dataset , which should be downloaded from Kaggle or <a href="https : // huggingface .co / datasets / xbgoose / ravdess"> this Hugging Face link </a> .</p>
<p>Figure 11
11
Figure 11 An Example Task in DevAI: Task 19.</p>
<p>Follow the instructions from the blog post Hidden in Plain Sight to set up the script for generating images with hidden text in src/visualize.py.
Dependencies → {}
■ R0 Criteria: ■ R1 Criteria: Ensure the generated images are of 1080p resolution and saved in results/.Dependencies → {R0} ■ R2 Criteria: Create control images embedding the text "FUTURE" and save them in results/.Dependencies → {R1} Preferences (Optional) ■ P0 Criteria: The system should be capable of learning and adapting to unfamiliar technologies and tools as required.Figure 3 A task example in DevAI.This task is adapted from a real-world demo given at https://www.cognition.ai/blog/introducing-devin.As this example shows, task requirements in DevAI are structured as a Directed Acyclic Graph (DAG), with nodes representing individual requirements and directed edges showing dependencies.More examples are in Appendix G.</p>
<p>Table 1
1
Preliminary Statistics of AI Developers.We compare three leading open-source code agents using metrics such as average cost, average time, and the average number of generated files.
MetricMetaGPT (Hong et al., 2024b)GPT-Pilot (Pythagora.io, 2023) OpenHands (Wang et al., 2024d)Basic StatisticsVersionData Interpreter (Hong et al., 2024a)0.2.13CodeAct v1.9 (Wang et al., 2024c)(1) Average Cost$1.19$3.92$6.38(2) Average Time775.29s1622.38s362.41s(3) Average Input Tokens1528636067071252482(4) Average Output Tokens28546597078457(5) Average Saved Code Files0.423.842.53(6) Average Saved Code Lines11.15273.3396.56(7) Average Saved Files4.425.913.60in an average of 362.41s, while GPT-Pilot takes the longest at 1622.38s. On average, a full evaluation onDevAI with one of these three took around 210.65 USD and 14 hours to perform. While running, GPT-Pilotgenerates the most output tokens at 59707 tokens, whereas OpenHands processed the most at 1252482 tokenswhile producing the fewest at 8457 tokens. This suggests that OpenHands's internal communication is morecomplicated but is more parsimonious in its decisions.MetaGPT, while being the most cost-effective, generates fewer saved code files (0.42), suggesting it may beless inclined to save files. In contrast, GPT-Pilot generates the most saved files (3.84), reflecting a moreprolific output. The difference in saved code lines, with GPT-Pilot saving 273.33 lines versus MetaGPT's11.15, underscores GPT-Pilot's extensive output. Meanwhile, OpenHands, despite handling larger inputs,seems less focused on executing code to generate files, as evidenced by its lower file output (2.53 saved files).These statistics align with real user experiences (as discussed in Appendix F).</p>
<p>Table 2
2
Human-as-a-Judge for AI Developers.(I) and (D) represent independent performance versus performance considering task dependencies.indicates multiple experts evolved, and means the evaluations use white-box testing (allowing access to the generated workspace, human-collected trajectories, and open-source codebases).The results were derived from expert judgments and deliberations (see Appendix H).
MetricMetaGPT (Hong et al., 2024b) GPT-Pilot (Pythagora.io, 2023) OpenHands (Wang et al., 2024d)/Human-as-a-Judge(A) Requirements Met (I)22.13%44.80%42.89%(B) Requirements Met (D)6.55%28.96%28.68%(C) Self-Termination41.81%5.45%54.54%(D) Task Solve Rate0.00%1.81%1.81%</p>
<p>(Xie et al., 2024)hou et al., 2023c)unication, improving collaboration among agents.However, the challenge of maintaining coherence in agents' dialogue and preventing hallucination remains prominent(Du et al., 2024;Zhou et al., 2023c).Besides, Agent-trust(Xie et al., 2024)examines if LLM agents, like GPT-4, can simulate human trust behaviors in Trust Games, showing that they can align with human behavior.</p>
<p>Task 13: Style Transfer with Perceptual Loss in PyTorch Style intensity is adjusted by tuning the weights of style loss and content loss in src/model.py.
■ R2Criteria: The Perceptual Loss model is implemented in PyTorch and loaded in src/model.py.Dependencies → {}■ R3Criteria: Stylized images are saved to the specified folder results/figures/.Dependencies → {R0, R1, R2}■ R4Criteria: Dependencies → {R0, R1, R2}■ R5Criteria: Processing time is recorded and saved asresults/processing time.txt.Dependencies → {R0, R1, R2, R3, R4}■ P1Criteria: The project should handle high-resolution images efficiently without excessive processing time.
Figure 10 An Example Task in DevAI: Task 13.</p>
<p>Task 19: Time Series Forecasting with Seq2Seq LSTM on Rossmann Store Sales The data is split into training and testing sets and implemented in src/data loader.py.
Requirements■ R0Criteria: The Rossmann Store Sales dataset is used, potentially downloaded from (this link) and loaded insrc/data loader.py.Dependencies → {}■ R1Criteria: Dependencies → {R0}■ R2Criteria: A sequence-to-sequence model based on LSTM is used. The trained model should be saved undermodels/saved models/.Dependencies → {R1}■ R5Criteria: An HTML report containing forecast results and comparison plots is generated and saved asresults/report.html.Dependencies → {R1, R2, R3, R4}■ R6Criteria: The HTML report should include interactive elements that allow users to explore different forecasthorizons.Dependencies → {R5}Preferences (Optional)
■ P0</p>
<p>Table 5
5
Component Ablation Studies for Agent-as-a-Judge.We analyze the impact of sequentially adding various components (ask, graph, read, locate, retrieve, planning, memory, and search) on the performance of Agent-as-a-Judge for judging OpenHands.
Agent-as-a-Judge PerformanceAlignment Rate 65.03% 75.95% 82.24% 90.44%86.06%90.16%88.52%87.97%
Metric+ ask + graph + read + locate + search + retrieve + planning + memory</p>
<p>Table 6
6
Comparisons on Search module with different engines.
Search MethodAlignment RateBM2.586.06%Sentence-BERT87.70%Fuzzy Search85.52%without Search Module90.44%
https://www.cognition.ai/blog/introducing-devin
https://www.cursor.com/
Additionally, we were pleased to find that a recent industry blog (https://www.cognition.ai/blog/evaluating-coding-ag ents), published 3 weeks before our submission, shares very similar ideas and provides further evidence that the Agent-as-a-Judge could have practical applications in agent systems.
AcknowledgementsThe authors thank Haozhe Liu, Piotr Piekos, Firas Laakom, Matteo Paltenghi for their suggestions or paper review.The research reported in this publication was supported by funding from the King Abdullah University of Science and Technology (KAUST) -Center of Excellence for Generative AI under award number 5940 and the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence.https://huggingface.co/devai-benchmark Project: https://github.com/metauto-ai/agent-as-a-judgeNote: First four authors made core contributions.KAUST crafted the dataset.Work doneB Experiment DesignsThis section outlines the experimental designs aimed at evaluating developer agents' performance, analyzing human evaluations, and comparing AI-based judging systems.The experiments are structured across three levels, as illustrated below.Judging Developer AgentsJudging Human-as-a-Judge Judging Agent(/LLM)-as-a-Judge Level 1Level 2 Level 3Section 2.3 Section 3.1 Section 3.2 Section 4.2 Section 4.3B.1 Summary of ExperimentsThe experiments are categorized into three levels as follows:Level 1Experiment 1a: Basic performance statistics for developer agents (Section 2.3)Experiment 1b: Human evaluations of developer agents (Section 3.1)Level 2Experiment 2a: Error analysis of human evaluations (Section 3.2)Level 3Experiment 3a: AI judge baselines (Section 4.2)Experiment 3b: Ablation studies for Agent-as-a-Judge (Section 4.3)C Agent-as-a-Judge PipelineUserHi! Please follow the instruction and set up the script from the blog https://www.factsmachine.ai/p/hiddenin-plain-sight to generate 1080p images with hidden text ("FUTURE," in src/visualize.py.Save them in results/ and verify the text is embedded.Are the requirement {R0} satisfied?AgenticCollect information based on {R0} and the workspace graph.Judge AgentGraph Read Retrieve Ask Verify all requirements and their related analyses.Write and run src/visualize.py to generate masks and save the figures in the results/ directory.Open the mentioned link and carefully read the blog mentioned in the user query.Go to the GitHub repository mentioned in the blog, and read the code and the README file.Set up the environment, then install the package.Set up the Modal library by installing it and configuring the API for serverless deployment.Develop ProcessDeveloper AgentLocate Figure9The pipelines of developer agents and judge agent.Some materials in this figure are from original blog (https://www.factsmachine.ai/p/hidden-in-plain-sight).H Human Evaluation ProcedureWe recruited three AI experts from the authors to perform human evaluation on the output of agentic code generation systems.There we present the evaluation details.First round For the first round of evaluations, our three evaluators reported spending 16.5, 19.5, and 22.0 hours, respectively.To capture the bias that a human evaluator will have, the instructions given to our experts were minimal, with them only receiving a scorecard to complete for each agentic system and each task.Results that all evaluators agree on are considered trustworthy.The assumption here is that it is unlikely that all three evaluators make a mistake or have an effective bias in the same judgment.The self-reported post-hoc evaluation criteria are shown in Figure12.Second round In the second round, the evaluators present and discuss their reasons for disagreeing with judges.In doing so, human errors are likely corrected by their peers.Discussion among evaluators also helps reduce human bias by examining each other's thought processes thoroughly.Furthermore, the consensed results are considered trustworthy given the assumption that it is unlikely that all three evaluators are convinced by the same mistake or the same cognitive bias.The three evaluators took 9.5 hours together for this second round of evaluation.Self-reported Post-hoc Evaluation Criteria after round one Evaluator 231a EXECUTED SUCCESSFULLY: Yes [ ] / No [ ] 1. Must be checked based on the overall completeness of the task, based on looking at the code, the artifacts, and the trajectory.2. The training has finished, the model snapshot and the metrics breakdown have been saved, and at least one artifact of required analytics has been produced -mark as successful.If some analytic artifacts are missing but not all, mark as successful.3.No need to run the code.4. If training was finished but on fake data, mark as successful.Requirements:Marking a requirement as satisfied must be made for this specific requirement disregarding the dependency list.If a file (code, image, snapshot) is there but is empty or without any meaningful content -mark as No. 1. Code: The functionality must be in a file with the requested path.The real data is replaced by simple synthetic -not satisfied.2. Visualization/Reports: The contents must be there and make sense even if not perfect from the ML/DS point of view.3. Snapshots: If a binary snapshot is not empty, mark as Yes.Evaluator 38bb EXECUTED SUCCESSFULLY: Yes [ ] / No [ ]An output is marked yes if none of the following is satisfied.1.The time spent is close to the time limit.2. The last environmental message includes an error.3. The last thought indicates that the task is completed.4. The last step of the trajectory is incomplete.Requirements:If a required is unsatisfied only because of an unsatisification of a previously marked unsatisfied requirment, then judge it based on the assumption that a minimum implementation satisfies the previous requirment exists.1. Code: Mark yes if the code executes and does the required function.If no entrypoint is given, the evaluator will set an entrypoint.If the code is not executable due to previous unsatisfied requirements, then it is judged based on eye-checking.2. Visualization/Reports: Mark yes if the visualization or report exists in a right path and the content aligns the requirement.3. Snapshots: Mark yes if the snapshot exists in a right path and is not empty." model " : {" type " : " string " , " description " : " The name of the LLM model used ." } , " cost " : { " type " : " number " , " description " : " The cost of the LLM inference , in USD ." } , " llm_inference_time " : { " type " : " number " , " description " : " The time taken by the LLM to generate the output tokens , in seconds ." } , " step_execution_time " : { " type " : " number " , " description " : " The time taken to make an entire step including LLM inference and environment execution , in seconds ." } } , " required " : [ " input_tokens " , " output_tokens " , " model " , " cost " , " llm_inference_time " , " step_execution_time " ] } , " accumulated_usage " : { " type " : " object " , " properties " : { " accumulated_cost " : { " type " : " number " , " description " : " The total cost of the trajectory up to this step , in USD ." } , " accumulated_time " : { " type " : " number " , " description " : " The total time taken by the agent to complete the trajectory up to this step , in seconds ." } } , " required " : [ " accumulated_cost " , " accumulated_time " ] } } , " required " : [ " step " , " user_message " , " agent " , " environment " , " step_usage " , "accumulated_usage " ] } }J.2 A Sample of TrajectoryThis is sample trajectories (we select 4 steps of it) in OpenHands(Wang et al., 2024d).All steps are recorded to enable replay of the trajectory.{" step " : 0 , " user_message " : " Help me develop an object detection system using the YOLOv 3 model and the COCO dataset .Download the dataset and preprocess the images by resizing and normalization in <code>src / data_loader .py</code>.Implement the YOLOv 3 model and use Non -Maximum Suppression ( NMS ) to refine the results in <code>src / model .py</code>.Save the detected objects to <code>results / figures /</code>, and create an interactive Streamlit web page in <code>src / app .py to display the detection results .Finally , evaluate the model ' s performance , including metrics such as mAP and inference time , and save the evaluation results to</code>results / metrics / model_performance .txt <code>.The system should properly manage the launch and termination of the Streamlit application to prevent unnecessary resource usage .\n \ nThis is a task that requires you to write , execute , and save source code .You have a hard time limit of 3 0 minutes to produce your programmatic solution to the given task .This time limit includes execution time .The quality of your solution will be judged based on what you left in the working folder by the time 3 0 minutes expire .Additionally , the hardware you are running on is unknown , and the presence of a GPU is not guaranteed .\n \ nKeep the time limit in mind when setting hyperparameters for training .\n " , " agent " : { " thought " : " Let ' s break down the task into manageable steps and start implementing them one by one .2 eae 8 da : / workspace $ " , " step_usage " : { " input_tokens " : 4 3 3 1 , " output_tokens " : 2 2 0 , " llm_inference_time " : 4 .4 4 5 7 8 9 3 3 7 1 5 8 2 0 3 , " model " : " gpt -4o -2 0 2 4 -0 5 -1 3 " , " cost " : 0 .0 2 4 9 5 5 , " step_execution_time " : 1 6 . 2 4 9 2 1 2 2 6 5 0 1 4 6 5 } , " accumulated_usage " : { " accumulated_cost " : 0 .0 2 4 9 5 5 , " accumulated_time " : 1 6 . 2 4 9 2 2 2 0 4 0 1 7 6 3 9 } } , { " step " : 1 , " user_message " : null , " agent " : { " thought " : " Great !The directory structure and necessary files are created .Let ' s proceed with implementing the data loader to download and preprocess the COCO dataset .\n \ n ### Step 2 : Implement</code>src / data_loader .py <code>\ nWe will : \ n 1 .Download the COCO dataset .\n 2 .Resize and normalize the images .\n \ nLet ' s start by writing the code for</code>src / data_loader .py <code>. " , " action " : " Running Python code interactively : create_file ( ' src / data_loader .py ') " } , " environment " : " FileExistsError Traceback ( most recent call last ) \ nCell In [ 1 ] , line 1 \n ----&gt; 1 create_file ( ' src / data_loader .py ') \ n \ nFile / openhands / code / openhands / runtime / plugins / agent_skills / file_ops / file_ops .py : 2 5 7 , in create_file ( filename ) \ n 2 5 1 \ " \ " \ " Creates and opens a new file with the given name .\n 2 5 \ n 2 5 3 Args : \ n 2 5 4 filename : str : The name of the file to create .\n 2 5 5 \ " \ " \ " \ n 2 5 6 if os .path .exists ( filename ) : --&gt; 2 5 7 raise FileExistsError ( f \ " File '{ filename } ' already exists .\" ) \ n 2 5 9 with open ( filename , 'w ') as file : \ n 2 6 0 file .write ( '\\ n ') \ n \ nFileExistsError : File ' src / data_loader .py ' already exists .\n [ Jupyter current working directory : / workspace ]\ n [ Jupyter Python interpreter : / openhands / poetry / openhands -5 O 4 _aCHf -py 3 .1 1 / bin / python ] " , " step_usage " : { " input_tokens " : 4 6 7 5 , " output_tokens " : 8 4 , " llm_inference_time " : 2 . 1 3 6 1 4 3 2 0 7 5 5 0 0 4 9 , " model " : " gpt -4o -2 0 2 4 -0 5 -1 3 " , " cost " : 0 .0 2 4 6 3 5 0 0 0 0 0 0 0 0 0 0 0 4 , " step_execution_time " : 3 .3 4 5 3 8 4 3 5 9 3 5 9 7 4 1 } , " accumulated_usage " : { " accumulated_cost " : 0 .0 4 9 5 9 0 0 0 0 0 0 0 0 0 0 0 1 , " accumulated_time " : 1 9 .5 9 4 6 0 8 0 6 8 4 6 6 1 8 7 } } , { " step " : 2 , " user_message " : null , " agent " : { " thought " : " The file</code>src / data_loader .py <code>already exists .Let ' s open it and start writing the code to download and preprocess the COCO dataset .\n \ n ### Step 2 : Implement</code>src / data_loader .py <code>" , " action " : " Running Python code interactively : open_file ( ' src / data_loader .py ') " } , " environment " : " [ File : / workspace / src / data_loader .py ( 1 lines total ) ]\ n ( this is the beginning of the file ) \ n 1 |\ n ( this is the end of the file ) \ n [ Jupyter current working directory : / workspace ]\ n [ Jupyter Python interpreter : / openhands / poetry / openhands -5 O 4 _aCHf -py 3 .1 1 / bin / python ] " , " step_usage " : { " input_tokens " : 4 9 8 2 , " output_tokens " : 5 3 , " llm_inference_time " : 2 . 2 0 9 7 5 6 8 5 1 1 9 6 2 8 9 , " model " : " gpt -4o -2 0 2 4 -0 5 -1 3 " , " cost " : 0 .0 2 5 7 0 5 0 0 0 0 0 0 0 0 0 0 0 2 , " step_execution_time " : 2 .3 1 8 8 6 1 9 6 1 3 6 4 7 4 6 } , " accumulated_usage " : { " accumulated_cost " : 0 .0 7 5 2 9 5 0 0 0 0 0 0 0 0 0 0 1 , " accumulated_time " : 2 1 .9 1 3 4 7 1 9 3 7 1 7 9 5 6 5 } } , ... { " step " : 2 0 , " user_message " : null , " agent " : { " thought " : " It seems the indentation issue persists .Let ' s correct the indentation manually .with open (\ " results / metrics / model_performance .txt \ " , \ " w \ " ) as f : \ n f .write ( f \ " Mean Inference Time : { mean_inference_time : .4 f } seconds \\ n \ " ) \ n f .write ( f \ " mAP : { mAP : .4 f }\\ n \ " ) \ n \ nif __name__ == \ " __main__ \ " : \ n evaluate_model () \n ```\ n \ nLet ' s update the file with the corrected code ." , " action " : " Running Python code interactively : EDITED_CODE = \ " \ " \ " import time \ nimport numpy as np \ nfrom model import yolo_v 3 _model , non_max_suppression \ nfrom data_loader import preprocess_image , dow nloa d_coc o_da taset , extract_coco_dataset \ nimport os \ n \ ndef evaluate_model () : \ n model = yolo_v 3 _model () \ n dow nloa d_coc o_da taset () \ n extract_coco_dataset () \ n \ n image_dir = \ " data / val 2 0 1 7 \ " \ n image_files = [ os .path .join ( image_dir , f ) for f in os .listdir ( image_dir ) if f .endswith ( '.K.3 Search Algorithms in Retrieve ModuleIn our experiments, we found that accurately locating relevant information within a trajectory is a challenging task.Although the addition of the retrieve component (gray-box) did not lead to a significant improvement in performance in this specific case, its impact has been notable in other settings, such as in GPT-Pilot.As shown in Table3, the integration of retrieve in GPT-Pilot brought substantial gains.Step Truncate (head) 86.34%Step Truncate (middle) 86.61%Step Truncate (tail) 83.88%We conducted an ablation study on GPT-Pilot to optimize the retrieval of useful information at each step.Our experiments revealed that in large trajectories, truncating the final sections of the file often results in losing critical information, as the latter part of the trajectory typically contains dense information about the final development state.Truncating the beginning of the trajectory proved to be the most effective in improving the retrieval efficiency.For individual steps, truncating the middle section worked best.This is because error messages usually appear early in the output, while the corresponding file paths and specific error locations are found towards the end.By focusing on these retrieval strategies, we can significantly enhance the performance of the retrieve component, particularly in complex scenarios like GPT-Pilot.L.4 Prompt for Ask Module (for requirement check)def get_ask_prompt ( criteria : str , evidence : str ) -&gt; str :return f " " " Provided below is relevant information about the project : { evidence } Kindly perform an evaluation of the following criteria : { criteria } As per the guidelines , respond with either &lt; SATISFIED &gt; or &lt; UNSATISFIED &gt; , followed by a concise justification that references specific elements from the project information , such as code snippets , data samples , or output results ." " " L.5 Prompt for Locate Module def g et_locate_prompt ( criteria : str , evidence : str ) -&gt; str : return f " " " Provided below is the structure of the workspace : { workspace_info } This is the criteria related to the task : { criteria } Follow the format in the example below and return only the file paths that match the criteria :Example :Suppose the criteria is : ' The database functionality is implemented in</code>src / db .py <code>, and the logging system is defined in</code>src / logging .py <code>. ' Based on the criteria , the following paths ( no more than 5 ) should be returned , each wrapped in dollar signs (</code>$ `) : $ / project / src / db .py$ $ / project / src / logging .py$ " " " M Judge Evidences Collected from Agent-as-a-JudgeThe Agent-as-a-Judge system dynamically collects evidence throughout the development process of the codegenerating agentic system to evaluate whether each requirement is satisfied.Using this auxiliary information, the judge agent decides to focus on relevant data such as code, file structures, and trajectories.This gathered evidence supports the agent's final judgment, ensuring an informed and accurate assessment of the project's compliance with its requirements.A sample of the collected evidence is shown in the following Appendix M.
Learning to generalize from sparse and underspecified rewards. Rishabh Agarwal, Chen Liang, Dale Schuurmans, Mohammad Norouzi, International conference on machine learning. PMLR2019</p>
<p>Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, Nagarajan Natarajan, Masai, arXiv:2406.11638Modular architecture for software-engineering ai agents. 2024arXiv preprint</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, arXiv:2406.184032024arXiv preprint</p>
<p>Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, Tushar Khot, arXiv:2409.07440Super: Evaluating agents on setting up and executing tasks from research repositories. 2024arXiv preprint</p>
<p>Multipl-e: a scalable and polyglot approach to benchmarking neural code generation. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, IEEE Transactions on Software Engineering. 4972023</p>
<p>Can it edit? evaluating the ability of large language models to follow code editing instructions. Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Jacob Ginesin, Edward Berman, George Chakhnashvili, Anton Lozhkov, Carolyn Jane Anderson, Arjun Guha, 2024</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal A Maksin, Lilian Patwardhan, Aleksander Weng, Mkadry, 2024</p>
<p>. Harrison Chase, Langchain, 2022</p>
<p>Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, arXiv:2406.01304Issue resolving with multi-agent and task graphs. 2024aarXiv preprint</p>
<p>Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun, arXiv:2402.047882024barXiv preprint</p>
<p>Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, arXiv:2406.10819A dataset for gui-oriented multimodal llm-based agents. 2024carXiv preprint</p>
<p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang, arXiv:2402.10669Humans or llms as the judge? a study on judgement biases. 2024darXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, arXiv:2401.013352024earXiv preprint</p>
<p>Trace is the new autodiff-unlocking efficient optimization of computational workflows. Ching-An Cheng, Allen Nie, Adith Swaminathan, arXiv:2406.162182024arXiv preprint</p>
<p>Combining forecasts: A review and annotated bibliography. Robert T Clemen, International journal of forecasting. 541989</p>
<p>Support-vector networks. Corinna Cortes, Machine Learning. 1995</p>
<p>Can llm be a personalized judge?. Yijiang River, Dong , Tiancheng Hu, Nigel Collier ; Zhuoyun, Chen Du, Wei Qian, Zihao Liu, Yifei Xie, Yufan Wang, Weize Dang, Cheng Chen, Yang, arXiv:2406.11657arXiv:2406.08979Multi-agent software development through cross-team collaboration. 2024. 2024arXiv preprint</p>
<p>From data mining to knowledge discovery in databases. Usama Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, AI magazine. 1731996</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023arXiv preprint</p>
<p>Monetary relationships: a view from Threadneedle Street. Charles Goodhart, 1976. 2023University of WarwickSignificant Gravitas. Auto-gpt. GitHub repository</p>
<p>Thirteen theorems in search of the truth. Bernard Grofman, Guillermo Owen, Scott L Feld, Theory and decision. 1531983</p>
<p>Ds-agent: Automated data science by empowering large language models with case-based reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, arXiv:2402.174532024arXiv preprint</p>
<p>Fixeval: Execution-based evaluation of program fixes for competitive programming problems. Md Mahim, Anjum Haque, 2023Virginia TechPhD thesis</p>
<p>The robust beauty of majority rules in group decisions. Reid Hastie, Tatsuya Kameda, Psychological review. 11224942005</p>
<p>Trevor Hastie, Robert Tibshirani, Jerome H Friedman, 10.1007/978-0-387-84858-7The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer20092 edition</p>
<p>Automl: A survey of the state-of-the-art. Knowledge-based systems. Xin He, Kaiyong Zhao, Xiaowen Chu, 2021212106622</p>
<p>Measuring coding challenge competence with apps. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, arXiv:2105.099382021arXiv preprint</p>
<p>Long short-term memory. Hochreiter, 1997Neural Computation MIT-Press</p>
<p>Metagpt: Meta programming for a multi-agent collaborative framework. Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, arXiv:2402.18679The Twelfth International Conference on Learning Representations. 2024a. 2024barXiv preprintData interpreter: An llm agent for data science</p>
<p>Automated design of agentic systems. Shengran Hu, Cong Lu, Jeff Clune, arXiv:2408.084352024arXiv preprint</p>
<p>Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. Dong Huang, Qingwen Bu, M Jie, Michael Zhang, Heming Luck, Cui, arXiv:2312.130102023arXiv preprint</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Forty-first International Conference on Machine Learning. 2024</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024arXiv preprint</p>
<p>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, arXiv:2406.067692024arXiv preprint</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues?. 2023arXiv preprint</p>
<p>From llms to llm-based agents for software engineering: A survey of current, challenges and future. Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen, arXiv:2408.024792024arXiv preprint</p>
<p>Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu, Dsbench, arXiv:2409.07703How far are data science agents to becoming data science experts?. 2024arXiv preprint</p>
<p>Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, arXiv:2310.03714Compiling declarative language model calls into self-improving pipelines. 2023arXiv preprint</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried, arXiv:2401.13649Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. 2024arXiv preprint</p>
<p>Ds-1000: A natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida Wang, Tao Yu, International Conference on Machine Learning. PMLR2023. 2024</p>
<p>Intuitions about combining opinions: Misappreciation of the averaging principle. P Richard, Jack B Larrick, Soll, Management science. 5212006</p>
<p>Binary codes capable of correcting deletions, insertions, and reversals. V Levenshtein, Proceedings of the Soviet physics doklady. the Soviet physics doklady1966</p>
<p>Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Camel, arXiv:2303.17760Communicative agents for" mind" exploration of large scale language model society. 2023arXiv preprint</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024arXiv preprint</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>Large language model-based agents for software engineering: A survey. Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, Yiling Lou, arXiv:2409.029772024arXiv preprint</p>
<p>Repobench: Benchmarking repository-level code auto-completion systems. Tianyang Liu, Canwen Xu, Julian Mcauley, arXiv:2306.030912023aarXiv preprint</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, arXiv:2308.03688Evaluating llms as agents. 2023barXiv preprint</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-Eval, arXiv:2303.16634Nlg evaluation using gpt-4 with better human alignment. 2023carXiv preprint</p>
<p>Ml-bench: Large language models leverage open-source libraries for machine learning tasks. Yuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu, Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan Hu, Zengxian Yang, Kaikai An, arXiv:2311.098352023darXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024aarXiv preprint</p>
<p>Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation. Yujie Lu, Xianjun Yang, Xiujun Li, Xin , Eric Wang, William Yang, Wang , Advances in Neural Information Processing Systems. 2024b36</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Qingwei Lin, Jianguang Lou, Shifeng Chen, Yansong Tang, Weizhu Chen, arXiv:2407.10627Arena learning: Build data flywheel for llms post-training via simulated chatbot arena. 2024arXiv preprint</p>
<p>Code agents are state of the art software testers. Niels Mündler, Mark Niklas Müller, Jingxuan He, Martin Vechev, arXiv:2406.129522024arXiv preprint</p>
<p>Autonomous evaluation and refinement of digital agents. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, Alane Suhr, First Conference on Language Modeling. 2024</p>
<p>Constructive multiple-choice testing system. Jooyong Park, 10.1111/j.1467-8535.2010.01058.xBritish Journal of Educational Technology. 4162010</p>
<p>Hyperagent: Generalist software engineering agents to solve coding tasks at scale. Phong X Huy Nhat Phan, Nguyen, Nghi, Bui, arXiv:2409.162992024arXiv preprint</p>
<p>Pythagora, Io, Gpt-pilot: Your ai copilot for software development. 2023</p>
<p>Chatdev: Communicative agents for software development. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024a1</p>
<p>Scaling large-language-model-based multi-agent collaboration. Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun, arXiv:2406.071552024barXiv preprint</p>
<p>Bo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai Yang, Hang Dong, Jue Zhang, Lu Wang, arXiv:2311.17541A code-first agent framework. 2023arXiv preprint</p>
<p>Is llm-as-a-judge robust? investigating universal adversarial attacks on zero-shot llm assessment. Adian Vyas Raina, Mark Liusie, Gales, arXiv:2402.140162024arXiv preprint</p>
<p>Reimers, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms. Lin Shi, Weicheng Ma, Soroush Vosoughi, arXiv:2406.077912024arXiv preprint</p>
<p>Linxin Song, Jiale Liu, Jieyu Zhang, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, Chi Wang, arXiv:2405.19425Adaptive in-conversation team building for language model agents. 2024arXiv preprint</p>
<p>Towards general computer control: A multimodal agent for red dead redemption ii as a case study. Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, arXiv:2403.031862024arXiv preprint</p>
<p>Magis: Llm-based multi-agent framework for github issue resolution. Wei Tao, Yucheng Zhou, Wenqiang Zhang, Yu Cheng, arXiv:2403.179272024arXiv preprint</p>
<p>Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes, arXiv:2406.12624Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. 2024arXiv preprint</p>
<p>Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu, Maosong Sun, Debugbench, arXiv:2401.04621Evaluating debugging capability of large language models. 2024arXiv preprint</p>
<p>Autodev: Automated ai-driven development. Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian Moghaddam, Neel Sundaresan, arXiv:2403.082992024arXiv preprint</p>
<p>Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, Jitao Sang, arXiv:2406.010142024aarXiv preprint</p>
<p>. Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li, arXiv:2408.026662024bSelf-taught evaluators. arXiv preprint</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji, arXiv:2402.01030Executable code actions elicit better llm agents. 2024carXiv preprint</p>
<p>Opendevin: An open platform for ai software developers as generalist agents. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, arXiv:2407.167412024darXiv preprint</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Crisp-dm: Towards a standard process model for data mining. Rüdiger Wirth, Jochen Hipp, Proceedings of the 4th international conference on the practical applications of knowledge discovery and data mining. the 4th international conference on the practical applications of knowledge discovery and data miningManchester20001</p>
<p>Intelligent agents. Multiagent systems: A modern approach to distributed artificial intelligence. Michael Wooldridge, 19991</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.081552023arXiv preprint</p>
<p>Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, Qingyun Wu, arXiv:2403.11322Stateflow: Enhancing llm task-solving through state-driven workflows. 2024aarXiv preprint</p>
<p>Os-copilot: Towards generalist computer agents with self-improvement. Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong, arXiv:2402.074562024barXiv preprint</p>
<p>Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, Lingming Zhang, arXiv:2407.01489Agentless: Demystifying llm-based software engineering agents. 2024arXiv preprint</p>
<p>Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi, Ziniu Hu, Philip Torr, Bernard Ghanem, Guohao Li, ; Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, Chunyuan Li, arXiv:2402.04559arXiv:2410.02712Can large language model agents simulate human trust behaviors?. 2024. 2024arXiv preprintLlava-critic: Learning to evaluate multimodal models</p>
<p>Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie, Yongchao Chen, Shilong Liu, Bochen Qian, arXiv:2407.01511Cross-environment agent benchmark for multimodal language model agents. 2024arXiv preprint</p>
<p>John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, Ofir Press, arXiv:2405.15793Swe-agent: Agent-computer interfaces enable automated software engineering. 2024aarXiv preprint</p>
<p>Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, Gang Yu, arXiv:2312.13771Appagent: Multimodal agents as smartphone users. 2023arXiv preprint</p>
<p>Matplotagent: Method and evaluation for llm-based agentic scientific data visualization. Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, arXiv:2402.114532024barXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Repocoder: Repository-level code completion through iterative retrieval and generation. Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, Weizhu Chen, arXiv:2303.125702023arXiv preprint</p>
<p>Auto arena of llms: Automating llm evaluations with agent peer-battles and committee discussions. Ruochen Zhao, Wenxuan Zhang, Ken Yew, Deli Chia, Lidong Zhao, Bing, arXiv:2405.202672024arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>Codebertscore: Evaluating code generation with pretrained models of code. Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig, arXiv:2302.055272023aarXiv preprint</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, arXiv:2307.13854A realistic web environment for building autonomous agents. 2023barXiv preprint</p>
<p>Agents: An open-source framework for autonomous language agents. Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, arXiv:2309.078702023carXiv preprint</p>
<p>Symbolic learning enables self-evolving agents. Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, arXiv:2406.18532arXiv:2305.17066Mindstorms in natural languagebased societies of mind. 2024. 2023arXiv preprint</p>
<p>Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, Jurgen Schmidhuber, arXiv:2402.16823Language agents as optimizable graphs. 2024arXiv preprint</p>
<p>Ice-score: Instructing large language models to evaluate code. Terry Yue, Zhuo , arXiv:2304.143172023arXiv preprint</p>
<p>Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur, Bani Yusuf, Haolan Zhan, arXiv:2406.15877Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. 2024arXiv preprint</p>
<p>Judges and Subjects of Evaluation The following table summarizes the judge and the subject being evaluated in each experiment: Experiment Who is the Judge? Who is being Judged?. </p>
<p>LLM-as-a-Judge. Human, </p>            </div>
        </div>

    </div>
</body>
</html>
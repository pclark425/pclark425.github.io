<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1026 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1026</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1026</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-266435305</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.13565v1.pdf" target="_blank">Automatic Curriculum Learning with Gradient Reward Signals</a></p>
                <p><strong>Paper Abstract:</strong> This paper investigates the impact of using gradient norm reward signals in the context of Automatic Curriculum Learning (ACL) for deep reinforcement learning (DRL). We introduce a framework where the teacher model, utilizing the gradient norm information of a student model, dynamically adapts the learning curriculum. This approach is based on the hypothesis that gradient norms can provide a nuanced and effective measure of learning progress. Our experimental setup involves several reinforcement learning environments (PointMaze, AntMaze, and AdroitHandRelocate), to assess the efficacy of our method. We analyze how gradient norm rewards influence the teacher's ability to craft challenging yet achievable learning sequences, ultimately enhancing the student's performance. Our results show that this approach not only accelerates the learning process but also leads to improved generalization and adaptability in complex tasks. The findings underscore the potential of gradient norm signals in creating more efficient and robust ACL systems, opening new avenues for research in curriculum learning and reinforcement learning.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1026.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1026.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PointMaze Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Student agent in the PointMaze environment (soft actor-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated point-mass navigation agent trained with Soft Actor-Critic (SAC) as the student in a Teacher-Student Automatic Curriculum Learning setup; teacher selects initial states ρ0 and is rewarded using student gradient-norm signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Curriculum Learning with Gradient Reward Signals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Student (Point agent, SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A reinforcement-learning student agent trained with Soft Actor-Critic (SAC); learns navigation/policy from episodes starting from teacher-selected initial states; teacher (policy-gradient) receives gradient-norm-based rewards derived from the student's parameter gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>PointMaze</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 2D navigation / maze environment where a point-like agent must navigate to a goal; relatively low-dimensional state and action spaces compared to the other environments in the paper, used to assess curriculum effects on straightforward navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task difficulty approximated qualitatively by goal distance and maze geometry; implicit measures used in the paper include episode returns (train/eval) and gradient norm magnitudes. No explicit numeric state-space size reported for PointMaze.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low-to-medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation is realized via teacher-selected initial state distribution ρ0 (starting positions) and implicitly by different maze start-goal pairings; paper reports tracking average goal distance selected by the teacher as a proxy for task difficulty/variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low-to-medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Train return, Eval return, and student gradient norms (average gradient per timestep and overall episode gradient)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: agents with a teacher (especially teacher using average gradient per timestep as reward) converged faster and reached peak returns earlier on PointMaze compared to no-teacher baseline; explicit numeric returns not reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Described qualitatively: the teacher learns to increase task difficulty over time (e.g., increasing average goal distance), indicating a relationship where increasing complexity (farther goals) is scheduled as the student improves. The paper also notes that different gradient-reward formulations influence learning speed and gradient dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>With relatively low complexity and limited variation (baseline PointMaze), curriculum via a trained teacher using average gradient per timestep produced faster convergence and improved eval-return trajectories versus no teacher (qualitative result; no numeric values provided).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic Curriculum Learning (Teacher-Student ACL) where teacher controls initial state distribution ρ0; student trained with SAC; teacher trained with policy-gradient using gradient-norm rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative improvement reported: teacher-led curricula (gradient-norm rewards) reduced samples-to-peak (faster convergence) compared to no-teacher; experiments ran up to ~1e6 environment steps but no precise sample counts to specific performance thresholds are given.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In the simpler PointMaze domain, a teacher that optimizes average gradient per timestep as reward accelerates student learning (faster convergence in train/eval returns) compared to both an episode-gradient teacher and no teacher; gradient-norm-based rewards enable the teacher to produce progressively harder initial states (larger goal distance) that improve learning speed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning with Gradient Reward Signals', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1026.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1026.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AntMaze Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Student agent in the AntMaze environment (soft actor-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated quadruped (Ant) navigation agent trained with Soft Actor-Critic in maze navigation; used to test Teacher-Student ACL, but the paper reports little to no improvement from teaching in this environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Curriculum Learning with Gradient Reward Signals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Student (Ant agent, SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A reinforcement-learning student agent using Soft Actor-Critic (SAC) controlling an Ant (quadruped) morphology to solve navigation tasks in a maze (AntMaze); teacher supplies initial states ρ0 and is trained with policy-gradient using student's gradient-norm signals.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AntMaze</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A more complex navigation environment involving a simulated quadruped (Ant) that must navigate maze-like spaces; dynamics and state/action dimensionality are larger and more challenging than PointMaze, testing both locomotion and path-planning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity characterized qualitatively by environment difficulty for the given agent and capacity of the learning algorithm; authors note inability to obtain significant learning—suggesting large state/action dimensionality and exploration difficulty. No explicit numeric complexity metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation implemented via teacher-selected initial states ρ0; paper does not report extensive procedural variation parameters for AntMaze. Teacher-controlled starting-state variation exists but did not yield clear benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Train return and Eval return over environment steps; student gradient norms monitored.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: after 1,000,000 environment steps agents with and without a teacher performed similarly (no significant improvement from teaching). No numeric returns provided.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper suggests a negative outcome of high complexity: AntMaze may be too challenging for the student models/algorithms used, so curriculum by initial-state selection did not improve learning—implying that when environment complexity exceeds agent capacity, variation or curriculum may not help without stronger models or different learning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Tested implicitly: high complexity (AntMaze) with teacher-controlled initial states produced no significant improvement; agents with and without teachers performed similarly after 1e6 steps (qualitative, no numeric values).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic Curriculum Learning (Teacher-Student ACL) where teacher controls initial states; student trained with SAC. Variants of A_S also tested (reparametrize, reinforce) without success.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No improvement observed in sample efficiency for AntMaze under the tested settings; after 1,000,000 steps teacher did not yield faster progress compared to no teacher.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AntMaze appeared too challenging for the model capacity and learning algorithms applied; teacher-driven curricula via initial-state selection and gradient-norm rewards did not produce measurable improvements, indicating a limitation: high environment complexity can overwhelm curriculum benefits if student capacity or algorithms are insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning with Gradient Reward Signals', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1026.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1026.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdroitHandRelocate Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Student agent in the AdroitHandRelocate environment (soft actor-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated high-dimensional hand manipulation agent (Adroit hand) trained with Soft Actor-Critic in a relocation/manipulation task; teacher selects initial states and demonstrates strong curriculum benefits when using average gradient-per-timestep rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Curriculum Learning with Gradient Reward Signals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Student (Adroit hand, SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A reinforcement-learning student agent using Soft Actor-Critic (SAC) controlling a high-dimensional simulated robotic hand (Adroit) to relocate a ball to a goal; teacher (policy-gradient) chooses initial states (including hand positions/velocities and ball positions) and is rewarded using student's gradient-norm signals.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (manipulator)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AdroitHandRelocate</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A complex, high-dimensional manipulation task where a multi-finger simulated hand must relocate a ball to a target; environment complexity arises from high-dimensional state (hand pose, velocities) and control, contact dynamics, and multiple controllable initial conditions (goal distance, hand states). The teacher can control many starting-state dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity characterized by high-dimensional starting-state space, contact-rich manipulation dynamics, and larger state/action dimensionality; paper uses average goal distance and gradient-norm signals as proxies for task difficulty; exact dimensionalities not enumerated in text but described as 'much higher-dimensional than the mazes.'</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation measured via the teacher-controlled initial-state distribution ρ0 across many dimensions (initial hand position, velocity, ball position), and monitored via average goal distance chosen by teacher; described qualitatively as high variation due to many degrees of freedom.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Eval return, Train return, and student gradient norms (average per timestep and overall episode gradient); also tracked curriculum proxy 'average goal distance'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: teacher using average gradient per timestep outperformed both the teacher using overall episode gradient and the no-teacher baseline—showing faster learning and improved eval returns; episode-gradient teacher outperformed no teacher for much of training but converged near similar returns later. No numeric return values provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Qualitatively described: the teacher learned to increase task difficulty (e.g., larger goal distances and manipulation-challenging initial hand states) over time, and doing so improved student performance; also observed that average-gradient-per-timestep teacher produced lower-magnitude but more directionally precise gradients in the student, which correlated with better eval performance—implying a trade-off between gradient magnitude and precision in high-complexity, high-variation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Tested: in this high-complexity, high-variation environment, teacher-led curriculum (especially average gradient-per-timestep reward) improved learning speed and eval returns relative to no-teacher; qualitative improvements reported but no numeric values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic Curriculum Learning (Teacher-Student ACL) with teacher control over initial states ρ0 (many high-dimensional parameters); student trained with SAC; teacher trained with policy-gradient using gradient-norm rewards (average per timestep and episode-level variants compared).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative improvement: teacher using average gradient per timestep increased sample efficiency (faster progress to higher eval returns) in AdroitHandRelocate versus no teacher; experiments ran up to ~1e6 environment steps but no quantitative sample-efficiency numbers to fixed thresholds are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In a high-dimensional manipulation domain, gradient-norm-based teacher rewards (especially average gradient per timestep) enable the teacher to produce increasingly difficult curricula (greater goal distance and varied initial hand/ball states) that materially accelerate student learning and improve eval returns; student gradient norms under this teacher were smaller but more precise, suggesting a trade-off where lower-magnitude gradients aligned better with productive parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Curriculum Learning with Gradient Reward Signals', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teachmyagent: a benchmark for automatic curriculum learning in deep rl <em>(Rating: 2)</em></li>
                <li>Automatic curriculum learning for deep rl: A short survey <em>(Rating: 2)</em></li>
                <li>A review of the evaluation system for curriculum learning <em>(Rating: 1)</em></li>
                <li>Curriculum learning: A survey <em>(Rating: 1)</em></li>
                <li>Gymnasium robotics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1026",
    "paper_id": "paper-266435305",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "PointMaze Agent",
            "name_full": "Student agent in the PointMaze environment (soft actor-critic)",
            "brief_description": "A simulated point-mass navigation agent trained with Soft Actor-Critic (SAC) as the student in a Teacher-Student Automatic Curriculum Learning setup; teacher selects initial states ρ0 and is rewarded using student gradient-norm signals.",
            "citation_title": "Automatic Curriculum Learning with Gradient Reward Signals",
            "mention_or_use": "use",
            "agent_name": "Student (Point agent, SAC)",
            "agent_description": "A reinforcement-learning student agent trained with Soft Actor-Critic (SAC); learns navigation/policy from episodes starting from teacher-selected initial states; teacher (policy-gradient) receives gradient-norm-based rewards derived from the student's parameter gradients.",
            "agent_type": "simulated agent",
            "environment_name": "PointMaze",
            "environment_description": "A 2D navigation / maze environment where a point-like agent must navigate to a goal; relatively low-dimensional state and action spaces compared to the other environments in the paper, used to assess curriculum effects on straightforward navigation tasks.",
            "complexity_measure": "Task difficulty approximated qualitatively by goal distance and maze geometry; implicit measures used in the paper include episode returns (train/eval) and gradient norm magnitudes. No explicit numeric state-space size reported for PointMaze.",
            "complexity_level": "low-to-medium",
            "variation_measure": "Variation is realized via teacher-selected initial state distribution ρ0 (starting positions) and implicitly by different maze start-goal pairings; paper reports tracking average goal distance selected by the teacher as a proxy for task difficulty/variation.",
            "variation_level": "low-to-medium",
            "performance_metric": "Train return, Eval return, and student gradient norms (average gradient per timestep and overall episode gradient)",
            "performance_value": "Qualitative: agents with a teacher (especially teacher using average gradient per timestep as reward) converged faster and reached peak returns earlier on PointMaze compared to no-teacher baseline; explicit numeric returns not reported in text.",
            "complexity_variation_relationship": "Described qualitatively: the teacher learns to increase task difficulty over time (e.g., increasing average goal distance), indicating a relationship where increasing complexity (farther goals) is scheduled as the student improves. The paper also notes that different gradient-reward formulations influence learning speed and gradient dynamics.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "With relatively low complexity and limited variation (baseline PointMaze), curriculum via a trained teacher using average gradient per timestep produced faster convergence and improved eval-return trajectories versus no teacher (qualitative result; no numeric values provided).",
            "training_strategy": "Automatic Curriculum Learning (Teacher-Student ACL) where teacher controls initial state distribution ρ0; student trained with SAC; teacher trained with policy-gradient using gradient-norm rewards.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Qualitative improvement reported: teacher-led curricula (gradient-norm rewards) reduced samples-to-peak (faster convergence) compared to no-teacher; experiments ran up to ~1e6 environment steps but no precise sample counts to specific performance thresholds are given.",
            "key_findings": "In the simpler PointMaze domain, a teacher that optimizes average gradient per timestep as reward accelerates student learning (faster convergence in train/eval returns) compared to both an episode-gradient teacher and no teacher; gradient-norm-based rewards enable the teacher to produce progressively harder initial states (larger goal distance) that improve learning speed.",
            "uuid": "e1026.0",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning with Gradient Reward Signals",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "AntMaze Agent",
            "name_full": "Student agent in the AntMaze environment (soft actor-critic)",
            "brief_description": "A simulated quadruped (Ant) navigation agent trained with Soft Actor-Critic in maze navigation; used to test Teacher-Student ACL, but the paper reports little to no improvement from teaching in this environment.",
            "citation_title": "Automatic Curriculum Learning with Gradient Reward Signals",
            "mention_or_use": "use",
            "agent_name": "Student (Ant agent, SAC)",
            "agent_description": "A reinforcement-learning student agent using Soft Actor-Critic (SAC) controlling an Ant (quadruped) morphology to solve navigation tasks in a maze (AntMaze); teacher supplies initial states ρ0 and is trained with policy-gradient using student's gradient-norm signals.",
            "agent_type": "simulated robotic agent",
            "environment_name": "AntMaze",
            "environment_description": "A more complex navigation environment involving a simulated quadruped (Ant) that must navigate maze-like spaces; dynamics and state/action dimensionality are larger and more challenging than PointMaze, testing both locomotion and path-planning capabilities.",
            "complexity_measure": "Complexity characterized qualitatively by environment difficulty for the given agent and capacity of the learning algorithm; authors note inability to obtain significant learning—suggesting large state/action dimensionality and exploration difficulty. No explicit numeric complexity metrics reported.",
            "complexity_level": "high",
            "variation_measure": "Variation implemented via teacher-selected initial states ρ0; paper does not report extensive procedural variation parameters for AntMaze. Teacher-controlled starting-state variation exists but did not yield clear benefits.",
            "variation_level": "medium",
            "performance_metric": "Train return and Eval return over environment steps; student gradient norms monitored.",
            "performance_value": "Qualitative: after 1,000,000 environment steps agents with and without a teacher performed similarly (no significant improvement from teaching). No numeric returns provided.",
            "complexity_variation_relationship": "Paper suggests a negative outcome of high complexity: AntMaze may be too challenging for the student models/algorithms used, so curriculum by initial-state selection did not improve learning—implying that when environment complexity exceeds agent capacity, variation or curriculum may not help without stronger models or different learning algorithms.",
            "high_complexity_low_variation_performance": "Tested implicitly: high complexity (AntMaze) with teacher-controlled initial states produced no significant improvement; agents with and without teachers performed similarly after 1e6 steps (qualitative, no numeric values).",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic Curriculum Learning (Teacher-Student ACL) where teacher controls initial states; student trained with SAC. Variants of A_S also tested (reparametrize, reinforce) without success.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "No improvement observed in sample efficiency for AntMaze under the tested settings; after 1,000,000 steps teacher did not yield faster progress compared to no teacher.",
            "key_findings": "AntMaze appeared too challenging for the model capacity and learning algorithms applied; teacher-driven curricula via initial-state selection and gradient-norm rewards did not produce measurable improvements, indicating a limitation: high environment complexity can overwhelm curriculum benefits if student capacity or algorithms are insufficient.",
            "uuid": "e1026.1",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning with Gradient Reward Signals",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "AdroitHandRelocate Agent",
            "name_full": "Student agent in the AdroitHandRelocate environment (soft actor-critic)",
            "brief_description": "A simulated high-dimensional hand manipulation agent (Adroit hand) trained with Soft Actor-Critic in a relocation/manipulation task; teacher selects initial states and demonstrates strong curriculum benefits when using average gradient-per-timestep rewards.",
            "citation_title": "Automatic Curriculum Learning with Gradient Reward Signals",
            "mention_or_use": "use",
            "agent_name": "Student (Adroit hand, SAC)",
            "agent_description": "A reinforcement-learning student agent using Soft Actor-Critic (SAC) controlling a high-dimensional simulated robotic hand (Adroit) to relocate a ball to a goal; teacher (policy-gradient) chooses initial states (including hand positions/velocities and ball positions) and is rewarded using student's gradient-norm signals.",
            "agent_type": "simulated robotic agent (manipulator)",
            "environment_name": "AdroitHandRelocate",
            "environment_description": "A complex, high-dimensional manipulation task where a multi-finger simulated hand must relocate a ball to a target; environment complexity arises from high-dimensional state (hand pose, velocities) and control, contact dynamics, and multiple controllable initial conditions (goal distance, hand states). The teacher can control many starting-state dimensions.",
            "complexity_measure": "Complexity characterized by high-dimensional starting-state space, contact-rich manipulation dynamics, and larger state/action dimensionality; paper uses average goal distance and gradient-norm signals as proxies for task difficulty; exact dimensionalities not enumerated in text but described as 'much higher-dimensional than the mazes.'",
            "complexity_level": "high",
            "variation_measure": "Variation measured via the teacher-controlled initial-state distribution ρ0 across many dimensions (initial hand position, velocity, ball position), and monitored via average goal distance chosen by teacher; described qualitatively as high variation due to many degrees of freedom.",
            "variation_level": "high",
            "performance_metric": "Eval return, Train return, and student gradient norms (average per timestep and overall episode gradient); also tracked curriculum proxy 'average goal distance'.",
            "performance_value": "Qualitative: teacher using average gradient per timestep outperformed both the teacher using overall episode gradient and the no-teacher baseline—showing faster learning and improved eval returns; episode-gradient teacher outperformed no teacher for much of training but converged near similar returns later. No numeric return values provided in text.",
            "complexity_variation_relationship": "Qualitatively described: the teacher learned to increase task difficulty (e.g., larger goal distances and manipulation-challenging initial hand states) over time, and doing so improved student performance; also observed that average-gradient-per-timestep teacher produced lower-magnitude but more directionally precise gradients in the student, which correlated with better eval performance—implying a trade-off between gradient magnitude and precision in high-complexity, high-variation settings.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Tested: in this high-complexity, high-variation environment, teacher-led curriculum (especially average gradient-per-timestep reward) improved learning speed and eval returns relative to no-teacher; qualitative improvements reported but no numeric values provided.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic Curriculum Learning (Teacher-Student ACL) with teacher control over initial states ρ0 (many high-dimensional parameters); student trained with SAC; teacher trained with policy-gradient using gradient-norm rewards (average per timestep and episode-level variants compared).",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Qualitative improvement: teacher using average gradient per timestep increased sample efficiency (faster progress to higher eval returns) in AdroitHandRelocate versus no teacher; experiments ran up to ~1e6 environment steps but no quantitative sample-efficiency numbers to fixed thresholds are provided.",
            "key_findings": "In a high-dimensional manipulation domain, gradient-norm-based teacher rewards (especially average gradient per timestep) enable the teacher to produce increasingly difficult curricula (greater goal distance and varied initial hand/ball states) that materially accelerate student learning and improve eval returns; student gradient norms under this teacher were smaller but more precise, suggesting a trade-off where lower-magnitude gradients aligned better with productive parameter updates.",
            "uuid": "e1026.2",
            "source_info": {
                "paper_title": "Automatic Curriculum Learning with Gradient Reward Signals",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teachmyagent: a benchmark for automatic curriculum learning in deep rl",
            "rating": 2,
            "sanitized_title": "teachmyagent_a_benchmark_for_automatic_curriculum_learning_in_deep_rl"
        },
        {
            "paper_title": "Automatic curriculum learning for deep rl: A short survey",
            "rating": 2,
            "sanitized_title": "automatic_curriculum_learning_for_deep_rl_a_short_survey"
        },
        {
            "paper_title": "A review of the evaluation system for curriculum learning",
            "rating": 1,
            "sanitized_title": "a_review_of_the_evaluation_system_for_curriculum_learning"
        },
        {
            "paper_title": "Curriculum learning: A survey",
            "rating": 1,
            "sanitized_title": "curriculum_learning_a_survey"
        },
        {
            "paper_title": "Gymnasium robotics",
            "rating": 1,
            "sanitized_title": "gymnasium_robotics"
        }
    ],
    "cost": 0.01000425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automatic Curriculum Learning with Gradient Reward Signals
21 Dec 2023</p>
<p>Ryan Campbell ryancampbell@berkeley.edu 
Junsang Yoon junyoon@berkeley.edu </p>
<p>University of California
Berkeley</p>
<p>University of California
Berkeley</p>
<p>Automatic Curriculum Learning with Gradient Reward Signals
21 Dec 2023AA01015F46E2C760DD7A6DBDDCE32DC4arXiv:2312.13565v1[cs.LG]
This study 1 explores an approach to Automatic Curriculum Learning (ACL) in reinforcement learning (RL), focusing on the integration of gradient norm reward signals.Traditional ACL methods primarily rely on predefined metrics that may not adequately capture the intricacies of learning dynamics.Our work proposes a method where the 'teacher' algorithm adapts the curriculum based on the gradient norm information from the 'student' model, hypothesizing that these signals offer a more refined insight into the student's learning progression.We developed a framework (Teacher-Student ACL) where the teacher dynamically adjusts the learning curriculum, guided by the gradient norms of the student's model.This approach allows for a more responsive and tailored learning experience.The experimental setup utilized the PointMaze, AntMaze, and AdroitHandRelocate environments.These diverse settings provided a robust platform to assess the versatility and effectiveness of our proposed method.Our experiments showed that incorporating gradient norm-based rewards significantly impacts the learning curve of the student model.In PointMaze, we observed a marked acceleration in learning, with the student models achieving proficiency more rapidly with the use of a teacher trained on gradient reward signals compared to without any teacher.In the AdroitHandRelocate environment, the student models demonstrated not only faster learning but also improved evaluation return scores.These results suggest that gradient norm signals offer a more nuanced understanding of learning progression, enabling the teacher to devise more effective and challenging curriculums.* Contributions from Ryan Campbell: wrote all of the code; ran all of the experiments; generated all of the visuals and figures; formalized the teacher-student ACL algorithm; thought of idea of using gradient reward signals; writeup † Contributions from Junsang Yoon: Project direction and framework, idea generation.Analysis for experiments, writeup.</p>
<p>The implications of our findings are substantial for the future of curriculum learning in RL.The ability of the gradient norm signals to provide detailed insights into the learning process opens new possibilities for more efficient training of RL models.This could lead to advancements in various applications, from autonomous systems to complex decision-making tasks.Furthermore, our approach underscores the importance of adaptive and dynamic curriculum strategies in enhancing the learning capabilities of AI systems.</p>
<p>Our study establishes the potential of gradient norm reward signals.Future research could explore the integration of this approach in other learning paradigms, further expanding its applicability and impact.Additionally, comparisons to other ACL methods that use other reward signals could lead to further understanding of the efficacy of gradient reward signals.</p>
<p>Introduction</p>
<p>Curriculum learning has emerged as a significant paradigm in artificial intelligence, offering a structured and efficient approach to training machine learning models.This concept, drawing inspiration from the way humans learn, involves progressively training models on tasks of increasing complexity.Curriculum learning stands on the premise that learning is more effective when it follows a certain order of increasing difficulty.Just as a human student learns better when concepts are introduced from simple to complex, machine learning models to benefit from a similar approach.This method contrasts with the traditional random presentation of data, offering a more guided and focused learning trajectory.[7] [8] In domains such as natural language processing and computer vision, curriculum learning has shown to improve model performance, especially in dealing with complex tasks.By starting with simpler, easier-to-learn examples and gradually introducing more challenging data, models can develop a robust understanding of the underlying patterns, leading to better generalization and performance.</p>
<p>Recent developments in RL focus on dynamic curriculum strategies that adjust based on the agent's performance, optimizing the training process and potentially leading to breakthroughs in more sophisticated and adaptive RL systems.Our work attempts to explore the intricacies and benefits of applying curriculum learning principles to reinforcement learning environments.Our desired benefit is that by starting with less complex tasks, agents can quickly grasp basic skills, which can be incrementally built upon for increasingly complex tasks.In particular, we are most interested in being able to automatically generate "good curriculums."While curriculum learning is met with widespread success, there is no clear evaluation method on the effectiveness of a given curriculum.[4] [6] 2 Related Works</p>
<p>Curriculum Learning for Language Modeling</p>
<p>In the domain of language modeling, curriculum learning has been effectively employed to enhance model training and performance.The application of curriculum learning applied to Neural machine translation achieved notable improvements in translation quality.More recently, this approach has been adapted to the pre-training phases of transformer models, enhancing their efficiency in handling complex language tasks.[7]</p>
<p>Curriculum Learning for Vision</p>
<p>In the field of computer vision, curriculum learning has also shown promising results in fields such as object recognition, where models were gradually exposed to increasingly complex objects and scenes.Incremental learning of features and its integration with convolutional neural networks, led to more effective training and better generalization in tasks like image classification and object detection.[7]</p>
<p>Curriculum Learning for Reinforcement Learning</p>
<p>In reinforcement learning, curriculum learning has been employed to structure the learning process of agents, beginning with simpler and progressively moving to more complex environments.This step-by-step approach has not only improved learning efficiency but also enhanced policy generalization in agents, as evidenced in various studies.Its applications range from sequencing tasks for robotic learning to adjusting difficulty levels in game environments, contributing to more robust and adaptable learning processes.Recent developments in RL focus on dynamic curriculum strategies that adjust based on the agent's performance, optimizing the training process and potentially leading to breakthroughs in more sophisticated and adaptive RL systems.[4] The work of curriculum generation is an evolving work in RL as well.One way of curriculum generation is a regret-based curriculum, adapting the training distribution over the parameters of an environment, constantly producing levels at the frontier of an agent's capabilities, resulting in curricula that start simple but become increasingly complex.</p>
<p>3 Formal Background</p>
<p>Automatic Curriculum Learning</p>
<p>In ACL, there is a student and a teacher.The student can be thought of as a regular DRL agent, while the teacher can be thought of as a meta-learner attempting to maximize the sample efficiency of the student.</p>
<p>Consider the Markov Decision Process (MDP) of the student, represented as (S, A, P, R, ρ 0 ).Here S is the state space, A is the action space, P is the transition function, R is the reward function, and ρ 0 is the distribution of initial states.In standard DRL algorithms, the goal is to learn a policy π θ .</p>
<p>Consider the following ACL problem: learn a task-selection function π ϕ : H → T , where H contains information about the student's progress and T is space of tasks assignable by the teacher [5].In this paper, we use the student's rewards from previous assignments as H and initial states, ρ 0 , as tasks assignable by the teacher.Thus, this problem can be understood as another MDP for the teacher.States in this MDP are represented by initial state and reward pairs, (ρ 0 , r).Actions in this MDP are given by initial state assignments to the student, ρ 0 .Finally, the rewards are given by some measure of learning progress by the student, denoted by ∇θ for now.</p>
<p>Teacher-Student ACL Algorithm</p>
<p>Now that the student and teacher MDPs have been formalized, consider how one might go about training the teacher agent.Let A S and A T be DRL algorithms, where A T is an on-policy DRL algorithm.Use the teacher to generate the student dataset, D S , and facilitate student learning by using A S .The outcome of learning from A S can then be used to construct the teacher dataset, D T .Next, use A T to facilitate teacher learning.Repeat this process of student learning followed by teacher learning until the student has achieved some threshold set beforehand.See Algorithm 1 for a more concrete explanation of this process, or Figure 1 for a visualization.Repeat k times:
5: DS = π ϕ (H) 6: ∇θ = AS(π θ , DS) 7: θ ← θ − αS∇θ 8: DT = {(ρ0, r)i, (ρ0)i, (∇θ)i} k i=1 9: ∇ϕ = AT (π ϕ , DT ) 10: ϕ ← ϕ − αT ∇ϕ 11: Return π θ , π ϕ
The hope is that the teacher will learn which environments increase the rate of student learning, thus reducing the number of samples needed otherwise.This is distinct from learning an efficient exploration scheme however, as the teacher controls starting states rather than actions.</p>
<p>Methodology</p>
<p>Implementing Teacher-Student ACL</p>
<p>Many design choices are left to made from Algorithm 1 before actually implementing anything.The two DRL Algorithms A S and A T , as well as the metric of learning progress, ∇θ are all important decisions that need to be made.For A S we used soft actor-critic and for A T we used policy gradient.For learning progress, The first metric, the average gradient size during episode, accounts for how much the student was able to learn at each step starting from the initial state given by the teacher.This may be better than the alternative as it still values steps that slightly counteract each other.The second metric, the size of the overall episode gradient, accounts for the total change that comes from the episode starting from the initial state given by the teacher.</p>
<p>Training Data</p>
<p>Our research utilizes OpenAI gym environments [1], particularly the PointMaze, AntMaze, and AdroitHandRelocate environments [2], as seen in Figure 2. The PointMaze and AntMaze environments are designed for testing reinforcement learning algorithms in navigation and pathfinding tasks.The AdroitHandRelocate environment is a more complicated environment that is designed to test reinforcement learning algorithms in spatial awareness and decision-making.Additionally, the potential starting states of the AdroitHandRelocate environment are much higher-dimensional than the mazes, giving the teacher more control over the student's curriculum.</p>
<p>Model Architecture / Configurations</p>
<p>All our experiments were performed with the following model configurations.These can be found in more depth in the corresponding yaml files.• Total Steps: 1,000,000</p>
<p>• Batch Size: 256</p>
<p>• Hidden Layer Size: 256</p>
<p>• Number of Layers: 3</p>
<p>• Discount Factor: 0.99</p>
<p>• Temperature: 0.05</p>
<p>Smoothing</p>
<p>Each experiment ran for at least 750, 000 steps, and many values were logged frequently.Thus, many graphs look very chaotic and are hard to interpret on their own.Thus, the following smoothing technique was used: ỹi = λỹ i−1 + (1 − λ)y i .Larger λ values (closer to 1) were used whenever the data was logged more frequently.</p>
<p>The raw, unsmoothed graphs can all be found in the appendix.</p>
<p>Findings</p>
<p>To evaluate the effectiveness of teaching, we employed several experiments comparing the students with (1) a teacher with average gradient per time-step as a reward, (2) a teacher with overall episode gradient as a reward, and (3) no teacher.Our findings suggest that curriculum learning with a teacher using gradients as a reward signal is effective.This effect is pronounced after label smoothing, as described in the above section.For most experiments, either one or both teaching methods were shown to outperform the no-teacher baseline, either converging to the peak faster, or having a bigger peak overall.Subsections below discuss these results in more depth.Dissecting the curriculum learned by the teacher showed a clear improvement in the teacher in generating harder tasks over time, which we approximate by the average goal distance that the teacher's generated curriculum provides.</p>
<p>PointMaze Performance</p>
<p>As shown in Figure 3, the train return and eval return converge faster with a teacher than compared to without a teacher.The different teachers tested are the teachers trained with different rewards.</p>
<p>While each agent is able to converge, it does seem that the agent with the teacher using average gradient per time-step is slightly faster than the teacher using overall episode gradient.This is more clear from the eval return, while the train return they seem to be indistinguishable.We were unable to get a significant outcome employing teaching on the AntMaze environment.As exhibited by Figure 4, the agents with a teacher and no teacher all performed similarly after a million environment steps.Note that slight variations of A S were also tested (reparametrize, reinforce), but no significant learning was able to happen regardless.This suggests that the AntMaze environment is either too challenging for the capacity of the models used or the learning algorithms employed.</p>
<p>AdroitHandRelocate Performance</p>
<p>In the AdroitHandRelocate environment, the teacher using average gradient per time-step outperformed both the agent with the teacher using overall episode gradient and no teacher, as shown in Figure 5.The overall episode gradient over-performed no teacher throughout the environment steps, until converging towards a similar return near the end.</p>
<p>Teacher Effect on Gradients</p>
<p>As shown in Figure 6, the student's gradient norms varied over time in ways that depended on the type of teacher present.In both the PointMaze and AdroitHandRelocate environments, the agent with the teacher with average gradient per time-step rewards started with the lowest gradients, but eventually caught up to the agent with the teacher with overall episode gradient rewards.This is surprising, as the agent with the teacher with average gradient per time-step as rewards performed the best in terms of eval return.Thus, this seems to indicate that while the gradient norms are smaller, they are more precise and in the correct direction.</p>
<p>Visualizing the Learned Curriculum</p>
<p>In order to visualize the learned curriculum of the teacher, the distance of the ball to the target was tracked in the AdroitHandRelocate environment.Intuitively, one would expect an easier curriculum to put the ball further and further away as the agent gets better at moving it, and this is what can be seen in Figure 7.This aligns with the intuitive reasoning that harder tasks are associated with farther goals, meaning that the teacher correctly learns to generate more difficult curriculum over time, to improve the performance of the agent.</p>
<p>Additionally, it should be noted that the teacher had control over other things such as initial hand position and velocity, so the teacher did more than just move the ball away.</p>
<p>Conclusion</p>
<p>Through the implementation of our Teacher-Student ACL algorithm, we have created a novel instance of using student model gradients to effectively train a teacher model.The teacher model, guided by the feedback from the student's gradients, succeeded in significantly enhancing the student's learning process.This is evident in the marked improvements in sample efficiency and peak return, surpassing the outcomes when the Looking ahead, this study opens several exciting avenues for future exploration.One particularly promising direction is the investigation of different reward signals based on gradient norms.Focusing on the gradients of specific layers within the agent's network, such as the final layer, might yield intriguing insights and further enhance the teacher's capability to fine-tune the learning process.Moreover, the current restriction to initial state control for the teacher can be seen as a starting point rather than a limitation.Expanding the teacher's control to encompass more aspects of the learning environment could provide a richer dataset for the teacher to learn from.Such an expansion of control parameters could lead to a more comprehensive and effective training regime, potentially unlocking new levels of performance and efficiency in student models.</p>
<p>In conclusion, our research presents a meaningful advancement in the use of gradient norms for ACL.This approach not only improves the performance of student models in reinforcement learning but also opens up new possibilities for refining training processes.Future investigations could explore different gradient-based reward signals and expand the teacher's control within the learning environment, potentially leading to more nuanced and effective training methods.This study contributes to the ongoing evolution of AI training strategies, suggesting new paths for exploration and development.</p>
<p>Algorithm 1 :
1
Teacher-Student ACL 1: Input: DRL Algorithms AS, AT 2: Output: π θ , π ϕ 3: Repeat: 4:</p>
<p>Figure 1 :
1
Figure 1: The Teacher-Student ACL process</p>
<p>Figure 2 :
2
Figure 2: The three different gym environments used in this paper</p>
<p>(a) Eval Return on PointMaze environment (b) Train Return on PointMaze environment</p>
<p>Figure 3 :
3
Figure 3: PointMaze</p>
<p>Figure 4 :
4
Figure 4: Teacher vs No Teacher on AntMaze environment</p>
<p>(a) Eval Return on AdroitHandRelocate environment (b) Train Return on AdroitHandRelocate environment</p>
<p>Figure 5 :
5
Figure 5: AdroitHandRelocate</p>
<p>( a )
a
Gradient norms on PointMaze environment (b) Gradient norms on AdroitHandRelocate environment</p>
<p>Figure 6 :
6
Figure 6: Gradient norms over time for different teachers</p>
<p>Figure 7 :
7
Figure 7: Curriculum learned by teacher with respect to goal distance</p>
<p>Figure 9 :
9
Figure 9: Gradient norm for different number of teacher updates with clear buffer</p>
<p>Figure 10 :
10
Figure 10: clear buffer vs no clear buffer on AdroitHandRelocate</p>
<p>Figure 11 :
11
Figure 11: Unsmoothed graphs of figures</p>
<p>Figure 12 :
12
Figure 12: Unsmoothed graphs of figures</p>
<p>Figure 13 :
13
Figure 13: Unsmoothed graphs of figures</p>
<p>Figure 14 :
14
Figure 14: Unsmoothed graphs of figures</p>
<p>Figure 15 :
15
Figure 15: Unsmoothed graphs of figures</p>
<p>Appendix A: Teacher Updates and Buffer ClearingThe following are two hyperparameter settings that were studied that seemed to not make too much of an impact, but are nonetheless interesting:1. number of teacher updates: determines the number of times to perform an update to the teacher network each time it is updated 2. clear buffer: toggles whether or not to clear the student's replay buffer after each time the teacher gives a new initial state
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, Openai gym. 2016</p>
<p>. Rodrigo De Lazcano, Kallinteris Andreas, Jun Jet Tai, Seungjae Ryan Lee, Jordan Terry, Gymnasium robotics. 2023</p>
<p>. Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, Koray Kavukcuoglu, 2017</p>
<p>A review of the evaluation system for curriculum learning. Fengchun Liu, Tong Zhang, Chunying Zhang, Lu Liu, Liya Wang, Bin Liu, Electronics. 1272023</p>
<p>Automatic curriculum learning for deep rl: A short survey. Rémy Portelas, Cédric Colas, Lilian Weng, Katja Hofmann, Pierre-Yves Oudeyer, 2020</p>
<p>Teachmyagent: a benchmark for automatic curriculum learning in deep rl. Clément Romac, Rémy Portelas, Katja Hofmann, Pierre-Yves Oudeyer, Proceedings of the 38th International Conference on Machine Learning. Marina Meila, Tong Zhang, the 38th International Conference on Machine LearningPMLRJul 2021139</p>
<p>Curriculum learning: A survey. Petru Soviany, Tudor Radu, Paolo Ionescu, Nicu Rota, Sebe, 2022</p>
<p>A survey on curriculum learning. Xin Wang, Yudong Chen, Wenwu Zhu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4492022</p>            </div>
        </div>

    </div>
</body>
</html>
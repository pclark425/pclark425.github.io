<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8969 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8969</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8969</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-e47e6c814d2742527fdd352db13a5fd95b7ce24b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e47e6c814d2742527fdd352db13a5fd95b7ce24b" target="_blank">Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This model consists of a Graph2Seq generator with a novel Bidirectional Gated Graph Neural Network based encoder to embed the passage, and a hybrid evaluator with a mixed objective combining both cross-entropy and RL losses to ensure the generation of syntactically and semantically valid text.</p>
                <p><strong>Paper Abstract:</strong> Natural question generation (QG) aims to generate questions from a passage and an answer. Previous works on QG either (i) ignore the rich structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, or (iii) fail to fully exploit the answer information. To address these limitations, in this paper, we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq generator with a novel Bidirectional Gated Graph Neural Network based encoder to embed the passage, and a hybrid evaluator with a mixed objective combining both cross-entropy and RL losses to ensure the generation of syntactically and semantically valid text. We also introduce an effective Deep Alignment Network for incorporating the answer information into the passage at both the word and contextual levels. Our model is end-to-end trainable and achieves new state-of-the-art scores, outperforming existing methods by a significant margin on the standard SQuAD benchmark.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8969.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8969.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static-syntax-graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Syntax-based static passage graph (dependency-parse)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A directed, unweighted graph constructed from dependency parses of sentences in a passage, where each word is a node and dependency edges connect syntactic relations; sentence-boundary adjacent nodes are also connected to form a passage-level graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>syntax-based static graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct a directed, unweighted graph from text by obtaining dependency parse trees for each sentence (nodes = words, directed edges = dependency relations). To form a passage-level graph, connect nodes at sentence boundaries that are adjacent in the original text. The resulting graph is consumed by a Graph2Seq encoder (BiGGNN) to produce node and graph embeddings for downstream decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>dependency parse trees / directed syntactic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No linearization to text; rather the graph is encoded by a Graph-to-Sequence encoder (BiGGNN) to produce node embeddings and a pooled graph embedding; an RNN decoder (attention-based LSTM with copy and coverage) then generates text autoregressively using the graph-level embedding as initial state and node embeddings as attention memory.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Natural question generation from text (QG) — generate an answerable question given passage and answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Best-performing full model using this static graph: G2S_sta + BERT + RL achieved (Split-1) BLEU-4=17.94, METEOR=21.76, ROUGE-L=46.02, Q-BLEU1=55.60; (Split-2) BLEU-4=18.30, METEOR=21.70, ROUGE-L=45.98, Q-BLEU1=55.20. Ablation: G2S_sta (no BERT, no RL) BLEU-4=16.96 on split-2; removing DAN (answer alignment) drops BLEU-4 to 12.62. Units: percent BLEU / metric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms the semantics-aware dynamic-graph variant (G2S_dyn+BERT+RL: BLEU-4 18.06 on split-2) and several Seq2Seq baselines (e.g., MPQG+R: BLEU-4 14.71 on split-2). Graph2Seq with static syntax graph outperformed Seq2Seq and GCN-based encoders in this work; BiGGNN (fusion of forward/backward) yielded better results than variants that only used forward or backward GGNN.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Encodes linguistic prior knowledge (syntax) explicitly, yielding higher QG performance in experiments; helps graph encoder leverage useful domain structure and improves quality of generated questions (higher BLEU/METEOR/ROUGE and human ratings). Works well with BiGGNN fusion and benefit from BERT embeddings and RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires external syntactic parsers and prior knowledge about the domain; may not generalize if syntactic parses are unreliable or domain-specific syntax differs; introduces dependency on parser quality and extra preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May suffer in domains with poor/noisy parses or lacking prior syntactic knowledge. Paper notes overall errors in generated questions (repeated/unknown words, some semantic/relevance issues) even for static-graph models; when answer alignment (DAN) is removed performance drops sharply, indicating sensitivity to answer incorporation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8969.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8969.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic-semantics-graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantics-aware dynamic passage graph (attention-based kNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A directed, weighted graph built dynamically from contextualized passage embeddings using self-attention to produce a dense adjacency, sparsified by a kNN masking to keep top-K attention links per node, with separate normalized adjacency for incoming and outgoing directions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>semantics-aware dynamic graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute a dense adjacency matrix A from passage word contextual embeddings via A = ReLU(U X)^T ReLU(U X). Apply kNN sparsification to keep the top-K neighbors (including self) per node yielding sparse adjacency Ã, then obtain normalized incoming/outgoing adjacency matrices via softmax on Ã and its transpose to form a directed, weighted graph capturing semantic similarity. This graph is encoded by BiGGNN and decoded to text by an RNN decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>semantic similarity graph among tokens (directed, weighted)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No explicit textual linearization: adjacency computed via self-attention, kNN sparsified, directional normalization; the graph is encoded (node embeddings aggregated with attention-weighted sums) and decoded to text by the Graph2Seq pipeline (BiGGNN encoder + attention LSTM decoder with copy/coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Natural question generation from text (QG).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>G2S_dyn + BERT + RL achieved (Split-1) BLEU-4=17.55, METEOR=21.42, ROUGE-L=45.59, Q-BLEU1=55.40; (Split-2) BLEU-4=18.06, METEOR=21.53, ROUGE-L=45.91, Q-BLEU1=55.00. Ablation: G2S_dyn (no BERT/RL) BLEU-4=16.81 on split-2. K (neighborhood size) used = 10. Units: metric scores as in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Performs slightly worse than the static syntax-based graph variant in most settings reported (G2S_dyn+BERT+RL vs G2S_sta+BERT+RL). Compared favorably to several Seq2Seq baselines, but static graphs yielded higher BLEU and ROUGE in the authors' experiments. Dynamic graph has advantage of not requiring external parsers.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Does not require prior syntactic knowledge or parsing; automatically captures semantic relations via attention and kNN sparsification; backpropagation can flow through sparsification (authors keep top-K attention scores), enabling end-to-end learning of structure.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Slightly lower empirical performance than static syntactic graph on SQuAD QG experiments; dependent on quality of learned attention affinities and hyperparameters (e.g., K).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Underperforms syntax-based static graphs in the reported QG experiments; may be less effective when attention fails to capture useful structural relations. Authors note that static graphs may be beneficial when domain prior knowledge exists, so dynamic may be suboptimal there.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8969.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8969.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph2Seq-BiGGNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-Sequence model with Bidirectional Gated Graph Neural Network encoder and attention LSTM decoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Graph2Seq pipeline that encodes token-level graphs with a novel Bidirectional Gated Graph Neural Network (BiGGNN) fusing incoming and outgoing aggregated messages at each hop, and decodes sequences using an attention-based LSTM with copy and coverage; trained with mixed cross-entropy and RL (BLEU + WMD) objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph2Seq (BiGGNN encoder + attention LSTM decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph encoder: initialize node states from Deep Alignment Network (DAN) passage embeddings; at each hop BiGGNN aggregates incoming and outgoing neighbor information (mean or weighted sum using normalized adjacency), fuses them via a gated sum, updates node states with a GRU; after n hops, project node vectors and max-pool to get graph-level embedding. Decoder: attention-based LSTM (See et al. style) with copy and coverage mechanisms uses the graph-level embedding as initial state and node embeddings as attention memory to generate text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general directed token graphs (syntax-based or semantics-based graphs over passage words)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No explicit serialization; graph->sequence conversion is achieved by: (1) BiGGNN encodes graph to node and pooled graph embeddings; (2) decoder LSTM attends over node embeddings at each decode step, optionally copies tokens from input (pointer mechanism), and outputs a textual sequence via beam search at test time. Training includes cross-entropy pretraining and REINFORCE (SCST) fine-tuning with BLEU-4 and negative WMD (semantic) as reward.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Natural question generation (text generation conditioned on passage + answer); more generally graph-to-text generation tasks referenced in related work (AMR-to-text, SQL-to-text, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Full Graph2Seq models with BiGGNN achieved state-of-the-art on SQuAD QG: G2S_sta+BERT+RL (Split-2) BLEU-4=18.30, METEOR=21.70, ROUGE-L=45.98, Q-BLEU1=55.20; G2S_dyn+BERT+RL (Split-2) BLEU-4=18.06. Ablations: replacing BiGGNN with Seq2Seq: BLEU-4 drops to 16.14; replacing BiGGNN with (undirected) GCN: BLEU-4 drops to 14.47; GGNN-forward/back only variants: BLEU-4 ~16.53/16.75. RL fine-tuning and BERT embeddings further improve metrics (e.g., + ~1-1.3 BLEU points). Human eval: G2S_sta+BERT+RL scored syntactic correctness 4.41, semantic correctness 4.31, relevance 3.79 (vs MPQG+R syntactic 4.34, semantic 4.01, relevance 3.21; ground-truth higher).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Graph2Seq (BiGGNN) outperforms Seq2Seq baselines and other graph encoders in these experiments; BiGGNN (fused forward/backward at each hop) performs better than separate forward/backward GGNN variants and better than GCN on undirected graphs. RL fine-tuning (SCST) with BLEU+WMD reward yields additional improvements over cross-entropy-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Exploits graph structure to model non-local interactions among tokens, leverages both incoming/outgoing directions via fused BiGGNN, supports weighted semantic graphs, integrates answer information via DAN, benefits from pretrained contextual embeddings (BERT) and RL fine-tuning to optimize discrete/semantic metrics. Demonstrated empirical gains and better human-rated question quality.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>More complex architecture requiring graph construction, multiple components (DAN, BiGGNN, RL), and additional hyperparameters (GNN hops, K in kNN). Performance sensitive to components: removing DAN, BiGGNN or BERT substantially reduces performance. GCN-based encoders (undirected) performed poorly in their tests.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Common generation errors reported include repeated or unknown words (syntactic errors) and occasional semantic/relevance failures; model performance degrades significantly without DAN (answer alignment). GCN-based conversion (undirected) gave notably worse results. Transformer baseline trained from scratch also performed poorly, suggesting sensitivity to architecture/pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph2seq: Graph to sequence learning with attention-based neural networks <em>(Rating: 2)</em></li>
                <li>A graph-to-sequence model for amr-to-text generation <em>(Rating: 2)</em></li>
                <li>Gated graph sequence neural networks <em>(Rating: 2)</em></li>
                <li>Learning conditioned graph structures for interpretable visual question answering <em>(Rating: 2)</em></li>
                <li>Graphflow: Exploiting conversation flow with graph neural networks for conversational machine comprehension <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8969",
    "paper_id": "paper-e47e6c814d2742527fdd352db13a5fd95b7ce24b",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Static-syntax-graph",
            "name_full": "Syntax-based static passage graph (dependency-parse)",
            "brief_description": "A directed, unweighted graph constructed from dependency parses of sentences in a passage, where each word is a node and dependency edges connect syntactic relations; sentence-boundary adjacent nodes are also connected to form a passage-level graph.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "syntax-based static graph",
            "representation_description": "Construct a directed, unweighted graph from text by obtaining dependency parse trees for each sentence (nodes = words, directed edges = dependency relations). To form a passage-level graph, connect nodes at sentence boundaries that are adjacent in the original text. The resulting graph is consumed by a Graph2Seq encoder (BiGGNN) to produce node and graph embeddings for downstream decoding.",
            "graph_type": "dependency parse trees / directed syntactic graphs",
            "conversion_method": "No linearization to text; rather the graph is encoded by a Graph-to-Sequence encoder (BiGGNN) to produce node embeddings and a pooled graph embedding; an RNN decoder (attention-based LSTM with copy and coverage) then generates text autoregressively using the graph-level embedding as initial state and node embeddings as attention memory.",
            "downstream_task": "Natural question generation from text (QG) — generate an answerable question given passage and answer.",
            "performance_metrics": "Best-performing full model using this static graph: G2S_sta + BERT + RL achieved (Split-1) BLEU-4=17.94, METEOR=21.76, ROUGE-L=46.02, Q-BLEU1=55.60; (Split-2) BLEU-4=18.30, METEOR=21.70, ROUGE-L=45.98, Q-BLEU1=55.20. Ablation: G2S_sta (no BERT, no RL) BLEU-4=16.96 on split-2; removing DAN (answer alignment) drops BLEU-4 to 12.62. Units: percent BLEU / metric scores.",
            "comparison_to_others": "Outperforms the semantics-aware dynamic-graph variant (G2S_dyn+BERT+RL: BLEU-4 18.06 on split-2) and several Seq2Seq baselines (e.g., MPQG+R: BLEU-4 14.71 on split-2). Graph2Seq with static syntax graph outperformed Seq2Seq and GCN-based encoders in this work; BiGGNN (fusion of forward/backward) yielded better results than variants that only used forward or backward GGNN.",
            "advantages": "Encodes linguistic prior knowledge (syntax) explicitly, yielding higher QG performance in experiments; helps graph encoder leverage useful domain structure and improves quality of generated questions (higher BLEU/METEOR/ROUGE and human ratings). Works well with BiGGNN fusion and benefit from BERT embeddings and RL fine-tuning.",
            "disadvantages": "Requires external syntactic parsers and prior knowledge about the domain; may not generalize if syntactic parses are unreliable or domain-specific syntax differs; introduces dependency on parser quality and extra preprocessing.",
            "failure_cases": "May suffer in domains with poor/noisy parses or lacking prior syntactic knowledge. Paper notes overall errors in generated questions (repeated/unknown words, some semantic/relevance issues) even for static-graph models; when answer alignment (DAN) is removed performance drops sharply, indicating sensitivity to answer incorporation.",
            "uuid": "e8969.0",
            "source_info": {
                "paper_title": "Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Dynamic-semantics-graph",
            "name_full": "Semantics-aware dynamic passage graph (attention-based kNN)",
            "brief_description": "A directed, weighted graph built dynamically from contextualized passage embeddings using self-attention to produce a dense adjacency, sparsified by a kNN masking to keep top-K attention links per node, with separate normalized adjacency for incoming and outgoing directions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "semantics-aware dynamic graph",
            "representation_description": "Compute a dense adjacency matrix A from passage word contextual embeddings via A = ReLU(U X)^T ReLU(U X). Apply kNN sparsification to keep the top-K neighbors (including self) per node yielding sparse adjacency Ã, then obtain normalized incoming/outgoing adjacency matrices via softmax on Ã and its transpose to form a directed, weighted graph capturing semantic similarity. This graph is encoded by BiGGNN and decoded to text by an RNN decoder.",
            "graph_type": "semantic similarity graph among tokens (directed, weighted)",
            "conversion_method": "No explicit textual linearization: adjacency computed via self-attention, kNN sparsified, directional normalization; the graph is encoded (node embeddings aggregated with attention-weighted sums) and decoded to text by the Graph2Seq pipeline (BiGGNN encoder + attention LSTM decoder with copy/coverage).",
            "downstream_task": "Natural question generation from text (QG).",
            "performance_metrics": "G2S_dyn + BERT + RL achieved (Split-1) BLEU-4=17.55, METEOR=21.42, ROUGE-L=45.59, Q-BLEU1=55.40; (Split-2) BLEU-4=18.06, METEOR=21.53, ROUGE-L=45.91, Q-BLEU1=55.00. Ablation: G2S_dyn (no BERT/RL) BLEU-4=16.81 on split-2. K (neighborhood size) used = 10. Units: metric scores as in experiments.",
            "comparison_to_others": "Performs slightly worse than the static syntax-based graph variant in most settings reported (G2S_dyn+BERT+RL vs G2S_sta+BERT+RL). Compared favorably to several Seq2Seq baselines, but static graphs yielded higher BLEU and ROUGE in the authors' experiments. Dynamic graph has advantage of not requiring external parsers.",
            "advantages": "Does not require prior syntactic knowledge or parsing; automatically captures semantic relations via attention and kNN sparsification; backpropagation can flow through sparsification (authors keep top-K attention scores), enabling end-to-end learning of structure.",
            "disadvantages": "Slightly lower empirical performance than static syntactic graph on SQuAD QG experiments; dependent on quality of learned attention affinities and hyperparameters (e.g., K).",
            "failure_cases": "Underperforms syntax-based static graphs in the reported QG experiments; may be less effective when attention fails to capture useful structural relations. Authors note that static graphs may be beneficial when domain prior knowledge exists, so dynamic may be suboptimal there.",
            "uuid": "e8969.1",
            "source_info": {
                "paper_title": "Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Graph2Seq-BiGGNN",
            "name_full": "Graph-to-Sequence model with Bidirectional Gated Graph Neural Network encoder and attention LSTM decoder",
            "brief_description": "A Graph2Seq pipeline that encodes token-level graphs with a novel Bidirectional Gated Graph Neural Network (BiGGNN) fusing incoming and outgoing aggregated messages at each hop, and decodes sequences using an attention-based LSTM with copy and coverage; trained with mixed cross-entropy and RL (BLEU + WMD) objectives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph2Seq (BiGGNN encoder + attention LSTM decoder)",
            "representation_description": "Graph encoder: initialize node states from Deep Alignment Network (DAN) passage embeddings; at each hop BiGGNN aggregates incoming and outgoing neighbor information (mean or weighted sum using normalized adjacency), fuses them via a gated sum, updates node states with a GRU; after n hops, project node vectors and max-pool to get graph-level embedding. Decoder: attention-based LSTM (See et al. style) with copy and coverage mechanisms uses the graph-level embedding as initial state and node embeddings as attention memory to generate text.",
            "graph_type": "general directed token graphs (syntax-based or semantics-based graphs over passage words)",
            "conversion_method": "No explicit serialization; graph-&gt;sequence conversion is achieved by: (1) BiGGNN encodes graph to node and pooled graph embeddings; (2) decoder LSTM attends over node embeddings at each decode step, optionally copies tokens from input (pointer mechanism), and outputs a textual sequence via beam search at test time. Training includes cross-entropy pretraining and REINFORCE (SCST) fine-tuning with BLEU-4 and negative WMD (semantic) as reward.",
            "downstream_task": "Natural question generation (text generation conditioned on passage + answer); more generally graph-to-text generation tasks referenced in related work (AMR-to-text, SQL-to-text, etc.).",
            "performance_metrics": "Full Graph2Seq models with BiGGNN achieved state-of-the-art on SQuAD QG: G2S_sta+BERT+RL (Split-2) BLEU-4=18.30, METEOR=21.70, ROUGE-L=45.98, Q-BLEU1=55.20; G2S_dyn+BERT+RL (Split-2) BLEU-4=18.06. Ablations: replacing BiGGNN with Seq2Seq: BLEU-4 drops to 16.14; replacing BiGGNN with (undirected) GCN: BLEU-4 drops to 14.47; GGNN-forward/back only variants: BLEU-4 ~16.53/16.75. RL fine-tuning and BERT embeddings further improve metrics (e.g., + ~1-1.3 BLEU points). Human eval: G2S_sta+BERT+RL scored syntactic correctness 4.41, semantic correctness 4.31, relevance 3.79 (vs MPQG+R syntactic 4.34, semantic 4.01, relevance 3.21; ground-truth higher).",
            "comparison_to_others": "Graph2Seq (BiGGNN) outperforms Seq2Seq baselines and other graph encoders in these experiments; BiGGNN (fused forward/backward at each hop) performs better than separate forward/backward GGNN variants and better than GCN on undirected graphs. RL fine-tuning (SCST) with BLEU+WMD reward yields additional improvements over cross-entropy-only training.",
            "advantages": "Exploits graph structure to model non-local interactions among tokens, leverages both incoming/outgoing directions via fused BiGGNN, supports weighted semantic graphs, integrates answer information via DAN, benefits from pretrained contextual embeddings (BERT) and RL fine-tuning to optimize discrete/semantic metrics. Demonstrated empirical gains and better human-rated question quality.",
            "disadvantages": "More complex architecture requiring graph construction, multiple components (DAN, BiGGNN, RL), and additional hyperparameters (GNN hops, K in kNN). Performance sensitive to components: removing DAN, BiGGNN or BERT substantially reduces performance. GCN-based encoders (undirected) performed poorly in their tests.",
            "failure_cases": "Common generation errors reported include repeated or unknown words (syntactic errors) and occasional semantic/relevance failures; model performance degrades significantly without DAN (answer alignment). GCN-based conversion (undirected) gave notably worse results. Transformer baseline trained from scratch also performed poorly, suggesting sensitivity to architecture/pretraining.",
            "uuid": "e8969.2",
            "source_info": {
                "paper_title": "Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph2seq: Graph to sequence learning with attention-based neural networks",
            "rating": 2
        },
        {
            "paper_title": "A graph-to-sequence model for amr-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Gated graph sequence neural networks",
            "rating": 2
        },
        {
            "paper_title": "Learning conditioned graph structures for interpretable visual question answering",
            "rating": 2
        },
        {
            "paper_title": "Graphflow: Exploiting conversation flow with graph neural networks for conversational machine comprehension",
            "rating": 2
        }
    ],
    "cost": 0.013917249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>REINFORCEMENT LEARNING BASED <br> GRAPH-TO-SEQUENCE MODEL FOR NATURAL QUESTION GENERATION</h1>
<p>Yu Chen<br>Department of Computer Science<br>Rensselaer Polytechnic Institute<br>cheny39@rpi.edu</p>
<h2>Lingfei Wu* <br> IBM Research <br> lwu@email.wm.edu</h2>
<p>Mohammed J. Zaki<br>Department of Computer Science<br>Rensselaer Polytechnic Institute<br>zaki@cs.rpi.edu</p>
<h4>Abstract</h4>
<p>Natural question generation ( QG ) aims to generate questions from a passage and an answer. Previous works on QG either (i) ignore the rich structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, or (iii) fail to fully exploit the answer information. To address these limitations, in this paper, we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq generator with a novel Bidirectional Gated Graph Neural Network based encoder to embed the passage, and a hybrid evaluator with a mixed objective combining both cross-entropy and RL losses to ensure the generation of syntactically and semantically valid text. We also introduce an effective Deep Alignment Network for incorporating the answer information into the passage at both the word and contextual levels. Our model is end-to-end trainable and achieves new state-of-the-art scores, outperforming existing methods by a significant margin on the standard SQuAD benchmark.</p>
<h2>1 INTRODUCTION</h2>
<p>Natural question generation ( QG ) has many useful applications such as improving the question answering task (Chen et al., 2017; 2019a) by providing more training data (Tang et al., 2017; Yuan et al., 2017), generating practice exercises and assessments for educational purposes (Heilman \&amp; Smith, 2010; Danon \&amp; Last, 2017), and helping dialog systems to kick-start and continue a conversation with human users (Mostafazadeh et al., 2016). While many existing works focus on QG from images (Fan et al., 2018; Li et al., 2018) or knowledge bases (Serban et al., 2016; Elsahar et al., 2018), in this work, we focus on QG from text.</p>
<p>Conventional methods (Mostow \&amp; Chen, 2009; Heilman \&amp; Smith, 2010; Heilman, 2011) for QG rely on heuristic rules or hand-crafted templates, leading to the issues of low generalizability and scalability. Recent attempts have been focused on exploiting Neural Network (NN) based approaches that do not require manually-designed rules and are end-to-end trainable. Encouraged by the huge success of neural machine translation, these approaches formulate the QG task as a sequence-tosequence (Seq2Seq) learning problem. Specifically, attention-based Seq2Seq models (Bahdanau et al., 2014; Luong et al., 2015) and their enhanced versions with copy (Vinyals et al., 2015; Gu et al., 2016) and coverage (Tu et al., 2016) mechanisms have been widely applied and show promising results on this task (Du et al., 2017; Zhou et al., 2017; Song et al., 2018a; Kumar et al., 2018a). However, these methods typically ignore the hidden structural information associated with a word</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>sequence such as the syntactic parsing tree. Failing to utilize the rich text structure information beyond the simple word sequence may limit the effectiveness of these models for QG.</p>
<p>It has been observed that in general, cross-entropy based sequence training has several limitations like exposure bias and inconsistency between train/test measurement (Ranzato et al., 2015; Wu et al., 2016). As a result, they do not always produce the best results on discrete evaluation metrics on sequence generation tasks such as text summarization (Paulus et al., 2017) or question generation (Song et al., 2017). To cope with these issues, some recent QG approaches (Song et al., 2017; Kumar et al., 2018b) directly optimize evaluation metrics using Reinforcement Learning (RL) (Williams, 1992). However, existing approaches usually only employ evaluation metrics like BLEU and ROUGE-L as rewards for RL training. More importantly, they fail to exploit other important metrics such as syntactic and semantic constraints for guiding high-quality text generation.</p>
<p>Early works on neural QG did not take into account the answer information when generating a question. Recent works have started to explore various means of utilizing the answer information. When question generation is guided by the semantics of an answer, the resulting questions become more relevant and readable. Conceptually, there are three different ways to incorporate the answer information by simply marking the answer location in the passage (Zhou et al., 2017; Zhao et al., 2018; Liu et al., 2019), or using complex passage-answer matching strategies (Song et al., 2017), or separating answers from passages when applying a Seq2Seq model (Kim et al., 2018; Sun et al., 2018). However, they neglect potential semantic relations between passage words and answer words, and thus fail to explicitly model the global interactions among them in the embedding space.</p>
<p>To address these aforementioned issues, in this paper, we present a novel reinforcement learning based generator-evaluator architecture that aims to: i) make full use of rich hidden structure information beyond the simple word sequence; ii) generate syntactically and semantically valid text while maintaining the consistency of train/test measurement; iii) model explicitly the global interactions of semantic relationships between passage and answer at both word-level and contextual-level.</p>
<p>In particular, to achieve the first goal, we explore two different means to either construct a syntaxbased static graph or a semantics-aware dynamic graph from the text sequence, as well as its rich hidden structure information. Then, we design a graph-to-sequence (Graph2Seq) model based generator that encodes the graph representation of a text passage and decodes a question sequence using a Recurrent Neural Network (RNN). Our Graph2Seq model is based on a novel bidirectional gated graph neural network, which extends the gated graph neural network (Li et al., 2015) by considering both incoming and outgoing edges, and fusing them during the graph embedding learning.</p>
<p>To achieve the second goal, we design a hybrid evaluator which is trained by optimizing a mixed objective function that combines both cross-entropy and RL loss. We use not only discrete evaluation metrics like BLEU, but also semantic metrics like word mover's distance (Kusner et al., 2015) to encourage both syntactically and semantically valid text generation. To achieve the third goal, we propose a novel Deep Alignment Network (DAN) for effectively incorporating answer information into the passage at multiple granularity levels.</p>
<p>Our main contributions are as follows:</p>
<ul>
<li>We propose a novel RL-based Graph2Seq model for natural question generation. To the best of our knowledge, we are the first to introduce the Graph2Seq architecture for QG.</li>
<li>We explore both static and dynamic ways of constructing graph from text and are the first to systematically investigate their performance impacts on a GNN encoder.</li>
<li>The proposed model is end-to-end trainable, achieves new state-of-the-art scores, and outperforms existing methods by a significant margin on the standard SQuAD benchmark for QG. Our human evaluation study also corroborates that the questions generated by our model are more natural (semantically and syntactically) compared to other baselines.</li>
</ul>
<h1>2 AN RL-BASED GENERATOR-EVALUATOR ARCHITECTURE</h1>
<p>In this section, we define the question generation task, and then present our RL-based Graph2Seq model for question generation. We first motivate the design, and then present the details of each component as shown in Fig. 1.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overall architecture of the proposed model. Best viewed in color.</p>
<h1>2.1 Problem Formulation</h1>
<p>The goal of question generation is to generate natural language questions based on a given form of data, such as knowledge base triples or tables (Bao et al., 2018), sentences (Du et al., 2017; Song et al., 2018a), or images (Li et al., 2018), where the generated questions need to be answerable from the input data. In this paper, we focus on QG from a given text passage, along with a target answer.
We assume that a text passage is a collection of word tokens $X^{p}=\left{x_{1}^{p}, x_{2}^{p}, \ldots, x_{N}^{p}\right}$, and a target answer is also a collection of word tokens $X^{a}=\left{x_{1}^{a}, x_{2}^{a}, \ldots, x_{L}^{a}\right}$. The task of natural question generation is to generate the best natural language question consisting of a sequence of word tokens $\hat{Y}=\left{y_{1}, y_{2}, \ldots, y_{T}\right}$ which maximizes the conditional likelihood $\hat{Y}=\arg \max _{Y} P\left(Y \mid X^{p}, X^{a}\right)$. Here $N, L$, and $T$ are the lengths of the passage, answer and question, respectively. We focus on the problem setting where we have a set of passage (and answers) and target questions pairs, to learn the mapping; existing QG approaches (Du et al., 2017; Song et al., 2018a; Zhao et al., 2018; Kim et al., 2018) make a similar assumption.</p>
<h3>2.2 DEEP Alignment Network</h3>
<p>Answer information is crucial for generating relevant and high quality questions from a passage. Unlike previous methods that neglect potential semantic relations between passage and answer words, we explicitly model the global interactions among them in the embedding space. To this end, we propose a novel Deep Alignment Network (DAN) component for effectively incorporating answer information into the passage with multiple granularity levels. Specifically, we perform attentionbased soft-alignment at the word-level, as well as at the contextual-level, so that multiple levels of alignments can help learn hierarchical representations.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The attention-based soft-alignment mechanism.</p>
<p>Let $\mathbf{X}^{p} \in \mathbb{R}^{F \times N}$ and $\widetilde{\mathbf{X}}^{p} \in \mathbb{R}^{\tilde{F}<em a="a">{p} \times N}$ denote two embeddings associated with passage text. Similarly, let $\mathbf{X}^{a} \in \mathbb{R}^{F \times L}$ and $\widetilde{\mathbf{X}}^{a} \in \mathbb{R}^{\tilde{F}</em>} \times L}$ denote two embeddings associated with answer text. Conceptually, as shown in Fig. 2, the soft-alignment mechanism consists of three steps: i) compute the attention score $\beta_{i, j}$ for each pair of passage word $x_{i}^{p}$ and answer word $x_{j}^{a}$ : ii) multiply the attention matrix $\boldsymbol{\beta}$ with the answer embeddings $\widetilde{\mathbf{X}}^{a}$ to obtain the aligned answer embeddings $\mathbf{H}^{p}$ for the passage; iii) concatenate the resulting aligned answer embeddings $\mathbf{H}^{p}$ with the passage embeddings $\widetilde{\mathbf{X}}^{p}$ to get the final passage embeddings $\widetilde{\mathbf{H}}^{p} \in \mathbb{R}^{\left(\tilde{F<em a="a">{p} \times \tilde{F}</em>$.}\right) \times N</p>
<p>Formally, we define our soft-alignment function as following:</p>
<p>$$
\widetilde{\mathbf{H}}^{p}=\operatorname{Align}\left(\mathbf{X}^{p}, \mathbf{X}^{a}, \widetilde{\mathbf{X}}^{p}, \widetilde{\mathbf{X}}^{a}\right)=\operatorname{CAT}\left(\widetilde{\mathbf{X}}^{p} ; \mathbf{H}^{p}\right)=\operatorname{CAT}\left(\widetilde{\mathbf{X}}^{p} ; \widetilde{\mathbf{X}}^{a} \boldsymbol{\beta}^{T}\right)
$$</p>
<p>where the matrix $\widetilde{\mathbf{H}}^{p}$ is the final passage embedding, the function CAT is a simple concatenation operation, and $\boldsymbol{\beta}$ is a $N \times L$ attention score matrix, computed by</p>
<p>$$
\boldsymbol{\beta} \propto \exp \left(\operatorname{ReLU}\left(\mathbf{W} \mathbf{X}^{p}\right)^{T} \operatorname{ReLU}\left(\mathbf{W} \mathbf{X}^{a}\right)\right)
$$</p>
<p>where $\mathbf{W} \in \mathbb{R}^{d \times F}$ is a trainable weight matrix, with $d$ being the hidden state size and ReLU is the rectified linear unit (Nair \&amp; Hinton, 2010). After introducing the general soft-alignment mechanism, we next introduce how we do soft-alignment at both word-level and contextual-level.</p>
<h1>2.2.1 Word-LEVEL Alignment</h1>
<p>In the word-level alignment stage, we first perform a soft-alignment between the passage and the answer based only on their pretrained GloVe embeddings and compute the final passage embeddings by $\widetilde{\mathbf{H}}^{p}=\operatorname{Align}\left(\mathbf{G}^{p}, \mathbf{G}^{a},\left[\mathbf{G}^{p} ; \mathbf{B}^{p} ; \mathbf{L}^{p}\right], \mathbf{G}^{a}\right)$, where $\mathbf{G}^{p}, \mathbf{B}^{p}$, and $\mathbf{L}^{p}$ are the corresponding GloVe embedding (Pennington et al., 2014), BERT embedding (Devlin et al., 2018), and linguistic feature (i.e., case, NER and POS) embedding of the passage text, respectively. Then a bidirectional LSTM (Hochreiter \&amp; Schmidhuber, 1997) is applied to the final passage embeddings $\widetilde{\mathbf{H}}^{p}=\left{\widetilde{\mathbf{h}}<em i="1">{i}^{p}\right}</em>$.
On the other hand, for the answer text $\mathbf{X}^{a}$, we simply concatenate its GloVe embedding $\mathbf{G}^{a}$ and its BERT embedding $\mathbf{B}^{a}$ to obtain its word embedding matrix $\mathbf{H}^{a} \in \mathbb{R}^{\tilde{d} \times L}$. Another BiLSTM is then applied to the concatenated answer embedding sequence to obtain the contextualized answer embeddings $\widetilde{\mathbf{H}}^{a} \in \mathbb{R}^{\tilde{F} \times L}$.}^{N}$ to obtain contextualized passage embeddings $\widetilde{\mathbf{H}}^{p} \in \mathbb{R}^{\tilde{F} \times N</p>
<h3>2.2.2 ContextuAl-LEVEL Alignment</h3>
<p>In the contextual-level alignment stage, we perform another soft-alignment based on the contextualized passage and answer embeddings. Similarly, we compute the aligned answer embedding, and concatenate it with the contextualized passage embedding to obtain the final passage embedding matrix $\operatorname{Align}\left(\left[\mathbf{G}^{p} ; \mathbf{B}^{p} ; \widetilde{\mathbf{H}}^{p}\right],\left[\mathbf{G}^{a} ; \mathbf{B}^{a} ; \widetilde{\mathbf{H}}^{a}\right], \widetilde{\mathbf{H}}^{p}, \widetilde{\mathbf{H}}^{a}\right)$. Finally, we apply another BiLSTM to the above concatenated embedding to get a $\tilde{F} \times N$ passage embedding matrix $\mathbf{X}$.</p>
<h3>2.3 BidiECTIONAL GRAPH-TO-SEQUENCE GENERATOR</h3>
<p>While RNNs are good at capturing local dependencies among consecutive words in text, GNNs have been shown to better utilize the rich hidden text structure information such as syntactic parsing (Xu et al., 2018b) or semantic parsing (Song et al., 2018b), and can model the global interactions (relations) among sequence words to further improve the representations. Therefore, unlike most of the existing methods that rely on RNNs to encode the input passage, we first construct a passage graph $\mathcal{G}$ from text where each passage word is treated as a graph node, and then employ a novel Graph2Seq model to encode the passage graph (and answer), and to decode the question sequence.</p>
<h3>2.3.1 PASSAGE GRAPH CONSTRUCTION</h3>
<p>Existing GNNs assume a graph structured input and directly consume it for computing the corresponding node embeddings. However, we need to construct a graph from the text. Although there</p>
<p>are early attempts on constructing a graph from a sentence (Xu et al., 2018b), there is no clear answer as to the best way of representing text as a graph. We explore both static and dynamic graph construction approaches, and systematically investigate the performance differences between these two methods in the experimental section.</p>
<p>Syntax-based static graph construction: We construct a directed and unweighted passage graph based on dependency parsing. For each sentence in a passage, we first get its dependency parse tree. We then connect neighboring dependency parse trees by connecting those nodes that are at a sentence boundary and next to each other in text.</p>
<p>Semantics-aware dynamic graph construction: We dynamically build a directed and weighted graph to model semantic relationships among passage words. We make the process of building such a graph depend on not only the passage, but also on the answer. The graph construction procedure consists of three steps: i) we compute a dense adjacency matrix $\mathbf{A}$ for the passage graph by applying self-attention to the word-level passage embeddings $\widetilde{\mathbf{H}}^{p}$, ii) a kNN-style graph sparsification strategy (Chen et al., 2019c) is adopted to obtain a sparse adjacency matrix $\widetilde{\mathbf{A}}$, where we only keep the $K$ nearest neighbors (including itself) as well as the associated attention scores (i.e., the remaining attentions scores are masked off) for each node; and iii) inspired by BiLSTM over LSTM, we also compute two normalized adjacency matrices $\mathbf{A}^{\neg}$ and $\mathbf{A}^{l-}$ according to their incoming and outgoing directions, by applying softmax operation on the resulting sparse adjacency matrix $\widetilde{\mathbf{A}}$ and its transpose, respectively.</p>
<p>$$
\mathbf{A}=\operatorname{ReLU}\left(\mathbf{U} \widetilde{\mathbf{H}}^{p}\right)^{T} \operatorname{ReLU}\left(\mathbf{U} \widetilde{\mathbf{H}}^{p}\right), \quad \widetilde{\mathbf{A}}=\operatorname{kNN}(\mathbf{A}), \quad \mathbf{A}^{\neg}, \mathbf{A}^{l-}=\operatorname{softmax}\left(\left{\widetilde{\mathbf{A}}, \widetilde{\mathbf{A}}^{T}\right}\right)
$$</p>
<p>where $\mathbf{U}$ is a $d \times\left(\widetilde{F}<em a="a">{p}+\widetilde{F}</em>\right)$ trainable weight matrix. Note that the supervision signal is able to back-propagate through the graph sparsification operation as the $K$ nearest attention scores are kept.</p>
<h1>2.3.2 Bidirectional Gated Graph Neural Networks</h1>
<p>To effectively learn the graph embeddings from the constructed text graph, we propose a novel Bidirectional Gated Graph Neural Network (BiGGNN) which extends Gated Graph Sequence Neural Networks (Li et al., 2015) by learning node embeddings from both incoming and outgoing edges in an interleaved fashion when processing the directed passage graph. Similar idea has also been exploited in (Xu et al., 2018a), which extended another popular variant of GNNs - GraphSAGE (Hamilton et al., 2017). However, one of key difference between our BiGGNN and their bidirectional GraphSAGE is that we fuse the intermediate node embeddings from both incoming and outgoing directions in every iteration, whereas their model simply learns the node embeddings of each direction independently and concatenates them in the final step.</p>
<p>In BiGGNN, node embeddings are initialized to the passage embeddings $\mathbf{X}$ returned by DAN. The same set of network parameters are shared at every hop of computation. At each computation hop, for every node in the graph, we apply an aggregation function which takes as input a set of incoming (or outgoing) neighboring node vectors and outputs a backward (or forward) aggregation vector. For the syntax-based static graph, we use a mean aggregator for simplicity although other operators such as max or attention (Veličković et al., 2017) could also be employed,</p>
<p>$$
\begin{aligned}
&amp; \mathbf{h}<em -i_v_="-i(v)">{\mathcal{N}</em>}}^{k}=\operatorname{MEAN}\left(\left{\mathbf{h<em u="u">{v}^{k-1}\right} \cup\left{\mathbf{h}</em>}^{k-1}, \forall u \in \mathcal{N<em _mathcal_N="\mathcal{N">{-i(v)}\right}\right) \
&amp; \mathbf{h}</em><em v="v">{l-(v)}}^{k}=\operatorname{MEAN}\left(\left{\mathbf{h}</em>}^{k-1}\right} \cup\left{\mathbf{h<em l-_v_="l-(v)">{u}^{k-1}, \forall u \in \mathcal{N}</em>\right}\right)
\end{aligned}
$$</p>
<p>For the semantics-aware dynamic graph we compute a weighted average for aggregation where the weights come from the normalized adjacency matrices $\mathbf{A}^{\neg}$ and $\mathbf{A}^{l-}$, defined as,</p>
<p>$$
\mathbf{h}<em -i_v_="-i(v)">{\mathcal{N}</em>}}^{k}=\sum_{\forall u \in \mathcal{N<em u="u" v_="v,">{-i(v)}} \mathbf{a}</em>}^{\neg} \mathbf{h<em _mathcal_N="\mathcal{N">{u}^{k-1}, \quad \mathbf{h}</em><em _forall="\forall" _in="\in" _mathcal_N="\mathcal{N" u="u">{l-(v)}}^{k}=\sum</em><em u="u" v_="v,">{l-(v)}} \mathbf{a}</em>
$$}^{l} \mathbf{h}_{u}^{k-1</p>
<p>While (Xu et al., 2018a) learn separate node embeddings for both directions independently, we opt to fuse information aggregated in two directions at each hop, which we find works better in general.</p>
<p>$$
\mathbf{h}<em _v_="(v)">{\mathcal{N}</em>}}^{k}=\operatorname{Fuse}\left(\mathbf{h<em -i_v_="-i(v)">{\mathcal{N}</em>}}^{k}, \mathbf{h<em l-_v_="l-(v)">{\mathcal{N}</em>\right)
$$}}^{k</p>
<p>We design the fusion function as a gated sum of two information sources,</p>
<p>$$
\operatorname{Fuse}(\mathbf{a}, \mathbf{b})=\mathbf{z} \odot \mathbf{a}+(1-\mathbf{z}) \odot \mathbf{b}, \quad \mathbf{z}=\sigma\left(\mathbf{W}<em z="z">{z}[\mathbf{a} ; \mathbf{b} ; \mathbf{a} \odot \mathbf{b} ; \mathbf{a}-\mathbf{b}]+\mathbf{b}</em>\right)
$$</p>
<p>where $\odot$ is the component-wise multiplication, $\sigma$ is a sigmoid function, and $\mathbf{z}$ is a gating vector.
Finally, a Gated Recurrent Unit (GRU) (Cho et al., 2014) is used to update the node embeddings by incorporating the aggregation information.</p>
<p>$$
\mathbf{h}<em v="v">{v}^{k}=\operatorname{GRU}\left(\mathbf{h}</em>}^{k-1}, \mathbf{h<em _v_="(v)">{N</em>\right)
$$}}^{k</p>
<p>After $n$ hops of GNN computation, where $n$ is a hyperparameter, we obtain the final state embedding $\mathbf{h}_{v}^{n}$ for node $v$. To compute the graph-level embedding, we first apply a linear projection to the node embeddings, and then apply max-pooling over all node embeddings to get a $d$-dim vector $\mathbf{h}^{d}$.</p>
<h1>2.3.3 RNN DECODER</h1>
<p>On the decoder side, we adopt the same model architecture as other state-of-the-art Seq2Seq models where an attention-based (Bahdanau et al., 2014; Luong et al., 2015) LSTM decoder with copy (Vinyals et al., 2015; Gu et al., 2016) and coverage mechanisms (Tu et al., 2016) is employed. The decoder takes the graph-level embedding $\mathbf{h}^{d}$ followed by two separate fully-connected layers as initial hidden states (i.e., $\mathbf{c}<em 0="0">{0}$ and $\mathbf{s}</em>\right}$ as the attention memory, and generates the output sequence one word at a time. The particular decoder used in this work closely follows (See et al., 2017). We refer the readers to Appendix A for more details.}$ ) and the node embeddings $\left{\mathbf{h}_{v}^{n}, \forall v \in \mathcal{G</p>
<h3>2.4 Hybrid Evaluator</h3>
<p>It has been observed that optimizing such cross-entropy based training objectives for sequence learning does not always produce the best results on discrete evaluation metrics (Ranzato et al., 2015; Wu et al., 2016; Paulus et al., 2017). Major limitations of this strategy include exposure bias and evaluation discrepancy between training and testing. To tackle these issues, some recent QG approaches (Song et al., 2017; Kumar et al., 2018b) directly optimize evaluation metrics using REINFORCE. We further use a mixed objective function with both syntactic and semantic constraints for guiding text generation. In particular, we present a hybrid evaluator with a mixed objective function that combines both cross-entropy loss and RL loss in order to ensure the generation of syntactically and semantically valid text.</p>
<p>For the RL part, we employ the self-critical sequence training (SCST) algorithm (Rennie et al., 2017) to directly optimize the evaluation metrics. SCST is an efficient REINFORCE algorithm that utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. In SCST, at each training iteration, the model generates two output sequences: the sampled output $Y^{s}$, produced by multinomial sampling, that is, each word $y_{t}^{s}$ is sampled according to the likelihood $P\left(y_{t} \mid X, y_{&lt;t}\right)$ predicted by the generator, and the baseline output $\hat{Y}$, obtained by greedy search, that is, by maximizing the output probability distribution at each decoding step. We define $r(Y)$ as the reward of an output sequence $Y$, computed by comparing it to corresponding ground-truth sequence $Y^{*}$ with some reward metrics. The loss function is defined as:</p>
<p>$$
\mathcal{L}<em t="t">{r l}=\left(r(\hat{Y})-r\left(Y^{s}\right)\right) \sum</em>\right)
$$} \log P\left(y_{t}^{s} \mid X, y_{&lt;t}^{s</p>
<p>As we can see, if the sampled output has a higher reward than the baseline one, we maximize its likelihood, and vice versa.</p>
<p>One of the key factors for RL is to pick the proper reward function. To take syntactic and semantic constraints into account, we consider the following metrics as our reward functions:</p>
<p>Evaluation metric as reward function: We use one of our evaluation metrics, BLEU-4, as our reward function $f_{\text {eval }}$, which lets us directly optimize the model towards the evaluation metrics.</p>
<p>Semantic metric as reward function: One drawback of some evaluation metrics like BLEU is that they do not measure meaning, but only reward systems that have exact n-gram matches in the reference system. To make our reward function more effective and robust, we additionally use word movers distance (WMD) as a semantic reward function $f_{\text {sem }}$. WMD is the state-of-the-art approach to measure the dissimilarity between two sentences based on word embeddings (Kusner et al., 2015). Following Gong et al. (2019), we take the negative of the WMD distance between a generated sequence and the ground-truth sequence and divide it by the sequence length as its semantic score.
We define the final reward function as $r(Y)=f_{\text {eval }}\left(Y, Y^{<em>}\right)+\alpha f_{\text {sem }}\left(Y, Y^{</em>}\right)$ where $\alpha$ is a scalar.</p>
<h1>2.5 Training and Testing</h1>
<p>We train our model in two stages. In the first state, we train the model using regular cross-entropy loss, defined as,</p>
<p>$$
\mathcal{L}<em t="t">{l m}=\sum</em>^{}-\log P\left(y_{t<em>} \mid X, y_{&lt;t}^{</em>}\right)+\lambda \operatorname{covloss}_{t}
$$</p>
<p>where $y_{t}^{*}$ is the word at the $t$-th position of the ground-truth output sequence and $\operatorname{covloss}<em i="i">{t}$ is the coverage loss defined as $\sum</em>$ being the $i$-th element of the attention vector over the input sequence at time step $t$. Scheduled teacher forcing (Bengio et al., 2015) is adopted to alleviate the exposure bias problem. In the second stage, we fine-tune the model by optimizing a mixed objective function combining both cross-entropy loss and RL loss, defined as,} \min \left(a_{i}^{t}, c_{i}^{t}\right)$, with $a_{i}^{t</p>
<p>$$
\mathcal{L}=\gamma \mathcal{L}<em l="l" m="m">{r l}+(1-\gamma) \mathcal{L}</em>
$$</p>
<p>where $\gamma$ is a scaling factor controling the trade-off between cross-entropy loss and RL loss. During the testing phase, we use beam search to generate final predictions.</p>
<h2>3 EXPERIMENTS</h2>
<p>We evaluate our proposed model against state-of-the-art methods on the SQuAD dataset (Rajpurkar et al., 2016). Our full models have two variants G2S ${ }<em d="d" n="n" y="y">{s t a}+$ BERT + RL and G2S $</em>+$ BERT + RL which adopts static graph construction or dynamic graph construction, respectively. For model settings and sensitivity analysis, please refer to Appendix B and C. The implementation of our model is publicly available at https://github.com/hugochan/RL-based-Graph2Seq-for-NQG.</p>
<h3>3.1 BASELINE METHODS</h3>
<p>We compare against the following baselines in our experiments: i) Transformer (Vaswani et al., 2017), ii) SeqCopyNet (Zhou et al., 2018), iii) NQG++ (Zhou et al., 2017), iv) MPQG+R (Song et al., 2017), v) AFPQA (Sun et al., 2018), vi) s2sa-at-mp-gsa (Zhao et al., 2018), vii) ASs2s (Kim et al., 2018), and viii) CGC-QG (Liu et al., 2019). Detailed descriptions of the baselines are provided in Appendix D. Experiments on baselines followed by * are conducted using released code. Results of other baselines are taken from the corresponding papers, with unreported metrics marked as - .</p>
<h3>3.2 DATA AND METRICS</h3>
<p>SQuAD contains more than 100 K questions posed by crowd workers on 536 Wikipedia articles. Since the test set of the original SQuAD is not publicly available, the accessible parts $(\approx 90 \%)$ are used as the entire dataset in our experiments. For fair comparison with previous methods, we evaluated our model on both data split-1 (Song et al., 2018a) ${ }^{1}$ that contains 75,500/17,934/11,805 (train/development/test) examples and data split-2 (Zhou et al., 2017) ${ }^{2}$ that contains $86,635 / 8,965 / 8,964$ examples.</p>
<p>Following previous works, we use BLEU-4 (Papineni et al., 2002), METEOR (Banerjee \&amp; Lavie, 2005), ROUGE-L (Lin, 2004) and Q-BLEU1 (Nema \&amp; Khapra, 2018) as our evaluation metrics. Initially, BLEU-4 and METEOR were designed for evaluating machine translation systems and ROUGE-L was designed for evaluating text summarization systems. Recently, Q-BLEU1 was designed for better evaluating question generation systems, which was shown to correlate significantly better with human judgments compared to existing metrics.
Besides automatic evaluation, we also conduct a human evaluation study on split-2. We ask human evaluators to rate generated questions from a set of anonymized competing systems based on whether they are syntactically correct, semantically correct and relevant to the passage. The rating scale is from 1 to 5 , on each of the three categories. Evaluation scores from all evaluators are collected and averaged as final scores. Further details on human evaluation can be found in Appendix E.</p>
<p>Table 1: Automatic evaluation results on the SQuAD test set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Split-1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Split-2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">BLEU-4</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">Q-BLEU1</td>
<td style="text-align: center;">BLEU-4</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">Q-BLEU1</td>
</tr>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">8.98</td>
<td style="text-align: center;">26.01</td>
<td style="text-align: center;">16.70</td>
<td style="text-align: center;">3.09</td>
<td style="text-align: center;">9.68</td>
<td style="text-align: center;">28.86</td>
<td style="text-align: center;">20.10</td>
</tr>
<tr>
<td style="text-align: left;">SeqCopyNet</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">13.02</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">44.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">NQG++</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">13.29</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MPQG+R*</td>
<td style="text-align: center;">14.39</td>
<td style="text-align: center;">18.99</td>
<td style="text-align: center;">42.46</td>
<td style="text-align: center;">52.00</td>
<td style="text-align: center;">14.71</td>
<td style="text-align: center;">18.93</td>
<td style="text-align: center;">42.60</td>
<td style="text-align: center;">50.30</td>
</tr>
<tr>
<td style="text-align: left;">AFPQA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.64</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">s2sa-at-mp-gsa</td>
<td style="text-align: center;">15.32</td>
<td style="text-align: center;">19.29</td>
<td style="text-align: center;">43.91</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.82</td>
<td style="text-align: center;">19.67</td>
<td style="text-align: center;">44.24</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ASs2s</td>
<td style="text-align: center;">16.20</td>
<td style="text-align: center;">19.92</td>
<td style="text-align: center;">43.96</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.17</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">CGC-QG</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.55</td>
<td style="text-align: center;">21.24</td>
<td style="text-align: center;">44.53</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{d y n}+$ BERT+RL</td>
<td style="text-align: center;">17.55</td>
<td style="text-align: center;">21.42</td>
<td style="text-align: center;">45.59</td>
<td style="text-align: center;">55.40</td>
<td style="text-align: center;">18.06</td>
<td style="text-align: center;">21.53</td>
<td style="text-align: center;">45.91</td>
<td style="text-align: center;">55.00</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{s t a}+$ BERT+RL</td>
<td style="text-align: center;">$\mathbf{1 7 . 9 4}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 7 6}$</td>
<td style="text-align: center;">$\mathbf{4 6 . 0 2}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 6 0}$</td>
<td style="text-align: center;">$\mathbf{1 8 . 3 0}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 7 0}$</td>
<td style="text-align: center;">$\mathbf{4 5 . 9 8}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 2 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Human evaluation results ( $\pm$ standard deviation) on the SQuAD split-2 test set. The rating scale is from 1 to 5 (higher scores indicate better results).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Syntactically correct</th>
<th style="text-align: center;">Semantically correct</th>
<th style="text-align: center;">Relevant</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MPQG+R*</td>
<td style="text-align: center;">$4.34(0.15)$</td>
<td style="text-align: center;">$4.01(0.23)$</td>
<td style="text-align: center;">$3.21(0.31)$</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{s t a}+$ BERT+RL</td>
<td style="text-align: center;">$4.41(0.09)$</td>
<td style="text-align: center;">$4.31(0.12)$</td>
<td style="text-align: center;">$3.79(0.45)$</td>
</tr>
<tr>
<td style="text-align: left;">Ground-truth</td>
<td style="text-align: center;">$\mathbf{4 . 7 4 ( 0 . 1 4 )}$</td>
<td style="text-align: center;">$\mathbf{4 . 7 4 ( 0 . 1 9 )}$</td>
<td style="text-align: center;">$\mathbf{4 . 2 5 ( 0 . 3 8 )}$</td>
</tr>
</tbody>
</table>
<h1>3.3 Experimental Results and Human Evaluation</h1>
<p>Table 1 shows the automatic evaluation results comparing our proposed models against other state-of-the-art baseline methods. First of all, we can see that both of our full models G2S $<em d="d" n="n" y="y">{s t a}+$ BERT+RL and G2S $</em>+$ BERT+RL achieve the new state-of-the-art scores on both data splits and consistently outperform previous methods by a significant margin. This highlights that our RL-based Graph2Seq model, together with the deep alignment network, successfully addresses the three issues we highlighted in Sec. 1. Between these two variants, G2S $<em d="d" n="n" y="y">{s t a}+$ BERT+RL outperforms G2S $</em>+$ BERT+RL on all the metrics. Also, unlike the baseline methods, our model does not rely on any hand-crafted rules or ad-hoc strategies, and is fully end-to-end trainable.</p>
<p>As shown in Table 2, we conducted a human evaluation study to assess the quality of the questions generated by our model, the baseline method MPQG+R, and the ground-truth data in terms of syntax, semantics and relevance metrics. We can see that our best performing model achieves good results even compared to the ground-truth, and outperforms the strong baseline method MPQG+R. Our error analysis shows that main syntactic error occurs in repeated/unknown words in generated questions. Further, the slightly lower quality on semantics also impacts the relevance.</p>
<h3>3.4 Ablation Study</h3>
<p>Table 3: Ablation study on the SQuAD split-2 test set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">BLEU-4</th>
<th style="text-align: left;">Methods</th>
<th style="text-align: left;">BLEU-4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">G2S $_{d y n}+$ BERT+RL</td>
<td style="text-align: center;">18.06</td>
<td style="text-align: left;">G2S $_{d y n}$</td>
<td style="text-align: left;">16.81</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{s t a}+$ BERT+RL</td>
<td style="text-align: center;">18.30</td>
<td style="text-align: left;">G2S $_{s t a}$</td>
<td style="text-align: left;">16.96</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{s t a}+$ BERT-fixed+RL</td>
<td style="text-align: center;">18.20</td>
<td style="text-align: left;">G2S $_{d y n}$ w/o DAN</td>
<td style="text-align: left;">12.58</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{d y n}+$ BERT</td>
<td style="text-align: center;">17.56</td>
<td style="text-align: left;">G2S $_{s t a}$ w/o DAN</td>
<td style="text-align: left;">12.62</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{s t a}+$ BERT</td>
<td style="text-align: center;">18.02</td>
<td style="text-align: left;">G2S $_{s t a}$ w/o BiGGNN, w/ Seq2Seq</td>
<td style="text-align: left;">16.14</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{s t a}+$ BERT-fixed</td>
<td style="text-align: center;">17.86</td>
<td style="text-align: left;">G2S $_{s t a}$ w/o BiGGNN, w/ GCN</td>
<td style="text-align: left;">14.47</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{d y n}+$ RL</td>
<td style="text-align: center;">17.18</td>
<td style="text-align: left;">G2S $_{s t a}$ w/ GGNN-forward</td>
<td style="text-align: left;">16.53</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{s t a}+$ RL</td>
<td style="text-align: center;">17.49</td>
<td style="text-align: left;">G2S $_{s t a}$ w/ GGNN-backward</td>
<td style="text-align: left;">16.75</td>
</tr>
</tbody>
</table>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>As shown in Table 3, we perform an ablation study to systematically assess the impact of different model components (e.g., BERT, RL, DAN, and BiGGNN) for two proposed full model variants (static vs dynamic) on the SQuAD split-2 test set. It confirms our finding that syntax-based static graph construction (G2S ${ }<em d="d" n="n" y="y">{s t a}+$ BERT+RL) performs better than semantics-aware dynamic graph construction (G2S ${ }</em>+$ BERT+RL) in almost every setting. However, it may be too early to conclude which one is the method of choice for QG. On the one hand, an advantage of static graph construction is that useful domain knowledge can be hard-coded into the graph, which can greatly benefit the downstream task. However, it might suffer if there is a lack of prior knowledge for a specific domain knowledge. On the other hand, dynamic graph construction does not need any prior knowledge about the hidden structure of text, and only relies on the attention matrix to capture these structured information, which provides an easy way to achieve a decent performance. One interesting direction is to explore effective ways of combining both static and dynamic graphs.</p>
<p>By turning off the Deep Alignment Network (DAN), the BLEU-4 score of G2S ${ }<em d="d" n="n" y="y">{s t a}$ (similarly for G2S ${ }</em>}$ ) dramatically drops from $16.96 \%$ to $12.62 \%$, which indicates the importance of answer information for QG and shows the effectiveness of DAN. This can also be verified by comparing the performance between the DAN-enhanced Seq2Seq model (16.14 BLEU-4 score) and other carefully designed answer-aware Seq2Seq baselines such as NQG++ (13.29 BLEU-4 score), MPQG+R (14.71 BLEU-4 score) and AFPQA (15.82 BLEU-4 score). Further experiments demonstrate that both word-level (G2S ${ <em a="a" s="s" t="t">{s t a}$ w/ DAN-word only) and contextual-level (G2S ${ }</em>$ w/ DAN-contextual only) answer alignments in DAN are helpful.</p>
<p>We can see the advantages of Graph2Seq learning over Seq2Seq learning on this task by comparing the performance between G2S ${ }_{s t a}$ and Seq2Seq. Compared to Seq2Seq based QG methods that completely ignore hidden structure information in the passage, our Graph2Seq based method is aware of more hidden structure information such as semantic similarity between any pair of words that are not directly connected or syntactic relationships between two words captured in a dependency parsing tree. In our experiments, we also observe that doing both forward and backward message passing in the GNN encoder is beneficial. Surprisingly, using GCN (Kipf \&amp; Welling, 2016) as the graph encoder (and converting the input graph to an undirected graph) does not provide good performance. In addition, fine-tuning the model using REINFORCE can further improve the model performance in all settings (i.e., w/ and w/o BERT), which shows the benefits of directly optimizing the evaluation metrics. Besides, we find that the pretrained BERT embedding has a considerable impact on the performance and fine-tuning BERT embedding even further improves the performance, which demonstrates the power of large-scale pretrained language models.</p>
<h1>3.5 CASE STUDY</h1>
<p>Table 4: Generated questions on SQuAD split-2 test set. Target answers are underlined.
Passage: for the successful execution of a project, effective planning is essential .
Gold: what is essential for the successful execution of a project?
G2S $<em a="a" s="s" t="t">{s t a}$ w/o BiGGNN (Seq2Seq): what type of planning is essential for the project?
G2S $</em>$ w/o DAN.: what type of planning is essential for the successful execution of a project?
G2S $<em a="a" s="s" t="t">{s t a}$ : what is essential for the successful execution of a project?
G2S $</em>+$ BERT: what is essential for the successful execution of a project?
G2S $<em d="d" n="n" y="y">{s t a}+$ BERT+RL: what is essential for the successful execution of a project?
G2S $</em>+$ BERT+RL: what is essential for the successful execution of a project?
Passage: the church operates three hundred sixty schools and institutions overseas .
Gold: how many schools and institutions does the church operate overseas?
G2S $<em a="a" s="s" t="t">{s t a}$ w/o BiGGNN (Seq2Seq): how many schools does the church have?
G2S $</em>$ w/o DAN.: how many schools does the church have?
G2S $<em a="a" s="s" t="t">{s t a}$ : how many schools and institutions does the church have?
G2S $</em>+$ BERT: how many schools and institutions does the church have?
G2S $<em d="d" n="n" y="y">{s t a}+$ BERT+RL: how many schools and institutions does the church operate?
G2S $</em>+$ BERT+RL: how many schools does the church operate?
In Table 4, we further show a few examples that illustrate the quality of generated text given a passage under different ablated systems. As we can see, incorporating answer information helps the</p>
<p>model identify the answer type of the question to be generated, and thus makes the generated questions more relevant and specific. Also, we find our Graph2Seq model can generate more complete and valid questions compared to the Seq2Seq baseline. We think it is because a Graph2Seq model is able to exploit the rich text structure information better than a Seq2Seq model. Lastly, it shows that fine-tuning the model using REINFORCE can improve the quality of the generated questions.</p>
<h1>4 Related Work</h1>
<h3>4.1 Natural Question Generation</h3>
<p>Early works (Mostow \&amp; Chen, 2009; Heilman \&amp; Smith, 2010) for QG focused on rule-based approaches that rely on heuristic rules or hand-crafted templates, with low generalizability and scalability. Recent attempts have focused on NN-based approaches that do not require manually-designed rules and are end-to-end trainable. Existing NN-based approaches (Du et al., 2017; Yao et al.; Zhou et al., 2018) rely on the Seq2Seq model with attention, copy or coverage mechanisms. In addition, various ways (Zhou et al., 2017; Song et al., 2017; Zhao et al., 2018) have been proposed to utilize the target answer for guiding the question generation. Some recent approaches (Song et al., 2017; Kumar et al., 2018b) aim at directly optimizing evaluation metrics using REINFORCE. Concurrent works have explored tackling the QG task with various semantics-enhanced rewards (Zhang \&amp; Bansal, 2019) or large-scale pretrained language models (Dong et al., 2019).
However, the existing approaches for QG suffer from several limitations; they (i) ignore the rich structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, and (iii) fail to fully exploit the answer information. To address these limitations, we propose a RL based Graph2Seq model augmented with a deep alignment network to effectively tackle the QG task. To the best of our knowledge, we are the first to introduce the Graph2Seq architecture to solve the question generation task.</p>
<h3>4.2 Graph Neural Networks</h3>
<p>Over the past few years, graph neural networks (GNNs) (Kipf \&amp; Welling, 2016; Gilmer et al., 2017; Hamilton et al., 2017) have attracted increasing attention. Due to more recent advances in graph representation learning, a number of works have extended the widely used Seq2Seq architectures (Sutskever et al., 2014; Cho et al., 2014) to Graph2Seq architectures for machine translation, semantic parsing, $\operatorname{AMR}(\mathrm{SQL})$-to-text, and online forums health stage prediction tasks (Bastings et al., 2017; Beck et al., 2018; Xu et al., 2018a;b;c; Song et al., 2018b; Gao et al., 2019). While the high-quality graph structure is crucial for the performance of GNN-based approaches, most existing works use syntax-based static graph structures when applied to textual data. Very recently, researchers have started exploring methods to automatically construct a graph of visual objects (Norcliffe-Brown et al., 2018) or words (Liu et al., 2018; Chen et al., 2019c;b) when applying GNNs to non-graph structured data. To the best of our knowledge, we are the first to investigate systematically the performance difference between syntactic-aware static graph construction and semantics-aware dynamic graph construction in the context of question generation.</p>
<h2>5 CONCLUSION</h2>
<p>We proposed a novel RL based Graph2Seq model for QG, where the answer information is utilized by an effective Deep Alignment Network and a novel bidirectional GNN is proposed to process the directed passage graph. On the SQuAD dataset, our method outperforms existing methods by a significant margin and achieves the new state-of-the-art results. Future directions include investigating more effective ways of automatically learning graph structures from text and exploiting Graph2Seq models for question generation from structured data like knowledge graphs or tables.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work is supported by IBM Research AI through the IBM AI Horizons Network. We thank the human evaluators who evaluated our system. We also thank the anonymous reviewers for their constructive feedback.</p>
<h1>REFERENCES</h1>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 65-72, 2005.</p>
<p>Junwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuanhua Lv, Ming Zhou, and Tiejun Zhao. Table-to-text: Describing table region with natural language. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p>
<p>Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima'an. Graph convolutional encoders for syntax-aware neural machine translation. arXiv preprint arXiv:1704.04675, 2017.</p>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. Graph-to-sequence learning using gated graph neural networks. arXiv preprint arXiv:1806.09835, 2018.</p>
<p>Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 1171-1179, 2015.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer opendomain questions. arXiv preprint arXiv:1704.00051, 2017.</p>
<p>Yu Chen, Lingfei Wu, and Mohammed J Zaki. Bidirectional attentive memory networks for question answering over knowledge bases. arXiv preprint arXiv:1903.02188, 2019a.</p>
<p>Yu Chen, Lingfei Wu, and Mohammed J Zaki. Deep iterative and adaptive learning for graph neural networks. arXiv preprint arXiv:1912.07832, 2019b.</p>
<p>Yu Chen, Lingfei Wu, and Mohammed J Zaki. Graphflow: Exploiting conversation flow with graph neural networks for conversational machine comprehension. arXiv preprint arXiv:1908.00059, 2019c.</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP, pp. 1724-1734, 2014.</p>
<p>Guy Danon and Mark Last. A syntactic approach to domain-specific automatic question generation. arXiv preprint arXiv:1712.09827, 2017.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems, pp. 13042-13054, 2019.</p>
<p>Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading comprehension. arXiv preprint arXiv:1705.00106, 2017.</p>
<p>Hady Elsahar, Christophe Gravier, and Frederique Laforest. Zero-shot question generation from knowledge graphs for unseen predicates and entity types. arXiv preprint arXiv:1802.06842, 2018.</p>
<p>Zhihao Fan, Zhongyu Wei, Siyuan Wang, Yang Liu, and Xuanjing Huang. A reinforcement learning framework for natural question generation using bi-discriminators. In Proceedings of the 27th International Conference on Computational Linguistics, pp. 1763-1774, 2018.</p>
<p>Yuyang Gao, Lingfei Wu, Houman Homayoun, and Liang Zhao. Dyngraph2seq: Dynamic-graph-tosequence interpretable learning for health stage prediction in online health forums. arXiv preprint arXiv:1908.08497, 2019.</p>
<p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.</p>
<p>Hongyu Gong, Suma Bhat, Lingfei Wu, Jinjun Xiong, and Wen-mei Hwu. Reinforcement learning based text style transfer without parallel training corpus. arXiv preprint arXiv:1903.10671, 2019.</p>
<p>Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. Incorporating copying mechanism in sequence-to-sequence learning. arXiv preprint arXiv:1603.06393, 2016.</p>
<p>Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.</p>
<p>Michael Heilman. Automatic factual question generation from text. 2011.
Michael Heilman and Noah A Smith. Good question! statistical ranking for question generation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 609-617. Association for Computational Linguistics, 2010.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$.</p>
<p>Yanghoon Kim, Hwanhee Lee, Joongbo Shin, and Kyomin Jung. Improving neural question generation using answer separation. arXiv preprint arXiv:1809.02393, 2018.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pp. 2575-2583, 2015.</p>
<p>Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.</p>
<p>Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT: Opensource toolkit for neural machine translation. In Proceedings of ACL 2017, System Demonstrations, pp. 67-72, Vancouver, Canada, July 2017. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/P17-4012.</p>
<p>Vishwajeet Kumar, Kireeti Boorla, Yogesh Meena, Ganesh Ramakrishnan, and Yuan-Fang Li. Automating reading comprehension by generating question and answer pairs. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 335-348. Springer, 2018a.</p>
<p>Vishwajeet Kumar, Ganesh Ramakrishnan, and Yuan-Fang Li. A framework for automatic question generation from text using deep reinforcement learning. arXiv preprint arXiv:1808.04961, 2018b.</p>
<p>Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document distances. In International Conference on Machine Learning, pp. 957-966, 2015.</p>
<p>Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and Ming Zhou. Visual question generation as dual task of visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6116-6124, 2018.</p>
<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.</p>
<p>Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out, 2004.</p>
<p>Bang Liu, Mingjun Zhao, Di Niu, Kunfeng Lai, Yancheng He, Haojie Wei, and Yu Xu. Learning to generate questions by learning what not to generate. arXiv preprint arXiv:1902.10418, 2019.</p>
<p>Pengfei Liu, Shuaichen Chang, Xuanjing Huang, Jian Tang, and Jackie Chi Kit Cheung. Contextualized non-local neural networks for sequence learning. arXiv preprint arXiv:1811.08600, 2018.</p>
<p>Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.</p>
<p>Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, and Lucy Vanderwende. Generating natural questions about an image. arXiv preprint arXiv:1603.06059, 2016.</p>
<p>Jack Mostow and Wei Chen. Generating instruction automatically for the reading strategy of selfquestioning. 2009.</p>
<p>Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814, 2010.</p>
<p>Preksha Nema and Mitesh M Khapra. Towards a better metric for evaluating question generation systems. arXiv preprint arXiv:1808.10192, 2018.</p>
<p>Will Norcliffe-Brown, Stathis Vafeias, and Sarah Parisot. Learning conditioned graph structures for interpretable visual question answering. In Advances in Neural Information Processing Systems, pp. 8344-8353, 2018.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.</p>
<p>Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532-1543, 2014.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.</p>
<p>Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.</p>
<p>Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7008-7024, 2017.</p>
<p>Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointergenerator networks. arXiv preprint arXiv:1704.04368, 2017.</p>
<p>Iulian Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, and Yoshua Bengio. Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus. arXiv preprint arXiv:1603.06807, 2016.</p>
<p>Linfeng Song, Zhiguo Wang, and Wael Hamza. A unified query-based generative model for question generation and question answering. arXiv preprint arXiv:1709.01058, 2017.</p>
<p>Linfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang, and Daniel Gildea. Leveraging context information for natural question generation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 569-574, 2018a.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. A graph-to-sequence model for amr-to-text generation. arXiv preprint arXiv:1805.02473, 2018b.</p>
<p>Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yanjun Ma, and Shi Wang. Answer-focused and position-aware neural question generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3930-3939, 2018.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104-3112, 2014.</p>
<p>Duyu Tang, Nan Duan, Tao Qin, Zhao Yan, and Ming Zhou. Question answering and question generation as dual tasks. arXiv preprint arXiv:1706.02027, 2017.</p>
<p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. Modeling coverage for neural machine translation. arXiv preprint arXiv:1601.04811, 2016.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural Information Processing Systems, pp. 2692-2700, 2015.</p>
<p>Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.</p>
<p>Kun Xu, Lingfei Wu, Zhiguo Wang, and Vadim Sheinin. Graph2seq: Graph to sequence learning with attention-based neural networks. arXiv preprint arXiv:1804.00823, 2018a.</p>
<p>Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, and Vadim Sheinin. Exploiting rich syntactic information for semantic parsing with graph-to-sequence model. arXiv preprint arXiv:1808.07624, 2018b.</p>
<p>Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, and Vadim Sheinin. Sql-to-text generation with graph-to-sequence model. arXiv preprint arXiv:1809.05255, 2018c.</p>
<p>Kaichun Yao, Libo Zhang, Tiejian Luo, Lili Tao, and Yanjun Wu. Teaching machines to ask questions.</p>
<p>Xingdi Yuan, Tong Wang, Caglar Gulcehre, Alessandro Sordoni, Philip Bachman, Sandeep Subramanian, Saizheng Zhang, and Adam Trischler. Machine comprehension by text-to-text neural question generation. arXiv preprint arXiv:1705.02012, 2017.</p>
<p>Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semisupervised question answering. arXiv preprint arXiv:1909.06356, 2019.</p>
<p>Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question generation with maxout pointer and gated self-attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3901-3910, 2018.</p>
<p>Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. Neural question generation from text: A preliminary study. In National CCF Conference on Natural Language Processing and Chinese Computing, pp. 662-671. Springer, 2017.</p>
<p>Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou. Sequential copying networks. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p>
<h1>A Details on the RNN Decoder</h1>
<p>At each decoding step $t$, an attention mechanism learns to attend to the most relevant words in the input sequence, and computes a context vector $\mathbf{h}<em t="t">{t}^{<em>}$ based on the current decoding state $\mathbf{s}<em _gen="{gen" _text="\text">{t}$, the current coverage vector $\mathbf{c}^{t}$ and the attention memory. In addition, the generation probability $p</em>^{}} \in[0,1]$ is calculated from the context vector $\mathbf{h}_{t</em>}$, the decoder state $\mathbf{s}</em>$, which is the sum of attention distributions over all previous decoder time steps. A coverage loss is also computed to penalize repeatedly attending to the same locations of the input sequence.}$ and the decoder input $y_{t-1}$. Next, $p_{\text {gen }}$ is used as a soft switch to choose between generating a word from the vocabulary, or copying a word from the input sequence. We dynamically maintain an extended vocabulary which is the union of the usual vocabulary and all words appearing in a batch of source examples (i.e., passages and answers). Finally, in order to encourage the decoder to utilize the diverse components of the input sequence, a coverage mechanism is applied. At each step, we maintain a coverage vector $\mathbf{c}^{t</p>
<h2>B Model Settings</h2>
<p>We keep and fix the 300-dim GloVe vectors for the most frequent 70,000 words in the training set. We compute the 1024-dim BERT embeddings on the fly for each word in text using a (trainable) weighted sum of all BERT layer outputs. The embedding sizes of case, POS and NER tags are set to 3, 12 and 8, respectively. We set the hidden state size of BiLSTM to 150 so that the concatenated state size for both directions is 300. The size of all other hidden layers is set to 300. We apply a variational dropout (Kingma et al., 2015) rate of 0.4 after word embedding layers and 0.3 after RNN layers. We set the neighborhood size to 10 for dynamic graph construction. The number of GNN hops is set to 3. During training, in each epoch, we set the initial teacher forcing probability to 0.75 and exponentially increase it to $0.75 * 0.9999^{t}$ where $i$ is the training step. We set $\alpha$ in the reward function to $0.1, \gamma$ in the mixed loss function to 0.99 , and the coverage loss ratio $\lambda$ to 0.4 . We use Adam (Kingma \&amp; Ba, 2014) as the optimizer, and the learning rate is set to 0.001 in the pretraining stage and 0.00001 in the fine-tuning stage. We reduce the learning rate by a factor of 0.5 if the validation BLEU-4 score stops improving for three epochs. We stop the training when no improvement is seen for 10 epochs. We clip the gradient at length 10 . The batch size is set to 60 and 50 on data split-1 and split-2, respectively. The beam search width is set to 5. All hyperparameters are tuned on the development set.</p>
<h2>C SENSITIVITY ANALYSIS OF HYPERPARAMETERS</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Effect of the number of GNN hops.</p>
<p>To study the effect of the number of GNN hops, we conduct experiments on the G2S ${ }_{\text {s6n }}$ model on the SQuAD split-2 data. Fig. 3 shows that our model is not very sensitive to the number of GNN hops and can achieve reasonably good results with various number of hops.</p>
<p>Table 5: Ablation study on the SQuAD split-2 test set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: left;">BLEU-4</th>
<th style="text-align: left;">Methods</th>
<th style="text-align: left;">BLEU-4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">G2S $_{\text {dyn }}+$ BERT+RL</td>
<td style="text-align: left;">18.06</td>
<td style="text-align: left;">G2S $_{\text {dyn }}$ w/o feat</td>
<td style="text-align: left;">16.51</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{\text {sta }}+$ BERT+RL</td>
<td style="text-align: left;">18.30</td>
<td style="text-align: left;">G2S $_{\text {sta }}$ w/o feat</td>
<td style="text-align: left;">16.65</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{\text {sta }}+$ BERT-fixed+RL</td>
<td style="text-align: left;">18.20</td>
<td style="text-align: left;">G2S $_{\text {dyn }}$ w/o DAN</td>
<td style="text-align: left;">12.58</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{\text {dyn }}+$ BERT</td>
<td style="text-align: left;">17.56</td>
<td style="text-align: left;">G2S $_{\text {sta }}$ w/o DAN</td>
<td style="text-align: left;">12.62</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{\text {sta }}+$ BERT</td>
<td style="text-align: left;">18.02</td>
<td style="text-align: left;">G2S $_{\text {sta }}$ w/ DAN-word only</td>
<td style="text-align: left;">15.92</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{\text {sta }}+$ BERT-fixed</td>
<td style="text-align: left;">17.86</td>
<td style="text-align: left;">G2S $_{\text {sta }}$ w/ DAN-contextual only</td>
<td style="text-align: left;">16.07</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{\text {dyn }}+$ RL</td>
<td style="text-align: left;">17.18</td>
<td style="text-align: left;">G2S $_{\text {sta }}$ w/ GGNN-forward</td>
<td style="text-align: left;">16.53</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{\text {sta }}+$ RL</td>
<td style="text-align: left;">17.49</td>
<td style="text-align: left;">G2S $_{\text {sta }}$ w/ GGNN-backward</td>
<td style="text-align: left;">16.75</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{\text {dyn }}$</td>
<td style="text-align: left;">16.81</td>
<td style="text-align: left;">G2S $_{\text {sta }}$ w/o BiGGNN, w/ Seq2Seq</td>
<td style="text-align: left;">16.14</td>
</tr>
<tr>
<td style="text-align: left;">G2S $_{\text {sta }}$</td>
<td style="text-align: left;">16.96</td>
<td style="text-align: left;">G2S $_{\text {sta }}$ w/o BiGGNN, w/ GCN</td>
<td style="text-align: left;">14.47</td>
</tr>
</tbody>
</table>
<h1>D Details on Baseline Methods</h1>
<p>Transformer (Vaswani et al., 2017) We included a Transformer-based Seq2Seq model augmented with attention and copy mechanisms. We used the open source implementation ${ }^{3}$ provided by the OpenNMT (Klein et al., 2017) library and trained the model from scratch. Surprisingly, this baseline performed very poorly on the benchmarks even though we conducted moderate hyperparameter search and trained the model for a large amount of epochs. We suspect this might be partially because this method is very sensitive to hyperparameters as reported by Klein et al. (2017) and probably data-hungry on this task. We conjecture that better performance might be expected by extensively searching the hyperparameters and using a pretrained transformer model.</p>
<p>SeqCopyNet (Zhou et al., 2018) proposed an extension to the copy mechanism which learns to copy not only single words but also sequences from the input sentence.</p>
<p>NQG++ (Zhou et al., 2017) proposed an attention-based Seq2Seq model equipped with a copy mechanism and a feature-rich encoder to encode answer position, POS and NER tag information.
MPQG+R (Song et al., 2017) proposed an RL-based Seq2Seq model with a multi-perspective matching encoder to incorporate answer information. Copy and coverage mechanisms are applied.
AFPQA (Sun et al., 2018) consists of an answer-focused component which generates an interrogative word matching the answer type, and a position-aware component which is aware of the position of the context words when generating a question by modeling the relative distance between the context words and the answer.
s2sa-at-mp-gsa (Zhao et al., 2018) proposed a model which contains a gated attention encoder and a maxout pointer decoder to tackle the challenges of processing long input sequences. For fair comparison, we report the results of the sentence-level version of their model to match with our settings.</p>
<p>ASs2s (Kim et al., 2018) proposed an answer-separated Seq2Seq model which treats the passage and the answer separately.</p>
<p>CGC-QG (Liu et al., 2019) proposed a multi-task learning framework to guide the model to learn the accurate boundaries between copying and generation.</p>
<h2>E DETAILS ON HUMAN EVALUATION</h2>
<p>We conducted a small-scale (i.e., 50 random examples per system) human evaluation on the split-2 data. We asked 5 human evaluators to give feedback on the quality of questions generated by a set of anonymized competing systems. In each example, given a triple containing a source passage, a target answer and an anonymised system output, they were asked to rate the quality of the output by answering the following three questions: i) is this generated question syntactically correct? ii) is this generated question semantically correct? and iii) is this generated question relevant to the passage? For each evaluation question, the rating scale is from 1 to 5 where a higher score means</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>better quality (i.e., 1: Poor, 2: Marginal, 3: Acceptable, 4: Good, 5: Excellent). Responses from all evaluators were collected and averaged.</p>
<h1>F MORE RESULTS ON AbLATION STUDY</h1>
<p>We perform the comprehensive ablation study to systematically assess the impact of different model components (e.g., BERT, RL, DAN, BiGGNN, FEAT, DAN-word, and DAN-contextual) for two proposed full model variants (static vs dynamic) on the SQuAD split-2 test set. Our experimental results confirmed that every component in our proposed model makes the contribution to the overall performance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://opennmt.net/OpenNMT-py/FAQ.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
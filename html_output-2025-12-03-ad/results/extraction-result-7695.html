<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7695 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7695</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7695</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-271843192</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.04646v2.pdf" target="_blank">Efficacy of Large Language Models for Systematic Reviews</a></p>
                <p><strong>Paper Abstract:</strong> This study investigates the effectiveness of Large Language Models (LLMs) in interpreting existing literature through a systematic review of the relationship between Environmental, Social, and Governance (ESG) factors and financial performance. The primary objective is to assess how LLMs can replicate a systematic review on a corpus of ESG-focused papers. We compiled and hand-coded a database of 88 relevant papers published from March 2020 to May 2024. Additionally, we used a set of 238 papers from a previous systematic review of ESG literature from January 2015 to February 2020. We evaluated two current state-of-the-art LLMs, Meta AI’s Llama 3 8B and OpenAI’s GPT-4o, on the accuracy of their interpretations relative to human-made classifications on both sets of papers. We then compared these results to a "Custom GPT" and a fine-tuned GPT-4o Mini model using the corpus of 238 papers as training data. The fine-tuned GPT-4o Mini model outperformed the base LLMs by 28.3% on average in overall accuracy on prompt 1. At the same time, the "Custom GPT" showed a 3.0% and 15.7% improvement on average in overall accuracy on prompts 2 and 3, respectively. Our findings reveal promising results for investors and agencies to leverage LLMs to summarize complex evidence related to ESG investing, thereby enabling quicker decision-making and a more efficient market.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7695.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7695.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompted LLM systematic review (A-C)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompted Large Language Model Systematic Review with A/B/C Context Levels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodology that evaluates LLMs' ability to replicate systematic review classification by issuing three incremental prompt-context levels (A: basic labels, B: added codebook context and examples, C: chain-of-thought steps plus requested reasoning and confidence) to classify papers across three review questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Efficacy of Large Language Models for Systematic Reviews</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Aaditya Shah; Shridhar Mehendale; Siddha Kanthi</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Prompted LLM classification (Prompts 1A-1C, 2A-2C, 3A-3C)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Issue structured prompts of increasing context to base LLMs (Llama 3 and GPT-4o) to classify each scholarly paper on three review questions (relationship between ESG and financial performance; financial metric type; sustainability implementation), collect the model label plus optional reasoning and confidence, and aggregate per-paper predictions for evaluation against human-coded labels.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>abstracts (Llama 3) and full-text PDFs (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>categorical classifications per review question, optional model reasoning text and self-reported confidence score</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>graded prompting (A/B/C) including chain-of-thought for C-level prompts; examples in B-level prompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta AI Llama 3; OpenAI GPT-4o (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Llama 3 (8B parameters); GPT-4o (unspecified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>New corpus: 88 ESG-related papers (Mar 2020–May 2024); Reference corpus: 238 papers from Atz et al. (Jan 2015–Feb 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Overall accuracy; sub-accuracy per class; Weighted Power Mean of F1 scores (p=2); confusion matrix; agreement across prompts</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Base Llama 3 outperformed base GPT-4o on most prompts (Llama overall accuracy range 62.5%–85.2%); GPT-4o's best improvement occurred with chain-of-thought (C) prompts; maximum base-model accuracy for relationship detection was 68.2%; chain-of-thought improved some metrics and shifted error patterns (e.g., many 'Positive' misclassified as 'Mixed').</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Token input limits forced Llama 3 to use only abstracts; models had difficulty with 'Mixed' and 'Other' classes; LLM confidence scores are model-derived and not calibrated; sensitivity to prompt design and training data; base models insufficient to replace human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficacy of Large Language Models for Systematic Reviews', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7695.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7695.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Custom GPT (ESG Returns Insights)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Custom GPT Chatbot 'ESG Returns Insights' using OpenAI GPTs feature with attached labeled corpus and chain-of-thought instructions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chat-based custom GPT built via OpenAI's 'GPTs' tool combining comprehensive instruction, chain-of-thought prompts for three review questions, and attachments of a labeled 238-paper corpus to provide contextual reference and improve classification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Efficacy of Large Language Models for Systematic Reviews</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Aaditya Shah; Shridhar Mehendale; Siddha Kanthi</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Custom GPT (GPTs platform) with reference corpus</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Construct a tailored ChatGPT 'GPT' by supplying a consolidated instruction file that embeds the chain-of-thought prompts for all three categories and attaching the old labeled corpus (238 papers) as reference material; for each new paper, launch a fresh chat session so the model uses the provided context and reference examples to output classifications and reasoning across all three questions in a single response.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>full-text PDFs (when provided) and attachments containing labeled example papers from the 238-paper corpus; per-paper input via new chat session</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>multi-question classification answers with model-provided reasoning and confidence; combined single-response answering all three prompts</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>chain-of-thought prompts embedded in system instruction; example-based/contextual augmentation via attached labeled corpus</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI ChatGPT (custom GPT wrapper over underlying ChatGPT model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified (uses OpenAI ChatGPT infrastructure)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Reference/labeled training sample: 238-paper corpus from Atz et al.; evaluated on 88-paper new corpus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Overall accuracy; sub-accuracy; mean F1 scores; prompt agreement</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Custom GPT outperformed base GPT-4o on eight of nine prompts and exceeded Llama 3 on several prompts; showed average improvements of ~3.0% (prompt 2) and ~15.7% (prompt 3) in overall accuracy versus base models; higher mean F1 in categories 2 and 3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Custom GPT still underperformed base Llama 3 in some category-1 prompts; model behavior depends on provided attachments and prompt engineering; potential for overfitting to reference examples; lack of transparency on how attachments are used internally by the ChatGPT GPTs feature.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficacy of Large Language Models for Systematic Reviews', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7695.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7695.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned GPT-4o Mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o Mini model fine-tuned on labeled ESG corpus for targeted classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller GPT-4o Mini was fine-tuned on the 238-paper labeled corpus (focused on the first review question) using specific hyperparameters to increase classification accuracy for determining the relationship between ESG and financial performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Efficacy of Large Language Models for Systematic Reviews</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Aaditya Shah; Shridhar Mehendale; Siddha Kanthi</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GPT-4o Mini fine-tuning (Category 1 specialized)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune the GPT-4o Mini model on the 238-paper labeled corpus using OpenAI API with batch size = 1, epochs = 3, learning rate multiplier = 1.8, training specifically to answer Prompt 1; deploy the fine-tuned model to classify new papers for Category 1.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>full labeled corpus (238 papers) for fine-tuning; new papers (88 corpus) for evaluation, provided as inputs to classification API</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>one-word categorical classification for Prompt 1 (Positive/Negative/Mixed)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>fine-tuning on a curated labeled dataset; system instruction using context prompt during fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI GPT-4o Mini (fine-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o Mini (smaller model; exact parameter count unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Training: 238-paper corpus from Atz et al.; Evaluation: 88-paper new corpus (Mar 2020–May 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Overall accuracy; improvement versus base GPT-4o Mini and other baseline models</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Fine-tuned GPT-4o Mini outperformed base LLMs by an average of 28.3% on Prompt 1 (as reported) and exceeded the standard GPT-4o Mini by 30.8% on category-1 tasks, achieving the best performance among tested models for that specific question.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Fine-tuned model returned only one-word classifications, preventing inspection of reasoning and error diagnosis; fine-tuning limited to Category 1 only; potential overfitting to the 238-paper corpus; sensitive to dataset composition and class imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficacy of Large Language Models for Systematic Reviews', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7695.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7695.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought prompting (systematic review)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting for Systematic Review Classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of chain-of-thought prompts that request stepwise reasoning and an explicit reasoning and confidence output to improve LLM transparency and accuracy when extracting structured judgments from scholarly papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Efficacy of Large Language Models for Systematic Reviews</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Aaditya Shah; Shridhar Mehendale; Siddha Kanthi</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Chain-of-Thought prompting (C-level prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Embed stepwise reasoning instructions into prompts so models follow explicit sub-steps to derive classification answers and return both reasoning text and a confidence estimate; used as the highest-context prompt level (C) and applied to both base and custom models.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>full-text (when available) or abstracts as provided to LLM</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>model reasoning trace plus categorical prediction and model-reported confidence</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>chain-of-thought prompting; explicit multi-step instructions</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to Llama 3 and GPT-4o (and Custom GPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Llama 3 (8B); GPT-4o (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Evaluated on the 88-paper new corpus and reference 238-paper corpus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Change in accuracy and weighted F1 when using C-level chain-of-thought prompts versus A/B; confusion matrices and prompt-agreement statistics</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Chain-of-thought prompts increased GPT-4o accuracy substantially for some prompts (e.g., Prompt 1C) and produced the highest weighted power mean F1 for GPT-4o; however, Llama 3's performance was more stable across prompts and sometimes decreased with chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Chain-of-thought improved some metrics but not uniformly across models or classes; self-reported confidence was not calibrated; stepwise outputs did not eliminate common misclassification modes (e.g., bias toward 'Mixed').</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficacy of Large Language Models for Systematic Reviews', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Extracting structured insights from financial news: An augmented llm driven approach <em>(Rating: 2)</em></li>
                <li>Towards reducing hallucination in extracting information from financial reports using large language models <em>(Rating: 2)</em></li>
                <li>Revolutionizing finance with llms: An overview of applications and insights <em>(Rating: 2)</em></li>
                <li>A survey on evaluation of large language models <em>(Rating: 1)</em></li>
                <li>FinBERT: Financial sentiment analysis with pre-trained language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7695",
    "paper_id": "paper-271843192",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "Prompted LLM systematic review (A-C)",
            "name_full": "Prompted Large Language Model Systematic Review with A/B/C Context Levels",
            "brief_description": "A methodology that evaluates LLMs' ability to replicate systematic review classification by issuing three incremental prompt-context levels (A: basic labels, B: added codebook context and examples, C: chain-of-thought steps plus requested reasoning and confidence) to classify papers across three review questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Efficacy of Large Language Models for Systematic Reviews",
            "authors": "Aaditya Shah; Shridhar Mehendale; Siddha Kanthi",
            "year": 2024,
            "method_name": "Prompted LLM classification (Prompts 1A-1C, 2A-2C, 3A-3C)",
            "method_description": "Issue structured prompts of increasing context to base LLMs (Llama 3 and GPT-4o) to classify each scholarly paper on three review questions (relationship between ESG and financial performance; financial metric type; sustainability implementation), collect the model label plus optional reasoning and confidence, and aggregate per-paper predictions for evaluation against human-coded labels.",
            "input_type": "abstracts (Llama 3) and full-text PDFs (GPT-4o)",
            "output_type": "categorical classifications per review question, optional model reasoning text and self-reported confidence score",
            "prompting_technique": "graded prompting (A/B/C) including chain-of-thought for C-level prompts; examples in B-level prompts",
            "model_name": "Meta AI Llama 3; OpenAI GPT-4o (base)",
            "model_size": "Llama 3 (8B parameters); GPT-4o (unspecified in paper)",
            "datasets_used": "New corpus: 88 ESG-related papers (Mar 2020–May 2024); Reference corpus: 238 papers from Atz et al. (Jan 2015–Feb 2020)",
            "evaluation_metric": "Overall accuracy; sub-accuracy per class; Weighted Power Mean of F1 scores (p=2); confusion matrix; agreement across prompts",
            "reported_results": "Base Llama 3 outperformed base GPT-4o on most prompts (Llama overall accuracy range 62.5%–85.2%); GPT-4o's best improvement occurred with chain-of-thought (C) prompts; maximum base-model accuracy for relationship detection was 68.2%; chain-of-thought improved some metrics and shifted error patterns (e.g., many 'Positive' misclassified as 'Mixed').",
            "limitations": "Token input limits forced Llama 3 to use only abstracts; models had difficulty with 'Mixed' and 'Other' classes; LLM confidence scores are model-derived and not calibrated; sensitivity to prompt design and training data; base models insufficient to replace human reviewers.",
            "counterpoint": true,
            "uuid": "e7695.0",
            "source_info": {
                "paper_title": "Efficacy of Large Language Models for Systematic Reviews",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Custom GPT (ESG Returns Insights)",
            "name_full": "Custom GPT Chatbot 'ESG Returns Insights' using OpenAI GPTs feature with attached labeled corpus and chain-of-thought instructions",
            "brief_description": "A chat-based custom GPT built via OpenAI's 'GPTs' tool combining comprehensive instruction, chain-of-thought prompts for three review questions, and attachments of a labeled 238-paper corpus to provide contextual reference and improve classification accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Efficacy of Large Language Models for Systematic Reviews",
            "authors": "Aaditya Shah; Shridhar Mehendale; Siddha Kanthi",
            "year": 2024,
            "method_name": "Custom GPT (GPTs platform) with reference corpus",
            "method_description": "Construct a tailored ChatGPT 'GPT' by supplying a consolidated instruction file that embeds the chain-of-thought prompts for all three categories and attaching the old labeled corpus (238 papers) as reference material; for each new paper, launch a fresh chat session so the model uses the provided context and reference examples to output classifications and reasoning across all three questions in a single response.",
            "input_type": "full-text PDFs (when provided) and attachments containing labeled example papers from the 238-paper corpus; per-paper input via new chat session",
            "output_type": "multi-question classification answers with model-provided reasoning and confidence; combined single-response answering all three prompts",
            "prompting_technique": "chain-of-thought prompts embedded in system instruction; example-based/contextual augmentation via attached labeled corpus",
            "model_name": "OpenAI ChatGPT (custom GPT wrapper over underlying ChatGPT model)",
            "model_size": "unspecified (uses OpenAI ChatGPT infrastructure)",
            "datasets_used": "Reference/labeled training sample: 238-paper corpus from Atz et al.; evaluated on 88-paper new corpus",
            "evaluation_metric": "Overall accuracy; sub-accuracy; mean F1 scores; prompt agreement",
            "reported_results": "Custom GPT outperformed base GPT-4o on eight of nine prompts and exceeded Llama 3 on several prompts; showed average improvements of ~3.0% (prompt 2) and ~15.7% (prompt 3) in overall accuracy versus base models; higher mean F1 in categories 2 and 3.",
            "limitations": "Custom GPT still underperformed base Llama 3 in some category-1 prompts; model behavior depends on provided attachments and prompt engineering; potential for overfitting to reference examples; lack of transparency on how attachments are used internally by the ChatGPT GPTs feature.",
            "counterpoint": true,
            "uuid": "e7695.1",
            "source_info": {
                "paper_title": "Efficacy of Large Language Models for Systematic Reviews",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Fine-tuned GPT-4o Mini",
            "name_full": "GPT-4o Mini model fine-tuned on labeled ESG corpus for targeted classification",
            "brief_description": "A smaller GPT-4o Mini was fine-tuned on the 238-paper labeled corpus (focused on the first review question) using specific hyperparameters to increase classification accuracy for determining the relationship between ESG and financial performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Efficacy of Large Language Models for Systematic Reviews",
            "authors": "Aaditya Shah; Shridhar Mehendale; Siddha Kanthi",
            "year": 2024,
            "method_name": "GPT-4o Mini fine-tuning (Category 1 specialized)",
            "method_description": "Fine-tune the GPT-4o Mini model on the 238-paper labeled corpus using OpenAI API with batch size = 1, epochs = 3, learning rate multiplier = 1.8, training specifically to answer Prompt 1; deploy the fine-tuned model to classify new papers for Category 1.",
            "input_type": "full labeled corpus (238 papers) for fine-tuning; new papers (88 corpus) for evaluation, provided as inputs to classification API",
            "output_type": "one-word categorical classification for Prompt 1 (Positive/Negative/Mixed)",
            "prompting_technique": "fine-tuning on a curated labeled dataset; system instruction using context prompt during fine-tuning",
            "model_name": "OpenAI GPT-4o Mini (fine-tuned variant)",
            "model_size": "GPT-4o Mini (smaller model; exact parameter count unspecified)",
            "datasets_used": "Training: 238-paper corpus from Atz et al.; Evaluation: 88-paper new corpus (Mar 2020–May 2024)",
            "evaluation_metric": "Overall accuracy; improvement versus base GPT-4o Mini and other baseline models",
            "reported_results": "Fine-tuned GPT-4o Mini outperformed base LLMs by an average of 28.3% on Prompt 1 (as reported) and exceeded the standard GPT-4o Mini by 30.8% on category-1 tasks, achieving the best performance among tested models for that specific question.",
            "limitations": "Fine-tuned model returned only one-word classifications, preventing inspection of reasoning and error diagnosis; fine-tuning limited to Category 1 only; potential overfitting to the 238-paper corpus; sensitive to dataset composition and class imbalance.",
            "counterpoint": true,
            "uuid": "e7695.2",
            "source_info": {
                "paper_title": "Efficacy of Large Language Models for Systematic Reviews",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Chain-of-Thought prompting (systematic review)",
            "name_full": "Chain-of-Thought Prompting for Systematic Review Classification",
            "brief_description": "Use of chain-of-thought prompts that request stepwise reasoning and an explicit reasoning and confidence output to improve LLM transparency and accuracy when extracting structured judgments from scholarly papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Efficacy of Large Language Models for Systematic Reviews",
            "authors": "Aaditya Shah; Shridhar Mehendale; Siddha Kanthi",
            "year": 2024,
            "method_name": "Chain-of-Thought prompting (C-level prompts)",
            "method_description": "Embed stepwise reasoning instructions into prompts so models follow explicit sub-steps to derive classification answers and return both reasoning text and a confidence estimate; used as the highest-context prompt level (C) and applied to both base and custom models.",
            "input_type": "full-text (when available) or abstracts as provided to LLM",
            "output_type": "model reasoning trace plus categorical prediction and model-reported confidence",
            "prompting_technique": "chain-of-thought prompting; explicit multi-step instructions",
            "model_name": "Applied to Llama 3 and GPT-4o (and Custom GPT)",
            "model_size": "Llama 3 (8B); GPT-4o (unspecified)",
            "datasets_used": "Evaluated on the 88-paper new corpus and reference 238-paper corpus",
            "evaluation_metric": "Change in accuracy and weighted F1 when using C-level chain-of-thought prompts versus A/B; confusion matrices and prompt-agreement statistics",
            "reported_results": "Chain-of-thought prompts increased GPT-4o accuracy substantially for some prompts (e.g., Prompt 1C) and produced the highest weighted power mean F1 for GPT-4o; however, Llama 3's performance was more stable across prompts and sometimes decreased with chain-of-thought.",
            "limitations": "Chain-of-thought improved some metrics but not uniformly across models or classes; self-reported confidence was not calibrated; stepwise outputs did not eliminate common misclassification modes (e.g., bias toward 'Mixed').",
            "counterpoint": true,
            "uuid": "e7695.3",
            "source_info": {
                "paper_title": "Efficacy of Large Language Models for Systematic Reviews",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Extracting structured insights from financial news: An augmented llm driven approach",
            "rating": 2,
            "sanitized_title": "extracting_structured_insights_from_financial_news_an_augmented_llm_driven_approach"
        },
        {
            "paper_title": "Towards reducing hallucination in extracting information from financial reports using large language models",
            "rating": 2,
            "sanitized_title": "towards_reducing_hallucination_in_extracting_information_from_financial_reports_using_large_language_models"
        },
        {
            "paper_title": "Revolutionizing finance with llms: An overview of applications and insights",
            "rating": 2,
            "sanitized_title": "revolutionizing_finance_with_llms_an_overview_of_applications_and_insights"
        },
        {
            "paper_title": "A survey on evaluation of large language models",
            "rating": 1,
            "sanitized_title": "a_survey_on_evaluation_of_large_language_models"
        },
        {
            "paper_title": "FinBERT: Financial sentiment analysis with pre-trained language models",
            "rating": 1,
            "sanitized_title": "finbert_financial_sentiment_analysis_with_pretrained_language_models"
        }
    ],
    "cost": 0.011271,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Efficacy of Large Language Models for Systematic Reviews</p>
<p>Aaditya Shah 
Illinois Mathematics and Science Academy
AuroraIllinois</p>
<p>Shridhar Mehendale 
Illinois Mathematics and Science Academy
AuroraIllinois</p>
<p>Siddha Kanthi 
Rick Reedy High School
FriscoTexas</p>
<p>Efficacy of Large Language Models for Systematic Reviews
07610A1B595E9525134127E4E7C7221FLarge Language ModelsTextual AnalysisESGSustainabilityCSRFinancial Performance
This study investigates the effectiveness of Large Language Models (LLMs) in interpreting existing literature through a systematic review of the relationship between Environmental, Social, and Governance (ESG) factors and financial performance.The primary objective is to assess how LLMs can replicate a systematic review on a corpus of ESG-focused papers.We compiled and hand-coded a database of 88 relevant papers published from March 2020 to May 2024.Additionally, we used a set of 238 papers from a previous systematic review of ESG literature from January 2015 to February 2020.We evaluated two current state-of-the-art LLMs, Meta AI's Llama 3 8B and OpenAI's GPT-4o, on the accuracy of their interpretations relative to human-made classifications on both sets of papers.We then compared these results to a "Custom GPT" and a fine-tuned GPT-4o Mini model using the corpus of 238 papers as training data.The fine-tuned GPT-4o Mini model outperformed the base LLMs by 28.3% on average in overall accuracy on prompt 1.At the same time, the "Custom GPT" showed a 3.0% and 15.7% improvement on average in overall accuracy on prompts 2 and 3, respectively.Our findings reveal promising results for investors and agencies to leverage LLMs to summarize complex evidence related to ESG investing, thereby enabling quicker decisionmaking and a more efficient market.</p>
<p>I. INTRODUCTION</p>
<p>A. Background and Motivation</p>
<p>Environmental, Social, and Governance (ESG) investing has continued to grow, with nearly a 60% increase in the use of ESG as investment criteria within the last two decades [1].Increasing awareness of sustainability issues has made ESG considerations a mainstream component for asset managers and institutional investors.The United Nations Principles for Responsible Investment now has over 609 asset owners holding USD $121.3 trillion combined assets under management in 2021 [2].Additionally, the percentage of corporations releasing ESG reports has increased from 35% in 2010 to 86% in 2020 [3].</p>
<p>However, there is no consensus on whether "ESG pays."Ľuboš Pástor, Robert F. Stambaugh, and Lucian A. Taylor [4], for example, find that U.S. green stocks outperform brown stocks.In contrast, Juddoo et al. [5] conclude impact investing strategies do not provide greater returns than traditional strategies.Many others have concluded that ESG does pay [6], [7], † Both Shah and Mehendale contributed equally to this work; order of authorship is random.</p>
<p>that ESG doesn't pay [8], [9], or that ESG has no impact on financial returns [10], [11].</p>
<p>In addition to this dispute in the industry, there are concerns regarding the volume, transparency, and usefulness of ESG reports released by corporations [12].These inconsistencies complicate accurate measurement of a firm's ESG performance and its impact on financial outcomes.Large Language Models (LLMs) offer a promising solution in this context.LLMs can rapidly interpret and analyze vast amounts of text and numerical information, enabling them to improve the efficiency and accuracy of ESG data analysis and serve as a tool to help companies familiarize themselves with ESG standards [13].Thus, LLMs offer the potential to reduce the complexity of the ESG investing landscape [14].</p>
<p>B. Problem Statement</p>
<p>With the rapidly changing nature of ESG investing and the large volume of available information, staying informed and rapidly summarizing new information is imperative.However, this requires extensive expertise and time, which not all industry players have access to.Therefore, it is essential to evaluate LLM tools in the context of ESG to determine if they can expedite the review process without compromising accuracy.</p>
<p>C. Objectives</p>
<p>The primary objective of this research is to evaluate the effectiveness of LLM tools (Llama 3 8B from Meta AI, GPT-4o from OpenAI, a "Custom GPT" chatbot from OpenAI, and a fine-tuned GPT-4o Mini Model from OpenAI) in conducting systematic reviews of the literature on the relationship between ESG performance and financial returns.By comparing the performance of LLMs to traditional manual systematic reviews, we evaluate the effectiveness of LLMs as a tool to help researchers and industry leaders efficiently interpret ESG information without compromising accuracy.</p>
<p>D. Outcome and Significance</p>
<p>We aim to contribute to two main strands of research: 1) the effectiveness of LLMs in summarizing and interpreting data and 2) the promise and pitfalls of systematic review.</p>
<p>Our development of a "Custom GPT" and fine-tuned GPT-4o Mini model as well as the evaluation of two base model LLMs (Llama 3 and GPT-4o) highlighted the potential for arXiv:2408.04646v2[cs.CL] 26 Oct 2024</p>
<p>LLMs to become a useful tool for interpreting ESG-related data.There were numerous occasions where all models excelled, such as with prompt 2 for papers classified as "accounting-based" or "both," with accuracies ranging from 87.1% to 100%.Additionally, the fine-tuned GPT-4o Mini model outperformed the base LLMs by 28.3% on average in overall accuracy on prompt 1.The "Custom GPT" also outperformed the base LLMs but by 3.0% and 15.7% on average in overall accuracy on prompts 2 and 3, respectively.Ultimately, this signifies that LLMs can serve as a helpful resource in interpreting ESG literature, especially after finetuning with relevant data.</p>
<p>Secondly, our study evaluated the use of LLMs in the context of systematic reviews to see if they can remove humans from the process.Systematic reviews are a key resource for advancing research, policy, and industry.However, the current best practices for systematic reviews are slow and labor-intensive, so much so that some reviews are no longer relevant by the time they are completed.Our study provides valuable insights into which models excel in specific contexts by assessing various LLMs in replicating systematic reviews.We highlight the potential for LLMs to transform systematic review best practices, given that the right prompt, model, and context have been selected.</p>
<p>II. PREVIOUS WORKS</p>
<p>In recent years, large language models (LLMs) have made significant progress in their Natural Language Processing (NLP) abilities, marking an important milestone in realworld applications of Artificial Intelligence (AI).These models can understand complex scenarios, analyze vast datasets, and generate relevant content, highlighting the immense potential within the sustainable financial industry [15], [16].</p>
<p>In the context of finance, Araci [17] developed FinBERT, a pre-trained language model specifically fine-tuned for financial sentiment analysis.Leveraging BERT's transformer architecture, FinBERT was trained on a large corpus of financial texts to grasp the nuances of financial language, enabling it to effectively classify sentiments in financial news and reports.FinBERT outperformed state-of-the-art models at the time, with an accuracy increase of 15 Gössi et al. [18] then fine-tuned the FinBERT model on a custom training set of 3,535 complex sentences from Federal Open Market Committee (FOMC) financial texts.This finetuned model saw significant accuracy gains over the previous FinBERT model, demonstrating the ability of LLMs to extract meaning from nuanced language and financial jargon.</p>
<p>Furthermore, Sarmah et al. [19] investigated methods such as retrieval-augmented generation and the use of metadata to minimize hallucinations when extracting information from financial reports with LLMs.Meanwhile, Dolphin et al. [20] employed chain-of-thought prompting to enhance the LLM's sentiment analysis performance and rigorously validated output to ensure dependable results, increasing reliable information retrieval from a broader array of news sources by over 400%.</p>
<p>In the specific context of this study-ESG analysis-AI models have been instrumental in redefining methods of scoring ESG performance for companies.Their effectiveness in interpreting financial data makes them powerful tools for analyzing vast amounts of unstructured data, such as company reports, financial statements, and news articles, to predict a company's ESG performance [21].Moreover, if AI models are trained on unbiased data, they can help eliminate human subjectivity and bias from ratings, reducing discrepancies across agencies [21].</p>
<p>Del Vitto et al. [22] evaluated white-box algorithms (Ridge and Lasso linear regression) and black-box algorithms (Random Forests and Artificial Neural Networks) on their ability to predict ESG scores using a training set of 13,052 firms stratified by industry sector and geographic region.Ridge and Lasso algorithms achieved root mean squared errors (RMSE) as low as 0.071, while artificial neural networks (ANNs) outperformed Ridge and Lasso within the Social pillar, achieving a lowest RMSE of 0.090 [22].</p>
<p>III. METHODS</p>
<p>In this study, we assessed the performance of two base large language models (LLMs): Meta AI's Llama 3 8B 1 and OpenAI's GPT-4 Omni (GPT-4o).Our evaluation was conducted on two sets of papers: 88 papers published between March 2020 and May 2024 and a previously compiled set of 238 papers from January 2015 to February 2020 by Atz et al. [23].Each LLM was tested using nine specific prompts in a 3x3 experimental design (Fig. 5).Due to API token input limitations, Llama 3 8B was only provided with the abstracts of the papers, while GPT-4o was given the full PDF.</p>
<p>We also developed a custom version of ChatGPT, named "Custom GPT," using the "GPTs" feature on OpenAI's Chat-GPT platform.This custom model was given comprehensive context plus chain-of-thought prompts for all three categories.Additionally, the 238-paper set with hand-coded labels was provided as reference data.The custom model, titled "ESG Returns Insights," was subsequently published on the ChatGPT platform.Additionally, we fine-tuned the GPT-4o Mini model on the 238-paper set and evaluated its performance on the new 88-paper set, benchmarking it against the standard GPT-4o Mini model.The fine-tuning process focused on Prompt 1, using the context prompt as the system instruction.</p>
<p>A. Data Collection</p>
<p>We queried ProQuest, Science Direct, JSTOR, Google Scholar, and Social Science Research Network (SSRN) to find literature on ESG and financial performance published between March 2020 and May 2024.To ensure unbiased and high-quality papers were sourced, we filtered the results to include only papers from journals rated 3, 4, or 4* and classified as either ACCOUNT, FINANCE, ECON, ETHICS-CSR-MAN, or STRAT from the Chartered Association of Business 2021 Academic Journal Guide [24].We compiled a corpus of 98 papers and hand-coded each in a typical systematic review format.However, we removed 10 papers, all from the Journal of Business Ethics, as they were deemed not salient enough [23].Of the remaining 88 papers, 14 manuscripts are not peer-reviewed.In addition to the new papers, we downloaded an older set from January 2015 to February 2020 comprising 238 papers from Atz et al. [23].</p>
<p>B. Data Classification on New Set</p>
<p>The new data set labels followed the same structure as Atz et al.'s [25] paper, as detailed in the online appendix [23].Three main questions were answered, and their responses were coded into the respective categories:</p>
<p>1) Does the study conclude a relationship between sustainability and financial performance?(positive, negative, or mixed/neutral) 2) How is financial success implemented?(market-based, accounting-based, both, or other) 3) How is sustainability implemented?(ESG, E, S, G, CSR, or other) Two researchers independently coded the papers, achieving a rater agreement of 90%, with a third researcher resolving the discrepancy.</p>
<p>C. Prompt Categories and Definitions</p>
<p>We split up three questions into three categories (1-3), which were structured incrementally with increasing context (A-C).The A-level prompts were basic and simply asked the questions and listed the possible classifications.The B-level prompts provided increased context from the Atz et al. [25] codebook behind the meaning of the classifications, along with one or two examples.The C-level prompts included all of the information from the B-level and A-level but were designed to follow a chain-of-thought approach.They asked the model to take specific steps in determining its response.In addition, the model was asked to return a reasoning and confidence score for its prediction.This confidence score was derived within the prediction of the LLM and, therefore, should not be interpreted as a technical measure of confidence.These prompts were used for Llama 3 and base GPT-4o.</p>
<p>D. Custom GPT Development and Model Fine-tuning</p>
<p>The "Custom GPT" model was developed using the Chat-GPT interface's "GPTs" tool, where we provided a set of instructions and relevant attachments for the default model context.The three chain-of-through prompts (1C, 2C, and 3C) were combined into one large set of instructions for the "Custom GPT."This way, following the input of a paper, all three prompts would be answered within a single response and without additional prompting.Each paper was tested in a new chat session to ensure that previous interactions did not influence the model's responses.In addition to a specialized prompt, the model was provided the old corpus 238 papers as a labeled training sample.Then, using the 238 papers in the old corpus of studies, we fine-tuned the smaller GPT-4o Mini model using the OpenAI API.We used a batch size of 1, 3 epochs, and a learning rate multiplier of 1.8.This model was only fine-tuned on Category 1 and instructed to return a prediction only for the answer to question 1.</p>
<p>E. Data Processing and Analysis</p>
<p>After obtaining the outputs from each LLM for the new and old datasets, we ensured that all results were standardized.We then compiled the results for further analysis.The primary focus was comparing the models' performance in correctly classifying the study findings and implementation methods across different prompts.We calculated the following metrics:</p>
<p>1) Overall Accuracy 2) Sub-accuracy for Each Prompt Class 3) F1 Score Given the disproportionate number of classes across prompts and the varying number of instances per class, we calculated the Weighted Power Mean (Equation 1) with p = 2 for the F1 scores for each predicted class.This approach provides a nuanced evaluation of our model's performance, addressing the challenges of imbalanced data and varying class frequencies.</p>
<p>Weighted Power Mean
= w i x 2 i w i 1 2(1)
IV. RESULTS</p>
<p>A. Old Data Set</p>
<p>We observed relatively no change in accuracy between the prompt variations for Llama 3 and noted a poor performance from base GPT-4o, nearly equivalent to guessing.</p>
<p>B. New Data Set</p>
<p>The results show a substantial improvement in the GPT-4o base for prompt 1C, while Llama's accuracy remained steady across all prompts.Both base LLMs showed poor performance on papers classified as "Mixed."GPT-4o performed its best   Base GPT-4o saw an increase in overall accuracy with the chain-of-thought prompt, whereas Llama 3 saw a decrease.Custom GPT-4o outperformed base GPT-4o across all three prompts, outperformed Llama 3 in 2C, and matched in 2B.All LLMs performed strongly on papers labeled as "Accounting-Based" or "Both" and poorly on papers labeled "Other."The "Other" column has been removed from the table as it contained values of 0 for all models except for CGPT, which had a value of 0.5.Only two papers were within the "Other" category, accounting for 2.2% of all papers.</p>
<p>There was no notable difference when switching between prompts A-C for either Llama 3 or base GPT-4o.Custom GPT outperformed both base LLMs across all prompts.Llama 3 classified papers in the "ESG", "E", and "G" categories well.</p>
<p>Base GPT-4o performed better with papers in the "G" and "CSR" categories.All three LLMs struggled with papers in the "S" and "Other" categories.There was no significant difference between the confidence scores when comparing both LLMs against each other and when comparing the scores for correct and incorrect classifications.</p>
<p>Result: Positive Confidence: 0.9</p>
<p>Reason: The study concludes a significant and positive relationship between higher carbon emissions and higher stock returns across various sectors and countries.GPT-4o overlooked the implication that higher carbon emissions equate to lower ESG, implying a negative correlation between ESG and financial performance.Misclassifications for similar reasons were prevalent across all three LLMs, occurring 9 times for base GPT-4o, 7 times for CustomGPT, and 5 times for Llama 3.Both LLMs had a low agreement between predictions from prompts A-C, with Llama being consistent across all three prompts 80.8% of the time, compared to 63.3% for GPT-4o.Fig. 4 presents the confusion matrix for GPT-4o's performance on the new data set with prompt 1A."Positive" samples were incorrectly classified as "Mixed" 58% of the time.Overall, the model's responses were skewed to the "Mixed" category, incorrectly classifying true "Positive" or "Negative" studies as "Mixed" frequently.</p>
<p>V. DISCUSSION</p>
<p>A. Base Llama 3</p>
<p>Even though Llama only provided the abstracts of the papers, it outperformed GPT-4o on eight out of nine of the prompts.Furthermore, within Category 1, Llama 3 achieved the highest mean F1 score on Prompt C. Llama's overall accuracy ranged from 62.5% to 85.2%, with the model struggling on category 1, a phenomenon witnessed across all models.Additionally, Llama had an average prompt agreement of 80.8%, making it reliant on needing a better prompt.</p>
<p>Despite many papers in our corpus of 88 mentioning both metrics within the text, they typically used only one for their conclusions and thus were not classified as "Both."Llama 3 excelled in category 2 for "accounting-based" research papers, achieving accuracies between 90.3% and 100% and weighted power mean of F1 scores between 0.811 and 0.852, suggesting the model's ability to efficiently scan text for key details while filtering out irrelevant information.</p>
<p>Furthermore, Llama 3 classified papers coded as "ESG", "E", and "G" within category 3 well with accuracies ranging from 90.0% to 100% with one outlier of 82.8% for prompt 3A under "ESG" classified papers.While many papers mention keywords such as "climate" and "environment," a broader analysis was necessary to comprehend the overarching theme and differentiate between "ESG" and the other individual categories.This highlights Llama 3's remarkable ability to understand overall themes within the literature.</p>
<p>Overall, Llama 3's performance within certain subcategories and consistent outperformance over base GPT-4o exemplify its viability as an LLM to assist during systematic review, but not entirely replace humans.</p>
<p>B. Base GPT-4o</p>
<p>Although base GPT-4o performed worse than Llama 3, it displayed highlights within certain prompts.For papers classified as "Negative" in category 1, the model displayed relatively strong accuracies ranging from 82.6% to 87.0%.Because papers classified as "Negative" often state this explicitly, the increase in accuracy may indicate that GPT-4o focuses more on searching for sentiment keywords throughout the paper.However, the model struggled with the "Positive" class and misclassified 58% of "Positive" papers as "Mixed."Furthermore, similar to Llama's increase in performance for category 2, GPT-4o performed better on the "Accounting-Based" paper, again highlighting the model's ability to identify key information while ignoring the noise.Finally, GPT-4o was strong at classifying papers labeled as "CSR" within category 3, which further supports the idea that GPT-4o can scan for keywords and sentiment well because often when CSR was used as a metric by the paper, it was explicitly stated as "CSR" multiple times.The accuracy differences between GPT-4o and Llama 3 suggest a potential difference in training data between both LLMs.Additionally, the weighted power mean of the F1 score was the highest when using the chain-of-thought prompt, signifying that prompt design and structure play a significant role in the model outcome.</p>
<p>C. Fine-Tuned Models</p>
<p>The "Custom GPT" outperformed the base GPT-4o model for eight out of the nine prompts, only performing worse on prompt 1C.Compared to Llama 3, the "Custom GPT" outperformed on prompts 2C, 3A, 3B, and 3C, matched on prompts 2A, and underperformed on all category 1 prompts and 2C.Additionally, the Custom GPT achieved a higher mean F1 score than all other models in categories 2 and 3. Due to the "Custom GPT's" poor performance, in prompt 1, we fine-tuned a GPT-4o Mini model to excel within that category specifically.This new model outperformed all of the other models in category 1 and beat the standard GPT-4o Mini model by 30.8%.This indicates that with fine-tuning and proper prompting/context, LLMs can effectively interpret ESG literature.</p>
<p>D. Evaluation of LLMs as a Whole</p>
<p>Ultimately, the application of LLMs within ESG shows potential.However, while LLMs can analyze vast amounts of data, the base model accuracy levels indicate that there is still room for improvement in terms of accuracy and deeper understanding.Specifically, the maximum accuracy achieved by base LLM models in determining the relationship between financial performance and ESG metrics was 68.2%.When broken down into specific categories, models reached an accuracy of 77.6% for financial metrics and 81.6% for sustainability metrics.Notably, the base Llama 3 model consistently outperformed GPT-4o across these tasks, yet even Llama 3's performance did not achieve an accuracy threshold that could replace human reviewers.This gap suggests that LLMs in their current form can support, but not fully automate, ESG analysis workflows.</p>
<p>To address these challenges, fine-tuning methods have been identified as a viable approach to enhance LLM accuracy and reliability.By training LLMs on ESG-specific datasets or tailoring them to a particular set of review standards, models can be refined to more accurately interpret and evaluate complex ESG data.With such fine-tuning, LLMs can potentially act as tools to expedite traditional systematic reviews, offering initial insights and analyses that streamline the review process while enabling human experts to focus on higherlevel evaluation and decision-making.This approach positions LLMs as complementary assets to human reviewers rather than replacements, bridging the gap between automation and expert analysis in ESG-related applications.</p>
<p>E. Future Works</p>
<p>Our research underlines the importance of choosing the appropriate model architecture, prompting techniques, and contextual information to maximize the effectiveness of LLMs in ESG analysis.Notably, chain-of-thought prompting emerged as an effective method, enhancing accuracy by guiding the model through logical steps and clarifying its "thought process."This approach provided greater transparency into the model's reasoning, making chain-of-thought prompting a valuable area for further exploration and refinement in ESG applications.</p>
<p>Future research should prioritize the fine-tuning of LLMs using larger, more diverse, and ESG-focused datasets to improve their capability to interpret nuanced and complex information.Additionally, efforts should be directed toward developing frameworks that seamlessly integrate LLMs into existing systematic review workflows.With better-equipped models, researchers can employ LLMs to assist in essential review tasks, such as data extraction, synthesis, and reporting.These enhanced models have the potential to streamline the systematic review process while improving consistency and efficiency, ultimately contributing to the ongoing relevance and accuracy of ESG-related reviews and ensuring that findings remain aligned with industry standards over time.</p>
<p>VI. ABBREVIATIONS AND ACRONYMS</p>
<p>2
2</p>
<p>Fig. 1 .
1
Fig. 1.Accuracy of interpretation of Llama 3 on prompts 1A-C and GPT-4o on prompt 1A</p>
<p>Fig. 2 .
2
Fig. 2. Accuracy of interpretation of Llama 3 and GPT-4o on prompts 1A-C.</p>
<p>Fig. 3 .
3
Fig. 3. Base GPT-4o Misclassification Example on Prompt 1C</p>
<p>Fig. 4 .
4
Fig. 4. Confusion Matrix for GPT4o (1A)</p>
<p>TABLE I ACCURACY
I
OF INTERPRETATION OF LLAMA 3 AND GPT-4O ON PROMPTS 1A-C OVERALL AND SUBSETS
ModelOverall Positive Negative MixedL3 (1A)0.6250.6950.5650.438L3 (1B)0.6700.7120.6960.500L3 (1C)0.6820.7290.6960.5004o (1A)0.4890.3730.8700.2504o (1B)0.4890.3900.8700.2504o (1C)0.6700.6440.8260.500CGPT0.5800.5760.6960.3754oM0.5910.5600.5330.6964oM FT0.7730.7800.7330.783in the negative category with a high accuracy of 87.0%. Thecustom GPT-4o only outperformed GPT-4o in prompts 1A and1B but not 1C and underperformed Llama 3 across all prompts.However, the custom GPT-4o Mini outperformed both baseLLMs across all prompts.</p>
<p>TABLE III ACCURACY
III
OF INTERPRETATION OF LLAMA 3 AND GPT-4O ON PROMPTS 3A-C OVERALL AND SUBSETS
ModelOverallESGESGCSRL3 (3A)0.7730.828 1.000 0.200 0.933 0.786L3 (3B)0.8070.931 1.000 0.200 0.900 0.857L3 (3C)0.8180.931 1.000 0.400 0.900 0.7864o (3A)0.6020.414 0.333 0.200 0.800 1.0004o (3B)0.7270.621 0.667 0.400 0.900 0.9294o (3C)0.6930.793 0.000 0.200 0.800 0.857CGPT0.8520.966 0.333 0.500 0.967 0.786Note:</p>
<p>TABLE IV CONFIDENCE
IV
SCORE GIVEN BY LLAMA 3 AND GPT-4O ON
CHAIN-OF-THOUGHT PROMPTSPromptCorrect IncorrectL3 (1C)0.8660.832L3 (2c)0.8580.833L3 (3C)0.8800.8724o (1C)0.9370.9244o (2C)0.9690.9244o (3C)0.9660.926</p>
<p>3</p>
<p>TABLE V A
V
-C SUB-PROMPT AGREEMENT ON PROMPTS 1-3
Model Overall1A-C2A-C3A-CL30.8080.807 0.750 0.8644o0.6330.568 0.750 0.580</p>
<p>L3: Meta AI's Llama 3 Model with 8B Parameters • 4o: OpenAI's GPT4o Model • 4oM: OpenAI's GPT4o Mini Model • 4oM F: OpenAI's GPT4o Mini Model Finetuned on Corpus of Papers from 2015 to 2022 [23] • CGPT: Custom GPT using ChatGPT GPT-builder feature • Mixed: Mixed/Neutral • Accounting: Accounting-based metric of financial performance (ROA, ROE, etc.) • Market: Market-based metric of financial performance (Tobin's Q, Jensen's Alpha, etc.) • ESG: A holistic ESG score provided by third-party data providers • E: Environmental factors exclusively • G: Governance factors exclusively • S: Social factors exclusively • CSR: Corporate Social Responsibility • Prompt Category + Type (ex.1C)</p>
<p>•</p>
<p>The old corpus served primarily as a reference rather than modifying the model's parameters.
The fine-tuned GPT-4o Mini model only returned one-word classifications, therefore we were not able to determine the reason for misclassification.
ACKNOWLEDGMENTWe thank Ulrich Atz for his guidance, review, and provision of OpenAI API computing credits.We also thank the seminar participants at the NYU Stern Center for Sustainable Business for their valuable feedback.
The static and dynamic connectedness of environmental, social, and governance investments: International evidence. Z Umar, D Kenourgios, S Papathanasiou, Economic Modelling. 932020</p>
<p>A review on esg investing: Investors' expectations, beliefs and perceptions. R Kräussl, T Oladiran, D Stefanova, Journal of Economic Surveys. 3822024</p>
<p>Sustainability meets substance: Evaluating esg reports in the context of 10-ks and firm performance. E Rouen, K Sachdeva, A Yoon, Mar 2024</p>
<p>Dissecting green returns. R F Ľuboš Pástor, L A Stambaugh, Taylor, Journal of Financial Economics. 14622022</p>
<p>An impact investment strategy. K Juddoo, I Malki, S Mathew, S Sivaprasad, Review of Quantitative Finance and Accounting. 61Jul 2023</p>
<p>Climate change news risk and corporate bond returns. T D Huynh, Y Xia, Journal of Financial and Quantitative Analysis. 5662021</p>
<p>An examination of whether and how prevention climate alters the influence of turnover on performance. D R Hekman, H P Vanwagoner, B P Owens, T R Mitchell, B C Holtom, T M Lee, J Dinger, Journal of Management. 4832022</p>
<p>Do esg funds make stakeholderfriendly investments?. A Raghunandan, S , Review of Accounting Studies. 27Sep 2022</p>
<p>Do investors care about carbon risk?. P Bolton, M Kacperczyk, Journal of Financial Economics. 14222021</p>
<p>Evidence of an inverted u-shaped relationship between stakeholder management performance variation and firm performance. A O Laplume, J S Harrison, Z Zhang, X Yu, K Walker, Business Ethics Quarterly. 3222022</p>
<p>Trust, social capital, and the bond market benefits of esg performance. H Amiraslani, K V Lins, H Servaes, A Tamayo, Review of Accounting Studies. 28Jun 2023</p>
<p>Practical ai cases for solving esg challenges. E Burnaev, E Mironov, A Shpilman, M Mironenko, D Katalevsky, Sustainability. 15172023</p>
<p>Navigating the challenges of environmental, social, and governance (esg) reporting: The path to broader sustainable development. S S Chopra, S S Senadheera, P D Dissanayake, P A Withana, R Chib, J H Rhee, Y S Ok, Sustainability. 1622024</p>
<p>Past, present, and future of sustainable finance: insights from big data analytics through machine learning of scholarly research. S Kumar, D Sharma, S Rao, W M Lim, S K Mangla, Annals of Operations Research. 2022</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, W Ye, Y Zhang, Y Chang, P S Yu, Q Yang, X Xie, 2023</p>
<p>Bloomberggpt: A large language model for finance. S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, 2023</p>
<p>Finbert: Financial sentiment analysis with pre-trained language models. D Araci, ArXiv. 1908.10063, 2019</p>
<p>Finbert-fomc: Fine-tuned finbert model with sentiment focus method for enhancing sentiment analysis of fomc minutes. S Gössi, Z Chen, W Kim, B Bermeitinger, S Handschuh, Proceedings of the Fourth ACM International Conference on AI in Finance, ICAIF '23. the Fourth ACM International Conference on AI in Finance, ICAIF '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Towards reducing hallucination in extracting information from financial reports using large language models. B Sarmah, D Mehta, S Pasquali, T Zhu, Proceedings of the Third International Conference on AI-ML Systems, AIMLSystems '23. the Third International Conference on AI-ML Systems, AIMLSystems '23New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Extracting structured insights from financial news: An augmented llm driven approach. R Dolphin, J Dursun, J Chow, J Blankenship, K Adams, Q Pike, 2024</p>
<p>Revolutionizing finance with llms: An overview of applications and insights. H Zhao, Z Liu, Z Wu, Y Li, T Yang, P Shu, S Xu, H Dai, L Zhao, G Mai, N Liu, T Liu, 2024</p>
<p>Esg ratings explainability through machine learning techniques. A Del, D Vitto, D Marazzina, Stocco, Annals of Operations Research. Jul 2023</p>
<p>Online appendix: Does sustainability generate better financial performance? review, metaanalysis, and propositions. U Atz, T Van Holt, Z Z Liu, C Bruno, Sep 2021</p>
<p>Academic journal guide 2021. tech. rep., Chartered Association of Business Schools. 2021. June 2021Chartered Association of Business SchoolsMethodology</p>
<p>Does sustainability generate better financial performance? review, meta-analysis, and propositions. Z Z L Ulrich Atz, Tracy Van Holt, C C Bruno, Journal of Sustainable Finance &amp; Investment. 1312023</p>            </div>
        </div>

    </div>
</body>
</html>
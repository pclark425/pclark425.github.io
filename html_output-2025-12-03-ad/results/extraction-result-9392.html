<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9392 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9392</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9392</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-265221214</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.09358v1.pdf" target="_blank">Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown remarkable achievements in natural language processing tasks, producing high-quality outputs. However, LLMs still exhibit limitations, including the generation of factually incorrect information. In safety-critical applications, it is important to assess the confidence of LLM-generated content to make informed decisions. Retrieval Augmented Language Models (RALMs) is relatively a new area of research in NLP. RALMs offer potential benefits for scientific NLP tasks, as retrieved documents, can serve as evidence to support model-generated content. This inclusion of evidence enhances trustworthiness, as users can verify and explore the retrieved documents to validate model outputs. Quantifying uncertainty in RALM generations further improves trustworthiness, with retrieved text and confidence scores contributing to a comprehensive and reliable model for scientific applications. However, there is limited to no research on UQ for RALMs, particularly in scientific contexts. This study aims to address this gap by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific tasks. This research investigates how uncertainty scores vary when scientific knowledge is incorporated as pretraining and retrieval data and explores the relationship between uncertainty scores and the accuracy of model-generated outputs. We observe that an existing RALM finetuned with scientific knowledge as the retrieval data tends to be more confident in generating predictions compared to the model pretrained only with scientific knowledge. We also found that RALMs are overconfident in their predictions, making inaccurate predictions more confidently than accurate ones. Scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue. We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9392",
    "paper_id": "paper-265221214",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0031349999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Pre-print Version EMPIRICAL EVALUATION OF UNCERTAINTY QUANTIFICATION IN RETRIEVAL-AUGMENTED LANGUAGE MODELS FOR SCIENCE
15 Nov 2023</p>
<p>Sridevi Wagle sridevi.wagle@pnnl.gov 
Pacific Northwest National Laboratory Richland
WAUSA</p>
<p>Sai Munikoti sai.munikoti@pnnl.gov 
Pacific Northwest National Laboratory Richland
WAUSA</p>
<p>Anurag Acharya anurag.acharya@pnnl.gov 
Pacific Northwest National Laboratory Richland
WAUSA</p>
<p>Sara Smith sara.smith@pnnl.gov 
Pacific Northwest National Laboratory Richland
WAUSA</p>
<p>Sameera Horawalavithana yasanka.horawalavithana@pnnl.gov 
Pacific Northwest National Laboratory Richland
WAUSA</p>
<p>Pre-print Version EMPIRICAL EVALUATION OF UNCERTAINTY QUANTIFICATION IN RETRIEVAL-AUGMENTED LANGUAGE MODELS FOR SCIENCE
15 Nov 2023428C434E2FC25F1427D5984D218C38E5arXiv:2311.09358v1[cs.CL]
Large language models (LLMs) have shown remarkable achievements in natural language processing tasks, producing high-quality outputs.However, LLMs still exhibit limitations, including the generation of factually incorrect information.In safety-critical applications, it is important to assess the confidence of LLM-generated content to make informed decisions.Retrieval Augmented Language Models (RALMs) is relatively a new area of research in Natural Language Processing (NLP).RALMs offer potential benefits for scientific NLP tasks, as retrieved documents, can serve as evidence to support model-generated content.This inclusion of evidence enhances trustworthiness, as users can verify and explore the retrieved documents to validate model outputs.Quantifying uncertainty in RALM generations further improves trustworthiness, with retrieved text and confidence scores contributing to a comprehensive and reliable model for scientific applications.However, there is limited to no research on UQ for RALMs, particularly in scientific contexts.This study aims to address this gap by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific tasks.This research investigates how uncertainty scores vary when scientific knowledge is incorporated as pretraining and retrieval data and explores the relationship between uncertainty scores and the accuracy of model-generated outputs.We observe that an existing RALM finetuned with scientific knowledge as the retrieval data tends to be more confident in generating predictions compared to the model pretrained only with scientific knowledge.We also found that RALMs are overconfident in their predictions, making inaccurate predictions more confidently than accurate ones.Scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue.We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.</p>
<p>INTRODUCTION</p>
<p>The continuous development of various Large Language Models (LLMs) has achieved a human level accuracy on various natural language processing tasks including machine translation, question answering and code generation.The rapid progress in this area has been driven by the availability of large datasets of text and code, as well as advances in computing power and machine learning algorithms.Some of the most notable LLMs include GPT-3 (Brown et al., 2020), LaMDA (Thoppilan et al., 2022), Codex (Finnie-Ansley et al., 2022) and Megatron-Turing NLG (Smith et al., 2022).These have been trained on datasets of hundreds of billions or even trillions of words, and can generate text that is often indistinguishable from human-written text.However, the models can still provide factually incorrect answers or what is commonly known as "hallucinations".Therefore, it is crucial to access the model confidence on its generation for informed decisions in safety critical applications.</p>
<p>There are standard UQ approaches in machine learning which are extended to LLMs.Notable among them are Temperature scaling/calibration (Xiao et al., 2022), Ensembles via Monte-carlo dropout (Gal &amp; Ghahramani, 2016), Last-Layer Stochastic Variational Inference (LL SVI ) (Lakshminarayanan et al., 2017).However, most of the existing approaches are not valid for generative LLMs.Therefore, Kuhn et al. (2023) proposed semantic entropy to accurately assess the uncertainty in generative models in a computational efficient manner.Furthermore, retrieval augmented language models (RALMs) are an advancement on top of regular LLMs that have become increasingly popular (Izacard et al., 2022;Munikoti et al., 2023).This is mainly due to their grounding capability and adaptability to work with new data sets and being small in size by decentralizing learning from parameters to external knowledge store.RALMs are being used in a variety of applications, including Google Search, Bard (Thoppilan et al., 2022), and GPT-3 Retriever (Brown et al., 2020;Karpukhin et al., 2020).These applications use RALMs to generate snippets of text, answer questions, and generate creative content.RALM consists of two major components, namely Retriever and Reader.Retriever retrieves documents (text chunks) from the corpus and Reader (Language model) uses the retrieved documents as extra context to support its generation.RALMs appear promising for scientific NLP applications because the retrieved scientific texts during generation can serve as supporting evidence for the model's outputs.Evidence instills trustworthiness in the model since experts can further examine the retrieved documents to validate the model's reasoning and conclusions.</p>
<p>The trustworthiness of RALM can further be improved by quantifying uncertainty in RALM generations.The combination of retrieved scientific text along with uncertainty estimates provides a comprehensive and reliable model suited for practical usage in the scientific domain.In this study, we experiment with different uncertainty measures to assess the reliability of RALM using three scientific benchmark datasets.In this regard, we choose ATLAS (Izacard et al., 2022) model which is a state-of-the-art RALM.Our objective is to empirically analyze how uncertainty scores vary when scientific knowledge is used either as the pretraining or retrieval data.We conduct a series of experiments with a scientific document corpus to train two RALMs, where one model uses the scientific document corpus in both Retriever and Reader while another model is only finetuned with the scientific document corpus provided as a retrieval corpus in the Retriever.</p>
<p>RELATED WORK</p>
<p>There has been significant amount of work in the field of uncertainty quantification for deep neural networks.Xiao et al. (2022) mentions different quantifiers for uncertainty.Temperature Scaling is one such technique that is used to quantify uncertainty in machine learning model predictions by providing a measure of how confident the model is in its predictions.Gal &amp; Ghahramani (2016) propose the Monte Carlo Dropout technique to estimate the uncertainty of machine learning model predictions.They do so by normalizing the results of multiple forward passes through the model with dropout turned on.It is simple to implement and can be used with a variety of different model architectures.However, it is expensive for very large scale models such as LLMs.Other uncertainty quantifiers include Ensemble and LL SVI (Last-Layer Stochastic Variational Inference).Ensemble learning works by combining the predictions of multiple independently trained models (Lakshminarayanan et al., 2017), while LL SVI uses variational inference to approximate the posterior predictive distribution of the model (Blundell et al., 2015).Both techniques can be effective for improving the reliability and robustness of machine learning models.However, despite this host of works in the field of UQ, there has been limited number of works when it comes to quantifying uncertainty of generative large language models (LLMs).More recently, however, uncertainty quantification for large language models(LLMs) has become a key area of research, as there has been increased interest in employing LLMs in real-world critical applications.Lin et al. (2023) propose a number of methods for quantifying the uncertainty of LLMs, using the entropy of the distribution of generated tokens, the diversity of the generated responses, and the average semantic dispersion of the generated responses.Their methods are simple to implement and can be used to improve the reliability of LLMs.Jiang et al. (2021) experiment with different techniques to improve the reliability of language models by making them aware of their own limitations by developing methods to calibrate the confidence scores of LMs so that they better reflect the actual probability of correctness.They develop several methods for calibrating LMs, which have shown to be effective on different datasets, but also have limitations since they may not work well for different types of LMs or tasks.Kadavath et al. (2022) show that self-evaluation can be approached on open-ended tasks by first asking models to propose answers, and then asking them to evaluate the probability that their answers are correct.They also investigate whether models can be trained to predict the probability that they know the answer to a question, without reference to any particular proposed answer.Models perform well at predicting this probability, and partially generalize across tasks.However, they struggle to calibrate their predictions on new tasks.Kuhn et al. (2023) use a relatively new method of Semantic Entropy for estimating uncertainty in natural language generation (NLG), which is a measure of the uncertainty of a generated text sequence that uses linguistic invariances.They evaluate this measure on two NLG datasets, TriviaQA (Joshi et al., 2017) and CoQA (Reddy et al., 2019), and find that it outperforms previous methods on both datasets.The authors' experiments show that semantic entropy is more accurate and informative than previous methods, and that it can be used to improve the performance of NLG models on downstream tasks.Despite these works, there is a lack of research in the area of uncertainty quantification for RALM both in general as well as science focused tasks.Therefore, in this work, we conduct comprehensive evaluation of UQ in RALMs for science tasks.</p>
<p>METHODOLOGY</p>
<p>In this section, we discuss RALMs and uncertainty measures used in our empirical study.</p>
<p>RETRIEVAL AUGMENTED LANGUAGE MODELS</p>
<p>Retrieval augmented language model (RALM) consists of two major components, Retriever and Reader.Retriever retrieves relevant documents from the text corpus and use them as context to support Reader (i.e., language model) to generate output (Figure 1).We choose RALM over other types of LLM for science tasks because the retrieved scientific texts can be used as evidences to instill trust in model predictions and enable grounding which is critical in scientific applications.There are various works in this space, such as REALM (Guu et al., 2020), DSP (Khattab et al., 2022), ATLAS (Izacard et al., 2022), and so on.We choose the ATLAS model to conduct all our experiments described in this paper.ATLAS is specifically designed to work for knowledge intensive tasks (e.g., fact checking question answering, etc.) in few-shot settings.ATLAS retrieves relevant documents based on the input query by using a dense retriever based on the Contriever (Izacard et al., 2021).The retrieved documents are processed along with the input query to a T5 model using the Fusion-in-Decoder architecture that generates the corresponding output (Izacard &amp; Grave, 2020).We prefer ATLAS for science tasks because it (i) allows end to end training of retriever and language model (enables us to pretrain model with scientific data), (ii) offers various techniques/configurations (e.g., query side finetuning) for efficient training and evaluation, and (iii) has vector database and few-shot ability enables seamless adaptation to several domains (Humanities, Social Sciences, STEM, etc) in science tasks.</p>
<p>UQ MEASURES FOR RETRIEVAL AUGMENTED LANGUAGE MODELS</p>
<p>Calibration is a widely used UQ approach for large scale deep neural network (DNN).This is due to its low computational complexity compared to other UQ approaches such as Ensemble and MC Dropout.In calibration, logits from the outermost layer of a DNN are normalized to provide confidence scores such that expected accuracy matches confidence scores (Guo et al., 2017).Calibration has been used for various discriminative tasks in LLM (Wang et al., 2022;Si et al., 2022), including text classification, entity recognition.However, it is not suitable for generative tasks in LLM due to variable output length.Therefore, several approximation UQ measures have been proposed recently in the literature.</p>
<p>For this work, we use two of those measures to perform the uncertainty quantification, Normalized Predictive Entropy and Semantic Entropy, which are described in detail in the following subsections.</p>
<p>NORMALIZED PREDICTIVE ENTROPY</p>
<p>The entropy is a statistical parameter which measures, in a certain sense, how much information is produced on the average for each letter of a text in the language.Let us assume the RALM generates M output sequence and sequence s consists of N tokens.The entropy of the sequence s is the sum of product of conditional probabilities of all tokens in s and their corresponding log values.
E(s) = N i=0 P (s i |s &lt;i , x)logP (s i |s &lt;i , x)(1)
where s i denotes i th token of sequence s and s &lt;i represents all tokens upto i th position.</p>
<p>To calculate the net predictive entropy of the model for a given query (prompt) x, we average the predictive entropy across all M generated sequences in the set S as shown below:
P E(x) = 1 M s∈S E(s) = 1 M s∈S N i=0 P (s i |s &lt;i , x)logP (s i |s &lt;i , x)(2)
Generated sequences can be of different length.Therefore, as for eqn.2, the longer sequences have lower joint likelihoods because of the conditional independence of the token probabilities.Hence, negative log-probability of a sequence grows linearly with the length of the sequence, so longer sequences tend to contribute more to predictive entropy (Kuhn et al., 2023).To counter this effect, the log-probabilities are normalized by the length of the sequence while calculating sequence entropy.</p>
<p>The normalized form of predictive entropy can be expressed as:
N P E(x) = 1 M s∈S 1 N s Ns i=0 P (s i |s &lt;i , x)logP (s i |s &lt;i , x),(3)
where N s denotes the length of the sequence s.</p>
<p>SEMANTIC ENTROPY</p>
<p>Predictive entropy as we discussed in the above subsection, approximates model uncertainty via token wise likelihoods (i.e.lexical confidence).However, in free form text generation, one always cares about the semantic meaning of the entire generated sequence.For instance, predictive entropy could be high if a model is uncertain about whether to generate "Japan's capital is Tokyo" or "Tokyo is Japan's capital".However model's uncertainty is actually low in this example since both sequence are semantically equiavalent (Kuhn et al., 2023).To address this shortcoming of predictive entropy, Kuhn et al. (2023) introduces new UQ metric, semantic entropy.To this end, the authors compute semantic likelihoods -probabilities attached to meanings of generated sequence rather than standard sequence-likelihoods.A clustering algorithm is implemented so that sequences with similar meaning group together.Then, semantic-likelihoods are computed for each meanings set (cluster) rather than each sequence.</p>
<p>Lets assume we obtain a finite number of meaning sets, C. We can use the sum of these sets to calculate the Semantic Entropy similar to the way we calculate predictive entropy for a sequence.
SE(x) = − c s∈c P (s|x) log s∈c P (s|x)(4)
Since one cannot have all possible meaning class-c in the limited number of generations, we need to take the estimation in eqn. 4 to get the semantic entropy as shown below:
SE(x) ≈ − 1 |C| |C| i=1 log p(C i |x) = − 1 |C| |C| i=1 log s∈ci P (s|x) ,(5)
where C i belongs to cluster (meaning set) i.</p>
<p>We prefer Normalized entropy and Semantic entropy over other UQ measures because normalized entropy performs token wise quantification and semantic entropy captures the semantic meaning, thus covering multiple UQ aspects for RALM.</p>
<p>EXPERIMENTAL SETUP</p>
<p>In this section, we describe the datasets (Section 4.1), and model variants (Section 4.2) used in our experiments.We also release our implementation and Human-AI reasoning dashboards (see Figure 4 in Appendix 6) used to conduct experiments publicly1 .</p>
<p>BENCHMARKS AND DATASETS</p>
<p>We leverage two types of scientific benchmarks for the uncertainty evaluation of RALMs.The first is SciRepEval, which offers 25 tasks spanning classification, regression, ranking, and search formats.Specifically, we focus on the classification tasks of Fields of Study (FoS) and MAG because they will test the ability of the models to recognize diverse scientific domains.The second benchmark we use is MMLU, which provides 57 multi-choice question-answering datasets retrieved from real-world examinations across diverse scientific fields.It is categorized into four major subdomains -humanities, social sciences, STEM, and other.This benchmark tests the ability of the models to understand the diverse science context for accurate generations.</p>
<p>We use the S2ORC (Lo et al., 2019) data which is a large corpus of curated 31.1MEnglish-language academic papers spanning many academic disciplines.We preprocess the S2ORC (Lo et al., 2019) dataset to create a corpus of 354M text passages.Each passage has a maximum of 512 tokens, or 100 words, that are concatenated with the corresponding title of the document the passage belongs to.</p>
<p>Our text corpus spans across 19 different scientific domains.</p>
<p>ATLAS MODEL VARIANTS</p>
<p>We analyze uncertainty results across two ATLAS model variants as shown in Figure 1.</p>
<p>ATLAS (CC + Wiki + S2ORC): ATLAS model combines autoregressive text generation with retrieval-based language model pretraining based on the encoder-decoder architecture and fine-tuned on open-domainQA (Izacard et al., 2022).ATLAS (CC + Wiki + S2ORC) uses the Fusion-in-decoder architecture to fuse the retrieved text chunks with the input queries during the pretraining.In this configuration, ATLAS model is pretrained with a Common Crawl (CC) and Wikipedia.The S2ORC dataset is a retrieval corpus in instruction fine-tuning and evaluation.All our experiments are based on ATLAS base model with 220M parameters.</p>
<p>ATLAS (S2ORC):</p>
<p>We train the ATLAS (S2ORC) model from scratch with the S2ORC scientific text datasets.For a fair comparison with ATLAS (CC + Wiki + S2ORC), we initialize the ATLAS (S2ORC) model with the T5-lm-adapt (Raffel et al., 2020) model and trained jointly with the retrieval model, Contriever (Izacard et al., 2021).We encode the scientific text passages (354M) with the Contriever model and construct a document index in the FLAT (Izacard et al., 2022) mode for faster retrieval.Additionally, we train the retriever with the query side fine-tuning approach which was originally proposed in the ATLAS.This approach is very efficient in model training since it keeps the document encoder frozen while training the parameters corresponding to the query encoder.</p>
<p>PERFORMANCE ANALYSIS</p>
<p>In this section, we analyze the performance of the model with respect to the model uncertainty and accuracy scores to address following research questions.</p>
<p>(RQ 1) How does the model confidence differ in scientific tasks when scientific document corpus is used as pretraining and retrieval data?(RQ 2) Does a model have higher confidence in accurate predictions as opposed to inaccurate predictions when provided a scientific document corpus?</p>
<p>To this end, we compute Normalized Predictive Entropy and Semantic Entropy for all the benchmark datasets mentioned in Section 4.1 and compared them across ATLAS (CC + Wiki + S2ORC) and ATLAS (S2ORC) models.We use the logits per sequence from the outermost layer of the decoder in the ATLAS model to compute the predictive entropy scores.In addition to the token-wise logits, we also use the corresponding text for computing semantic entropy.We employ beam search with beam width as 3 and number of output sequences as 5.</p>
<p>RALM finetuned with scientific knowledge as the retrieval data tends to be more confident than the model pretrained only with scientific knowledge We report the uncertainty scores of the two ATLAS model variants across three scientific benchmark datasets in Figure 3.The results suggest that the uncertainty scores are higher in the ATLAS (S2ORC) model compared to ATLAS (CC+Wiki+S2ORC) for all the benchmark datasets.We also observe that uncertainty scores differ across different scientific disciplines when measured with semantic entropy as shown in Tables 2 and 3 (see Appendix).For example, ATLAS (S2ORC) model is less confident in all subject areas except Chemistry, Engineering and Psychology related tasks.This demonstrates that the ATLAS (S2ORC) model is relatively less confident when only scientific knowledge is used as pretraining and retrieval data rather than the mixed of general and scientific training data used in ATLAS (CC+Wiki+S2ORC) model.On the other hand, general domain data from sources like CC and Wikipedia (news, scientific blogs, articles, and encyclopedia articles) has much more diversity and heterogeneity, which allows for more robust pretraining of the language model (Horawalavithana et al., 2022).</p>
<p>Another reason could be the complexity of the tasks as covered by the scientific benchmark datasets used in the experiments.FOS and MAG datasets cover more than 10 scientific disciplines and test the ability of the models to understand diverse scientific domains and disciplines2 .MMLU assess how well models can understand and use knowledge to answer questions from diverse scientific disciplines in Biology, Chemistry, Computer science, Mathematics, Physics, etc. Retrieving scientific knowledge from two or more different disciplines might help the Reader component to identify the connections between different ideas and concepts, and solve problems more effectively.Having a Reader trained with a mix of general and scientific datasets would allow it to better understand and generate text on a variety of topics.The general datasets would provide the Reader with a broad understanding of the world, while the scientific datasets would give it the knowledge to understand and generate text on more specific topics.This would make the Reader more versatile and useful for a variety of tasks.</p>
<p>RALMs make inaccurate predictions more confidently than making accurate predictions.In this section, we compare the uncertainty scores when the models make accurate and inaccurate  3d).The results suggest that the uncertainty scores are higher for accurate predictions in comparison to the inaccurate predictions.To state it differently, RALMs probability estimates are not in good agreement with the actual probability of the answer being correct.</p>
<p>One can argue this might be due to the distributional characteristics of the accurate and inaccurate predictions made by the models.However, the majority of predictions (&gt; 80%) in FOS and MAG tasks are accurate, while the majority of MMLU predictions (&gt; 60%) are inaccurate.Despite these distributional differences, both models are less confident while predicting the accurate answers and vice versa.We believe this is mainly due to LLMs/RALMs not calibrating well to the downstream tasks.</p>
<p>Our findings are consistent with the previously reported calibration issues (Jiang et al., 2021) in standard large language models (LLMs) such as T5, BART, and GPT-2.We demonstrate that these issues continue to exist in RALMs when applied to scientific tasks, and that scientific knowledge provided either as pretraining or retrieval data does not help the model to alleviate the calibration issues.Finding calibration methods for RALMs, especially when applied to scientific tasks, remains a work in progress.In the future, we also plan to conduct experiments on larger scale models to analyze the impact of scale on UQ measures.In this paper, we conduct a comprehensive evaluation of uncertainties in retrieval augmented language models for scientific tasks.We leverage ATLAS as our test models with SciRepEval and MMLU as benchmarks.We compare the general domain ATLAS model with that of pretrained from scratch on scientific literature (S2ORC).Experiments indicate that the model finetuned with scientific knowledge as the retrieval data tends to be more confident than the model pretrained only with scientific knowledge.We also observed calibration issues in RALMs that are more confident in inaccurate predictions compared to accurate ones in science tasks.</p>
<p>Figure 1 :
1
Figure 1: ATLAS Model Variants</p>
<p>Figure 2 :
2
Figure 2: Uncertainty scores across FOS, MAG and MMLU scientific benchmarks</p>
<p>Figure 3: Uncertainty scores as grouped by the accurate and inaccurate predictions</p>
<p>Figure 4: Human-AI Reasoning Dashboard with Uncertainty Quantification</p>
<p>https://github.com/pnnl/EXPERT2
FoS tasks include instructions from following domains; Materials science, Economics, Chemistry, Medicine, Psychology, Geography, Geology, Political science, Engineering, Philosophy, Sociology, Physics, Computer science, Law, History, Biology, Agricultural and Food sciences, Environmental science, Business, Education, Art, Linguistics, Mathematics
ACKNOWLEDGEMENTSThis work was supported by the NNSA Office of Defense Nuclear Nonproliferation Research and Development, U.S. Department of Energy, and Pacific Northwest National Laboratory, which is operated by Battelle Memorial Institute for the U.S. Department of Energy under Contract DE-AC05-76RLO1830.This article has been cleared by PNNL for public release as PNNL-SA-191164.APPENDIX A: DOMAIN WISE PERFORMANCE ANALYSISFigure4shows the uncertainty quantification widgets that are built using Plotly Dash, a python-based framework used for rapid prototyping and analytic tool development.The dashboard supports three different types of text generation methods:1. Sampling with temperature: This method randomly picks the next token from a set of high-probability tokens 2. Nucleus (Top-p) Sampling: This method chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability p 3. Beam Search: This method keeps the most likely number of beams for hypotheses at each time step and eventually chooses the hypothesis that has the overall highest probability.
Weight uncertainty in neural network. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra, International conference on machine learning. PMLR2015</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>The robots are coming: Exploring the implications of openai codex on introductory programming. James Finnie-Ansley, Paul Denny, Brett A Becker, Andrew Luxton-Reilly, James Prather, Proceedings of the 24th Australasian Computing Education Conference. the 24th Australasian Computing Education Conference2022</p>
<p>Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, international conference on machine learning. PMLR2016</p>
<p>On calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, International conference on machine learning. PMLR2017</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, International conference on machine learning. PMLR2020</p>
<p>Foundation models of scientific knowledge for chemistry: Opportunities, challenges and lessons learned. Sameera Horawalavithana, Ellyn Ayton, Shivam Sharma, Scott Howland, Megha Subramanian, Scott Vasquez, Robin Cosbey, Maria Glenski, Svitlana Volkova, Proceedings of BigScience Episode# 5-Workshop on Challenges &amp; Perspectives in Creating Large Language Models. BigScience Episode# 5-Workshop on Challenges &amp; Perspectives in Creating Large Language Models2022</p>
<p>Leveraging passage retrieval with generative models for open domain question answering. Gautier Izacard, Edouard Grave, arXiv:2007.012822020arXiv preprint</p>
<p>Unsupervised dense information retrieval with contrastive learning. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, arXiv:2112.091182021arXiv preprint</p>
<p>Few-shot learning with retrieval augmented language models. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave, arXiv:2208.032992022arXiv preprint</p>
<p>How can we know when language models know? on the calibration of language models for question answering. Zhengbao Jiang, Jun Araki, Haibo Ding, Graham Neubig, Transactions of the Association for Computational Linguistics. 92021</p>
<p>TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer, 10.18653/v1/P17-1147Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171</p>
<p>Language models (mostly) know what they know. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, arXiv:2207.052212022arXiv preprint</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, arXiv:2004.049062020arXiv preprint</p>
<p>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.140242022arXiv preprint</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, arXiv:2302.096642023arXiv preprint</p>
<p>Simple and scalable predictive uncertainty estimation using deep ensembles. Alexander Balaji Lakshminarayanan, Charles Pritzel, Blundell, 201730Advances in neural information processing systems</p>
<p>Generating with confidence: Uncertainty quantification for black-box large language models. Zhen Lin, Shubhendu Trivedi, Jimeng Sun, arXiv:2305.191872023arXiv preprint</p>
<p>S2orc: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Dan S Weld, arXiv:1911.027822019arXiv preprint</p>
<p>Evaluating the effectiveness of retrieval-augmented large language models in scientific document reasoning. Sai Munikoti, Anurag Acharya, Sridevi Wagle, Sameera Horawalavithana, arXiv:2311.043482023arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Coqa: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, Transactions of the Association for Computational Linguistics. 72019</p>
<p>Chenglei Si, Chen Zhao, Sewon Min, Jordan Boyd-Graber, arXiv:2205.12507Revisiting calibration for question answering. 2022arXiv preprint</p>
<p>Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, arXiv:2201.119902022arXiv preprint</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Language models for dialog applications. 2022arXiv preprint</p>
<p>Uncertainty estimation and reduction of pre-trained models for text regression. Yuxia Wang, Daniel Beck, Timothy Baldwin, Karin Verspoor, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, Louis-Philippe Morency, arXiv:2210.047142022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7718 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7718</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7718</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-273532791</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.17309v2.pdf" target="_blank">Literature Meets Data: A Synergistic Approach to Hypothesis Generation</a></p>
                <p><strong>Paper Abstract:</strong> AI holds promise for transforming scientific processes, including hypothesis generation. Prior work on hypothesis generation can be broadly categorized into theory-driven and data-driven approaches. While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other. To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation. We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97% over few-shot, 15.75% over literature-based alone, and 3.37% over data-driven alone). Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection. Our results show that human accuracy improves significantly by 7.44% and 14.19% on these tasks, respectively. These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7718.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7718.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LITERATURE-ONLY</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Literature-based hypothesis generation (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven pipeline that ingests manually collected paper summaries (up to ~10 papers) and prompts an LLM to generate interpretable hypotheses distilled from the scholarly literature; used as a standalone baseline and as a component for refinement and union strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Literature Meets Data: A Synergistic Approach to Hypothesis Generation</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LITERATURE-ONLY (literature-based hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Collect short paper summaries and prompt an LLM (system + user prompts) to summarize key findings into concrete, testable hypothesis pairs; optionally apply an LLM-based specificity booster to make literature-derived hypotheses more actionable.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Manually-collected paper summaries (small literature corpus, up to ~10 papers), task descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Textual hypothesis pairs / hypothesis list (natural-language hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Structured system+user prompts for summarization and hypothesis synthesis; specificity booster (LLM-based) to add concrete examples; prompts reproduce citation/summary grounding constraints</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O-MINI, LLaMA-3.1-70B-INSTRUCT (used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (GPT-4O-MINI), 70B parameters (LLaMA-3.1-70B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Manually collected literature corpora (task-specific, up to 10 papers); downstream task datasets: DECEPTIVE REVIEWS, AIGC datasets (GPTGC/LLAMAGC), DREADDIT, PERSUASIVE PAIRS</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Downstream inference accuracy and F1 on OOD and IND sets; human evaluation of utility and novelty</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>As a standalone method, literature-only hypotheses were useful but underperformed data-driven HYPOGENIC on average; when unionized or used to refine data-driven hypotheses, literature information improved generalization on most tasks. Literature+data overall outperforms literature-only by ~15.75% (OOD accuracy) in reported aggregates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Literature corpus is small and manually collected (up to ~10 papers), which limits coverage and scalability; literature-only hypotheses can be brief and lack specificity without a specificity booster; performance varies by task depending on whether the available literature contains interpretable features.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7718.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7718.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HYPOGENIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HYPOGENIC (data-driven hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven LLM-based hypothesis discovery pipeline that initializes a hypothesis bank from a small set of examples and iteratively updates hypotheses using an exploration-exploitation reward (UCB-inspired) driven by performance on data and a wrong-examples pool.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hypothesis generation with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Hypothesis generation with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>HYPOGENIC</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt an LLM with initial data examples to generate an initial hypothesis bank; for each training example select top-k hypotheses, update hypothesis rewards based on accuracy plus an exploration bonus (UCB-like), accumulate wrong examples and periodically generate new hypotheses from that pool to expand the bank.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Labeled training examples / data instances</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Textual hypothesis list / hypothesis bank</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>LLM few-shot initialization from examples; iterative hypothesis evaluation and regeneration driven by a reward function with exploration term (UCB-inspired)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Used in this paper on DECEPTIVE REVIEWS, AIGC detection datasets, DREADDIT, PERSUASIVE PAIRS (task datasets); original HYPOGENIC paper evaluated on similar classification/discovery tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and F1 on IND and OOD held-out datasets; cross-model inference robustness; human utility studies</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>In this work HYPOGENIC (data-driven) consistently outperformed few-shot baselines (average +5.61% accuracy) and served as the backbone for refinements; when combined with literature signals the combined methods outperform HYPOGENIC by an average of ~3.37% (overall OOD gains reported for best combined method).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Can overfit to idiosyncrasies of the training dataset (less generalizable); undervalues literature-derived hypotheses during update because reward is based solely on dataset performance; requires careful hyperparameter settings (e.g., bank size, α, k, w_max).</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7718.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7718.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HYPOREFINE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HYPOREFINE (literature-refined hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid method that injects literature-derived summaries into HYPOGENIC initialization and applies iterative alternating refinement where newly generated data-driven hypotheses are refined by alternating data-driven and literature-based LLM refinement agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Literature Meets Data: A Synergistic Approach to Hypothesis Generation</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>HYPOREFINE</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>At initialization, prompt the LLM with both initial examples and paper summaries to create H_D; during updates, when new hypotheses H0 are generated from a wrong-examples pool, iteratively refine H0 across alternating refinement agents R_data and R_lit (i mod 2 alternation) for max_refine rounds (default 6), then feed refined hypotheses back into HYPOGENIC's hypothesis bank and reward/update procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Paper summaries (literature) + labeled training examples / wrong examples pool</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined textual hypothesis bank (natural-language hypotheses enriched with both data patterns and literature-grounded insights)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Iterative alternating LLM refinement (data-driven agent and literature-driven agent), system+user prompts, reuse of HYPOGENIC reward function to rank/update hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O-MINI, LLaMA-3.1-70B-INSTRUCT (used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (GPT-4O-MINI), 70B parameters (LLaMA-3.1-70B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Same downstream task datasets as experiments: DECEPTIVE REVIEWS, AIGC datasets (GPTGC/LLAMAGC), DREADDIT, PERSUASIVE PAIRS; paper summaries collected manually per task</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>OOD/IND accuracy and F1; human evaluation of utility and novelty; cross-model transfer performance</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>HYPOREFINE (and literature+data combinations) improved OOD generalization over HYPOGENIC on many tasks (average increases reported: combined best method > HYPOGENIC by ~3.37% overall; HYPOREFINE improved over HYPOGENIC by ~3.92% for several tasks), and improved human decision-making utility in user studies. However, for AIGC detection refining with literature sometimes degraded performance (up to −13.64%) when literature lacked useful interpretable signals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends on the relevance and quality of the literature corpus; iterative refinement can hurt performance when literature provides weak or misleading signals (observed for AIGC tasks); literature collection was manual and small-scale (limiting scalability); hyperparameters not exhaustively searched.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7718.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7718.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LITERATURE∪HYPOGENIC (Union)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Union + redundancy-elimination of literature-based and data-driven hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanistic ensemble strategy that builds separate hypothesis banks (literature-based and data-driven), applies an LLM-based redundancy checker to remove entailed/duplicative hypotheses, and composes a final hypothesis bank by selecting top hypotheses from both sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Literature Meets Data: A Synergistic Approach to Hypothesis Generation</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LITERATURE∪HYPOGENIC (Union + redundancy elimination)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Generate two hypothesis banks (one literature-only, one HYPOGENIC/HYPOREFINE), use an LLM-based redundancy checker to compute entailment/redundancy matrix A, rank hypotheses by training accuracy, and construct a balanced final bank (e.g., top-n from HYPOGENIC plus randomly chosen literature hypotheses until size limit), optionally applying a specificity booster to literature items.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Literature-derived hypothesis bank + data-driven hypothesis bank</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Final merged hypothesis bank (textual hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>LLM-based pairwise redundancy/entailment checking; ranking by training accuracy; random selection to maintain balance</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O-MINI, LLaMA-3.1-70B-INSTRUCT (used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (GPT-4O-MINI), 70B parameters (LLaMA-3.1-70B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Downstream task datasets as above; literature summaries used to produce literature bank</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>OOD/IND accuracy and F1; human utility studies</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Unionized hypothesis banks frequently produced the best performance across tasks and models (notably on OOD datasets), outperforming literature-only and data-only baselines in aggregate; union approach mitigates undervaluation of literature hypotheses during reward-driven updates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Randomized selection and manual thresholds (e.g., moving top-10 HYPOGENIC hypotheses first) can introduce variance; redundancy checking via LLM entailment is costly and may be noisy; literature bank quality limits benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7718.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7718.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Specificity Booster</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Specificity Booster for literature hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-processing LLM module applied to literature-only generated hypotheses that expands and concretizes brief literature hypotheses by adding concrete examples and illustrations to improve usability during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Literature Meets Data: A Synergistic Approach to Hypothesis Generation</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Specificity Booster</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply an LLM to literature-derived hypothesis text to add concrete, specific exemplars and clarifications (based on the model's pretraining knowledge) so the hypotheses are more directly applicable during multi-hypothesis inference.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Brief literature-derived hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Expanded, concrete natural-language hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Single-shot or few-shot LLM prompting to elaborate hypotheses with examples</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4O-MINI (used when applied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (GPT-4O-MINI)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Applied to deception detection, mental stress detection, and persuasive argument prediction tasks in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Downstream inference accuracy, human utility (qualitative reports on usability)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Applying the specificity booster improved the applicability of literature-only hypotheses and helped unionized banks perform better in tasks where literature items were initially too brief; exact quantitative uplift not isolated across all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on model pretraining knowledge, which can introduce hallucinated examples if not carefully prompted; only applied to a subset of tasks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7718.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7718.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NotebookLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NotebookLM (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial LLM-powered research assistance tool that accepts uploaded literature and produces source-grounded responses; used here as a literature-based hypothesis generation baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NotebookLM</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>NotebookLM</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Google</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>NotebookLM (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Upload collected literature into the NotebookLM interface and prompt it to generate hypotheses/summaries grounded in the provided sources; used as an off-the-shelf literature-based generator baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Uploaded full text or document summaries (user-supplied literature)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Source-grounded textual summaries / hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Product-provided interface prompts / retrieval-style grounding</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Used with the manually collected literature per task in this study (as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Downstream inference accuracy and F1 when using NotebookLM-generated hypotheses; qualitative validity checks</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>NotebookLM sometimes produced invalid or irrelevant hypotheses that degraded inference performance compared to task-specific HYPOGENIC and the paper's combined approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Off-the-shelf behavior produced some invalid/irrelevant hypotheses in these tasks; product details and internals not disclosed.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7718.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7718.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HyperWrite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HyperWrite (OthersideAI) - Hypothesis Maker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI-driven commercial tool that offers a Hypothesis Maker function to generate hypotheses from an input research question or literature; used here as a literature-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HyperWrite</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>HyperWrite</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>OthersideAI</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>HyperWrite Hypothesis Maker (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Provide the tool with a task description or literature, and it generates candidate hypotheses leveraging pretraining knowledge and any provided context; used as a baseline for comparison with bespoke literature-generation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>User-provided research question / literature summaries</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Textual hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Product-level prompts and internal LLM prompting (details proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Applied to task literature in this study as a baseline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Downstream inference accuracy and F1 when using HyperWrite-generated hypotheses; qualitative inspection</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>HyperWrite generated some hypotheses that were invalid or irrelevant for the evaluated tasks, resulting in degraded inference compared to the paper's methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Proprietary internals not disclosed; produced occasional invalid or irrelevant hypotheses in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7718.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7718.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchAgent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent / LLM framework (Baek et al., 2024) that iteratively consumes scientific literature to propose research ideas and assist scientific writing and discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ResearchAgent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>ResearchAgent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An LLM-agent framework that iteratively reads and summarizes literature and proposes research ideas/iterations over the collection of scientific documents; cited in this paper as a related method for literature-driven idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Scientific literature (raw papers / summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research ideas, summaries, potentially hypotheses or research plans</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Iterative agent-based prompting and literature summarization (multi-step LLM interactions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7718.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7718.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sci-Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sci-Agents: Automating scientific discovery through multi-agent intelligent graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed multi-agent system that uses LLMs to construct knowledge graphs and perform reasoning over literature to assist automated scientific discovery and hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sci-Agents: Automating scientific discovery through multi-agent intelligent graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Sci-Agents: Automating scientific discovery through multi-agent intelligent graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Alireza Ghafarollahi, Markus J. Buehler</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Sci-Agents</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Multi-agent LLM framework that creates structured knowledge (knowledge graphs) from literature and applies graph reasoning to generate hypotheses and support discovery (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Scientific articles / literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured knowledge (knowledge graphs), generated hypotheses / discoveries</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Multi-agent coordination, graph construction and reasoning with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7718.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7718.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qi et al. (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs as zero-shot hypothesis proposers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work demonstrating that large language models can propose hypotheses in a zero-shot manner from text; cited here as evidence for LLMs' capability to generate novel hypotheses from corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero shot hypothesis proposers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Large language models are zero shot hypothesis proposers</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Zero-shot hypothesis proposing with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Demonstrates LLMs' capability to propose hypotheses without task-specific fine-tuning, used as prior work motivating the current study's hybrid approach.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Raw text corpora / examples</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Natural-language hypothesis proposals</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7718.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7718.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Li et al. (2024) / MLR-Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLR-Copilot: Autonomous machine learning research based on large language models (related example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-agent framework that aims to autonomously propose research ideas and implement/execute experiments based on literature and other inputs; cited as an example of automating parts of the research process from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MLR-Copilot: Autonomous machine learning research based on large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>MLR-Copilot: Autonomous machine learning research based on large language models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2024</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MLR-Copilot (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An autonomous research agent that leverages LLMs to propose research directions, implement experiments, and iterate — cited as related work demonstrating literature-to-experiment automation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Scientific literature, experiment specifications</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research ideas, implemented experiments, results</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Agent-based orchestration and LLM-driven generation/execution</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Literature Meets Data: A Synergistic Approach to Hypothesis Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hypothesis generation with large language models <em>(Rating: 2)</em></li>
                <li>ResearchAgent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>Sci-Agents: Automating scientific discovery through multi-agent intelligent graph reasoning <em>(Rating: 2)</em></li>
                <li>Large language models are zero shot hypothesis proposers <em>(Rating: 2)</em></li>
                <li>MLR-Copilot: Autonomous machine learning research based on large language models <em>(Rating: 1)</em></li>
                <li>NotebookLM <em>(Rating: 1)</em></li>
                <li>HyperWrite <em>(Rating: 1)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7718",
    "paper_id": "paper-273532791",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "LITERATURE-ONLY",
            "name_full": "Literature-based hypothesis generation (this paper)",
            "brief_description": "An LLM-driven pipeline that ingests manually collected paper summaries (up to ~10 papers) and prompts an LLM to generate interpretable hypotheses distilled from the scholarly literature; used as a standalone baseline and as a component for refinement and union strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
            "authors": "Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan",
            "year": 2024,
            "method_name": "LITERATURE-ONLY (literature-based hypothesis generation)",
            "method_description": "Collect short paper summaries and prompt an LLM (system + user prompts) to summarize key findings into concrete, testable hypothesis pairs; optionally apply an LLM-based specificity booster to make literature-derived hypotheses more actionable.",
            "input_type": "Manually-collected paper summaries (small literature corpus, up to ~10 papers), task descriptions",
            "output_type": "Textual hypothesis pairs / hypothesis list (natural-language hypotheses)",
            "prompting_technique": "Structured system+user prompts for summarization and hypothesis synthesis; specificity booster (LLM-based) to add concrete examples; prompts reproduce citation/summary grounding constraints",
            "model_name": "GPT-4O-MINI, LLaMA-3.1-70B-INSTRUCT (used in experiments)",
            "model_size": "proprietary (GPT-4O-MINI), 70B parameters (LLaMA-3.1-70B-Instruct)",
            "datasets_used": "Manually collected literature corpora (task-specific, up to 10 papers); downstream task datasets: DECEPTIVE REVIEWS, AIGC datasets (GPTGC/LLAMAGC), DREADDIT, PERSUASIVE PAIRS",
            "evaluation_metric": "Downstream inference accuracy and F1 on OOD and IND sets; human evaluation of utility and novelty",
            "reported_results": "As a standalone method, literature-only hypotheses were useful but underperformed data-driven HYPOGENIC on average; when unionized or used to refine data-driven hypotheses, literature information improved generalization on most tasks. Literature+data overall outperforms literature-only by ~15.75% (OOD accuracy) in reported aggregates.",
            "limitations": "Literature corpus is small and manually collected (up to ~10 papers), which limits coverage and scalability; literature-only hypotheses can be brief and lack specificity without a specificity booster; performance varies by task depending on whether the available literature contains interpretable features.",
            "counterpoint": false,
            "uuid": "e7718.0",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "HYPOGENIC",
            "name_full": "HYPOGENIC (data-driven hypothesis generation)",
            "brief_description": "A data-driven LLM-based hypothesis discovery pipeline that initializes a hypothesis bank from a small set of examples and iteratively updates hypotheses using an exploration-exploitation reward (UCB-inspired) driven by performance on data and a wrong-examples pool.",
            "citation_title": "Hypothesis generation with large language models",
            "mention_or_use": "use",
            "paper_title": "Hypothesis generation with large language models",
            "authors": "Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan",
            "year": 2024,
            "method_name": "HYPOGENIC",
            "method_description": "Prompt an LLM with initial data examples to generate an initial hypothesis bank; for each training example select top-k hypotheses, update hypothesis rewards based on accuracy plus an exploration bonus (UCB-like), accumulate wrong examples and periodically generate new hypotheses from that pool to expand the bank.",
            "input_type": "Labeled training examples / data instances",
            "output_type": "Textual hypothesis list / hypothesis bank",
            "prompting_technique": "LLM few-shot initialization from examples; iterative hypothesis evaluation and regeneration driven by a reward function with exploration term (UCB-inspired)",
            "model_name": null,
            "model_size": null,
            "datasets_used": "Used in this paper on DECEPTIVE REVIEWS, AIGC detection datasets, DREADDIT, PERSUASIVE PAIRS (task datasets); original HYPOGENIC paper evaluated on similar classification/discovery tasks",
            "evaluation_metric": "Accuracy and F1 on IND and OOD held-out datasets; cross-model inference robustness; human utility studies",
            "reported_results": "In this work HYPOGENIC (data-driven) consistently outperformed few-shot baselines (average +5.61% accuracy) and served as the backbone for refinements; when combined with literature signals the combined methods outperform HYPOGENIC by an average of ~3.37% (overall OOD gains reported for best combined method).",
            "limitations": "Can overfit to idiosyncrasies of the training dataset (less generalizable); undervalues literature-derived hypotheses during update because reward is based solely on dataset performance; requires careful hyperparameter settings (e.g., bank size, α, k, w_max).",
            "counterpoint": false,
            "uuid": "e7718.1",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "HYPOREFINE",
            "name_full": "HYPOREFINE (literature-refined hypothesis generation)",
            "brief_description": "A hybrid method that injects literature-derived summaries into HYPOGENIC initialization and applies iterative alternating refinement where newly generated data-driven hypotheses are refined by alternating data-driven and literature-based LLM refinement agents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
            "authors": "Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan",
            "year": 2024,
            "method_name": "HYPOREFINE",
            "method_description": "At initialization, prompt the LLM with both initial examples and paper summaries to create H_D; during updates, when new hypotheses H0 are generated from a wrong-examples pool, iteratively refine H0 across alternating refinement agents R_data and R_lit (i mod 2 alternation) for max_refine rounds (default 6), then feed refined hypotheses back into HYPOGENIC's hypothesis bank and reward/update procedure.",
            "input_type": "Paper summaries (literature) + labeled training examples / wrong examples pool",
            "output_type": "Refined textual hypothesis bank (natural-language hypotheses enriched with both data patterns and literature-grounded insights)",
            "prompting_technique": "Iterative alternating LLM refinement (data-driven agent and literature-driven agent), system+user prompts, reuse of HYPOGENIC reward function to rank/update hypotheses",
            "model_name": "GPT-4O-MINI, LLaMA-3.1-70B-INSTRUCT (used in experiments)",
            "model_size": "proprietary (GPT-4O-MINI), 70B parameters (LLaMA-3.1-70B-Instruct)",
            "datasets_used": "Same downstream task datasets as experiments: DECEPTIVE REVIEWS, AIGC datasets (GPTGC/LLAMAGC), DREADDIT, PERSUASIVE PAIRS; paper summaries collected manually per task",
            "evaluation_metric": "OOD/IND accuracy and F1; human evaluation of utility and novelty; cross-model transfer performance",
            "reported_results": "HYPOREFINE (and literature+data combinations) improved OOD generalization over HYPOGENIC on many tasks (average increases reported: combined best method &gt; HYPOGENIC by ~3.37% overall; HYPOREFINE improved over HYPOGENIC by ~3.92% for several tasks), and improved human decision-making utility in user studies. However, for AIGC detection refining with literature sometimes degraded performance (up to −13.64%) when literature lacked useful interpretable signals.",
            "limitations": "Depends on the relevance and quality of the literature corpus; iterative refinement can hurt performance when literature provides weak or misleading signals (observed for AIGC tasks); literature collection was manual and small-scale (limiting scalability); hyperparameters not exhaustively searched.",
            "counterpoint": false,
            "uuid": "e7718.2",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LITERATURE∪HYPOGENIC (Union)",
            "name_full": "Union + redundancy-elimination of literature-based and data-driven hypotheses",
            "brief_description": "A mechanistic ensemble strategy that builds separate hypothesis banks (literature-based and data-driven), applies an LLM-based redundancy checker to remove entailed/duplicative hypotheses, and composes a final hypothesis bank by selecting top hypotheses from both sources.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
            "authors": "Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan",
            "year": 2024,
            "method_name": "LITERATURE∪HYPOGENIC (Union + redundancy elimination)",
            "method_description": "Generate two hypothesis banks (one literature-only, one HYPOGENIC/HYPOREFINE), use an LLM-based redundancy checker to compute entailment/redundancy matrix A, rank hypotheses by training accuracy, and construct a balanced final bank (e.g., top-n from HYPOGENIC plus randomly chosen literature hypotheses until size limit), optionally applying a specificity booster to literature items.",
            "input_type": "Literature-derived hypothesis bank + data-driven hypothesis bank",
            "output_type": "Final merged hypothesis bank (textual hypotheses)",
            "prompting_technique": "LLM-based pairwise redundancy/entailment checking; ranking by training accuracy; random selection to maintain balance",
            "model_name": "GPT-4O-MINI, LLaMA-3.1-70B-INSTRUCT (used in experiments)",
            "model_size": "proprietary (GPT-4O-MINI), 70B parameters (LLaMA-3.1-70B-Instruct)",
            "datasets_used": "Downstream task datasets as above; literature summaries used to produce literature bank",
            "evaluation_metric": "OOD/IND accuracy and F1; human utility studies",
            "reported_results": "Unionized hypothesis banks frequently produced the best performance across tasks and models (notably on OOD datasets), outperforming literature-only and data-only baselines in aggregate; union approach mitigates undervaluation of literature hypotheses during reward-driven updates.",
            "limitations": "Randomized selection and manual thresholds (e.g., moving top-10 HYPOGENIC hypotheses first) can introduce variance; redundancy checking via LLM entailment is costly and may be noisy; literature bank quality limits benefit.",
            "counterpoint": false,
            "uuid": "e7718.3",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Specificity Booster",
            "name_full": "LLM-based Specificity Booster for literature hypotheses",
            "brief_description": "A post-processing LLM module applied to literature-only generated hypotheses that expands and concretizes brief literature hypotheses by adding concrete examples and illustrations to improve usability during inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
            "authors": "Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan",
            "year": 2024,
            "method_name": "Specificity Booster",
            "method_description": "Apply an LLM to literature-derived hypothesis text to add concrete, specific exemplars and clarifications (based on the model's pretraining knowledge) so the hypotheses are more directly applicable during multi-hypothesis inference.",
            "input_type": "Brief literature-derived hypotheses",
            "output_type": "Expanded, concrete natural-language hypotheses",
            "prompting_technique": "Single-shot or few-shot LLM prompting to elaborate hypotheses with examples",
            "model_name": "GPT-4O-MINI (used when applied)",
            "model_size": "proprietary (GPT-4O-MINI)",
            "datasets_used": "Applied to deception detection, mental stress detection, and persuasive argument prediction tasks in experiments",
            "evaluation_metric": "Downstream inference accuracy, human utility (qualitative reports on usability)",
            "reported_results": "Applying the specificity booster improved the applicability of literature-only hypotheses and helped unionized banks perform better in tasks where literature items were initially too brief; exact quantitative uplift not isolated across all tasks.",
            "limitations": "Relies on model pretraining knowledge, which can introduce hallucinated examples if not carefully prompted; only applied to a subset of tasks in the paper.",
            "counterpoint": false,
            "uuid": "e7718.4",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "NotebookLM",
            "name_full": "NotebookLM (Google)",
            "brief_description": "A commercial LLM-powered research assistance tool that accepts uploaded literature and produces source-grounded responses; used here as a literature-based hypothesis generation baseline.",
            "citation_title": "NotebookLM",
            "mention_or_use": "use",
            "paper_title": "NotebookLM",
            "authors": "Google",
            "year": 2024,
            "method_name": "NotebookLM (baseline)",
            "method_description": "Upload collected literature into the NotebookLM interface and prompt it to generate hypotheses/summaries grounded in the provided sources; used as an off-the-shelf literature-based generator baseline in experiments.",
            "input_type": "Uploaded full text or document summaries (user-supplied literature)",
            "output_type": "Source-grounded textual summaries / hypotheses",
            "prompting_technique": "Product-provided interface prompts / retrieval-style grounding",
            "model_name": null,
            "model_size": null,
            "datasets_used": "Used with the manually collected literature per task in this study (as a baseline)",
            "evaluation_metric": "Downstream inference accuracy and F1 when using NotebookLM-generated hypotheses; qualitative validity checks",
            "reported_results": "NotebookLM sometimes produced invalid or irrelevant hypotheses that degraded inference performance compared to task-specific HYPOGENIC and the paper's combined approaches.",
            "limitations": "Off-the-shelf behavior produced some invalid/irrelevant hypotheses in these tasks; product details and internals not disclosed.",
            "counterpoint": null,
            "uuid": "e7718.5",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "HyperWrite",
            "name_full": "HyperWrite (OthersideAI) - Hypothesis Maker",
            "brief_description": "An AI-driven commercial tool that offers a Hypothesis Maker function to generate hypotheses from an input research question or literature; used here as a literature-based baseline.",
            "citation_title": "HyperWrite",
            "mention_or_use": "use",
            "paper_title": "HyperWrite",
            "authors": "OthersideAI",
            "year": 2024,
            "method_name": "HyperWrite Hypothesis Maker (baseline)",
            "method_description": "Provide the tool with a task description or literature, and it generates candidate hypotheses leveraging pretraining knowledge and any provided context; used as a baseline for comparison with bespoke literature-generation methods.",
            "input_type": "User-provided research question / literature summaries",
            "output_type": "Textual hypotheses",
            "prompting_technique": "Product-level prompts and internal LLM prompting (details proprietary)",
            "model_name": null,
            "model_size": null,
            "datasets_used": "Applied to task literature in this study as a baseline",
            "evaluation_metric": "Downstream inference accuracy and F1 when using HyperWrite-generated hypotheses; qualitative inspection",
            "reported_results": "HyperWrite generated some hypotheses that were invalid or irrelevant for the evaluated tasks, resulting in degraded inference compared to the paper's methods.",
            "limitations": "Proprietary internals not disclosed; produced occasional invalid or irrelevant hypotheses in experiments.",
            "counterpoint": null,
            "uuid": "e7718.6",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "ResearchAgent: Iterative research idea generation over scientific literature with large language models",
            "brief_description": "A multi-agent / LLM framework (Baek et al., 2024) that iteratively consumes scientific literature to propose research ideas and assist scientific writing and discovery.",
            "citation_title": "ResearchAgent: Iterative research idea generation over scientific literature with large language models",
            "mention_or_use": "mention",
            "paper_title": "ResearchAgent: Iterative research idea generation over scientific literature with large language models",
            "authors": "Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang",
            "year": 2024,
            "method_name": "ResearchAgent",
            "method_description": "An LLM-agent framework that iteratively reads and summarizes literature and proposes research ideas/iterations over the collection of scientific documents; cited in this paper as a related method for literature-driven idea generation.",
            "input_type": "Scientific literature (raw papers / summaries)",
            "output_type": "Research ideas, summaries, potentially hypotheses or research plans",
            "prompting_technique": "Iterative agent-based prompting and literature summarization (multi-step LLM interactions)",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7718.7",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Sci-Agents",
            "name_full": "Sci-Agents: Automating scientific discovery through multi-agent intelligent graph reasoning",
            "brief_description": "A proposed multi-agent system that uses LLMs to construct knowledge graphs and perform reasoning over literature to assist automated scientific discovery and hypothesis generation.",
            "citation_title": "Sci-Agents: Automating scientific discovery through multi-agent intelligent graph reasoning",
            "mention_or_use": "mention",
            "paper_title": "Sci-Agents: Automating scientific discovery through multi-agent intelligent graph reasoning",
            "authors": "Alireza Ghafarollahi, Markus J. Buehler",
            "year": 2024,
            "method_name": "Sci-Agents",
            "method_description": "Multi-agent LLM framework that creates structured knowledge (knowledge graphs) from literature and applies graph reasoning to generate hypotheses and support discovery (cited as related work).",
            "input_type": "Scientific articles / literature",
            "output_type": "Structured knowledge (knowledge graphs), generated hypotheses / discoveries",
            "prompting_technique": "Multi-agent coordination, graph construction and reasoning with LLMs",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7718.8",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Qi et al. (2023)",
            "name_full": "LLMs as zero-shot hypothesis proposers",
            "brief_description": "Work demonstrating that large language models can propose hypotheses in a zero-shot manner from text; cited here as evidence for LLMs' capability to generate novel hypotheses from corpora.",
            "citation_title": "Large language models are zero shot hypothesis proposers",
            "mention_or_use": "mention",
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "authors": "Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou",
            "year": 2023,
            "method_name": "Zero-shot hypothesis proposing with LLMs",
            "method_description": "Demonstrates LLMs' capability to propose hypotheses without task-specific fine-tuning, used as prior work motivating the current study's hybrid approach.",
            "input_type": "Raw text corpora / examples",
            "output_type": "Natural-language hypothesis proposals",
            "prompting_technique": "Zero-shot prompting",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7718.9",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Li et al. (2024) / MLR-Copilot",
            "name_full": "MLR-Copilot: Autonomous machine learning research based on large language models (related example)",
            "brief_description": "An LLM-agent framework that aims to autonomously propose research ideas and implement/execute experiments based on literature and other inputs; cited as an example of automating parts of the research process from literature.",
            "citation_title": "MLR-Copilot: Autonomous machine learning research based on large language models",
            "mention_or_use": "mention",
            "paper_title": "MLR-Copilot: Autonomous machine learning research based on large language models",
            "authors": "Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du",
            "year": 2024,
            "method_name": "MLR-Copilot (related work)",
            "method_description": "An autonomous research agent that leverages LLMs to propose research directions, implement experiments, and iterate — cited as related work demonstrating literature-to-experiment automation.",
            "input_type": "Scientific literature, experiment specifications",
            "output_type": "Research ideas, implemented experiments, results",
            "prompting_technique": "Agent-based orchestration and LLM-driven generation/execution",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": null,
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7718.10",
            "source_info": {
                "paper_title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hypothesis generation with large language models",
            "rating": 2,
            "sanitized_title": "hypothesis_generation_with_large_language_models"
        },
        {
            "paper_title": "ResearchAgent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "Sci-Agents: Automating scientific discovery through multi-agent intelligent graph reasoning",
            "rating": 2,
            "sanitized_title": "sciagents_automating_scientific_discovery_through_multiagent_intelligent_graph_reasoning"
        },
        {
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zero_shot_hypothesis_proposers"
        },
        {
            "paper_title": "MLR-Copilot: Autonomous machine learning research based on large language models",
            "rating": 1,
            "sanitized_title": "mlrcopilot_autonomous_machine_learning_research_based_on_large_language_models"
        },
        {
            "paper_title": "NotebookLM",
            "rating": 1,
            "sanitized_title": "notebooklm"
        },
        {
            "paper_title": "HyperWrite",
            "rating": 1,
            "sanitized_title": "hyperwrite"
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        }
    ],
    "cost": 0.023583999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Literature Meets Data: A Synergistic Approach to Hypothesis Generation
22 Oct 2024</p>
<p>Haokun Liu haokunliu@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Yangqiaoyu Zhou zhouy1@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Mingxuan Li mingxuanl@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Chenfei Yuan 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Chenhao Tan chenhao@uchicago.edu 
Department of Computer Science
University of Chicago ♣
Tsinghua University † Chicago
60637ILUSA</p>
<p>Literature Meets Data: A Synergistic Approach to Hypothesis Generation
22 Oct 20242D7B3BAAC6C7286A72FEB90112AD0DE1arXiv:2410.17309v1[cs.AI]
AI holds promise for transforming scientific processes, including hypothesis generation.Prior work on hypothesis generation can be broadly categorized into theory-driven and datadriven approaches.While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other.To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation.We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97% over fewshot, 15.75% over literature-based alone, and 3.37% over data-driven alone).Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection.Our results show that human accuracy improves significantly by 7.44% and 14.19% on these tasks, respectively.These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry.</p>
<p>Introduction</p>
<p>"It is the theory that decides what can be observed."</p>
<p>-Albert Einstein Large language models (LLMs) excel at synthesizing information and hold promise for transforming hypothesis generation, a critical yet understudied step in scientific discoveries.Many recent studies recognize this potential and use LLMs to generate hypotheses (e.g., Yang et al., 2024b;Batista and Ross, 2024).We broadly categorize them into theory-driven and data-driven methods.* Equal contributions.</p>
<p>On one hand, theory-driven approaches leverage LLMs to review existing literature and generate novel hypotheses (Yang et al., 2024b;Baek et al., 2024).These methods have shown promising results in terms of the hypotheses' novelty, validity, and usefulness to researchers, while remaining grounded in established human knowledge (Si et al., 2024).However, they come with notable limitations: they require high-quality literature, struggle to adapt to new data, and lack empirical support.Data-driven approaches, on the other hand, propose hypotheses by discovering patterns in data (Zhou et al., 2024;Qiu et al., 2024).These hypotheses are data-adaptive and can exhibit strong performance in explaining the data.However, they could be too overly tailored to the specific datasets used, which can hinder their generalizability.</p>
<p>We hypothesize that theory can guide the discovery from data and propose to integrate literaturebased and data-driven hypothesis generation (see Figure 1).For the data-driven component, we use HYPOGENIC as the backbone (Zhou et al., 2024).HYPOGENIC leverages an LLM to initialize hypotheses from a small number of examples and then updates them iteratively to improve the quality of hypotheses.To enhance this process with literature insights, we introduce a literature-based hypothesis agent.This agent interacts with the data-driven hypothesis agent (HYPOGENIC), refining and maintaining a shared pool of hypotheses through continuous collaboration, ensuring that the hypotheses benefit from both data-driven adaptability and the grounding of existing scientific knowledge.In addition to the refinement approach, we also propose to directly unionize literature-based and data-driven hypotheses.</p>
<p>To comprehensively evaluate these hypotheses, we conduct automatic and human evaluation to assess their generalizability, utility, and novelty.We apply our method to address research questions in social sciences: deception detection, AI generated content (AIGC) detection, mental stress detection, and persuasive argument prediction.Automatic evaluation results show that integrating literature and data outperforms other baselines: 8.97% over few-shot, 15.75% over literature-based alone, and 3.37% over data-driven alone in accuracy on out-ofdistribution datasets, a measure of generalizability.</p>
<p>Moreover, we conduct the first study to assess the utility of AI-generated hypotheses in improving human decision-making and show that our generated hypotheses improve human accuracy by 7.44% and 14.19% on deception detection and AIGC detection.Additionally, we find that literature-based and data-driven hypotheses complement each other, as one set often contains novel information not found in the other set.</p>
<p>In addition to our own implementation, we also use commercial ones such as NOTEBOOKLM (Google, 2024) and HYPERWRITE (OthersideAI, 2024) as baselines.</p>
<p>Data-Driven Hypothesis Generation</p>
<p>Our data-driven hypothesis generation largely follows HYPOGENIC in Zhou et al. (2024).Here we give a brief overview.During the initialization stage of HYPOGENIC, an LLM is prompted with a set of initial data instances D init from the training set D and asked to generate initial hypotheses that forms the initial hypothesis bank H D .</p>
<p>In the update stage, for each example s ∈ D, top k high-reward hypotheses from H D are selected and each used to prompt the LLM to make a prediction on s.The accuracy and reward of the k hypotheses are updated accordingly.Among the k hypotheses, if at least w hyp predicted wrong on s, s is added to a wrong examples pool W. Once the size of W reaches w max , a set of new hypotheses are generated from W and added to H D according to their reward.Inspired by the upper confidence bound (UCB) algorithm (Auer, 2003), the reward function of HYPOGENIC is defined as follows:
r i = (x j ,y j )∈S i I(y j = ŷj ) |S i | + α log t |S i | ,
where S i is the set of examples used to evaluate hypothesis h i , t is the training time step, and α is the reward coefficient that controls the exploration term of the reward function.</p>
<p>Integration of Literature-based and Data-driven Hypotheses</p>
<p>One main contribution of our work is proposing the first approach to integrating literature-based and data-driven hypothesis generation so that we can effectively leverage the strengths of each approach, increasing the generalizability and utility of generated hypotheses.We consider two strategies.</p>
<p>Refinement of literature-based hypotheses.HYPOREFINE integrates paper summaries from § 2.1 with HYPOGENIC.In the initialization stage of HYPOGENIC, an LLM is asked to generate initial hypotheses based on both a set of initial examples and paper summaries relevant to the task.</p>
<p>In the update stage, we propose an iterative refinement approach to integrate patterns from data and key findings from literature into new hypotheses.Specifically, each time HYPOGENIC generates a set of new hypotheses H 0 from the wrong examples pool W, these hypotheses are refined multiple rounds by a data-driven refinement agent and a literature-based refinement agent.Take R M as the refinement agent based on model M, each time H 0 is generated from the wrong examples pool W, it is iteratively refined as follows:
H i = R M (q i , H i−1 , P) if i mod 2 = 0 R M (q i , H i−1 , W) if i mod 2 = 1,
where P represents the literature information, W represents the data pool used to generate H 0 , and q being the queries.After max_refine rounds of refinement, the final hypothesis bank H max_refine is fed back to the HYPOGENIC pipeline.</p>
<p>The reward function and update process for the hypothesis bank H remain consistent with those of the original HYPOGENIC.</p>
<p>Union and redundancy elimination.As the reward function of HYPOGENIC focuses only on the hypotheses' performance on the datasets at hand, literature-based hypotheses are sometimes undervalued during the update stage.On occasions they can even be replaced by hypotheses that have especially good performances on data but are not necessarily generalizable on real-world tasks.To counter this issue, we use a union approach to mechanistically combine literature-based and data-based hypotheses.We first generate two hypothesis banks: one literature-based hypothesis bank and another bank using HYPOGENIC or HYPOREFINE.Then we build a redundancy checker to remove hypotheses that express overly similar or repeating information in each bank.Lastly, we construct the final hypothesis bank of size n by randomly choosing</p>
<p>Experiments</p>
<p>In this section, we introduce our evaluation framework and the tasks to operationalize it.</p>
<p>Evaluation Framework</p>
<p>Formally evaluating hypotheses requires rigorous protocols and vast amounts of resources.In this work, we mainly evaluate our generated hypotheses along two dimensions: utility and novelty.We perform both automatic and human evaluations to show that our generated hypotheses can help models and humans in challenging real-world classification tasks and bring novel information.</p>
<p>Automatic evaluation on out-of-distribution (OOD) and in-distribution (IND) datasets and cross-model inference.Since we work with classification tasks, a natural way of evaluating the hypotheses is prompting the LLMs to do inference with the hypotheses.For all the methods that generate hypotheses, we provide a new data example and all the generated hypotheses to the LLMs.Then we prompt the LLMs to first extract the most relevant hypotheses to the new example and make inference using the hypotheses.For detailed information about the prompts, please refer to Appendix A. For each task, we report accuracy and F1 scores on held-out OOD and IND sets for 5 different random seeds.Since we are most interested in the generalizability of the generated hypotheses, we focus on performance on the OOD set in the main paper.</p>
<p>In addition to predicting on out-of-distribution datasets, we test our hypotheses' generalizability by taking the hypotheses generated by one model and performing inference with another model.</p>
<p>Human evaluation on utility and novelty.We design human studies to assess the practical utility and novelty of the generated hypotheses.Specifically, we aim to (1) evaluate whether these hypotheses have meaningful impact on human decisionmaking and (2) determine whether data-driven and literature-driven hypotheses indeed offer distinct perspectives and contribute novel information to the unionized hypotheses pool.We run human studies on Deception Detection and AIGC Detection.Screenshots of the studies are in Appendix C. We pay participants at an average hourly rate of $12.</p>
<p>Human Study I: Utility in human decisionmaking.We recruit 60 participants on prolific.comand randomly assign them into experimental and control groups.The control group performs the task without hypotheses, while the experiment group is given a set of three generated hypotheses to complete the same task.Specifically, each participant is randomly assigned 14 instances, and we include attention check questions to ensure the quality of the collected responses.We evaluate the practical utility of our generated hypotheses by comparing the performance of the two groups.</p>
<p>We pick the hypotheses based on their impact on performance in an ablation setting.Specifically, we choose the top three hypotheses that cause the greatest drop in performance when removed from the hypotheses pool during multi-hypothesis inference.In addition, to motivate participants to perform at their best, we offer a bonus of $0.1 for each correctly predicted instance.</p>
<p>At the end of the study, participants in the experiment group are also asked to give overall ratings and an assessment of the given hypotheses.There are five scales: "Not at all helpful", "Slightly helpful", "Moderately helpful", "Very helpful", and "Extremely helpful".</p>
<p>Human study II: Novelty and nuance.To compare data-driven hypotheses and literature-driven hypotheses, we present one hypothesis of each type to participants and ask them to judge whether the second hypothesis provides meaningfully novel information that is not covered in the first hypothesis.</p>
<p>We sample 50 pairs of hypotheses (h 1 , h 2 ), one from literature-based and one from data-driven, with duplications removed within each group.We recruit 10 Prolific participants to annotate whether h 2 provides new information to h 1 for each pair.Each participant is randomly assigned to annotate 15 pairs.For each pair, we take the majority vote to determine the final novelty label.</p>
<p>Tasks</p>
<p>We consider four tasks in social sciences.Deception Detection is a widely studied problem in psychology and other social sciences (Granhag and Vrij, 2005).We use the dataset introduced by Ott et al. ( 2013) (DECEPTIVE REVIEWS), which consists of 800 genuine hotel reviews and 800 fake hotel reviews, as our IND dataset.For the OOD dataset, we use hotel reviews from different source websites and different cities (Li et al., 2013).AI-Generated Content (AIGC) Detection has attracted significant attention in recent years (Tang et al., 2023).Most existing works focus on developing black-box detection methods and rarely take interpretability into account (Wu et al., 2024).We thus build our own dataset for this task.We take 800 distinct prompts and human-written stories in the WRITINGPROMPTS dataset (Fan et al., 2018).Then we use the same prompts to generate AIwritten stories with LLAMA-3.1-70B-INSTRUCT(Dubey et al., 2024) and GPT-4O-MINI (OpenAI, 2023), constituting our LLAMAGC and GPTGC datasets.The IND data contains stories generated by the corresponding model.The stories generated by the other model are treated as OOD data.Mental Stress Detection from social media content is an important task in mental health (Lupien et al., 2009).We use DREADDIT, a corpus of lengthy Reddit posts with stress status labels developed by Turcan and McKeown (2019).The dataset contains 3.5k post segments annotated using Amazon Mechanical Turk, with labels indicating the presence or absence of stress in posts.Our IND and OOD sets are separated based on subreddits that the posts come from.Persuasive Argument Prediction examines persuasion and social interactions to reveal predictive cues of persuasiveness (Tan et al., 2016).We use PERSUASIVE PAIRS, a dataset with pairs of short texts constructed by Pauli et al. (2024).Within each pair of texts, one is from existing corpora with signals of persuasiveness, while the other one is generated by an LLM with instructions for it to be more/less persuasive than the one from existing corpora.We formulate this task as predicting the more persuasive one of each pair of texts.The dataset contains human-annotated ground-truth labels and is pre-processed by removing examples where there exists disagreement among annotators.The IND and OOD datasets are then created based on different original sources of texts.</p>
<p>For each task, we split the IND dataset with at least 200 examples in train set, 300 in test set (on which we perform inference), 300 in validation set, and sample at least 300 instances from OOD (see Appendix B.1 for more details).</p>
<p>Implementation and Baselines</p>
<p>Our method works with any LLM (M).We use GPT-4O-MINI and LLAMA-3.1-70B-INSTRUCT in this work.Throughout this paper, we refer to GPT-4O-MINI as "GPT-4-MINI" and LLAMA-3.1-70B-INSTRUCT as "LLAMA-70B-I".We compare our method with the following baselines.</p>
<ol>
<li>
<p>Zero-shot and few-shot prompting.We give the LLMs detailed task instructions (zero-shot) and optionally provide three demonstrating examples (few-shot).This approach does not involve any hypothesis.</p>
</li>
<li>
<p>Zero-shot hypothesis generation.Inspired by Qi et al. (2023), we provide specific task descriptions and instructions, and then we prompt the LLMs to generate hypotheses directly without incorporating literature or data.</p>
</li>
<li>
<p>Literature-driven hypothesis generation.We use the implementation in §2.1.In addition to our own implementation, we compare two of the recently released agent frameworks for scientific writing, NOTEBOOKLM (Google, 2024) and HYPERWRITE (OthersideAI, 2024)</p>
</li>
</ol>
<p>Results</p>
<p>We first present automatic evaluation results to demonstrate the utility of generated hypotheses for model inference.We then show that the generated hypotheses can improve human decision-making in challenging tasks and that literature-based and datadriven hypotheses provide unique insights from each other.</p>
<p>Automatic Evaluation</p>
<p>Hypotheses generated by combining information from literature and data achieves the best performance across all task and model configurations (Table 1).First, few-shot inference outperforms zero-shot inference for all task and model configurations, with an average improvement of 6.84% in accuracy.In addition, few-shot inference surpasses zero-shot generation and the best of literature-based methods on average accuracy by 7.21% and 6.78%, respectively, suggesting off-theshelve LLMs or literature alone does not generate effective hypotheses for predictive purposes.In fact, NOTEBOOKLM and HYPERWRITE can generate some invalid or irrelevant hypotheses, which degrades their inference performance (see Table 9 in Appendix).</p>
<p>In contrast, HYPOGENIC consistently outperforms few-shot inference, improving average accuracy by 5.61%, highlighting the advantage of data-driven hypotheses.Compared to few-shot inference, the hypotheses also offer more interpretable insights.Furthermore, our best hypothesis generation method combining literature and data outperforms HYPOGENIC by 3.37% on average (i.e., 8.97% over few-shot and 15.75% over literature-based methods), demonstrating the benefit of incorporating literature with data.</p>
<p>For DECEPTIVE REVIEWS, PERSUASIVE PAIRS, and DREADDIT, refining the hypotheses with literature consistently improves inference accuracy compared to HYPOGENIC, with a 3.92% improvement on average.On the other hand, refining the hypotheses with literature does not help with GPTGC and LLAMAGC, but the union of HYPOGENIC and hypotheses generated from literature consistently performs the best.Comparing with HYPOGENIC for these two tasks, refining the hypotheses with literature actually results in an accuracy drop by 13.64%.This is likely due to that the literature for AIGC detection has relatively few insights on interpretable features to detect AI generated contents, and refining the data-driven hypotheses with that information degrades performance.</p>
<p>To further illustrate our approach, we present a case study of our generated hypotheses in Table 3.For most cases, LITERATURE-ONLY and HYPOGENIC generate different hypotheses as in Case I: one is about first-person singular pronouns, while the other one is about past experiences.We include more details on the differences between hypotheses generated by different methods in § 4.2.More examples of hypotheses generated using LIT-ERATURE∪HYPOREFINE are in Table 8.</p>
<p>Under some cases, the methods can generate similar hypotheses, and HYPOREFINE improves the quality of the hypothesis.In Case II, all three hypotheses focus on balanced perspectives being indicative of truthful reviews.HYPOREFINE incorporates the "reviews that seem to be promoting a competitor" insight from LITERATURE-ONLY, while also capturing the emphasis on "lack of nuance" from HYPOGENIC.By doing so, HYPORE-FINE offers a more nuanced hypothesis that not only explains how deceptive reviews may manipulate reader emotions, but also provides specific examples to illustrate how balanced perspectives can contribute to truthful assessments.This combination of insights from literature and data allows HYPOREFINE to offer a more comprehensive and   1, the hypotheses generated from both literature and data performs the best on all methods for OOD datasets.</p>
<p>Generated hypotheses can be effectively transferred to a different model.To further check the generalization behavior of our generated hypotheses, we take the hypotheses from the bestperforming method with our literature+data approach and then use the other model to perform inference.Table 2 shows that the generated hypotheses from one model remain effective for the other model, the performance exhibits no significant change in most cases (&lt;3% in 10 out of 20 cases).Even with this performance drop, our methods still outperform the few-shot inference baseline by 3.76% and 3.66% on OOD and IND settings.This finding further demonstrates the robustness of our hypothesis generation method and hypothesisbased inference.A significant outlier case is for LLAMAGC OOD: when using LLAMA-70B-I-generated hypotheses for GPTGC (OOD) and ask GPT-4-MINI to perform hypothesis-based inference, the inference per-formance can degrade significantly.This can be due to innate deficits in the task setting, as LLMs tend to favor and better detect their own writing (Panickssery et al., 2024).</p>
<p>Human Evaluation</p>
<p>Generated hypotheses improve human decisionmaking in both AIGC Detection and Deception Detection.In AIGC Detection, the average human accuracy improves by 14.19% (58.86% → 73.05%) when we provide hypotheses as assistance.We perform a statistical t-test and obtain a p-value of 0.01, indicating that the improvement is significant.In Deception Detection, the introduction of hypotheses boosts human accuracy by 7.44% (57.14% → 64.58%), with a p-value of 0.04.</p>
<p>When hypotheses are present, participants would use them to assist decision-making for over 90% of the time.All three presented hypotheses are selected to be used with frequency greater than 30% (Table 4, Table 5 in the Appendix).For example, the most used hypothesis, with frequency of 44.55%, in AIGC detection is "Human-written texts tend to have a more conversational tone and colloquial language, while AI-generated texts tend to be more formal and lack idiomatic expressions."For both tasks, 100% of the participants find the hypotheses to be helpful, and over 40% find them to be "Very helpful" or "Extremely helpful".</p>
<p>Humans rate literature-based and data-driven hypotheses as distinct.We determine the novelty label based on majority vote from three human annotators.84% of the pairs are considered novel to each other for Deception Detection, and 80% are considered novel for AIGC Detection, demonstrating the complementarity between literature-based and data-driven approaches.</p>
<p>Case I: LITERATURE-ONLY and HYPOGENIC generate different hypotheses LITERATURE-ONLY: Deceptive reviews often contain a higher frequency of first-person singular pronouns, while truthful reviews may use these pronouns less frequently.HYPOGENIC: Reviews that reference the reviewer's previous experiences with the hotel brand or similar hotels are more likely to be truthful, while reviews that do not provide any context or comparison to past experiences are more likely to be deceptive.</p>
<p>Case II: LITERATURE-ONLY and HYPOGENIC generate similar hypotheses LITERATURE-ONLY: Truthful reviews often provide a balanced perspective, while deceptive reviews may seem overly promotional or biased towards a competitor.HYPOGENIC: Reviews that express a balanced perspective, mentioning both positive and negative aspects of the stay, are more likely to be truthful, whereas reviews that are overly positive or negative without nuance tend to be deceptive.HYPOREFINE: Reviews that present a balanced perspective by discussing both positive and negative aspects of the stay, particularly with specific examples (e.g., "The location was fantastic, but the air conditioning was broken"), are more likely to be truthful, while reviews that are excessively positive or negative without acknowledging any redeeming qualities (e.g., "This is the best hotel ever!" or "I will never stay here again!") tend to be more deceptive, as they may reflect an attempt to manipulate reader emotions rather than provide an honest assessment.</p>
<p>Related Work</p>
<p>Theory-driven hypothesis generation.Yang et al. (2024b) generates hypotheses from raw web corpus, but their method requires human annotated hypotheses from literature.Baek et al. (2024), Wang et al. (2024), and Ghafarollahi and Buehler (2024) use LLMs to create knowledge graph and generate hypotheses from existing literature.We implement our own literature-based generation because these papers either do not provide sufficient implementation details or require significant effort to adapt to new tasks.2024) uses LLMs to generate hypotheses and conducts comprehensive experiments to study human engagements with headlines.We choose HYPOGENIC as the backbone for data-driven hypothesis generation as their tasks are most similar to ours, and their approach to hypothesis updates integrates naturally into our refinement process.</p>
<p>Data</p>
<p>Automated scientific research with LLMs.</p>
<p>There is growing interest in developing LLMpowered methods and multi-agent frameworks to assist scientific research.Lu et al. (2024) designs an LLM agent to generate full research papers.Li et al. (2024) proposes a method to generate research ideas from existing literature and automatically implement and execute experiments.In contrast, our work focuses primarily on hypothesis generation, as we believe it is crucial to preserve human agency and oversight in the scientific research process.</p>
<p>To evaluate LLM generated hypotheses, Qi et al. ( 2023) examines whether they contain novel information not found in existing literature.Si et al. (2024) asks experts to rate the novelty of LLMproposed research ideas in the NLP domain.While these studies highlight LLMs' ability to generate novel hypotheses, they do not conduct human subject experiments to validate the effectiveness of hypotheses.To this end, we conduct the first human study to test the utility of LLM-generated hypotheses in supporting human decision-making.</p>
<p>Significant efforts have also been made to benchmark multi-agent frameworks on data analysis tasks (Majumder et al., 2024;Gu et al., 2024;Hu et al., 2024;Chen et al., 2024;Huang et al., 2024;Guo et al., 2024), literature processing and informa-</p>
<p>Hypotheses</p>
<p>Frequency of Selection</p>
<p>Hypothesis 1: AI-generated texts tend to use more elaborate and descriptive language, including adjectives and adverbs, to create a sense of atmosphere and immersion.Human-written texts, on the other hand, tend to be more concise and straightforward in their language use.</p>
<p>38.79%</p>
<p>Hypothesis 2: Human-written texts are more likely to contain errors or idiosyncrasies in grammar and punctuation, reflecting the natural imperfections of human writing, while AI-generated texts typically maintain a higher level of grammatical accuracy.</p>
<p>34.55%</p>
<p>Hypothesis 3: Human-written texts tend to have a more conversational tone and colloquial language, while AI-generated texts tend to be more formal and lack idiomatic expressions.</p>
<p>44.55%</p>
<p>No hypothesis selected 3.94%</p>
<p>Table 4: How often participants use hypotheses in AIGC Detection.We allow users to select multiple hypotheses for each instance they make prediction on, so the total frequency can exceed 100%.</p>
<p>tion retrieval tasks (Press et al., 2024;Ajith et al., 2024;Kang and Xiong, 2024;Zhang et al., 2024), and more general research tasks (Tian et al., 2024;Jansen et al., 2024).</p>
<p>Conclusion</p>
<p>We propose a novel approach that integrates literature and data to generate hypotheses, with extensive and systematic evaluations.Our method consistently outperforms all baselines, including existing literature-based and data-driven approaches.Furthermore, human evaluations reveal that our generated hypotheses also improve human decisionmaking in challenging tasks.</p>
<p>Limitations</p>
<p>Our automated evaluation uses two recent models on datasets across various domains, showing the effectiveness of our method across diverse settings.However, we did not further evaluate our hypotheses on some tasks that require representations beyond natural language, such as math problem solving and code generation.The literature corpus used for literature-based hypothesis generation is limited in terms of size and collection method.The collection is carried out by manually searching and collecting up to 10 papers on Semantic Scholar or Google Scholar.Though with the limited literature corpus we already show that our methods yield competent performance, a natural future direction is to enhance the literature component with automatic and scalable retrieval.</p>
<p>Similarly, we achieved satisfactory performance across different models and tasks with the initial set of hyperparameters.However, we did not perform an exhaustive hyperparameter search, which may have yielded further enhancements to the performance of our methods.This represents a limitation of our study that could be addressed in future work.</p>
<p>Our experiments with human subjects is a proof of concept.The number of participants in our human evaluation is relatively small.As a result, we do not believe that we have the statistical power to distinguish, for example, the difference between HYPOGENIC and HYPOREFINE.Although this is not the focus of our study, we encourage future work to conduct large-scale experiments in focused domains to validate the hypotheses generated through human-AI collaboration.</p>
<p>Last but not least, we manually chose three hypotheses through ablation-style study and subjective judgment for experiments with human subjects.We believe this process is the essence of human-AI collaboration in future scientific processes.It requires future exploration to identify the optimal collaboration regime.</p>
<p>A Prompts</p>
<p>All our prompts for LLMs are separated into system prompts and user prompts.System prompts contain role and tone information, followed by detailed descriptions of the task and the expected response format.User prompts contain useful information for hypothesis generation, refinement, or inference, including information from literature, instances from datasets, and previously generated hypotheses.Below are some examples of the prompts that we use for each task.</p>
<p>A.1 Deception Detection</p>
<p>System Prompt You're a professional hotel review analyst.Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.</p>
<p>Using the given examples, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided reviews.System Prompt You're a professional hotel review analyst.Given some key findings from a series of research papers, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.</p>
<p>Using the given relevant literatures, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided reviews.</p>
<p>System Prompt</p>
<p>You're a social scientist working on a project to identify deceptive hotel reviews.Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for predicting whether a review is truthful or deceptive.When refining the hypotheses, feel free to change the key information or topic of a hypothesis based on the provided prevailing patterns in data if you think it is necessary.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].</p>
<p>Refined hypotheses:</p>
<p>Example 4: Hypothesis Refinement Based on Data.</p>
<p>System Prompt You're a social scientist working on a project to identify deceptive hotel reviews.Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive.In other words, we want to know whether the review is written by a someone who actually lived in the hotel.</p>
<p>Using the given relevant literatures, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided reviews.</p>
<p>User Prompt</p>
<p>We have some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for predicting whether a review is truthful or deceptive.When refining the hypotheses, feel free to change the key information or topic of a hypothesis based on the provided key findings if you think it is necessary.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].Refined hypotheses:</p>
<p>Example 5: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt You are a professional deceptive detection agent and your job is to determine whether a hotel review is truthful or deceptive.In other words, we want to know whether the review is written by someone who had real experiences with the hotel.From past experiences, you learned some patterns.You need to determine whether each of the patterns holds for the current hotel review, and also predict whether the current hotel review is truthful or deceptive.Give an answer.The answer should be one word ( truthful or deceptive).</p>
<p>Give your final answer in the format of {Final answer: answer} User Prompt Our learned patterns: <generated_hypotheses> A hotel review is the following: <review> Given the pattern you learned above, give an answer of whether the hotel review above is deceptive or truthful.Think step by step.First step: Think about which pattern can be applied to the hotel review.Second step: Based on the pattern, is this hotel review deceptive or truthful?Final step: give your final answer in the format of {Final answer: answer} Example 6: Multiple-Hypothesis-Based Inference.</p>
<p>A.2 AI-Generated Content (AIGC) Detection</p>
<p>System Prompt You are a professional writer.You will be given a story prompt.Your task is to write a coherent and creative story based on the prompt.Ensure the story has a clear structure, engaging characters, and stays true to the prompt' s core idea.Keep the language clear and appropriate to the story's tone.</p>
<p>User Prompt</p>
<p>We have some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for predicting whether a piece of text is written by human or AI.When refining the hypotheses, feel free to change the key information or topic of a hypothesis based on the provided prevailing patterns in data if you think it is necessary.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].</p>
<p>Refined hypotheses:</p>
<p>Example 12: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt</p>
<p>You are an AI generated content detection agent and want to determine whether a piece of text is written by a human or generated by an AI.In other words, we want to know whether the text is written by a human or generated by AI.From past experiences, you learned some patterns.You need to determine whether each of the patterns holds for the current text, and also predict whether the current text is written by human or AI.</p>
<p>Give an answer. The answer should be one word (AI or HUMAN).</p>
<p>Give your final answer in the format of "Final answer: ANSWER" User Prompt Our learned patterns: <generated_hypotheses> New text: Here is a story: <story> Given the patterns you learned above, give an answer of whether the current text is written by human or AI.Think step by step.First step: Think about which pattern can be applied to the story.Second step: Based on the pattern, is this story written by human or AI?You must give your final answer in the format of " Final answer: ANSWER".</p>
<p>Example 13: Multiple-Hypothesis-Based Inference.</p>
<p>A.3 Mental Stress Detection</p>
<p>System Prompt You're a psychologist and social scientist studying people's stress and their online posts.given a set of reddit posts, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.System Prompt You're a psychologist and social scientist studying people's stress and their online posts.Given some key findings from a series of research papers, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.</p>
<p>Using the given relevant literatures, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided posts.We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for deciding people's stress status (has stress or no stress) based on reddit post.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].</p>
<p>Refined hypotheses:</p>
<p>Example 17: Hypothesis Refinement Based on Data.</p>
<p>System Prompt You're a psychologist and social scientist working on a project to identify whether a person has stress based on reddit posts.given a set of reddit posts, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.</p>
<p>Using the given relevant literatures, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided posts.</p>
<p>User Prompt</p>
<p>We have some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• We have some hypotheses need to be refined: ... hypotheses to be refined here ... Please refine these hypotheses to make them more specific and useful for deciding people's stress status (has stress or no stress) based on reddit post.Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.</p>
<p>[hypothesis].Refined hypotheses:</p>
<p>Example 18: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt You're a psychologist and social scientist working on a project to identify whether a person has stress based on reddit posts.From past experiences, you learned some patterns.You need to determine whether each of the patterns holds for the current reddit post, and also predict whether the poster of the reddit post has stress or not based on the content of the post.Give an answer.The answer should be "has stress" or "no stress".</p>
<p>Give your final answer in the format of {Final answer: answer} User Prompt Our learned patterns: <generated_hypotheses> A reddit post is the following: <post> Given the pattern you learned above, give an answer of whether the poster of the reddit post has stress or not based on the content of the post.</p>
<p>Think step by step.First step: Think about which pattern can be applied to the reddit post.Second step: Based on the pattern, does the poster of a reddit post has stress or not?Answer should be "has stress" or "no stress".</p>
<p>A.4 Persuasive Argument Prediction</p>
<p>System Prompt You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of observations of the format: Argument 1: [argument_1] Argument 2: [argument_2] Observation: The first/second argument uses more persuasive language.Based on the observations, please generate hypotheses that are useful for explaining why one argument uses more persuasive language than the other.</p>
<p>Proposed hypotheses:</p>
<p>Example 20: Data-Based Hypothesis Generation with HypoGeniC.</p>
<p>System Prompt</p>
<p>You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of literature of the format: Title: [title] Key Findings: [summary] Based on the literature, please generate hypotheses that are useful for explaining why one argument uses more persuasive language than the other.These hypotheses should identify patterns, phrases, wordings etc. that you can find in the literature.They should also be generalizable to new instances.Please propose <num_hypotheses> refined hypotheses and generate them in the format of 1. [hypothesis ], 2. [hypothesis], ... <num_hypotheses>.[ hypothesis].</p>
<p>User Prompt</p>
<p>Here are some key findings from a series of research papers that might be useful for generating hypotheses: ••• information from literature here ••• Please generate hypotheses that can help determine which argument uses more persuasive language.Please propose <num_hypotheses> possible hypotheses.</p>
<p>Generate them in the format of 1. [hypothesis], 2.</p>
<p>[hypothesis], ... <num_hypotheses>.[hypothesis].</p>
<p>Proposed hypotheses:</p>
<p>Example 21: Literature-Based Hypothesis Generation.</p>
<p>System Prompt You are a helpful assistant for summarizing key findings in research papers on a given topic.</p>
<p>User Prompt Summarize the following research paper, focusing ONLY on this question: What characterizes texts that use more persuasive language?In other words, how can one determine which one of two sentences uses more persuasive language?Focus on hypotheses of what characterizes texts that use more persuasive language, do not include technical details in the paper.... literature texts here ...</p>
<p>Example 22: Paper Summarization.</p>
<p>System Prompt You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of observations of the format: Argument 1: [argument_1] 2: [argument_2] Observation: The first/second argument uses more persuasive language.Based on the observations, please refine hypotheses provided to make them more useful for explaining why one argument uses more persuasive language than the other.These hypotheses should identify patterns, phrases, wordings etc. that occur across the provided examples.They should also be generalizable to new instances.Please propose <num_hypotheses> refined hypotheses and generate them in the format of 1. [</p>
<p>Refined hypotheses:</p>
<p>Example 23: Hypothesis Refinement Based on Data.</p>
<p>System Prompt</p>
<p>You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.What difference between the two arguments makes one more persuasive than the other?You will be given a set of literature of the format: ••• information from literature here ••• Based on the literature, please refine hypotheses provided to make them more useful for explaining why one argument uses more persuasive language than the other.These hypotheses should identify patterns, phrases, wordings etc. that you can find in the literature.</p>
<p>Refined hypotheses:</p>
<p>Example 24: Hypothesis Refinement Based on Literature.</p>
<p>System Prompt</p>
<p>You are an intelligent rhetorician and debater who masters persuasiveness in language.Given a pair of arguments, you are asked to determine which one of them uses more persuasive language.The two arguments are often on the same topic and are similar, so focus on their differences.From past experiences, you learned some patterns.Now, at each time, you should apply the learned patterns to a new pair of arguments and determine which one uses more persuasive language.The answer for the more persuasive language should be of the form "the _ argument" where _ is either first or second.Please give your final answer in the format of { Final answer: the _ argument uses more persuasive language} User Prompt Our learned patterns: <generated_hypotheses> Given the patterns you learned above, determine which of the following arguments uses more persuasive language: Argument 1: <first_argument> Argument 2: <second_argument> Think step by step.</p>
<p>Step 1: Think about which learned patterns can be applied to the arguments.</p>
<p>Step 2: Analyze the difference between "Argument 1" and "Argument 2".</p>
<p>Step 3: Based on the pattern, which argument uses more persuasive language?You MUST give your final answer in the following format: Final answer: the _ argument uses more persuasive language.</p>
<p>Example 25: Multiple-Hypothesis-Based Inference.</p>
<p>B Automated Experiments</p>
<p>Implementation Details  (Goffredo et al., 2023), Persuasion For Good (Wang et al., 2020), and Webis-Clickbait-17 (Potthast et al., 2018), while OOD dataset is from PT-Corpus (Da San Martino et al., 2019).</p>
<p>B.2 Specificity Boost</p>
<p>We further observed that sometimes the solely literature-based hypotheses generated by gpt-4omini are often too short and brief, making it harder to apply during inference.After removing redundancies of hypothesis banks, we unite two hypothesis banks to create a final bank H f inal with a balanced prioritization strategy.We first move the top 10 hypotheses from the HYPOGENIC or HYPOREFINE hypothesis bank to H f inal .If there is less than 10 hypotheses in the banks, we move all hypotheses to H f inal .Then we randomly choose hypotheses from the literature-based hypothesis bank until the size of f inal reaches 20.</p>
<p>B.4 Multiple-Hypothesis Inference Implementation</p>
<p>During multiple-hypothesis based inference, each time we feed a LLM with our final hypothesis bank of size 20 (see Appendix B.5) and an instance of our IND or OOD datasets with labels removed.The LLM is asked to generate an answer for the given instance using Chain-of-Thought prompting (Wei et al., 2022) that considers both the relevance of the hypotheses to the given instance and the utility of the hypothesis bank (see Appendix A for the exact prompts we used).For F1 scores, we report the macro-averaged F1 scores.</p>
<p>B.5 Technical Details of NotebookLM and HyperWrite</p>
<p>NotebookLM is an LLM-powered research assistance tool that generates source-grounding responses to user prompts.Specifically in our case, collected literature are uploaded in the Note-bookLM interface, followed by a hypothesis generation prompt asking to generate hypotheses based on given literature.Given its functionality and our usage, it is placed under the literature-based hypothesis generation category in our evaluations.</p>
<p>For HyperWrite, we use its Hypothesis Maker function, which is an AI-driven tool that generates hypotheses based on a given research question.Though there is no publicly available technical report for this tool, it generally leverages LLM's pretraining knowledge and literature information to produce hypotheses.</p>
<p>B.6 Hyperparameters</p>
<p>We use the same set of hyperparameters across all tasks, models, and methods.</p>
<p>During the training stage of HypoGeniC, the limit of the hypothesis bank size H is set to 20, and the size of training set is set to 200.In the initialization stage, we set num_init = 10.In the update stage, we use reward coefficient α = 0.5, w max = 10, k = 10, and generate 1 hypothesis per update.</p>
<p>In our HYPOREFINE method, the round of refinement max_refine is set to 6.</p>
<p>We use 5 random seeds for multiple-hypothesis inference: 11376, 8271, 39660, 543, 3.</p>
<p>Across all tasks and methods and for both GPT-4o-mini and Llama-3.1-70B-Instruct,we use temperature = 1 × 10 −5 and max_tokens = 4000.</p>
<p>B.7 Licensing Details</p>
<p>DECEPTIVE REVIEWS is released under CC BY-NC-SA 3.0, and PERSUASIVE PAIRS is released under CC BY-NC 4.0.The WRITINGPROMPTS dataset which we use to create the AIGC Detection datasets are under MIT License.The LLA-MAGC and GPTGC datasets will be released under the same licensing as this work, CC BY 4.0 License, should it be accepted.DREADDIT and FOUR-CITIES do not have licenses specified in their original papers, but are considered under CC BY 4.0 and CC BY-NC-SA 3.0 license respectively as they are ACL materials.</p>
<p>For the LLMs, GPT-4-MINI is a proprietary and not released under any open-source license, while LLAMA-70B-I is released under Llama 3.1 Community License Agreement.</p>
<p>Throughout our study, we find that we are in compliance with the licensing agreements of all the datasets and models used in this work.</p>
<p>B.8 Estimated Cost</p>
<p>For LLAMA-70B-I, we run all of our experiments with 4 NVIDIA A100s, and it takes on average 1.5 hours to run all of our hypothesis generation pipelines, including HYPOGENIC, HYPORE-FINE, LITERATURE∪HYPOGENIC , and LITER-ATURE∪HYPOREFINE .With GPT-4-MINI, the average cost for running the same pipelines is $0.6.</p>
<p>C Human Study Details C.1 Decision-making Utility Study Details</p>
<p>The instructions of the practical relevance study can be found in Figure 4 and Figure 6.For the interface, we present an example of the control group interface for Deception Detection in Figure 5, and examples of the experiment group interface in Figure 7.</p>
<p>The subjects of the control group are instructed to perform deception detection or AIGC detection tasks without any assistance from the hypotheses.Subjects in the experiment group are asked to first read the presented 3 hypotheses and then make their predictions on the given instance.They are then required to choose which ones, if any, of the hypotheses that were used in their prediction.At the end of the study, participants in the experiment group are also asked to give overall rating and assessment</p>
<p>Hypotheses</p>
<p>Frequency of Selection Hypothesis 1: Reviews present a balanced perspective by detailing both positive and negative experiences with specific examples (e.g., "the room was spacious and clean, but the noise from the street was disruptive at night") are more likely to be truthful, whereas reviews that express extreme sentiments without acknowledging any redeeming qualities (e.g., "everything was perfect" or "it was a total disaster") are more likely to be deceptive.</p>
<p>50.00%</p>
<p>Hypothesis 2: Reviews that mention specific dates of stay or unique circumstances surrounding the visit (e.g., "We stayed during the busy Memorial Day weekend and faced long lines") are more likely to be truthful, while reviews that use vague temporal references (e.g., "I stayed recently") without concrete details are more likely to be deceptive, as they often lack the specificity that suggests a real and engaged experience.</p>
<p>34.44%</p>
<p>Hypothesis 3: Reviews that provide detailed sensory descriptions of the hotel experience, such as the specific decor of the room, the quality of bedding, and the overall ambiance (e.g., "the room featured luxurious furnishings, highthread-count sheets, and soft lighting that created a relaxing atmosphere") are more likely to be truthful, while reviews that use vague or overly simplistic descriptors (e.g., "the hotel was nice and comfortable") are more likely to be deceptive.</p>
<p>46.39%</p>
<p>No hypothesis selected 7.50%</p>
<p>Table 5: How often humans use hypotheses in Deception Detection human study.We allow users to select multiple hypotheses for each instance they make prediction on, so the total frequency can exceed 100%.</p>
<p>of the helpfulness of the given hypotheses.There are five scales: "Not at all helpful", "Slightly helpful", "Moderately helpful", "Very helpful", and "Extremely helpful".</p>
<p>We choose top 3 hypotheses from the hypothesis bank generated using LITERA-TURE∪HYPOREFINE that cause the greatest drop in performance when removed from the hypotheses pool during multi-hypothesis inference.The chosen hypotheses for Deception Detection and AIGC Detection can be found in Table 4 and  Table 5.</p>
<p>We recruit 30 participants for the control group and 30 for the experimental group.For the control group, 4 people timed out, and 25 out of the remaining 26 participants passed attention checks.For the experimental group, 3 people timed out, and 22 out of the remaining 27 passed attention checks.We compute human accuracy based on responses from people who finished tasks in time and passed attention checks.The average time spent is around 25 minutes and participants are timed out by the system if they spend more than 60 minutes in the study, which can happen when they accidentally leave the study website tab open but forget to do the task.</p>
<p>C.2 Novelty and Nuance Study Details</p>
<p>For the Novelty and Nuance Study, we present the instructions for AIGC Detection in Figure 2. We showcase the interfaces for AIGC Detection in Figure 3.</p>
<p>For both Deception Detection and AIGC Detection, the two hypothesis banks compared are generated using LITERATURE-ONLY and HYPOGENIC respectively.</p>
<p>We recruit 10 participants each task and all particpants passed attention the check question.</p>
<p>C.3 IRB</p>
<p>We received IRB exempt (and will provide study number in the non-anonymous version of the paper).For both of the human studies, we present a detailed description of the study, incentives, risks and benefits, confidentiality, and contacts &amp; questions in our consent form.The study proceeds only if the participant agrees to give consent.</p>
<p>D Examples of Generated Hypotheses and Qualitative Analysis</p>
<p>We include examples of generated hypotheses using our LITERATURE∪HYPOREFINE approach and GPT-4-MINI, together with a brief qualitative analysis of its source in Table 8.We also showcase example hypotheses generated using NOTE-Table 7: Accuracy and F1 scores on the held-out IND datasets.Literature + data outperforms all other methods in 7 out of 10 configurations.For LLAMA-70B-I on GPTGC, LLAMAGC, and PERSUASIVE PAIRS, HYPOGENIC performs the best.This is likely due to that the literature in these tasks do not offer helpful information for the IND data, but they can still provide useful information for the tasks in general.As in Table 6, our approaches with literature + data performs the best in all configurations for the OOD datasets.</p>
<p>Dataset Generated Hypothesis</p>
<p>Literature Source/Novel DECEPTIVE REVIEWS Deceptive reviews often contain a higher frequency of first-person singular pronouns, while truthful reviews may use these pronouns less frequently.Li et al. (2014) The use of repetitive phrasing across multiple reviews is a strong indicator of deception, while truthful reviews are more likely to exhibit unique language and perspectives.</p>
<p>Maurya et al. (2022)</p>
<p>Reviews that provide specific accounts of the checkin and check-out processes, including exact times, the names of staff members involved, and descriptions of any unique features or services utilized (e.g., "I used the self-check-in kiosk at 3 PM"), are more likely to be truthful.Conversely, reviews that mention issues like long wait times or check-in problems without contextual details or specific examples (e.g., "the check-in took too long") are more likely to be deceptive.</p>
<p>Novel (from data)</p>
<p>GPTGC and LLAMAGC AI-generated content may struggle with maintaining coherence over longer passages, while human writing typically maintains clarity and focus.Posts that reflect on personal struggles with mental health or addiction (e.g., "I was a severe addict") are more likely to indicate that the poster has stress, while posts that discuss academic or professional experiences without emotional turmoil (e.g., "I've explained the aforementioned to people") are more likely to indicate that the poster does not have stress.</p>
<p>Novel (from data) PERSUASIVE PAIRS Persuasive texts that incorporate rhetorical devices, such as rhetorical questions and direct appeals, are more likely to engage the reader and compel them to consider the writer's viewpoint.</p>
<p>Wagemans (2023)</p>
<p>Texts that utilize strong, action-oriented verbs are generally more persuasive, as they convey confidence and urgency, compelling the audience to take action.</p>
<p>Novel (from data)</p>
<p>Arguments that include a clear and compelling call to action are more persuasive, as they provide the audience with a specific next step to take, reinforcing the urgency and importance of the message.</p>
<p>Novel (from data)</p>
<p>Table 8: Examples of generated hypotheses using our method accompanied by labels indicating their sources.For hypotheses from literature, we include the specific paper, while for hypotheses that are not explicitly suggested by our literature base, we set the label to "Novel (from data)".</p>
<p>Method</p>
<p>Figure 1 :
1
Figure 1: Illustration of how we combine literature-based and data-driven hypotheses.See algorithmic details in § 2.</p>
<p>n2</p>
<p>hypotheses from the literature-based hypothesis bank and adding the top n 2 hypotheses from the other hypothesis bank based on training accuracies.For detailed information of the implementation, please refer to Appendix B.3.</p>
<p>-driven hypothesis generation.Besides HYPOGENIC, we review additional works on discovering unseen patterns from data.Zhong et al. (2023) discovers patterns by analyzing difference between large corpora.Pham et al. (2024) makes discovery by generating and refining interpretable topics.Romera-Paredes et al. (2024) uncovers new solutions in open math problems by iteratively updating programs.Qiu et al. (2024) and Yang et al. (2024a) evaluate LLMs' ability in performing inductive reasoning in synthetic settings.Batista and Ross (</p>
<p>Using the given examples, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided reviews.Each hypothesis should contain a pair of the following: a.A hypothesis about what makes reviews more likely to be truthful b.The opposite hypothesis about what makes reviews more likely to be deceptive Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.User Prompt We have seen some hotel reviews: ••• more examples here •••</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes reviews more likely to be truthful b.The opposite hypothesis about what makes reviews more likely to be deceptive Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.</p>
<p>Using the given examples, please propose <num_hypotheses> possible hypothesis pairs.These hypotheses should identify specific patterns that occur across the provided posts.Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.User PromptWe have seen some reddit posts: ••• more examples here ••• Please generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.Propose <num_hypotheses> possible hypotheses.Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].Proposed hypotheses: Example 14: Data-Based Hypothesis Generation with HypoGeniC.</p>
<p>a psychologist and social scientist working on a project to identify whether a person has stress based on reddit posts.given a set of reddit posts, we want to generate hypotheses that are useful for deciding people's stress status (has stress or no stress) based on reddit post.Using the given examples, refine the hypothesis pairs provided.The desired hypotheses should identify specific patterns that occur across the provided posts.Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.User Prompt We have seen some reddit posts: ••• more examples here •••</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.</p>
<p>Final step: give your final answer in the format of {Final answer: answer} Example 19: Multiple-Hypothesis-Based Inference.</p>
<p>Figure 2 :
2
Figure 2: Instruction page for novelty check.</p>
<p>Figure 3 :
3
Figure 3: Annotation page for novelty check.</p>
<p>Figure</p>
<p>Figure Instruction page for prediction task without hypotheses.</p>
<p>Figure 5 :
5
Figure 5: Annotation page for prediction task without hypotheses.</p>
<p>Figure 6 :
6
Figure 6: Instruction page for prediction task with the guide of hypotheses.</p>
<p>Figure 7 :
7
Figure 7: Annotation page for prediction task with the guide of hypotheses.</p>
<p>reviews are more likely to be written in a style and tone that aligns with the reviewer's demographic information available on the platform, if any.<strong> Conversely, deceptive reviews might exhibit inconsistencies between the writing style and the reviewer's claimed demographic, signaling a potential fabrication.</strong>Truthful reviews are more likely to be posted at various times and days, reflecting the organic behavior of genuine guests.<strong>Conversely, deceptive reviews, particularly those orchestrated by paid posters, might be posted in clusters or at unusual times, indicating a coordinated effort.</strong>Truthful reviews are more likely to be written in a way that aligns with the overall sentiment expressed in the review's star rating.<strong>Conversely, deceptive reviews might show inconsistency between the sentiment expressed in the written content and the assigned star rating, indicating a potential attempt to manipulate perception.HYPERWRITE </strong>Relevant Images:<strong> Truthful reviews are more likely to include relevant images.Deceptive reviews less likely to include images.</strong>First-Person Pronouns:<strong> Truthful reviews use first-person pronouns (I, my).Deceptive reviews use third-person (one).</strong>Overly Formal Language:** Deceptive reviews use overly formal language.Truthful reviews use conversational tone.</p>
<p>Table 2 :
2
Cross-model inference performance.Performance on IND held-out datasets.Similarly with Table1, our hypothesis generation methods utilizing literature and data information are able to achieve the best accuracy and F1 scores in most cases on the held-out IND datasets (see Table 7 in the Appendix).For some cases, such as using Llama on the IND datasets for GPTGC, LLA-
explanatory hypothesis.
MAGC, and PERSUASIVE PAIRS, HYPOGENIC gets the top performance compared to other methods.This is not surprising, since HYPOGENIC generates hypotheses by looking at the IND data examples only.In contrast, our methods that take information from both literature and data may generate hypotheses that are more generally applicable but with slightly worse performance on the IND data, whereas in Table</p>
<p>Table 3 :
3
Examples of generated hypotheses from different methods.We show cases where LITERATURE-ONLY and HYPOGENIC generate different hypotheses or similar hypotheses, and how HYPOREFINE combines them in the case if they express unifiable ideas.</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes reviews more likely to be truthful b.The opposite hypothesis about what makes reviews more likely to be deceptive Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.Example 2: Literature-Based Hypothesis Generation.What is useful for one to decide whether a review is truthful or deceptive in real life?Focus on hypotheses of what kind of reviews tend to be deceptive, do not include technical details in the paper.... literature texts here ...
User PromptWe have some key findings from a series ofresearch papers that might be useful forgenerating the required <num_hypotheses>.hypotheses:••• information from literature here •••Please generate hypotheses that are useful forpredicting whether a review is truthful ordeceptive.When generating hypotheses, remember not tooveruse your own knowledge. Always refer to thekey findings from research papers provided.Directly cite passages in the key findings whengenerating a hypothesis.Propose <num_hypotheses> possible hypotheses.Remember to generate <num_hypotheses> hypotheses!Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].Proposed hypotheses:System PromptYou are a helpful assistant for summarizing keyfindings in research papers on a given topic.User PromptSummarize the following research paper, focusingONLY on this question: Example 3: Paper Summarization.</p>
<p>Example 11: Hypothesis Refinement Based on Data.
list multiple constructs so if there are manyexamples.things changing , pick one);v. usable (i.e., a human equipped with thisGenerate refined hypotheses in the format of 1. [insight could use it to predict if a new piece ofhypothesis], 2. [hypothesis], ... <num_hypotheses>.text is generated AI in a similar way)[hypothesis].The hypotheses should analyze what kind of text isProposed hypotheses:likely to be written by human or AI.Example 8: Data-Based Hypothesis Generation withUser PromptHypoGeniC.We have seen some texts: ••• more examples here •••We have some hypotheses need to be refined:... hypotheses to be refined here ...System PromptPlease refine these hypotheses to make them moreYou're a professional AI content detector.specific and useful for predicting whether a pieceGiven some key findings from a series of researchof text is written by human or AI.papers, we want to generate hypotheses that areWhen refining the hypotheses, feel free to changeuseful for detecting whether a piece of text isthe key information or topic of a hypothesis basedwritten by human or AI.on the provided prevailing patterns in data ifUser Prompt you think it is necessary.Your task is to identify what patterns or traits... story-writing prompt here ... Generate refined hypotheses in the format of 1. [show up more in AI generated texts, and what showsexample: hypothesis], 2. [hypothesis], ... <num_hypotheses>.up more in human written texts. Focus on the[ WP ] You 've been able to read minds since you [hypothesis].generalizable insight that can be applied in otherturned 7 . Mostly you watch people 's thoughts Refined hypotheses:contexts. Ignore things that are specific to thispassively and undetected but one day someone talksstory. Do not make references this story they mayback .\nnot be for others.Using the given relevant literatures, pleaseExample 7: AIGC Detection Dataset Generation.propose <num_hypotheses> possible hypothesis pairs.System PromptYou're a an AI generated content detection expert.These hypotheses should identify specific patternsSystem Prompt You are great at detecting what type of text isthat occur across the provided texts.You're a an AI generated content detection expert. generated by AI.You are great at detecting what type of text is Given a set of texts, we want to generateGenerate them in the format of 1. [hypothesis], 2.generated by AI. hypotheses that are useful for predicting whether[hypothesis], ... <num_hypotheses>. [hypothesis].Given a set of texts, we want to generate a piece of text is generated by AI. In other words,The hypotheses should analyze what kind of text ishypotheses that are useful for predicting whether we want to know whether the text is written by alikely to be written by human or AI.a piece of text is generated by AI. In other words, human or generated by AI.we want to know whether the text is written by aUser Prompthuman or generated by AI. Using the given relevant literatures, refine theWe have some key findings from a series ofhypothesis pairs provided.research papers that might be useful forYour task is to identify what patterns or traits The desired hypotheses should identify specificgenerating the required <num_hypotheses>show up more in AI generated texts, and what shows patterns that occur across the provided texthypotheses:up more in human written texts. Focus on the examples.••• information from literature here •••generalizable insight that can be applied in otherPlease generate hypotheses that are useful forcontexts. Ignore things that are specific to this Generate refined hypotheses in the format of 1. [predicting whether a piece of text is written ofstory. Do not make references this story they may hypothesis], 2. [hypothesis], ... <num_hypotheses>.human or AI.not be for others. [hypothesis].Propose <num_hypotheses> possible hypotheses.The hypotheses should analyze what kind of text isRemember to generate <num_hypotheses> hypotheses!Using the given examples, please propose likely to be written by human or AI.Generate them in the format of 1. [hypothesis], 2.<num_hypotheses> possible hypothesis pairs.[hypothesis], ... <num_hypotheses>. [hypothesis].When proposing hypothesis, look closely into theProposed hypotheses:given examples and identify specific patterns thatoccur across the provided text examples.Example 9: Literature-Based Hypothesis Generation.The hypotheses should be clear, easy to understand, and have specific details such that one can applythe hypotheses to predict whether a piece of textSystem Promptis written by human or AI.You are a helpful assistant for summarizing key findings in research papers on a given topic.Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>. [hypothesis].User Prompt Summarize the following research paper, focusingThe hypotheses should analyze what kind of text is likely to be written by human or AI.ONLY on this question: What is useful for one to detect whether some text is generated by AI? Focus on hypotheses of what kind of text tend to be generated by AI, do not include technical details in the paper. ... literature texts here ...User Prompt We have seen some texts: ... more examples here ... Please generate hypotheses that are useful for predicting predicting whether a piece of text is written by human or AI.Example 10: Paper Summarization.Propose <num_hypotheses> possible hypotheses. Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].System PromptWhen proposing hypothesis, look closely into theYou're a an AI generated content detection expert.given examples and identify specific patterns thatYou are great at detecting what type of text isoccur across the provided text examples.generated by AI.Given a set of texts, we want to generatePlease make sure that the hypotheses are:hypotheses that are useful for predicting whetheri. clear (i.e., precise , not too wordy , and easya piece of text is generated by AI. In other words,to understand);we want to know whether the text is written by aii. generalizable to novel situations (i.e., theyhuman or generated by AI.would make sense if applied to other AI generatedcontent detection experiments or other messagingUsing the given examples, refine the hypothesiscontexts);pairs provided.iii. empirically plausible (i.e., this is aThe desired hypotheses should identify specificdimension on which messages can vary on);patterns that occur across the provided textiv. unidimensional (i.e., avoid hypotheses that</p>
<p>Each hypothesis should contain a pair of the following: a.A hypothesis about what makes the post more likely to indicate that the poster has stress b.The opposite hypothesis about what makes the post more likely to indicate that the poster does not have stress Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].The hypotheses should analyze what kind of posts are likely to indicate stress or no stress.Example 15: Literature-Based Hypothesis Generation.
User PromptWe have some key findings from a series ofresearch papers that might be useful forgenerating the required <num_hypotheses>hypotheses:••• information from literature here •••Please generate hypotheses that are useful fordeciding people's stress status (has stress or nostress) based on reddit post.Propose <num_hypotheses> possible hypotheses.Remember to generate <num_hypotheses> hypotheses!Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].Proposed hypotheses:System PromptYou are a helpful assistant for summarizing keyfindings in research papers on a given topic.User PromptSummarize the following research paper, focusingONLY on this question: What is useful for one tojudge whether a reddit poster has stress based onone of their reddit post content?Focus on hypotheses of what kind of posts indicatestress, do not include technical details in thepaper.... literature texts here ...</p>
<p>These hypotheses should identify patterns, phrases, wordings etc. that occur across the provided examples.They should also be generalizable to new instances.Please propose <num_hypotheses> possible hypotheses and generate them in the format of 1. [ hypothesis], 2. [hypothesis], ... <num_hypotheses>.[hypothesis].
User PromptHere are the Observations:••• more examples here •••Please generate hypotheses that can help determinewhich argument uses more persuasive language.Please propose <num_hypotheses> possiblehypotheses.Generate them in the format of 1. [hypothesis], 2.[hypothesis], ... <num_hypotheses>. [hypothesis].</p>
<p>Each time we take the best-performing hypothesis h out of the original hypothesis bank H and check if there exists a hypothesis h new in H new such that redundancy is recorded in A for the pair h and h new , i.e., A h,hnew = 1 or A hnew,h = 1.If yes, h is moved out of the original bank H and skipped; if not, h is moved to H new with a rank determined by its training accuracy.
specificity booster is not applied to Llama-3.1-70B-Instruct because it can already generate reasonablyspecific hypotheses.B.3 Refinement and Union ImplementationRefinement is implemented as an extensionbased on the original HypoGeniC pipeline. Dur-ing the initialization stage, an LLM is instructedto generate an initial hypothesis bank H based ona set of initial examples S init and a series of gen-erated paper summaries. These initial hypothesesare then evaluated and re-ranked using the samereward function as in HypoGeniC. In the updatestage, once the size of the wrong examples bankW reaches w max , 1 new hypothesis is generated byfeeding both the wrong examples bank and papersummaries to the LLM. H is then updated with thenew hypothesis according to the reward, followingthe same procedure as HypoGeniC.Union and Redundancy Elimination is imple-mented by combining the hypothesis bank gener-ated using HYPOGENIC or HYPOREFINE and thebank generated by our literature-based hypothe-sis generation method. We first generate the twohypothesis banks separately using HYPOGENIC,HYPOREFINE, and LITERATURE-ONLY, followingthe procedures described above and in Section 3.Each hypothesis bank is then fed to a redundancychecker module. For a hypothesis bank of size20, the LLM-based redundancy checker checkseach pair of hypotheses and see if one entails theother, with results recorded as a 20 × 20 matrix Aof 1 (redundant) or 0 (not redundant). To createthe new no-redundancy hypothesis bank H new , wefirst rank the hypotheses based on their trainingaccuracy.To address this, we adda LLM-based specificity booster after the literature-based hypothesis generation that adds more con-crete illustrations and examples to each of the hy-potheses based solely on its pre-training knowledge.Specifically we apply the specificity booster onour Deception Detection, Mental Stress Detection,and Persuasive Argument Prediction tasks. The</p>
<p>Tang et al. (2023)AI-generated texts are more likely to follow conventional narrative structures, while human-written texts may experiment with form and structure.Novel (from data)DREADDITPosts that show erratic posting behavior or changes in tone (e.g., from positive to negative) are more likely to indicate stress, while consistent posting patterns with a stable tone are more likely to indicate no stress.
Wan and Tian (2024)Posts that exhibit avoidance behaviors (e.g., avoidingDoan et al. (2017)social situations or responsibilities) are more likely toindicate stress, while posts that demonstrate proactiveengagement with challenges are more likely to indicateno stress.</p>
<p>Table 9 :
9
Examples of generated hypotheses using NOTEBOOKLM and HYPERWRITE on DECEPTIVE REVIEWS that are invalid or irrelevant, leading to degraded inference performance for these methods.</p>
<p>AcknowledgmentsWe thank members of the Chicago Human+AI Lab for their helpful comments.We also thank the anonymous participants on Prolific for participating in our study.Methods
Lit-Search: A retrieval benchmark for scientific literature search. Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, Tianyu Gao, arXiv:2407.189402024Preprint</p>
<p>Using confidence bounds for exploitation-exploration trade-offs. Peter Auer, J. Mach. Learn. Res. 32003</p>
<p>ResearchAgent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024Preprint</p>
<p>Words that work: Using language to generate hypotheses. M Rafael, James Batista, Ross, 2024</p>
<p>ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, arXiv:2410.050802024Preprint</p>
<p>Fine-grained analysis of propaganda in news article. Giovanni Da, San Martino, Seunghak Yu, Alberto Barrón-Cedeño, Rostislav Petrov, Preslav Nakov, Proceedings of EMNLP-IJCNLP. EMNLP-IJCNLP2019</p>
<p>How do you #relax when you're #stressed? a content analysis and infodemiology study of stress-related tweets. Son Doan, Amanda Ritchart, Nicholas Perry, Juan D Chaparro, Mike Conway, 10.2196/publichealth.59392017JMIR Public Health and Surveillance3e35</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, arXiv:2407.217832024Preprint</p>
<p>Hierarchical neural story generation. Angela Fan, Mike Lewis, Yann Dauphin, Proceedings of ACL. ACL2018</p>
<p>Sci-Agents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, arXiv:2409.055562024Preprint</p>
<p>Argument-based detection and classification of fallacies in political debates. Pierpaolo Goffredo, Mariana Chaves, Serena Villata, Elena Cabrio, 10.18653/v1/2023.emnlp-main.684Proceedings of EMNLP. EMNLP2023</p>
<p>. Google, 2024NotebookLM</p>
<p>Deception detection. Psychology and law: An empirical perspective. 2005Pär Anders Granhag and Aldert Vrij</p>
<p>BLADE: Benchmarking language model agents for data-driven science. Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, M Tianmai, Lanyi Zhang, Mike A Zhu, Jeffrey Merrill, Tim Heer, Althoff, arXiv:2408.096672024Preprint</p>
<p>DS-Agent: Automated data science by empowering large language models with case-based reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, Proceedings of ICML. ICML2024</p>
<p>InfiAgent-DABench: Evaluating agents on data analysis tasks. Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li, Kun Kuang, Yang Yang, Hongxia Yang, Fei Wu, Proceedings of ICML, Proceedings of Machine Learning Research. ICML, Machine Learning Research2024</p>
<p>MLAgentBench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Proceedings of ICML. ICML2024</p>
<p>DISCOVERYWORLD: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, arXiv:2406.067692024Preprint</p>
<p>Hao Kang, Chenyan Xiong, arXiv:2406.10291ResearchArena: Benchmarking llms' ability to collect and organize information as research agents. 2024Preprint</p>
<p>Identifying manipulated offerings on review portals. Jiwei Li, Myle Ott, Claire Cardie, Proceedings of EMNLP. EMNLP2013</p>
<p>Towards a general rule for identifying deceptive opinion spam. Jiwei Li, Myle Ott, Claire Cardie, Eduard Hovy, 10.3115/v1/P14-1147Proceedings of ACL. ACL2014</p>
<p>MLR-Copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024Preprint</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, Proceedings of ACL. ACLOnline2020</p>
<p>The AI scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024Preprint</p>
<p>Effects of stress throughout the lifespan on the brain, behaviour and cognition. Sonia J Lupien, Bruce S Mcewen, Megan R Gunnar, Christine Heim, Nature Reviews Neuroscience. 2009</p>
<p>Discov-eryBench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, arXiv:2407.017252024Preprint</p>
<p>Deceptive opinion spam detection approaches: a literature survey. Sushil Kumar Maurya, Dinesh Singh, Ashish , 10.1007/s10489-022-03427-1Applied Intelligence. 532Kumar Maurya. 2022</p>
<p>GPT-4 technical report. OthersideAI. 2024. HyperWrite. 2023OpenAI</p>
<p>Negative deceptive opinion spam. Myle Ott, Claire Cardie, Jeffrey T Hancock, Proceedings of NAACL. NAACL2013</p>
<p>Llm evaluators recognize and favor their own generations. Arjun Panickssery, R Samuel, Shi Bowman, Feng, arXiv:2404.130762024Preprint</p>
<p>Measuring and benchmarking large language models' capabilities to generate persuasive language. Amalie Brogaard, Pauli , Isabelle Augenstein, Ira Assent, arXiv:2406.177532024Preprint</p>
<p>Topicgpt: A prompt-based topic modeling framework. Minh Chau, Alexander Pham, Simeng Hoyle, Mohit Sun, Iyyer, Proceedings of NAACL. NAACL2024</p>
<p>Crowdsourcing a large corpus of clickbait on Twitter. Martin Potthast, Tim Gollub, Kristof Komlossy, Sebastian Schuster, Matti Wiegmann, Erika Patricia, Garces Fernandez, Matthias Hagen, Benno Stein, Proceedings of COLING. COLING2018</p>
<p>CiteME: Can language models accurately cite scientific claims?. Ori Press, Andreas Hochlehnert, Ameya Prabhu, arXiv:2407.128612024PreprintVishaal Udandarao, Ofir Press, and Matthias Bethge</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren ; Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, 10.1038/s41586-023-06924-6Proceedings of ICLR. Bernardino Romera-Paredes, Mohammadamin Barekatain. ICLR. Bernardino Romera-Paredes, Mohammadamin Barekatain2024. 2024625Mathematical discoveries from program search with large language models</p>
<p>Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024Preprint</p>
<p>Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions. Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, Lillian Lee, 10.1145/2872427.2883081Proceedings of WWW. WWW2016</p>
<p>The science of detecting llm-generated texts. Ruixiang Tang, Yu-Neng Chuang, Xia Hu, arXiv:2303.072052023Preprint</p>
<p>Minyang Tian, Luyu Gao, Dylan Shizhuo, Xinan Zhang, Cunwei Chen, Xuefei Fan, Roland Guo, Pan Haas, Kittithat Ji, Yao Krongchon, Shengyan Li, Di Liu, Yutao Luo, Hao Ma, Kha Tong, Chenyu Trinh, Zihan Tian, Bohao Wang, Yanyu Wu, Shengzhu Xiong, Minhui Yin, Kilian Zhu, Yanxin Lieret, Genglin Lu, Yufeng Liu, Tianhua Du, Tao, arXiv:2407.13168SciCode: A research coding benchmark curated by scientists. Jamie CallanEliu Huerta, and Hao Peng2024Preprint</p>
<p>Dreaddit: A reddit dataset for stress analysis in social media. Elsbeth Turcan, Kathleen Mckeown, arXiv:1911.001332019Preprint</p>
<p>How to identify an argument type? on the hermeneutics of persuasive discourse. H M Jean, Wagemans, 10.1016/j.pragma.2022.11.015Journal of Pragmatics. 2032023</p>
<p>User stress detection using social media text: A novel machine learning approach. Xiangxuan Wan, Li Tian, 10.15837/ijccc.2024.5.6772International Journal of Computers Communications &amp; Control. 2024. 19</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Proceedings of ACL. ACL2024</p>
<p>Persuasion for good: Towards a personalized persuasive dialogue system for social good. Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, Zhou Yu, arXiv:1906.067252020Preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, Proceedings of NeurIPS. NeurIPS2022</p>
<p>A survey on llm-generated text detection: Necessity, methods, and future directions. Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F Wong, Lidia S Chao, arXiv:2310.147242024Preprint</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Proceedings of EACL. EACL2024a</p>
<p>Soujanya Poria, and Erik Cambria. 2024b. Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Proceedings of ACL. ACL</p>
<p>MASSW: A new dataset and benchmark tasks for ai-assisted scientific workflows. Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, Qiaozhu Mei, arXiv:2406.063572024Preprint</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, Proceedings of NeurIPS. NeurIPS2023</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
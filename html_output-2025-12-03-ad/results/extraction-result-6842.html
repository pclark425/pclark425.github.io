<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6842 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6842</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6842</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-262064851</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/27751/27545" target="_blank">Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) can be used as repositories of biological and chemical information to generate pharmacological lead compounds. However, for LLMs to focus on specific drug targets typically require experimentation with progressively more refined prompts. Results thus become dependent not just on what is known about the target, but also on what is known about the prompt-engineering. In this paper, we separate the prompt into domain-constraints that can be written in a standard logical form, and a simple text-based query. We investigate whether LLMs can be guided, not by refining prompts manually, but by refining the the logical component automatically, keeping the query unchanged. We describe an iterative procedure LMLF (“Language Models with Logical Feedback”) in which the constraints are progressively refined using a logical notion of generalisation. On any iteration, newly generated instances are verified against the constraint, providing “logical-feedback” for the next iteration’s refinement of the constraints. We evaluate LMLF using two well-known targets (inhibition of the Janus Kinase 2; and Dopamine Receptor D2); and two different LLMs (GPT-3 and PaLM). We show that LMLF, starting with the same logical constraints and query text, can guide both LLMs to generate potential leads. We find: (a) Binding affinities of LMLF-generated molecules are skewed towards higher binding affinities than those from existing baselines; LMLF results in generating molecules that are skewed towards higher binding affinities than without logical feedback; (c) Assessment by a computational chemist suggests that LMLF generated compounds may be novel inhibitors. These findings suggest that LLMs with logical feedback may provide a mechanism for generating new leads without requiring the domain-specialist to acquire sophisticated skills in prompt-engineering.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6842.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6842.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMLF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Models with Logical Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative neuro-symbolic procedure that guides large language models to generate candidate small molecules by combining a fixed natural-language query with a separately encoded, formally represented set of domain constraints that are iteratively refined using constraint-based labelling and numeric generalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003) and PaLM (text-bison-001) (used as sampling engines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>prompt-based decoder-only LLMs accessed via API; used within an iterative constraint-feedback pipeline (prompt-only, sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this work beyond the general description that these are foundation LLMs trained on very large, diverse text corpora; no chemical-specific training corpora for the LMLF wrapper are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Assembled textual prompt combining background knowledge (examples, labeled molecules, functions) and a formal logical constraint string; LLM samples SMILES strings (sampling temperature 0.7). Iterative loop: generated molecules are evaluated against constraints, labelled, background updated, numeric constraints generalised (tightened) and new prompts issued (up to k=10 iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (validity checked via RDKit)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Early-stage drug discovery / lead generation for protein targets (demonstrated for JAK2 and DRD2 inhibitors).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Three regimes tested: none (C_0^phi); target-agnostic constraints (C_0^+): molecular weight 200–700, synthetic accessibility score (SAS) < 5, LogP < 5; target-specific constraints (C_0^{++}): target docking/binding affinity threshold (affinity ≥ 7 for first 5 iterations then ≥ 8 for next 5). Numeric constraints are generalised iteratively (thresholds can be increased).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit (validity, molecular weight, LogP, SAS, Tanimoto similarity/substructure matching); GNINA 1.0 for docking-based binding-affinity estimates; Python used to assemble prompts (f-strings); API calls to text-davinci-003 (OpenAI) and text-bison-001 (PaLM).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEMBL-derived labelled sets for JAK2 (4100 molecules, ~3700 labelled active) and DRD2 (4070 molecules, ~3670 active) used as background labelled examples and to set initial constraint thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Estimated binding affinity (docking score) distributions (histograms); summary stats (mean, median, standard deviation, range); Mann-Whitney U nonparametric test to compare affinity distributions (p<0.05 as significance threshold); qualitative chemist assessment including exact substructure matches to patent-derived selective functional groups, Tanimoto similarity for novelty (threshold <0.75), and expert judgement of plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Quantitative: Table summaries (binding affinity docking scores). For JAK2: GPTLF (no constraints) mean 6.50 median 6.61 SD 1.07 range 4.21 to -7.55; GPTLF+ (target-agnostic) mean 7.53 median 7.57 SD 0.88; GPTLF++ (with target-specific) mean 7.74 median 7.71 SD 0.30 range 7.18 to -8.52. PaLMLF++ (PaLM with full constraints) for JAK2: mean 7.20 median 7.40 SD 0.58. For DRD2: GPTLF++ mean 7.66 median 7.71 SD 0.29 range 7.25 to -8.29; PaLMLF++ mean 7.60 median 7.55 SD 0.37 range 7.01 to -8.19. Comparisons: LMLF variants (with constraints) produced binding-affinity distributions skewed to higher affinities than unconstrained sampling and better than a reported VAE-GNN baseline (VAE-GNN reported mean 6.53 median 6.95 for JAK2). Statistical testing: authors report using Mann-Whitney U (p<0.05) to identify significant differences but individual p-values are not reported. Qualitative: chemist review of 20 molecules (10 GPTLF++ + 10 PaLMLF++ per target) found JAK2: 3/20 (15%) generated molecules had at least one JAK2-selective functional group; DRD2: 8/20 (40%) had at least one DRD2-selective functional group. Of 11 molecules flagged as containing selective groups, 10 had Tanimoto similarity <0.75 to existing inhibitors. Of 11 molecules judged possibly effective inhibitors, 7 were from GPT-3 sampling and 4 from PaLM; of possibly novel inhibitors, 7 from GPT-3 and 3 from PaLM. No wet-lab experimental syntheses or biological assays were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reported limitations include: dependence on prompt construction if not formalised (motivation for LMLF), subjectivity and reproducibility issues with free-text prompts, the implemented generalisation mechanism is restricted to numeric inequalities (other constraint types are unchanged), evaluation relies solely on in silico docking (GNINA) and chemist judgement rather than wet-lab validation, comparisons to baselines are confounded because LLMs may have access to more information than baselines (making fair comparison difficult), and some differences between LLMs (GPT-3 vs PaLM) were not statistically significant. The paper also discusses the broader conceptual limitation that natural-language flexibility can hide representation mismatches between human and machine concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6842.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6842.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (OpenAI text-davinci-003, used via API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large foundation language model used as the sampling engine in the LMLF pipeline to generate SMILES strings conditioned on assembled prompts containing background examples and formal constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3 family) accessed via OpenAI API</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>prompt-only decoder LLM (used as a sampling/generation engine within LMLF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Described in the paper only as a foundation LLM trained on very large, diverse corpora of text; no chemical-specific training details provided in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompted generation of SMILES strings using assembled prompt including labeled examples and logical constraints; sampling with temperature 0.7; iterative LMLF loop for constraint-based labelling and numeric constraint generalisation (up to 10 iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Lead generation for protein inhibitors (JAK2 and DRD2 demonstrated).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Same LMLF constraints described above: none / target-agnostic (MW 200–700, SAS<5, LogP<5) / target-specific (docking affinity thresholds 7→8 during iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit for property calculation and validity; GNINA for docking; API calls to text-davinci-003; Python prompt assembly.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEMBL-derived labelled JAK2 and DRD2 sets used as background examples and for constraints (JAK2: 4100 molecules with ~3700 actives; DRD2: 4070 with ~3670 actives).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Binding affinity (docking) distributions (mean/median/SD/range), Mann-Whitney U test, chemist qualitative assessment (substructure matches, Tanimoto similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-3 (as GPTLF++) produced the strongest quantitative results in this study: for JAK2 GPTLF++ mean binding score 7.74 median 7.71 SD 0.30 (range 7.18 to -8.52); for DRD2 GPTLF++ mean 7.66 median 7.71 SD 0.29. Experts identified 7 of 11 potentially effective/novel inhibitors as originating from GPT-3 sampling in the submitted sample.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Same as LMLF-level limitations; paper notes that GPT-3 performed better than PaLM in these experiments but differences were not statistically significant and expert assessments only weakly favored GPT-3. The paper does not provide model parameter counts or detailed training corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6842.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6842.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM (text-bison-001)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM (Google text-bison-001, used via API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large foundation language model (PaLM variant) used as an alternative sampling engine in the LMLF pipeline to generate SMILES conditioned on prompts and logical constraints, enabling a comparison of different LLMs within the same logical-feedback framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-bison-001 (PaLM family) accessed via API</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>prompt-only decoder LLM (used as a sampling/generation engine within LMLF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified beyond general statements that it is a foundation model trained on very large diverse text corpora; no chemical-specific training details provided in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompted generation of SMILES using the same assembled prompt & LMLF iterative procedure; sampling temperature 0.7; up to 10 iterations with numeric constraint schedule (7 then 8).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Early-stage drug lead generation (JAK2 and DRD2 inhibitors in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>None / target-agnostic (MW 200–700, SAS<5, LogP<5) / target-specific docking affinity thresholds (≥7 then ≥8) as implemented in LMLF.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit, GNINA 1.0, Python prompt assembly, API calls to PaLM text-bison-001.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEMBL-derived JAK2 and DRD2 labelled datasets used as background; same counts as for GPT-3 experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Binding affinity (docking) distributions (mean/median/SD/range), Mann-Whitney U test, qualitative chemist assessment (functional group matches, Tanimoto similarity <0.75 for novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>PaLMLF++ achieved improved binding-affinity distributions relative to unconstrained sampling; for JAK2 PaLMLF++ mean 7.20 median 7.40 SD 0.58 range 7.03 to -8.36; for DRD2 PaLMLF++ mean 7.60 median 7.55 SD 0.37. In the chemist review, 4 of 11 molecules judged possibly effective/novel inhibitors came from PaLM sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper reports that PaLM under this protocol performed worse than GPT-3 on some metrics but differences were not statistically significant; same limitations regarding reliance on in silico docking and lack of wet-lab testing apply. No model size or detailed training corpora reported in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Adaptive language model training for molecular design <em>(Rating: 2)</em></li>
                <li>Generative Deep Learning for Targeted Compound Design <em>(Rating: 2)</em></li>
                <li>Regression Transformer: Concurrent Conditional Generation and Regression by Blending Numerical and Textual Tokens <em>(Rating: 2)</em></li>
                <li>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules <em>(Rating: 1)</em></li>
                <li>Junction tree variational autoencoder for molecular graph generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6842",
    "paper_id": "paper-262064851",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "LMLF",
            "name_full": "Language Models with Logical Feedback",
            "brief_description": "An iterative neuro-symbolic procedure that guides large language models to generate candidate small molecules by combining a fixed natural-language query with a separately encoded, formally represented set of domain constraints that are iteratively refined using constraint-based labelling and numeric generalisation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003) and PaLM (text-bison-001) (used as sampling engines)",
            "model_type": "prompt-based decoder-only LLMs accessed via API; used within an iterative constraint-feedback pipeline (prompt-only, sampling)",
            "model_size": "",
            "training_data_description": "Not specified in this work beyond the general description that these are foundation LLMs trained on very large, diverse text corpora; no chemical-specific training corpora for the LMLF wrapper are reported.",
            "generation_method": "Assembled textual prompt combining background knowledge (examples, labeled molecules, functions) and a formal logical constraint string; LLM samples SMILES strings (sampling temperature 0.7). Iterative loop: generated molecules are evaluated against constraints, labelled, background updated, numeric constraints generalised (tightened) and new prompts issued (up to k=10 iterations).",
            "chemical_representation": "SMILES strings (validity checked via RDKit)",
            "target_application": "Early-stage drug discovery / lead generation for protein targets (demonstrated for JAK2 and DRD2 inhibitors).",
            "constraints_used": "Three regimes tested: none (C_0^phi); target-agnostic constraints (C_0^+): molecular weight 200–700, synthetic accessibility score (SAS) &lt; 5, LogP &lt; 5; target-specific constraints (C_0^{++}): target docking/binding affinity threshold (affinity ≥ 7 for first 5 iterations then ≥ 8 for next 5). Numeric constraints are generalised iteratively (thresholds can be increased).",
            "integration_with_external_tools": "RDKit (validity, molecular weight, LogP, SAS, Tanimoto similarity/substructure matching); GNINA 1.0 for docking-based binding-affinity estimates; Python used to assemble prompts (f-strings); API calls to text-davinci-003 (OpenAI) and text-bison-001 (PaLM).",
            "dataset_used": "ChEMBL-derived labelled sets for JAK2 (4100 molecules, ~3700 labelled active) and DRD2 (4070 molecules, ~3670 active) used as background labelled examples and to set initial constraint thresholds.",
            "evaluation_metrics": "Estimated binding affinity (docking score) distributions (histograms); summary stats (mean, median, standard deviation, range); Mann-Whitney U nonparametric test to compare affinity distributions (p&lt;0.05 as significance threshold); qualitative chemist assessment including exact substructure matches to patent-derived selective functional groups, Tanimoto similarity for novelty (threshold &lt;0.75), and expert judgement of plausibility.",
            "reported_results": "Quantitative: Table summaries (binding affinity docking scores). For JAK2: GPTLF (no constraints) mean 6.50 median 6.61 SD 1.07 range 4.21 to -7.55; GPTLF+ (target-agnostic) mean 7.53 median 7.57 SD 0.88; GPTLF++ (with target-specific) mean 7.74 median 7.71 SD 0.30 range 7.18 to -8.52. PaLMLF++ (PaLM with full constraints) for JAK2: mean 7.20 median 7.40 SD 0.58. For DRD2: GPTLF++ mean 7.66 median 7.71 SD 0.29 range 7.25 to -8.29; PaLMLF++ mean 7.60 median 7.55 SD 0.37 range 7.01 to -8.19. Comparisons: LMLF variants (with constraints) produced binding-affinity distributions skewed to higher affinities than unconstrained sampling and better than a reported VAE-GNN baseline (VAE-GNN reported mean 6.53 median 6.95 for JAK2). Statistical testing: authors report using Mann-Whitney U (p&lt;0.05) to identify significant differences but individual p-values are not reported. Qualitative: chemist review of 20 molecules (10 GPTLF++ + 10 PaLMLF++ per target) found JAK2: 3/20 (15%) generated molecules had at least one JAK2-selective functional group; DRD2: 8/20 (40%) had at least one DRD2-selective functional group. Of 11 molecules flagged as containing selective groups, 10 had Tanimoto similarity &lt;0.75 to existing inhibitors. Of 11 molecules judged possibly effective inhibitors, 7 were from GPT-3 sampling and 4 from PaLM; of possibly novel inhibitors, 7 from GPT-3 and 3 from PaLM. No wet-lab experimental syntheses or biological assays were reported.",
            "experimental_validation": false,
            "challenges_or_limitations": "Reported limitations include: dependence on prompt construction if not formalised (motivation for LMLF), subjectivity and reproducibility issues with free-text prompts, the implemented generalisation mechanism is restricted to numeric inequalities (other constraint types are unchanged), evaluation relies solely on in silico docking (GNINA) and chemist judgement rather than wet-lab validation, comparisons to baselines are confounded because LLMs may have access to more information than baselines (making fair comparison difficult), and some differences between LLMs (GPT-3 vs PaLM) were not statistically significant. The paper also discusses the broader conceptual limitation that natural-language flexibility can hide representation mismatches between human and machine concepts.",
            "uuid": "e6842.0",
            "source_info": {
                "paper_title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-3 (text-davinci-003)",
            "name_full": "GPT-3 (OpenAI text-davinci-003, used via API)",
            "brief_description": "A large foundation language model used as the sampling engine in the LMLF pipeline to generate SMILES strings conditioned on assembled prompts containing background examples and formal constraints.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3 family) accessed via OpenAI API",
            "model_type": "prompt-only decoder LLM (used as a sampling/generation engine within LMLF)",
            "model_size": "",
            "training_data_description": "Described in the paper only as a foundation LLM trained on very large, diverse corpora of text; no chemical-specific training details provided in this work.",
            "generation_method": "Prompted generation of SMILES strings using assembled prompt including labeled examples and logical constraints; sampling with temperature 0.7; iterative LMLF loop for constraint-based labelling and numeric constraint generalisation (up to 10 iterations).",
            "chemical_representation": "SMILES strings",
            "target_application": "Lead generation for protein inhibitors (JAK2 and DRD2 demonstrated).",
            "constraints_used": "Same LMLF constraints described above: none / target-agnostic (MW 200–700, SAS&lt;5, LogP&lt;5) / target-specific (docking affinity thresholds 7→8 during iterations).",
            "integration_with_external_tools": "RDKit for property calculation and validity; GNINA for docking; API calls to text-davinci-003; Python prompt assembly.",
            "dataset_used": "ChEMBL-derived labelled JAK2 and DRD2 sets used as background examples and for constraints (JAK2: 4100 molecules with ~3700 actives; DRD2: 4070 with ~3670 actives).",
            "evaluation_metrics": "Binding affinity (docking) distributions (mean/median/SD/range), Mann-Whitney U test, chemist qualitative assessment (substructure matches, Tanimoto similarity).",
            "reported_results": "GPT-3 (as GPTLF++) produced the strongest quantitative results in this study: for JAK2 GPTLF++ mean binding score 7.74 median 7.71 SD 0.30 (range 7.18 to -8.52); for DRD2 GPTLF++ mean 7.66 median 7.71 SD 0.29. Experts identified 7 of 11 potentially effective/novel inhibitors as originating from GPT-3 sampling in the submitted sample.",
            "experimental_validation": false,
            "challenges_or_limitations": "Same as LMLF-level limitations; paper notes that GPT-3 performed better than PaLM in these experiments but differences were not statistically significant and expert assessments only weakly favored GPT-3. The paper does not provide model parameter counts or detailed training corpora.",
            "uuid": "e6842.1",
            "source_info": {
                "paper_title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "PaLM (text-bison-001)",
            "name_full": "PaLM (Google text-bison-001, used via API)",
            "brief_description": "A large foundation language model (PaLM variant) used as an alternative sampling engine in the LMLF pipeline to generate SMILES conditioned on prompts and logical constraints, enabling a comparison of different LLMs within the same logical-feedback framework.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-bison-001 (PaLM family) accessed via API",
            "model_type": "prompt-only decoder LLM (used as a sampling/generation engine within LMLF)",
            "model_size": "",
            "training_data_description": "Not specified beyond general statements that it is a foundation model trained on very large diverse text corpora; no chemical-specific training details provided in this work.",
            "generation_method": "Prompted generation of SMILES using the same assembled prompt & LMLF iterative procedure; sampling temperature 0.7; up to 10 iterations with numeric constraint schedule (7 then 8).",
            "chemical_representation": "SMILES strings",
            "target_application": "Early-stage drug lead generation (JAK2 and DRD2 inhibitors in experiments).",
            "constraints_used": "None / target-agnostic (MW 200–700, SAS&lt;5, LogP&lt;5) / target-specific docking affinity thresholds (≥7 then ≥8) as implemented in LMLF.",
            "integration_with_external_tools": "RDKit, GNINA 1.0, Python prompt assembly, API calls to PaLM text-bison-001.",
            "dataset_used": "ChEMBL-derived JAK2 and DRD2 labelled datasets used as background; same counts as for GPT-3 experiments.",
            "evaluation_metrics": "Binding affinity (docking) distributions (mean/median/SD/range), Mann-Whitney U test, qualitative chemist assessment (functional group matches, Tanimoto similarity &lt;0.75 for novelty).",
            "reported_results": "PaLMLF++ achieved improved binding-affinity distributions relative to unconstrained sampling; for JAK2 PaLMLF++ mean 7.20 median 7.40 SD 0.58 range 7.03 to -8.36; for DRD2 PaLMLF++ mean 7.60 median 7.55 SD 0.37. In the chemist review, 4 of 11 molecules judged possibly effective/novel inhibitors came from PaLM sampling.",
            "experimental_validation": false,
            "challenges_or_limitations": "Paper reports that PaLM under this protocol performed worse than GPT-3 on some metrics but differences were not statistically significant; same limitations regarding reliance on in silico docking and lack of wet-lab testing apply. No model size or detailed training corpora reported in this work.",
            "uuid": "e6842.2",
            "source_info": {
                "paper_title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Adaptive language model training for molecular design",
            "rating": 2,
            "sanitized_title": "adaptive_language_model_training_for_molecular_design"
        },
        {
            "paper_title": "Generative Deep Learning for Targeted Compound Design",
            "rating": 2,
            "sanitized_title": "generative_deep_learning_for_targeted_compound_design"
        },
        {
            "paper_title": "Regression Transformer: Concurrent Conditional Generation and Regression by Blending Numerical and Textual Tokens",
            "rating": 2,
            "sanitized_title": "regression_transformer_concurrent_conditional_generation_and_regression_by_blending_numerical_and_textual_tokens"
        },
        {
            "paper_title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
            "rating": 1,
            "sanitized_title": "automatic_chemical_design_using_a_datadriven_continuous_representation_of_molecules"
        },
        {
            "paper_title": "Junction tree variational autoencoder for molecular graph generation",
            "rating": 1,
            "sanitized_title": "junction_tree_variational_autoencoder_for_molecular_graph_generation"
        }
    ],
    "cost": 0.0130135,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Generating Novel Leads for Drug Discovery Using LLMs with Logical Feedback</p>
<p>Shreyas Bhat Brahmavar 
Department of Electrical and Electronics Engineering
BITS Pilani
Goa CampusIndia</p>
<p>Department of Biological Sciences
BITS Pilani
Goa CampusIndia</p>
<p>Ashwin Srinivasan ashwin@goa.bits-pilani.ac.in 
Department of Computer Science
BITS Pilani
Goa CampusIndia</p>
<p>Tirtharaj Dash tirtharaj@goa.bits-pilani.ac.in 
Department of Pediatrics
University of California
San DiegoUSA</p>
<p>Sowmya Ramaswamy Krishnan sowmya.rk1@tcs.com 
TCS Innovation Labs (Life Sciences Division)
India</p>
<p>Lovekesh Vig lovekesh.vig@tcs.com 
TCS Research
India</p>
<p>Arijit Roy roy.arijit3@tcs.com 
TCS Innovation Labs (Life Sciences Division)
India</p>
<p>Raviprasad Aduri aduri@goa.bits-pilani.ac.in 
Department of Biological Sciences
BITS Pilani
Goa CampusIndia</p>
<p>Generating Novel Leads for Drug Discovery Using LLMs with Logical Feedback
376A1CF780BD76ECEBDC7A89E2C078D2
Large Language Models (LLMs) can be used as repositories of biological and chemical information to generate pharmacological lead compounds.However, for LLMs to focus on specific drug targets typically require experimentation with progressively more refined prompts.Results thus become dependent not just on what is known about the target, but also on what is known about the prompt-engineering.In this paper, we separate the prompt into domain-constraints that can be written in a standard logical form, and a simple textbased query.We investigate whether LLMs can be guided, not by refining prompts manually, but by refining the the logical component automatically, keeping the query unchanged.We describe an iterative procedure LMLF ("Language Models with Logical Feedback") in which the constraints are progressively refined using a logical notion of generalisation.On any iteration, newly generated instances are verified against the constraint, providing "logical-feedback" for the next iteration's refinement of the constraints.We evaluate LMLF using two well-known targets (inhibition of the Janus Kinase 2; and Dopamine Receptor D2); and two different LLMs (GPT-3 and PaLM).We show that LMLF, starting with the same logical constraints and query text, can guide both LLMs to generate potential leads.We find: (a) Binding affinities of LMLF-generated molecules are skewed towards higher binding affinities than those from existing baselines; (b) LMLF results in generating molecules that are skewed towards higher binding affinities than without logical feedback; (c) Assessment by a computational chemist suggests that LMLF generated compounds may be novel inhibitors.These findings suggest that LLMs with logical feedback may provide a mechanism for generating new leads without requiring the domain-specialist to acquire sophisticated skills in prompt-engineering.</p>
<p>Introduction</p>
<p>In 1982, Edward Feigenbaum identified a difficulty in the transfer of human-knowledge to a machine, now famous as "the Feigenbaum bottleneck" (Feigenbaum et al. 1977).In a curious twist of fate, we now appear confronted by a "reverse bottleneck".Machine knowledge, such as those contained in large foundation models, is at least as difficult for humans to access as it was to represent human knowledge in a machine-understandable form.Surprisingly, this reverse bottleneck also appears to have been first identified in 1982.The problem of 'The Human Window' (Kopec 1982;Michie 1982) refers to the difficulties faced by humans when interacting with a complex computing system, due to a mismatch in representations of the human and machine.Modern large language models (LLMs) would appear to have resolved this difficulty through their impressive facility to use natural language as a mechanism of communicating with humans (Brown et al. 2020;Narang and Chowdhery 2022).In fact, the true difficulties concerning the Human Window arise from the need for a conceptual interface, not simply a linguistic interface.That is, the mismatch has to be addressed at a concept-level. 1 The intense interest in the methods and practices of 'prompt engineering' as an approach to extract useful information from LLMs could be seen as evidence of the deeper, conceptual mismatch that exists between LLMs and human representations.In this paper, we are concerned with an immediate practical manifestation of this, namely in the apparent need to be a sophisticated prompt-engineer to be able to use the capability of an LLM best.</p>
<p>Our specific interest is in the generation of novel leads for early-stage drug-design.Here, the human is typically a computational-or synthetic-chemist, who often uses knowledge that can be expressed in a logical form.For example, this may be in the form of generic constraints on values like molecular weight, hydrophobicity, synthetic accessibility score, etc.; and specific constraints like estimated binding energy to the target site, size of the binding site, presence of any specific anchors and so on.The usual route to provide this as input to an LLM would be through prompts, which combine what the chemist knows, and what the chemist needs from the LLM.However, the free-text interface prompts make it difficult to settle on a single form for this input, and the process becomes one of experimentation with phrasings or text, and orderings of textual se-quences.Results are, therefore, dependent not just on what is known bio-chemically, but on the content and sequence of text provided.This makes the experiments highly subjective and difficult to reproduce.</p>
<p>In this paper, we attempt to reduce this subjectivity while retaining one very important feature of LLMs, namely, the ability to adapt quickly to new probability distributions and to sample effectively from them.Our approach is to separate the content of a prompt into two parts: a domainspecific component, and a domain-independent component.In this paper, by 'domain-specific' here, we mean the drugdesign aspects that allow the LLM to update its probability distribution.The domain-independent part will be a simple query enabling the LLM to generate instances from its (updated) distribution.Further, we will require that the domain-specific component be encoded in a standardised form that can be refined automatically and that the domainindependent form is simple enough to be independent of the LLM used.This is done to ensure clarity and repeatability of the experimental protocol.Here, the standardised form we employ is formal logic, and LLM updates are done through a procedure LMLF that employs what we call 'logical feedback'.</p>
<p>The rest of the paper is organised as follows.In the Background section, we summarise the need for automated discovery of new leads in early-stage drug-design, and a description of some constraints on lead-generation.Although we expect readers to be familiar with LLMs, at least in their use through interactive interfaces like ChatGPT, we nevertheless include a short description of LLMs as implementations of probabilistic generative models.In the section following, we describe the LMLF procedure as a method of using LLMs in conjunction with satisfaction of logical constraints acting as feedback to update its probability distribution.In the Empirical Evaluation section, we describe our evaluation of LMLF empirically using two benchmark drug-design targets and two well-known LLMs.Finally, we present some related works, followed by conclusions.</p>
<p>Background Lead Discovery in Early-Stage Drug Design</p>
<p>Drugs are small molecules that usually attach themselves to parts of a larger molecule (like a protein), called a 'target'.This attachment takes place at a location known as the 'target site'.The attachment occurs mainly by the usual physical electrostatic mechanisms, and the process is known as binding.Binding results in structural and functional change of the target molecule.Usually, this change means stopping some activity, and the small molecule is said to inhibit the target.Leads are small molecules that could potentially bind to a target molecule.</p>
<p>Artificial Intelligence is currently revolutionizing drug development (Williams et al. 2015), especially in various steps of early-stage drug design (see Fig. 1(a) showing the use of a Robot Scientist) as virtual screening, identifying qualitative and quantitative structure-activity relations (SARs) and so on.The broader picture is of a semi-automated scientific discovery pipeline involving feedback from from computa-tional chemists, synthetic chemists, and biologists and manufacturers, using results from simulation, synthesis protocols, and biological testing (see Fig. 1(b) and (Zenil et al. 2023) for the broader context of closed-loop scientific discovery).In this paper, we restrict ourselves to domain-specialists in the form of computational chemists with knowledge of chemical synthesis.We envisage 2 kinds of interaction between such this specialist and the computational engine: (A) Provision of chemical knowledge.This could be of a general nature on drug-likeness, or specific to the target or output of the computational engine; (B) Asking chemical queries, usually about possible new structures, or specific aspects of existing structures.</p>
<p>If the computational engine is a large language model (LLM), then all specialists in Fig. 1(b) should be able to interact using natural language.But, as pointed out in Sec., the very flexibility allowed by natural language instruction poses difficulties to the construction a pipeline capable of repeatable, standardised experiments.We will be looking at a mechanism that requires: the specialist's knowledge (A) to be provided in a standardised form that can then be refined automatically; and the chemical queries (B) that are to be posed as simple text concerning the generation of new molecular structures.In effect, (A) and its subsequent refinements are used to alter automatically the conditioning information (used here in a probabilistic sense) provided to the LLM; and (B) is used to sample from the resulting conditional probability distribution over small molecules.</p>
<p>Language Models as Probabilistic Machines</p>
<p>A language model is a probabilistic model of natural language that learns a probability distribution over sequences of tokens called sentences.Let W denote one such sentence with N tokens, W = (w 1 , . . ., w N ), where, N is arbitrary.A language model estimates the probability of observing the sentence W , denoted by the joint probability P (W ).In practice, however, P (W ) is approximated using n-gram models (Jelinek 1980;Katz 1987) or Neural language models or NLMs (Bengio, Ducharme, and Vincent 2000).These models use Markov's assumption that the probability of a word</p>
<p>The Thirty-Eighth AAAI Conference on Artificial Intelligence  depends only on the previous n &lt; N words.That is,
P (W ) = N i=1 P (w i |w i−(n−1) , . . . , w i−1 )(1)
Neural language models or NLM (Bengio, Ducharme, and Vincent 2000) are probabilistic language models based on (deep) neural networks that can handle the problems associated with n-gram models, such as handling long-range dependency, context understanding, handling noise and ambiguity, learning complex relationships by learning a distributed representation of text tokens.An NLM approximates each term on the right-hand side of Eq. ( 1) using a neural network.Large Language Models (LLMs) such as GPTs (Radford et al. 2019) and PALM (Narang and Chowdhery 2022) are large and complex neural language models that use the transformer architecture (Vaswani et al. 2017) to learn from vast and diverse corpora of text data.</p>
<p>Prompt-based LLMs, such as ChatGPT and BARD are LLMs that can learn with human-feedback (Ouyang et al. 2022).A prompt is an input sequence written by a human in a natural language, which serves as the starting point that sets the context of the LLM to generate the next (highly) probable text sequence in an auto-regressive manner.</p>
<p>Using Language Models with Logical Feedback (LMLF)</p>
<p>We differentiate the information provided by a human in a prompt for an LLM into 2 parts: contextual information, which can be encoded in a formal language, and a query, which is in a natural language.It is further helpful for us to distinguish the former into background knowledge B consisting of definitions, functions, procedures and factual statements, and C, consisting of domain-constraints.For the specific task of lead-discovery that we are interested in:</p>
<p>• B will include: example molecules; facts about the molecules obtained from computation by a generalpurpose molecular modelling package (computing, for example, bulk properties like molecular weight, synthesis accessibility scores etc.); facts about the molecule obtained from computation by special-purpose molecular modelling package (computing for example, binding affinity to the target site).We also consider part of B any standard mathematical and arithmetic procedures used in the constraints C. • C will typically be a conjunction of desirable properties of leads, like molecular weight between 200 and 700; logP below 5; SA score below 5; and binding affinity is −8 or less, etc.</p>
<p>In developing LMLF, we are inspired by the MIMIC algorithm (De Bonet, Isbell, and Viola 1996), which uses an iterative procedure for model-assisted sampling.MIMIC assumes that we are looking to generate instances with low values of an objective function F .On any iteration i, MIMIC has access to a sample of instances; and a model M i that can be used both for discrimination and for sampling (generation).True F -values are computed for the sample of known instances, and M i is revised to M i+1 that can discriminate accurately between instances with F -value below and above some threshold θ i .That is, M discriminates between F (x) ≤ θ (labelled "1") and F (x) &gt; θ (labelled "0").M i+1 is then used to generate new data instances, the threshold θ i is lowered to θ i+1 , and the process is repeated (say k times).</p>
<p>We first recast MIMIC in terms of background knowledge and constraints.Let B denote the function F , the thresholds θ i , and standard arithmetic definitions of ≤, &gt;.Let C be the conjunction
C 1 ∧ C 2 ∧ • • • C n , where C i = (F (x) ≤ θ i ).
We are able to abstract two general principles about the algorithm:</p>
<p>• On any iteration i, feedback to M i is provided by instances labelled based on whether (B ∧ C i ) is true (label 1) or false (label 0).We call this the "constraint-based labelling" property.
• Since θ i+1 ≤ θ i , if F (x) ≤ θ i+1 then F (x) ≤ θ i . That is, (B ∧ C i+1 ) |= (B ∧ C i ).
We call this the "constraintgeneralisation" property.</p>
<p>We now devise a general iterative procedure with these two properties to alter the conditioning sequence for an LLM for discrimination and generation.The steps are shown in Procedure 1.For reasons of space, we do not provide procedures for the auxiliary functions.An idealised worked example below is intended to help clarify their intended behaviour.</p>
<p>Procedure 1: Incremental sampling from an LLM's conditional distribution using iterative constraint-based labelling and constraint generalisation.Input: L: an LLM; B 0 : background knowledge, which contains a sample D 0 of labelled instances; C 0 : a logical formula representing constraints; Q: a query; k: an upperbound on the number of iterations; and n: an upper-bound on the number of samples Output: a set of instances 1: j := 1 2: while (j ≤ k and D j−1 ̸ = ∅) do 3:
P j := AssembleP rompt(B j−1 , C j−1 , Q) 4:
E j := Sample(n, L, P j ) 5:</p>
<p>D j := {(e, l) : e ∈ E j and l = Satisf ies(e, B j−1 , C j−1 )} 6:
B j := U pdateBack(B j−1 , D j ) 7: C j := GeneraliseConstraint(B j , C j−1 )
8:</p>
<p>j := j + 1 9: end while 10: return D j Example 1.We want to generate molecules to inhibit the target protein, Janus Kinase 2 (JAK2).We work through one complete iteration of LMLF, albeit without actual details.For ease of explanation, we will be using a logic-based syntax to describe background knowledge and constraints, and restricted natural language (Kuhn 2014) to describe the query (the actual implementation used in experiments does not use either of these representations).</p>
<p>1.The background knowledge B 0 contains facts about the target, some known molecules, and labels (say, 1 and 0 inhibitors).</p>
<p>• These are conjunctions of facts.For example: target(jak2) ∧ mol(m 1 ) ∧ label(m 1 , 1) ∧ mol(m 2 ) ∧ label(m2, 0) ∧ . . . .• Additional facts may describe the properties of the molecules; for example, molwt(m 1 , 245.5) ∧ logp(m 1 , 4.0) ∧ . . . .B 0 also contains information for use by the constraints.</p>
<p>• This may be facts.For example, a threshold for binding affinity, like aff thresh(8.0);or a range of allowed values for molecular weight, like molwt ([200, 700]) and so on.• B 0 also contains functions for performing computation.For example, the definition for computing affinity scores: aff inity(M ol, T arget) = Score) if (GNINA(M ol, T arget) = Score), . . . . 2. C 0 describes a set of constraints required to be satisfied.Some examples are:
• mol(m 10 ) ∧ molwt(m 10 , w) ∧ molwt([x, y]) ∧ (x ≤ w) ∧ (w ≤ y) • mol(m 10 )∧aff inity(m 10 , a)∧aff thresh(z)∧(a &gt; z).
Here a, w, x, y, z are all variables.3. The query Q we use is a simple textual one: Generate valid SMILES string for n molecules that are labelled "1" and are not found in any known database.4. Using the information in B 0 and C 0 , and Q, AssembleP rompt returns text string that includes strings for labelled molecules (like 1 m 1 , 0 m 2 and so on) and the query Q.The LLM uses this as a prompt to sample n new molecules.5.Each new molecule e is tested against the constraints.</p>
<p>The function Satisf ies returns 1 if e satisfies B 0 ∧ C 0 and 0 otherwise.6.The background knowledge is updated to B 1 with the newly labelled instances.7. The constraint C 0 is generalised to C 1 by GeneraliseConstraint. Generalisation is restricted to numeric constraints with inequalities (all other constraints are left unchanged).A constraint of the form x ≤ θ (where θ is some numeric value) is generalised to x ≤ θ ′ where θ ′ &lt; θ.Similarly x &gt; θ is changed to x &gt; θ ′ .It is assumed that the background knowledge contains a function to compute θ ′ given θ (for example, increment and decrement functions that add or subtract pre-specified amounts to θ).</p>
<p>The implementation used for experiments in the paper has aspects related to efficiency and book-keeping, which introduce unnecessary detail but retains the essential feature of iteration over constraint-based labelling and constraintgeneralisation.In the following, we will call the implementation PyLMLF.For our purpose, PyLMLF will be used as a tool for investigating the use of LMLF to generate new leads for early-stage drug-design.The code for PyLMLF can be found at: https://github.com/Shreyas-Bhat/LMLF.</p>
<p>Empirical Evaluation Aims</p>
<p>We use PyLMLF as a tool to investigate the following conjecture:</p>
<p>• The use of LLMs with logical feedback generates better lead molecules for early-stage drug-design than LLMs without logical feedback.</p>
<p>We will make the following design choices to conduct the experiments: (a) We consider two classic drug-design targets and two well-known LLMs; (b) We will use two methods of assessing the results: quantitatively, using the distribution of binding affinities of generated molecules; and qualitatively, using assessments by a computational chemist.</p>
<p>Materials</p>
<p>Biological Targets and LLMs We conduct our evaluations on JAK2, with 4100 molecules provided with labels (3700 active) and DRD2 (4070 molecules with labels, of which 3670 are active).These datasets were collected from ChEMBL (Gaulton et al. to some generic bulk properties of the small molecules.Specifically, we use the generic constraints used in (Dash et al. 2021) for one of the datasets used here (JAK2).These constraints encode the following requirements of potential leads: (i) Molecular weight must be between 200 and 700;</p>
<p>(ii) Synthetic accessibility score (SAS) must be below 5;</p>
<p>(iii) LogP value must be below 5; and (iv) Binding affinity must be above 7.For experiments here, we limit constraints in (c) to estimates of binding affinity to the target site obtained from the approach described in (McNutt et al. 2021).For a small molecule m, the constraint encodes the condition: aff inity(m, x) ∧ x ≥ θ.We ensure updates to the background knowledge performed by the PyLMLF implementation of the LMLF procedure ensure that θ values</p>
<p>The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)</p>
<p>either stay the same or increase on each iteration.This ensures the constraint-generalisation condition is not violated.</p>
<p>In the description of the experimental method below, we will denote the trivial case of not having any constraints as C ϕ 0 ; the case with only target-agnostic constraints as C + 0 ; and the case with both target-agnostic and target-specific constraints as C ++ 0 .Query A query in our experiments is a restricted (unambiguous) English statement: Generate a molecule that is valid and not in any known database.The query, in combination with the available background knowledge and constraints, is assembled to construct the prompt for the LLMs.</p>
<p>Algorithms and Machines</p>
<p>All the experiments are conducted using a Linux (Ubuntu) based workstation with 64GB of main memory and 16-core Intel Xeon 3.10GHz processors.All the implementations are in Python3, with API calls to the respective model engines for GPT-3.0 and PaLM.We use RDKit (version: 2022.9.5) for computing molecular properties and GNINA 1.0 for computing docking scores (binding affinities) of molecules.</p>
<p>Method</p>
<p>Our method is straightforward: 1.For each biological target T ∈ {JAK2, DRD2}:</p>
<p>(a) For each LLM L ∈ {GP T 3, P aLM } i.Let M T,i be the set of molecules returned by PyLMLF provided with LLM L, background B 0 and constraints C ϕ 0 for target T ; ii.Let M + T,i be the set of molecules returned by PyLMLF provided with LLM L, background B 0 and constraints C + 0 for target T ; and iii.Let M ++ T,i be the set of molecules returned by PyLMLF provided with LLM L, background B 0 and constraints C ++ 0 for target T .iv. Compare the sets M , M + 0 and M ++ quantitatively using the distribution of estimated target-specific binding affinities v. Compare the sets M , M + and M ++ qualitatively using assessments by domain-specialists The following additional details are relevant:</p>
<p>• We make API calls to text-davinci-003 for GPT-3.0 and text-bison-001 for PaLM.For both LLMs, temperature is set to 0.7.• The upper-bound on the number of iterations (k in Procedure 1) is 10.• In our constraint C, we use a threshold of 7 on binding affinity for the first 5 iterations and 8 for the next 5 iterations.• Quantitative comparison of performance is done as follows.For any set of molecules, we obtain a histogram of binding affinities to act as an estimate of the probability distribution of affinities.Comparison of any 2 sets of molecules M 1 and M 2 is done by using the nonparametric Mann-Whitney U test on these estimated distributions.If p &lt; 0.05, then we reject the null hypothesis that M 1 and M 2 are from the same distribution.If the null hypothesis is rejected, and the median values of binding affinities from M 1 are higher than those from M 2 , then we will say the performance of the procedure generating M 1 is better (respectively equal or worse).</p>
<p>Results</p>
<p>In the following, we use GPTLF to denote PyLMLF provided with GPT3, background B 0 , and constraints C ϕ 0 ; GPTLF + to denote PyLMLF provided with GPT3, background B 0 and constraints C + 0 ; and GPTLF ++ to denote PyLMLF with GPT3, background B 0 , and constraints C ++ 0 .Similarly for PaLMLF, PaLMLF + and PaLMLF ++ .Summaries of quantitative results are in Table 1.Histograms showing the distribution of estimated binding affinities are in Figure 2. It is evident from both the tabulation and diagrams that for both targets and both LLMs: (a) LLMs are capable of generating molecules without any constraints (C ϕ 0 ); (b) When the LLMs are provided target-agnostic constraints (C + 0 ), performance is better than without constraints (case (a) above); and (c) when the LLMs are provided with both target-agnostic and targetspecific constraints (C ++ 0 ), performance is better than with just generic constraints (case (b) above).These results provide quantitative support to the conjecture that logical feedback is beneficial in using LLMs to generate potential leads.</p>
<p>Qualitative Assessment by Chemists.The chemists were provided with 20 molecules each of potential JAK2 and DRD2 inhibitors (10 of each were generated by GPTLF ++ and PaLMLF ++ , although the chemists were not told which LLM was involved).A summary of the assessment made by the chemists is reproduced below.Additional details are available at: https://doi.org/10.1101/2023.09.14.557698.</p>
<p>Efficacy.(a) JAK2.A set of 13 JAK2-selective functional groups was identified based on patent literature and used for exact substructure match against the generated molecules.From the search, 3 generated molecules (15%) were found to have at least one JAK2-selective functional group.4,6-Diamino pyrimidine, morpholine, [1,2,4]-triazolo[1,5-a]pyridine and 3-amino pyrazole groups were predominantly observed to enhance JAK2 selectivity in the generated small molecules (b) DRD2.A set of 11 functional groups were identified from patented DRD2 inhibitors.Since these functional groups have been proven to enhance selectivity toward DRD2, an exact substructure match was performed with the generated small molecules using RDKit, to identify the presence of these selectivity features from patent literature.From the search, 8 generated molecules (40%) were found to have at least one DRD2-selective functional group indicating the model's capability to optimize molecular features to capture selectivity, based on the docking score observed.Dimethyl piperazine and chlorobenzene was observed to be the predominant DRD2-selective group among the Novelty.10 of the 11 molecules that contain JAK2 or DRD2 selective functional groups of interest also had less than 0.75 Tanimoto similarity to the existing JAK2 or DRD2 inhibitors, respectively.Therefore, it can be interpreted  that although some fractions of the molecules generated have less similarity to existing inhibitors, the presence of selective functional groups indicates their potential to act as novel and selective inhibitors for the target protein of interest.Overall.It appears that the model has learned to generate more inhibitors with better similarity to existing JAK2 inhibitors (50%) compared to DRD2 (15%).But this difference is compensated by the fact that 40% of generated DRD2 inhibitors have highly selective functional groups, while only 15% of generated JAK2 inhibitors have selective groups.Hence, the model exhibits a balance in generation of similar and novel molecules depending on the nature and diversity of the training dataset used for the target protein of interest.</p>
<p>We now report on some additional comparisons that do not directly impinge on the experimental conjecture in Sec., but are nevertheless of interest to practitioners.First, the quantitative and qualitative assessments provide us with an opportunity to compare the capabilities of GPT3 and PaLM under controlled conditions.It is evident from the tabulation in Table 1 that the LMLF using GPT appears to be better than using PaLM.However, the expert assessment provides a slightly different story.Of the 11 molecules identified to be possibly effective JAK2 or DRD2 inhibitors, 7 were obtained using GPT3 and 4 was obtained using PaLM.Of the possible inhibitors that were also identified as being possibly novel, 7 was from GPT3, and 3 was from PaLM.These numbers are indicative of some benefit in using GPT.However, the differences are not statistically significant.</p>
<p>Secondly, we are able to perform a comparison of the use of LLMs against baselines provided by: (a) known inhibitors of JAK2 and DRD2; and (b) results reported on the same dataset(s) on novel lead-generation.Results have been reported on the JAK2 dataset most recently in (Dash et al. 2021).This uses a combination of 2 variational autoencoders (VAEs) for generating molecules, and a graph-based neural network (GNN) that acts as a discriminator, which found to be perform better than the previous reports (in (Krishnan et al. 2021)) using reinforcement learning in combination</p>
<p>Related Work</p>
<p>Over the last few years, deep generative models have been used successfully in generating novel compounds for specific biological targets and with desired molecular properties.A comprehensive review of some of the techniques can be found in (Sousa et al. 2021).Among these techniques, deep sequence models such as variational autoencoders (VAE), deep structure-based models such as graph neural networks (GNNs) are shown to be very effective.Some of these techniques are paired with each other and also with reinforcement learning to allow these models to be biased towards generating models with desired properties (Jin, Barzilay, and Kang and Kim 2023;Born and Manica 2022).LLMs with some (human) feedback are also adopted for molecular generation (Blanchard et al. 2023;Fang et al. 2023).Our present work is in a similar vein, albeit with additional domain-knowledge and desired constraints used to progressively guide the LLM's sampling engine to draw molecules from a more restricted joint distribution, allowing more novelty and diversity in molecule generation against a specific biological target.</p>
<p>Conclusion</p>
<p>In this paper, we have proposed a simple iterative procedure called LMLF that progressively alters the conditioning string provided to a large language model (LLM).The alterations are the result of testing answers generated by the LLM against domain-specific constraints represented in a formal, rather than natural language.We investigate the performance of LMLF in the area of lead-discovery, and find that the logical constraints, enforced using our proposed feedback mechanism, provide much more effective conditioning information to the LLM.As a result, we are able to use the internal knowledge contained within the LLMs much more effectively to generate potentially novel inhibitors for specific biological targets.We present quantitative results supporting this claim on two separate targets, using two different LLMs; and qualitative results in the form of preliminary assessments by computational chemists.</p>
<p>Large 'foundation' models that have been constructed with vast amounts of data can be seen as storehouses of factual and hypothesised knowledge that can be of great value in tackling complex tasks in areas like drug-design.But how are human problem-solvers-like chemists and biologists-to draw on such knowledge?A long recognised concern of mismatch between human-and machine-representations of knowledge suggests that this is not an easy task.On the surface, it would seem that this will not be a concern when using LLMs, given their capability to interact with humans in a natural language.However, this may not follow for at least two reasons.First, the issue is of a mismatch in representation (what concepts are being used), and not of communication language.For example, the machine may be using a concept for which there is no simple description in a natural language.We are not addressing this problem here.Secondly, the flexibility of natural language introduces ambiguity and imprecision.Thus, human-concepts can be conveyed to a machine in many ways, not all of which may map to the same machine-concepts (however mismatched).The latter issue poses difficulties in using LLMs in a controlled manner.The position adopted in this paper is that for certain kinds of scientific problems-like the lead-discovery tasks here-it is possible to side-step the second difficulty and still use LLMs effectively.Specifically, we are concerned with tasks for which we are able to formulate task-specific requirements in a sufficiently formal language, which can in turn be used in conjunction with an LLM.We suggest that this simple neuro-symbolic approach could provide an effective basis for using LLMs in closed-loop scientific discovery of the kind envisaged in (Zenil et al. 2023).</p>
<p>Figure 1 :
1
Figure 1: (a) Early Stage Drug Design; and (b) Computational drug discovery with specialists-in-the-loop.The dotted arrows represent 2-way 'interactions'.</p>
<p>2012), which are selected based on their IC 50 values and docking scores with active JAK2 and DRD2 proteins less than −7.0.For all our experiments, we use 2 LLMs: GPT-3.0(Brown et al. 2020) and PaLM(Narang and Chowdhery 2022).Background knowledge There are 3 categories of background knowledge: (a) Factual statements: referring to what is already known about drug targets, for example, some subset of experimentally known inhibitors for JAK2 and DRD2; (b) Functional definitions to compute bulk molecular properties; and and (c) Procedures needed to assemble the prompt for sampling.For (b), we use the definitions available within the molecular modelling packages RDKit (Landrum et al. 2013) and GNINA 1.0 (McNutt et al. 2021) to compute the validity of SMILES string, molecular weight, synthetic accessibility score (SAS), LogP, binding affinity.For (c), we use Python's f-string syntax to incorporate the molecules and their class-labels (inhibitor or non-inhibitor) represented in (a).Constraints We conduct experiments with 3 categories of constraints: (a) No constraints; (b) Target-agnostic constraints; and (c) Target-specific constraints.Of these, (a) is self-explanatory.Constraints in category (b) refer only</p>
<p>Figure 2 :
2
Figure 2: Plot showing the distribution of binding affinities for molecules generated by the LLMs and the two drug targets.</p>
<p>Table 1 :
1
Statistics of binding affinities of LLM-generated molecules against the drug targets: (Left) JAK2, (Right) DRD2.
MethodMean Median S.D.RangeMethodMean Median S.D.RangeGPTLF6.506.611.07 4.21 -7.55GPTLF6.486.510.75 5.33 -7.42GPTLF +7.537.570.88 4.83 -8.29GPTLF +7.327.210.32 6.45 -8.02GPTLF ++7.747.710.30 7.18 -8.52GPTLF ++7.667.710.29 7.25 -8.29PaLMLF6.096.010.77 3.86 -7.55PaLMLF6.056.131.05 4.33 -7.49PaLMLF +6.126.240.69 4.65 -8.11PaLMLF +6.226.230.48 5.67 -8.24PaLMLF ++7.207.400.58 7.03 -8.36PaLMLF ++7.607.550.37 7.01 -8.19</p>
<p>Table 2 :
2
(Dash et al. 2021)ing affinities for the LMLF-generated molecules (Left: JAK2, Right: DRD2), in comparison to those of known inhibitors, and the molecules generated by VAE-GNN in(Dash et al. 2021).'-' denotes 'data not available'.
MethodMean Median S.D.RangeMethodMean Median S.D.RangeKnown Inhibitors7.267.420.64 4.19 -8.34Known Inhibitors6.866.970.63 4.21 -8.31VAE-GNN6.536.951.18 2.10 -8.18VAE-GNN----GPTLF ++7.747.710.30 7.18 -8.52GPTLF ++7.667.710.29 7.25 -8.29PaLMLF ++7.207.400.58 7.03 -8.36PaLMLF ++7.607.550.37 7.01 -8.19with an MLP. Table 2 compares and shows both LLMs per-form better than the VAE-GNN combination. To some ex-tent, this is unsurprising for 2 primary reasons: (1) the LLMshave access to substantially more information than the VAE-GNN model, and (2) the VAE-GNN model does not have ac-cess to the constraint(s) on the binding affinity of molecules.It is relatively straightforward to develop a variant of LMLFfor other kinds generative models like the VAE-GNN model.This would address (2), but it is unclear how the gap in (1)can be bridged.
 Kopec distinguishes between 'surface-level' and 'structurallevel'  mismatches. The use of natural language addresses the surface-level mismatch, but it does not necessarily alleviate the mismatch between the concepts employed.The Thirty-Eighth AAAI Conference on Artificial Intelligence 
The Thirty-Eighth AAAI Conference on Artificial Intelligence 
AcknowledgementsRA and AS acknowledge the funding from DBT-NNP (Grant No. BT/PR40236/BTIS/137/51/2022).The authors thank Shreyas V. for his assistance in setting up PaLM API.AS is a TCS-affiliated Professor and the Head of APPCAIR, BITS Pilani; a Visiting Professor at the Centre for Health Informatics, Macquarie University, Sydney; and a Visiting Professorial Fellow at the School of CSE, UNSW, Sydney.This work was initiated when TD was at BITS Pilani, Goa Campus.LV is a Professor of Practice at the Department of CS &amp; IS, BITS Pilani, Goa Campus.
A neural probabilistic language model. Advances in neural information processing systems. Y Bengio, R Ducharme, P Vincent, 200013</p>
<p>Adaptive language model training for molecular design. A E Blanchard, D Bhowmik, Z Fox, J Gounley, J Glaser, B S Akpa, S Irle, Journal of Cheminformatics. 1512023</p>
<p>Regression Transformer: Concurrent Conditional Generation and Regression by Blending Numerical and Textual Tokens. J Born, M Manica, CoRR, abs/2202.013382022</p>
<p>Language Models are Few-Shot Learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, CoRR, abs/2005.141652020</p>
<p>Do Large Language Models Understand Chemistry? A Conversation with ChatGPT. N D Cao, T Kipf, C M Castro Nascimento, A S Pimentel, arXiv:1805.1197336926868Journal of Chemical Information and Modeling. 6362022. 2023MolGAN: An implicit generative model for small molecular graphs</p>
<p>A review of some techniques for inclusion of domainknowledge into deep neural networks. T Dash, S Chitlangia, A Ahuja, A Srinivasan, Scientific Reports. 12110402022</p>
<p>Inclusion of domain-knowledge into gnns using mode-directed inverse entailment. T Dash, A Srinivasan, A Baskar, Machine Learning. 2022</p>
<p>Using domain-knowledge to assist lead discovery in early-stage drug design. T Dash, A Srinivasan, L Vig, A Roy, International Conference on Inductive Logic Programming. Springer2021</p>
<p>MIMIC: Finding optima by estimating probability densities. J De Bonet, C Isbell, P Viola, Advances in neural information processing systems. 19969</p>
<p>The art of artificial intelligence: Themes and case studies of knowledge engineering. Y Fang, N Zhang, Z Chen, X Fan, H Chen, E A Feigenbaum, CoRR, abs/2301.11259Computer Science Department, School of Humanities and Sciences. 40D12023. 1977. 2012Stanford University. Gaulton,Nucleic acids research</p>
<p>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. R Gómez-Bombarelli, J N Wei, D Duvenaud, J M Hernández-Lobato, B Sánchez-Lengeling, 29532027ACS Central Science. 422018</p>
<p>Interpolated estimation of Markov source parameters from sparse data. F Jelinek, Proc. Workshop on Pattern Recognition in Practice. Workshop on Pattern Recognition in Practice1980. 1980</p>
<p>Junction tree variational autoencoder for molecular graph generation. W Jin, R Barzilay, T Jaakkola, International conference on machine learning. PMLR2018</p>
<p>ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. Y Kang, J Kim, arXiv:2308.014232023arXiv preprint</p>
<p>Estimation of probabilities from sparse data for the language model component of a speech recognizer. S Katz, IEEE transactions on acoustics, speech, and signal processing. 198735</p>
<p>Human and machine representations of knowledge. D Kopec, 1982University of EdinburghPh.D. thesis</p>
<p>Accelerating de novo drug design against novel proteins using deep learning. S R Krishnan, N Bung, G Bulusu, A Roy, Journal of Chemical Information and Modeling. 6122021</p>
<p>RDKit: A software suite for cheminformatics, computational chemistry, and predictive modeling. T ; Kuhn, 2014. 2013Greg Landrum4031Landrum, GA survey and classification of controlled natural languages. Computational linguistics</p>
<p>Constrained graph variational autoencoders for molecule design. Q Liu, M Allamanis, M Brockschmidt, A Gaunt, Advances in neural information processing systems. 201831</p>
<p>GNINA 1.0: molecular docking with deep learning. A T Mcnutt, P Francoeur, R Aggarwal, T Masuda, R Meli, M Ragoza, J Sunseri, D R Koes, Journal of cheminformatics. 1312021</p>
<p>Experiments on the Mechanization of Game-Learning. 2-Rule-Based Learning and the Human Window. D Michie, Comput. J. 2511982</p>
<p>Pathways language model (palm): Scaling to 540 billion parameters for breakthrough performance. S Narang, A Chowdhery, 2022Google AI Blog</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, Advances in Neural Information Processing Systems. 202235</p>
<p>Better language models and their implications. A Radford, J Wu, D Amodei, D Amodei, J Clark, M Brundage, I Sutskever, OpenAI blog. 212019</p>
<p>Generative Deep Learning for Targeted Compound Design. T Sousa, J Correia, V Pereira, M Rocha, 34699719Journal of Chemical Information and Modeling. 61112021</p>
<p>Attention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730</p>
<p>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. K Williams, E Bilsland, A Sparkes, W Aubrey, M Young, L N Soldatova, K De Grave, J Ramon, M De Clare, W Sirawaraporn, S G Oliver, R D King, Journal of The Royal Society Interface. 12104201412892015</p>
<p>H Zenil, J Tegnér, F S Abrahão, A Lavin, V Kumar, arXiv:2307.07522The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6847 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6847</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6847</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-273098753</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.02198v1.pdf" target="_blank">G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> We introduce G2T-LLM, a novel approach for molecule generation that uses graph-to-tree text encoding to transform graph-based molecular structures into a hierarchical text format optimized for large language models (LLMs). This encoding converts complex molecular graphs into tree-structured formats, such as JSON and XML, which LLMs are particularly adept at processing due to their extensive pre-training on these types of data. By leveraging the flexibility of LLMs, our approach allows for intuitive interaction using natural language prompts, providing a more accessible interface for molecular design. Through supervised fine-tuning, G2T-LLM generates valid and coherent chemical structures, addressing common challenges like invalid outputs seen in traditional graph-based methods. While LLMs are computationally intensive, they offer superior generalization and adaptability, enabling the generation of diverse molecular structures with minimal task-specific customization. The proposed approach achieved comparable performances with state-of-the-art methods on various benchmark molecular generation datasets, demonstrating its potential as a flexible and innovative tool for AI-driven molecular design.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6847.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6847.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G2T-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that encodes molecular graphs as hierarchical tree-structured text (JSON/XML) and fine-tunes a large language model to perform molecular-completion generation with token-level constraints to ensure chemical validity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>G2T-LLM (fine-tuned LLaMA3.1-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>fine-tuned decoder-only LLM (supervised fine-tuning for molecular completion) with constrained decoding at inference</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (LLaMA3.1-8B base, fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on a curated set of molecular encodings: 5,000 molecules for main experiments (additional ablations using 1k, 5k, 10k from QM9/ZINC subsets); base LLaMA pretraining on large general text corpora (including structured JSON/XML-like data) as per LLaMA3 family.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised fine-tuning cast as a molecular completion task (provide partial tree-encoded molecule, predict remaining nodes/edges) and constrained token decoding during inference (token constraining enforces schema and chemical token validity). No reinforcement learning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Graph-to-tree hierarchical text encoding (JSON/XML-like) inspired by SMILES; atoms are nodes with unique IDs, bonds are represented as edges; ring closures handled via atom IDs to refer back rather than duplicating nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>General de novo molecular generation for drug discovery, material science, and computational chemistry (benchmarked on QM9 and ZINC250k; framed as molecular design / molecule generation).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Token constraining at inference: filters allowable tokens at each generation step to enforce valid parent-child relationships in the tree, restrict atom types and bond types to chemically valid sets, and enforce schema-level constraints. No explicit property or synthetic-accessibility filters reported.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>No domain-specific external chemistry tools (docking, quantum calculations, retrosynthesis) reported; implementation/tools mentioned for training: PyTorch, torchtune, QLoRA; hardware: NVIDIA A100/3090/4090 GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>QM9 and ZINC250k (used for fine-tuning evaluation and benchmarking). Fine-tuning set size reported as 5,000 molecules for main experiments; ablations with 1k/5k/10k on QM9.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (% valid molecules), Uniqueness (% unique among valid), Novelty (% not in training set), FCD (Fréchet ChemNet Distance), Scaf (scaffold similarity metric), NSPDK MMD, Tanimoto similarity on Morgan fingerprints (visual comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>QM9: Validity 99.47%, Novelty 88.29%, FCD 0.815, Scaf 0.9112 (Table 1/6). ZINC250k: Validity 98.03%, Novelty 100%, FCD 2.445, Scaf 0.6062 (Table 1/7). Ablation (SFT effect on ZINC250k): without SFT Validity 70.80%/Unique 61.12% -> with SFT Validity 98.60%/Unique 98.98% (Table 3). Token constraining effect (ZINC250k): w/o token constraining validity 41.60% vs w/ token constraining 98.60% (Table 5). Dataset-size ablation (QM9, 10 epochs): 1k: Valid 98.50%, Novelty 90.38%, FCD 1.226, Scaf 0.693; 5k: Valid 98.70%, Novelty 86.53%, FCD 1.219, Scaf 0.7779; 10k: Valid 98.50%, Novelty 73.89%, FCD 1.146, Scaf 0.798 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reported limitations include LLM computational cost; initial mismatch between graph structures and LLM sequence training (addressed by graph-to-tree encoding); need for supervised fine-tuning to reach high validity (base LLM produced only ~70.8% validity); risk of reduced novelty as fine-tuning data size grows (observed decrease in novelty on QM9 with larger fine-tune sets, likely due to dataset simplicity); omission of token constraining during SFT to allow exploration, but required during inference to ensure validity; no wet-lab synthesis or downstream experimental validation reported; comparison with rule-based methods complicated by baseline model size differences (e.g., GPT-4 baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6847.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6847.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.1 (8 billion parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter variant of the LLaMA3 family used as the base LLM for supervised fine-tuning in the G2T-LLM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only large language model (base model, then fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Base pretraining on large general text corpora (including structured text and examples consistent with LLaMA3 descriptions); fine-tuned on several thousand graph-to-tree encoded molecules (5,000 in main experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised fine-tuning for molecular completion plus constrained decoding at inference (as part of G2T-LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Fine-tuned to generate graph-to-tree JSON/XML-style encodings representing molecular graphs (rather than SMILES directly).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecular generation (benchmarked on QM9 and ZINC250k for molecule generation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Token constraining during inference to enforce valid tree/schema tokens and valid atom/bond types; no external chemical filters reported.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Training accelerated with QLoRA; implemented in PyTorch and torchtune; no chemistry-specific external tool integration reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Fine-tuning used 5,000 molecules drawn from benchmark datasets (QM9/ZINC250k subsets); ablation on QM9 with 1k/5k/10k.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity, Uniqueness, Novelty, FCD, Scaf, NSPDK, Tanimoto similarity (same metrics as G2T-LLM results).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>When fine-tuned: validity increased from 70.8% (pre-fine-tune) to 98.6% (post-fine-tune) on ZINC250k; uniqueness improved from 61.12% to 98.98% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Smaller model size relative to some baselines (e.g., GPT-4) complicates direct comparisons; base LLaMA without SFT produced many invalid molecules; computational cost for fine-tuning and inference acknowledged.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6847.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6847.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Talk Like a Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Talk Like a Graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that encodes graph structures into natural-language sentence descriptions (node connections/attributes as sentences) for processing by LLMs; evaluated as a baseline for graph-to-text molecular encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Talk Like a Graph encoding (used with LLMs in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>graph-to-text encoding (natural language sentences) used with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Encodes graphs into natural-language sentences describing nodes and edges; paper compares encoding effectiveness on ZINC250k.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Convert graph to NL descriptions and use LLMs to generate/complete molecule descriptions (prior work used few-shot or prompting approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Natural-language sentences describing each node's connections and attributes (as opposed to schema/JSON).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule generation via LLMs; used for evaluating how well LLMs can learn graph structures from NL encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Not detailed in the present paper; prior encoding described as 'naive' and potentially not aligned with LLM pretraining distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC250k (used in the comparison experiment reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity, FCD, Scaf, Novelty (used in Table 2 comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>On ZINC250k (Table 2): Validity 59.20%, FCD 19.81 (higher is worse), Scaf 0.1317, Novelty 100%. Compared to G2T-LLM (Validity 98.60%, FCD 5.69, Scaf 0.1522), Talk Like a Graph performed substantially worse in validity and FCD.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Authors note that natural-language graph encodings are unlikely to appear in typical LLM training data, causing weak performance; NL encodings may not tokenize or align well with LLM pretraining leading to poor structure learning and low validity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6847.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6847.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rule-based LLM prompting baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMLF / Grammar Prompting / LLM4GraphGen (rule-based prompt-engineering methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative prior approaches that apply rule-based prompt engineering or grammar-based constraints to guide LLMs (including large models like GPT-4) for molecule/graph generation tasks; discussed as non-comparable baselines due to methodological differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LMLF (Generating novel leads for drug discovery using LLMs with logical feedback), Grammar Prompting, LLM4GraphGen</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>prompt-only or rule-guided LLM approaches (often using large off-the-shelf LLMs, e.g., GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Typically very large LLMs (e.g., GPT-4) in cited works; not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>These approaches rely mainly on prompt engineering and rule- or grammar-based guidance rather than supervised fine-tuning on molecular corpora; training details vary by work.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Rule-based prompt engineering, grammar prompting, or logical feedback loops to guide generation (few-shot or prompt-only methods rather than supervised fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Varies by method; often uses textual or grammar-based representations and task-specific rules rather than tree JSON encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Drug-lead generation and domain-specific molecule generation tasks (drug discovery examples cited).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Rule/grammar-based constraints encoded in prompts; specific property filters depend on the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Some cited works incorporate logical feedback or property predictors, but specifics are from the cited papers (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Not specified / not directly comparable in this paper; authors note these baselines often use different methodologies and larger base models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported in this paper for these methods (authors stated direct comparisons are not feasible).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Paper notes these methods rely on predefined rules/heuristics limiting their ability to learn from data distributions and are often tested with larger LLMs (e.g., GPT-4), making direct comparisons with fine-tuned smaller LLMs (LLaMA8B) difficult; rule-based prompting may restrict generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Generating novel leads for drug discovery using llms with logical feedback <em>(Rating: 2)</em></li>
                <li>Grammar prompting for domain-specific language generation with large language models <em>(Rating: 2)</em></li>
                <li>LLM4GraphGen (Exploring the potential of large language models in graph generation) <em>(Rating: 2)</em></li>
                <li>Molx: Enhancing large language models for molecular learning with a multi-modal extension <em>(Rating: 1)</em></li>
                <li>DiGress: Discrete denoising diffusion for graph generation <em>(Rating: 1)</em></li>
                <li>GruM (Jaehyeong Jo et al. Graph generation with diffusion mixture / GruM) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6847",
    "paper_id": "paper-273098753",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "G2T-LLM",
            "name_full": "Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
            "brief_description": "A pipeline that encodes molecular graphs as hierarchical tree-structured text (JSON/XML) and fine-tunes a large language model to perform molecular-completion generation with token-level constraints to ensure chemical validity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "G2T-LLM (fine-tuned LLaMA3.1-8B)",
            "model_type": "fine-tuned decoder-only LLM (supervised fine-tuning for molecular completion) with constrained decoding at inference",
            "model_size": "8B (LLaMA3.1-8B base, fine-tuned)",
            "training_data_description": "Fine-tuned on a curated set of molecular encodings: 5,000 molecules for main experiments (additional ablations using 1k, 5k, 10k from QM9/ZINC subsets); base LLaMA pretraining on large general text corpora (including structured JSON/XML-like data) as per LLaMA3 family.",
            "generation_method": "Supervised fine-tuning cast as a molecular completion task (provide partial tree-encoded molecule, predict remaining nodes/edges) and constrained token decoding during inference (token constraining enforces schema and chemical token validity). No reinforcement learning reported.",
            "chemical_representation": "Graph-to-tree hierarchical text encoding (JSON/XML-like) inspired by SMILES; atoms are nodes with unique IDs, bonds are represented as edges; ring closures handled via atom IDs to refer back rather than duplicating nodes.",
            "target_application": "General de novo molecular generation for drug discovery, material science, and computational chemistry (benchmarked on QM9 and ZINC250k; framed as molecular design / molecule generation).",
            "constraints_used": "Token constraining at inference: filters allowable tokens at each generation step to enforce valid parent-child relationships in the tree, restrict atom types and bond types to chemically valid sets, and enforce schema-level constraints. No explicit property or synthetic-accessibility filters reported.",
            "integration_with_external_tools": "No domain-specific external chemistry tools (docking, quantum calculations, retrosynthesis) reported; implementation/tools mentioned for training: PyTorch, torchtune, QLoRA; hardware: NVIDIA A100/3090/4090 GPUs.",
            "dataset_used": "QM9 and ZINC250k (used for fine-tuning evaluation and benchmarking). Fine-tuning set size reported as 5,000 molecules for main experiments; ablations with 1k/5k/10k on QM9.",
            "evaluation_metrics": "Validity (% valid molecules), Uniqueness (% unique among valid), Novelty (% not in training set), FCD (Fréchet ChemNet Distance), Scaf (scaffold similarity metric), NSPDK MMD, Tanimoto similarity on Morgan fingerprints (visual comparisons).",
            "reported_results": "QM9: Validity 99.47%, Novelty 88.29%, FCD 0.815, Scaf 0.9112 (Table 1/6). ZINC250k: Validity 98.03%, Novelty 100%, FCD 2.445, Scaf 0.6062 (Table 1/7). Ablation (SFT effect on ZINC250k): without SFT Validity 70.80%/Unique 61.12% -&gt; with SFT Validity 98.60%/Unique 98.98% (Table 3). Token constraining effect (ZINC250k): w/o token constraining validity 41.60% vs w/ token constraining 98.60% (Table 5). Dataset-size ablation (QM9, 10 epochs): 1k: Valid 98.50%, Novelty 90.38%, FCD 1.226, Scaf 0.693; 5k: Valid 98.70%, Novelty 86.53%, FCD 1.219, Scaf 0.7779; 10k: Valid 98.50%, Novelty 73.89%, FCD 1.146, Scaf 0.798 (Table 4).",
            "experimental_validation": false,
            "challenges_or_limitations": "Reported limitations include LLM computational cost; initial mismatch between graph structures and LLM sequence training (addressed by graph-to-tree encoding); need for supervised fine-tuning to reach high validity (base LLM produced only ~70.8% validity); risk of reduced novelty as fine-tuning data size grows (observed decrease in novelty on QM9 with larger fine-tune sets, likely due to dataset simplicity); omission of token constraining during SFT to allow exploration, but required during inference to ensure validity; no wet-lab synthesis or downstream experimental validation reported; comparison with rule-based methods complicated by baseline model size differences (e.g., GPT-4 baselines).",
            "uuid": "e6847.0",
            "source_info": {
                "paper_title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA3.1-8B",
            "name_full": "LLaMA 3.1 (8 billion parameter variant)",
            "brief_description": "An 8B-parameter variant of the LLaMA3 family used as the base LLM for supervised fine-tuning in the G2T-LLM pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA3.1-8B",
            "model_type": "decoder-only large language model (base model, then fine-tuned)",
            "model_size": "8B parameters",
            "training_data_description": "Base pretraining on large general text corpora (including structured text and examples consistent with LLaMA3 descriptions); fine-tuned on several thousand graph-to-tree encoded molecules (5,000 in main experiments).",
            "generation_method": "Supervised fine-tuning for molecular completion plus constrained decoding at inference (as part of G2T-LLM).",
            "chemical_representation": "Fine-tuned to generate graph-to-tree JSON/XML-style encodings representing molecular graphs (rather than SMILES directly).",
            "target_application": "De novo molecular generation (benchmarked on QM9 and ZINC250k for molecule generation tasks).",
            "constraints_used": "Token constraining during inference to enforce valid tree/schema tokens and valid atom/bond types; no external chemical filters reported.",
            "integration_with_external_tools": "Training accelerated with QLoRA; implemented in PyTorch and torchtune; no chemistry-specific external tool integration reported.",
            "dataset_used": "Fine-tuning used 5,000 molecules drawn from benchmark datasets (QM9/ZINC250k subsets); ablation on QM9 with 1k/5k/10k.",
            "evaluation_metrics": "Validity, Uniqueness, Novelty, FCD, Scaf, NSPDK, Tanimoto similarity (same metrics as G2T-LLM results).",
            "reported_results": "When fine-tuned: validity increased from 70.8% (pre-fine-tune) to 98.6% (post-fine-tune) on ZINC250k; uniqueness improved from 61.12% to 98.98% (Table 3).",
            "experimental_validation": false,
            "challenges_or_limitations": "Smaller model size relative to some baselines (e.g., GPT-4) complicates direct comparisons; base LLaMA without SFT produced many invalid molecules; computational cost for fine-tuning and inference acknowledged.",
            "uuid": "e6847.1",
            "source_info": {
                "paper_title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Talk Like a Graph",
            "name_full": "Talk Like a Graph: Encoding graphs for large language models",
            "brief_description": "A prior method that encodes graph structures into natural-language sentence descriptions (node connections/attributes as sentences) for processing by LLMs; evaluated as a baseline for graph-to-text molecular encoding.",
            "citation_title": "Talk like a graph: Encoding graphs for large language models",
            "mention_or_use": "mention",
            "model_name": "Talk Like a Graph encoding (used with LLMs in prior work)",
            "model_type": "graph-to-text encoding (natural language sentences) used with LLMs",
            "model_size": "",
            "training_data_description": "Encodes graphs into natural-language sentences describing nodes and edges; paper compares encoding effectiveness on ZINC250k.",
            "generation_method": "Convert graph to NL descriptions and use LLMs to generate/complete molecule descriptions (prior work used few-shot or prompting approaches).",
            "chemical_representation": "Natural-language sentences describing each node's connections and attributes (as opposed to schema/JSON).",
            "target_application": "Molecule generation via LLMs; used for evaluating how well LLMs can learn graph structures from NL encodings.",
            "constraints_used": "Not detailed in the present paper; prior encoding described as 'naive' and potentially not aligned with LLM pretraining distributions.",
            "integration_with_external_tools": "Not reported in this paper.",
            "dataset_used": "ZINC250k (used in the comparison experiment reported in this paper).",
            "evaluation_metrics": "Validity, FCD, Scaf, Novelty (used in Table 2 comparison).",
            "reported_results": "On ZINC250k (Table 2): Validity 59.20%, FCD 19.81 (higher is worse), Scaf 0.1317, Novelty 100%. Compared to G2T-LLM (Validity 98.60%, FCD 5.69, Scaf 0.1522), Talk Like a Graph performed substantially worse in validity and FCD.",
            "experimental_validation": false,
            "challenges_or_limitations": "Authors note that natural-language graph encodings are unlikely to appear in typical LLM training data, causing weak performance; NL encodings may not tokenize or align well with LLM pretraining leading to poor structure learning and low validity.",
            "uuid": "e6847.2",
            "source_info": {
                "paper_title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Rule-based LLM prompting baselines",
            "name_full": "LMLF / Grammar Prompting / LLM4GraphGen (rule-based prompt-engineering methods)",
            "brief_description": "Representative prior approaches that apply rule-based prompt engineering or grammar-based constraints to guide LLMs (including large models like GPT-4) for molecule/graph generation tasks; discussed as non-comparable baselines due to methodological differences.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LMLF (Generating novel leads for drug discovery using LLMs with logical feedback), Grammar Prompting, LLM4GraphGen",
            "model_type": "prompt-only or rule-guided LLM approaches (often using large off-the-shelf LLMs, e.g., GPT-4)",
            "model_size": "Typically very large LLMs (e.g., GPT-4) in cited works; not used in experiments here.",
            "training_data_description": "These approaches rely mainly on prompt engineering and rule- or grammar-based guidance rather than supervised fine-tuning on molecular corpora; training details vary by work.",
            "generation_method": "Rule-based prompt engineering, grammar prompting, or logical feedback loops to guide generation (few-shot or prompt-only methods rather than supervised fine-tuning).",
            "chemical_representation": "Varies by method; often uses textual or grammar-based representations and task-specific rules rather than tree JSON encodings.",
            "target_application": "Drug-lead generation and domain-specific molecule generation tasks (drug discovery examples cited).",
            "constraints_used": "Rule/grammar-based constraints encoded in prompts; specific property filters depend on the cited work.",
            "integration_with_external_tools": "Some cited works incorporate logical feedback or property predictors, but specifics are from the cited papers (not detailed here).",
            "dataset_used": "Not specified / not directly comparable in this paper; authors note these baselines often use different methodologies and larger base models.",
            "evaluation_metrics": "Not reported in this paper for these methods (authors stated direct comparisons are not feasible).",
            "reported_results": "",
            "experimental_validation": null,
            "challenges_or_limitations": "Paper notes these methods rely on predefined rules/heuristics limiting their ability to learn from data distributions and are often tested with larger LLMs (e.g., GPT-4), making direct comparisons with fine-tuned smaller LLMs (LLaMA8B) difficult; rule-based prompting may restrict generalization.",
            "uuid": "e6847.3",
            "source_info": {
                "paper_title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Generating novel leads for drug discovery using llms with logical feedback",
            "rating": 2,
            "sanitized_title": "generating_novel_leads_for_drug_discovery_using_llms_with_logical_feedback"
        },
        {
            "paper_title": "Grammar prompting for domain-specific language generation with large language models",
            "rating": 2,
            "sanitized_title": "grammar_prompting_for_domainspecific_language_generation_with_large_language_models"
        },
        {
            "paper_title": "LLM4GraphGen (Exploring the potential of large language models in graph generation)",
            "rating": 2,
            "sanitized_title": "llm4graphgen_exploring_the_potential_of_large_language_models_in_graph_generation"
        },
        {
            "paper_title": "Molx: Enhancing large language models for molecular learning with a multi-modal extension",
            "rating": 1,
            "sanitized_title": "molx_enhancing_large_language_models_for_molecular_learning_with_a_multimodal_extension"
        },
        {
            "paper_title": "DiGress: Discrete denoising diffusion for graph generation",
            "rating": 1,
            "sanitized_title": "digress_discrete_denoising_diffusion_for_graph_generation"
        },
        {
            "paper_title": "GruM (Jaehyeong Jo et al. Graph generation with diffusion mixture / GruM)",
            "rating": 1,
            "sanitized_title": "grum_jaehyeong_jo_et_al_graph_generation_with_diffusion_mixture_grum"
        }
    ],
    "cost": 0.013009999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>G2T-LLM: GRAPH-TO-TREE TEXT ENCODING FOR MOLECULE GENERATION WITH FINE-TUNED LARGE LANGUAGE MODELS
3 Oct 2024</p>
<p>Zhaoning Yu znyu@iastate.edu 
Iowa State University Ames
50010IA</p>
<p>Xiangyang Xu xyxu@iastate.edu 
Iowa State University Ames
50010IA</p>
<p>Hongyang Gao hygao@iastate.edu 
Iowa State University Ames
50010IA</p>
<p>G2T-LLM: GRAPH-TO-TREE TEXT ENCODING FOR MOLECULE GENERATION WITH FINE-TUNED LARGE LANGUAGE MODELS
3 Oct 20248CF3DBE6AFF3318F3EB5333BC6BEB188arXiv:2410.02198v1[cs.LG]
We introduce G2T-LLM, a novel approach for molecule generation that uses graph-to-tree text encoding to transform graph-based molecular structures into a hierarchical text format optimized for large language models (LLMs).This encoding converts complex molecular graphs into tree-structured formats, such as JSON and XML, which LLMs are particularly adept at processing due to their extensive pre-training on these types of data.By leveraging the flexibility of LLMs, our approach allows for intuitive interaction using natural language prompts, providing a more accessible interface for molecular design.Through supervised finetuning, G2T-LLM generates valid and coherent chemical structures, addressing common challenges like invalid outputs seen in traditional graph-based methods.While LLMs are computationally intensive, they offer superior generalization and adaptability, enabling the generation of diverse molecular structures with minimal task-specific customization.The proposed approach achieved comparable performances with state-of-the-art methods on various benchmark molecular generation datasets, demonstrating its potential as a flexible and innovative tool for AI-driven molecular design.</p>
<p>INTRODUCTION</p>
<p>Molecular generation is a critical task in fields such as drug discovery, material science, and chemistry (Schneider &amp; Fechner, 2005;Simonovsky &amp; Komodakis, 2018;Elton et al., 2019).The ability to design and create novel molecules with specific properties can accelerate the development of new therapies, advanced materials, and innovative chemicals.Traditional approaches to molecular generation, such as rule-based systems (Schneider &amp; Fechner, 2005;Sastry et al., 2011) and graphbased (You et al., 2018;Madhawa et al., 2019;Shi et al., 2020) models, have provided foundational tools.However, these methods often face limitations in generating diverse, valid, and chemically coherent molecular structures, restricting their ability to explore the vast chemical space effectively (Vignac et al., 2022;Jo et al., 2022).Recent advancements in deep learning, especially the rise of large language models (LLMs), offer new opportunities for molecular generation (Brahmavar et al., 2024;Wang et al., 2024;Yao et al., 2024).Unlike traditional methods, LLMs are not constrained by domain-specific rules and can generalize from vast amounts of data.This flexibility allows them to generate creative and diverse content, potentially uncovering novel chemical compounds.Prior non-LLM approaches, such as graph-based generative models (You et al., 2018;Madhawa et al., 2019;Shi et al., 2020;Luo et al., 2021;Vignac et al., 2022;Jo et al., 2022), often struggle with limited generalization, rule-based rigidity, or difficulty scaling to more complex chemical structures.In contrast, LLMs can adapt to a wide range of prompts and provide greater flexibility, making them an attractive choice for AI-driven molecular generation.</p>
<p>Despite the promise of LLMs, applying them to molecular generation presents a unique challenge.Molecular structures are typically represented as graphs, with atoms as nodes and bonds as edges.LLMs, however, are trained to understand sequences of tokens (Vaswani, 2017), particularly in structured text formats such as XML and JSON (Brown, 2020), and are not inherently designed to process graph-based data.This mismatch creates a barrier when attempting to use LLMs for tasks that require understanding the relational and non-linear properties of molecular structures.LLMs • We achieve comparable performances with state-of-the-art models on benchmark molecular generation datasets, demonstrating the effectiveness and potential of our approach for AI-driven molecular design.</p>
<p>RELATED WORK</p>
<p>Graph Generation.The graph generation task aims to learn the distribution of graphs.The traditional approaches (Zang &amp; Wang, 2020;Shi et al., 2020;Luo et al., 2021;You et al., 2018;Madhawa et al., 2019;Dai et al., 2018) such as auto-regression, Generative Adversarial Network (GAN), and Variational Autoencoder (VAE) have been explored for this purpose.However, they have faced challenges in modeling the permutation-invariant nature of graph distribution and learning the relationship between edges and nodes, often due to limitations in their model capacity.Recent advancements in diffusion methods (Niu et al., 2020;Jo et al., 2022;Vignac et al., 2022;Jo et al., 2023) have significantly improved graph generation.GDSS (Jo et al., 2022) generates both node features and adjacency matrices simultaneously, resulting in better alignment with graph datasets.DiGress (Vignac et al., 2022) addresses the challenge of generating graphs with categorical node and edge attributes, which is a difficult task due to the unordered nature and sparsity of graphs.GruM (Jo et al., 2023) directly learns graph topology, improving connectivity and structure recovery.</p>
<p>Graph to Text for LLM.The emergence of large language models (LLMs) has driven significant advancements in the natural sciences (Taylor et al., 2022;Liu et al., 2024).These models are trained on vast amounts of text data, the most abundant type of data, contributing to their success across many tasks.Multi-modal methods (Luo et al., 2023;Le et al., 2024) have been proposed to incorporate both graph and text information.They typically rely on graph neural networks or transformers to encode graphs.However, these methods often use text, such as SMILES, to represent molecular features.SMILES may not tokenize the molecular structure effectively, limiting the ability to represent the molecule structure accurately.As a result, the graph embeddings may be too weak for intricate molecular structures, limiting performance in molecular generation tasks.Recently, there have been attempts (Fatemi et al., 2023) to represent graphs in natural language formats, encoding their structure using descriptive language.However, this naive approach introduces challenges, as such encodings are unlikely to appear in typical text, meaning that LLMs-trained predominantly on conventional text data-may struggle to process them effectively.Using an encoding that aligns with the LLMs' training data is essential.We propose leveraging tree-structured formats like JSON and XML to encode molecules to address this issue.The JSON format is a widely used and structured data representation commonly found in LLM training.This allows us to capture the complexity of molecular graphs while ensuring compatibility with LLMs.</p>
<p>G2T-LLM</p>
<p>This section introduces G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models.</p>
<p>CHALLENGES AND MOTIVATIONS</p>
<p>Molecular graphs pose a challenge for LLMs due to their inherently complex, non-linear structures, where atoms (nodes) and bonds (edges) form intricate connectivity patterns, including rings, branches, and cycles.Traditional LLMs excel at processing sequential data, such as natural language, where information flows in a linear manner.However, molecular graphs do not naturally conform to this format, as their connections often lack a clear, ordered sequence.This mismatch complicates the application of LLMs to molecule-related tasks.</p>
<p>Despite these challenges, LLMs have shown a capacity to handle structured, hierarchical data formats, such as JSON and XML.These formats share some of the complexity of graphs but are still expressed as trees, with clear parent-child relationships between elements.LLMs trained on such data can handle hierarchical structures by processing them as sequences while maintaining the relationships and nested dependencies inherent to these structures.This training has made LLMs particularly adept at handling data that can be decomposed into nested layers, making them better suited for tree-like representations than arbitrary graphs.</p>
<p>To leverage this strength, we propose encoding molecular graphs into a tree structure.This approach is inspired by SMILEs, which are essentially tree representations of molecular graphs, proving that molecular graphs can be effectively serialized as trees while preserving their chemical properties.This encoding acts as a bridge between the graph-based molecular structures and the LLM's ability to process and generate hierarchical data.The LLM can be trained on these tree-encoded molecules, and it can also output molecules in the same structured format, facilitating the generation of coherent Input: graph (dictionary of atom identifiers to connected atom identifiers)</p>
<p>3:</p>
<p>Output: text representation (tree structure in text format) return {"atom name": atom.atomname, "atom id": atom id, "bonds": bonds} return text representation molecular representations.By aligning graph data with a format that LLMs are well-equipped to handle, this method holds the potential for improving the coherence and plausibility of generated molecular structures.</p>
<p>GRAPH-TO-TREE TEXT ENCODING</p>
<p>To make molecular graphs accessible to LLMs, we introduce a tree-based encoding inspired by the SMILES format.SMILES encodes molecules by performing a depth-first traversal over the molecular graph and representing it as a linear string.In our approach, we extend this traversal to build a hierarchical tree structure, where atoms are represented as nodes and their bonds as edges connecting them.The hierarchical nature of the tree is well-suited for the LLM's training with tree-like structures.</p>
<p>However, molecular graphs often contain rings and cycles-features that trees cannot naturally represent.To address this, we assign each atom in the molecule a unique identifier (ID).When the traversal encounters a ring closure or cycle, the tree refers back to the atom's unique ID rather than creating a new node, thereby preserving both the hierarchical structure and chemical validity.This encoding technique ensures that we accurately capture the full molecular graph in a way the LLM can process, while maintaining the integrity of complex molecular features such as rings and branches.Algorithm 1 and Algorithm 2 describe the processes for converting a molecular graph to a tree-structured text representation and for reconstructing the graph from this format, respectively.Figure 3 illustrates the graph-to-tree text encoding.</p>
<p>TOKEN CONSTRAINING FOR VALID TREE-STRUCTURE GENERATION</p>
<p>Despite the advancements in LLMs, there remains a significant challenge in ensuring that the outputs adhere to valid tree-structured formats.LLMs, while capable of generating coherent text, may produce sequences that do not respect the hierarchical relationships required for molecular repre- Input: tree json (tree structure in JSON format)</p>
<p>3:</p>
<p>Output: graph (dictionary representing the molecular graph) return graph sentation.This can lead to outputs that are structurally invalid, failing to accurately represent the complex relationships inherent in molecular graphs.</p>
<p>To mitigate this issue, we implement a set of constraints that guide the token generation process of the LLM.These constraints filter the tokens allowed at each step, ensuring that generated outputs remain within the bounds of valid tree structures.Specifically, we impose rules that dictate acceptable parent-child relationships, enforce valid connections between atoms, and restrict the formation of non-hierarchical sequences.Additionally, we constrain the types of atoms and bonds that can be generated, ensuring that only valid atom types (e.g., carbon, oxygen) and bond types (e.g., single, double) are used in the output.This approach leverages domain knowledge of molecular structures to create a robust framework for guiding the LLM's outputs.</p>
<p>The application of token constraining significantly enhances the reliability of the generated treestructured outputs.By enforcing these constraints, we improve the chances that the LLM produces valid representations of molecular structures that can be effectively used in further analyses or applications.This technique not only aids in ensuring the accuracy of the generated data but also reinforces the overall effectiveness of our graph-to-tree text encoding approach, making it a vital component in achieving coherent and chemically valid molecular generation.</p>
<p>SUPERVISED FINE-TUNING LLMS FOR MOLECULAR GENERATION</p>
<p>A key challenge in leveraging large language models for molecular generation is that, without specialized training, they may struggle to produce valid molecular structures, particularly when dealing with complex features such as rings, cycles, and the inherent chemical constraints that govern molecular formation.Supervised fine-tuning addresses this issue by teaching the LLM domain-specific rules and patterns, enabling it to generate valid molecular structures that adhere to chemical principles.</p>
<p>We structure the fine-tuning process as a molecular completion task.The LLM is trained by prompting it with a partial molecular structure, encoded using the graph-to-tree text encoding and tasking it with predicting the remaining atoms and bonds necessary to complete the molecule.For each training example, we provide the LLM with an incomplete molecular graph, and the model is then expected to generate the missing parts based on the information provided.The model's output is evaluated against the full molecular structure's text encoding, and the loss is computed based on the accuracy of its predictions.By iterating through this process, the LLM learns to predict the completion of molecular graphs in a way that respects chemical validity, helping the model better handle challenging structural features.Note that token constraining is deliberately omitted during finetuning, allowing the LLM to explore and learn more freely before constraints are imposed during inference.Figure 3.4 illustrates the supervised fine-tuning process of G2T-LLM.</p>
<p>The fine-tuning process is integral to the success of our approach.By casting molecular generation as a completion task and using the proposed graph-to-tree encoding as a bridge between molecular structures and the LLM's capabilities, we enhance the model's ability to generate coherent and chemically valid outputs.This fine-tuning approach refines the LLM's understanding of molecular patterns and constraints, enabling it to produce outputs that are more reliable and scientifically grounded within the realm of molecular design.</p>
<p>INFERENCE PROCESS OF G2T-LLM</p>
<p>The molecular generation process begins with selecting a random molecular component, which could be an atom, a bond, or even a larger motif.This component serves as the initial prompt for the fine-tuned LLM.The component is encoded into the graph-to-tree text format, creating a tree-structured representation that the LLM can process.</p>
<p>Once the LLM receives this initial prompt, it is tasked with generating the subsequent components of the molecular structure.At each step, the LLM's output is constrained by the Token Constraining mechanism, ensuring that only chemical and schema-valid tokens-such as specific atom types and bond types-are generated.These constraints help guide the LLM in maintaining the coherence of the structure, preventing invalid or nonsensical outputs, and ensuring that the generated molecule adheres to the expected chemical rules.As the LLM iteratively predicts new components, these outputs are progressively combined into an expanding tree-structured text.This generated text represents the molecular graph, with nodes corresponding to atoms and edges corresponding to bonds.</p>
<p>Once the generation process is complete, the final tree-structured text is decoded back into a full molecular graph.This graph is then translated into a standard molecular format, fully reconstructing the molecule from the text generated by the LLM. Figure 3.4 illustrates the inference process of G2T-LLM.</p>
<p>EXPERIMENTS</p>
<p>In this section, we conduct comprehensive experiments on two real-world datasets to evaluate the effectiveness of our proposed methods.</p>
<p>EXPERIMENTAL SETUP</p>
<p>Datasets and Metrics.We evaluate the quality of molecule generation using two real-world datasets: QM9 (Ramakrishnan et al., 2014)  Although several studies have explored using LLMs for molecular generation, direct comparisons with our approach are not feasible.For instance, LMLF (Brahmavar et al., 2024), Grammar Prompting (Wang et al., 2024), and LLM4GraphGen (Yao et al., 2024) all employ rule-based promptengineering techniques that fundamentally differ from our SFT LLM approach.These models rely on predefined rules and heuristics to guide the generation process, which restricts their ability to learn from the underlying data distributions.In contrast, our method leverages a more flexible and adaptive encoding, allowing the LLM to capture the complexities of molecular structures more effectively.</p>
<p>Moreover, the baseline models utilize significantly larger architectures, such as GPT-4, whereas our experiments are conducted with LLaMA3.1-8B.This disparity in model size and complexity further complicates direct comparisons, as the performance capabilities and learned representations of these models can vary widely.Therefore, assessing our results against those achieved by larger, rule-based models may not provide a meaningful evaluation of performance, given the substantial differences in methodologies and model architectures.Implementation details.For our G2T-LLM, we conduct experiments using the LLaMA3.1-8Bmodel (Dubey et al., 2024) as our base LLM, selected for its strong performance in text generation tasks.The model parameters are fine-tuned with torchtune (Ansel et al., 2024), and we leverage QLoRA (Dettmers et al., 2024) to accelerate training while reducing memory consumption.The fine-tuning dataset consists of 5,000 molecules, and the model is trained with a batch size of 8, using the AdamW optimizer (Loshchilov, 2017) with a weight decay of 0.01 and a learning rate of 3e-4.The learning rate is adjusted by a cosine schedule with 100 warmup steps, and cross-entropy loss is employed for the loss computation.All model computations are performed with the bf16 data type.Fine-tuning is carried out on an NVIDIA A100 SXM4 80GB, and inference is done on NVIDIA GeForce RTX 3090 and 4090 GPUs.The implementation is done in PyTorch (Paszke et al., 2019).</p>
<p>EXPERIMENTAL RESULTS</p>
<p>Table 1 presents performance comparisons on both the QM9 and ZINC250k datasets against baseline models.Our approach consistently achieves top-two validity scores across both datasets, demonstrating its effectiveness in enabling the LLM to capture the underlying chemical rules essential for accurate molecule generation.For novelty, our method attains a perfect score of 100% on the ZINC250k dataset and 88% on QM9, highlighting its ability to consistently generate novel molecular structures.In terms of FCD and Scaf metrics-critical indicators of a model's ability to explore and replicate chemical space-our method delivers competitive performance compared to other baselines.While DiGress and Grum show strong FCD and Scaf scores on the QM9 dataset, their novelty scores fall significantly short (below 40%), suggesting potential overfitting to the training data rather than true generalization of molecular distributions.In contrast, our method not only maintains high novelty rates but also achieves strong performance on FCD and Scaf metrics.On the ZINC250k dataset, our approach attains the highest Scaf score and the second-best FCD score, further demonstrating its superior ability to generalize and innovate within chemical spaces.This robust performance underscores our model's advanced understanding and application of molecular distributions, making it a powerful tool for innovative molecular design in computational chemistry.</p>
<p>VISUALIZATION RESULTS OF GENERATED MOLECULES</p>
<p>In Fig. 4, we follow the experimental setup outlined in (Jo et al., 2022), using Tanimoto similarity based on Morgan fingerprints to evaluate the generated molecular graphs.For consistency and comparability, we select the same molecules as (Jo et al., 2022).Additionally, we perform experiments on molecular graphs generated by Grum (Jo et al., 2023).Across most cases, our method demonstrates superior performance compared to previous state-of-the-art diffusion-based approaches, showcasing its effectiveness and robustness in molecular graph generation.To evaluate how our proposed graphto-tree text encoding improves the LLM's ability to learn graph structures compared to the previous graphto-text methods such as Talk Like a Graph (Fatemi et al., 2023), we conducted experiments on the challenging Zinc250K dataset (Irwin et al., 2012), which contains larger molecules.Talk Like a Graph encodes graph structures by converting them into natural language, where each node's connections and attributes are described in sentence form.For the fine-tuning process, we randomly selected 5,000 molecules from the training set and generated 1,000 molecules for performance comparison.</p>
<p>As shown in Table 2, our method significantly outperforms the previous approach across all metrics, demonstrating that encoding molecular structures in JSON format enables LLMs to more effectively learn and replicate complex molecular structures.In this study, we aim to evaluate the impact of supervised fine-tuning on LLM performance.Specifically, we generate 1,000 molecules using the same prompt to compare the performance of the LLM before and after fine-tuning.This direct comparison allows us to assess how fine-tuning enhances the model's ability to accurately generate molecular structures.We conduct this experiment using the ZINC250k dataset, and the results are presented in Table 3.The results reveal that without fine-tuning, the LLM produces molecules with only 70.8% validity and 61.12% uniqueness, indicating that the model, in its initial state, struggles to fully comprehend and accurately replicate the text representation of molecular structures.However, after fine-tuning, there is a significant improvement, with validity and uniqueness increasing to 99.6% and 99.79%, respectively.These results highlight the effectiveness of fine-tuning in substantially improving the model's performance, demonstrating its critical role in enabling the LLM to better understand and generate precise molecular structures.</p>
<p>ABLATION STUDY: IMPACT OF SUPERVISED FINE-TUNING LLM</p>
<p>4.6 ABLATION STUDY: IMPACT OF SIZE OF THE FINE-TUNING DATASET In this section, we investigate the impact of dataset size on the performance of a LLM during fine-tuning.Our experiments use the QM9 dataset with three distinct dataset sizes for fine-tuning: 1,000, 5,000, and 10,000 molecules.Each model is trained over 10 epochs.This setup enables a systematic evaluation of how variations in fine-tuning data size affect the model's learning efficacy and its ability to generalize.Table 4 presents the results of these experiments.The results indicate an improvement in the FCD and Scaf scores as the dataset size increases.This improvement likely stems from the LLM's exposure to a larger array of data points, which enhances its understanding of the chemical distribution within the dataset.Conversely, we observe a decrease in novelty scores with larger datasets.This reduction may be attributed to the relatively small and structurally simple nature of the QM9 dataset, which comprises only four types of atoms and molecules not exceeding nine atoms.As the model encounters more data, it increasingly reproduces similar outputs, reflecting the limited diversity in the dataset.In this section, we examine the impact of token constraining on molecular generation, as introduced in Section 3.3.Token constraining is implemented to guide the LLM toward generating valid molecular structures by restricting its output to adhere to chemical rules.To evaluate the effectiveness of this approach, we perform an experimental comparison using the ZINC250k dataset.Specifically, we generate 1,000 molecules to compare the validity of the output with and without token constraining.The results, presented in Table 5, clearly demonstrate the efficacy of token constraining in improving the validity of generated molecules.Without token constraining, the validity of the generated molecules is only 41.6%.However, when token constraining is applied, validity dramatically increases to 98.6%.This significant improvement underscores the critical role of token constraining in guiding the LLM to produce valid molecular structures, ensuring closer adherence to the fundamental rules of chemical structure and leading to a higher rate of valid outputs.</p>
<p>ABLATION STUDY: IMPACT OF TOKEN CONSTRAINING</p>
<p>CONCLUSION</p>
<p>In this work, we introduced G2T-LLM, a novel approach for molecular generation that leverages LLMs to generate valid molecular structures through a novel graph-to-tree text encoding.By converting molecular graphs into hierarchical representations inspired by SMILES but adapted for LLMs, we bridge the gap between non-linear molecular structures and sequential data processing.This encoding allows the LLM to understand the molecular structure better and produce coherent chemical outputs.Our method addresses the challenges of generating valid molecular structures by introducing token constraints during the generation process, ensuring that the outputs respect some chemical and structural rules.Through supervised fine-tuning, we further align the LLM with molecular generation tasks, improving its ability to produce chemically valid molecules based on the learned data patterns from benchmark datasets like Zinc250K and QM9.Our results demonstrate the effectiveness of G2T-LLM, achieving state-of-the-art performance on benchmark datasets.This work highlights the potential of utilizing LLMs in molecular design, opening up new avenues for AI-driven discoveries in chemistry.The combination of hierarchical encoding, token constraining, and fine-tuning proves to be a powerful strategy for tackling the complexities of molecular generation.Future work will focus on refining these techniques to enhance efficiency and explore further applications in drug discovery and material science.</p>
<p>A ADDITIONAL EXPERIMENTS RESULTS</p>
<p>Here are additional experiment results on QM9 and ZINC250k datasets.The Neighborhood Subgraph Pairwise Distance Kernel (NSPDK) Maximum Mean Discrepancy (MMD) (Costa &amp; Grave, 2010) evaluates the difference between generated and test molecules, accounting for both node and edge features.Uniqueness refers to the percentage of valid molecules that are distinct from each other.Validity, FCD, Novelty, and Scaf have been introduced before.</p>
<p>Figure 1 :
1
Figure1: Illustration of the Graph-to-Tree Text Encoding process described in Section 3.2 and Algorithm 1.This figure shows how the molecular structure of cyclopropene is transformed into a hierarchical tree representation.Each atom and bond is mapped to nodes and edges in the tree, with unique identifiers assigned.</p>
<p>Algorithm 2
2
Convert Tree-Structured Text to Molecular Graph 1: function TREE2GRAPH(tree json) 2:</p>
<p>Figure 2 :Figure 3 :
23
Figure2: An illustration of the supervised fine-tuning process of G2T-LLM.The process begins by randomly selecting a starting component, exemplified by cyclopropene, which is encoded into a partial tree structure and passed as a prompt to the LLM.The LLM generates the remaining molecular structure, which is compared against the ground truth.A loss is computed and is used to fine-tune the model, iteratively improving its performance in generating valid molecular graphs.</p>
<p>Figure 4 :
4
Figure 4: Visualization of the generated molecules with Tanimoto similarity scores based on Morgan fingerprints.The best results are highlighted in bold.</p>
<p>Table 1 :
1
Generation results on the QM9 and ZINC250k datasets.We report the mean of 3 different runs.The best results are highlighted in bold.The second-best results are highlighted in underline.We provide the results of uniqueness, and NSPDK in Appendix A.
DatasetsQM9ZINC250KMethodsValid↑Novelty↑ FCD↓Scaf↑Valid↑Novelty↑ FCD↓Scaf↑MoFlow91.3694.724.4670.144763.11100.0020.9310.0133GraphAF74.4386.595.6250.304668.4799.9916.0230.0672GraphDF93.8898.5410.9280.097890.61100.0033.5460.0000EDP-GNN47.5286.582.6800.327082.97100.0016.7370.0000GDSS95.7286.272.9000.698397.01100.0014.6560.0467DiGress98.1925.580.0950.935394.9999.993.4820.4163Grum99.6924.150.1080.944998.6599.982.2570.5299Ours99.4788.290.8150.911298.03100.002.4450.6062</p>
<p>Table 2 :
2
Study of the impact of tree-structured text encoding on the ZINC250K dataset.
MethodsValid↑FCD↓Scaf↑Novelty↑Talk like a graph59.2019.81140.1317100Ours98.605.69060.1522100</p>
<p>Table 3 :
3
Comparison of LLM performance with and without SFT on the ZINC250k dataset.Methods
Valid↑Unique↑Novelty↑w/o SFT70.8061.12100.00w/ SFT98.6098.98100.00</p>
<p>Table 4
4: Comparison of LLM performance with differ-ent size of fine-tuning datasetsMethodsValid↑ Novelty↑ FCD ↓ Scaf ↑1k (10 epoch)98.5090.381.2260.69335k (10 epoch)98.7086.531.2190.777910k (10 epoch) 98.5073.891.1460.7980</p>
<p>Table 5
5: Comparison results of using token con-straining (TC) on molecular generation on theZINC250k dataset.w/o TCw/ TCValidity (%)↑41.6098.60</p>
<p>Table 6 :
6
Generation results on the QM9 dataset.We report the mean of 3 different runs.The best results are highlighted in bold.The second-best results are highlighted in underline.Methods Valid (%)↑ FCD ↓ NSPDK ↓ Scaf ↑ Unique (%)↑ Novelty (%)↑
MoFlow91.364.4670.0170.144798.6594.72GraphAF74.435.6250.0210.304688.6486.59GraphDF93.8810.9280.0640.097898.5898.54EDP-GNN47.522.6800.0050.327099.2586.58GDSS95.722.9000.0030.698398.4686.27DiGress98.190.0950.00030.935396.6725.58Grum99.690.1080.00020.944996.9024.15Ours99.470.8150.0020.911289.5788.29</p>
<p>Table 7 :
7
Generation results on the ZINC250k dataset.We report the mean of 3 different runs.The best results are highlighted in bold.The second-best results are highlighted in underline.
MoFlow63.1120.9310.0460.013399.99100.00GraphAF68.4716.0230.0440.067298.6499.99GraphDF90.6133.5460.1770.000099.63100.00EDP-GNN82.9716.7370.0490.000099.79100.00GDSS97.0114.6560.0190.046799.64100.00DiGress94.993.4820.00210.416399.9799.99Grum98.652.2570.00150.529999.9799.98Ours98.032.4450.00490.606294.69100.00
Methods Valid (%)↑ FCD ↓ NSPDK ↓ Scaf ↑ Unique (%)↑ Novelty (%)↑</p>
<p>PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary Devito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, Bert Luk, Yunjie Maher, Christian Pan, Matthias Puhrsch, Mark Reso, Marcos Yukio Saroufim, Helen Siraichi, Michael Suk, Phil Suo, Eikan Tillet, Xiaodong Wang, William Wang, Shunting Wen, Xu Zhang, Keren Zhao, Richard Zhou, Ajit Zou, Gregory Mathews, Peng Chanan, Soumith Wu, Chintala, 10.1145/3620665.364036629th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. ACMApril 2024224</p>
<p>Generating novel leads for drug discovery using llms with logical feedback. Shreyas Bhat Brahmavar, Ashwin Srinivasan, Tirtharaj Dash, Sowmya Ramaswamy Krishnan, Lovekesh Vig, Arijit Roy, Raviprasad Aduri, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Fast neighborhood subgraph pairwise distance kernel. Fabrizio Costa, Kurt De Grave, Proceedings of the 27th International Conference on International Conference on Machine Learning. the 27th International Conference on International Conference on Machine Learning2010</p>
<p>Syntax-directed variational autoencoder for structured data. Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, Le Song, arXiv:1802.087862018arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in Neural Information Processing Systems. 202436</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Deep learning for molecular design-a review of the state of the art. Zois Daniel C Elton, Boukouvalas, Peter W Mark D Fuge, Chung, Molecular Systems Design &amp; Engineering. 442019</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, arXiv:2310.045602023arXiv preprint</p>
<p>Zinc: a free tool to discover chemistry for biology. Teague John J Irwin, Sterling, Erin S Michael M Mysinger, Ryan G Bolstad, Coleman, Journal of chemical information and modeling. 5272012</p>
<p>Score-based generative modeling of graphs via the system of stochastic differential equations. Jaehyeong Jo, Seul Lee, Sung Ju Hwang, International conference on machine learning. PMLR2022</p>
<p>Graph generation with diffusion mixture. Jaehyeong Jo, Dongki Kim, Sung Ju Hwang, arXiv:2302.035962023arXiv preprint</p>
<p>Molx: Enhancing large language models for molecular learning with a multi-modal extension. Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Nitesh V Chawla, arXiv:2406.067772024arXiv preprint</p>
<p>Moleculargpt: Open large language model (llm) for few-shot molecular property prediction. Yuyan Liu, Sirui Ding, Sheng Zhou, Wenqi Fan, Qiaoyu Tan, arXiv:2406.12950arXiv:1711.051012024. I Loshchilov. Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Yizhen Luo, Kai Yang, Massimo Hong, Xing Yi Liu, Zaiqing Nie, arXiv:2307.09484Molfm: A multimodal molecular foundation model. 2023arXiv preprint</p>
<p>Graphdf: A discrete flow model for molecular graph generation. Youzhi Luo, Keqiang Yan, Shuiwang Ji, International conference on machine learning. PMLR2021</p>
<p>Graphnvp: An invertible flow model for generating molecular graphs. Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, Motoki Abe, arXiv:1905.116002019arXiv preprint</p>
<p>Permutation invariant graph generation via score-based generative modeling. Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, Stefano Ermon, International Conference on Artificial Intelligence and Statistics. PMLR2020</p>
<p>Pytorch: An imperative style, highperformance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 201932</p>
<p>Fréchet chemnet distance: a metric for generative models for molecules in drug discovery. Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, Gunter Klambauer, Journal of chemical information and modeling. 5892018</p>
<p>Quantum chemistry structures and properties of 134 kilo molecules. Raghunathan Ramakrishnan, Matthias Pavlo O Dral, O Rupp, Von Anatole, Lilienfeld, Scientific data. 112014</p>
<p>Rapid shape-based ligand alignment and virtual screening method based on atom/feature-pair similarities and volume overlap scoring. G Madhavi Sastry, Steven L Dixon, Woody Sherman, Journal of chemical information and modeling. 51102011</p>
<p>Computer-based de novo design of drug-like molecules. Gisbert Schneider, Uli Fechner, Nature Reviews Drug Discovery. 482005</p>
<p>Graphaf: a flow-based autoregressive model for molecular graph generation. Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, Jian Tang, arXiv:2001.093822020arXiv preprint</p>
<p>Graphvae: Towards generation of small graphs using variational autoencoders. Martin Simonovsky, Nikos Komodakis, Artificial Neural Networks and Machine Learning-ICANN 2018: 27th International Conference on Artificial Neural Networks. Rhodes, GreeceSpringerOctober 4-7, 2018. 2018Proceedings, Part I 27</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Attention is all you need. Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Digress: Discrete denoising diffusion for graph generation. Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, Pascal Frossard, arXiv:2209.147342022arXiv preprint</p>
<p>Grammar prompting for domain-specific language generation with large language models. Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A Saurous, Yoon Kim, Advances in Neural Information Processing Systems. 202436</p>
<p>Exploring the potential of large language models in graph generation. Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, Hong Mei, arXiv:2403.143582024arXiv preprint</p>
<p>Graphrnn: Generating realistic graphs with deep auto-regressive models. Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, Jure Leskovec, International conference on machine learning. PMLR2018</p>
<p>Moflow: an invertible flow model for generating molecular graphs. Chengxi Zang, Fei Wang, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining2020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8135 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8135</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8135</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-273661714</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.21272v2.pdf" target="_blank">Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics</a></p>
                <p><strong>Paper Abstract:</strong> Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal analysis, we identify a subset of the model (a circuit) that explains most of the model's behavior for basic arithmetic logic and examine its functionality. By zooming in on the level of individual circuit neurons, we discover a sparse set of important neurons that implement simple heuristics. Each heuristic identifies a numerical input pattern and outputs corresponding answers. We hypothesize that the combination of these heuristic neurons is the mechanism used to produce correct arithmetic answers. To test this, we categorize each neuron into several heuristic types-such as neurons that activate when an operand falls within a certain range-and find that the unordered combination of these heuristic types is the mechanism that explains most of the model's accuracy on arithmetic prompts. Finally, we demonstrate that this mechanism appears as the main source of arithmetic accuracy early in training. Overall, our experimental results across several LLMs show that LLMs perform arithmetic using neither robust algorithms nor memorization; rather, they rely on a"bag of heuristics".</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8135.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8135.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3-8B (analyzed checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of a pre-trained transformer LM (Llama3 family) with 8B parameters showing high arithmetic accuracy via a sparse circuit of MLP neurons and a few attention heads; arithmetic answers are produced by combining many localized heuristic neurons rather than a single algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ARITHMETIC WITHOUT ALGORITHMS: LANGUAGE MODELS SOLVE MATH WITH A BAG OF HEURISTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer autoregressive LM (Llama3 family), ~8B parameters; uses gated MLP layers in transformer blocks and tokenizes multi-digit positive integers as single tokens up to ~1000 for this model. Public checkpoint, pre-trained (no arithmetic fine-tuning used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (binary ops): addition, subtraction, multiplication, integer division; prompts of the form 'op1 <op> op2 =' with operands and correct result constrained to single-token representations (operands in [0,300] in most experiments, tokenization limit ~1000).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>A sparse arithmetic circuit composed mainly of MLP layers (middle & late layers) and a few attention heads; attention heads copy individual token representations (op1, operator, op2) to the final position, and mid/late-layer MLP neurons act as key-value memory-like 'heuristic' neurons that (i) activate on specific numeric patterns (operand ranges, mod patterns, digit patterns, identical operands, multi-result sets) and (ii) contribute value vectors whose logits boost particular numeric answer tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Causal activation patching at component and neuron level; mean ablation (meaning non-circuit components); linear probing (Logit probes) from layer outputs to answer tokens; Logit Lens projection of MLP output-row vectors; neuron knockouts by heuristic type and by prompt (zeroing h_post activations); checkpoint-wise analysis across training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy on single-token-result prompts (operands ∈ [0,300]): addition 0.97, subtraction 0.96, multiplication 0.84, division 0.92; average 0.95. Circuit faithfulness (fraction of model performance explained by identified circuit) ≈0.96 average across operators; operator-wise faithfulness (Llama3-8B table): +:0.97, -:0.98, ×:0.90, ÷:0.96. Linear probe: correct-answer information extractable only at final position starting at layer 16.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Failures are not primarily due to lack of heuristics but due to insufficient promotion (low aggregate logit contribution) of the correct answer token by heuristics; heuristics have imperfect recall and low per-heuristic logits; ablation of one heuristic reduces accuracy but typically not to zero because prompts rely on multiple heuristics; mechanism does not generalize like an algorithm — cannot guarantee correctness across all inputs; tokenization (multi-digit tokens) affects the learned strategies (limitation acknowledged).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Activation patching localized arithmetic to a small set of MLP layers and a few attention heads; neuron-level patching shows only a few neurons (top ~200 per layer, ~1.5%) produce most causal effect; Logit Lens projection of MLP output rows shows value vectors embed numeric tokens consistent with activation patterns (direct heuristics encode result tokens; indirect heuristics encode features); targeted knockout by heuristic type degrades accuracy more on prompts associated with that heuristic than on unassociated prompts (average drop ~29% of 95% baseline when ablating a heuristic); prompt-guided ablation (removing heuristics associated with a prompt) drops accuracy far more than removing same-size random neuron sets; probe results indicate answer written into final token representation by mid/late MLPs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not all high-effect neurons could be classified into human-identifiable heuristics (~9% of top neurons unclassified); some heuristics have low recall and do not fire for all associated prompts; ablations do not fully eliminate correct answers due to redundancy and overlapping heuristics; multiplication required more attention heads (20) in circuit to reach high faithfulness compared to other ops (6 heads sufficient for +,-,÷); these findings question whether Fourier-feature explanations (cited elsewhere) fully explain the mechanism — Fourier is partial view per authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8135.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8135.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3-70B (analyzed checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Larger Llama3 model (70B) analyzed and found to implement a similar bag-of-heuristics arithmetic mechanism, with even stronger knockout effects and a richer heuristic set consistent with findings in the 8B model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ARITHMETIC WITHOUT ALGORITHMS: LANGUAGE MODELS SOLVE MATH WITH A BAG OF HEURISTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer autoregressive LM (Llama3 family), ~70B parameters; uses gated MLPs, modern training methodology; evaluated without arithmetic fine-tuning in same experimental setup as Llama3-8B (single-token-number prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (addition, subtraction, multiplication, integer division) on prompts 'op1 <op> op2 =' with single-token operands/results (operands constrained in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Same high-level mechanism: attention heads copy operand/operator tokens to final position; mid/late-layer gated-MLP neurons implement heuristic key–value behaviors (range, modulo, pattern, identical operands, multi-result etc.); combination of independent heuristics (bag) promotes correct answer logits.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching, neuron-level interventions, linear probing, Logit Lens, heuristic classification and knockout experiments replicated on this model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy on single-token-result prompts (operands ∈ [0,300]): addition 0.97, subtraction 0.99, multiplication 0.99, division 0.73; average 0.88. Authors report stronger knockout effects in Llama3-70B compared to other models, indicating a more developed bag of heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Despite very high accuracy on +/−/×, division accuracy lower (0.73) due to distributional skew and tokenization; heuristics still imperfect (recall & logit strength limitations) and do not guarantee algorithmic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Replicated middle/late-layer MLP neuron faithfulness and heuristic classification; prompt-guided knockout experiments show greater decrease in accuracy when ablating associated heuristic neurons than random neurons; circuit structure (attention heads + MLPs) similar to Llama3-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Division lower accuracy suggests operator-specific limitations; while heuristics are richer in larger model, they still fail to guarantee full algorithm-like generalization; authors hypothesize larger size produced a more sophisticated bag of heuristics rather than an algorithmic procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8135.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8135.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-6.9B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia-6.9B (analyzed checkpoint series)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mid-scale open model (6.9B) whose training checkpoints were analyzed to track emergence of heuristic neurons; shows the bag-of-heuristics mechanism emerges early and grows gradually across training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ARITHMETIC WITHOUT ALGORITHMS: LANGUAGE MODELS SOLVE MATH WITH A BAG OF HEURISTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia-6.9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>6.9B-parameter autoregressive transformer from the Pythia suite (public checkpoints available); uses simple MLPs (non-gated) in transformer blocks. Authors analyzed checkpoints from 23K to 143K steps to study heuristic development.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (addition, subtraction, multiplication, integer division) on single-token-result prompts (operands restricted in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Same bag-of-heuristics phenomenon: mid/late-layer MLP neurons functioning as key–value heuristics, attention heads copying inputs to final position. Heuristic neurons from final checkpoint are already present early in training and explain most heuristic-circuit behavior at intermediate checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching, neuron-level patching, linear probes, heuristic neuron classification across checkpoints, prompt-guided neuron knockout at multiple checkpoints to test causal role over training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy on single-token-result prompts (operands ∈ [0,300]): addition 0.30, subtraction 0.04, multiplication 0.27, division 0.75; average 0.43. (Lower arithmetic performance than Llama models at these operand ranges.)</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Low overall accuracy for some operators; heuristics emerge early but are imperfect and can be vestigial; distributional effects in integer division (many zeros) inflate apparent division performance; heuristics' imperfect recall and low logit contributions cause failures.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Checkpoint analysis: (i) many final-checkpoint heuristic neurons are present early (percent mutual increases across training), (ii) mutual heuristics explain ~79% of heuristic contribution at intermediate checkpoints, (iii) ablating associated heuristic neurons at early checkpoints substantially reduces accuracy — indicating heuristics are primary arithmetic mechanism across training.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Pythia shows lower baseline arithmetic capability, so heuristics alone sometimes insufficient; vestigial heuristics appear and later disappear, indicating training dynamics produce and prune heuristic features; division accuracy inflated by answer distribution biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8135.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8135.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J-6B (analyzed checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open 6B-parameter autoregressive transformer (GPT-J) analyzed and found to exhibit the bag-of-heuristics pattern in arithmetic, though with lower overall arithmetic accuracy and similar mechanistic signatures (attention copying + MLP heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ARITHMETIC WITHOUT ALGORITHMS: LANGUAGE MODELS SOLVE MATH WITH A BAG OF HEURISTICS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-J-6B autoregressive transformer (6B parameters) using simple MLP layers; evaluated on the same single-token-result arithmetic prompts without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand arithmetic (addition, subtraction, multiplication, integer division) with single-token operands/results used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Same high-level pattern: a small set of attention heads copy inputs to final position; mid/late MLP neurons act as heuristics (key–value memory-like neurons) whose activations and output embeddings boost particular numeric tokens according to numeric-pattern triggers.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching, neuron-level patching and ablations, linear probes, Logit Lens, heuristic classification and knockout experiments duplicated on GPT-J.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy on single-token-result prompts (operands ∈ [0,300]): addition 0.23, subtraction 0.09, multiplication 0.46, division 0.64; average 0.37.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Low addition/subtraction accuracy; similar failure modes to other models: heuristics have imperfect recall and weak logit contributions for some prompts, causing incorrect completions; distributional effects in division can elevate division accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Replication of circuit discovery (attention heads + MLPs), linear probe localization of answer information to final token after mid-layer MLPs, neuron-level faithfulness (sparse set of neurons explains most circuit effect), and prompt-guided heuristic ablation showing causal links.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Lower baseline accuracy makes it harder to draw strong claims about universality of heuristics; some operator-specific differences (e.g., multiplication behaviors) require more heads/neuron variety to reach high faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8135.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8135.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bag-of-Heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bag of Heuristics arithmetic mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed internal mechanism for arithmetic in pre-trained LLMs where many sparse MLP neurons implement simple, pattern-triggered heuristics (range, modulo, digit patterns, identical operands, multi-result) whose unordered additive contributions to answer-token logits collectively produce correct outputs, rather than a single robust algorithm or exhaustive memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mechanism (applies across analyzed LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but a mechanistic hypothesis instantiated as sparse MLP neurons (key activations) and their associated MLP output-row value vectors in mid/late layers; relies on attention heads that copy operand/operator tokens to final position so heuristic neurons can see combined context.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Explains behavior on two-operand arithmetic prompts (addition, subtraction, multiplication, integer division) where operands and result are represented as single tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>MLP neurons behave as key–value memory entries: keys (input-weight rows/up-projection activations) detect numeric patterns in operands/results and produce h_post activations; values (output-weight rows) embed logits for numeric tokens. Heuristics are of types: range, result-range, modulo, digit/pattern (regex on digits), identical-operands, multi-result (division), and indirect heuristics that encode features for downstream neurons. Attention heads copy individual token information to the final position to enable the MLP heuristics to act.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Discovery via activation patching (component, neuron-level), mean ablation of non-circuit components (faithfulness measure), linear probes (to find where answer is readable), Logit Lens to map value vectors to numeric-token logits, neuron knockout experiments by heuristic and by prompt, checkpoint (training-step) tracking of neuron emergence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mechanism explains majority of model performance: circuit faithfulness ~0.96 (Llama3-8B average); top ~200 neurons per layer (~1.5%) reproduce high faithfulness; ablating heuristic neurons associated with prompts causes large accuracy drops compared to random ablations (example: ablating a single heuristic type causes average 29% drop out of 95% baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Heuristic recall errors (neurons fail to fire when they should), low per-neuron/value logit magnitude (insufficient promotion), overlapping/partial heuristics producing noisy or conflicting logits, operator-specific needs (multiplication needs more heads), distributional artifacts (e.g., division skew), and limitations introduced by multi-digit tokenization (prevents digit-by-digit algorithmic strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Multiple converging causal experiments: circuit discovery (activation patching) localizes arithmetic to few components; neuron-level patching shows small neuron sets suffice; Logit Lens maps output-row vectors to numeric logits matching activation-trigger patterns; heuristic classification + targeted ablation shows causal, prompt-specific drops; checkpoint analysis shows heuristics emerge early and persist as primary mechanism across training.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Does not provide algorithm-like perfect generalization (models still make errors); some high-effect neurons resist human classification (~9% unclassified), suggesting either more complex heuristics or other mechanisms; other studies report Fourier-feature-like representations for addition — authors argue Fourier is a partial view that coexists with many other heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Pre-trained large language models use Fourier features to compute addition <em>(Rating: 2)</em></li>
                <li>Transformer feed-forward layers are key-value memories <em>(Rating: 2)</em></li>
                <li>Interpreting GPT: The logit lens <em>(Rating: 1)</em></li>
                <li>Arithmetic with language models: From memorization to computation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8135",
    "paper_id": "paper-273661714",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Llama3-8B",
            "name_full": "Llama3-8B (analyzed checkpoint)",
            "brief_description": "Analysis of a pre-trained transformer LM (Llama3 family) with 8B parameters showing high arithmetic accuracy via a sparse circuit of MLP neurons and a few attention heads; arithmetic answers are produced by combining many localized heuristic neurons rather than a single algorithm.",
            "citation_title": "ARITHMETIC WITHOUT ALGORITHMS: LANGUAGE MODELS SOLVE MATH WITH A BAG OF HEURISTICS",
            "mention_or_use": "use",
            "model_name": "Llama3-8B",
            "model_description": "Transformer autoregressive LM (Llama3 family), ~8B parameters; uses gated MLP layers in transformer blocks and tokenizes multi-digit positive integers as single tokens up to ~1000 for this model. Public checkpoint, pre-trained (no arithmetic fine-tuning used in experiments).",
            "arithmetic_task_type": "Two-operand arithmetic (binary ops): addition, subtraction, multiplication, integer division; prompts of the form 'op1 &lt;op&gt; op2 =' with operands and correct result constrained to single-token representations (operands in [0,300] in most experiments, tokenization limit ~1000).",
            "mechanism_or_representation": "A sparse arithmetic circuit composed mainly of MLP layers (middle & late layers) and a few attention heads; attention heads copy individual token representations (op1, operator, op2) to the final position, and mid/late-layer MLP neurons act as key-value memory-like 'heuristic' neurons that (i) activate on specific numeric patterns (operand ranges, mod patterns, digit patterns, identical operands, multi-result sets) and (ii) contribute value vectors whose logits boost particular numeric answer tokens.",
            "probing_or_intervention_method": "Causal activation patching at component and neuron level; mean ablation (meaning non-circuit components); linear probing (Logit probes) from layer outputs to answer tokens; Logit Lens projection of MLP output-row vectors; neuron knockouts by heuristic type and by prompt (zeroing h_post activations); checkpoint-wise analysis across training.",
            "performance_metrics": "Accuracy on single-token-result prompts (operands ∈ [0,300]): addition 0.97, subtraction 0.96, multiplication 0.84, division 0.92; average 0.95. Circuit faithfulness (fraction of model performance explained by identified circuit) ≈0.96 average across operators; operator-wise faithfulness (Llama3-8B table): +:0.97, -:0.98, ×:0.90, ÷:0.96. Linear probe: correct-answer information extractable only at final position starting at layer 16.",
            "error_types_or_failure_modes": "Failures are not primarily due to lack of heuristics but due to insufficient promotion (low aggregate logit contribution) of the correct answer token by heuristics; heuristics have imperfect recall and low per-heuristic logits; ablation of one heuristic reduces accuracy but typically not to zero because prompts rely on multiple heuristics; mechanism does not generalize like an algorithm — cannot guarantee correctness across all inputs; tokenization (multi-digit tokens) affects the learned strategies (limitation acknowledged).",
            "evidence_for_mechanism": "Activation patching localized arithmetic to a small set of MLP layers and a few attention heads; neuron-level patching shows only a few neurons (top ~200 per layer, ~1.5%) produce most causal effect; Logit Lens projection of MLP output rows shows value vectors embed numeric tokens consistent with activation patterns (direct heuristics encode result tokens; indirect heuristics encode features); targeted knockout by heuristic type degrades accuracy more on prompts associated with that heuristic than on unassociated prompts (average drop ~29% of 95% baseline when ablating a heuristic); prompt-guided ablation (removing heuristics associated with a prompt) drops accuracy far more than removing same-size random neuron sets; probe results indicate answer written into final token representation by mid/late MLPs.",
            "counterexamples_or_challenges": "Not all high-effect neurons could be classified into human-identifiable heuristics (~9% of top neurons unclassified); some heuristics have low recall and do not fire for all associated prompts; ablations do not fully eliminate correct answers due to redundancy and overlapping heuristics; multiplication required more attention heads (20) in circuit to reach high faithfulness compared to other ops (6 heads sufficient for +,-,÷); these findings question whether Fourier-feature explanations (cited elsewhere) fully explain the mechanism — Fourier is partial view per authors.",
            "uuid": "e8135.0",
            "source_info": {
                "paper_title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Llama3-70B",
            "name_full": "Llama3-70B (analyzed checkpoint)",
            "brief_description": "Larger Llama3 model (70B) analyzed and found to implement a similar bag-of-heuristics arithmetic mechanism, with even stronger knockout effects and a richer heuristic set consistent with findings in the 8B model.",
            "citation_title": "ARITHMETIC WITHOUT ALGORITHMS: LANGUAGE MODELS SOLVE MATH WITH A BAG OF HEURISTICS",
            "mention_or_use": "use",
            "model_name": "Llama3-70B",
            "model_description": "Transformer autoregressive LM (Llama3 family), ~70B parameters; uses gated MLPs, modern training methodology; evaluated without arithmetic fine-tuning in same experimental setup as Llama3-8B (single-token-number prompts).",
            "arithmetic_task_type": "Two-operand arithmetic (addition, subtraction, multiplication, integer division) on prompts 'op1 &lt;op&gt; op2 =' with single-token operands/results (operands constrained in experiments).",
            "mechanism_or_representation": "Same high-level mechanism: attention heads copy operand/operator tokens to final position; mid/late-layer gated-MLP neurons implement heuristic key–value behaviors (range, modulo, pattern, identical operands, multi-result etc.); combination of independent heuristics (bag) promotes correct answer logits.",
            "probing_or_intervention_method": "Activation patching, neuron-level interventions, linear probing, Logit Lens, heuristic classification and knockout experiments replicated on this model.",
            "performance_metrics": "Accuracy on single-token-result prompts (operands ∈ [0,300]): addition 0.97, subtraction 0.99, multiplication 0.99, division 0.73; average 0.88. Authors report stronger knockout effects in Llama3-70B compared to other models, indicating a more developed bag of heuristics.",
            "error_types_or_failure_modes": "Despite very high accuracy on +/−/×, division accuracy lower (0.73) due to distributional skew and tokenization; heuristics still imperfect (recall & logit strength limitations) and do not guarantee algorithmic generalization.",
            "evidence_for_mechanism": "Replicated middle/late-layer MLP neuron faithfulness and heuristic classification; prompt-guided knockout experiments show greater decrease in accuracy when ablating associated heuristic neurons than random neurons; circuit structure (attention heads + MLPs) similar to Llama3-8B.",
            "counterexamples_or_challenges": "Division lower accuracy suggests operator-specific limitations; while heuristics are richer in larger model, they still fail to guarantee full algorithm-like generalization; authors hypothesize larger size produced a more sophisticated bag of heuristics rather than an algorithmic procedure.",
            "uuid": "e8135.1",
            "source_info": {
                "paper_title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Pythia-6.9B",
            "name_full": "Pythia-6.9B (analyzed checkpoint series)",
            "brief_description": "Mid-scale open model (6.9B) whose training checkpoints were analyzed to track emergence of heuristic neurons; shows the bag-of-heuristics mechanism emerges early and grows gradually across training.",
            "citation_title": "ARITHMETIC WITHOUT ALGORITHMS: LANGUAGE MODELS SOLVE MATH WITH A BAG OF HEURISTICS",
            "mention_or_use": "use",
            "model_name": "Pythia-6.9B",
            "model_description": "6.9B-parameter autoregressive transformer from the Pythia suite (public checkpoints available); uses simple MLPs (non-gated) in transformer blocks. Authors analyzed checkpoints from 23K to 143K steps to study heuristic development.",
            "arithmetic_task_type": "Two-operand arithmetic (addition, subtraction, multiplication, integer division) on single-token-result prompts (operands restricted in experiments).",
            "mechanism_or_representation": "Same bag-of-heuristics phenomenon: mid/late-layer MLP neurons functioning as key–value heuristics, attention heads copying inputs to final position. Heuristic neurons from final checkpoint are already present early in training and explain most heuristic-circuit behavior at intermediate checkpoints.",
            "probing_or_intervention_method": "Activation patching, neuron-level patching, linear probes, heuristic neuron classification across checkpoints, prompt-guided neuron knockout at multiple checkpoints to test causal role over training.",
            "performance_metrics": "Accuracy on single-token-result prompts (operands ∈ [0,300]): addition 0.30, subtraction 0.04, multiplication 0.27, division 0.75; average 0.43. (Lower arithmetic performance than Llama models at these operand ranges.)",
            "error_types_or_failure_modes": "Low overall accuracy for some operators; heuristics emerge early but are imperfect and can be vestigial; distributional effects in integer division (many zeros) inflate apparent division performance; heuristics' imperfect recall and low logit contributions cause failures.",
            "evidence_for_mechanism": "Checkpoint analysis: (i) many final-checkpoint heuristic neurons are present early (percent mutual increases across training), (ii) mutual heuristics explain ~79% of heuristic contribution at intermediate checkpoints, (iii) ablating associated heuristic neurons at early checkpoints substantially reduces accuracy — indicating heuristics are primary arithmetic mechanism across training.",
            "counterexamples_or_challenges": "Pythia shows lower baseline arithmetic capability, so heuristics alone sometimes insufficient; vestigial heuristics appear and later disappear, indicating training dynamics produce and prune heuristic features; division accuracy inflated by answer distribution biases.",
            "uuid": "e8135.2",
            "source_info": {
                "paper_title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-J",
            "name_full": "GPT-J-6B (analyzed checkpoint)",
            "brief_description": "Open 6B-parameter autoregressive transformer (GPT-J) analyzed and found to exhibit the bag-of-heuristics pattern in arithmetic, though with lower overall arithmetic accuracy and similar mechanistic signatures (attention copying + MLP heuristics).",
            "citation_title": "ARITHMETIC WITHOUT ALGORITHMS: LANGUAGE MODELS SOLVE MATH WITH A BAG OF HEURISTICS",
            "mention_or_use": "use",
            "model_name": "GPT-J",
            "model_description": "GPT-J-6B autoregressive transformer (6B parameters) using simple MLP layers; evaluated on the same single-token-result arithmetic prompts without fine-tuning.",
            "arithmetic_task_type": "Two-operand arithmetic (addition, subtraction, multiplication, integer division) with single-token operands/results used in experiments.",
            "mechanism_or_representation": "Same high-level pattern: a small set of attention heads copy inputs to final position; mid/late MLP neurons act as heuristics (key–value memory-like neurons) whose activations and output embeddings boost particular numeric tokens according to numeric-pattern triggers.",
            "probing_or_intervention_method": "Activation patching, neuron-level patching and ablations, linear probes, Logit Lens, heuristic classification and knockout experiments duplicated on GPT-J.",
            "performance_metrics": "Accuracy on single-token-result prompts (operands ∈ [0,300]): addition 0.23, subtraction 0.09, multiplication 0.46, division 0.64; average 0.37.",
            "error_types_or_failure_modes": "Low addition/subtraction accuracy; similar failure modes to other models: heuristics have imperfect recall and weak logit contributions for some prompts, causing incorrect completions; distributional effects in division can elevate division accuracy.",
            "evidence_for_mechanism": "Replication of circuit discovery (attention heads + MLPs), linear probe localization of answer information to final token after mid-layer MLPs, neuron-level faithfulness (sparse set of neurons explains most circuit effect), and prompt-guided heuristic ablation showing causal links.",
            "counterexamples_or_challenges": "Lower baseline accuracy makes it harder to draw strong claims about universality of heuristics; some operator-specific differences (e.g., multiplication behaviors) require more heads/neuron variety to reach high faithfulness.",
            "uuid": "e8135.3",
            "source_info": {
                "paper_title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Bag-of-Heuristics",
            "name_full": "Bag of Heuristics arithmetic mechanism",
            "brief_description": "A proposed internal mechanism for arithmetic in pre-trained LLMs where many sparse MLP neurons implement simple, pattern-triggered heuristics (range, modulo, digit patterns, identical operands, multi-result) whose unordered additive contributions to answer-token logits collectively produce correct outputs, rather than a single robust algorithm or exhaustive memorization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "mechanism (applies across analyzed LLMs)",
            "model_description": "Not a model but a mechanistic hypothesis instantiated as sparse MLP neurons (key activations) and their associated MLP output-row value vectors in mid/late layers; relies on attention heads that copy operand/operator tokens to final position so heuristic neurons can see combined context.",
            "arithmetic_task_type": "Explains behavior on two-operand arithmetic prompts (addition, subtraction, multiplication, integer division) where operands and result are represented as single tokens.",
            "mechanism_or_representation": "MLP neurons behave as key–value memory entries: keys (input-weight rows/up-projection activations) detect numeric patterns in operands/results and produce h_post activations; values (output-weight rows) embed logits for numeric tokens. Heuristics are of types: range, result-range, modulo, digit/pattern (regex on digits), identical-operands, multi-result (division), and indirect heuristics that encode features for downstream neurons. Attention heads copy individual token information to the final position to enable the MLP heuristics to act.",
            "probing_or_intervention_method": "Discovery via activation patching (component, neuron-level), mean ablation of non-circuit components (faithfulness measure), linear probes (to find where answer is readable), Logit Lens to map value vectors to numeric-token logits, neuron knockout experiments by heuristic and by prompt, checkpoint (training-step) tracking of neuron emergence.",
            "performance_metrics": "Mechanism explains majority of model performance: circuit faithfulness ~0.96 (Llama3-8B average); top ~200 neurons per layer (~1.5%) reproduce high faithfulness; ablating heuristic neurons associated with prompts causes large accuracy drops compared to random ablations (example: ablating a single heuristic type causes average 29% drop out of 95% baseline).",
            "error_types_or_failure_modes": "Heuristic recall errors (neurons fail to fire when they should), low per-neuron/value logit magnitude (insufficient promotion), overlapping/partial heuristics producing noisy or conflicting logits, operator-specific needs (multiplication needs more heads), distributional artifacts (e.g., division skew), and limitations introduced by multi-digit tokenization (prevents digit-by-digit algorithmic strategies).",
            "evidence_for_mechanism": "Multiple converging causal experiments: circuit discovery (activation patching) localizes arithmetic to few components; neuron-level patching shows small neuron sets suffice; Logit Lens maps output-row vectors to numeric logits matching activation-trigger patterns; heuristic classification + targeted ablation shows causal, prompt-specific drops; checkpoint analysis shows heuristics emerge early and persist as primary mechanism across training.",
            "counterexamples_or_challenges": "Does not provide algorithm-like perfect generalization (models still make errors); some high-effect neurons resist human classification (~9% unclassified), suggesting either more complex heuristics or other mechanisms; other studies report Fourier-feature-like representations for addition — authors argue Fourier is a partial view that coexists with many other heuristics.",
            "uuid": "e8135.4",
            "source_info": {
                "paper_title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Pre-trained large language models use Fourier features to compute addition",
            "rating": 2,
            "sanitized_title": "pretrained_large_language_models_use_fourier_features_to_compute_addition"
        },
        {
            "paper_title": "Transformer feed-forward layers are key-value memories",
            "rating": 2,
            "sanitized_title": "transformer_feedforward_layers_are_keyvalue_memories"
        },
        {
            "paper_title": "Interpreting GPT: The logit lens",
            "rating": 1,
            "sanitized_title": "interpreting_gpt_the_logit_lens"
        },
        {
            "paper_title": "Arithmetic with language models: From memorization to computation",
            "rating": 1,
            "sanitized_title": "arithmetic_with_language_models_from_memorization_to_computation"
        }
    ],
    "cost": 0.01515975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ARITHMETIC WITHOUT ALGORITHMS: LANGUAGE MODELS SOLVE MATH WITH A BAG OF HEURISTICS
20 May 2025</p>
<p>Yaniv Nikankin yaniv.n@cs.technion.ac.il 
Technion -Israel Institute of Technology</p>
<p>Anja Reusch 
Technion -Israel Institute of Technology</p>
<p>Aaron Mueller 
Technion -Israel Institute of Technology</p>
<p>Northeastern University</p>
<p>Yonatan Belinkov 
Technion -Israel Institute of Technology</p>
<p>ARITHMETIC WITHOUT ALGORITHMS: LANGUAGE MODELS SOLVE MATH WITH A BAG OF HEURISTICS
20 May 20254DC485464346B46C36DE2F1DCF0B348FarXiv:2410.21272v2[cs.CL]
Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data?To investigate this question, we use arithmetic reasoning as a representative task.Using causal analysis, we identify a subset of the model (a circuit) that explains most of the model's behavior for basic arithmetic logic and examine its functionality.By zooming in on the level of individual circuit neurons, we discover a sparse set of important neurons that implement simple heuristics.Each heuristic identifies a numerical input pattern and outputs corresponding answers.We hypothesize that the combination of these heuristic neurons is the mechanism used to produce correct arithmetic answers.To test this, we categorize each neuron into several heuristic types-such as neurons that activate when an operand falls within a certain range-and find that the unordered combination of these heuristic types is the mechanism that explains most of the model's accuracy on arithmetic prompts.Finally, we demonstrate that this mechanism appears as the main source of arithmetic accuracy early in training.Overall, our experimental results across several LLMs show that LLMs perform arithmetic using neither robust algorithms nor memorization; rather, they rely on a "bag of heuristics". 1</p>
<p>INTRODUCTION</p>
<p>Do large language models (LLMs) implement robust reusable algorithms to solve tasks, or are they merely memorizing aspects of the training distribution?This distinction is crucial (Tänzer et al., 2022;Henighan et al., 2023): while memorization might suffice for limited problem sets, true algorithmic comprehension allows for generalization and efficient scaling to new problems.</p>
<p>Arithmetic reasoning provides a lens for this investigation, as it can be solved using various methods: learning known algorithms, developing novel approaches, or by memorizing vast quantities of inputoutput pairs.Thus, we ask the following: Do LLMs implement robust algorithms to correctly complete arithmetic prompts, similar to children learning vertical addition to add two numbers, or do LLMs merely memorize the arithmetic prompts that appear in their vast training data?</p>
<p>Previous studies have made progress in identifying arithmetic mechanisms in LLMs.Stolfo et al. (2023) and Zhang et al. (2024) have identified a subset of model components (a circuit) responsible for arithmetic calculations in several LLMs and characterized the information flow between them.Zhou et al. (2024) suggested that pre-trained LLMs use features in Fourier space to accurately answer addition prompts.However, Stolfo et al. (2023) and Zhang et al. (2024) stopped short of elucidating the mechanism implemented by the circuit they identified-a required feat to understand the trade-off between generalization and memorization in this task.Zhou et al. (2024) studied addition prompts in models that were fine-tuned on arithmetic data.Their findings regarding Fourier features are significant, but we claim these represent only a part of a more complex mechanism.Our work aims to bridge these gaps: we investigate how the arithmetic circuit works qualitatively-and specifically, whether it implements a mathematical algorithm or memorizes arithmetic training data.</p>
<p>To do so, we reverse-engineer the arithmetic mechanism applied by LLMs.We use causal analysis to examine their arithmetic circuits, focusing on individual neurons within the circuit responsible for generating the correct answer.Our analysis reveals that a sparse subset of neurons is sufficient for accurate responses, with each neuron implementing a distinct heuristic.Each heuristic fires for a specific pattern in input operands or in their combination, and some increase the logits of relevant result tokens accordingly.For instance, one heuristic (Figure 1b) increases logits of tokens between 150 and 180 for subtraction prompts whose answer falls within this range.By examining these neurons, we classify each into one or more heuristic types.For example, the mentioned neuron (Figure 1b) falls within the type of result-range heuristics, which promote continuous value ranges.We discover that successful prompt completion relies on a combination of several unrelated heuristic types, forming a "bag of heuristics" approach.This finding suggests that LLMs may not be employing a single, cohesive algorithm for arithmetic reasoning, nor are they memorizing all possible inputs and outputs; rather, they deploy a collection of simpler rules and patterns.</p>
<p>We investigate if the bag of heuristics emerges as the primary arithmetic mechanism from the onset of training, or whether it overrides an earlier mechanism.To do that, we analyze how the heuristics evolve over the course of training.We show arithmetic heuristics appear throughout the model training, gradually converging towards the heuristics observed in the final checkpoint.Furthermore, we provide evidence that the bag of heuristics mechanism explains most of the model's behavior even in early stages, indicating it is the main mechanism used for solving arithmetic prompts.</p>
<p>We contribute by providing a high-resolution understanding of the mechanism that LLMs use to answer arithmetic prompts.We (i) show pre-trained LLMs implement a "bag of heuristics" approach, (ii) investigate when and why this mechanism fails to generalize, and (iii) discover how it emerges across training.This allows us to better understand the source of current capabilities and limitations of LLMs in arithmetic reasoning-a finding that could apply to additional reasoning tasks.</p>
<p>ARITHMETIC CIRCUIT DISCOVERY</p>
<p>In transformer-based LLMs, a circuit (Elhage et al., 2021) refers to a minimal subset of interconnected model components (multi-layer perceptrons (MLP) or attention heads) that perform the computations required for a specific task.We locate, and later analyze, the circuit responsible for arithmetic calculations.</p>
<p>CIRCUIT DISCOVERY AND EVALUATION</p>
<p>Models and Data</p>
<p>We analyze four LLMs: Llama3-8B/70B (Dubey et al., 2024), Pythia-6.9B(Biderman et al., 2023), and GPT-J (Wang &amp; Komatsuzaki, 2021).For each, we locate and analyze the circuit responsible for arithmetic calculations.We focus on Llama3-8B in the main paper and report similar results for the additional models in Appendix I. We use pre-trained models without fine-tuning them on arithmetic prompts, as our goal is to uncover the mechanisms induced by typical language model training.Each model tokenizes positive numbers as a single token, up to some limit; e.g., in Llama3-8B, numbers in [0, 1000] are tokenized to a single token.To locate the arithmetic circuit, we use two-operand arithmetic prompts with Arabic numerals and the four basic operators (+, −, ×, ÷), such that each prompt has four tokens: op1, the operator, op2, and the "=" sign.We sample a list of 100 prompts per operator for circuit discovery, and an identical amount for evaluation.Each prompt is chosen so that both its operands and result will be tokenized to a single token; e.g., in Llama3-8B, the operands and result must be between 0 and 1, 000.Unlike previous studies (Stolfo et al., 2023), we do not use in-context prompting, to ensure the circuit does not include any component not directly linked to arithmetic calculations.To reduce noise and ensure the circuits only contain components responsible for correct arithmetic completions, we only use prompts that are correctly completed by the model, similar to previous studies (Wang et al., 2022;Prakash et al., 2024).Throughout the paper, the prompt "226 − 68 =" is used as a running example.</p>
<p>Method To locate the circuit components, we conduct a series of activation patching experiments (Vig et al., 2020) that allow us to assess the importance of each MLP and attention head at each sequence position.Each experiment involves sampling a prompt p with result r from the dataset (for example, "226 − 68 ="), and sampling a random counterfactual prompt p ′ that leads to a different result r ′ (for example, "21 + 17 =").After pre-computing the activations of the model for the counterfactual prompt p ′ , we introduce the prompt p to the model.We intervene on ("patch") the computation, which means we replace the activation of a single MLP layer or attention head with its pre-computed activation for p ′ .Following Stolfo et al. (2023), we observe how this intervention affects the probabilities of both answer tokens, r and r ′ , by measuring the following:
E(r, r ′ ) = 1 2 P * (r ′ ) − P(r ′ ) P(r ′ ) + P (r) − P * (r) P * (r)(1)
where P and P * are the pre-and post-intervention probability distributions, respectively.The two summands in Equation (1) increase if patching raises the probability of r ′ or decreases the probability of r, respectively.High effect for an intervention on a component indicates its high importance in prompt calculation.The effect is averaged across prompts and measured separately per component.</p>
<p>Results</p>
<p>The patching results, shown in Figure 2a, reveal that the MLP layers affect the output probabilities more than the attention heads.The first MLP affects the representation at the operator and operand positions (see Appendix B.1), while middle-and late-layer MLPs exhibit a strong effect at the final position, likely reflecting their role in predicting the answer token in that position (further discussed in Section 2.2). Figure 2a also shows that very few attention heads are important to the circuit.Each such attention head copies information from a single position (either an operand or operator position) to the final position (see Appendix B.2). Figure 2b summarizes the information flow within the circuit, consistent in structure with prior work (Stolfo et al., 2023).</p>
<p>To evaluate the circuit c, we measure its faithfulness (Wang et al., 2022), the proportion of the full model's behavior on arithmetic prompts that can be explained solely by the circuit.To measure faithfulness, we first pre-compute mean activations for each model component (in each position) across all arithmetic prompts.We then intervene on the evaluation prompts by replacing non-circuit component activations with their means.To quantify performance, we measure NL(c), the logit of the correct answer token normalized by the maximal logit, as a proxy for accuracy, when meanablating all components not in the circuit c.The circuit's faithfulness is calculated as:
F(c) = NL(c) − NL(∅) NL(M) − NL(∅) (2)
where M is the entire model and NL(M) is the normalized correct-answer logit when no component is ablated (always 1.0 for correctly completed prompts).NL(∅) is the normalized correct answer logit when all components are mean-ablated.This formula normalizes faithfulness to a [0.0, 1.0] range.</p>
<p>The circuit achieves a high faithfulness of 0.96 on average across the four arithmetic operators; i.e., the circuit accounts for 96% of the entire model's performance.We can therefore conclude that the components identified in this section comprise the arithmetic circuit, and explain most of the model's accuracy for arithmetic prompts.See Appendix A for results across various circuit sizes, and a discussion of Pareto-optimality with respect to faithfulness and size.The linear probes are successful in extracting the correct answer from the final position, starting at layer 16.</p>
<p>IDENTIFYING ANSWER-PROMOTING COMPONENTS</p>
<p>To understand the mechanism implemented by the circuit to promote the correct answer, we first search for the specific circuit components that increase the probability of the correct answer.For this, we employ linear probing (Belinkov, 2022).For each layer l and sequence position p, we train a linear classifier f l,p : R d → R 1000 (where d is the dimension of each layer's output representation) using a training set of correctly completed arithmetic prompts.We pass these prompts through the model and calculate the output representation h l,p ∈ R d at layer l and position p for each prompt.The classifier f l,p receives h l,p as input, and outputs a probability distribution over the 1, 000 possible arithmetic answers.The classifier f l,p is evaluated on a separate test set of correctly completed prompts, showing to what extent the correct answer can be extracted from the output representation at layer l and position p.</p>
<p>We find that the answer can only be extracted with high accuracy from the final position (Figure 3), after the token representation is processed by the later layers of the model, starting from layer l = 16.</p>
<p>Given that the arithmetic circuit contains only MLPs in these layers (Figure 2), this suggests that these MLPs in layers [16,32] are the components that write the correct answer into the representation at the last position.The following section zooms into these middle-and late-layer MLPs, and presents evidence for the role they play in generating the correct answer-specifically, in how they promote the correct answer token through a combination of many independent arithmetic heuristics.</p>
<p>MLP NEURONS IMPLEMENT ARITHMETIC HEURISTICS</p>
<p>DECOMPOSING CIRCUIT MLPS TO INDIVIDUAL NEURONS</p>
<p>Having shown that the model generates the arithmetic answer in middle-and late-layer MLPs at the final position p = 4, we zoom in on these MLPs and their calculations at this position to investigate the implemented mechanism.The MLP at layer l can be described by the following equation:
h l out = MLP in h l in • W l out = h l post • W l out = d mlp n=0 h l,n post v l,n out(3)
where h l in , h l out ∈ R d are the input and output representations of the MLP at layer l, respectively.h l post ∈ R d mlp is the output of the up-projection of the MLP,2 where we define the n th value h  Figure 4: Analyzing effect of individual circuit MLP neurons.Our results demonstrate that a small amount of neurons is required to correctly predict the result.</p>
<p>R as a neuron.W l out ∈ R d mlp ×d is the output projection matrix, and v l,n out is its n th row vector.Biases are omitted.By expressing the output representation h l out as a linear combination of row vectors v l,n out and their corresponding neuron activations h l,n post , we can identify the neurons that most affect the completion of arithmetic prompts.</p>
<p>To measure the effect of each neuron h l,n post , we perform activation patching experiments on individual neurons (as described in Section 2.1), and measure the average effect across prompts.We observe that very few neurons have a high effect; an example for layer l = 17 is shown in Figure 4a.Additionally, we notice the neurons with the highest effect are different between operators.In fact, roughly 45% of the important neurons for each operator are unique (Appendix D).Thus, when analyzing the circuit at the neuron level, we analyze it as 4 separate circuits-one for each arithmetic operator.We hypothesize that for each operator, the highest-effect neurons are sufficient to explain most of the model's arithmetic behavior.To verify this, we measure the faithfulness of the arithmetic circuit when mean ablating non-circuit components and lower effect MLP neurons in middle-and late-layer MLPs.The results (Figure 4b) confirm that only 200 neurons (roughly 1.5%) per layer are needed to achieve high faithfulness and correctly compute arithmetic prompts.</p>
<p>MLP NEURONS ACT AS MEMORIZED HEURISTICS</p>
<p>To understand how the top-200 middle-and late-layer important MLP neurons contribute to the generation of correct answers, we view them as key-value memories (Geva et al., 2021). 3In this view, the input to each MLP layer h l in is multiplied by a key (a row vector in the MLP input weight matrix) to generate a neuron activation h l,n post that determines how strongly does a value (a row vector v l,n out in the MLP output weight matrix) gets written to the MLP output (Equation ( 3)).Geva et al. (2021) demonstrated that keys correspond to specific topics or n-grams, triggering high neuron activations when these are given as input, and their corresponding values represent tokens that can serve as appropriate completions for these topics or n-grams.Building on this insight, we hypothesize that (i) in arithmetic contexts, keys correspond to numerical patterns, e.g., a neuron might activate strongly when both operands in an arithmetic operation are odd numbers; and (ii) the associated value vectors encode numerical tokens that represent plausible answers to the key patterns.</p>
<p>To test the first hypothesis, we investigate the activation pattern of the top-200 neurons in each layer.</p>
<p>For each neuron l, n, we plot the activations h l,n post (at position p = 4) as a function of operand values, separately for each operator.We find that many neurons in the arithmetic circuit exhibit distinct, human-identifiable patterns.For instance, in "226 − 68 =", neuron h 24,12439 post shows high activation values for subtraction prompts with results between 150 and 180 (Figure 1).Additional examples are provided in Appendix J.</p>
<p>To verify the second hypothesis, we check whether the tokens embedded in the value vectors of the top neurons relate to their activation patterns.Using the Logit Lens (nostalgebraist, 2020), a method of projecting a vector v ∈ R d onto a probability distribution over the vocabulary space P d vocab , we project each value vector v l,n out to find the numerical tokens whose logits are highest.This reveals two distinct patterns: First, in some neurons, the activation pattern depends on both operands and the value vector encodes the expected result of the arithmetic calculation (Figure 1b,c).We term such neurons direct heuristics.Second, in neurons where the activation pattern depends on a single operand, the value vectors often encode features for downstream processing, rather than the result tokens directly (Figure 1a).We term such neurons indirect heuristics.Next, we demonstrate how these heuristic neurons combine to produce correct arithmetic answers.</p>
<p>ARITHMETIC PROMPTS ARE ANSWERED WITH A BAG OF HEURISTICS</p>
<p>Observing the example prompt, "226 − 68 =", we have shown that it satisfies the pattern of several heuristic neurons, where each such neuron slightly increases the logit of the result token, r = 158 (Figure 1).These small increases combine to promote the correct token as the final answer.We hypothesize that a combination of independent heuristics-termed a bag of heuristics-emerges across arithmetic prompts, comprising the mechanism used by the model to produce correct answers.</p>
<p>CLASSIFYING NEURONS TO HEURISTIC TYPES</p>
<p>To present evidence for the causal effect of the bag of heuristics on generating correct answers, we first systematically classify neurons into heuristic types.Through manual observation of key activation patterns, we identify several categories of human-identifiable heuristics, exemplified in Figure 5, and further detailed in Appendix E. To determine if a neuron n at layer l implements a specific heuristic, we examine the intersection between the prompts that activate the neuron and the prompts expected to be activated for this heuristic.A visual example of this procedure is shown in Figure 6.An automated algorithm of this approach is described in Appendix F.   Figure 7: For each heuristic, we measure the accuracy of 100 correctly completed prompts associated with a heuristic (blue) and 100 correctly completed prompts not associated with the heuristic (yellow), after ablating that heuristic's neurons.The heuristics are sorted by the accuracy drop induced on associated prompts.Across most heuristics, ablating heuristic neurons causes a larger decrease in accuracy in prompts associated with that heuristic than in not associated prompts.</p>
<p>(200 per layer across 16 layers).Manual inspection of neurons that fail to classify into one of the defined heuristics reveals patterns that are not clearly identifiable (Appendix J).</p>
<p>HEURISTIC TYPES ARE COMBINED TO ANSWER ARITHMETIC PROMPTS</p>
<p>Following the classification of the important neurons in each middle-and late-layer MLP into one or several heuristic types, we provide evidence that the bag of heuristics is the primary mechanism the model uses to correctly answer arithmetic prompts.We show this through two ablation experiments.</p>
<p>Knocking out neurons by heuristic type.We now verify that the neurons in each heuristic type contribute to the accuracy of associated prompts by knocking out entire heuristic types and observe the resulting changes in model accuracy.We define a prompt as associated with a heuristic if and only if its components meet the conditions specified by that heuristic.(For instance, the example prompt "226 − 68 =" is associated with the heuristic "op1 ≡ 0 (mod 2)".)For each heuristic, we sample two sets of 100 correctly completed prompts each, one containing prompts associated with the heuristic and the other containing prompts not associated with that heuristic.For each heuristic, we knock out all neurons classified into it (by setting each h l,n post activation to zero) and then remeasure the accuracy on both sets of prompts.We expect a higher decrease in accuracy on the associated prompts, since we claim each heuristic is causally linked only to its associated prompts.</p>
<p>The results (Figure 7) show that ablating neurons of a specific heuristic causes a significant accuracy drop on associated prompts, more than on not associated prompts, on average.This confirms the causal importance of heuristic neurons in promoting correct answer logits specifically in prompts that are associated with their heuristic type, verifying the targeted functional role of these heuristics.</p>
<p>However, the ablation does not result in a complete accuracy drop; it causes an average drop of 29% out of 95% average pre-ablation accuracy.We find two reasons for this.First, some heuristics have low recall: they do not apply to all associated prompts as they should (see Appendix J).Second, each prompt relies on several unrelated heuristic types, so even when one is ablated, others still contribute to increasing the correct answer's logit.In the following ablation experiment, we verify that this interplay of heuristics provides a fuller image than focusing on one heuristic at a time.</p>
<p>Knocking out neurons by prompt.To provide further evidence that the bag of heuristics is causally linked to correct arithmetic completion, we conduct a second ablation experiment.For each correct prompt, we identify the heuristic types that should affect it based on its operands and ground truth result.We then ablate the neurons with the highest classification scores (Section 4.1) in these heuristics, up to a certain neuron count, and check if the model's completion changes.</p>
<p>The results (Figure 8) show that ablating neurons from associated heuristics significantly drops the model's accuracy, much more than the accuracy drop caused by ablating the same number of randomly chosen neurons from unassociated heuristics.This demonstrates that we can identify the neurons important to a given prompt solely based on its associated heuristics.This also indicates a causal link between the neurons belonging to several heuristics and the prompt's correct completion.This supports our bag of heuristics claim: each heuristic only slightly boosts the correct answer logit, but combined, they cause the model to produce the correct answer with high probability.Figure 8: Knocking out neurons that implement heuristics associated with each prompt (full lines) leads to a greater decrease in accuracy than knocking out the same number of neurons whose heuristics are not associated with each prompt (dashed lines).This effect occurs across model sizes.</p>
<p>FAILURE MODES OF THE BAG OF HEURISTICS</p>
<p>The bag of heuristics mechanism employed in Llama3-8B does not generalize perfectly: it fails to achieve perfect accuracy across all arithmetic prompts (Appendix H).This limitation contrasts with the theoretical robustness of a genuine algorithmic approach.Here, we aim to elucidate the specific failures of this mechanism, focusing on why it falters for some prompts.</p>
<p>We hypothesize that the bag of heuristics mechanism completes prompts incorrectly in two ways.</p>
<p>(1) The "bag" might not be big enough; i.e., a prompt might lack sufficient associated neurons.( 2) the heuristics might have imperfect recall (e.g., a neuron that fires for most prompts where the first operand is even, but does not fire for the prompt "226 − 68 =") or have low logits for the correct answer token in the value vectors.To test these hypotheses, we randomly sample 50 correctly completed and 50 incorrectly completed prompts.To test hypothesis (i), we count the number of heuristic neurons associated with each prompt.We find that on average, incorrect prompts have more heuristic neurons associated with them than correct prompts.Therefore, we find no support for this hypothesis.To check hypothesis (ii), we calculate the total contribution of all heuristic neurons to the logit of the correct answer for each prompt.This measurement considers both the specific activation of each neuron for the prompt, as well as the logit of the correct answer token embedded in each neuron's value vector.On average, there is indeed a slight advantage in the total logit contribution for correct prompts over incorrect prompts (Figure 9).This suggests that the primary reason for the bag of heuristics failure on certain prompts is poor promotion of correct answer logits, rather than a lack of heuristics.</p>
<p>TRACKING HEURISTICS DEVELOPMENT ACROSS TRAINING STEPS</p>
<p>Does the bag of heuristics emerge as the primary arithmetic mechanism from the onset of training, or does it override an earlier, different mechanism that initially drives arithmetic performance?We conduct an analysis of heuristic development across the training trajectory of the Pythia-6.9Bmodel (Biderman et al., 2023), due to the public availability of its training checkpoints.Specifically, we analyze the model at its final checkpoint (143K steps) and at 10K-step intervals down to 23K steps.The 23K checkpoint is the earliest checkpoint showing good arithmetic performance; Thus, we begin our analysis at this checkpoint.To guide this analysis, we aim to answer three sub-questions:</p>
<p>When do the final heuristic neurons first appear?We examine when each heuristic neuron from the final checkpoint first appears during training.For each neuron classified into a particular heuristic type, we check if the same neuron gets classified into the same heuristic in earlier checkpoints.Averaging this measure across all heuristic types and operators provides insight into when the final heuristics initially appear during training.We observe (Figure 10a) that the model develops its final heuristic mechanism gradually across training, starting from an early checkpoint.Do additional heuristic neurons exist mid-training?Next, for each mid-training checkpoint, we investigate whether its heuristic neurons that are mutual with the final checkpoint make up the entire heuristic mechanism in that checkpoint, or whether other heuristics exist that later become vestigial.We examine the faithfulness (Section 2.1) of the arithmetic circuit at each checkpointonce when including only the neurons mutual with the final checkpoint, and once when including all heuristic neurons in that checkpoint.The difference between these two measurements gives us an estimate of the importance of the mutual heuristics.Using this metric, we observe that these final heuristics explain most of the circuit performance for each intermediate checkpoint: they account for an average of 79% of the total heuristics' contribution to accuracy at each checkpoint.This indicates that, while other non-mutual heuristics exist in each checkpoint, these are less important to the circuit's accuracy and slowly become vestigial as the circuit converges to its final form.</p>
<p>Does a competing arithmetic mechanism exist mid-training?Finally, we determine if the heuristics appear as the main arithmetic mechanism from early on in training, or if they co-exist with an unrelated mechanism that becomes vestigial in later checkpoints.We repeat the prompt-guided neuron knockout experiment (Section 4.2) for each checkpoint; i.e., in each checkpoint, we sample 50 correctly completed prompts for each operator.For each prompt, we ablate 5, 10, and 25 heuristic neurons associated with the prompt in that checkpoint.We test if this targeted ablation significantly impairs the model's accuracy, even in earlier stages of training, and compare this to a baseline, where we ablate a similar amount of randomly chosen heuristic neurons.The results (Figure 10c) demonstrate that removing any amount of neurons from heuristics associated with a prompt substantially reduces the model's accuracy on these prompts even at earlier checkpoints, much more than ablating a random set of important neurons.We also observe that ablating 25 heuristic neurons per layer is enough to cause near-zero accuracy in all stages of training.This finding asserts that the causal link between a prompt's associated heuristics and its correct completion exists throughout training.</p>
<p>RELATED WORK</p>
<p>Mechanistic interpretability (MI) aims to reverse-engineer mechanisms implemented by LMs by analyzing model weights and components.Causal mediation techniques (Pearl, 2001) like activation patching (Vig et al., 2020;Geiger et al., 2021), path patching (Wang et al., 2022), and attribution patching (Nanda, 2022;Syed et al., 2023;Hanna et al., 2024b) allow localizing model behaviors to specific model components.Other studies have also presented techniques to explain the effect of specific weight matrices on input tokens (Elhage et al., 2021;Dar et al., 2023), or to analyze activations (nostalgebraist, 2020;Geva et al., 2021).Many studies have aimed to use these techniques to reverse-engineer specific behaviors of pre-trained LMs (Wang et al., 2022;Hanna et al., 2024a;Gould et al., 2024;Hou et al., 2023).We leverage MI techniques to reverse-engineer the arithmetic mechanisms implemented by pre-trained LLMs and explain them at a single-neuron resolution.</p>
<p>Memorization and generalization in LLMs.Whether models memorize training data or generalize to unseen data has been extensively studied in deep learning (e.g., Zhang et al., 2021) and specifically in LLMs (Tänzer et al., 2022;Carlini et al., 2023;Antoniades et al., 2024), but not many studies have observed this question through the lens of model internals.Among those that do, Bansal et al. (2022) attempt to predict this trade-off by observing the diversity of internal activations; Dankers &amp; Titov (2024) show memorization in language classification tasks is not local to specific layers, and Varma et al. (2023) explain grokking using memorizing and generalizing circuits.We use this lens to observe how model internals operate in arithmetic reasoning-a task that could theoretically be solved either through extensive memorization or by learning a robust algorithm.Concurrent work (jylin et al., 2024) has shown that a LM trained to predict legal board game moves (Li et al., 2022) does so by implementing many heuristics.While heuristics would suffice to robustly predict legal moves in a board game setting, we find that the extent to which LLMs rely on heuristics is greater than prior work suggests: sets of heuristics are used to accomplish even generic tasks like arithmetic, where no heuristic is likely to generalize to all possible results.</p>
<p>Arithmetic reasoning interpretability.</p>
<p>Recent studies on how LMs process arithmetic prompts (Stolfo et al., 2023;Zhang et al., 2024) reveal the general structure of arithmetic circuits, but do not fully explain how they combine operand information to produce correct answers.Our research bridges this gap by revealing the mechanism used for promoting the correct answer.Some studies show the emergence of mathematical algorithms for modular addition (Nanda et al., 2023;Zhong et al., 2024;Ding et al., 2024) and binary arithmetic (Maltoni &amp; Ferrara, 2023) in simple, specialized toy LMs, but it is unclear if these findings extend to larger, general-purpose LMs or other operators.</p>
<p>In pre-trained LLMs, Zhou et al. (2024) found that Fourier space features are used for addition.However, we claim this is only a partial view, as many additional types of features and heuristics relying on these features are involved in calculating answers across arithmetic operations.In this work, we give a wide view of these heuristics and how they combine to generate arithmetic answers.</p>
<p>CONCLUSIONS</p>
<p>Do LLMs rely on a robust algorithm or on memorization to solve arithmetic tasks?Our analysis suggests that the mechanism behind the arithmetic abilities of LLMs is somewhere in the middle: LLMs implement a bag of heuristics-a combination of many memorized rules-to perform arithmetic reasoning.To reach this conclusion, we performed a set of causal analysis experiments to locate a circuit, i.e., a subset of model components, responsible for arithmetic calculations.We examined the circuit at the level of individual neurons and pinpointed the arithmetic calculations to a sparse set of MLP neurons.We showed that each neuron acts as a memorized heuristic, activating for a specific pattern of inputs, and that the combination of many such neurons is required to correctly answer the prompts.In addition, we found that this mechanism gradually evolves over the course of training, emerging steadily rather than appearing abruptly or replacing other mechanisms.</p>
<p>Our results, showing LLMs' reliance on the bag of heuristics, suggest that improving LLMs' mathematical abilities may require fundamental changes to training and architectures, rather than post-hoc techniques like activation steering (Subramani et al., 2022;Turner et al., 2023).Additionally, the evolution of this mechanism across training indicates that models learn these heuristics early and reinforce them over time, potentially overfitting to early simple strategies; it is unclear if regularization can improve this, and this is a possible avenue for future research.</p>
<p>LIMITATIONS AND DISCUSSION</p>
<p>Interpretability work is often fundamentally limited by human biases.As researchers, we often impose human abstractions onto models, whereas the goal of interpretability is to understand the abstractions that models learn and apply in a way that we can understand.Our work is also subject to this limitation, namely with respect to the definition of heuristic types: We define heuristic abstractions based on our human-identifiable definitions.A possible improvement would be to develop a method to identify these abstractions without human bias.Another important detail is that our analysis focuses on LLMs that combine digits in tokenization.That is, every token can contain more than one digit.The robust algorithms used by humans depend on our ability to separate larger numbers to single digits.Thus, a similar analysis might lead to different conclusions for models that perform single-digit tokenization.In Figure 11, we present an analysis of the faithfulness of the identified circuit in Llama3-8B (Section 2.1) as a function of the number of attention heads within the circuit.The circuit includes all MLPs (the neuron-level analysis in Section 3.1 does not apply in this context).Our goal is to find a minimal subset of the model with the fewest attention heads possible while maintaining high faithfulness.We assess faithfulness independently for each operator to obtain a more nuanced understanding of the necessary attention heads.For addition, subtraction, and division operations, we observe that 6 heads suffice to attain high faithfulness (97% on average).In contrast, in multiplication, 20 attention heads are required to achieve a faithfulness score exceeding 90%.The faithfulness of the full arithmetic circuit for each operator, corresponding to the Pareto-optimal number of heads (in terms of achieving high faithfulness with as few heads as possible), is documented in Table 1.We explore the attention patterns of these attention heads in the next section.</p>
<p>B LLAMA3-8B CIRCUIT ADDITIONAL COMPONENTS</p>
<p>To provide a more comprehensive understanding of the arithmetic circuit in Llama3-8B, we analyze the additional components that compose it, namely the first MLP layer (MLP0) and the high-effect attention heads.</p>
<p>B.1 MLP0</p>
<p>To analyze the role of MLP0 in the arithmetic circuit, we first test if-similarly to the middle-and late-layer MLPs-only a sparse set of neurons within the MLP is required.We measure the intervention effect of each neuron, averaged across the operands and operator positions (p ∈ [1, 2, 3]).</p>
<p>The results, shown in Figure 12a, reveal that few MLP0 neurons have a high effect, similar to the middle-and late-layer MLPs.To verify these neurons are sufficient for arithmetic calculations, we repeat the experiment from Section 3.1.Specifically, we measure the faithfulness of the circuitconsisting of the top 1% of neurons in each of the middle and late layers (l ∈ [16, 32]) as well as a varying number of neurons in MLP0.The results, shown in Figure 12b, reveal that also in the first MLP layer, as little as 1% of neurons is sufficient for the circuit to achieve high faithfulness, similarly to the middle-and late-layer MLPs.</p>
<p>Because Llama3-8B applies a positional embedding only at attention layers, the activation of MLP0 is not affected by the position of any token.Additionally, due to a lack of attention heads that move information between the operand and operator positions before MLP0, we can analyze its effect directly on single tokens.Thus, we view MLP0 as an "effective embedding" (McDougall et al.,  Figure 12: Analyzing effect of individual MLP0 neurons.A small number of neurons is sufficient for circuit accuracy.</p>
<p>2023), and hypothesize that the role of each MLP0 neuron is to incorporate additional numerical information into each token embedding.To verify this, we pass a list of numerical tokens, each representing an operand in our analyzed operand range (t ∈ [0, 300]), through the model.We measure the activation of each high-effect neuron for each token.As exemplified in Figure 13, the activations of these high-effect neurons correspond to varied numerical features.For example, Neuron 6206 (Figure 13a activates for numbers near 170 or 17; Neuron 7101 activates for numbers greater than 100; Neuron 8969 activates for numbers that are congruent to 8 (mod 10).Overall, the set of patterns identified by these neurons can be used by the middle-and late-layer heuristic neurons to perform their more complex functionalities.</p>
<p>B.2 ATTENTION HEAD PATTERNS</p>
<p>We present the attention patterns of the attention heads that are contained in the arithmetic circuit.In Section 2.1, for simplicity, we consider the arithmetic circuit as a single circuit for all arithmetic operators, containing all attention heads used across the four operators.We observe that to achieve high faithfulness of the circuit with a minimal number of heads (Appendix A), some general arithmetic heads, that significantly improve faithfulness across all operators, are required.Other heads, while contributing to increased faithfulness, are operator-specific.They exhibit varying levels of importance for different operators.A full description of the attention heads used in the arithmetic circuit for each operator is provided in Table 2.</p>
<p>To better understand the role of the general arithmetic heads (L2H2, L15H13, L16H21), we compute their attention patterns, averaging them across our prompt dataset (Section 2.1).These patterns (Figure 14) reveal a clear signal: each of the three heads attends to a single input token, copying the representation from that position and projecting it to the last position.Specifically, L16H21 attends to the first operand, L2H2 attends to the operator and L15H13 attends to the second operand.This implies the role of each such head is to move the representation from that position, which includes to the last position, where it is further processed by the bag of heuristics implemented in the middle-and late-layer MLPs.To confirm that the information copied from each position to the last position consists solely of data from the token at that position, we conduct an ablation study.For each general arithmetic head, we zero out all preceding attention patterns that move information Table 2: Llama3-8B operator-specific arithmetic circuit attention heads.The general arithmetic heads are marked bold.LiHj denotes the j th attention head in Layer i.</p>
<p>Operator</p>
<p>Circuit Heads
+ L2H2, L5H3, L5H31, L14H12, L15H13, L16H21 − L2H2, L13H21, L13H22, L14H12, L15H13, L16H21 × L2H2, L5H30, L8H15, L9H26, L13H18, L13H21, L13H22, L14H12, L14H13, L15H8, L15H13, L15H14, L15H15, L16H3, L16H21, L17H24, L17H26, L18H16, L20H2, L22H1 ÷ L2H2, L5H31, L15H13, L15H14, L16H21, L18H16
to its attended position (e.g., for L15H13 we zero out all patterns that move information to the op 2 position), preventing any influence from previous positions.We observe this does not affect the circuit's performance, indicating that the representation copied by each general arithmetic head to the final position contains information only regarding the token at its original position.Figure 14: The attention patterns for the general arithmetic attention heads in Llama3-8B, show that each head attends, at the last token, to a single previous token across all prompts.When combined, these heads attend to all three operand and operator positions.</p>
<p>C MULTI-LAYER PERCEPTRON IMPLEMENTATION DETAILS</p>
<p>Across the models we analyze, different implementations exist for the MLP layer in a transformer block.More specifically, Pythia-6.9Band GPT-J use simple MLP layers, consisting of two matrix multiplications.For a layer l, it can be described as:
h l post = σ h l in W l ⊤ in (4) h l out = h l post W l out (5)
where h l in , h l out ∈ R d are the input and output representations of the MLP at layer l, respectively.h l post ∈ R d mlp is the post-activation vector, W l in , W l out ∈ R d mlp ×d are parameter matrices, and σ is a non-linearity function.Llama3-8B and Llama3-70B use a Gated MLP layer (Liu et al., 2021), described in the following two equations for layer l:
h l post = σ(h l in W l ⊤ gate ) • (h l in W l ⊤ in )(6)h l out = h l post W l out (7)
where W l gate ∈ R d mlp ×d is an additional parameter matrix and • is Hadamard product.Biases are omitted in both presentations.</p>
<p>While the key-value view that was used in Section 3 was devised for simple MLPs (Geva et al., 2021), it can be applied to the Gated MLP mechanism of Llama3 models as well.For the n th neuron, we treat the element h l,n post of h l post as the activation of the n th key vector.Each such activation is multiplied with v l,n out , a row vector of W l out , resulting in the same multiplication as performed in simple MLPs.[16,32], ranked based on their mean effect (Section 3.1), totaling 3, 200 neurons.The average IoU between any two operator-specific circuits is relatively low (54%), indicating that a substantial proportion of key neurons are unique to each operator.Consequently, we define a distinct circuit for each arithmetic operator at the neuron level (Section 3.1).</p>
<p>D NEURON INTERSECTION BETWEEN OPERATORS</p>
<p>E HEURISTIC TYPES DESCRIPTIONS</p>
<p>In this section, we present the various heuristic types that were manually identified.</p>
<p>Most heuristic types are defined with parameters.A neuron is defined as a heuristic H, if it matches the heuristic definition given a parameter value.For example, the neuron in Figure 1b is a range heuristic with a range parameter [150,180].In Algorithm 1, we describe the process of matching between each important neuron and each heuristic H.The parameters are sub-sampled evenly to cover most observed possibilities, which causes some inaccuracies (for example, a neuron that fires when op 2 ∈ [105, 205] will be classified as a range heuristic with op 2 ∈ [100, 200] due to our parameter choices).</p>
<p>Several types of heuristics (Range, Modulo, Pattern) can apply either to an operand in a prompt or to the prompt's result.For example, an "operand range" heuristic is triggered when op 1 or op 2 falls within a specific numerical range, while a "result range" heuristic activates when the prompt's ground truth result is within a defined range.</p>
<p>The full list of heuristic types is as follows:</p>
<p>• Range heuristic: A neuron that activates when a value (either an operand or result) falls within a specified range  2,10,100] for division (due to a different distribution of potential results for integer division).The range start, a, is defined as a ∈</p>
<p>x n : x n = x 0 + n • max(⌊ len 3 ⌋, 10), n ∈ N, x 0 = 0, provided a remains below the maximum prompt result.This definition of range start and length allows for ranges to intersect or overlap.</p>
<p>• Modulo heuristic: A neuron that activates when a value (either operand or result) is congruent to m modulo n.The parameters are n ∈ {2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 15} and m ∈ [0, n − 1].We exclude n = 10 or n = 100 as these are specific cases of the broader "pattern" heuristics, discussed next.Additionally, n = 12, 14, and n ≥ 16 are excluded, as no neurons were classified under these parameter values.• Pattern heuristic: A neuron that activates when a value (either operand or result) matches a specific regular expression p.The parameter p is a 3-digit regular expression, potentially padded with leading zeros.For example, a neuron classified under the "operand pattern 1.2" heuristic activates when one of the operands has 1 in the hundreds place and 2 in the units place.• Identical operands heuristic: A neuron that activates when both operands are equal (op 1 == op 2 ).The tokens embedded in the value vector change depending on the operator for which the neuron activates.For instance, in subtraction, such neurons have been observed to promote the "0" token (the result of subtracting a number from itself).• Multi-result heuristic: A neuron that promotes a set S of several unrelated results (For example, a neuron that promote the results [4,5,7]).This heuristic type is defined exclusively for division, where the parameter S can consist of several values, where |S| ∈ [2, 4].This range was chosen based on observations of activation patterns.The values included in S are chosen according to the result tokens that the neuron contributes the most to: we determine S by examining the top prompts for which the neuron promotes their answer.From these top answers, we identify a minimal set of 2 to 4 distinct result values.This set of result values is chosen if it accounts for more than a threshold percentage of the results.This approach captures the most significant result values that the neuron consistently promotes in division operations.If no such set of different results exists, a neuron is not classified as this heuristic.</p>
<p>F HEURISTIC NEURON CLASSIFICATION ALGORITHM</p>
<p>We present the full algorithm used to match between each neuron and each heuristic type in Algorithm 1 (as exemplified in Figure 6).The goal of this process is to check if a neuron implements a specific heuristic.We repeat this process for each pair of neuron n in layer l and heuristic H.This method allows a single neuron to be classified as several heuristics.The matching of each neuron to heuristics is done separately for each arithmetic operator.</p>
<p>Algorithm 1 Neuron Classification To Heuristic Type The algorithm first calculates the neuron's activations h l,n post for all prompts, yielding a 2D activation pattern (Line 1), as those seen in Appendix J.When checking if a neuron implements a direct heuristic (heuristics that directly promote relevant result tokens (Section 3.2)), we also need to take into consideration the tokens that are embedded in the neuron's value vector v l,n out .This is not done in indirect heuristics because we do not expect relevant result tokens to be promoted in such heuristics.Thus, we extract the logits of all numerical tokens ("0", "1", "2", ... "999") from the value vector
Inputs: Heuristic H, Layer l ∈ [1, l max ], Neuron Index n ∈ [1, n max ], Operator o ∈ {+, −, ×, ÷}, Threshold t ∈ [0, 1.0] 1: A ← GenerateActivationPattern(l, n, o) ▷ 2D Activation pattern 2: if H is a direct heuristic then 3: l ← LogitLens(v l,n out ) ▷ Logits over vocabulary 4: L ← ConvertToPattern(l) ▷ 2D Logits Pattern 5: A ← A • L ▷ Element-</p>
<p>H MODEL ACCURACIES ON ARITHMETIC PROMPTS</p>
<p>Table 3 presents the accuracy of the analyzed models on arithmetic prompts.The accuracy is evaluated across prompts where the result is represented by a single token, with both operands constrained to the interval [0, 300], where 300 represents the maximum value of operands in the analyzed prompts (Section 2.1), chosen for efficiency.It is noteworthy that the high accuracy rates observed in division operations for the smaller-scale models (GPT-J, Pythia-6.9B)can be attributed to the non-uniform distribution of answers in integer division, i.e. -half of the legal prompts result in the token '0'.</p>
<p>I RESULTS ON ADDITIONAL MODELS</p>
<p>To demonstrate the generalizability of our findings across differently-trained LLMs, we conduct our primary experiments on Llama3-70B (Dubey et al., 2024), Pythia-6.9B(Biderman et al., 2023), and GPT-J (Wang &amp; Komatsuzaki, 2021).We replicate the experiments for circuit discovery and evaluation (Section 2.1), linear probing for answer token embeddings (Section 2.2), top-k neuron faithfulness analysis (Section 3.1), and heuristic analysis (Section 4.2).The results obtained are similar to those of Llama3-8B across all three LLMs:</p>
<p>• The circuit comprises a sparse subset of attention heads that project operand and operator information to the final position, along with all MLP layers.Early-layer MLPs process information at the operand and operator positions, while middle-and late-layer MLPs process the combined information at the last token.These findings are illustrated for Llama3-70B (Figure 16a), Pythia-6.9B(Figure 17a), and GPT-J (Figure 18a).• Linear probing results indicate that the correct answer can only be extracted with high accuracy in the final position, following the processing initiated by middle-layer MLPs.These observations are presented for Llama3-70B (Figure 16b), Pythia-6.9B(Figure 17b), and GPT-J (Figure 18b).• The circuit requires a sparse subset of middle-and late-layer MLP neurons to achieve maximal faithfulness.These results are depicted for Llama3-70B (Figure 16c), Pythia-6.9B(Figure 17c), and GPT-J (Figure 18c).• The heuristic neurons in the middle-and late-layer MLPs are the model components that</p>
<p>write the correct answer to the model output.This is shown by repeating the prompt-guided knockout experiment (Section 4.2).The ablation of specific heuristic neurons associated with prompts results in a more significant reduction in model accuracy compared to the ablation of a random set of neurons of equivalent size.These findings are illustrated for Llama3-70B (Figure 8), Pythia-6.9B(Figure 17d), and GPT-J (Figure 18d).</p>
<p>The results observed in Llama3-70B exhibit the highest similarity to those reported in Llama3-8B, with a more pronounced knockout effect compared to the other two models.We hypothesize that this indicates a more sophisticated development of the bag of heuristics in Llama3-70B, potentially facilitated by its larger size and modern training methodology.</p>
<p>J ADDITIONAL EXAMPLES FOR ARITHMETIC HEURISTICS</p>
<p>We present supplementary examples of heuristic neurons (Table 4, Table 5, Table 6, and Table 7 for the four arithmetic operators, respectively).Each neuron h l,n post found to be important for a specific arithmetic operator (Section 3.1) is categorized as one or more heuristics.For each designated operator, we randomly present four neurons.We compute and report each neuron's activation pattern, the ten numerical tokens with the strongest embeddings in each neuron's value vector v l,n out , and provide a non-exhaustive list of the heuristics it implements.Furthermore, we present in Table 8 several examples of causally significant neurons that have not been classified as specific heuristics.These neurons are relevant to the aforementioned limitation in our methodology (Section 8); it is conceivable that they too may be considered components of the bag of heuristics, depending upon our ability to comprehend the abstractions they implement.'171', '169', '17', '156', '160', '16', '785', '168' range result ∈ [150, 180], result ≡ 0 (mod 2)  '873', '985', '548', '858', '657', '995', '788', '716'
pattern op2 0.1
Figure 1 :
1
Figure 1: Bag of heuristics visualization.We show that transformer LLMs solve arithmetic prompts by combining several unrelated heuristics, each activating according to rules based on the input values of operands, and boosting the logits of corresponding result tokens.These heuristics are manifested in single MLP neurons in mid to late layers.</p>
<p>Figure 2 :
2
Figure 2: Llama3-8B arithmetic circuit discovery results.(a): Few attention heads have a high effect on arithmetic prompts.Most MLPs take part in the computation.The first MLP noticeably affects operand and operator positions, while mid-and late-layer MLPs influence the final position.(b): The arithmetic circuit in Llama3-8B.The attention heads project token information to the last position, where the middle-and late-layer MLPs promote the logits for the correct answer.</p>
<p>Figure 3 :
3
Figure 3: Answer token probe accuracy.The linear probes are successful in extracting the correct answer from the final position, starting at layer 16.</p>
<p>measure the faithfulness when including only a fraction of high-effect neurons in the circuit.This circuit achieves high faithfulness.</p>
<p>Figure 5 :
5
Figure 5: Heuristic pattern examples.Each heatmap is the activation pattern of an example neuron, implementing a specific heuristic type.Within the heatmap, each pixel at location (op 1 , op 2 ) represents the activation strength of the neuron under the addition prompt "op 1 + op 2 =".</p>
<p>1+1 . . .130 + 45 . . .145 + 7 . . .299 + 291 . . .range ∈ [150, 180] Does neuron implement heuristic?</p>
<p>Figure 6 :
6
Figure 6: Neuron to heuristic matching example.(a) Measure the value of h 29,2850 post for each operand pair (op 1 , op 2 ), using the chosen operator (addition).(b) Calculate the logits of numerical tokens embedded in v 29,2850 out , using Logit Lens (nostalgebraist, 2020).(c) Convert the logits vector to a 2D pattern, where the cell in index (op 1 , op 2 ) is the logit of the result token of applying the operator to (op 1 , op 2 ) (i.e.op 1 + op 2 ).(d) Multiply both patterns element-wise, to get the effective logit contribution of the neuron to the correct answer token for each prompt.(e) Extract the prompts that activate the neuron the most from the activation pattern.(f) Create a list of prompts associated with the tested heuristic.(g) Measure the intersection between the two prompt lists.If this intersection is larger than a threshold (we use t = 0.6), the neuron is said to implement the heuristic.</p>
<p>Figure 9 :
9
Figure9: The model's failures can be explained by a lower total logit contribution of the heuristic neurons to the correct answers.</p>
<p>Figure 10 :
10
Figure 10: Heuristic analysis across Pythia-6.9Btraining checkpoints.(a) The percentage of heuristic neurons from the last checkpoint that also appear in previous checkpoints increases over training, revealing a gradual creation of the bag of heuristics.(b)The heuristic neurons that are mutual with the last checkpoint (full line) explain most of the total heuristic behavior (dashed line) at each checkpoint.Thus, the heuristics that disappear across training are less important to the model.(c) Ablating specific heuristic neurons heavily drops the model's accuracy across all training checkpoints.This suggests arithmetic accuracy primarily stems from heuristics, even in early stages.</p>
<p>Figure 11 :
11
Figure 11: Llama3-8B arithmetic circuit faithfulness as function of number of circuit heads.</p>
<p>Few neurons in the first MLP have a high effect on arithmetic prompts.</p>
<p>The circuit faithfulness when including a percentage of high-effect MLP0 neurons in the circuit.</p>
<p>Figure 13 :
13
Figure 13: Individual MLP0 neurons have identifiable activation patterns for numerical tokens.</p>
<p>Figure 15 :
15
Figure 15: IoU of MLP neurons between operator-specific circuits Figure 15 presents the IoU (intersection over union) of causally important neuron sets in the arithmetic circuits, identified separately for each operator.Each circuit consists of the top-200 neurons from each layer l ∈[16, 32], ranked based on their mean effect (Section 3.1), totaling 3, 200 neurons.The average IoU between any two operator-specific circuits is relatively low (54%), indicating that a substantial proportion of key neurons are unique to each operator.Consequently, we define a distinct circuit for each arithmetic operator at the neuron level (Section 3.1).</p>
<p>[a, b].The parameters a, b ∈ N are chosen such that the range length len = (b − a) ∈ [10, 30, 50, 100] for addition, subtraction, and multiplication, and len = (b − a) ∈ [</p>
<p>Llama3-70B faithfulness as function of MLP neurons in each layer.</p>
<p>Figure</p>
<p>Figure 16: Llama3-70B analysis results</p>
<p>Figure 17 :
17
Figure17: Results for all analyses on Pythia-6.9B.</p>
<p>Figure 18 :
18
Figure18: Results for all analyses on GPT-J.</p>
<p>Table 1 :
1
Llama3-8B arithmetic circuit faithfulness, per operator.
Operator+-×÷Faithfulness0.97 0.98 0.90 0.96# Attn Heads66206</p>
<p>wise multiplication 6: end if 7: prompts heuristic ← GetAssociatedPrompts(H) ▷ Expected prompts to activate in H 8: k ← |prompts heuristic | 9: prompts neuron ← GetTopKActivatingPrompts(A, k) ▷ Prompts that activate neuron 10: prompts intersection ← prompts heuristic ∩ prompts neuron
11: score ←|prompts intersection | k▷ Normalize intersection to [0, 1.0]
12: return score ≥ t</p>
<p>Table 3 :
3
Accuracy of the analyzed models on arithmetic prompts.
OperatorModel+−×÷AverageLlama3-8B0.97 0.96 0.84 0.920.95Llama3-70B 0.97 0.99 0.99 0.730.88Pythia-6.9B 0.30 0.04 0.27 0.750.43GPT-J0.23 0.09 0.46 0.640.37
The up-projection MLPin is implemented differently in each LLM we analyze. See Appendix C.
Not to be confused with the attention heads' keys and values.
We apply this algorithm to each pair of important MLP neuron n in layer l and heuristic H. Through this method, we classify as arithmetic heuristics 91% of the 3,200 top neurons for each operator ACKNOWLEDGMENTS We are grateful to Dana Arad and Alessandro Stolfo for providing feedback for this work.This research was supported by the Israel Science Foundation (grant No. 448/20), an Azrieli Foundation Early Career Faculty Fellowship, and an AI Alignment grant from Open Philanthropy.AM is supported by a postdoctoral fellowship under the Zuckerman STEM Leadership Program.AR is supported by a postdoctoral fellowship under the Azrieli International Postdoctoral Fellowship Program and the Ali Kaufman Postdoctoral Fellowship.This research was funded by the European Union (ERC, Control-LM, 101165402).Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency.Neither the European Union nor the granting authority can be held responsible for them.v l,n out (Line 3), using LogitLens (nostalgebraist, 2020).To match the logit of each result token to the prompts that result in it (e.g., match the logit of "151" with the prompts "1 + 150 =","2 + 149 =", etc.), we convert the logits to a 2D pattern L, where L ij is the logit of applying the operator on i and j (Line 4).Multiplying the 2D activation pattern and 2D logit pattern element-wise (Line 5) yields the neuron's effective logit contribution to the correct answer of each prompt, i.e., the value at each index i, j marks how much the neuron promotes the answer token to the result of applying the operator on i and j.We then create two lists of prompts.One list contains prompts associated with the heuristic (Line 7), to be used as the "ground truth".We mark the number of associated prompts as k (Line 8), and find a second list, containing the top-k prompts that are most contributed to by the neuron (Line 9).We measure if the intersection of these two lists, normalized by k, is larger than the threshold t = 0.6 (Line 10-Line 12) to decide if the neuron implements the heuristic.G ADDITIONAL IMPLEMENTATION DETAILSAll experimental procedures were executed using the TransformerLens libraryNanda &amp; Bloom (2022).Experiments involving Llama3-8B, GPT-J, and Pythia-6.9Bwere conducted on a single Nvidia-L40 GPU with 48GB of GPU memory.Llama3-70B was loaded in 16-bit precision across four Nvidia A100 GPUs, each with 80GB of GPU memory.Arithmetic prompts.Unless otherwise specified, our experiments utilize prompts of the form op 1 • op 2 =", where op 1 , op 2 ∈ [0, 300] and • ∈ {+, −, ×, ÷}.The division operator (÷) is interpreted as integer division, considering only the integer part of the result (e.g., the ground truth result for 45 ÷ 4 =" is 11).For each prompt, we compute the ground-truth result and exclude prompts whose results are not tokenized to a single token.This filtering process eliminates prompts with negative results and prompts for which the correct answer exceeds the single-token limit (The highest token from which the model splits numbers into two tokens).The single-token limit s for Llama3-8B and Llama3-70B is s llama3 = 1, 000, while for GPT-J and Pythia-6.9B, it is s gptj = s pythia = 520.Circuit discovery via activation patching.In Section 2.1, we quantify the effect of patching on probabilities.We find that applying our effect measure (Equation (1)) on logits instead of probabilities, does not alter the result significantly-the most effective components remain consistent, differing only in the scale of the effect.Circuit faithfulness evaluation.Thus far, to measure the faithfulness of the circuit, we have performed mean ablation on all non-circuit components.For that, we calculate the mean activation output of each component, across all arithmetic prompts.In this context, we do not filter correct prompts exclusively, but instead use all prompts of the form "op 1 • op 2 =", for op 1 , op 2 ∈ [0, 300].This leads to an equal amount of prompts per operator, thus maintaining the mean activation balanced across the different operators.When evaluating a circuit with partial MLP layers (Section 3.1), we mean ablate lower-effect neurons in middle-and late-layer MLPs only, starting and ending in the earliest and latest layers from which the answer is extractable via linear probe (Section 2.2).These layer ranges are[16,32],[39,80],[14,32],[17,28]In Llama3-8B, Llama3-70B, Pythia-6.9B,and GPT-J, respectively.Linear probing for correct answers.We define a linear probe f l,p : R → R s , where s is the single-token limit, to predict the correct answer token from the output representation at layer l and position p.We compute these outputs by using all correctly completed prompts, with 80% used for training the classifier and 20% for evaluation.Each probing classifier is implemented as a one-layer fully connected model.We train it using the Adam optimizer(Kingma &amp; Ba, 2015)with a learning rate of 0.0003 and a batch size of 32, optimizing a cross-entropy loss function.Heuristic classification.In applying the heuristic classification algorithm (Section 4.1), we use all activations from the last position p = 4 and employ a classification score threshold t = 0.6.Due to a lack of ground truth data for neuron-to-heuristic matches, the threshold is chosen to achieve a Pareto-optimal balance of false positives, that occur more for a lower threshold, and false negatives, that occur more in a higher threshold.The amount of false classifications is estimated manually.'792', '979', '372', '502', '882', '962', '602', '02'op1 ≡ 2 (mod 5), op2 ≡ 2 (mod 5)'761', '762', '522', '918', '972', '783', '325', '531' pattern op2..8   '409', '257',  '454', '451', '570',  '290', '287', '470',  '439'   h 20,11020   post , Division'983', '801', '731', '501', '751', '070', '663', '985', '713'
Generalization vs. memorization: Tracing language models' capabilities back to pretraining data. Antonis Antoniades, Xinyi Wang, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, William Yang, Wang , ICML 2024 Workshop on Foundation Models in the Wild. 2024</p>
<p>Measures of information reflect memorization patterns. Rachit Bansal, Danish Pruthi, Yonatan Belinkov, Advances in Neural Information Processing Systems. 2022</p>
<p>Probing classifiers: Promises, shortcomings, and advances. Yonatan Belinkov, Computational Linguistics. 4812022</p>
<p>Pythia: A suite for analyzing large language models across training and scaling. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, O' Kyle, Eric Brien, Mohammad Hallahan, Shivanshu Aflah Khan, Purohit, Sai Usvsn, Edward Prashanth, Raff, International Conference on Machine Learning. PMLR2023</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Generalisation first, memorisation second? Memorisation localisation for natural language classification tasks. Verna Dankers, Ivan Titov, The 62nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics2024</p>
<p>Analyzing transformers in embedding space. Guy Dar, Mor Geva, Ankit Gupta, Jonathan Berant, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Survival of the fittest representation: A case study with modular addition. Delores Xiaoman, Zifan Ding, Eric J Carl Guo, Ziming Michaud, Max Liu, Tegmark, ICML 2024 Workshop on Mechanistic Interpretability. 2024</p>
<p>The Llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>A mathematical framework for transformer circuits. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam Mccandlish, Chris Olah, 2021Transformer Circuits Thread</p>
<p>Causal abstractions of neural networks. Atticus Geiger, Hanson Lu, Thomas Icard, Christopher Potts, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, J Wortman Vaughan, Curran Associates, Inc202134</p>
<p>Transformer feed-forward layers are key-value memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Successor heads: Recurring, interpretable attention heads in the wild. Rhys Gould, Euan Ong, George Ogden, Arthur Conmy, The Twelfth International Conference on Learning Representations. 2024</p>
<p>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. Michael Hanna, Ollie Liu, Alexandre Variengien, Advances in Neural Information Processing Systems. 2024a36</p>
<p>Have faith in faithfulness: Going beyond circuit overlap when finding model mechanisms. Michael Hanna, Sandro Pezzelle, Yonatan Belinkov, arXiv:2403.178062024barXiv preprint</p>
<p>Superposition, memorization, and double descent. Tom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort, Nicholas Schiefer, Christopher Olah, Transformer Circuits Thread. 6242023</p>
<p>Towards a mechanistic interpretation of multi-step reasoning capabilities of language models. Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, Mrinmaya Sachan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Othellogpt learned a bag of heuristics. Jylin, Adam Jacks, Can Karvonen, Rager, 2024</p>
<p>Adam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, International Conference on Learning Representations (ICLR). San Diega, CA, USA2015</p>
<p>Emergent world representations: Exploring a sequence model trained on a synthetic task. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Pay attention to MLPs. Hanxiao Liu, Zihang Dai, David So, Quoc V Le, Advances in neural information processing systems. 202134</p>
<p>Arithmetic with language models: From memorization to computation. Davide Maltoni, Matteo Ferrara, arXiv:2308.011542023arXiv preprint</p>
<p>Neel Nanda. Attribution patching: Activation patching at industrial scale. Callum Mcdougall, Arthur Conmy, Cody Rushing, Thomas Mcgrath, Neel Nanda, arXiv:2310.046252023. 2022arXiv preprintCopy suppression: Comprehensively understanding an attention head</p>
<p>. Neel Nanda, Joseph Bloom, Transformerlens, </p>
<p>. Transformerlensorg/Transformerlens, 2022</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, The Eleventh International Conference on Learning Representations, 2023. nostalgebraist. Interpreting GPT: The logit lens. 2020</p>
<p>Direct and indirect effects. Direct and Indirect Effects. Judea Pearl, 2001</p>
<p>Fine-tuning enhances existing mechanisms: A case study on entity tracking. Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, David Bau, The Twelfth International Conference on Learning Representations. 2024</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Extracting latent steering vectors from pretrained language models. Nishant Subramani, Nivedita Suresh, Matthew E Peters, Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>Attribution patching outperforms automated circuit discovery. Aaquib Syed, Can Rager, Arthur Conmy, NeurIPS Workshop on Attributing Model Behavior at Scale. 2023</p>
<p>Memorisation versus generalisation in pre-trained language models. Michael Tänzer, Sebastian Ruder, Marek Rei, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Activation addition: Steering language models without optimization. Matt Alexander, Lisa Turner, Gavin Thiergart, David Leech, Juan J Udell, Ulisse Vazquez, Monte Mini, Macdiarmid, arXiv:2308.102482023arXiv preprint</p>
<p>Explaining grokking through circuit efficiency. Vikrant Varma, Rohin Shah, Zachary Kenton, János Kramár, Ramana Kumar, 20232309arXiv e-prints</p>
<p>Investigating gender bias in language models using causal mediation analysis. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart Shieber, Advances in neural information processing systems. 202033</p>
<p>Ben Wang, Aran Komatsuzaki, Gpt-J-6b, A 6 Billion Parameter Autoregressive Language Model. May 2021</p>
<p>Interpretability in the wild: A circuit for indirect object identification in GPT-2 small. Kevin Ro, Wang , Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Understanding deep learning (still) requires rethinking generalization. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, Communications of the ACM. 6432021</p>
<p>Interpreting and improving large language models in arithmetic calculation. Wei Zhang, Chaoqun Wan, Yonggang Zhang, Xinmei Yiu Ming Cheung, Xu Tian, Jieping Shen, Ye, Forty-first International Conference on Machine Learning. 2024</p>
<p>The clock and the pizza: Two stories in mechanistic explanation of neural networks. Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas, Advances in Neural Information Processing Systems. 202436</p>
<p>Pre-trained large language models use Fourier features to compute addition. Tianyi Zhou, Deqing Fu, Sharan Vatsal, Robin Jia, arXiv:2406.034452024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
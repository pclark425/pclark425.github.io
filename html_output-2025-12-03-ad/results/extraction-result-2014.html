<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2014 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2014</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2014</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-276558330</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.15277v1.pdf" target="_blank">Analyzing the Inner Workings of Transformers in Compositional Generalization</a></p>
                <p><strong>Paper Abstract:</strong> The compositional generalization abilities of neural models have been sought after for human-like linguistic competence. The popular method to evaluate such abilities is to assess the models' input-output behavior. However, that does not reveal the internal mechanisms, and the underlying competence of such models in compositional generalization remains unclear. To address this problem, we explore the inner workings of a Transformer model by finding an existing subnetwork that contributes to the generalization performance and by performing causal analyses on how the model utilizes syntactic features. We find that the model depends on syntactic features to output the correct answer, but that the subnetwork with much better generalization performance than the whole model relies on a non-compositional algorithm in addition to the syntactic features. We also show that the subnetwork improves its generalization performance relatively slowly during the training compared to the in-distribution one, and the non-compositional solution is acquired in the early stages of the training.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2014.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2014.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer (base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Encoder–Decoder Transformer trained from scratch (base model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard encoder–decoder Transformer (trained from scratch on synthetic PCFG data) evaluated on compositional generalization patterns PP-IOBJ and PP-SUBJ in two task domains (machine translation and semantic parsing); shows near-perfect in-distribution accuracy but large failures on out-of-distribution compositional splits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard encoder–decoder Transformer trained from scratch: 3 encoder layers, 3 decoder layers, 4 attention heads, standard attention and MLP blocks; trained end-to-end on supervised examples generated by PCFG rules.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard attention-based encoder–decoder; no explicit modularity or symbolic components; no pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Custom Machine Translation (English→Japanese) and Semantic Parsing (rule-based logical forms)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Datasets generated by PCFGs with controlled splits: training set contains PPs modifying NPs only in certain syntactic positions (e.g., direct object); generalization sets require the model to handle the same PP modifications in novel grammatical roles/positions (PP-IOBJ: PP in indirect-object NP; PP-SUBJ: PP in subject NP). Exact-match evaluation checks whether outputs follow compositional rules.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel structural patterns / novel grammatical role of prepositional phrase (PP) modifiers (PP-IOBJ, PP-SUBJ)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>out-of-distribution split: novel structural patterns where particular PP→NP modification combinations appear only in generalization set (OOD structural split)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard supervised learning from scratch on 80k training samples (no pretraining); 500 epochs, batch size 256, lr=1e-4, weight decay=0.1, three runs averaged</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Nearly perfect exact-match accuracy on the in-distribution test set (described as "nearly perfectly"; ≈100% exact-match in text)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Substantially worse on compositional (generalization) sets for both PP-IOBJ and PP-SUBJ (text reports large drops relative to in-distribution performance; exact base-model generalization percentages not listed in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Large gap between near-100% in-distribution accuracy and much lower OOD generalization accuracy (magnitude unspecified in text for base model).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Fails primarily on PP placed in novel grammatical roles/positions (PP-IOBJ and PP-SUBJ); failure pattern noted to differ between tasks (machine translation vs semantic parsing).</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against a pruned subnetwork found via subnetwork probing (same trained weights with a learned mask), and against interventions produced by concept scrubbing (LEACE): - Base model vs discovered subnetwork: base model has near-perfect IID but much worse OOD; subnetwork achieves much higher OOD (esp. PP-IOBJ). - Base model before/after removal of syntactic constituency/dependency (via LEACE): removal substantially degrades generalization indicating reliance on syntactic features.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A standard Transformer trained from scratch attains near-perfect in-distribution exact-match accuracy but fails to generalize compositionally to PP-IOBJ and PP-SUBJ patterns; model performance drops substantially when syntactic constituency/dependency information is removed, showing reliance on syntactic features but not necessarily a compositionally-generalizing algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Primary failures occur on sentences where PP modifiers appear in novel positions/roles relative to training (PP modifying indirect-object NPs or subject NPs). The failure mode differs across tasks: machine translation shows larger drops after removing syntactic features than semantic parsing in some comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2014.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2014.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Subnetwork (pruned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subnetwork discovered by pruning-based subnetwork probing (mask learned on generalization set)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparsified subnetwork (learned binary/prunable mask over the trained Transformer weights using subnetwork probing) that preserves in-distribution performance and substantially improves out-of-distribution accuracy for PP-IOBJ and partially for PP-SUBJ, but relies partly on non-compositional heuristics in addition to syntactic features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Subnetwork of Transformer (mask-pruned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture as the base Transformer but with a learned binary mask (hard concrete parametrization) applied to weights (pruning); proportions of unmasked weights ~50–70% per layer with more unmasked in deeper layers and in decoder, and more unmasked in MLP blocks than attention blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard Transformer architecture but sparsified by learned mask (subnetwork); no additional modules</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same custom Machine Translation and Semantic Parsing tasks (PP-IOBJ and PP-SUBJ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Subnetwork masks are trained using the generalization set (pruning-based probing) to find subnetworks that maximize generalization-set accuracy while inducing sparsity; evaluation uses exact-match accuracy on both test and generalization sets.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel structural patterns (PP modifiers in novel grammatical roles/positions)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD structural split (generalization set contains novel PP→NP role/position combinations absent from training)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Post-hoc subnetwork probing (trainable mask) on trained base models: mask training batch size 256, 300 epochs, lr=5e-4; final masks binarized at inference. Base model training unchanged (see base entry).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>Generalization set was split; one part used for training masks (i.e., the subnetwork probing used held-out generalization examples as supervision for mask training), the other part reserved for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Keeps nearly perfect in-distribution exact-match accuracy (same as base model).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>PP-IOBJ: subnetwork scored >90% exact-match accuracy in both machine translation and semantic parsing (text reports "more than 90%" for PP-IOBJ). PP-SUBJ: subnetwork performed substantially better than base model but remains far from 100% (exact numbers not given).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>For PP-IOBJ the subnetwork substantially closes the IID–OOD gap (from near-0 or low base-model OOD to >90%); for PP-SUBJ the gap narrows but a substantial gap remains (quantities not fully specified).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Causal ablations (concept scrubbing): removing all syntactic constituency or dependency typically reduces subnetwork OOD accuracy to almost 0% (exception: removing dependency in MT had a smaller effect). Removing narrow concepts (e.g., constituency only for PP modification of indirect object NPs) reduces accuracy somewhat but not to zero, indicating reliance on both syntactic features and additional non-compositional heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Comparisons include: - Base trained Transformer vs discovered subnetwork (same weights, masked): subnetwork >90% on PP-IOBJ vs base much lower. - Subnetwork before/after removal of syntactic constituency/dependency (LEACE): removal typically collapses OOD performance to near 0%. - Training with additional hints (RC augmentations) vs original training (see hint augmentation): hints raise generalization accuracy by ~40% for base model and affect which features the model relies on.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A subnetwork inside a standard Transformer can achieve high OOD accuracy for certain compositional patterns (PP-IOBJ >90%) while preserving IID performance, but causal analysis shows this subnetwork uses a mix of syntactic-feature-driven processing and a non-compositional algorithm; the non-compositional component is learned early (by ~200 epochs) and persists, while syntactic/compositional behavior improves slowly with more epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>PP-SUBJ generalization remains poor (subnetwork far from perfect). The subnetwork shows overfitting to direct-object NP modifications: removing direct-object NP-modification constituency sometimes improves PP-SUBJ accuracy, indicating maladaptive specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Adding explicit training hints (e.g., augmenting train set with sentences containing relative-clause-modified NPs) improves OOD generalization substantially (~40% improvement reported) and makes the learned algorithm rely more on the targeted syntactic features; semantic parsing (explicit structured outputs) tends to encourage greater reliance on syntax than machine translation in some comparisons.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2014.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2014.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEACE / Concept scrubbing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LEACE (linear concept erasure) used with sequential concept scrubbing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear concept-erasure method (LEACE) applied iteratively layer-by-layer (concept scrubbing) to remove specific syntactic concepts (constituency/dependency or narrower PP-mod concepts) from model representations to test causal effects on compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LEACE: Perfect linear concept erasure in closed form</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LEACE (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Linear concept erasure that projects representations to remove linearly encoded concept subspaces; applied sequentially to each layer ('concept scrubbing') so that classifiers cannot recover the concept more accurately than a constant baseline while preserving other information as much as possible.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>analysis/intervention method rather than an architecture; operates on intermediate representations</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Syntactic constituency and dependency concept removal tasks (multi-label sequence tagging used to represent concepts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-label tagging tasks represent constituency boundaries (begin/end of phrase) and dependency relations; narrower concepts label only tokens involved in PP modification of certain NP types (iobj-mod, dobj-mod, subj-mod). LEACE scrubs those concepts and downstream effects on main tasks are measured.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>LEACE applied sequentially layer-by-layer (concept scrubbing) using a classification task built from the same PCFG-generated sentences; linear probes used post-hoc to validate removal.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Post-scrubbing linear probing on encoder final layer found 0% of test sentences had all labels predicted correctly in almost all removed-concept combinations; two exceptions gave max probe accuracies of 9.7% (all syntactic dependencies removed) and 1.3% (PP modification of all NPs removed), indicating strong removal success.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Empirical findings: - Removing all syntactic constituency or dependency typically collapses base-model and subnetwork generalization accuracy to almost zero (except removing dependency in MT had a smaller effect). - Removing only the constituency/dependency related to specific PP-modification types (e.g., indirect-object NP modification) reduces but does not always eliminate subnetwork generalization, showing partial dependence on those narrower features.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Ablations compare removing (a) all constituency or dependency information, (b) constituency/dependency restricted to PP-modification of particular NP roles (iobj-mod, dobj-mod, subj-mod), and (c) no removal; results show graded impacts and enable causal attribution of which syntactic features the model/subnetwork depends on.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LEACE-based scrubbing effectively removes the targeted syntactic information (as measured by linear probes) and causally demonstrates that both base model and discovered subnetworks depend on syntactic constituency/dependency for compositional generalization; however, subnetworks also retain non-syntactic heuristics that allow partial generalization even after certain narrow syntactic features are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Potential limitation: LEACE is linear and might miss non-linearly encoded concepts, but probing results indicate most syntactic information used here is encoded in linear subspaces and thus effectively removed.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>LEACE works well when the targeted concepts are linearly encoded in representations; probe validation should be used to confirm removal (as done in the paper).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Low-complexity probing via finding subnetworks <em>(Rating: 2)</em></li>
                <li>LEACE: Perfect linear concept erasure in closed form <em>(Rating: 2)</em></li>
                <li>Evaluating structural generalization in neural machine translation <em>(Rating: 2)</em></li>
                <li>On compositional generalization of neural machine translation <em>(Rating: 1)</em></li>
                <li>Structural generalization is hard for sequence-to-sequence models <em>(Rating: 1)</em></li>
                <li>COGS: A compositional generalization challenge based on semantic interpretation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2014",
    "paper_id": "paper-276558330",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "Transformer (base)",
            "name_full": "Encoder–Decoder Transformer trained from scratch (base model)",
            "brief_description": "Standard encoder–decoder Transformer (trained from scratch on synthetic PCFG data) evaluated on compositional generalization patterns PP-IOBJ and PP-SUBJ in two task domains (machine translation and semantic parsing); shows near-perfect in-distribution accuracy but large failures on out-of-distribution compositional splits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer",
            "model_description": "Standard encoder–decoder Transformer trained from scratch: 3 encoder layers, 3 decoder layers, 4 attention heads, standard attention and MLP blocks; trained end-to-end on supervised examples generated by PCFG rules.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "standard attention-based encoder–decoder; no explicit modularity or symbolic components; no pretraining",
            "task_domain": "linguistic/semantic",
            "task_name": "Custom Machine Translation (English→Japanese) and Semantic Parsing (rule-based logical forms)",
            "task_description": "Datasets generated by PCFGs with controlled splits: training set contains PPs modifying NPs only in certain syntactic positions (e.g., direct object); generalization sets require the model to handle the same PP modifications in novel grammatical roles/positions (PP-IOBJ: PP in indirect-object NP; PP-SUBJ: PP in subject NP). Exact-match evaluation checks whether outputs follow compositional rules.",
            "compositional_depth": null,
            "composition_type": "novel structural patterns / novel grammatical role of prepositional phrase (PP) modifiers (PP-IOBJ, PP-SUBJ)",
            "split_type": "out-of-distribution split: novel structural patterns where particular PP→NP modification combinations appear only in generalization set (OOD structural split)",
            "training_strategy": "standard supervised learning from scratch on 80k training samples (no pretraining); 500 epochs, batch size 256, lr=1e-4, weight decay=0.1, three runs averaged",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Nearly perfect exact-match accuracy on the in-distribution test set (described as \"nearly perfectly\"; ≈100% exact-match in text)",
            "compositional_performance": "Substantially worse on compositional (generalization) sets for both PP-IOBJ and PP-SUBJ (text reports large drops relative to in-distribution performance; exact base-model generalization percentages not listed in main text).",
            "generalization_gap": "Large gap between near-100% in-distribution accuracy and much lower OOD generalization accuracy (magnitude unspecified in text for base model).",
            "performance_by_depth": null,
            "performance_by_composition_type": "Fails primarily on PP placed in novel grammatical roles/positions (PP-IOBJ and PP-SUBJ); failure pattern noted to differ between tasks (machine translation vs semantic parsing).",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against a pruned subnetwork found via subnetwork probing (same trained weights with a learned mask), and against interventions produced by concept scrubbing (LEACE): - Base model vs discovered subnetwork: base model has near-perfect IID but much worse OOD; subnetwork achieves much higher OOD (esp. PP-IOBJ). - Base model before/after removal of syntactic constituency/dependency (via LEACE): removal substantially degrades generalization indicating reliance on syntactic features.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "A standard Transformer trained from scratch attains near-perfect in-distribution exact-match accuracy but fails to generalize compositionally to PP-IOBJ and PP-SUBJ patterns; model performance drops substantially when syntactic constituency/dependency information is removed, showing reliance on syntactic features but not necessarily a compositionally-generalizing algorithm.",
            "failure_analysis": "Primary failures occur on sentences where PP modifiers appear in novel positions/roles relative to training (PP modifying indirect-object NPs or subject NPs). The failure mode differs across tasks: machine translation shows larger drops after removing syntactic features than semantic parsing in some comparisons.",
            "success_conditions": null,
            "uuid": "e2014.0"
        },
        {
            "name_short": "Subnetwork (pruned)",
            "name_full": "Subnetwork discovered by pruning-based subnetwork probing (mask learned on generalization set)",
            "brief_description": "A sparsified subnetwork (learned binary/prunable mask over the trained Transformer weights using subnetwork probing) that preserves in-distribution performance and substantially improves out-of-distribution accuracy for PP-IOBJ and partially for PP-SUBJ, but relies partly on non-compositional heuristics in addition to syntactic features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Subnetwork of Transformer (mask-pruned)",
            "model_description": "Same architecture as the base Transformer but with a learned binary mask (hard concrete parametrization) applied to weights (pruning); proportions of unmasked weights ~50–70% per layer with more unmasked in deeper layers and in decoder, and more unmasked in MLP blocks than attention blocks.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "standard Transformer architecture but sparsified by learned mask (subnetwork); no additional modules",
            "task_domain": "linguistic/semantic",
            "task_name": "Same custom Machine Translation and Semantic Parsing tasks (PP-IOBJ and PP-SUBJ)",
            "task_description": "Subnetwork masks are trained using the generalization set (pruning-based probing) to find subnetworks that maximize generalization-set accuracy while inducing sparsity; evaluation uses exact-match accuracy on both test and generalization sets.",
            "compositional_depth": null,
            "composition_type": "novel structural patterns (PP modifiers in novel grammatical roles/positions)",
            "split_type": "OOD structural split (generalization set contains novel PP→NP role/position combinations absent from training)",
            "training_strategy": "Post-hoc subnetwork probing (trainable mask) on trained base models: mask training batch size 256, 300 epochs, lr=5e-4; final masks binarized at inference. Base model training unchanged (see base entry).",
            "curriculum_details": null,
            "inoculation_details": "Generalization set was split; one part used for training masks (i.e., the subnetwork probing used held-out generalization examples as supervision for mask training), the other part reserved for evaluation.",
            "iid_performance": "Keeps nearly perfect in-distribution exact-match accuracy (same as base model).",
            "compositional_performance": "PP-IOBJ: subnetwork scored &gt;90% exact-match accuracy in both machine translation and semantic parsing (text reports \"more than 90%\" for PP-IOBJ). PP-SUBJ: subnetwork performed substantially better than base model but remains far from 100% (exact numbers not given).",
            "generalization_gap": "For PP-IOBJ the subnetwork substantially closes the IID–OOD gap (from near-0 or low base-model OOD to &gt;90%); for PP-SUBJ the gap narrows but a substantial gap remains (quantities not fully specified).",
            "performance_by_depth": null,
            "performance_by_composition_type": "Causal ablations (concept scrubbing): removing all syntactic constituency or dependency typically reduces subnetwork OOD accuracy to almost 0% (exception: removing dependency in MT had a smaller effect). Removing narrow concepts (e.g., constituency only for PP modification of indirect object NPs) reduces accuracy somewhat but not to zero, indicating reliance on both syntactic features and additional non-compositional heuristics.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Comparisons include: - Base trained Transformer vs discovered subnetwork (same weights, masked): subnetwork &gt;90% on PP-IOBJ vs base much lower. - Subnetwork before/after removal of syntactic constituency/dependency (LEACE): removal typically collapses OOD performance to near 0%. - Training with additional hints (RC augmentations) vs original training (see hint augmentation): hints raise generalization accuracy by ~40% for base model and affect which features the model relies on.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "A subnetwork inside a standard Transformer can achieve high OOD accuracy for certain compositional patterns (PP-IOBJ &gt;90%) while preserving IID performance, but causal analysis shows this subnetwork uses a mix of syntactic-feature-driven processing and a non-compositional algorithm; the non-compositional component is learned early (by ~200 epochs) and persists, while syntactic/compositional behavior improves slowly with more epochs.",
            "failure_analysis": "PP-SUBJ generalization remains poor (subnetwork far from perfect). The subnetwork shows overfitting to direct-object NP modifications: removing direct-object NP-modification constituency sometimes improves PP-SUBJ accuracy, indicating maladaptive specialization.",
            "success_conditions": "Adding explicit training hints (e.g., augmenting train set with sentences containing relative-clause-modified NPs) improves OOD generalization substantially (~40% improvement reported) and makes the learned algorithm rely more on the targeted syntactic features; semantic parsing (explicit structured outputs) tends to encourage greater reliance on syntax than machine translation in some comparisons.",
            "uuid": "e2014.1"
        },
        {
            "name_short": "LEACE / Concept scrubbing",
            "name_full": "LEACE (linear concept erasure) used with sequential concept scrubbing",
            "brief_description": "A linear concept-erasure method (LEACE) applied iteratively layer-by-layer (concept scrubbing) to remove specific syntactic concepts (constituency/dependency or narrower PP-mod concepts) from model representations to test causal effects on compositional generalization.",
            "citation_title": "LEACE: Perfect linear concept erasure in closed form",
            "mention_or_use": "use",
            "model_name": "LEACE (method)",
            "model_description": "Linear concept erasure that projects representations to remove linearly encoded concept subspaces; applied sequentially to each layer ('concept scrubbing') so that classifiers cannot recover the concept more accurately than a constant baseline while preserving other information as much as possible.",
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": "analysis/intervention method rather than an architecture; operates on intermediate representations",
            "task_domain": "linguistic/semantic",
            "task_name": "Syntactic constituency and dependency concept removal tasks (multi-label sequence tagging used to represent concepts)",
            "task_description": "Multi-label tagging tasks represent constituency boundaries (begin/end of phrase) and dependency relations; narrower concepts label only tokens involved in PP modification of certain NP types (iobj-mod, dobj-mod, subj-mod). LEACE scrubs those concepts and downstream effects on main tasks are measured.",
            "compositional_depth": null,
            "composition_type": null,
            "split_type": null,
            "training_strategy": "LEACE applied sequentially layer-by-layer (concept scrubbing) using a classification task built from the same PCFG-generated sentences; linear probes used post-hoc to validate removal.",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Post-scrubbing linear probing on encoder final layer found 0% of test sentences had all labels predicted correctly in almost all removed-concept combinations; two exceptions gave max probe accuracies of 9.7% (all syntactic dependencies removed) and 1.3% (PP modification of all NPs removed), indicating strong removal success.",
            "compositional_performance": null,
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": "Empirical findings: - Removing all syntactic constituency or dependency typically collapses base-model and subnetwork generalization accuracy to almost zero (except removing dependency in MT had a smaller effect). - Removing only the constituency/dependency related to specific PP-modification types (e.g., indirect-object NP modification) reduces but does not always eliminate subnetwork generalization, showing partial dependence on those narrower features.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Ablations compare removing (a) all constituency or dependency information, (b) constituency/dependency restricted to PP-modification of particular NP roles (iobj-mod, dobj-mod, subj-mod), and (c) no removal; results show graded impacts and enable causal attribution of which syntactic features the model/subnetwork depends on.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "LEACE-based scrubbing effectively removes the targeted syntactic information (as measured by linear probes) and causally demonstrates that both base model and discovered subnetworks depend on syntactic constituency/dependency for compositional generalization; however, subnetworks also retain non-syntactic heuristics that allow partial generalization even after certain narrow syntactic features are removed.",
            "failure_analysis": "Potential limitation: LEACE is linear and might miss non-linearly encoded concepts, but probing results indicate most syntactic information used here is encoded in linear subspaces and thus effectively removed.",
            "success_conditions": "LEACE works well when the targeted concepts are linearly encoded in representations; probe validation should be used to confirm removal (as done in the paper).",
            "uuid": "e2014.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Low-complexity probing via finding subnetworks",
            "rating": 2
        },
        {
            "paper_title": "LEACE: Perfect linear concept erasure in closed form",
            "rating": 2
        },
        {
            "paper_title": "Evaluating structural generalization in neural machine translation",
            "rating": 2
        },
        {
            "paper_title": "On compositional generalization of neural machine translation",
            "rating": 1
        },
        {
            "paper_title": "Structural generalization is hard for sequence-to-sequence models",
            "rating": 1
        },
        {
            "paper_title": "COGS: A compositional generalization challenge based on semantic interpretation",
            "rating": 1
        }
    ],
    "cost": 0.014797999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Analyzing the Inner Workings of Transformers in Compositional Generalization
21 Feb 2025</p>
<p>Ryoma Kumon 
The University of Tokyo</p>
<p>Hitomi Yanaka hyanaka@is.s.u-tokyo.ac.jp 
The University of Tokyo</p>
<p>Analyzing the Inner Workings of Transformers in Compositional Generalization
21 Feb 20254294B0EDE9FBD48E76A335FC982A96B4arXiv:2502.15277v1[cs.CL]
The compositional generalization abilities of neural models have been sought after for human-like linguistic competence.The popular method to evaluate such abilities is to assess the models' input-output behavior.However, that does not reveal the internal mechanisms, and the underlying competence of such models in compositional generalization remains unclear.To address this problem, we explore the inner workings of a Transformer model by finding an existing subnetwork that contributes to the generalization performance and by performing causal analyses on how the model utilizes syntactic features.We find that the model depends on syntactic features to output the correct answer, but that the subnetwork with much better generalization performance than the whole model relies on a non-compositional algorithm in addition to the syntactic features.We also show that the subnetwork improves its generalization performance relatively slowly during the training compared to the in-distribution one, and the non-compositional solution is acquired in the early stages of the training.1 Our code and data are available at https://github.</p>
<p>Introduction</p>
<p>Compositional generalization, the ability to understand the meaning of a novel language expression based on the composition of known words and syntactic structures (Partee et al., 1984;Fodor and Pylyshyn, 1988), is a crucial aspect for robustness against unseen language data.To assess the compositional generalization abilities of neural models, most existing studies have primarily focused on evaluating model outputs in compositional generalization benchmarks (Kim and Linzen, 2020;Li et al., 2021;Dankers et al., 2022;Li et al., 2023).</p>
<p>However, the model outputs do not necessarily reflect the underlying competence because good performance in the benchmarks does not guarantee that the model implements a solution that generalizes based on compositional rules (i.e., composi- tional syntax) and vice versa.In addition, while a growing body of work on model interpretability has investigated the inner workings of Transformer (Vaswani et al., 2017)-based models (Ferrando et al., 2024;Rai et al., 2024), compositional generalization has rarely been the focus of such studies.Yao and Koller (2022) and Murty et al. (2023) analyzed the internal mechanisms in compositional generalization but did not focus on the usage of syntactic features, which is the central part of compositional generalization.Thus, the internal mechanisms of the model in compositional generalization are still unclear, and unveiling them would enhance the understanding of the model's competence.</p>
<p>In this work, we analyze the inner workings of neural models to understand what type of syntactic features the models depend on in tasks requiring compositional generalization.Our analysis method 1 consists of (i) identifying subnetworks within the model that perform well in the generalization and (ii) investigating how syntactic features causally affect the original model and its subnetwork, as shown in Figure 1.The causal analysis involves removing the linguistic concept of interest from the model and comparing the generalization performance before and after the removal.To rigorously evaluate and analyze the compositional generalization abilities, we focus not on pretrained models but on a Transformer model trained from scratch; this is because pretraining data contain syntactic structures that should be unseen in this experiment, and pretrained models may not need to generalize compositionally (Kim et al., 2022).</p>
<p>We also aim to conduct a detailed analysis by experimenting with various settings.This study employs two commonly used tasks for evaluating compositional generalization: machine translation and semantic parsing.We test the models with two patterns of compositional generalization, PP-IOBJ and PP-SUBJ, which are similar yet different (see Section 4.1 for details).</p>
<p>The findings from our experiments suggest that the model and its subnetwork that contributes to the generalization performance indeed leverage syntactic structures.However, intriguingly, we also discover that the subnetwork implements other solutions that do not depend on syntactic structures.From these results, we argue that the solutions that the models employ are partly non-compositional.Moreover, analysis of the model at different epochs during the training revealed that the model gradually develops a subnetwork with better generalization performance.The causal analysis of the subnetwork showed that the non-compositional solution was learned during the early phase of the training.We argue that Transformer models need a better inductive bias to generalize compositionally utilizing syntactic features.</p>
<p>Background</p>
<p>Compositional Generalization</p>
<p>Several studies have explored the compositional generalization abilities of modern neural models by focusing on the model performance in tasks such as semantic parsing (Kim and Linzen, 2020;Csordás et al., 2021;Yao and Koller, 2022;Kim et al., 2022;Li et al., 2023) and machine translation (Li et al., 2021;Dankers et al., 2022;Kumon et al., 2024).Such studies indicate that the models lack compocom/ynklab/CG_interp. sitional generalization abilities in general, while others have worked on improving them by modifying the model architectures (Bergen et al., 2021;Ontanon et al., 2022).Yao and Koller (2022) briefly analyzed which part of seq2seq models causes poor performance in compositional generalization by probing the encoder.They attributed the poor performance to the decoder, stating that the encoder has the linguistic knowledge needed to solve the generalization tasks but the decoder does not use it.These studies focused mainly on the model outputs and the encoded properties, and there has been a debate on whether behavioral evaluation is sufficient to assess compositionality in neural models (Mc-Curdy et al., 2024).Our research analyzes the inner workings of models with subnetwork search and causal analysis, which should give insights into what features causally impact the model behavior.We consider that a compositional solution generalizes by leveraging syntactic structures consistently in a bottom-up manner.In our experiments, we focus on syntactic features that a compositional solution should utilize.Thus, we regard a solution that does not employ these features as noncompositional.</p>
<p>Linguistic Approach to Interpretability</p>
<p>One approach to interpreting neural models is to study the causal relationship between the target feature and the model behavior based on interventions.Interventions alter either a model's inputs or its inner representations so that the target feature is the only change, and they test how that change affects the model's outputs.Recent work has analyzed the linguistic mechanisms (Tucker et al., 2021;Elazar et al., 2021;Ravfogel et al., 2021;Feder et al., 2021;Amini et al., 2023;Belrose et al., 2023;Arora et al., 2024).Belrose et al. (2023) utilized concept erasure, which removes only the concept of interest from the model and tests causal links by comparing the predictions by the model before and after the removal.We employ their method for causally analyzing the causal role of syntactic features in compositional generalization because of its compatibility with our compositional generalization tasks.</p>
<p>Another line of work has explored finding subnetworks with specific properties of interest as a method for model analysis.Cao et al. (2021) proposed subnetwork probing, a pruning-based method that searches for a subnetwork that performs a target linguistic task.As for linguis-tic generalization, previous studies have found subnetworks that perform syntactic generalization (Bhaskar et al., 2024), hierarchical generalization (Ahuja et al., 2024), andcompositional generalization (Hu et al., 2024).</p>
<p>Analysis Method</p>
<p>Training Base Model</p>
<p>To test the compositional generalization abilities of a model, we first construct a dataset that contains the training set, in-distribution test set, and out-of-distribution generalization set.Hereinafter, we refer to the in-distribution test set as the test set and the out-of-distribution generalization set as the generalization set.The generalization set contains unseen syntactic structures that are combinations of those in the training set and requires models to fill the gaps.The dataset is constructed based on a rulebased pipeline used in SGET (Kumon et al., 2024), which evaluates the compositional generalization abilities of neural models on English-Japanese translation tasks.The strict control of sentence generation in SGET utilizing PCFGs (Probabilistic Context-Free Grammars) allows for controlled gaps between the training set and generalization set, which enables the precise evaluation of generalization abilities.</p>
<p>Next, we train a Transformer model from scratch with the training set.As for training tasks, we adopt machine translation and semantic parsing, which have been commonly used in existing studies of evaluating compositional generalization.The reason for using two tasks instead of just one is to investigate how the output format of a task impacts the models' inner processes.The logical forms in the semantic parsing dataset are created mostly based on the rules proposed by Reddy et al. (2017) and postprocessings, following Kim and Linzen (2020).We remove redundant tokens while maintaining semantic interpretation, following Wu et al. (2023).</p>
<p>Subnetwork Probing</p>
<p>Neural models have been shown to develop subnetworks for several types of linguistic generalization (Bhaskar et al., 2024;Ahuja et al., 2024) and modular solutions for compositionality (Lepori et al., 2023).Based on these findings, we hypothesize that the vanilla Transformer develops a subnetwork that generalizes compositionally.To test this hypothesis, to the trained base model we apply subnetwork probing (Cao et al., 2021), a method for discovering an existing subnetwork that achieves high accuracy on a task.Subnetwork probing performs pruning-based probing by training a learnable mask.This method is shown to have low complexity, which means that the mask itself does not learn the task much, and the abilities of the original models are preserved as desired.</p>
<p>In this work, we acquire a subnetwork that performs well in compositional generalization, if any, through subnetwork probing.We use the generalization set to train masks and prune the models.The details of subnetwork probing are in Appendix A.</p>
<p>Causal Analysis</p>
<p>Next, we analyze the trained models and discovered subnetworks in terms of the extent to which they depend on syntactic structures to generate answers in machine translation and semantic parsing.One of the methods for analyzing the inner workings is to remove target features from a model and observe the causal effect of the removal.LEACE (Belrose et al., 2023) is a method for concept erasure in which only the target concept is removed, with as little impact on the original model as possible.The method updates inner representations so that no linear classifiers can predict concept labels more accurately than a constant function and other concepts are preserved in the model.Removing a concept from deep neural networks is achieved by a procedure called concept scrubbing (Belrose et al., 2023), which sequentially applies LEACE to every layer of a model from the first to the last.In particular, after LEACE is applied to a layer and scrubs a concept therein, the scrubbed representations are passed to the next layer, where LEACE is applied again.</p>
<p>We apply concept scrubbing to both the base models and the discovered subnetworks, removing the target syntactic knowledge.After the concept removal, we evaluate the model predictions in the test set and generalization set of machine translation and semantic parsing.Comparing the model performances before and after the concept removal reveals the causal effect of the syntactic feature of interest on the predictions.Concept scrubbing is  suitable for our analysis because it does not require creating alternative inputs or interchange interventions, which are difficult to define for controlling syntactic features in the generalization setting.</p>
<p>Concept scrubbing uses a classification task that represents the concept of interest to erase it.We choose syntactic constituency and syntactic dependency as the concepts for model analysis.We define multi-label classification tasks to represent each concept based on sequence tagging tasks by Elazar et al. (2021), as shown in Figure 3.The task for syntactic constituency is tagging the beginning and end of a phrase, and the task for syntactic dependency is labeling dependency relations.We also test the impact of removing narrower concepts, which differs according to each generalization pattern, such as syntactic constituency regarding only the prepositional phrase (PP) modification of an indirect object noun phrase (NP).In this case, labels are assigned only to the tokens involved in the concepts.For example, when considering syntactic constituency regarding the PP modification of an indirect object NP, the beginning and end of the PP and the NP containing the PP and the modified NP (not the modified NP) are labeled as ones.</p>
<p>As for the dataset for this classification task, English sentences are generated using the same rulebased method as the one for the main task datasets.The labels are tagged based on syntax trees generated as by-products in the sentence generation process.Constituency boundaries are based on the Penn Treebank (Marcus et al., 1993) definitions, and dependency relations are based on the Universal Dependencies (McDonald et al., 2013) definitions.Using these tasks and datasets, we test whether the models depend on syntactic constituency and syntactic dependency in compositional generalization.</p>
<p>Experimental settings 4.1 Compositional Generalization Pattern</p>
<p>In this work, we focus on two compositional generalization patterns, i.e., PP in indirect object NP (PP-IOBJ) and PP in subject NP (PP-SUBJ), as shown in Table 1.Syntactically, these two patterns are relatively simple, which allows for easier causal analyses using multi-label classification tasks.The child gave the pen on the table to Liam.</p>
<p>The friend gave the girl in the room a hat.</p>
<p>PP in subject NP</p>
<p>The child broke a cup on the table.</p>
<p>The friend in the room broke a cup.NP goes across the PP, which makes the generalization more complex.</p>
<p>For causal analysis in this pattern, we test the impact of three narrower syntactic constituency and dependency concepts, namely, the PP modification of indirect object NPs, direct object NPs, and all NPs, along with overall syntactic constituency and dependency.</p>
<p>PP in subject NP (PP-SUBJ) Similarly to PP-IOBJ, all the NPs modified by PPs in the training set appear in the direct object position.Models trained on the training set are expected to generalize to PPs modifying subject NPs in the generalization set.One aspect that makes PP-SUBJ difficult is that PP modifiers do not appear at the beginning of sentences in the training set.The models may have to generalize to the novel placement of PP modifiers in addition to the novel grammatical role of modified NPs.To mitigate this issue, Wu et al. (2023) added sentences with preposed PP modifiers in the training set, but we avoid that approach for the sake of simpler comparisons between PP-IOBJ and PP-SUBJ.</p>
<p>For causal analysis in this pattern, we test the impact of three narrower syntactic constituency and dependency concepts, namely, the PP modification of subject NPs, direct object NPs, and all NPs, along with overall syntactic constituency and dependency.</p>
<p>Dataset</p>
<p>As explained in Section 3, we newly construct datasets for each of machine translation and semantic parsing and for classification tasks used in concept scrubbing, using PCFGs with vocabulary of 123 proper nouns, 423 common nouns, 178 verbs, and 43 adjectives.Each of the machine translation and semantic parsing datasets consists of a training set of 80,000 samples, a test set of 10,000 samples, and a generalization set of 30,000 samples.We split the generalization set into two parts: one part is used in training masks in subnetwork probing (Section 3.2), and the other is used in evaluating the trained models and subnetworks (Section 3.3).Note that the generalization set is constructed for each generalization pattern, and subnetwork probing is performed for each pattern as well.The dataset for classification tasks contains 9,000 samples, and all of them are used for concept scrubbing.</p>
<p>Training Details</p>
<p>We train an encoder-decoder Transformer model from scratch on our dataset.The model has 3 encoder and 3 decoder layers, 4 attention heads.We set the batch size to 256, the number of epochs to 500, the learning rate to 0.0001, and the weight decay to 0.1.We do not use early stopping because Csordás et al. (2021) showed that continued training without it improves model performance in compositional generalization.</p>
<p>As for subnetwork probing, we train a pruning mask for the trained model.We set the batch size to 256, the number of training epochs to 300, and the learning rate to 0.0005.We do not use early stopping in subnetwork probing.</p>
<p>We run the experiments three times with random seeds and report the average scores as the results.The final checkpoints of each training run are used for the main results (Section 5).</p>
<p>Evaluation Metric</p>
<p>Following previous studies of evaluating compositional generalization (Kim and Linzen, 2020;Kumon et al., 2024), we adopt exact match accuracy as the evaluation metrics for both machine translation and semantic parsing.The rule-based pipeline for our dataset generation is designed so that a correct output can be determined uniquely if a model follows compositional rules; thus, using exact match accuracy in this experiment is appropriate.</p>
<p>Results</p>
<p>Output Evaluation</p>
<p>Before analyzing the inner workings of the models, we present the model performance in main  tasks without concept scrubbing.Table 2 shows the results of the base models and subnetworks in machine translation and semantic parsing.</p>
<p>The base models and subnetworks both performed nearly perfectly in the test set of both machine translation and semantic parsing.The performance of the base model was much worse in both PP-IOBJ and PP-SUBJ than that in the test set, which is consistent with the results of previous studies testing the same generalization patterns (Li et al., 2023;Kumon et al., 2024).On the other hand, the subnetwork scored more than 90% accuracy in PP-IOBJ in both main tasks while keeping the in-distribution performance.This suggests that some part of the trained model implements a certain algorithm that solves these compositional general-ization tasks.The subnetwork in PP-SUBJ also performed much better in the generalization set than did the base model.It is surprising to see these positive results, considering that previous studies have shown Transformers' poor performance in compositional generalization tasks.</p>
<p>Causal Analysis</p>
<p>Generalization in PP-IOBJ</p>
<p>Figures 4a-4d present the results of causal analysis of the base model and subnetwork in PP-IOBJ.First, the base model performed much worse when syntactic constituency or dependency was removed, which shows the model's reliance on these syntactic features to correctly solve the main tasks.However, the base model cannot be considered to have a compositionally generalizing solution because the generalization performance overall was far from perfect, and a compositionally generalizing model should perform nearly perfectly in the generalization set.</p>
<p>Next, we focus on the subnetwork, which achieved better accuracy in the generalization set than did the base model.Entirely removing syntactic constituency or dependency decreased the accuracy of the subnetwork to almost zero except when removing syntactic dependency in machine translation.This shows that the subnetwork also depends on the syntactic features in general.</p>
<p>We then discuss the impact of the removal of constituency information on the modification of indirect object NPs.If the subnetwork implements a compositional solution, then removing the constituency information on the modification of indirect object NPs would decrease the accuracy to zero.However, the difference in the performance before and after this concept removal is not as large as when the concept of the constituency is removed entirely, although the generalization performance of the subnetwork decreases to some extent.A similar trend was seen in the removal of syntactic dependency, although the decline in the generalization performance was smaller.These results suggest that the subnetwork depends somewhat on the constituency and dependency regarding the modification of indirect object NPs.At the same time, the subnetwork implements a solution that somehow handles PP-IOBJ in machine translation and semantic parsing yet cannot be considered as a compositionally generalizing one.Furthermore, regarding the differences in the results between the two tasks, the drop in the generalization performance after the removal of syntactic information was smaller in semantic parsing than in machine translation.It indicates that the models relied on syntactic features instead of a noncompositional solution more in semantic parsing than in machine translation.</p>
<p>Generalization in PP-SUBJ</p>
<p>In contrast to PP-IOBJ, the generalization performance of the subnetwork is far from 100% accuracy, so the subnetwork is not expected to have a perfect solution that generalizes compositionally in PP-SUBJ.However, we still examine on what the subnetwork depends in the main tasks.</p>
<p>Figures 4e-4h show the results of causal analysis of the subnetwork in PP-SUBJ.Similar to PP-IOBJ, when the information of syntactic constituency or dependency was entirely removed, the performance of the subnetwork dropped to almost 0% except when removing syntactic dependency in machine translation.The subnetwork depends on this information in PP-SUBJ as well.</p>
<p>As for the impact of the removal of the constituency regarding the modification of subject NPs, the performance dropped almost as much as with the removal of the entire constituency.This suggests the subnetworks' heavy reliance on the modification of subject NPs and their ability to properly use compositional rules at least when the output is correct.On the other hand, when the constituency regarding direct object NPs was removed, the performance improved considerably especially in semantic parsing.This implies that the subnetwork was overfitted to the modification of direct object NPs, and this prevented the subnetwork from achieving better accuracy in PP-SUBJ.Also, the subnetwork seems capable of using the information of modification of NPs regardless of whether NPs are subjects or direct objects, as this removal would not improve the accuracy if the subnetwork learns only that the modification can only come with direct object NPs.</p>
<p>Transition During Training</p>
<p>We present how the model performance evolved throughout training in PP-IOBJ, shown in Figure 5.As can be seen, the accuracy in the test set grew rapidly, whereas the accuracy in the generalization set improved slowly.In addition, comparing the base model and subnetwork in machine translation, the generalization performance of the subnetwork continued to improve through 500 epochs, whereas that of the base model improved only slightly.Therefore, the model may gradually learn an algorithm that can solve the generalization task through the training process with the machine translation task without changing the behavior of the whole model much.The difference in the performance transition between the base model and subnetwork is much less noticeable in semantic parsing.These results suggest that differences in the task settings-such as output formats where structures are represented more explicitly in semantic parsing-influence generalization performance.A similar tendency was discovered in PP-SUBJ (see Appendix B for details), although the generalization performance was generally lower.</p>
<p>Next, we investigate how the inner workings of the subnetwork changed as the training went on.Figure 6 shows the shift of the generalization performance in machine translation of the subnetwork with each linguistic feature removed.In machine translation, the generalization performance of the subnetwork with certain syntactic feature removed was mostly consistent after 200 epochs.This strongly suggests that the subnetwork learned a non-compositional solution in the early stage of the training and retained it throughout the training.Combined with the observation that the original subnetwork continued to improve its accuracy beyond 200 epochs, this result also indicates that a compositional solution relying on syntactic features was acquired gradually.</p>
<p>Similarly, in semantic parsing, the subnetwork with a certain linguistic feature removed mostly retained its performance after 200 epochs, regardless of the generalization accuracy of the original subnetwork.The detailed results of semantic parsing are in Appendix B.</p>
<p>Discussion</p>
<p>Reliability of LEACE</p>
<p>Since LEACE is a linear concept erasure method, it may fail to remove concepts encoded non-linearly.However, the results when all syntactic information was removed (Figure 4) indicate that most syntactic information used in machine translation and semantic parsing is encoded in linear subspaces.Furthermore, we use linear probing to assess whether the concepts are perfectly removed after the application of concept scrubbing.We probe the representation of the final layer of the encoder after each concept scrubbing, and measure the accuracy in the multi-label classification tasks used in concept scrubbing.As a result, the probing classifier predicted the correct labels for all the words in 0% of the test sentences in almost all combination of the removed concepts, generalization patterns, and main tasks.The only exceptions occurred when either all syntactic dependencies or those related to the PP modification of all NPs were removed, with maximum accuracies of 9.7% and 1.3%, respectively.Thus, the impact of non-linearly encoded features should be negligible, and concept scrubbing effectively removes syntactic features.We also validate that LEACE does not erase concepts orthogonal to syntactic ones.We test the model with a word-to-word translation (English to Japanese) of content words; word-to-word translation of content words can be solved without relying on syntax at all.We probe the representation of the final layer of the encoder by a one-layer linear classifier for word-to-word translation, focusing on evaluating the models trained with machine translation datasets.Accuracy is calculated as the proportion of sentences in which all content words are translated correctly.As shown in Table 3, the results suggest that the removal of syntactic features only slightly decreases performance in wordto-word translation, confirming that concepts orthogonal to syntactic ones are mostly preserved in LEACE.</p>
<p>Impact of Adding Hints in Training</p>
<p>Finally, we investigate how a Transformer model performs under a setting where compositional generalization is easier.We augment the training set with sentences containing syntactic structures that provide clues for generalization.In particular, we focus on PP-IOBJ and augment the training set with sentences that have a relative clause (RC) modifying an indirect object NP and with ones that have an RC modifying a direct object NP.It should be easier for the model trained with the data involving PPs modifying direct object NPs to generalize to a PP modifying an indirect object NP based on the newly provided hints.</p>
<p>Figure 7 presents the results of causal analysis.Compared with the results without any hint (Figures 4a and 4b), the generalization performance without any concept removal improved by about 40% in both main tasks.Moreover, the performance after the removal of syntactic features regarding the PP modifications of indirect object NPs improved only slightly.This suggests that the algorithm that was implemented in the base model and contributed to the gain in the generalization performance relied on those specific syntactic features.Thus, the model might implement a more robust compositional solution by utilizing the provided hints.</p>
<p>Conclusion</p>
<p>In this work, we investigated the inner mechanisms of a Transformer model in compositional generalization tasks.The experimental results showed that the model utilizes syntactic features to some extent in the generalization but that its subnetwork with better generalization accuracy depends on non-syntactic features as well.This indicates that the model develops a non-compositional solution internally and fails to generalize compositionally even when the generalization performance is decent.This paper serves as a foundation for analyzing the underlying mechanisms in compositional generalization from the linguistic perspective.Future work might consider other generalization patterns or other linguistic features to obtain a more profound insight into the linguistic competence of neural models.</p>
<p>Limitations</p>
<p>The compositional generalization tasks used in this work are based on synthetically generated datasets and so might not represent sufficiently the variety in natural language expressions.However, these controlled settings are required for precise evaluation requires because all the lexical items and syntactic structures must be split properly into the training and generalization sets.Therefore, using a natural corpus for this experiment would have required much effort, and we leave that for future work.</p>
<p>Another limitation is that the results of this experiment do not necessarily transfer to larger models because we tested relatively small models trained on a small synthetic dataset following previous studies.It would be worth exploring how the trends discovered here change as the model size increases.</p>
<p>Ethical Considerations</p>
<p>All of our datasets were constructed for the sole purpose of the model analysis from the linguistic perspective.They contain no potentially harmful or offensive content.</p>
<p>Figure 1 :
1
Figure 1: In this study, we investigate what neural models employ in compositional generalization tasks.</p>
<p>Figure 2
2
Figure 2 presents our method of analyzing the inner workings of a Transformer model in compositional generalization.It consists of the following three phases: training a base model, subnetwork probing, and causal analysis.</p>
<p>Figure 2 :
2
Figure 2: Overview of analysis process, which consists of three phases: training base models, subnetwork probing, and causal analysis.</p>
<p>Figure 3 :
3
Figure 3: Multi-label classification task used in concept scrubbing for removing syntactic constituency.</p>
<p>PP in indirect object NP (PP-IOBJ) In this pattern, all the NPs modified by PPs in the training set appear in the direct object position.Then, models trained on the training set are expected to generalize to PPs modifying indirect object NPs in the generalization set.As Li et al. (2023) pointed out, some sentences have an indirect object NP modified by a PP before a direct object NP, and the dependency between a verb and the direct object Pattern Training Generalization PP in indirect object NP</p>
<p>Figure 4 :
4
Figure4: Results of causal analysis in PP-IOBJ (4a-4d) and in PP-SUBJ (4e-4h).Each bar shows the performance in the generalization set after the corresponding concept removal."All" refers to entirely removing the corresponding syntactic feature."Iobj-mod" (resp."dobj-mod", "subj-mod") refers to removing the corresponding syntactic feature regarding the PP modifications of indirect object (resp.direct object, subject) NPs."Mod" refers to removing the corresponding syntactic feature regarding the PP modifications.</p>
<p>Figure 5 :
5
Figure 5: Shift of average accuracy of models in PP-IOBJ over training epochs.Test and Gen. stand for the accuracy on the test and generalization set, respectively.Base and Sub.stand for the accuracy of the base model and subnetwork, respectively.</p>
<p>Figure 6 :
6
Figure 6: Shift of average accuracy on the generalization set of the subnetworks with each concept removed in machine translation over training epochs.</p>
<p>Figure 7 :
7
Figure 7: Results of causal analysis in PP-IOBJ when trained with a hint.</p>
<p>Table 1 :
1
Two compositional generalization patterns tested in the experiments.</p>
<p>Table 2
2: Average exact match accuracy (%) of basemodels and subnetworks in machine translation (MT)and semantic parsing (SP). PP-IOBJ Sub. (resp. PP-SUBJSub.) stands for the subnetwork for PP-IOBJ (resp. PP-SUBJ). The column labeled PP-IOBJ (resp. PP-SUBJ)represents the generalization performance in PP-IOBJ(resp. PP-SUBJ).</p>
<p>Table 3 :
3
Average accuracy in word-to-word translation of content words.
ModelOriginalConstituencyDependencyremovedremovedBase90.1±2.888.8±4.690.0±3.6PP-IOBJ Sub.87.9±4.883.8±6.389.3±3.0PP-SUBJ Sub. 88.0±2.181.2±5.488.2±2.3
AcknowledgmentsThis work was supported by JSPS KAKENHI grant number JP24H00809, JST PRESTO grant number JPMJPR21C8.A Subnetwork ProbingSubnetwork probing(Cao et al., 2021)trains a mask to find a subnetwork of interest.Let ϕ ∈ R d be the weights of a model and Z i ∈ [0, 1] be the mask for the weight ϕ i .Z i follows the hard concrete function parameterized with temperature β i and a random variable θ i , that is,and ζ = 1.1 and γ = −0.1 are fixed here.Subnetwork probing optimizes the mask parameter θ by minimizing the following loss function:The first term is the loss function for the model f masked by Z i = z(U i , θ i ), and the second term corresponds to the penalty for non-zero masks to induce sparsity.During inference, the mask Z i is binarized to {0, 1} based on a threshold.B Other Results of the Transition During TrainingFigure8shows how the model performance changes during training in PP-SUBJ, and Figure9shows the shift of the generalization performance in semantic parsing when a certain syntactic feature is removed.C Details of Discovered SubnetworksFollowingCao et al. (2021), we calculated the proportion of unmasked weights, and its trend was mostly the same for all tasks and patterns.The proportion of unmasked weights in each encoder and decoder layer was around 50%-70%, but the proportion was larger for deeper layers in both the encoder and decoder.Also, there were generally more unmasked weights in the decoder than in the encoder.As for the proportion of unmasked weights in multilayer perceptron (MLP) blocks and attention blocks, most layers had more unmasked weights in MLP blocks than in attention blocks.Table4and 5 show the proportions of unmasked weights in the extracted subnetworks.Table5: Average proportion of unmasked weights in PP-IOBJ and machine translation.D Computational ResourcesWe used NVIDIA V100 GPUs for all the experiments.The total runtime for the training and evaluation was around 300 hours.
Learning syntax without planting trees: Understanding when and why transformers generalize hierarchically. Kabir Ahuja, Vidhisha Balachandran, Madhur Panwar, Tianxing He, Noah A Smith, Navin Goyal, Yulia Tsvetkov, arXiv:2404.163672024PreprintVersion 2</p>
<p>Naturalistic causal probing for morpho-syntax. Afra Amini, Tiago Pimentel, Clara Meister, Ryan Cotterell, 10.1162/tacl_a_00554Transactions of the Association for Computational Linguistics. 112023</p>
<p>CausalGym: Benchmarking causal interpretability methods on linguistic tasks. Aryaman Arora, Dan Jurafsky, Christopher Potts, 10.18653/v1/2024.acl-long.785Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241Long Papers)</p>
<p>LEACE: Perfect linear concept erasure in closed form. Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, Stella Biderman, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Systematic generalization with edge transformers. Leon Bergen, Timothy J O'donnell, Dzmitry Bahdanau, Thirty-fifth Conference on Neural Information Processing Systems. 2021</p>
<p>The heuristic core: Understanding subnetwork generalization in pretrained language models. Adithya Bhaskar, Dan Friedman, Danqi Chen, 10.18653/v1/2024.acl-long.774Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Low-complexity probing via finding subnetworks. Steven Cao, Victor Sanh, Alexander Rush, 10.18653/v1/2021.naacl-main.74Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>The devil is in the detail: Simple tricks improve systematic generalization of transformers. Róbert Csordás, Kazuki Irie, Juergen Schmidhuber, 10.18653/v1/2021.emnlp-main.49Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>The paradox of the compositionality of natural language: A neural machine translation case study. Verna Dankers, Elia Bruni, Dieuwke Hupkes, 10.18653/v1/2022.acl-long.286Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Amnesic probing: Behavioral explanation with amnesic counterfactuals. Yanai Elazar, Shauli Ravfogel, Alon Jacovi, Yoav Goldberg, 10.1162/tacl_a_00359Transactions of the Association for Computational Linguistics. 92021</p>
<p>CausaLM: Causal model explanation through counterfactual language models. Amir Feder, Nadav Oved, Uri Shalit, Roi Reichart, 10.1162/coli_a_00404Computational Linguistics. 4722021</p>
<p>A primer on the inner workings of transformer-based language models. Javier Ferrando, Gabriele Sarti, Arianna Bisazza, Marta R Costa-Jussà, arXiv:2405.002082024PreprintVersion 3</p>
<p>Connectionism and cognitive architecture: A critical analysis. Jerry A Fodor, Zenon W Pylyshyn, Cognition. 281-21988</p>
<p>Compositional cores: Persistent attention patterns compositionally generalizing subnetworks. Y Michael, Chuan Hu, Tal Shi, Linzen, Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP2024</p>
<p>COGS: A compositional generalization challenge based on semantic interpretation. Najoung Kim, Tal Linzen, 10.18653/v1/2020.emnlp-main.731Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Uncontrolled lexical exposure leads to overestimation of compositional generalization in pretrained models. Najoung Kim, Tal Linzen, Paul Smolensky, arXiv:2212.1076920221Preprint</p>
<p>Evaluating structural generalization in neural machine translation. Ryoma Kumon, Daiki Matsuoka, Hitomi Yanaka, 10.18653/v1/2024.findings-acl.783Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Break it down: Evidence for structural compositionality in neural networks. A Michael, Thomas Lepori, Ellie Serre, Pavlick, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>SLOG: A structural generalization benchmark for semantic parsing. Bingzhi Li, Lucia Donatelli, Alexander Koller, Tal Linzen, Yuekun Yao, Najoung Kim, 10.18653/v1/2023.emnlp-main.194Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>On compositional generalization of neural machine translation. Yafu Li, Yongjing Yin, Yulong Chen, Yue Zhang, 10.18653/v1/2021.acl-long.368Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Building a large annotated corpus of English: The Penn Treebank. Mitchell P Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, Computational Linguistics. 1921993</p>
<p>Toward compositional behavior in neural models: A survey of current views. Kate Mccurdy, Paul Soulos, Paul Smolensky, Roland Fernandez, Jianfeng Gao, 10.18653/v1/2024.emnlp-main.524Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Universal Dependency annotation for multilingual parsing. Ryan Mcdonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Täckström, Claudia Bedini, Núria Bertomeu Castelló, Jungmee Lee, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational LinguisticsSofia, BulgariaAssociation for Computational Linguistics20132Short Papers)</p>
<p>Characterizing intrinsic compositionality in transformers with tree projections. Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher D Manning, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Making transformers solve compositional tasks. Santiago Ontanon, Joshua Ainslie, Zachary Fisher, Vaclav Cvicek, 10.18653/v1/2022.acl-long.251Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>. Barbara Partee, Compositionality. Varieties of formal semantics. 31984</p>
<p>A practical review of mechanistic interpretability for transformer-based language models. Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, Ziyu Yao, arXiv:2407.026462024PreprintVersion 1</p>
<p>Counterfactual interventions reveal the causal effect of relative clause representations on agreement prediction. Shauli Ravfogel, Grusha Prasad, Tal Linzen, Yoav Goldberg, 10.18653/v1/2021.conll-1.15Proceedings of the 25th Conference on Computational Natural Language Learning. the 25th Conference on Computational Natural Language LearningOnline. Association for Computational Linguistics2021</p>
<p>Mark Steedman, and Mirella Lapata. Siva Reddy, Oscar Täckström, Slav Petrov, 10.18653/v1/D17-1009Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017Universal semantic parsing</p>
<p>What if this modified that? syntactic interventions with counterfactual embeddings. Mycal Tucker, Peng Qian, Roger Levy, 10.18653/v1/2021.findings-acl.76Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Thirty-first Conference on Neural Information Processing Systems. 2017</p>
<p>ReCOGS: How incidental details of a logical form overshadow an evaluation of semantic interpretation. Zhengxuan Wu, Christopher D Manning, Christopher Potts, 10.1162/tacl_a_00623Transactions of the Association for Computational Linguistics. 112023</p>
<p>Structural generalization is hard for sequence-to-sequence models. Yuekun Yao, Alexander Koller, 10.18653/v1/2022.emnlp-main.337Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>            </div>
        </div>

    </div>
</body>
</html>
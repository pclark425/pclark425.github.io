<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5959 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5959</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5959</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-262217537</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.12881v2.pdf" target="_blank">Affect Recognition in Conversations Using Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Affect recognition, encompassing emotions, moods, and feelings, plays a pivotal role in human communication. In the realm of conversational artificial intelligence, the ability to discern and respond to human affective cues is a critical factor for creating engaging and empathetic interactions. This study investigates the capacity of large language models (LLMs) to recognise human affect in conversations, with a focus on both open-domain chit-chat dialogues and task-oriented dialogues. Leveraging three diverse datasets, namely IEMOCAP (Busso et al., 2008), EmoWOZ (Feng et al., 2022), and DAIC-WOZ (Gratch et al., 2014), covering a spectrum of dialogues from casual conversations to clinical interviews, we evaluate and compare LLMs’ performance in affect recognition. Our investigation explores the zero-shot and few-shot capabilities of LLMs through in-context learning as well as their model capacities through task-specific fine-tuning. Additionally, this study takes into account the potential impact of automatic speech recognition errors on LLM predictions. With this work, we aim to shed light on the extent to which LLMs can replicate human-like affect recognition capabilities in conversations.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5959",
    "paper_id": "paper-262217537",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004302749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Affect Recognition in Conversations Using Large Language Models</p>
<p>Shutong Feng fengs@hhu.de 
Heinrich Heine University Düsseldorf
Germany</p>
<p>Guangzhi Sun 
University of Cambridge
UK</p>
<p>Nurul Lubis lubis@hhu.de 
Heinrich Heine University Düsseldorf
Germany</p>
<p>Wen Wu 
University of Cambridge
UK</p>
<p>Chao Zhang 
Tsinghua University
China</p>
<p>Milica Gašić 
Heinrich Heine University Düsseldorf
Germany</p>
<p>Affect Recognition in Conversations Using Large Language Models
88861E528A84F712D38C7EE3EF98DCD4
Affect recognition, encompassing emotions, moods, and feelings, plays a pivotal role in human communication.In the realm of conversational artificial intelligence, the ability to discern and respond to human affective cues is a critical factor for creating engaging and empathetic interactions.This study investigates the capacity of large language models (LLMs) to recognise human affect in conversations, with a focus on both open-domain chit-chat dialogues and task-oriented dialogues.Leveraging three diverse datasets, namely IEMOCAP(Busso et al., 2008), EmoWOZ (Feng et al., 2022), and DAIC-WOZ (Gratch et al., 2014), covering a spectrum of dialogues from casual conversations to clinical interviews, we evaluate and compare LLMs' performance in affect recognition.Our investigation explores the zero-shot and few-shot capabilities of LLMs through incontext learning as well as their model capacities through task-specific fine-tuning.Additionally, this study takes into account the potential impact of automatic speech recognition errors on LLM predictions.With this work, we aim to shed light on the extent to which LLMs can replicate human-like affect recognition capabilities in conversations.</p>
<p>Introduction</p>
<p>Affect refers to the broad range of subjective experiences related to emotions, moods, and feelings (Russell, 1980).It encompasses the various ways individuals perceive, experience, and express their emotional states and is an essential aspect of human experience and communication (Gross, 2002).</p>
<p>The ability to recognise human affect is an important ability of conversational artificial intelligence (AI, Mayer et al. 1999).It empowers the dialogue agent to go beyond mere information exchange and engage users on an emotional level.By leveraging affect recognition techniques, they can discern the emotional nuances in user inputs, including sentiment, mood, and subtle cues like sarcasm or frustration (Picard, 1997).This capability allows the system to respond with greater sensitivity, empathy, and relevance, leading to more meaningful and satisfying interactions (Zeng et al., 2009).</p>
<p>Large language models (LLMs) have demonstrated promising performance in many tasks (Beeching et al., 2023).They have also shown promising capability in adapting to new tasks via prompting (Heck et al., 2023;Sun et al., 2023), in-context learning (ICL, Zhao et al. 2023), as well as task-specific finetuning (Taori et al., 2023).With the advancement in LLMs, it is possible to use LLMs as the backend of dialogue systems (OpenAI, 2022(OpenAI, , 2023;;Touvron et al., 2023b).This brings up the question: can LLMs recognise human affects in conversations in a similar capacity as human beings?</p>
<p>In the context of conversational AI, dialogues can be broadly categorised into two classes: 1) chitchat or open-domain dialogues where users interact with the system for entertainment and engagement, and 2) task-oriented dialogues (ToDs) where users converse with the system for specific goals (Jurafsky and Martin, 2009).Under ToDs, depending on the type of user goals, dialogues can be further grouped as information-retrieval, medical consultations, education, and many more.</p>
<p>Regarding the affective information in conversations, we are particularly interested in the following: (1) categorical emotion classes from generic emotion models such as "basic emotions" proposed by Ekman and Friesen (1971), (2) custom categorical emotion classes defined for a particular context, such as the emotion labels defined by Feng et al. (2022) to encode task performance simultaneously in ToDs, and (3) depression, a medical illness that negatively affects how a person feels, thinks and acts, and causes feelings of sadness and/or a loss of interest in activities the person once enjoyed (Amer-arXiv:2309.12881v2[cs.CL] 5 Aug 2024 ican Psychiatric Association, 2020).</p>
<p>The emergence of LLMs has signified a shift of paradigm from training small models for one specific task to large models for multiple tasks.Therefore, in this work, we investigate the affect recognition ability of a range of LLMs on vastly different types of dialogues and labels1 to ascertain the validity of this direction.Specifically,</p>
<p>• We evaluated and compared the ability of a range of LLMs to recognise human affect under different dialogue set-ups (chit-chat dialogues and ToDs) and recognition targets (emotion classes and binary depression diagnosis).We used the following datasets: IEMOCAP (Busso et al., 2008), EmoWOZ (Feng et al., 2022), and DAIC-WOZ (Gratch et al., 2014).• We investigated into LLMs' zero-shot and fewshot capabilities through an array of ICL set-ups as well as their model capacities through taskspecific fine-tuning.• We considered text-based LLMs as a part of spoken dialogues systems.Therefore, we also experimented with inputs containing automatic speech recognition (ASR) errors to investigate the potential influence of ASR errors on LLM predictions.</p>
<p>2 Related Work</p>
<p>LLM</p>
<p>Large Language Model (LLM) refers to a type of pre-trained models designed for natural language processing tasks.LLMs are characterised by their enormous number of model parameters and extensive training data.</p>
<p>Some well-known examples of LLMs include Ope-nAI GPT family models (Radford et al., 2019;Ope-nAI, 2022, 2023), which can have billions or even trillions of model parameters.Examples of opensource text-based foundation models include the LLaMA family (Touvron et al., 2023a,b;AI@Meta, 2024) and their corresponding chat-optimised models.These models have demonstrated remarkable abilities in various natural language understanding and generation tasks, including text completion, language translation, text summarisation, and even chatbot applications (Beeching et al., 2023).They also demonstrate "emergent abilities" such as few-shot prompting and chain-of-thought reasoning, which were not present in their smaller predecessors (Wei et al., 2022).While there are also multi-modal LLMs such as SALMONN (Tang et al., 2024), these are at an earlier stage compared to uni-modal text-based LLMs, and it is still a common practice to use text-based LLMs as the textprocessing backend, pipelined with other modules such as ASR and image generator for more complex applications.</p>
<p>Affective Capabilities of LLMs</p>
<p>With the growing attention on LLMs from the research community, there have been several works investigating the affective abilities of LLMs.Huang et al. (2023) evaluated the empathy ability of LLMs by utilising the emotion appraisal theory from psychology.Wang et al. (2023) assessed the emotional intelligence of LLMs in terms of Emotional Quotient (EQ) scores.Zhang et al. (2023) investigated how LLMs could be leveraged for a range of sentiment analysis tasks under zero-shot or few-shot learning set-ups.Zhao et al. (2023) investigated the emotional dialogue ability of ChatGPT through a range of understanding and generation tasks.In our work, we focus on the affect recognition ability of text-based LLMs.Our investigation spans across different types of dialogues and model learning set-ups.We also consider real-world applications of LLMs and consider ASR-inferred noisy input to models.</p>
<p>Methodology</p>
<p>The ability of human-beings to recognise affect can be reflected in many ways.Yet, being able to narrate what emotion has been expressed in the utterances of the other interlocutor is a straightforward and strong sign of such an ability.Therefore, we took LLMs' ability to verbalise the emotion given the dialogue context as a proxy to both qualitatively and quantitatively analyse LLMs' ability for affect recognition.</p>
<p>Affect Recognition using LLMs</p>
<p>The pipeline for affect recognition using LLMs with the option to take speech as input is illustrated in Figure 1.When using the speech input, a Whisper-medium model was used to transcribe the speech (see Section 4.5 for details).The prompt is then constructed as designed and fed into the LLM to generate a text sequence.For open-source LLMs, we examined the probability of each class token and considered the one with the maximum probability as the final model prediction, as shown in Equation 1.
W L * = arg max W L P (W L |W P ),(1)
where W L belongs to the set of pre-defined labels and W P is the prompt token sequence.</p>
<p>For commercial models, there is no access to logits of model outputs and model outputs do not always follow the format specified in the prompt.Therefore, we used regular expressions to derive the final prediction.</p>
<p>Task-specific Fine-tuning</p>
<p>For efficient training of LLMs, we utilise low-rank adaptation (LoRA, Hu et al. 2022) to accelerate the fine-tuning of LLMs while conserving memory.This is also a common approach for fine-tuning LLMs as seen in many existing works (Sun et al., 2023;Zhao et al., 2024).</p>
<p>LoRA hypothesises that the change in weights during model training has a low "intrinsic rank".Therefore, instead of directly updating the full-rank weight matrices of dense layers during training, LoRA optimises the low-rank decomposition matrices of those dense layers' changes while keeping the pre-trained weights frozen.Specifically, for a pre-trained weight matrix W 0 ∈ R m×n from a particular attention block in a transformer-based LLM, its update ∆W is constrained using a low-rank decomposition of the update as following:
W 0 + ∆W = W + AB (2)
where matrices A ∈ R m×r and B ∈ R r×n contain trainable parameters and r ≤ min(m, n).The pretrained parameters in W 0 are fixed.When r is set to a much smaller value than the dimensions of W 0 , the number of trainable parameters will be greatly reduced.4 Experimental Setup</p>
<p>Datasets and Evaluation</p>
<p>The IEMOCAP dataset (Busso et al., 2008)  Emotion recognition is performed for every speaker utterance.We report the weighted accuracy (WA) and unweighted accuracy (UA) for both label sets.</p>
<p>EmoWOZ (Feng et al., 2022) is a text-based ERC corpus built for emotion recognition in ToDs.It comprises 10,438 human-human dialogues from the entire MultiWOZ dataset (Budzianowski et al., 2018), as well as 1,000 human-machine dialogues in the same set of domains.It encompasses seven distinct user emotions, namely: Neutral, Fearful, Dissatisfied, Apologetic, Abusive, Excited, and Satisfied.These emotion labels are designed to encode the task performance.Specifically, each emotion is defined as a valence reaction to certain elicitor under certain conduct.For example, Dissatisfied is defined as a negative emotion elicited by the system expressed in a neutral or polite conduct.</p>
<p>Emotion recognition is performed for each user utterance.For existing benchmarks reported in Feng et al. (2022), neutral class was excluded from calculating the metrics because they take up more than 70% of the labels in EmoWOZ.To have a direct comparison, we report macro-averaged F1 and weighted average F1 excluding neutral.We include the F1, precision, and recall of the neutral class in Table B3 1) for the model.</p>
<p>Notably, participants in the AVEC2016 challenge (Yang et al., 2016;Williamson et al., 2016) and subsequent research efforts (Ravi et al., 2022;Wu et al., 2023) primarily focused on optimising the F1 score of the Depressed class.We report this metric in Section 5 for direct comparison and also include the F1 score of the Not Depressed in Appendix B.</p>
<p>Prompt Design</p>
<p>The prompt design aims to exploit the language modelling and in-context learning ability of LLMs.</p>
<p>Due to the different task set-ups and label sets in each datasets, the prompt templates used are different as illustrated in GPT-4 (OpenAI, 2023) is an improved version of GPT-3.5.Its size is six times that of GPT-3.5.</p>
<p>Although it is considered a multi-modal model because it additionally accepts images as input, we only explored its text modality.We used the version released on the 13 th of June, 2023.</p>
<p>LLaMA-7B (Touvron et al., 2023a) is a large and causal language model introduced by Meta AI in 2023.It has transformer decoder architecture, 7 billion parameters and was pre-trained on 1 trillion tokens.</p>
<p>Alpaca-7B (Taori et al., 2023) is fine-tuned from LLaMA-7B with 52K instruction-following demonstrations generated in the style of self-instruct using text-davinci-003, a specific version of Instruct-GPT (Ouyang et al., 2022).</p>
<p>LLaMA-3-8B (AI@Meta, 2024) is the most recent model of the LLaMA family, featuring enhanced usefulness and safety.It was pre-trained on 15 trillion tokens.</p>
<p>Supervised Models for Comparison</p>
<p>While comparing zero-shot and few-shot ICL results of LLMs with supervised SOTAs does not paint the fairest picture, it does provide us with insights into how far LLMs are from achieving the performance levels of supervised SOTAs.</p>
<p>We compare LLMs' performance with the following supervised models on each dataset: Wu et al. (2020) for IEMOCAP, Feng et al. (2023) for EmoWOZ, and Wu et al. (2023) for DAIC-WOZ.Specifically,</p>
<p>For IEMOCAP: Wu et al. ( 2020) proposed an emotion recognition model which takes 1) a timesynchronous representation that fuses the audio features with the corresponding text information at each time step, as well as 2) a time-asynchronous representation that captures the text information embedded across the transcriptions of a number of consecutive utterances.These two types of framelevel vectors, after being pooled in their respective branches with self-attentive layers across the input time window, are fused using an fully connected layer for emotion classification.</p>
<p>For EmoWOZ: Feng et al. (2023) proposed a model that is dedicated for textual emotion recognition in task-oriented dialogues.Based on a transformer-based classifier that considers the dialogue history and speaker roles, the proposed model adopts data augmentation with chit-chat dialogues, dialogue state features, multi-task classification for emotional aspects, and a distance-based loss that considers the similarity of the custom emotion labels in EmoWOZ.</p>
<p>For DAIC-WOZ: Wu et al. (2023) proposed to extract utterance-level representations from pretrained speech-based foundation model.The foundation model was further fine-tuned for speech recognition and emotion recognition.The averagepooled dialogue-level features were fed into a depression detection block for binary classification.</p>
<p>To address the issue of data sparsity in speech depression detection, authors also performed data augmentation using sub-dialogue shuffling.</p>
<p>Training Configurations</p>
<p>We implement LoRA (Section 3.2) when training LLaMA-7B, Alpaca-7B, and LLaMA-3-8B but not GPT-2.For all open-source LLMs, we constrain the decoding space of the model output to ensure it generates the desired class labels.Details can be found in Appendix A.</p>
<p>ASR System Specifications</p>
<p>In order to observe how LLMs perform with the presence of substantial ASR errors rather than building a pipeline for speech-based ERC, we use an "off-the-shelf" OpenAI Whisper-medium model (Radford et al., 2022), which has been trained solely on English data and not been fine-tuned.We use a decoding beam size of 3. The text normalisation only involves removing punctuation marks.</p>
<p>The ASR word error rates (WER) for IEMOCAP and DAIC-WOZ are 12.0% and 16.5% respectively.Since EmoWOZ does not come with raw audio data, we build an ASR simulator.We formulate the simulation as a sequence generation task where the source is the ground-truth text and the target is the ASR-transcribed text (as described in Appendix A.2).The resulted simulated WER in EmoWOZ is 17.1%.</p>
<p>Results and Discussions</p>
<p>In this section, we aim to answer the questions below.Full results can be found in Appendix B. It's noteworthy that although GPT-4, the largest model, underperforms when compared with the supervised SOTA on EmoWOZ, its reported macroaveraged F1 is still comparable to some supervised learning models benchmarked in Feng et al. (2022).This suggests the good capability of GPT-4 in leveraging the label definitions in the prompt to recognise emotions in EmoWOZ, irrespective of their prevalence.Supervised models, however, may be more susceptible to issues such as label imbalance.</p>
<p>Larger models do not necessarily lead to better performance.For IEMOCAP, Alpaca-7B demonstrates the best performance, even surpassing much larger models (GPT-3.5 and GPT-4).Conversely, for EmoWOZ and DAIC-WOZ, the performance generally improves as the model size increased.</p>
<p>While chit-chat utterances in IEMOCAP are labelled with emotion classes from generic emotion models, EmoWOZ's labels are specifically designed to encode the eliciting conditions of emotions in ToDs.This design necessitates more explicit reasoning in ERC within EmoWOZ compared to IEMOCAP.Although LLMs rely on their language modelling capabilities when performing zero-shot ERC, the greater reasoning ability facilitated by the substantial number of parameters in GPT-3.5 and GPT-4 results in improved performance in EmoWOZ.</p>
<p>Likewise in DAIC-WOZ, the recognition is performed for the entire dialogue.Larger models demonstrate greater ability to leverage the more nuanced affective state of the patient in the larger context.</p>
<p>Fine-tuning LLMs with instruction-following demonstrations facilitates more effective utilisation of the prompt.In all datasets, Alpaca-7B consistently outperforms LLaMA-7B and even the much more recent LLaMA-3-8B.This indicates that the additional fine-tuning of LLaMA-7B with instruction-following demonstrations has enhanced its capability in ERC.</p>
<p>LLaMA-7B appears to underperform compared to the much smaller GPT-2 on EmoWOZ.This discrepancy can be explained by LLaMA-7B's strong inclination towards predicting the neutral emotion (F1 = 82.1 with Recall = 100), which has been excluded from the metric calculation, resulting in the poor reported metrics.Fine-tuning with instruction-following demonstrations, as adopted in Alpaca-7B, effectively leverages the task and label definition in the prompt and reverts this trend.Such an inclination in predicting neutral emotion in LLaMA-7B does not appear in the more recent LLaMA-3-8B.</p>
<p>Zero-shot Learning with Noisy ASR Input
LLaMA-7B -0.3 -1.2 -1.1 -5.0 -1.1 -0.3 -1.6 -1.1 Alpaca-7B -1.3 -1.8 -1.8 -2.6 +0.3 -2.0 -1.6 +0.0 LLaMA-3-8B -2.1 -3.5 -1.2 -2.2 +0.1 -0.1 -0.7 -0.3 GPT-3.5 +0.1 -0.1 +0.2 0.0 +1.2 -0.2 -17.0 -8.3 GPT-4 -0.5 -0.5 -1.1 -0.7 +0.9 -1.5 -19.2 -17.6
Supervised SOTA -3.8 -3.7 -3.9 -3.5 -0.8 -0.4 -3.6 -4.1</p>
<p>Table 3: Change in zero-shot performance metrics of LLMs after using noisy ASR input.For metrics: WA = weighted average; UA = unweighted average; F1 = F1 for class Depressed.GPT-2 was omitted due to its poor zero-shot capability.</p>
<p>exhibit varying degrees of influence on different affect recognition tasks.Specifically, LLMs are generally robust to ASR errors when recognising emotion.This is exemplified by small changes in metrics for IEMOCAP compared with supervised SOTAs.The only one notable exception is the UA of LLaMA-7B in the 5-way classification task on IEMOCAP.Looking at the performance of each emotion in this experiment, we observed significant drops in the F1 scores for the emotions {Happy, Angry, and Sad}.Specifically, Happy and Angry experience major decreases in their recall scores (Happy: 12.3 → 7.3, Angry: 50.0 → 11.0), while Sad sees a substantial decline in its precision score (65.5 → 0.0).At the same time, there is an increase in the recall score for the Other category (47.3 → 78.2), resulting in an overall rise in its F1 score (44.5 → 48.0).These observations suggest that ASR errors introduced a tendency for LLaMA-7B to mis-classify more emotions as Other.</p>
<p>ASR errors have a more pronounced influence on the accuracy of depression detection.For DAIC-WOZ, the introduction of ASR errors had a significant impact on F1 scores.The impact diverges for open-source and commercial models.</p>
<p>For open-source models, which are also relatively smaller, the change in F1 was small, showing a similar trend when they recognise emotions from noisy dialogues.On the other hand, for larger commercial models, the F1 scores decrease more significantly.This phenomenon can be ascribed to the lengthy prompt for conducting dialogue-level analysis, in which ASR errors accumulated.While OpenAI models can better leverage information from the large context, such an ability adversely affects its depression detection ability in the presence of ASR errors.</p>
<p>In-context Learning</p>
<p>ICL samples are randomly selected for each class and are the same within each experiment set-up for all models.The performance of LLMs with different numbers of ICL samples is outlined in Table 4, from which we have derived the following observation:</p>
<p>Larger models tend to derive greater benefits from an increased number of ICL samples to recognise emotions.LLaMA-7B, Alpaca-7B, and LLaMA-3-8B do not consistently benefit from an increased number of ICL samples in the prompt.Optimal model performance generally occurs when N = 0 or N = 1.This suggests that effectively  utilising the full context remains as a challenge for LLMs.Larger models, GPT-3.5 and GPT-4, show more consistent improvement in performance with the increased number of ICL samples.GPT-4 derives the most significant benefits from ICL samples and performs the best across all models.</p>
<p>The effectiveness of ICL is limited for depression detection.The performance is in general the best when N = 0, followed by N = 3.This suggests that for depression detection, a task to detect more nuanced affective state than emotion from a longer sequence, a single ICL sample for each class could strongly bias the model.This leads to zero F1s where models predict all samples as Not Depressive.Including more ICL samples could mitigate this effect, but the performance is further limited by models' incapability to handle extremely lengthy input.This motivates further research ef-forts to handle huge context containing nuanced task-related cues when using LLMs.</p>
<p>Task-specific Fine-tuning</p>
<p>We conduct task-specific fine-tuning experiments with GPT-2, LLaMA-7B, Alpaca-7B, and LLaMA-3-8B using different proportions of training data to explore these models' capacity for ERC after fine-tuning.Results are summarised in Figure 2.</p>
<p>For DAIC-WOZ, fine-tuning would steer models to predict Not Depressed (see Table B4) for almost all test samples.This might be due to the small training set where more than 70% of the samples are labelled as Not Depressed.This suggests the limitation of language modelling objective, and therefore more carefully curated task-related learning objectives should be considered for depression detection using LLM.</p>
<p>Task-specific fine-tuning can effectively and efficiently enhance the ERC performance of LLMs.</p>
<p>For both IEMOCAP and EmoWOZ, we observe an initial significant improvement in performance when fine-tuning with 25% of the training data.Performance remains relatively stable and approaches SOTA levels as the proportion of training data increased to 50% and more for IEMOCAP (4-way) and EmoWOZ.This shows the potential of rapid deployment of LLMs as the emotion recognition frontend in dialogue systems, regardless of dialogue type, label set, or label distribution.</p>
<p>In the case of 5-way classification on IEMOCAP, a performance gap persists between fine-tuned LLMs and the supervised SOTA, even after fine-tuning of LLMs on the complete training set.We hypothesised that this disparity might be attributed to the presence of an additional Other class within the 5-way classification scheme.The class name "Other" lacked essential affective information and consequently failed to fully leverage the language modelling capabilities of LLMs.Therefore, we suggest that employing more semantically meaningful label names could be advantageous in the potential of LLMs for task-specific fine-tuning.</p>
<p>In the case of GPT-2, fine-tuning does not yield noticeable improvement in ERC.Its performance even deteriorated after fine-tuning with EmoWOZ, as depicted in Figure 2(c) because GPT-2 predominantly predicted Neutral, which are excluded from the metric calculation.</p>
<p>Conclusion</p>
<p>In this study, we explore the performance of LLMs for affect recognition in three distinct types of dialogues: chit-chat dialogues, information-seeking ToDs, and medical consultation dialogues for depression.We conduct benchmark experiments on these datasets using five LLMs: LLaMA-7B, Alpaca-7B, LLaMA-3-8B, GPT-3.5, and GPT-4.We also explore various setups, including zeroshot learning, few-shot in-context learning, and task-specific fine-tuning, all facilitated by specially designed prompts.Additionally, we examine the impact of ASR errors on LLMs' zero-shot performance.</p>
<p>Our zero-shot experiments underscore that while LLMs have made significant strides in various natural language understanding tasks, they still have some distance to cover in order to match the supervised SOTAs in affect recognition tasks.Adding emotion definitions explaining the eliciting conditions in ToDs to the prompt and fine-tuning LLMs for instruction-following could narrow the performance gap from supervised SOTAs.</p>
<p>Performing zero-shot affect recognition from utterances containing ASR errors shows that LLMs are robust to such errors for emotion recognition but not for depression detection.Therefore, when considering LLMs as a back-end module of a spoken dialogue system, it is crucial to exercise extra caution when processing dialogues laden with highly specific and nuanced affective content.</p>
<p>Our ICL experiments exemplify that larger models would benefit more from an increased number of ICL samples, highlighting the need to explore the optimal combination of the ICL sample size in the prompt and the model size.For smaller LLMs, effectively utilising lengthy context remains as a challenge.</p>
<p>Through task-specific fine-tuning, we achieve performance levels close to SOTA on IEMOCAP and EmoWOZ, using only 50% of the training data, with LLaMA-7B, Alpaca-7B, and LLaMA-3-8B.This highlights the great potential of fine-tuning LLMs for simpler tasks and integrating them as functional modules into dialogue systems.</p>
<p>Overall, LLMs have opened new avenues for affect recognition in conversations and building affectaware dialogue systems.Despite the limited performance under zero-shot set-up, their robustness to ASR errors, few-shot ICL capabilities, and ERC capabilities after fine-tuning offer exciting research opportunities for exploring affect recognition in conversations and building human-like conversational agents.We would also like to highlight challenge and also opportunities towards handling long context and nuanced emotion cues in LLMs.</p>
<p>Limitations</p>
<p>In our work, although we reduce computation resource of training LLMs by incorporating LoRA, the inference takes 1s for utterance-level emotion recognition on a Nvidia A100 40GB graphics card when there is no ICL sample in the prompt.The inference time increases when the number of ICL samples increases or dialogue-level classification is performed.While LLMs demonstrates superior abilities and potentials, further research efforts are still needed to ensure efficient LLM inference, which is necessary for its application in real-time systems.</p>
<p>With ICL experiments especially on DAIC-WOZ, we observe that the efficacy of long context is limited by the effective spans of the attention mechanisms.While substantial efforts have been invested into increasing the maximum allowed context size of LLMs and improving benchmark performance, the effectiveness of LLMs to make use of full context should not be overlooked.</p>
<p>We only investigate with one dataset from each of three dialogue domains.Although these datasets cover different dialogue settings, objectives, label sets, and classification scopes, there are more affect types and dialogue settings to explore.These datasets also exhibit various degrees of class im-balance, which selected reference SOTAs utilised data augmentation to address.While GPT-4 has demonstrated good zero-shot learning ability (Section 5.1), addressing data imbalance is out of the scope of this work, and data augmentation with LLMs may come a cost of potential divergence between synthetic language and real-word data (Li et al., 2023).</p>
<p>Ethics Statement</p>
<p>Models and datasets were used in accordance with their respective licenses.Data that we used and generated does not contain any information that uniquely identifies individual people.There is a tiny fraction of utterances labelled as "abusive" in EmoWOZ, but they are prompted to models in such a way for the recognition purpose only.Due to the fact LLMs were pre-trained with a huge amount of data, they may produce inaccurate information about people, places, or facts.This had negligible impact on our evaluation for affect recognition.When performing depression detection and analysis with DAIC-WOZ using GPT-3.5 and GPT-4, models output reminders about seeking professional advice from doctors for more accurate medical diagnosis along with their predictions.</p>
<p>Unlike running models locally, utilising OpenAI's server-based models would require us to send data to their server.In some cases, it is important to use the application programming interface (API) when for which OpenAI explicitly clarifies that the query data will not be stored or used in model training unless specifically configured.</p>
<p>Although this work focuses on LLMs' capability in recognising affect in conversations, we do envisage LLMs to be incorporated as an affect recognition frontend in affect-aware dialogue systems.It is therefore important to remember that these models are not perfect and can make errors in their predictions.Subsequently, any actions taken based on these predictions should be executed with an awareness of the possibility of errors.The relatively slow inference speed and the high computational resource requirement also pose a challenge in the usage of LLMs in high-throughput and timesensitive scenarios.</p>
<p>A Detailed Training Configurations</p>
<p>A.1 Task-Specific Fine-tuning For all model fine-tuning, the learning rate was 3e-5.The batch size was 2 with a gradient accumulation step of 4. We used a cosinusoidal learning rate scheduler without warming up.We applied a weight decay of 0.01 on all model parameters except for the biases and layer normalisation weights.For LLaMA-7B, Alpaca-7B, and LLaMA-3-8B, we stored model parameters in IEEE 754 half-precision float point format.</p>
<p>For GPT-2, we stored the model parameters in standard single-precision floating-point format and did not apply LoRA during the fine-tuning.We followed the default LoRA configuration provided in Huggingface PEFT library (Mangrulkar et al., 2022).We used the model perplexity on the development set as the early-stopping criterion.For EmoWOZ, we used the official development set.For IEMOCAP, when we performed the leave-one-session-out training, 10% of the training data were randomly sampled as the development set.We applied stratified sampling based on the emotion labels.All open-source models were trained on a single Nvidia A100 40GB Graphics Card.</p>
<p>A.2 ASR Simulation for EmoWOZ</p>
<p>We fine-tuned a LLaMA-7B model using LoRA following configurations specified in Section 3.2 and A.1 for one epoch on all IEMOCAP utterances.The source was each of the IEMOCAP utterance transcription and the target was the corresponding OpenAI Whisper-medium hypothesis.We utilised a prompt template that formatted the source and target in natural language would best exploit the language modelling capability of the model: T 47.2 (30.9/100.0)0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 30.9 25.0 GPT-2 0 50% T 47.2 (30.9/100.0)0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 30.9 25.0 GPT-2 0 75% T 47.2 (30.9/100.0)0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 30.9 25.0 GPT-2 0 100% T 47.2 (30.9/100.0)0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 30.9 25.0   T 51.1 (37.9/78.2) 58.9 (80.4/46.5)55.3 (49.9/61.9)54.5 (62.0/48.7)27.0 (45.6/19.1)49.0 50.9 GPT-4 3 0% T 49.6 (39.8/65.8)59.9 (81.7/47.2) 54.7 (53.5/56.1)58.3 (52.5/65.4)30.9 (40.5/24.9)49.4 51.9
After</p>
<p>LLaMA</p>
<p>Table B2: F1(precision/recall), UA, and WA of LLMs on IEMOCAP under the 5-Way classification set-up.In table headers, "N" stands for the number of ICL samples in the prompt; "P" stands for the proportion of training data used for fine-tuning; "M" stands for the modality of input, either transcription (T) or ASR hypothesis (A).</p>
<p>Table B3: F1(precision/recall), MF1 and WF1 of LLMs on EmoWOZ.In table headers, "N" stands for the number of ICL samples in the prompt; "P" stands for the proportion of training data used for fine-tuning; "M" stands for the modality of input, either transcription (T) or ASR hypothesis (A).</p>
<p>Figure 1: A flowchart illustrating the affect recognition pipeline using Whisper and LLM.The designed prompt comprises parts introduced in Table 1.Low-rank adaptation (LoRA) is used for fine-tuning open-source LLMs.</p>
<p>Figure 2 :
2
Figure 2: Change of model performance when fine-tuning with different proportions of the training data.</p>
<p>of Appendix B.
Not Depressed in the dataset, we add informationabout PHQ-8 level definition and quantisation cri-teria to the prompt to establish an aligned diagnosisstandard (TableDAIC-WOZ (Gratch et al., 2014) is a speech-based corpus for depression detection and anal-ysis. It includes the Patient Health Questionnaire-8(PHQ-8, Kroenke et al., 2008) scores of 193 clini-cal interviews, with 35 (12 are labelled depressed)interviews in the development set and 47 (14 are la-belled depressed) in the test set. The PHQ-8 scoreranges from 0 to 24 and quantifies the severity ofthe patient's depressive symptoms.For evaluation metrics, we follow the criteria estab-lished by the Audio/Visual Emotion Challenge andWorkshop challenge (AVEC2016) (Valstar et al.,2016) and perform binary classification on the di-alogue level. Interviewees with PHQ8 ≥ 10 isconsidered Depressed and PHQ8 &lt; 10 is consid-ered Not Depressed. Since patients with PHQ-8score of 5 to 9 are defined to show mild depressivesymptoms (Kroenke et al., 2008) but considered</p>
<p>Table 1 :
1
Consider the following list of concepts, called EMOTIONs: [EmotionA, EmotionB, ...] Definition EmoWOZ: Consider the following list of concepts, called EMOTIONs: [EmotionA: Emotion_DefinitionA; EmotionB: Emotion_DefinitionB; ...] DAIC-WOZ: Given that the SEVERITY of depression can be categorised into the following levels on a scale of 0 to 24: [No significant depressive symptoms (0 to 4), ...].A participant is considered depressed if the participant shows moderate depressive symptoms (10 to 14) and above.Prompt templates, consisting of the task definition, in-context samples, and the query.
Prompt TemplateTaskIEMOCAP:
(Ouyang et al., 2022)EmoWOZ uses custom emotion labels, DAIC-WOZ involves mapping from numerical values to binary classes, and IEMOCAP uses generic emotion labels.We therefore provide additional label explanations in the task definition of EmoWOZ and DAIC-WOZ.IEMOCAP on the other hand, contains self-explanatory emotion labels from a generic emotion model and does not come with any special definitions.Therefore, we do not include label definition in the prompt for IEMO-CAP.Since IEMOCAP and EmoWOZ involve utterance-level classification whereas DAIC-WOZ involves dialogue-level classification, we used different queries to accommodate this difference.4.3 Models4.3.1 LLMsGPT-2 (Radford et al., 2019) has a transformer architecture, pretrained on a substantial English corpus through self-supervised learning.While its size does not make it one of LLMs, it stands as one of the early achievements of OpenAI's GPT models.For our baseline reference, we utilised the version containing 124 million parameters.GPT-3.5, or ChatGPT (OpenAI, 2022), is a chatbot application developed by OpenAI.It follows a similar architecture as InstructGPT(Ouyang et al., 2022)and was fine-tuned for chat application via reinforcement learning from human feedback (RLHF).It contains 175 billion parameters.Specifically, we used the version released on the 13 th of June, 2023.ICLIEMOCAP / EmoWOZ: Given the dialogue history between SpeakerA and SpeakerB: [SpeakerA: Utterancet-3; SpeakerB: Utterancet-2; Samples SpeakerA: Utterancet-1], the EMOTION in the next utterance "SpeakerB: Utterancet" is EmotionA DAIC-WOZ: Given the depression consultation dialogue between Participant and Ellie: [Participant: Utterance0; Ellie: Utterance1; Participant: Utterance2; ...], the Participant's is (not) depressed.Query IEMOCAP / EmoWOZ: Given the dialogue history between SpeakerA and SpeakerB: [SpeakerA: Utterancet-3; SpeakerB: Utterancet-2; SpeakerA: Utterancet-1], the EMOTION in the next utterance "SpeakerB: Utterancet" is DAIC-WOZ: Given the depression consultation dialogue between Participant and Ellie: [Participant: Utterance0; Ellie: Utterance1; Participant: Utterance2; ...], the Participant's is</p>
<p>Table 3
3ModelIEMOCAP (4-way) IEMOCAP (5-way) WA (↑) UA (↑) WA (↑) UA (↑)EmoWOZ MF1 (↑) WF1 (↑) F1 (dev, ↑) F1 (test, ↑) DAIC-WOZGPT-225.829.219.022.37.324.00.00.0LLaMA-7B41.140.535.633.61.10.347.552.2Alpaca-7B48.851.440.536.224.044.647.553.3LLaMA-3-8B41.842.529.431.719.742.447.143.2GPT-3.542.237.637.935.139.040.054.564.3GPT-442.437.637.534.752.462.363.659.3Supervised SOTA77.678.473.374.465.983.988.685.7
provides a summary of LLMs' zero-shot performance when replacing the original dialogue transcripts with ASR-inferred inputs.ASR errors</p>
<p>Table 2 :
2
Zero-shot performance of LLMs compared with respective supervised SOTAs.The best zero-shot performance for each metric is made bold.For metrics: WA = weighted average; UA = unweighted average; MF1 = macro-averaged F1 excluding neutral; WF1 = weighted average F1 excluding neutral; F1 = F1 for class Depressed.
ModelIEMOCAP (4-way) IEMOCAP (5-way) WA (↑) UA (↑) WA (↑) UA (↑)EmoWOZ) MF1 (↑) WF1 (↑) F1 (dev, ↑) F1 (test, ↑) DAIC-WOZ</p>
<p>tion in the framework of the Sofja Kovalevskaja Award endowed by the Federal Ministry of Education and Research.G. Sun is partly funded by the Department of Engineering, University of Cambridge.Computing resources were provided by Google Cloud.
9 Acknowledgement
S. Feng and N. Lubis are supported by funding provided by the Alexander von Humboldt Founda-</p>
<p>adding automatic speech recognition errors, [SOURCE] becomes[TARGET]
B Detailed Experimental ResultsModelNPMNeutralHappyAngrySadWAUAGPT-200%T0.7 (60.0/0.4)32.3 (43.6/25.6)35.3 (22.0/90.6)0.5 (30.0/0.3)25.829.2GPT-210%T10.9 (43.4/6.2)9.1 (62.0/4.9)29.0 (21.8/43.6)33.3 (22.8/61.9)24.229.2GPT-2025%</p>
<p>Table B4 :
B4
F1(precision/recall) of LLMs on DAIC-WOZ.In table headers, "N" stands for the number of ICL samples in the prompt; "P" stands for the proportion of training data used for fine-tuning; "M" stands for the modality of input, either transcription (T) or ASR hypothesis (A).</p>
<p>The code can be found at https://gitlab.cs. uni-duesseldorf.de/general/dsml/llm4erc-public/
ModelGPT-2 0 0% T 0.1 (100.0/0.0)0.0 (0.0/0.0) 9.3 (5.6/27.8)0.0 (0.0/0.0) 0.0 (0.0/0.0) 2.8 (1.4/64.8)31.4 (35.7/28.1)7.3 24.0 GPT-2 1 0% T 81.2 (69.8/97.0)0.0 (0.0/0.0) 0.0 (0.0/0.0) 23.3 (14.8/54.8)0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 3.9 0.6 GPT-2 0 25% T 82.4 (70.1/99.8)0.0 (0.0/0.0) 0.0 (0.0/0.0) 69.9 (71.4/68.5)0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 11.7 1.9 GPT-2 0 50% T 82.3 (70.0/100.0)0.0 (0.0/0.0) 0.0 (0.0/0.0) 69.6 (95.2/54.8)0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 11.6 1.9 GPT-2 0 75% T 82.4 (70.3/99.5)0.0 (0.0/0.0) 0.0 (0.0/0.0) 58.9 (47.9/76.7)0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 9.8 1.6 GPT-2 0 100% T 82.3 (70.0/99.8)0.0 (0.0/0.0) 0.0 (0.0/0.0) 68.1 (74.2/63.0)0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 11.4 1.9LLaMA-7B 0 0% T 82.1 (69.7/100.0)0.0 (0.0/0.0) 0.3 (33.3/0.2) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 6.3 (75.0/3.3)0.0 (0.0/0.0) 1.1 0.3 LLaMA-7B 0 0% A 82.1 (69.7/100.0)0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 0.0 LLaMA-7B 1 0% T 83.0 (78.1/88.5)26.1 (60.0/16.7)2.6 (47.1/1.3)0.0 (0.0/0.0) 57.9 (52.4/64.7)16.0 (9.2/58.2) 59.0 (74.1/49.0)26.9 42.6 LLaMA-7B 3 0% T 27.9 (81.2/16.9)0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 0.0 (0.0/0.0) 39.2 (24.4/99.2) 6.5 27.2 LLaMA-7B 0 25% T 93.9 (91.5/96.4)26.1 (60.0/16.7)55.2 (81.6/41.7)72.3 (93.5/58.9)11.1 (100.0/5.9)43.6 (69.0/31.9)90.9 (89.1/92.7)49.9 79.5 LLaMA-7B 0 50% T 94.
Llama 3 model card. American Psychiatric Association. Ai@ References, Meta, 2024. 2020What Is Depression?</p>
<p>Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, Thomas Wolf, Open llm leaderboard. 2023</p>
<p>MultiWOZ -a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Milica Osman Ramadan, Gašić, 10.18653/v1/D18-1547Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>IEMOCAP: interactive emotional dyadic motion capture database. Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, Shrikanth S Narayanan, Language Resources and Evaluation. 4242008</p>
<p>Constants across cultures in the face and emotion. Paul Ekman, W V Friesen, Journal of personality and social psychology. 171971</p>
<p>From chatter to matter: Addressing critical steps of emotion recognition learning in task-oriented dialogue. Shutong Feng, Nurul Lubis, Christian Geishauser, Hsien-Chin Lin, Michael Heck, Carel Van Niekerk, Milica Gasic ; Nurul, Benjamin Lubis, Christian Ruppik, Michael Geishauser, Hsien-Chin Heck, Carel Lin, Renato Van Niekerk, Milica Vukovic, Gasic, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14). Stefan Scherer, Angela Nazarian, Rachel Wood, Jill Boberg, David Devault, Stacy Marsella, David Traum, Skip Rizzo, Louis-Philippe Morency, the Ninth International Conference on Language Resources and Evaluation (LREC'14)Marseille, France; Prague, Czechia; Jonathan Gratch, Ron Artstein, Gale Lucas; Reykjavik, IcelandEuropean Language Resources Association. Shutong Feng2022. 2023. 2014Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue. European Language Resources Association (ELRA</p>
<p>Emotion regulation: Affective, cognitive, and social consequences. James J Gross, 10.1017/S0048577201393198Psychophysiology. 3932002</p>
<p>ChatGPT for zero-shot dialogue state tracking: A solution or an opportunity?. Michael Heck, Nurul Lubis, Benjamin Ruppik, Renato Vukovic, Shutong Feng, Christian Geishauser, 10.18653/v1/2023.acl-short.81Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada20232Short Papers). Association for Computational Linguistics</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. Jen-tse Huang. Man Lam, Eric Li, Shujie Ren, Wenxuan WangWenxiang Jiao2022Zhaopeng Tu, and Michael Lyu</p>
<p>Emotionally numb or empathetic? evaluating how llms feel using emotionbench. </p>
<p>Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition. Dan Jurafsky, James H Martin, N J Kurt Kroenke, Tara W Strine, Robert L Spitzer, Janet B W Williams, Joyce T Berry, Ali H Mokdad, J Affect Disord. 1141-32009. 2008Pearson Prentice HallThe PHQ-8 as a measure of current depression in the general population</p>
<p>. Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ming Yin, </p>
<p>Synthetic data generation with large language models for text classification: Potential and limitations. 10.18653/v1/2023.emnlp-main.647Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022. Peft: State-of-the-art parameterefficient fine-tuning methods. SingaporeProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</p>
<p>Emotional intelligence meets traditional standards for an intelligence. David R John D Mayer, Peter Caruso, Salovey, 10.1016/S0160-2896(99)00016-1Intelligence. 2741999</p>
<p>Introducing ChatGPT. Gpt-4 technical report. 2022OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe, ; , Ma Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, Ilya Sutskever, 10.48550/arXiv.2203.02155Proceedings of NeurIPS 2022. Rosalind W. Picard. 1997. Affective Computing. Vijay Ravi, Jinhan Wang, Jonathan Flint, NeurIPS 2022. Rosalind W. Picard. 1997. Affective ComputingCambridgeMIT Press2022. 2022. 2019. 2022Language models are unsupervised multitask learners. and Abeer Alwan. 2022. A step towards preserving speakers' identity while detecting depression via speaker disentanglement. Interspeech</p>
<p>A circumplex model of affect. James A Russell, 10.1037/h0077714Journal of Personality and Social Psychology. 3961980</p>
<p>SALMONN: Towards generic hearing abilities for large language models. Guangzhi Sun, Shutong Feng, Dongcheng Jiang, Chao Zhang, Milica Gašić, Philip C Woodland, Wenyi Tang, Guangzhi Yu, Xianzhao Sun, Tian Chen, Wei Tan, Lu Li, Lu, M A Zejun, Chao Zhang, The Twelfth International Conference on Learning Representations. 2023. 2024Speech-based slot filling using large language models</p>
<p>Llama: Open and efficient foundation language models. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto ; Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample ; Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, Stanford alpaca: An instruction-following llama model. Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams; Angela Fan, Melanie Kambadur; Robert Stojnic, Sergey EdunovAurelien Rodriguez2023. 2023aPuxin Xu,. and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Avec 2016: Depression, mood, and emotion recognition workshop and challenge. Michel Valstar, Jonathan Gratch, Björn Schuller, Fabien Ringeval, Denis Lalanne, Mercedes Torres Torres, Stefan Scherer, Giota Stratou, 10.1145/2988257.2988258Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, AVEC '16. the 6th International Workshop on Audio/Visual Emotion Challenge, AVEC '16New York, NY, USAAssociation for Computing Machinery2016Roddy Cowie, and Maja Pantic</p>
<p>. Xuena Wang, Xueting Li, Zi Yin, Yue Wu, Liu Jia, </p>
<p>Emotional intelligence of large language models. </p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Emergent abilities of large language models. Transactions on Machine Learning Research. 2022Survey Certification</p>
<p>Detecting depression using vocal, facial and semantic communication cues. James R Williamson, Elizabeth Godoy, Miriam Cha, Adrianne Schwarzentruber, Pooya Khorrami, Youngjune Gwon, Hsiang-Tsung Kung, Charlie Dagli, Thomas F Quatieri, Wen Wu, Chao Zhang, Philip C Woodland, C Philip, Woodland, 10.1109/icassp49357.2023.10094910Emotion recognition by fusing time synchronous and time asynchronous representations. ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). New York, NY, USAIEEE2016. 2020. 2023ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</p>
<p>Decision tree based depression classification from audio video and language information. Le Yang, Dongmei Jiang, Lang He, Ercheng Pei, Meshia Cédric Oveneke, Hichem Sahli, 10.1145/2988257.2988269Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, AVEC '16. the 6th International Workshop on Audio/Visual Emotion Challenge, AVEC '16New York, NY, USAAssociation for Computing Machinery2016</p>
<p>A survey of affect recognition methods: Audio, visual, and spontaneous expressions. Zhihong Zeng, Maja Pantic, Glenn I Roisman, Thomas S Huang, 10.1109/TPAMI.2008.52IEEE Transactions on Pattern Analysis and Machine Intelligence. 3112009</p>
<p>Sentiment analysis in the era of large language models: A reality check. Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing, Justin Zhao, Timothy Wang, Wael Abid, Geoffrey Angus, Arnav Garg, Jeffery Kinnison, Alex Sherstinsky, Piero Molino, Travis Addair, Devvret Rishi ; Weixiang Zhao, Yanyan Zhao, Xin Lu, Lora land: 310 fine-tuned llms that rival gpt-4, a technical report. Shilong Wang, Yanpeng Tong2023. 2024and Bing Qin. 2023. Is chatgpt equipped with emotional dialogue capabilities</p>
<p>. Alpaca-7b , </p>
<p>. Alpaca-7b , </p>
<p>. Alpaca-7b , </p>
<p>. Alpaca-7b , </p>
<p>. Alpaca-7b , </p>
<p>. Alpaca-7b , </p>
<p>. Alpaca-7b , </p>
<p>. Alpaca-7b , </p>
<p>LLaMA-3-8B. </p>
<p>LLaMA-3-8B. </p>
<p>LLaMA-3-8B. </p>
<p>LLaMA-3-8B. </p>
<p>LLaMA-3-8B. </p>
<p>LLaMA-3-8B. </p>
<p>LLaMA-3-8B. </p>            </div>
        </div>

    </div>
</body>
</html>
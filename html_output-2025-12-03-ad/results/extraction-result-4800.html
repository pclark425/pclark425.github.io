<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4800 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4800</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4800</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-265609823</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.01054v1.pdf" target="_blank">Exploring and Improving the Spatial Reasoning Abilities of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) represent formidable tools for sequence modeling, boasting an innate capacity for general pattern recognition. Nevertheless, their broader spatial reasoning capabilities, especially applied to numerical trajectory data, remain insufficiently explored. In this paper, we investigate the out-of-the-box performance of ChatGPT-3.5, ChatGPT-4 and Llama 2 7B models when confronted with 3D robotic trajectory data from the CALVIN baseline and associated tasks, including 2D directional and shape labeling. Additionally, we introduce a novel prefix-based prompting mechanism, which yields a 33% improvement on the 3D trajectory data and an increase of up to 10% on SpartQA tasks over zero-shot prompting (with gains for other prompting types as well). The experimentation with 3D trajectory data offers an intriguing glimpse into the manner in which LLMs engage with numerical and spatial information, thus laying a solid foundation for the identification of target areas for future enhancements.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4800.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4800.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deployed OpenAI conversational LLM (GPT-3.5 family) evaluated in this paper for spatial labeling tasks including 2D direction/shape identification, 3D robotic trajectory classification, and textual spatial QA via SpartQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational large language model in the GPT-3.5 family (proprietary OpenAI model). Exact parameter count and training details are not provided in this paper; treated as a pre-trained transformer-based LLM used via API for zero-shot, in‑context, and chain-of-thought (CoT) prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>2D Direction Labeling; 2D Shape Identification; 3D Trajectory Labeling (CALVIN & CALVIN-Cleaned); SpartQA (spatial relationship QA)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>2D Direction Labeling: classify each segment between successive (x,y) points as left/right/up/down; Shape Identification: classify overall path shape (e.g., circle, checkmark) from noisy 2D finger-tracking data; 3D Trajectory Labeling: classify 3D end-effector trajectories into motion types (lifting, rotating, sliding) from CALVIN robot dataset and a human-cleaned subset; SpartQA: textual spatial reasoning QA (find relation, find blocks, choose object, yes/no). Each task requires interpreting numerical coordinates or textual spatial relations and mapping them to semantic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Prompting-based strategies were used: zero-shot prompting (single instruction), in-context learning (ICL / few-shot examples), chain-of-thought (CoT) prompting (few-shot examples augmented with step-by-step reasoning), and Spatial Prefix-Prompting (SPP) where the prompt first asks a simpler tangential spatial question to prime the model before the main query.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative and behavioral evidence: (1) Human-evaluator judged correctness of ChatGPT outputs for trajectory labeling; (2) performance patterns: acceptable few-shot performance on 2D direction tasks but degraded performance on noisy/irregular 3D trajectories; (3) improvements from SPP (and ICL) indicate the model can exploit simple spatial computations (e.g., coordinate differences) implicitly; (4) authors note cases where CoT reasoning steps refer explicitly to coordinate trends (e.g., changes in z implying lift) — providing qualitative traces that model used coordinate-based notions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>2D Direction Labeling (short integer sequences): accuracy 0.50, Avg. Err.# per sequence 0.15; 2D Direction Labeling (short float): acc 0.50, Err.# 0.25; 2D Direction Labeling (long integer): acc 0.00 (table value reported); 2D Shape Identification: integer version acc 0.71, float version acc 0.31 (no in-context used; zero-shot evaluation). 3D Trajectory (CALVIN): Zero-shot F1=0.19 Acc=0.26; In-context F1≈0.17 (reported), CoT F1=0.28 Acc≈0.33; SPP F1=0.32 Acc=0.43. 3D Trajectory (CALVIN-Cleaned): Zero-shot F1=0.24 Acc=0.30; CoT F1=0.14 Acc=0.34; SPP F1=0.38 Acc=0.43. SpartQA: ChatGPT-3.5 was not evaluated on SpartQA in this paper (Llama-2-7B used for SpartQA), so no SpartQA metrics for ChatGPT-3.5 are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles on irregular/noisy 3D trajectories (low F1 and accuracy); Chain-of-Thought prompting produced volatile results and sometimes degraded performance, likely because CoT reasoning depends heavily on representativeness of examples and is brittle to noisy coordinate patterns; small dataset sizes (only 30 CALVIN samples) and hand-cleaned subsets limit statistical confidence; tokenization/representation issues for high-precision floats (authors experimented with integer vs float scaling to conserve tokens); qualitative mismatches where extraneous motion points confused semantic label mapping (e.g., small z fluctuations in a slide labeled as lift).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across prompting methods: ICL and SPP often outperform zero-shot; SPP sometimes surpasses CoT and ICL (notably on CALVIN-Cleaned). ChatGPT-3.5 performs worse than ChatGPT-4 on most tasks (see ChatGPT-4 entry). No direct human performance numbers are provided for trajectories, though human evaluators judged outputs; SpartQA comparisons were performed with Llama-2-7B (other models not reported for that dataset in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Improving the Spatial Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4800.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4800.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 conversational LLM evaluated for spatial labeling and reasoning tasks; shows strongest reported performance among evaluated models, especially when combined with Spatial Prefix-Prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large transformer-based language model (GPT-4). The paper does not list parameter counts or training corpora beyond citing the GPT-4 technical report; used via API as a pre-trained language model for zero-shot, few-shot (ICL), CoT, and SPP prompting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>2D Direction Labeling; 2D Shape Identification; 3D Trajectory Labeling (CALVIN & CALVIN-Cleaned)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same tasks as for ChatGPT-3.5: segmentwise directional labeling on 2D coordinate sequences, overall shape identification from noisy 2D finger-tracking data, and classification of 3D robotic trajectories into lifting/rotating/sliding classes requiring understanding of trends in x/y/z coordinates or higher-dimensional motion patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Prompting strategies used: zero-shot, in-context learning (few-shot), chain-of-thought prompting (few-shot + stepwise reasoning), and Spatial Prefix-Prompting (SPP). SPP asks a simpler tangential spatial question first to prime the model, then asks the complex 3D labeling question.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Behavioral evidence: (1) High accuracy on 2D short trajectories (perfect classification reported for short sequences), indicating reliable exploitation of coordinate differences; (2) marked improvements with SPP, especially on cleaned 3D data, implying transfer from basic spatial sub-tasks to complex trajectory labeling; (3) qualitative CoT outputs that reference coordinate trends (e.g., recognizing 'down then up' in z-dimension as lifting) are reported, demonstrating internal use of spatial features in reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>2D Direction Labeling: short integer acc 1.00 Err.# 0.00; short float acc 1.00 Err.# 0.00; long integer acc 0.60 Err.# 0.13; 2D Shape Identification: integer acc 0.46, float acc 0.46. 3D Trajectory (CALVIN): Zero-shot F1=0.19 Acc=0.27; In-context F1≈0.36 Acc≈0.63; CoT F1≈0.33 Acc≈0.63; SPP F1=0.55 Acc=0.60. 3D Trajectory (CALVIN-Cleaned): Zero-shot F1=0.42 Acc=0.47; In-context F1=0.73 Acc=0.67; CoT F1=0.73 Acc=0.67; SPP F1=0.80 Acc=0.80.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite stronger performance, ChatGPT-4 still has subpar results on raw/unfiltered CALVIN trajectories (noisy, irregular motions), indicating brittleness to extraneous motion; CoT shows volatile gains (sometimes helps, sometimes hurts), suggesting sensitivity to example selection; experiments use small sample sizes (30 CALVIN samples), limiting generalization; inability to consistently segment trajectories or isolate motion-relevant subsequences without human pre-cleaning reduces practicality for raw robotic data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperforms ChatGPT-3.5 and Llama-2-7B on most reported tasks; SPP often surpasses both ICL and CoT for ChatGPT-4 on cleaned 3D data. The paper notes models perform better on simpler 2D direction tasks than on complex 3D trajectory labeling, i.e., models perform worse on spatial trajectory tasks than on simpler non-spatial textual tasks they are often evaluated on.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Improving the Spatial Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4800.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4800.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open foundation LLM (7 billion parameters) used in this paper primarily for the SpartQA spatial question-answering evaluation and compared across prompting methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama 2: Open foundation and fine-tuned chat models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source family model (7B parameter variant) from Meta (Llama 2). The paper uses the 7B model for faster evaluation on the SpartQA dataset; specific fine-tuning details are not provided beyond the citation.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpartQA (textual spatial reasoning benchmark: find relation (FR), find blocks (FB), choose object (CO), yes/no (YN))</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>SpartQA consists of textual questions that probe spatial relations and reasoning over described block scenes, requiring compositional spatial understanding (e.g., identifying which block is above or between others, choosing an object matching spatial descriptors).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Prompting methods evaluated: zero-shot, chain-of-thought (CoT), and Spatial Prefix-Prompting (SPP). For SpartQA, heuristics were applied for evaluation (e.g., checking if answer appears in first/last line) in addition to model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Behavioral evidence via improved accuracy under SPP: SPP yields higher per-subtask and overall accuracy compared to zero-shot and CoT, which the authors interpret as the model leveraging simpler spatial priming to better answer compositional textual spatial questions. No internal probing or representation visualizations were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SpartQA test set (510 instances), Llama-2-7B: Zero-shot overall accuracy not explicitly tabulated for all subtypes in a single overall figure in the table, but reported values: CoT overall acc ≈ 0.36; SPP overall acc = 0.41. Per-subtask (from Table 3): SPP: FR 0.42, FB 0.44, CO 0.42, YN 0.40; CoT: FR 0.21, FB 0.40, CO 0.47, YN 0.24 (these per-subtask values are reported in the paper's Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero-shot performance is poor on many subtasks (e.g., FR). CoT helps in some categories but is inconsistent; SPP improves consistency but absolute accuracies remain modest (overall ~0.41), indicating limited deep spatial understanding. Evaluation used heuristics for parsing outputs and no human adjudication for SpartQA in this study, which may bias reported accuracies. Also the Llama-2-7B model is smaller; scaling effects on performance are not exhaustively explored.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>SPP outperforms CoT and zero-shot in SpartQA for Llama-2-7B (overall 0.41 vs 0.36 for CoT). Authors compare Llama-2-7B's improvements qualitatively to ChatGPT family performance on other tasks, concluding SPP is a generally useful prompt design across model sizes, but ChatGPT-4 achieves higher absolute performance on trajectory tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Improving the Spatial Reasoning Abilities of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Calvin: A benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks <em>(Rating: 2)</em></li>
                <li>SPARTQA: A textual question answering benchmark for spatial reasoning <em>(Rating: 2)</em></li>
                <li>Chain-ofthought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models as general pattern machines <em>(Rating: 2)</em></li>
                <li>Pointllm: Empowering large language models to understand point clouds <em>(Rating: 1)</em></li>
                <li>3d-llm: Injecting the 3d world into large language models <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4800",
    "paper_id": "paper-265609823",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "ChatGPT-3.5",
            "name_full": "ChatGPT (GPT-3.5)",
            "brief_description": "A deployed OpenAI conversational LLM (GPT-3.5 family) evaluated in this paper for spatial labeling tasks including 2D direction/shape identification, 3D robotic trajectory classification, and textual spatial QA via SpartQA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT-3.5",
            "model_description": "Conversational large language model in the GPT-3.5 family (proprietary OpenAI model). Exact parameter count and training details are not provided in this paper; treated as a pre-trained transformer-based LLM used via API for zero-shot, in‑context, and chain-of-thought (CoT) prompting.",
            "puzzle_name": "2D Direction Labeling; 2D Shape Identification; 3D Trajectory Labeling (CALVIN & CALVIN-Cleaned); SpartQA (spatial relationship QA)",
            "puzzle_description": "2D Direction Labeling: classify each segment between successive (x,y) points as left/right/up/down; Shape Identification: classify overall path shape (e.g., circle, checkmark) from noisy 2D finger-tracking data; 3D Trajectory Labeling: classify 3D end-effector trajectories into motion types (lifting, rotating, sliding) from CALVIN robot dataset and a human-cleaned subset; SpartQA: textual spatial reasoning QA (find relation, find blocks, choose object, yes/no). Each task requires interpreting numerical coordinates or textual spatial relations and mapping them to semantic labels.",
            "mechanism_or_strategy": "Prompting-based strategies were used: zero-shot prompting (single instruction), in-context learning (ICL / few-shot examples), chain-of-thought (CoT) prompting (few-shot examples augmented with step-by-step reasoning), and Spatial Prefix-Prompting (SPP) where the prompt first asks a simpler tangential spatial question to prime the model before the main query.",
            "evidence_of_spatial_reasoning": "Qualitative and behavioral evidence: (1) Human-evaluator judged correctness of ChatGPT outputs for trajectory labeling; (2) performance patterns: acceptable few-shot performance on 2D direction tasks but degraded performance on noisy/irregular 3D trajectories; (3) improvements from SPP (and ICL) indicate the model can exploit simple spatial computations (e.g., coordinate differences) implicitly; (4) authors note cases where CoT reasoning steps refer explicitly to coordinate trends (e.g., changes in z implying lift) — providing qualitative traces that model used coordinate-based notions.",
            "performance_metrics": "2D Direction Labeling (short integer sequences): accuracy 0.50, Avg. Err.# per sequence 0.15; 2D Direction Labeling (short float): acc 0.50, Err.# 0.25; 2D Direction Labeling (long integer): acc 0.00 (table value reported); 2D Shape Identification: integer version acc 0.71, float version acc 0.31 (no in-context used; zero-shot evaluation). 3D Trajectory (CALVIN): Zero-shot F1=0.19 Acc=0.26; In-context F1≈0.17 (reported), CoT F1=0.28 Acc≈0.33; SPP F1=0.32 Acc=0.43. 3D Trajectory (CALVIN-Cleaned): Zero-shot F1=0.24 Acc=0.30; CoT F1=0.14 Acc=0.34; SPP F1=0.38 Acc=0.43. SpartQA: ChatGPT-3.5 was not evaluated on SpartQA in this paper (Llama-2-7B used for SpartQA), so no SpartQA metrics for ChatGPT-3.5 are reported here.",
            "limitations_or_failure_cases": "Struggles on irregular/noisy 3D trajectories (low F1 and accuracy); Chain-of-Thought prompting produced volatile results and sometimes degraded performance, likely because CoT reasoning depends heavily on representativeness of examples and is brittle to noisy coordinate patterns; small dataset sizes (only 30 CALVIN samples) and hand-cleaned subsets limit statistical confidence; tokenization/representation issues for high-precision floats (authors experimented with integer vs float scaling to conserve tokens); qualitative mismatches where extraneous motion points confused semantic label mapping (e.g., small z fluctuations in a slide labeled as lift).",
            "comparison_baseline": "Compared across prompting methods: ICL and SPP often outperform zero-shot; SPP sometimes surpasses CoT and ICL (notably on CALVIN-Cleaned). ChatGPT-3.5 performs worse than ChatGPT-4 on most tasks (see ChatGPT-4 entry). No direct human performance numbers are provided for trajectories, though human evaluators judged outputs; SpartQA comparisons were performed with Llama-2-7B (other models not reported for that dataset in this paper).",
            "uuid": "e4800.0",
            "source_info": {
                "paper_title": "Exploring and Improving the Spatial Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ChatGPT-4",
            "name_full": "ChatGPT (GPT-4)",
            "brief_description": "OpenAI's GPT-4 conversational LLM evaluated for spatial labeling and reasoning tasks; shows strongest reported performance among evaluated models, especially when combined with Spatial Prefix-Prompting.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "ChatGPT-4",
            "model_description": "Proprietary large transformer-based language model (GPT-4). The paper does not list parameter counts or training corpora beyond citing the GPT-4 technical report; used via API as a pre-trained language model for zero-shot, few-shot (ICL), CoT, and SPP prompting experiments.",
            "puzzle_name": "2D Direction Labeling; 2D Shape Identification; 3D Trajectory Labeling (CALVIN & CALVIN-Cleaned)",
            "puzzle_description": "Same tasks as for ChatGPT-3.5: segmentwise directional labeling on 2D coordinate sequences, overall shape identification from noisy 2D finger-tracking data, and classification of 3D robotic trajectories into lifting/rotating/sliding classes requiring understanding of trends in x/y/z coordinates or higher-dimensional motion patterns.",
            "mechanism_or_strategy": "Prompting strategies used: zero-shot, in-context learning (few-shot), chain-of-thought prompting (few-shot + stepwise reasoning), and Spatial Prefix-Prompting (SPP). SPP asks a simpler tangential spatial question first to prime the model, then asks the complex 3D labeling question.",
            "evidence_of_spatial_reasoning": "Behavioral evidence: (1) High accuracy on 2D short trajectories (perfect classification reported for short sequences), indicating reliable exploitation of coordinate differences; (2) marked improvements with SPP, especially on cleaned 3D data, implying transfer from basic spatial sub-tasks to complex trajectory labeling; (3) qualitative CoT outputs that reference coordinate trends (e.g., recognizing 'down then up' in z-dimension as lifting) are reported, demonstrating internal use of spatial features in reasoning.",
            "performance_metrics": "2D Direction Labeling: short integer acc 1.00 Err.# 0.00; short float acc 1.00 Err.# 0.00; long integer acc 0.60 Err.# 0.13; 2D Shape Identification: integer acc 0.46, float acc 0.46. 3D Trajectory (CALVIN): Zero-shot F1=0.19 Acc=0.27; In-context F1≈0.36 Acc≈0.63; CoT F1≈0.33 Acc≈0.63; SPP F1=0.55 Acc=0.60. 3D Trajectory (CALVIN-Cleaned): Zero-shot F1=0.42 Acc=0.47; In-context F1=0.73 Acc=0.67; CoT F1=0.73 Acc=0.67; SPP F1=0.80 Acc=0.80.",
            "limitations_or_failure_cases": "Despite stronger performance, ChatGPT-4 still has subpar results on raw/unfiltered CALVIN trajectories (noisy, irregular motions), indicating brittleness to extraneous motion; CoT shows volatile gains (sometimes helps, sometimes hurts), suggesting sensitivity to example selection; experiments use small sample sizes (30 CALVIN samples), limiting generalization; inability to consistently segment trajectories or isolate motion-relevant subsequences without human pre-cleaning reduces practicality for raw robotic data.",
            "comparison_baseline": "Outperforms ChatGPT-3.5 and Llama-2-7B on most reported tasks; SPP often surpasses both ICL and CoT for ChatGPT-4 on cleaned 3D data. The paper notes models perform better on simpler 2D direction tasks than on complex 3D trajectory labeling, i.e., models perform worse on spatial trajectory tasks than on simpler non-spatial textual tasks they are often evaluated on.",
            "uuid": "e4800.1",
            "source_info": {
                "paper_title": "Exploring and Improving the Spatial Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Llama-2-7B",
            "name_full": "Llama 2 7B",
            "brief_description": "An open foundation LLM (7 billion parameters) used in this paper primarily for the SpartQA spatial question-answering evaluation and compared across prompting methods.",
            "citation_title": "Llama 2: Open foundation and fine-tuned chat models",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B",
            "model_description": "Open-source family model (7B parameter variant) from Meta (Llama 2). The paper uses the 7B model for faster evaluation on the SpartQA dataset; specific fine-tuning details are not provided beyond the citation.",
            "puzzle_name": "SpartQA (textual spatial reasoning benchmark: find relation (FR), find blocks (FB), choose object (CO), yes/no (YN))",
            "puzzle_description": "SpartQA consists of textual questions that probe spatial relations and reasoning over described block scenes, requiring compositional spatial understanding (e.g., identifying which block is above or between others, choosing an object matching spatial descriptors).",
            "mechanism_or_strategy": "Prompting methods evaluated: zero-shot, chain-of-thought (CoT), and Spatial Prefix-Prompting (SPP). For SpartQA, heuristics were applied for evaluation (e.g., checking if answer appears in first/last line) in addition to model outputs.",
            "evidence_of_spatial_reasoning": "Behavioral evidence via improved accuracy under SPP: SPP yields higher per-subtask and overall accuracy compared to zero-shot and CoT, which the authors interpret as the model leveraging simpler spatial priming to better answer compositional textual spatial questions. No internal probing or representation visualizations were performed.",
            "performance_metrics": "SpartQA test set (510 instances), Llama-2-7B: Zero-shot overall accuracy not explicitly tabulated for all subtypes in a single overall figure in the table, but reported values: CoT overall acc ≈ 0.36; SPP overall acc = 0.41. Per-subtask (from Table 3): SPP: FR 0.42, FB 0.44, CO 0.42, YN 0.40; CoT: FR 0.21, FB 0.40, CO 0.47, YN 0.24 (these per-subtask values are reported in the paper's Table 3).",
            "limitations_or_failure_cases": "Zero-shot performance is poor on many subtasks (e.g., FR). CoT helps in some categories but is inconsistent; SPP improves consistency but absolute accuracies remain modest (overall ~0.41), indicating limited deep spatial understanding. Evaluation used heuristics for parsing outputs and no human adjudication for SpartQA in this study, which may bias reported accuracies. Also the Llama-2-7B model is smaller; scaling effects on performance are not exhaustively explored.",
            "comparison_baseline": "SPP outperforms CoT and zero-shot in SpartQA for Llama-2-7B (overall 0.41 vs 0.36 for CoT). Authors compare Llama-2-7B's improvements qualitatively to ChatGPT family performance on other tasks, concluding SPP is a generally useful prompt design across model sizes, but ChatGPT-4 achieves higher absolute performance on trajectory tasks.",
            "uuid": "e4800.2",
            "source_info": {
                "paper_title": "Exploring and Improving the Spatial Reasoning Abilities of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Calvin: A benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks",
            "rating": 2,
            "sanitized_title": "calvin_a_benchmark_for_languageconditioned_policy_learning_for_longhorizon_robot_manipulation_tasks"
        },
        {
            "paper_title": "SPARTQA: A textual question answering benchmark for spatial reasoning",
            "rating": 2,
            "sanitized_title": "spartqa_a_textual_question_answering_benchmark_for_spatial_reasoning"
        },
        {
            "paper_title": "Chain-ofthought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models as general pattern machines",
            "rating": 2,
            "sanitized_title": "large_language_models_as_general_pattern_machines"
        },
        {
            "paper_title": "Pointllm: Empowering large language models to understand point clouds",
            "rating": 1,
            "sanitized_title": "pointllm_empowering_large_language_models_to_understand_point_clouds"
        },
        {
            "paper_title": "3d-llm: Injecting the 3d world into large language models",
            "rating": 1,
            "sanitized_title": "3dllm_injecting_the_3d_world_into_large_language_models"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.012792249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring and Improving the Spatial Reasoning Abilities of Large Language Models
2 Dec 2023</p>
<p>Manasi Sharma manasis@cs.stanford.edu 
Stanford University</p>
<p>Exploring and Improving the Spatial Reasoning Abilities of Large Language Models
2 Dec 2023636E69B66B5F2E9616592854B09FBAFAarXiv:2312.01054v1[cs.RO]
Large Language Models (LLMs) represent formidable tools for sequence modeling, boasting an innate capacity for general pattern recognition.Nevertheless, their broader spatial reasoning capabilities, especially applied to numerical trajectory data, remain insufficiently explored.In this paper, we investigate the out-of-the-box performance of ChatGPT-3.5,ChatGPT-4 and Llama 2 7B models when confronted with 3D robotic trajectory data from the CALVIN baseline and associated tasks, including 2D directional and shape labeling.Additionally, we introduce a novel prefix-based prompting mechanism, which yields a 33% improvement on the 3D trajectory data and an increase of up to 10% on SpartQA tasks over zero-shot prompting (with gains for other prompting types as well).The experimentation with 3D trajectory data offers an intriguing glimpse into the manner in which LLMs engage with numerical and spatial information, thus laying a solid foundation for the identification of target areas for future enhancements.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), e.g.&amp; PaLM [4], are massive models trained on diverse corpora with billions of tokens of text.Recent works have established the competence of LLMs in extrapolating more abstract, non-linguistic patterns, thus allowing them to serve as "general pattern machines [15].As such, in addition to the text-based tasks for which they were trained, LLMs successfully demonstrate cross-disciplinary capabilities, such as high-level planning for robotic policies [9,23,1], reward function design [10,8], and math &amp; logic puzzles.</p>
<p>Labeling of various kinds of data [6] falls under the paradigm of general pattern matching, and is of utmost practical use in the prospective organization of unlabeled raw datasets.One such application is in the space of language &amp; robotics; there are a limited number of datasets that supply language annotations for each demonstration (e.g. the trajectory is described by the instruction "pick up the blue cup"), as human labeling is costly.Intrigued by the potential for LLMs in annotating large-scale robotic datasets, we investigate LLM labeling as applied to 2D and 3D robotic trajectory data, i.e. the ability to describe a sequence of n-dimensional points with the type of motion it embodies, such as "lifting".While it is possible to explicitly train models for these capabilities, this work instead focuses on the inherent abilities of LLMs out-of-the-box, which may have downstream implications for broader numerical and spatial queries, e.g.trend analysis or time-series data.</p>
<p>Additionally, considering that LLMs appear to struggle with spatial reasoning abilities [2,23,5] (as defined by an understanding of shapes and relationships between different objects and spaces), we also gauge whether there are unique factors about this kind of numerical &amp; spatial data that impact the improvements furnished by prompting techniques like In-context Learning (ICL) and Figure 1: Illustrations of the Various Spatial Tasks: 2D Path Labeling of both directions (e.g."up", "left", etc.) and shapes, 3D Trajectory Labeling of "lifting", "rotating" and "sliding" motions (as well as the cleaned versions), and relationship identification between blocks in an imagined setup.</p>
<p>Chain-of-Thought (CoT) prompting.Subsequently, we propose a strategy to pre-fix a prompt with a more general example to achieve greater performance gains for this type of application.</p>
<p>To summarize, we are overall interested in empirically answering the following research questions: RQ1 Can LLMs be used to identify simple spatial patterns (e.g.circles or straightforward directions)?RQ2 Can LLMs be used to identify and label more complex spatial patterns (such as more irregular 3D trajectories)?As such, does the irregularity of the pattern make it difficult to CoT-type reasoningbased prompting?RQ3 Is there any knowledge transfer that can happen from simpler spatial tasks to more complex ones that can impact overall performance?</p>
<p>Related works</p>
<p>LLMs and Spatial Pattern Matching.Previous work has shown that LLMs can improve and complete low-level robotic action sequences or repetitive progressions like a sinusoid [15].However, labeling of a sequence in its entirety has not been addressed, which requires long-form context retention, semantic comprehension to link a sequence to its textual annotation, and generalization to non-repetitive, complex patterns.Some works show that LLMs acceptably label one-dimensional time-series data [25,11], but assess much shorter sequences, exclude higher dimensional analysis, or use additional token embedding models.Furthermore, on the whole, prevailing examinations of LLMs' spatial reasoning abilities [2,23,5] have revealed a considerably poor performance.We extend these works by tackling the inherent performance of LLMs on the underexplored subproblem of higher-dimensional trajectory identification.</p>
<p>LLMs &amp; 3D Robotics.LLMs have been applied across a number of areas in robotics, most recently in originating high level step-by-step plans from task descriptions [1, 9] and robot policy code publication [10,11].However, our work falls into the bucket of whether LLMs can directly understand control (e.g., at the level of trajectories) in a zero-shot manner, which remains an open problem.There are also works in the embodied robotics domain where LLMs are used to reason about a 3D point-cloud scene [7,19,24], but these papers either use a vision model (or joint vision-language model) to integrate visual embeddings or append a finite number of 3D object positions acquired using a detection model to the prompt.Our approach is distinguished by focusing on understanding continuous sequences of 3D points on the prompt-side.</p>
<p>LLMs &amp; Prompting Several prompting approaches have been shown to improve results, such as Incontext Learning [3], which supplies few-shot examples that guide the model, and Chain-of-Thought [21], which harnesses the ability of LLMs to adhere to a guided thought process for problem solving.The presence of symbols, patterns and texts are crucial to the effectiveness of CoT and ICL [12], but whether spatial trends follow such an archetype has yet to be explicitly examined.</p>
<p>3 Language Models as Trajectory Labelers 3.1 2D Path Labeling</p>
<p>Direction Labeling</p>
<p>As discussed in [15] the ability of an LLM to pattern match is driven by in-context learning on the provided numerical tokens, which can be formulated as the problem of using the context s 1:k = (s 1 , ..., s k ), where each s i is a symbol and using it to autoregressively predict s k+1 by using the factorized conditional probability p(s 1:k ) = Π n i=1 p(s i |s 1 , ..., s i−1 ).Usual in-context learning examples segment the prompt into continuations of multiple examples, each a variable length sequence:
x 1:k = (x 1 , ..., x k ) where each x i = (s i 1 , s i 2 , ..., s i m i ).
We adapt this paradigm for directional labeling by having x 1 state the model's expertise in spatial analysis and prompt it to generate direction labels for a newly provided sequence given the examples.Then each x i from i = 2 onwards is an example, an input-output pair D i ; L i where D i is the sequence of symbol aggregates
(d i 1 , d i 2 , ...d i j ) of length j, with each d i k further being separated out into the symbols (d i kx , d i ky ), representing a coordinate in 2D Cartesian space. L i is similarly a sequence of words (l i 1 , l i 2 , ...l i j−1 ) of length j − 1,
where each l i k is a word representing the direction that describes the movement from d i k to d i k+1 and is one of [left, right, up, down].There are thus j points and j − 1 segment labels (see Fig. 1).</p>
<p>Shape Identification</p>
<p>Using a similar prompt framework to the one described above, x 1 states the model's expertise in spatial analysis and prompts it to identify the overall shape of the movement represented by the list of 2D coordinates; for example, "moves along a path that mirrors the pattern of a checkmark" or "moves in a circular path".Since the dataset size is fairly limited, no in-context learning is applied and the model is evaluated zero-shot by specifying x 2 as the input sequence of (x, y) coordinates (d i 1 , d i 2 , ...d i j ) (see Fig. 1).</p>
<p>3D Trajectory Labeling</p>
<p>Zero-shot Prompting</p>
<p>To provide a baseline for the various prompting mechanisms probed, we initially implement zeroshot prompting, in which no exemplars are imparted to the model.Therefore, analogous to 2D experiments, x 1 states the model's expertise in spatial analysis and prompts it to classify the type of motion exemplified by the sequence into one of N categories, but there are no further sequences (see Fig. 2).For our set of experiments, we designate three classes that correspond to meaningfully and spatially disparate motions -lifting, rotating and sliding.</p>
<p>In-context Learning</p>
<p>In-context Learning (ICL) [1] supplies a few samples that the model can generalize from.x 1 declares that the model that it is an expert in 3D trajectory labeling and prompts the model to classify the type of motion exemplified by the sequence into one of N categories.Then each x i from i = 2 onwards is an input-output pair D i ; l i where D i is the sequence of symbol aggregates one representing a coordinate in 3D Cartesian space (d i kx , d i ky ), d i kz ).l i is a single word describing the motion and belongs to one of the N classes (see Fig. 2).
(d i 1 , d i 2 , ...d i j ), with each</p>
<p>Chain-of-Thought Prompting</p>
<p>Chain-of-Thought Prompting (CoT) [21], in which the few-shot examples are augmented by a stepby-step reasoning, has superseded In-context Learning on an array of textual and mathematical reasoning tasks.We extend it to our task as follows.The starting guideline for x 1 is the same as in-context learning, with the declaration that the model is an expert in 3D trajectory labeling and a prompt for the model to classify the type of motion exemplified by the sequence into one of N categories.However in CoT, successive pairs x 2 , x 3 , ..., x 2t , x 2t+1 for some t, represent the few-shot examples.The even x 2t is the representative sequence of 3D coordinates and the odd x 2t+1 is its associated answer, which is the motion-type label and accompanying reasoning steps (such as the fact that back-and-forth changes in the x and y coordinates can hint at a rotating motion, see Fig. 2).</p>
<p>Spatial Prefix-Prompting</p>
<p>Anticipating the challenge with generalizing from irregular examples and bolstered by the model's performance on simpler 2D data, we also propose a new method of prompting called "Spatial Prefix-Prompting" (SPP).The method draws from a prior selection of fixed questions that instigate the model to first ponder a tangentially related spatial problem (e.g.identifying the single direction an object is moving in or checking whether a point is in the center of a circle), and then use the "knowledge gained" to answer an adjacent question, such as labeling a new, more complex 3D trajectory (see Fig. 2).This technique does not necessitate the more intensive CoT-style curation of step-by-step examples, and we hypothesize that it may build upon the more fundamental spatial concepts a model is trained on to generalize better than few-shot learning (wherein the selected examples may not be representative of all the trajectories).</p>
<p>Experiments</p>
<p>Implementation Details</p>
<p>2D Labeling and Description</p>
<p>There aren't many datasets for elementary 2D shapes and directions, and given the ease of generating such data, we decided to autogenerate the datasets.For the direction labeling task, a dataset of size 30 is generated with 10 short-horizon sequences of 2D coordinates (of length 6-8 segments), 10 long-horizon long sequences (length 35 -40), and 10 short floating-point sequences using Python's NumPy package, with each segment's size and the directions randomly chosen based on a fixed seed.We experiment with both 1) scaling the values to integers between [0, 100] (to use fewer tokens to represent a single number) 2) scaling the values to double-digit fractions between [0, 100] (to test whether the higher token representation translates to higher precision).Only Zero-shot and In-context Learning are applied for Shape and Direction Labeling respectively.For the shape identification, we use some previously collected hand-gesture data in which human demonstrators sign a variety of shapes, including circles and check marks, and 2D positions of the finger are recorded; the dataset tests the model on the inherent noise from human demonstrations.A subset of the dataset is "cleaned" for use by having an expert to remove extraneous points that don't belong to the shape, and it is normalized to the range [0, 100] (both integer and floating point).The size of the dataset is 13.</p>
<p>3D Trajectory Labeling</p>
<p>We use the CALVIN benchmark [13], a dataset for learning long-horizon language-conditioned tasks for robotics.It includes 3D end-effector positions (can be extracted from the low-dimensional state vector) and associated language descriptions of the action the robot is attempting to complete.Due to time and resource constraints from the human evaluations, we select only a small subset of the CALVIN dataset (30 samples) and three disparate subtasks ("rotate", "lift", "slide").The trajectories are often complex and may not intuitively always resemble the action being completed, with many extraneous movements (see Fig. 1).Therefore, we also create a version of this dataset called "CALVIN-Cleaned" in which a human annotator extracts the parts of the trajectory that match with the specific action (e.g.only the lifting portion, see Fig. 1).Note, the CALVIN-Cleaned dataset retains the original "rotate" trajectories, as the task description is linked to the back-and-forth changes in the entirety of the motion, not any particular subsection.Finally, the dataset is normalized to the range [0, 300] ∈ Z to increase the granularity of the trajectory but optimize for token conservation due to the long-range of many CALVIN trajectories (upwards of 50 -100 points).</p>
<p>Spatial Relationship Identification</p>
<p>In order to demonstrate the efficacy of the Spatial Prefix-Prompting mechanism beyond just 3D trajectory classification, we also run experiments with Llama-2-7B on the entire test set of the SpartQA dataset [16] (of size 510 instances), a textual QA benchmark for deeper spatial reasoning questions of four types: find relation (FR), find blocks (FB), choose object (CO) (see Fig. 1).</p>
<p>Metrics and Models</p>
<p>As all of the tasks are classification / labeling tasks, we opt for traditional classification metrics, i.e. accuracy and F1-score, to reflect the balanced metrics of precision and recall.For the direction labeling, we also analyze the average number of direction misclassifications / errors per sequence, normalized by sequence length, calling this metric Err # -this is effectively how many of the directions in a single sequence are erroneously predicted.Human evaluators evaluate the ChatGPT responses for correctness while heuristics (when the answer appears in the first or last line) are used for SpartQA.We use three models for our experiments: the first two are ChatGPT 3.5 and 4 [17] and the third is Llama-7B [20] for SpartQA, chosen due to the quicker evaluation time for the size of the test dataset.We selected these models for their common use by the general public and quicker outputs.</p>
<p>Results</p>
<p>Our main results across the three tasks are given in Table 1, Table 2 and Table 3, and the main findings are as follows.</p>
<p>LLMs perform acceptable few-shot identification of directions As seen in Table 1, ChatGPT-3.5 and 4 succeed in achieving at least 50% classification rates, with better performance (Err.# &lt;25) on shorter trajectories (len&gt;5) and a neutral effect from the integer vs. floating point.We also see that parameter size and extensive training data likely play a huge role, with ChatGPT-4 hitting perfect classification for short trajectories and impressive performance (60%) for long trajectories (len&gt;35).</p>
<p>The models struggle with Shape Labeling however, unsurprisingly from the lack of data in zero-shot evaluation.</p>
<p>LLMs demonstrate poorer capabilities on more complex 3D trajectories We find that, as seen in Table 2, LLMs achieve subpar performance when compared with the 2D scenario, especially on the raw data from the CALVIN benchmark, with the highest F1-score capped at 63%.In their current form, it is improbable that such LLMs can be used for robotic trajectory classification.A possible cause for the disparity between the CALVIN and CALVIN-Cleaned datasets could be the higher degree of irregularity in the CALVIN dataset, since as demonstrated in [15], such models excel in mimicking more repetitive patterns (e.g.sinusoidal graphs).Another factor could be that LLMs connect spatial patterns to pre-trained semantic concepts in the CALVIN-Cleaned datasetfor example, the cleaned "lifting" trajectory illustrates only a downward and upward motion in the z-dimension, matching a fundamental understanding perhaps baked in from pretraining, whereas the extraneous datapoints might muddle such comprehension.</p>
<p>CoT reveals a reduction in spatial reasoning performance Table 2 and Fig. 3 conveys the volatility in the performance of CoT, revealing either losses or marginal performance gains (mostly bounded at 11%) compared to In-context Learning (ICL), and even the gains are much lower than other tasks [21].A potential reason for the diminishing returns in this application is that the CoT reasoning steps are fairly dependent on the examples chosen.We have qualitatively seen examples in which the model understands a single example in the context of its action (e.g.since an object is performing the action "lift", its z-coordinate decreases), but then witnesses a slight decrease in  the z-coordinate of a "slide" trajectory due to noisiness and concludes that it belongs to the "lift" category.</p>
<p>LLMs seem to enable knowledge transfer from simple to more complex tasks From Tables 2  and 3, we observe that Spatial Prefix-Prompting (SPP) often surpasses CoT and ICL, particularly on the CALVIN-Cleaned dataset and the "Find Relationship" (FR) and "Choose Object" (CO) questions in the SpartQA dataset [16].This outcome hints that SPP might perhaps be better suited to scenarios in which the labels themselves hold morphological meaning, permitting the model to expand upon its pretrained knowledgebase (e.g. in the FR and CO questions, the labels refer to directional relationships like "above" or qualitative adjectives "medium black square").Furthermore, it has previously been corroborated that, taking a Bayesian lens, ICL operates by helping the model to locate latent concepts that it learned during pretraining [22,14,18], i.e. if terms in a particular instance are exposed many times in the pretraining data, the model is likely to know better about the distribution of the inputs.It can be that SPP operates similarly, with a simple spatial question (e.g.direction identification) prodding the model to draw upon a more fundamental mechanism that it has been trained on (e.g.calculating numerical differences between coordinates to designate directions), in order to solve more complex questions that may use an analogous thought-process.</p>
<p>Conclusion</p>
<p>We examined the performance of LLMs including ChatGPT 3.5, 4 and Llama 2 7B on a variety of spatial tasks, namely 2D direction and path labeling, 3D trajectory labeling and abstract relationship identification.We show that the selected models exhibit acceptable performance on 2D direction labeling but flounder to a greater deal on 3D trajectory labeling.We speculate on possible causes, settling on the likelihood that the irregularity of the trajectories makes classification more onerous.We also hypothesize that the brittleness of Chain-of-Thought prompting's reliance on specific examples influences its diminished yield in noisy scenarios.Finally, we propose a technique called Spatial Prefix-Prompting that first inquires a simple, related question in order to better answer more complex spatial queries.Our work could have implications in a multitude of other domains than just higherdimensional numerical data, such as multi-variable financial trend forecasting or aggregate health data analysis.Future work includes evaluation on a larger robotic dataset, extension to other spatial tasks (e.g.segmenting trajectories), and assessment of other LLMs like PaLM 4. Overall, we establish that the domain of spatial reasoning, especially with regards to numerical data, is an underexplored realm ripe for more research.</p>
<p>Figure 2 :
2
Figure 2: Different Types of Prompting Mechanisms -Zero-shot, In-context Learning, Chain-of-Thought and Spatial Prefix-Prompting.In Spatial Pre-Prompt, a tangential question is first asked, to which the model provides a response, following which the primary query is inquired.</p>
<p>Figure 3 :
3
Figure 3: Accuracy Improvements (%) for In-context Learning, Chain-of-Thought and Spatial Prefix-Prompting on both the CALVIN and CALVIN-Cleaned datasets, for ChatGPT-3.5 and ChatGPT-4.As we can see, overall the accuracy gains for ChatGPT-4 are higher.</p>
<p>Table 1 :
1
2D Direction and Shape Labeling performance on short, long and floating-point sequences
Direction LabelingShape LabelingInteger (short)Float (short)Integer (long)IntegerFloatLLMAcc. (↑) Err. # (↓) Acc. (↑) Err. # (↓) Acc. (↑) Err. # (↓) Acc. (↑) Acc. (↑)ChatGPT-3.5 0.500.150.500.250.000.710.310.23ChatGPT-41.000.001.000.000.600.130.460.46Table 2: 3D Trajectory Labeling performance for ChatGPT-3.5 &amp; 4 on CALVIN &amp; CALVIN-CleanedChatGPT-3.5ChatGPT-4DatasetMethodF1 (↑) Acc. (↑) F1 (↑) Acc. (↑)Zero-shot 0.190.260.190.27CALVINIn-context 0.17 CoT 0.280.33 0.360.63 0.330.63 0.37SPP0.320.430.550.60Zero-shot 0.240.300.420.47CALVIN-CleanedIn-context 0.16 CoT 0.140.34 0.260.62 0.730.67 0.73SPP0.380.430.800.80</p>
<p>Table 3 :
3
Llama 2 7B performance on the SpartQA Test Dataset, split by subtypes FR, FB, CO, YN
FRFBCOYNOverallLLM Acc. Llama 2 7B Method Zero-shot 0.14 CoT 0.210.40 0.470.24 0.160.39 0.480.32 0.36SPP0.420.440.420.400.41
(↑) Acc.(↑) Acc.(↑) Acc.(↑) Acc.(↑)</p>
<p>. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, A Herzog, D Ho, J Hsu, J Ibarz, B Ichter, A Irpan, E Jang, R J Ruano, K Jeffrey, S Jesmonth, N J Joshi, R Julian, D Kalashnikov, Y Kuang, K.-H Lee, S Levine, Y Lu, L Luu, C Parada, P Pastor, J Quiambao, K Rao, J Rettinghouse, D Reyes, P Sermanet, N Sievers, C Tan, A Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, S Xu, M Yan, A Zeng, 2022Do as i can, not as i say: Grounding language in robotic affordances</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Q V Do, Y Xu, P Fung, 2023</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, 2020</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, P Schuh, K Shi, S Tsvyashchenko, J Maynez, A Rao, P Barnes, Y Tay, N Shazeer, V Prabhakaran, E Reif, N Du, B Hutchinson, R Pope, J Bradbury, J Austin, M Isard, G Gur-Ari, P Yin, T Duke, A Levskaya, S Ghemawat, S Dev, H Michalewski, X Garcia, V Misra, K Robinson, L Fedus, D Zhou, D Ippolito, D Luan, H Lim, B Zoph, A Spiridonov, R Sepassi, D Dohan, S Agrawal, M Omernick, ; R Child, O Polozov, K Lee, Z Zhou, X Wang, B Saeta, M Diaz, O Firat, M Catasta, J Wei, K Meier-Hellstern, D Eck, J Dean, S Petrov, N Fiedel, M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira,2022</p>
<p>Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of llms. A G Cohn, J Hernandez-Orallo, 2023</p>
<p>Annollm: Making large language models to be better crowdsourced annotators. X He, Z Lin, Y Gong, A.-L Jin, H Zhang, C Lin, J Jiao, S M Yiu, N Duan, W Chen, 2023</p>
<p>Y Hong, H Zhen, P Chen, S Zheng, Y Du, Z Chen, C Gan, 3d-llm: Injecting the 3d world into large language models. 2023</p>
<p>Language instructed reinforcement learning for human-ai coordination. H Hu, D Sadigh, 2023</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, 2022</p>
<p>Large language models are few-shot health learners. M Kwon, S M Xie, K Bullard, D Sadigh, ; X Liu, D Mcduff, G Kovacs, I Galatzer-Levy, J Sunshine, J Zhan, M.-Z Poh, S Liao, P D Achille, S Patel, 2023. 2023Reward design with language models</p>
<p>Text and patterns: For effective chain of thought, it takes two to tango. A Madaan, A Yazdanbakhsh, 2022</p>
<p>Calvin: A benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, 2022</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. S Min, X Lyu, A Holtzman, M Artetxe, M Lewis, H Hajishirzi, L Zettlemoyer, 2022</p>
<p>Large language models as general pattern machines. S Mirchandani, F Xia, P Florence, B Ichter, D Driess, M G Arenas, K Rao, D Sadigh, A Zeng, 2023</p>
<p>SPARTQA: A textual question answering benchmark for spatial reasoning. R Mirzaee, H Faghihi, Q Ning, P Kordjamshidi, 10.18653/v1/2021.naacl-main.364Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Impact of pretraining term frequencies on fewshot numerical reasoning. Y Razeghi, R L Logan, I V , M Gardner, S Singh, 10.18653/v1/2022.findings-emnlp.59Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>A Takmaz, E Fedele, R W Sumner, M Pollefeys, F Tombari, F Engelmann, Openmask3d: Open-vocabulary 3d instance segmentation. 2023</p>
<p>. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I Kloumann, A Korenev, P S Koura, M.-A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, T Scialom, 2023Llama 2: Open foundation and fine-tuned chat models</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 2023</p>
<p>An explanation of in-context learning as implicit bayesian inference. S M Xie, A Raghunathan, P Liang, T Ma, 2022</p>
<p>Y Xie, C Yu, T Zhu, J Bai, Z Gong, H Soh, Translating natural language to planning goals with large-language models. 2023</p>
<p>Pointllm: Empowering large language models to understand point clouds. R Xu, X Wang, T Wang, Y Chen, J Pang, D Lin1, 2023</p>
<p>Promptcast: A new prompt-based learning paradigm for time series forecasting. H Xue, F D Salim, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
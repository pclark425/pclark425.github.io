<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7221 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7221</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7221</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-263909086</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-emnlp.111.pdf" target="_blank">Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7221.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7221.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 (Text-Davinci-003 / Chat-completion GPT-3.5 series)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive transformer language model from OpenAI used in this paper as a representative earlier-generation LLM; evaluated via the OpenAI completion/chat completion APIs at temperature 0.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (Text-Davinci-003 / GPT-3.5 chat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM from OpenAI (GPT family), used via OpenAI completion/chat-completion API; instruction-following tuned in chat variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>MoocRadar educational diagnostic assessment (mixed exercises annotated with Bloom's Taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>mixed cognitive domains (memory/knowledge, understanding, application, analysis, evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A multi-type (single choice, multiple choice, true/false) educational exercise set derived from MoocRadar, annotated by Bloom's taxonomy to probe different knowledge types and cognitive dimensions; models answer each question and provide explanations; average accuracy across questions is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (average score per question; multiple-choice partial credit 0.5 if not all correct options chosen)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 0.746 (average student score on the MoocRadar subset, as extracted from MOOCCubeX)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 0.436 (no context); 0.508 (with retrieved context)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot prompts tailored per question type; contexts optionally added via BM25 retrieval (two retrieved contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Average student behaviors / scores from MoocRadar dataset extracted from MOOCCubeX (as stated in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Performance reported as average accuracy over 8453 selected MoocRadar questions. Context retrieval (two contexts via BM25) improves GPT-3.5 performance. Paper notes dataset imbalances and that annotators judged model answers/explanations; no statistical tests vs humans reported. The paper also describes that GPT-3.5 exhibits a primacy bias (option-order sensitivity) in single-choice tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7221.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7221.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI conversational GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI conversational instruction-tuned GPT-3.5 chat model used via the chat completion API at temperature 0 to produce greedy outputs; evaluated on MoocRadar exercises for answers and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5 chat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational variant of GPT-3.5 (autoregressive transformer), accessed via OpenAI chat API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>MoocRadar educational diagnostic assessment (mixed exercises annotated with Bloom's Taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>mixed cognitive domains (memory/knowledge, understanding, application, analysis, evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same MoocRadar assessment described above; models are asked to provide an answer and an explanation for each exercise, across SC/MC/TF types.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (average score per question; MC partial credit rules apply)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 0.746 (average student score on the MoocRadar subset, as extracted from MOOCCubeX)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 0.506 (no context); 0.526 (with retrieved context)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot; answers requested along with explanations; contexts optionally prepended via BM25 retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Average student behaviors / scores from MoocRadar dataset extracted from MOOCCubeX (as stated in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>ChatGPT is reported to be more robust to option-order manipulations in single-choice questions (less primacy bias) than the other evaluated models. Explanations were independently annotated by human annotators; the paper reports ChatGPT intermediate likeness-to-human structure (Pearson correlation) relative to other models. No statistical significance tests vs human baseline are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7221.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7221.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art OpenAI GPT-4 model evaluated via the chat completion API at temperature 0; shows the highest accuracy among tested models on the MoocRadar assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latest-generation autoregressive transformer LLM from OpenAI, accessed via chat completion API for this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>MoocRadar educational diagnostic assessment (mixed exercises annotated with Bloom's Taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>mixed cognitive domains (memory/knowledge, understanding, application, analysis, evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MoocRadar exercise suite covering Bloom's taxonomy dimensions and four knowledge types; models answer SC/MC/TF items and provide explanations; performance aggregated as accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (average score per question; MC partial credit rules apply)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 0.746 (average student score on the MoocRadar subset, as extracted from MOOCCubeX)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 0.657 (no context); 0.687 (with retrieved context)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot; contextual retrieval via BM25 optionally added; models prompted to give answer and explanation</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Average student behaviors / scores from MoocRadar dataset extracted from MOOCCubeX (as stated in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-4 obtains the highest accuracy among the three models but the paper reports that models are still not uniformly better than humans across all disciplines; the authors specifically note GPT-4 can outperform humans in some STEM contexts when provided retrieved context (the paper does not provide formal hypothesis tests). The paper also documents GPT-4's sensitivity to option order (primacy effect) in single-choice items in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7221.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7221.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Primacy-effect (option-order robustness) test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-choice option-order robustness test probing primacy/recency effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experiment manipulating the order of options in single-choice questions to probe whether LLMs show primacy or recency biases and whether answers and explanations remain consistent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, ChatGPT, GPT-4 (evaluated comparatively)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same OpenAI models listed above; tested for sensitivity to the position of correct option in single-choice questions by moving the correct option to first vs last positions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Option-order single-choice test (primacy vs recency)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>attention / memory bias (cognitive psychology phenomenon)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Manipulate whether the correct option is listed early or late in single-choice questions and measure changes in accuracy and consistency between answers and explanations; used to detect primacy/recency biases analogous to human serial-position effects.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (single-choice accuracy) and answer/explanation consistency rate</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative: GPT-3.5 and GPT-4 exhibit a primacy bias (better when correct option appears earlier) and reduced consistency when correct option appears later; ChatGPT is reported as more robust to option order changes.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot single-choice prompts with controlled option order; explanations requested after answer</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>This test probes a classic cognitive-psychology serial-position effect (primacy). The paper provides case examples (e.g., word-origin questions) where changing option order flips model answers and produces conflicting but confident explanations. No human baseline for option-order sensitivity on the same items is provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mind meets machine: Unravelling gpt-4's cognitive psychology <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>MoocRadar: A fine-grained and multi-aspect knowledge repository for improving cognitive student modeling in MOOCs <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 1)</em></li>
                <li>Educational diagnostic assessment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7221",
    "paper_id": "paper-263909086",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GPT-3.5",
            "name_full": "OpenAI GPT-3.5 (Text-Davinci-003 / Chat-completion GPT-3.5 series)",
            "brief_description": "An autoregressive transformer language model from OpenAI used in this paper as a representative earlier-generation LLM; evaluated via the OpenAI completion/chat completion APIs at temperature 0.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (Text-Davinci-003 / GPT-3.5 chat)",
            "model_description": "Autoregressive transformer LLM from OpenAI (GPT family), used via OpenAI completion/chat-completion API; instruction-following tuned in chat variants.",
            "model_size": null,
            "test_name": "MoocRadar educational diagnostic assessment (mixed exercises annotated with Bloom's Taxonomy)",
            "test_category": "mixed cognitive domains (memory/knowledge, understanding, application, analysis, evaluation)",
            "test_description": "A multi-type (single choice, multiple choice, true/false) educational exercise set derived from MoocRadar, annotated by Bloom's taxonomy to probe different knowledge types and cognitive dimensions; models answer each question and provide explanations; average accuracy across questions is reported.",
            "evaluation_metric": "accuracy (average score per question; multiple-choice partial credit 0.5 if not all correct options chosen)",
            "human_performance": "accuracy 0.746 (average student score on the MoocRadar subset, as extracted from MOOCCubeX)",
            "llm_performance": "accuracy 0.436 (no context); 0.508 (with retrieved context)",
            "prompting_method": "zero-shot prompts tailored per question type; contexts optionally added via BM25 retrieval (two retrieved contexts)",
            "fine_tuned": false,
            "human_data_source": "Average student behaviors / scores from MoocRadar dataset extracted from MOOCCubeX (as stated in the paper)",
            "statistical_significance": null,
            "notes": "Performance reported as average accuracy over 8453 selected MoocRadar questions. Context retrieval (two contexts via BM25) improves GPT-3.5 performance. Paper notes dataset imbalances and that annotators judged model answers/explanations; no statistical tests vs humans reported. The paper also describes that GPT-3.5 exhibits a primacy bias (option-order sensitivity) in single-choice tests.",
            "uuid": "e7221.0",
            "source_info": {
                "paper_title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI conversational GPT-3.5 family)",
            "brief_description": "OpenAI conversational instruction-tuned GPT-3.5 chat model used via the chat completion API at temperature 0 to produce greedy outputs; evaluated on MoocRadar exercises for answers and explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5 chat)",
            "model_description": "Instruction-tuned conversational variant of GPT-3.5 (autoregressive transformer), accessed via OpenAI chat API.",
            "model_size": null,
            "test_name": "MoocRadar educational diagnostic assessment (mixed exercises annotated with Bloom's Taxonomy)",
            "test_category": "mixed cognitive domains (memory/knowledge, understanding, application, analysis, evaluation)",
            "test_description": "Same MoocRadar assessment described above; models are asked to provide an answer and an explanation for each exercise, across SC/MC/TF types.",
            "evaluation_metric": "accuracy (average score per question; MC partial credit rules apply)",
            "human_performance": "accuracy 0.746 (average student score on the MoocRadar subset, as extracted from MOOCCubeX)",
            "llm_performance": "accuracy 0.506 (no context); 0.526 (with retrieved context)",
            "prompting_method": "zero-shot; answers requested along with explanations; contexts optionally prepended via BM25 retrieval",
            "fine_tuned": false,
            "human_data_source": "Average student behaviors / scores from MoocRadar dataset extracted from MOOCCubeX (as stated in the paper)",
            "statistical_significance": null,
            "notes": "ChatGPT is reported to be more robust to option-order manipulations in single-choice questions (less primacy bias) than the other evaluated models. Explanations were independently annotated by human annotators; the paper reports ChatGPT intermediate likeness-to-human structure (Pearson correlation) relative to other models. No statistical significance tests vs human baseline are reported.",
            "uuid": "e7221.1",
            "source_info": {
                "paper_title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "State-of-the-art OpenAI GPT-4 model evaluated via the chat completion API at temperature 0; shows the highest accuracy among tested models on the MoocRadar assessment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Latest-generation autoregressive transformer LLM from OpenAI, accessed via chat completion API for this study.",
            "model_size": null,
            "test_name": "MoocRadar educational diagnostic assessment (mixed exercises annotated with Bloom's Taxonomy)",
            "test_category": "mixed cognitive domains (memory/knowledge, understanding, application, analysis, evaluation)",
            "test_description": "MoocRadar exercise suite covering Bloom's taxonomy dimensions and four knowledge types; models answer SC/MC/TF items and provide explanations; performance aggregated as accuracy.",
            "evaluation_metric": "accuracy (average score per question; MC partial credit rules apply)",
            "human_performance": "accuracy 0.746 (average student score on the MoocRadar subset, as extracted from MOOCCubeX)",
            "llm_performance": "accuracy 0.657 (no context); 0.687 (with retrieved context)",
            "prompting_method": "zero-shot; contextual retrieval via BM25 optionally added; models prompted to give answer and explanation",
            "fine_tuned": false,
            "human_data_source": "Average student behaviors / scores from MoocRadar dataset extracted from MOOCCubeX (as stated in the paper)",
            "statistical_significance": null,
            "notes": "GPT-4 obtains the highest accuracy among the three models but the paper reports that models are still not uniformly better than humans across all disciplines; the authors specifically note GPT-4 can outperform humans in some STEM contexts when provided retrieved context (the paper does not provide formal hypothesis tests). The paper also documents GPT-4's sensitivity to option order (primacy effect) in single-choice items in some cases.",
            "uuid": "e7221.2",
            "source_info": {
                "paper_title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Primacy-effect (option-order robustness) test",
            "name_full": "Single-choice option-order robustness test probing primacy/recency effects",
            "brief_description": "An experiment manipulating the order of options in single-choice questions to probe whether LLMs show primacy or recency biases and whether answers and explanations remain consistent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, ChatGPT, GPT-4 (evaluated comparatively)",
            "model_description": "Same OpenAI models listed above; tested for sensitivity to the position of correct option in single-choice questions by moving the correct option to first vs last positions.",
            "model_size": null,
            "test_name": "Option-order single-choice test (primacy vs recency)",
            "test_category": "attention / memory bias (cognitive psychology phenomenon)",
            "test_description": "Manipulate whether the correct option is listed early or late in single-choice questions and measure changes in accuracy and consistency between answers and explanations; used to detect primacy/recency biases analogous to human serial-position effects.",
            "evaluation_metric": "accuracy (single-choice accuracy) and answer/explanation consistency rate",
            "human_performance": null,
            "llm_performance": "Qualitative: GPT-3.5 and GPT-4 exhibit a primacy bias (better when correct option appears earlier) and reduced consistency when correct option appears later; ChatGPT is reported as more robust to option order changes.",
            "prompting_method": "zero-shot single-choice prompts with controlled option order; explanations requested after answer",
            "fine_tuned": false,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "This test probes a classic cognitive-psychology serial-position effect (primacy). The paper provides case examples (e.g., word-origin questions) where changing option order flips model answers and produces conflicting but confident explanations. No human baseline for option-order sensitivity on the same items is provided in the paper.",
            "uuid": "e7221.3",
            "source_info": {
                "paper_title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mind meets machine: Unravelling gpt-4's cognitive psychology",
            "rating": 2,
            "sanitized_title": "mind_meets_machine_unravelling_gpt4s_cognitive_psychology"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "MoocRadar: A fine-grained and multi-aspect knowledge repository for improving cognitive student modeling in MOOCs",
            "rating": 2,
            "sanitized_title": "moocradar_a_finegrained_and_multiaspect_knowledge_repository_for_improving_cognitive_student_modeling_in_moocs"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 1,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Educational diagnostic assessment",
            "rating": 1,
            "sanitized_title": "educational_diagnostic_assessment"
        }
    ],
    "cost": 0.01089375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach</p>
<p>Zheyuan Zhang zheyuan-22@mails.tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Jifan Yu 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Juanzi Li lijuanzi@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Lei Hou houlei@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach
AE4766523547252B8112242E46A92A5FRemembering RememberKnowIdentify... Understanding TranslateExplainInduce... Applying ProveEstimateExecute... Analyzing CompareSelectOrganize... Evaluating EvaluateJudgeCriticise... Creating DesignCreateProgram
Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence.Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains.However, cognitive research on the overall knowledge structure of LLMs is still lacking.In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy.We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities.This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs.By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.</p>
<p>Introduction</p>
<p>Large language models (LLMs), such as GPT series (Brown et al., 2020), Flan (Wei et al., 2022), and PaLM (Chowdhery et al., 2022), have gained significant attention worldwide due to their remarkable ability.Given their unprecedented human-like performances, researchers have started to explore alternative evaluation metrics beyond traditional benchmarks like MMLU (Hendrycks et al., 2021) and Big-Bench (Ghazal et al., 2017).</p>
<p>Existing Works on LLMs Evaluation with Exams.Researchers have long sought models capable of passing human exams (Nilsson, 2005).Recently, a new approach simulates professional exams designed for humans to evaluate LLMs.For example, OpenAI (2023) reports the performance of GPT series on a variety of exams, including AP exams, SAT, Leetcode, and so on.There are also emerging benchmarks that comprise common standardized exams, such as AGIEval (Zhong et al., 2023), C-Eval (Huang et al., 2023), M3Exam (Zhang et al., 2023), and CMExam (Liu et al., 2023).However, although standardized exams contain diverse information, these works condense them into a single overall score, lacking structured understanding of LLMs' knowledge and cognitive patterns.</p>
<p>For example, while LLMs demonstrate exceptional performance on tasks challenging for humans, they might still struggle with basic knowledge, as illustrated in Figure 1, which may lead to over-estimation of the validity of model generated contents.Therefore, there is a pressing need for further research of models' knowledge and cognitive distribution in comparison to humans.</p>
<p>Proposed Research.To investigate this problem, we draw inspiration from psychometric methods that use cognitive psychology theories to evaluate LLMs.This topic has gained traction as LLMs continue to demonstrate exceptional performances (Chollet, 2019;Singh et al., 2023;Bubeck et al., 2023).In this work, we adopt the Educational Diagnostic Assessment approach and leverage MoocRadar (Yu et al., 2023), a novel student exercise dataset annotated with Bloom's Taxonomy (Anderson and Krathwohl, 2001), to assess the cognitive capability of LLMs.Specifically, we delve into three primary research questions: 1) Performance Analysis: the proficiency and robustness of LLMs across various question domains; 2) Deficit Assessment: the knowledge structure and the extent to which LLMs are similar with humans; and 3) Error Assessment: the error pattern of LLMs in answers and explanations.Our findings contribute to a deeper understanding of the knowledge structure of LLMs and insights for evaluation.</p>
<p>Contributions.Our main contributions are:</p>
<p>(i) We introduce the topic of the cognitive knowledge structure of LLMs.(ii) We propose a method of Educational Diag-</p>
<p>â­•</p>
<p>Instructions</p>
<p>Single Choice</p>
<p>What's the answer of this single choice question?why?</p>
<p>True or False</p>
<p>Please determine true or false and provide your reason.</p>
<p>Multiple Choice</p>
<p>What's the answer of this multiple choice question?why?</p>
<p>Diagnostic</p>
<p>Performance</p>
<p>Disciplines Contexts</p>
<p>Deficit</p>
<p>Knowledge Cognitive</p>
<p>Error</p>
<p>Answers Explanations</p>
<p>Figure 1: ChatGPT correctly answers a question that is challenging with higher knowledge type and cognitive dimensions right (problem 2) but encounters difficulties in an easier one (problem 1).We design specific instructions to evaluate LLMs and assess their performances in three aspects.</p>
<p>nostic Assessment to evaluate LLMs on their cognitive knowledge structure.(iii) We assess LLMs' performance, deficits, and errors, gaining insights into their capabilities.</p>
<p>Method</p>
<p>Educational Diagnostic Assessment</p>
<p>In education scenarios, Diagnostic Assessment is a widely employed method to gauge students' knowledge structure (Falmagne et al., 2006), discovering their proficiency on certain subject matters and learning styles (Vuong et al., 2021), typically through sets of questions (Leighton and Gierl, 2007).Two main approaches of Diagnostic Assessment include deficit assessment, which focuses on identifying and addressing knowledge gaps in various domains and the degree of knowledge mastery, and error assessment, which focuses on error patterns and strategies for correction (Bejar, 1984).Drawing inspiration from Diagnostic Assessment methods, in this work, based on Bloom's Taxonomy, we use deficit assessment to test the accuracy of models on a wide range of exercises, and error assessment on their answers and explanations.</p>
<p>Experimental Setup</p>
<p>Dataset.In this section, we introduce the dataset utilized for assessment, MoocRadar, offering an extensive overview of its general information.</p>
<p>MoocRadar is a fine-grained and multi-aspect exercise repository designed for cognitive modeling and educational diagnostic assessment.We carefully select 8453 questions appropriate for model evaluation, which fall into three types: single choice (SC), multiple choice (MC), and true or false (TF).Additionally, we exclude the dimension of Create because of the scarcity of related exercises.We further classify these questions into four disciplines by their course information, including STEM, social science, humanity, and others.We test the performance of models on them and analyze the distribution of these features.More details of MoocRadar are illustrated in the appendix.</p>
<p>Model Selection.We carefully choose 3 advanced models that have consistently demonstrated leading performance and are widely recognized in the field, including: Text-Davinci-003, Chat-GPT, and GPT-4, which represent a series of most acknowledged models.All experiments are performed using the APIs provided by OpenAI.Specif-ically, we use the completion API for Text-Davinci-003 and the chat completion API for ChatGPT and GPT-4.To ensure consistency in the quality of the responses, we set the temperature to 0 to get greedy search responses generated by each model.</p>
<p>Experimental Design.As shown in Figure 1, we design different prompts tailored to each type of exercises to query LLMs for both answers and explanation.All tasks are conducted in zero-shot scenario.To simulate human-like behavior that solving exercises with relevant knowledge, we leverage the BM25 algorithm to retrieve the two most related discussions from the subtitles in the corresponding courses in MOOCCubeX (Yu et al., 2021) and test their effect.Moreover, we extract real student behaviors on MoocRadar dataset from MOOCCubeX and calculate their average scores to serve as a reference of humans.Based on both results from human and LLMs, this work provides a road map with investigation to the following research questions:</p>
<p>(RQ1) Performance Analysis: What's the features of LLMs' basic performance on different disciplines and their robustness to these questions?</p>
<p>(RQ2) Deficit Assessment: According to Bloom Taxonomy, compared with humans, what knowledge distribution does LLMs demonstrate?Are they similar to humans in knowledge structure?</p>
<p>(RQ3) Error Assessment: Based on answers and explanations, what's their pattern of errors?</p>
<p>Experiment</p>
<p>In this section, we conduct experiments and analyze the results from three perspectives in the following subsections.We assign a score of 1 to each question type.Following standardized exams, for multiple questions, models receive a score of 0.5 if they fail to select all correct options.We then calculate the average score across questions.</p>
<p>Performance Analysis</p>
<p>Firstly, we assess their performance both with and without contexts, compare their performance in different disciplines, and examine their robustness.</p>
<p>Disciplines and Context.We exhibit scores of model answers with or without context on the four disciplines (STEM, social science, humanity, and others).As shown in Table 2, the later versions of GPT significantly outperform previous models, with GPT-4 being the most advanced, but not better than humans' average.Additional knowledge from context indeed enhances the performance of the models.Comparatively, STEM exercises are more challenging as illustrated in human results, while LLMs demonstrate impressive capability in STEM knowledge.GPT-4 even outperforms humans with context.However, it is surprising that LLMs don't perform as effectively in social science and humanities exercises, even though these disciplines primarily involve natural language.Robustness.In single choice questions, we manipulate the order of the options by either placing the correct answer at the beginning or the end.This allows us to examine if such modifications affect the model's accuracy.As shown in table 3, we find that 1) ChatGPT is more robust to changing of options, while the other two exhibits a cognitive bias as Primacy Effect (Deese and Kaufman, 1957) that early appearance aids performance; 2) if the correct answer appears later in GPT-3.5 and GPT-4, they mistakenly change their answers and explanations; 3) later appearance causes less consistency in answers and explanations in less robust models.</p>
<p>Deficit Assessment</p>
<p>We utilize Bloom's Taxonomy in MoocRadar to demonstrate models' distribution in cognitive dimensions and knowledge types and design a score to measure similarities of models to humans.Bloom Taxonomy Distribution.As shown in Figure 2, we demonstrate the distribution based on Bloom's taxonomy, where deeper colors represent better performance.The 0 and 1 grids are due to the limited number of exercises, typically only one.Generally, both in knowledge types and cognitive dimensions, questions in the intermediate range are more challenging for models and humans.We design a similarity score for deeper understanding.Similarity Score.According to the accuracy of models in various dimensions of knowledge and cognition, we develop a metric to measure their similarity to humans, which primarily considers knowledge structure, beyond mere performance, and estimates the extent to which their cognitive structure is proportional to that of humans.Specifically, given a model M , the 4<em>5 vector of the model distribution in bloom's taxonomy x and human distribution y, convert x and y into 1</em>20 vectors x and á»¹, the similarity between M and human can be defined as: Likeness(M ) = Ï(x, á»¹), where Ï(x, á»¹) represents the Pearson Correlation Coefficient of x and á»¹.We calculate the Likeness of the three models in Table 4.The likeness also exhibits a rising tendency as the models evolve.Models that follow human instructions better are also more similar to humans in knowledge structure.</p>
<p>Error Assessment</p>
<p>In this section, we analyze the error of each models, by delving into their explanation of their answers.Explanation Accuracy.Table 5 demonstrate the accuracy of answers in each type.We mainly find that: 1) Models perform best on TF and worst on MC.MC could be more difficult than SC and TF, because of more thinking steps (determine TF of each options, and select multiple ones).2) Explanations and answers are more consistent in TF than in SC and MC for the same reason, as there are more chances to make errors.3) Accuracy of explanations falls behind answers in MC, where models can select some of the correct options for the wrong reason.4) Context does not necessary aid and even hurt explanation performances, but indeed aids answer accuracy.More advanced models are more consistent in their answers and explanations.</p>
<p>Discussion</p>
<p>In this section, we discuss our findings on the proposed three research questions:</p>
<p>Performance Analysis.We exhibit different models' performance.Comparing with humans, they are less proficient in disciplines primarily involve natural language, but better at STEM.Though with sufficient knowledge, they might have hallucination on specific long-tail concepts in humanity and social science.LLMs are not robust in option orders, and exhibit a cognitive bias as Primacy Effect rather than Recency Effect.</p>
<p>Deficit Assessment.Models are less proficiency in the intermediate range of Bloom's Taxonomy.The reason could be that application-based ques-tions, such as solving mathematical problems and making deductions using chemical theorems, are prone to errors and are inherently challenging for models.For analyzing and evaluating questions, the strong linguistic capabilities of models allow them to excel in these tasks, and perform even better than intermediate-level questions.More advanced models demonstrate more similarity with humans in knowledge structure, which might be an additional effect of human alignment.</p>
<p>Error Assessment.By comparing different kinds of questions, we find that gap exists for models between knowledge and answers.They perform worse in multiple choices, as there are more thinking steps and error chances.Accuracy of explanations can be worse than answers: as models were asked to generate answers first, their explanation could shift due to wrong answers and question orders, and cause their hallucinations.Due to the limitations of autoregressive architecture (Bubeck et al., 2023), their errors could snowball.</p>
<p>Conclusion</p>
<p>In this work, we introduce a new research question on LLMs analyzing, which calls for a deeper understanding of the knowledge structure of these models.We use Educational Diagnostic Assessment as a tool to test the performance of LLMs on various dimensions, and develop a metric to measure the similarity of their knowledge structure with humans.We provide findings and discussion for insight into research on the cognition of LLMs.</p>
<p>Limitations</p>
<p>In this section, we describe the limitations of this work in terms of the dataset and experiments.</p>
<p>Dataset.We investigated the knowledge distribution of LLMs based on the MoocRadar dataset.MoocRadar is a fine-grained, well-structured dataset that is distinct from commonly used benchmarks in terms of knowledge annotation.However, as a dataset for educational diagnostic assessment, it's still limited in the following aspects: 1) Different categories of exercises (e.g.question type, disciplines) have an unbalanced distribution; 2) As demonstrated in the Robustness section, the performance of the models can vary due to different forms of exercises.</p>
<p>Experiment.Due to time and cost constrains, 1) we only included three LLMs by OpenAI, which are all closed-source models.Therefore, we did not conduct experiments at the parameter level.2) though we have discovered some phenomena, further experiments and deeper analysis are not conducted.We include some of them in the case study section in the appendix.</p>
<p>Future Works.Future works include 1) more models for experiments, 2) further exploration on robustness and similarity with humans, and 3) as the next step of diagnostic assessment, investigate how to optimize the knowledge structure of LLMs.</p>
<p>Ethics Statement</p>
<p>We foresee no ethic concerns in this work.The MoocRadar dataset employed in our research is publicly available, and it does not contain any personal information.</p>
<p>A Appendix</p>
<p>A.1 Details of Experiment</p>
<p>This subsection shows details of the dataset we use, and experiment for diagnostic assessment.</p>
<p>Problem Statistics.Table 6 demonstrates the details of the dataset we use, which is selected from the original MoocRadar.Generally, we include three question types (single choice, multiple choice, and true or false), four knowledge types (factual knowledge, conceptual knowledge, procedural knowledge, and meta knowledge), and five cognitive dimensions (remember, understand, apply, analyze, and evaluate), to form a total dataset of 8430 questions.Problem Examples.Table 7 demonstrates examples for each type of questions.There are two or more than two options in single choices and only one correct options, while multiple choices have more than one correct options.True or false questions should be answered as True or False.</p>
<p>Querying Details.For context settings, we use the BM25 algorithm to retrieve the two most related contexts from the subtitles of the corresponding class.As illustrated in Figure 3, for the question about the pioneer of mathematical logic, the BM25 algorithm retrieves context about the emergence and development of logic and the concept of mathematical logic.The two contexts will be placed before instruction, along with the questions and options to form the prompt, and fed into LLMs.In non-context settings, the context position will simply be empty.We also test different instructions to make sure that models will follow them to provide answers and explanations.Annotation details.To accurately assess the models' performance on both answers and explanations, we first invited educational experts to filter questions, because a part of questions in the dataset have accompanying pictures, which can't be input into models.Then we asked human annotators to evaluate the answers and explanations generated by the three models respectively, to prevent misjudgment in automatic evaluation tools.These annotators are familiar with MOOC exercises, and provided with the original questions and correct answers, and have access to the internet to determine accurately.</p>
<p>A.2 Cases</p>
<p>In this subsection, we demonstrate some of the error cases we have seen during our experiments, and hope to provide insights into the model's error patterns.The answer is A. Yes.This is because uniform acceleration refers to ... In this case, the change in velocity is aligned with the direction of acceleration.</p>
<p>Table 9: Case 2: the Primacy Effect of GPT-4.For the exactly same questions, switching options causes a direct change in GPT-4's answer, and both answers are accompanied with solid but conflict explanations, which proves the instability of its knowledge.Statistically, models have a tendency to choose more forward contents, which influence their rationale more, just like the Primacy Effect in cognitive psychology.This phenomenon is also observed in GPT-3.5.</p>
<p>Question</p>
<p>The term "logic" originally derives from: A: English, B: French, C: Latin, D: Greek.</p>
<p>Correct Answer D: Greek Model Answer (ChatGPT)</p>
<p>The term "logic" originally derives from the language: C.Latin.The term "logic" originated from the Latin word "logica" which itself was derived from the Greek word "logikÄ“."Therefore, ...</p>
<p>Question</p>
<p>The term "logic" originally derives from: A: English, B: French, C: Greek, D: Latin.</p>
<p>Correct</p>
<p>Answer C: Greek Model Answer (ChatGPT)</p>
<p>The term "logic" originally derives from: C: Greek.The Greek philosopher Aristotle is often credited as one of the founders of formal logic and his work on reasoning and ...</p>
<p>Question</p>
<p>Which language does the term 'logic' originally derives from?</p>
<p>Correct Answer Greek</p>
<p>Model Answer (ChatGPT)</p>
<p>The term "logic" originally derives from the Greek word "logikÄ“," which is derived from "logos," meaning "word," "reason," or "thought."</p>
<p>The study of logic can be traced back to ... Table 10: Case 3: Easier question forms aid performances.We have discussed the effect of different question types.ChatGPT have the correct knowledge about the origin of this word, but it cannot give correct answers, as the first question demonstrates.The following two questions types improve the performance of ChatGPT, by two different ways: moving the correct option forward which is consistent to Case 2, or ease the burden of models to answer by simplifying the question form.This case corroborates the findings that models are better at TF than SC or MC, because there are fewer thinking steps.</p>
<p>Cognitive Dimension: 2 Question Type: TF [Question] Methane has a center of symmetry.[Options] True, False Problem 2 Knowledge Type: 3 Cognitive Dimension: 4 Question Type: SC [Question] The incorrect combination of quantum numbers for outer atomic electrons among the following is: [Options] A: n=3, l=2, m=0, ms=+1/2, B: n=2, l=1, m=-1, ms=-1/2, C: n=4, l=1, m=2, ms=-1/2, D: n=3, l=1, m=-1</p>
<p>Figure 2 :
2
Figure 2: The distributions of accuracy in Bloom's Taxonomy of different models and average of students.</p>
<p>775 (0.771) 0.605 (0.667) 0.578 (0.691) Table5: Accuracy of explanations.TF, SC, and MC are short for the three question types.The numbers in parentheses represent answer accuracy.</p>
<p>â€¦</p>
<p>Figure 3 :
3
Figure 3: Construction of prompts in experiments.</p>
<p>Question"</p>
<p>When undergoing uniform acceleration, the direction of velocity always lies along the same line as the direction of acceleration."Is it correct?The answer is A: No.The reason is that uniform acceleration only requires the acceleration to remain constant, but the direction of velocity and acceleration can be different.For example, in uniform circular motion, ...Question"When undergoing uniform acceleration, the direction of velocity always lies along the same line as the direction of acceleration."Is it correct?</p>
<p>Table 2 :
2
Models and human performance.S.S. and Human.are short for social science and humanity, and context is the result of models with context.Bold figures denote best performance.
Models Total STEMS.S.Human. OthersGPT-3.5 0.436 -context 0.5080.418 0.4680.461 0.5540.483 0.6140.421 0.504ChatGPT 0.506 -context 0.5260.480 0.4410.547 0.6420.533 0.6390.525 0.601GPT-4 0.657 -context 0.6870.613 0.6290.732 0.7650.684 0.7740.690 0.733Human 0.7460.6250.9240.8540.791</p>
<p>Table 3 :
3
Single choice accuracy of three models.First and last indicate the place of the correct answers.Real is when answers and explanations are both correct.Bold figures denote the best performance.
Models Answer ExplanationRealGPT-3.5 -first -last0.405 0.576 0.3620.436 0.554 0.3900.390 0.476 0.296ChatGPT -first -last0.489 0.440 0.4120.501 0.438 0.4300.487 0.438 0.412GPT-4 -first -last0.635 0.727 0.5200.643 0.728 0.5290.632 0.727 0.473</p>
<p>Table 4 :
4
Models' similarity to human, measured by Pearson Correlation Coefficient of knowledge structures.
ModelsGPT-3.5 ChatGPT GPT-4Likeness0.2620.3960.474</p>
<p>Table 6 :
6
Categories of data.Ex., SC, MC, and TF are short for Exercise, Single Choice, Multiple Choice and True or False questions.
Ex.NKnowledge TypesNCognitive DimensionsNSC5968 Factual2020 Remember1715MC1086 Conceptual4032 Understand4066TF1376 Procedural2268 Apply1667total 8430 Meta110 Analyze640// // Evaluate342Types QuestionsOptionsAnswersSCWhich of the following works was created by Vincent van Gogh?A: Les NymphÃ©as B: Sunflowers C: Grande OdalisqueBMCWhich of the following are rare earth elements?A: Uranium B: Lutecium D: Samarium C: DysprosiumB, C, DTFLight only exhibits the properties of waves.True FalseFalseTable 7: Question types examples: single choice (SC), multiple choice (MC), and true or false (TF). SC have only one correct option, while MC have 2 or more than 2 correct options. TF should be determined as True or False.</p>
<p>QuestionIf all birds can fly, then ostriches can fly.Ostriches are birds.Therefore, ostriches can fly.Does this reasoning contain any formal errors?
Correct AnswerNo.Model Answer (ChatGPT)Yes. ... While the premises state that every bird is capable of flying, it does not mean that every individual bird, including the ostrich, is capable of flying. Ostriches are actually flightless birds.QuestionIf all birds can fly, then sparrows can fly. Sparrows are birds. Therefore, sparrows can fly. Does this reasoning contain any formal errors?Correct AnswerNo.Model Answer (ChatGPT)No. The reasoning is an example of a valid deductive argument ... from a purely logical standpoint, the argument itself is valid.</p>
<p>Table 8 :
8
Case 1: Changing the entities infects the answer.If the question is consistent with its knowledge, ChatGPT indeed follows the task of focusing on the formal errors or purely logic.Otherwise, it fails to follow the question, as it refuses to override its own knowledge.This phenomenon indicates ChatGPT might be less robust when there are knowledge conflicts.</p>
<p>AcknowledgementsThis work is supported by a grant from the Institute for Guo Tsinghua University (2019GQB0003), and also supported by Tsinghua University Initiative Scientific Research Program.
A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of educational objectives. W Lorin, David R Anderson, Krathwohl, 2001Longman</p>
<p>Educational diagnostic assessment. I Isaac, Bejar, Journal of Educational Measurement. 2121984</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, ArXiv preprint, abs/2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>On the measure of intelligence. FranÃ§ois Chollet, ArXiv preprint. 2019. 1911.01547</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, ArXiv preprint, abs/2204.023112022</p>
<p>Serial effects in recall of unorganized and sequentially organized verbal material. James Deese, Roger A Kaufman, Journal of experimental psychology. 5431801957</p>
<p>The assessment of knowledge, in theory and in practice. Jean-Claude Falmagne, Eric Cosyn, Jean-Paul Doignon, Nicolas ThiÃ©ry, Formal Concept Analysis: 4th International Conference, ICFCA 2006. Dresden, GermanySpringer2006. February 13-17, 2006Proceedings</p>
<p>Bigbench v2: the new and improved bigbench. Ahmad Ghazal, Todor Ivanov, Pekka Kostamaa, Alain Crolotte, Ryan Voong, Mohammed Al-Kateb, Waleed Ghazal, Roberto V Zicari, Proc. of ICDE. of ICDEIEEE2017</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proc. of ICLR. OpenReview.net. of ICLR. OpenReview.net2021</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, </p>
<p>Jacqueline Leighton, Mark Gierl, Cognitive diagnostic assessment for education: Theory and applications. Cambridge University Press2007</p>
<p>Benchmarking large language models on cmexam -a comprehensive chinese medical exam dataset. Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, Michael Lingzhi, Li , 2023</p>
<p>Human-level artificial intelligence? be serious! AI magazine. Nils J Nilsson, 200526</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Mind meets machine: Unravelling gpt-4's cognitive psychology. Manmeet Singh, S B Vaisakh, Neetiraj Malviya, abs/2303.11436ArXiv preprint. 2023</p>
<p>A data collection on secondary school students' stem performance and reading practices in an emerging country. Quan-Hoang Vuong, Viet-Phuong La, Manh-Toan Ho, Thanh-Hang Pham, Thu-Trang Vuong, Ha-My Vuong, Minh-Hoang Nguyen, Data Intelligence. 322021</p>
<ol>
<li>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, Proc. of ICLR. OpenReview.net. M Dai, V Quoc, Le, of ICLR. OpenReview.net</li>
</ol>
<p>Moocradar: A fine-grained and multi-aspect knowledge repository for improving cognitive student modeling in moocs. Jifan Yu, Mengying Lu, Qingyang Zhong, Zijun Yao, Shangqing Tu, Zhengshan Liao, Xiaoya Li, Manli Li, Lei Hou, Hai-Tao Zheng, ArXiv preprint, abs/2304.022052023</p>
<p>Mooccubex: A large knowledge-centered repository for adaptive learning in moocs. Jifan Yu, Yuquan Wang, Qingyang Zhong, Gan Luo, Yiming Mao, Kai Sun, Wenzheng Feng, Wei Xu, Shulin Cao, Kaisheng Zeng, Zijun Yao, Lei Hou, Yankai Lin, Peng Li, Jie Zhou, Bin Xu, Juanzi Li, Jie Tang, Maosong Sun, 10.1145/3459637.3482010Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management, CIKM '21. the 30th ACM International Conference on Information &amp; Knowledge Management, CIKM '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Ken Yew, Lidong Chia, Bing, 2023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9166 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9166</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9166</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-259937644</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.08678v1.pdf" target="_blank">Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers"yes"to the input question"Can eagles fly?"with the explanation"all birds can fly", then humans would infer from the explanation that it would also answer"yes"to the counterfactual input"Can penguins fly?". If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9166.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9166.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model used in this paper both to generate counterfactual inputs and to act as an automatic "simulator" that predicts a target model's outputs on counterfactuals (i.e., to proxy human simulation judgments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 (name reported in paper). Used as both a counterfactual generator and as an automatic simulator to predict model outputs on generated counterfactuals; paper does not report model parameter count or training corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Natural language processing — multi-hop factual reasoning (StrategyQA) and reward modeling / human preference modeling (SHP)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Two text-based simulation uses: (1) generate diverse, explanation-relevant counterfactual natural-language inputs given a model's explanation; (2) act as a simulator that, given (explanation, original input, original model output), predicts the model's output on each counterfactual (automated human-simulator approximation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>For simulation-as-prediction: inter-annotator agreement (Cohen's kappa) with human simulators (reported in Table 3) and simulation precision (fraction of simulatable counterfactuals where humans' inferred outputs match the model's actual outputs). For counterfactual generation diversity: BLEU, cosine (sentence-embedding) similarity, Jaccard; also a 'Sim.%' diversity measure (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>As an approximator of human simulators: Table 3 reports normalized IAA values (Cohen's kappa based numbers reported in paper): StrategyQA — H-H: 0.504, H-GPT-4: 0.486; SHP — H-H: 0.265, H-GPT-4: 0.296. Simulation precision (humans simulating model outputs on generated counterfactuals, aggregated across explanation systems): StrategyQA overall simulation precision 79.8%; SHP overall simulation precision 89.0%. GPT-4 explanations were reported to be more precise than GPT-3 by +5.5 points on StrategyQA and +6.5 points on SHP (p < 0.002).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model scale/architecture (GPT-4 > GPT-3 in agreement with humans); task/subdomain (SHP had higher simulation precision than StrategyQA despite lower task accuracy); explanation method (CoT vs Post-Hoc showed little or inconsistent effect); counterfactual generation method (LLM prompting vs lexical baseline PolyJuice affects diversity of counterfactuals); human-vs-model commonsense gaps (differences in background knowledge affect whether humans' entailment judgments match model behavior); coverage of the counterfactual distribution (LLMs may not generate all relevant simulatable counterfactuals); subjectivity and annotator agreement (moderate/fair IAA limits measurement); explanation generality vs precision are not correlated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baselines: human simulators (gold), GPT-3.5 (called GPT-3 in paper) as an automatic simulator, and PolyJuice for counterfactual generation. GPT-4's agreement with humans is close to human-human agreement (Table 3) and substantially better than GPT-3's agreement; GPT-4 was chosen as the simulator proxy for SHP because its agreement with humans was similar or higher than humans' agreement with each other on that dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: LLM simulators are only approximations (subject to mismatches in commonsense knowledge); human annotation IAA was moderate/fair (noise); the counterfactual distribution depends on the LLM generator (coverage not guaranteed); explanations often have low simulation precision (authors report ~80% for binary settings and find explanations mislead humans), precision does not correlate with plausibility, and generality does not predict precision. Evaluation focused on classification tasks (not open-ended generation) and only English.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>GPT-4 can be an effective proxy for human simulators in automated evaluation pipelines (especially for SHP); mix GPT-3 and GPT-4 outputs to increase counterfactual diversity; both simulation precision and generality should be optimized because they are uncorrelated; naive optimization for human-pleasing explanations (e.g., RLHF) may not improve counterfactual simulatability; potential improvement paths include self-training or reinforcement learning that directly optimizes simulatability scores computed by LLM simulators, interactive clarification/dialogue to build better mental models, and extending evaluation to generation tasks with contrastive setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9166.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9166.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (175B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier large language model (reported as GPT-3.5, 175B in the paper) used both to generate counterfactuals and as an automatic simulator to approximate humans; included for comparison against GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (denoted GPT-3 in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 (reported parameter count 175B in paper). Used as both a counterfactual generator and as an automatic simulator; served as a baseline in comparisons with GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Natural language processing — multi-hop factual reasoning (StrategyQA) and reward modeling / human preference modeling (SHP)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same dual roles as GPT-4 in the paper: (1) generate explanation-relevant counterfactuals; (2) act as an automatic simulator predicting model outputs on counterfactuals (proxy for human simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Agreement with humans measured by Cohen's kappa (Table 3), and contribution to counterfactual diversity measured by BLEU / Cosine / Jaccard / Sim.% metrics (Table 1). Simulation precision comparisons (explanations produced by GPT-3.5 vs GPT-4) reported in Table 4 and main text.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Counterfactual-generation diversity (Table 1, StrategyQA): GPT-3 BLEU 69.6, Cosine 24.6, Jaccard 61.0, Sim.% 62.7. As simulator agreement with humans (Table 3): StrategyQA H-GPT-3: 0.339 (vs human-human 0.504); SHP H-GPT-3: 0.058 (vs human-human 0.265). Overall, GPT-3 explanations were less precise than GPT-4 by 5.5 points on StrategyQA and 6.5 points on SHP.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Smaller model scale (relative to GPT-4) correlated with worse agreement with humans; counterfactual diversity generation quality; explanation method (CoT vs Post-Hoc had minimal/variable effect); dataset/task differences; human annotation noise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-4 and human simulators; GPT-3 underperforms GPT-4 on simulator agreement and explanation precision. Compared to PolyJuice, GPT-3 generated more diverse counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower agreement with humans (noted especially on SHP), leading authors to prefer GPT-4 as the automatic simulator for that dataset; explanations from GPT-3 were less precise and more likely to mislead human mental models.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Mixing outputs from multiple LLMs (GPT-3.5 + GPT-4) improves counterfactual diversity; GPT-3.5 may be useful as part of a mixed generator but is less reliable as a simulator proxy than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9166.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9166.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM counterfactual generator (GPT-3/GPT-4 mix)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based counterfactual generator using GPT-3.5 and GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based pipeline using LLMs (GPT-3.5 and GPT-4, and mixtures thereof) to generate diverse counterfactual natural-language inputs that are relevant to a model's explanation; used as the source distribution of counterfactuals for the simulatability evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4 mix (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompted generation with GPT-3.5 and GPT-4 to produce multiple candidate counterfactual questions per explanation; a mixed output strategy (concatenating or combining candidates from both models) was evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Natural language processing — evaluation methodology for explanations in multi-hop QA and reward modeling</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate n counterfactuals per explanation (10 per explanation for StrategyQA, 6 for SHP), then filter to simulatable ones (based on human simulators) to form C* for computing simulation generality and precision.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Counterfactual diversity and simulatable fraction measured by BLEU, cosine embedding similarity, Jaccard similarity, and a Sim.% diversity metric (Table 1); downstream impact measured by simulation precision and generality computed over the generated counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Diversity results (Table 1): StrategyQA — GPT-3: BLEU 69.6 / Cos 24.6 / Jacc 61.0 / Sim.% 62.7; GPT-4: BLEU 67.0 / Cos 25.3 / Jacc 58.9 / Sim.% 56.1; GPT-mix: BLEU 72.9 / Cos 29.6 / Jacc 66.2 / Sim.% 58.7; PolyJuice (baseline ignoring explanations): BLEU 43.6 / Cos 15.1 / Jacc 33.6 / Sim.% 55.9. SHP — GPT-mix: BLEU 93.0 / Cos 65.3 / Jacc 90.0 / Sim.% 78.5. Authors report GPT-based prompting outperforms PolyJuice (p < 0.001) and mixing GPT-3 and GPT-4 increases diversity further (p < 0.002).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt design (the paper provides prompts in Appendix B) strongly affects generated counterfactual quality; mixing outputs from different LLMs increased diversity; PolyJuice (lexical/semantic perturbation baseline) produced substantially lower diversity; number of candidates generated per example; filtering by human simulability reduces noise; domain/task affects ease of producing diverse simulatable counterfactuals (SHP had higher Sim.%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against PolyJuice (Wu et al., 2021) which ignores explanations; GPT-based generation produced higher diversity by substantial margins (e.g., relative 68% improvement over PolyJuice on StrategyQA averaged across three metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLM-generated counterfactuals may not cover the full space of relevant simulatable counterfactuals; generator quality depends on prompts and the LLM used; some generated counterfactuals are unsimulatable and must be filtered; results dependent on the chosen similarity metrics and filtering heuristics; evaluation limited to two datasets/tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use LLM prompting rather than simple perturbation baselines to produce richer counterfactuals; mix outputs from multiple LLMs to increase diversity; filter out unsimulatable counterfactuals via human or automated simulation judgments before computing metrics; allow practitioners to define the desired counterfactual distribution for their application.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Training a helpful and harmless assistant with reinforcement learning from human feedback <em>(Rating: 1)</em></li>
                <li>Evaluating explainable AI: Which algorithmic explanations help users predict model behavior? <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9166",
    "paper_id": "paper-259937644",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4 (simulator)",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A state-of-the-art large language model used in this paper both to generate counterfactual inputs and to act as an automatic \"simulator\" that predicts a target model's outputs on counterfactuals (i.e., to proxy human simulation judgments).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4 (name reported in paper). Used as both a counterfactual generator and as an automatic simulator to predict model outputs on generated counterfactuals; paper does not report model parameter count or training corpus.",
            "scientific_subdomain": "Natural language processing — multi-hop factual reasoning (StrategyQA) and reward modeling / human preference modeling (SHP)",
            "simulation_task": "Two text-based simulation uses: (1) generate diverse, explanation-relevant counterfactual natural-language inputs given a model's explanation; (2) act as a simulator that, given (explanation, original input, original model output), predicts the model's output on each counterfactual (automated human-simulator approximation).",
            "evaluation_metric": "For simulation-as-prediction: inter-annotator agreement (Cohen's kappa) with human simulators (reported in Table 3) and simulation precision (fraction of simulatable counterfactuals where humans' inferred outputs match the model's actual outputs). For counterfactual generation diversity: BLEU, cosine (sentence-embedding) similarity, Jaccard; also a 'Sim.%' diversity measure (see Table 1).",
            "simulation_accuracy": "As an approximator of human simulators: Table 3 reports normalized IAA values (Cohen's kappa based numbers reported in paper): StrategyQA — H-H: 0.504, H-GPT-4: 0.486; SHP — H-H: 0.265, H-GPT-4: 0.296. Simulation precision (humans simulating model outputs on generated counterfactuals, aggregated across explanation systems): StrategyQA overall simulation precision 79.8%; SHP overall simulation precision 89.0%. GPT-4 explanations were reported to be more precise than GPT-3 by +5.5 points on StrategyQA and +6.5 points on SHP (p &lt; 0.002).",
            "factors_affecting_accuracy": "Model scale/architecture (GPT-4 &gt; GPT-3 in agreement with humans); task/subdomain (SHP had higher simulation precision than StrategyQA despite lower task accuracy); explanation method (CoT vs Post-Hoc showed little or inconsistent effect); counterfactual generation method (LLM prompting vs lexical baseline PolyJuice affects diversity of counterfactuals); human-vs-model commonsense gaps (differences in background knowledge affect whether humans' entailment judgments match model behavior); coverage of the counterfactual distribution (LLMs may not generate all relevant simulatable counterfactuals); subjectivity and annotator agreement (moderate/fair IAA limits measurement); explanation generality vs precision are not correlated.",
            "comparison_baseline": "Baselines: human simulators (gold), GPT-3.5 (called GPT-3 in paper) as an automatic simulator, and PolyJuice for counterfactual generation. GPT-4's agreement with humans is close to human-human agreement (Table 3) and substantially better than GPT-3's agreement; GPT-4 was chosen as the simulator proxy for SHP because its agreement with humans was similar or higher than humans' agreement with each other on that dataset.",
            "limitations_or_failure_cases": "Reported limitations include: LLM simulators are only approximations (subject to mismatches in commonsense knowledge); human annotation IAA was moderate/fair (noise); the counterfactual distribution depends on the LLM generator (coverage not guaranteed); explanations often have low simulation precision (authors report ~80% for binary settings and find explanations mislead humans), precision does not correlate with plausibility, and generality does not predict precision. Evaluation focused on classification tasks (not open-ended generation) and only English.",
            "author_recommendations_or_insights": "GPT-4 can be an effective proxy for human simulators in automated evaluation pipelines (especially for SHP); mix GPT-3 and GPT-4 outputs to increase counterfactual diversity; both simulation precision and generality should be optimized because they are uncorrelated; naive optimization for human-pleasing explanations (e.g., RLHF) may not improve counterfactual simulatability; potential improvement paths include self-training or reinforcement learning that directly optimizes simulatability scores computed by LLM simulators, interactive clarification/dialogue to build better mental models, and extending evaluation to generation tasks with contrastive setups.",
            "uuid": "e9166.0",
            "source_info": {
                "paper_title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "GPT-3.5 (simulator)",
            "name_full": "GPT-3.5 (175B)",
            "brief_description": "An earlier large language model (reported as GPT-3.5, 175B in the paper) used both to generate counterfactuals and as an automatic simulator to approximate humans; included for comparison against GPT-4.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (denoted GPT-3 in paper)",
            "model_description": "GPT-3.5 (reported parameter count 175B in paper). Used as both a counterfactual generator and as an automatic simulator; served as a baseline in comparisons with GPT-4.",
            "scientific_subdomain": "Natural language processing — multi-hop factual reasoning (StrategyQA) and reward modeling / human preference modeling (SHP)",
            "simulation_task": "Same dual roles as GPT-4 in the paper: (1) generate explanation-relevant counterfactuals; (2) act as an automatic simulator predicting model outputs on counterfactuals (proxy for human simulation).",
            "evaluation_metric": "Agreement with humans measured by Cohen's kappa (Table 3), and contribution to counterfactual diversity measured by BLEU / Cosine / Jaccard / Sim.% metrics (Table 1). Simulation precision comparisons (explanations produced by GPT-3.5 vs GPT-4) reported in Table 4 and main text.",
            "simulation_accuracy": "Counterfactual-generation diversity (Table 1, StrategyQA): GPT-3 BLEU 69.6, Cosine 24.6, Jaccard 61.0, Sim.% 62.7. As simulator agreement with humans (Table 3): StrategyQA H-GPT-3: 0.339 (vs human-human 0.504); SHP H-GPT-3: 0.058 (vs human-human 0.265). Overall, GPT-3 explanations were less precise than GPT-4 by 5.5 points on StrategyQA and 6.5 points on SHP.",
            "factors_affecting_accuracy": "Smaller model scale (relative to GPT-4) correlated with worse agreement with humans; counterfactual diversity generation quality; explanation method (CoT vs Post-Hoc had minimal/variable effect); dataset/task differences; human annotation noise.",
            "comparison_baseline": "Compared to GPT-4 and human simulators; GPT-3 underperforms GPT-4 on simulator agreement and explanation precision. Compared to PolyJuice, GPT-3 generated more diverse counterfactuals.",
            "limitations_or_failure_cases": "Lower agreement with humans (noted especially on SHP), leading authors to prefer GPT-4 as the automatic simulator for that dataset; explanations from GPT-3 were less precise and more likely to mislead human mental models.",
            "author_recommendations_or_insights": "Mixing outputs from multiple LLMs (GPT-3.5 + GPT-4) improves counterfactual diversity; GPT-3.5 may be useful as part of a mixed generator but is less reliable as a simulator proxy than GPT-4.",
            "uuid": "e9166.1",
            "source_info": {
                "paper_title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "LLM counterfactual generator (GPT-3/GPT-4 mix)",
            "name_full": "LLM-based counterfactual generator using GPT-3.5 and GPT-4",
            "brief_description": "A prompt-based pipeline using LLMs (GPT-3.5 and GPT-4, and mixtures thereof) to generate diverse counterfactual natural-language inputs that are relevant to a model's explanation; used as the source distribution of counterfactuals for the simulatability evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 / GPT-4 mix (prompted)",
            "model_description": "Prompted generation with GPT-3.5 and GPT-4 to produce multiple candidate counterfactual questions per explanation; a mixed output strategy (concatenating or combining candidates from both models) was evaluated.",
            "scientific_subdomain": "Natural language processing — evaluation methodology for explanations in multi-hop QA and reward modeling",
            "simulation_task": "Generate n counterfactuals per explanation (10 per explanation for StrategyQA, 6 for SHP), then filter to simulatable ones (based on human simulators) to form C* for computing simulation generality and precision.",
            "evaluation_metric": "Counterfactual diversity and simulatable fraction measured by BLEU, cosine embedding similarity, Jaccard similarity, and a Sim.% diversity metric (Table 1); downstream impact measured by simulation precision and generality computed over the generated counterfactuals.",
            "simulation_accuracy": "Diversity results (Table 1): StrategyQA — GPT-3: BLEU 69.6 / Cos 24.6 / Jacc 61.0 / Sim.% 62.7; GPT-4: BLEU 67.0 / Cos 25.3 / Jacc 58.9 / Sim.% 56.1; GPT-mix: BLEU 72.9 / Cos 29.6 / Jacc 66.2 / Sim.% 58.7; PolyJuice (baseline ignoring explanations): BLEU 43.6 / Cos 15.1 / Jacc 33.6 / Sim.% 55.9. SHP — GPT-mix: BLEU 93.0 / Cos 65.3 / Jacc 90.0 / Sim.% 78.5. Authors report GPT-based prompting outperforms PolyJuice (p &lt; 0.001) and mixing GPT-3 and GPT-4 increases diversity further (p &lt; 0.002).",
            "factors_affecting_accuracy": "Prompt design (the paper provides prompts in Appendix B) strongly affects generated counterfactual quality; mixing outputs from different LLMs increased diversity; PolyJuice (lexical/semantic perturbation baseline) produced substantially lower diversity; number of candidates generated per example; filtering by human simulability reduces noise; domain/task affects ease of producing diverse simulatable counterfactuals (SHP had higher Sim.%).",
            "comparison_baseline": "Compared against PolyJuice (Wu et al., 2021) which ignores explanations; GPT-based generation produced higher diversity by substantial margins (e.g., relative 68% improvement over PolyJuice on StrategyQA averaged across three metrics).",
            "limitations_or_failure_cases": "LLM-generated counterfactuals may not cover the full space of relevant simulatable counterfactuals; generator quality depends on prompts and the LLM used; some generated counterfactuals are unsimulatable and must be filtered; results dependent on the chosen similarity metrics and filtering heuristics; evaluation limited to two datasets/tasks.",
            "author_recommendations_or_insights": "Use LLM prompting rather than simple perturbation baselines to produce richer counterfactuals; mix outputs from multiple LLMs to increase diversity; filter out unsimulatable counterfactuals via human or automated simulation judgments before computing metrics; allow practitioners to define the desired counterfactual distribution for their application.",
            "uuid": "e9166.2",
            "source_info": {
                "paper_title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
            "rating": 2,
            "sanitized_title": "polyjuice_generating_counterfactuals_for_explaining_evaluating_and_improving_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "rating": 1,
            "sanitized_title": "training_a_helpful_and_harmless_assistant_with_reinforcement_learning_from_human_feedback"
        },
        {
            "paper_title": "Evaluating explainable AI: Which algorithmic explanations help users predict model behavior?",
            "rating": 1,
            "sanitized_title": "evaluating_explainable_ai_which_algorithmic_explanations_help_users_predict_model_behavior"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        }
    ],
    "cost": 0.01686725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations</p>
<p>Yanda Chen yanda.chen@cs.columbia.edu 
Columbia University</p>
<p>Ruiqi Zhong 
Berkeley</p>
<p>Narutatsu Ri 
Columbia University</p>
<p>Chen Zhao 
New York University</p>
<p>He He 
New York University</p>
<p>Jacob Steinhardt 
Berkeley</p>
<p>Zhou Yu 
Columbia University</p>
<p>Kathleen Mckeown 
Columbia University</p>
<p>Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations</p>
<p>Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate counterfactual simulatability of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers "yes" to the input question "Can eagles fly?" with the explanation "all birds can fly", then humans would infer from the explanation that it would also answer "yes" to the counterfactual input "Can penguins fly?". If the explanation is precise, then the model's answer should match humans' expectations.We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.</p>
<p>Introduction</p>
<p>An ideal explanation should enable humans to infer how a model processes different inputs (Johnson-Laird, 1980;Collins and Gentner, 1987;Garnham, 1987;Bansal et al., 2019). For example, when we ask GPT-4 (OpenAI, 2023) "Is it hard to get a BLT in Casablanca?", it answers "yes" and explains "Casablanca is a large city in Morocco. Morocco is a Muslim-majority country, and pork is not commonly consumed due Is it hard to get a BLT in Casablanca?</p>
<p>Explanation: Casablanca is a large city in Morocco. Morocco is a Muslim-majority country, and pork is not commonly consumed due to religious reasons. BLT contains bacon, which is pork. Thus, it might be hard to find a traditional BLT in Casablanca.</p>
<p>AI answers a user's question with an explanation</p>
<p>Answer: Yes</p>
<p>The user forms an expectation of how AI would answer related questions</p>
<p>How AI actually answers related questions</p>
<p>Is it hard to find pork belly in Casablanca?</p>
<p>Answer: Yes</p>
<p>Is it hard to find pork belly in Casablanca?</p>
<p>Answer: No</p>
<p>The user is misled by the explanation and forms a wrong mental model of the AI. to religious reasons. BLT contains bacon, which is pork. Thus, it might be hard to find a traditional BLT in Casablanca."</p>
<p>Such an explanation is logically coherent and provides factually correct background information helpful for the question (Joshi et al., 2023). 1 However, does it help humans correctly infer how GPT-4 answers other related questions? Based on the explanation, humans will infer that GPT-4 encodes the knowledge that "pork is not commonly consumed in Muslin countries" and will apply similar reasoning to relevant questions (counterfactuals), e.g., answering "Yes" to "Is it hard to find pork belly in Casablanca?" Unfortunately, GPT-4 actually answers "No" to this counterfactual, contradict-ing its own explanation and humans' expectations.</p>
<p>The above explanation is problematic because humans form a wrong mental model of GPT-4 (i.e., incorrectly infer how GPT-4 answers relevant counterfactuals) based on this explanation. Building a correct mental model of an AI system is important, as it helps humans understand what an AI system can and cannot achieve (Chandrasekaran et al., 2018), which informs humans how to improve the system or appropriately deploy the system without misuse or overtrust (Cassidy, 2009;Bansal et al., 2019;Ye and Durrett, 2022).</p>
<p>We propose to evaluate the counterfactual simulatability of natural language explanations to measure their ability to help humans build mental models of an AI model. A good mental model should generalize to diverse unseen inputs and precisely infer the model's outputs, so we propose two metrics accordingly for explanations ( Figure 2). The first, simulation generality, measures the generality of an explanation by tracking the diversity of the counterfactuals relevant to the explanation (e.g., "Humans do not consume meat" has more diverse relevant counterfactuals compared to "Muslims do not consume pork" and is thus more general). The second, simulation precision, tracks the fraction of counterfactuals where humans' inference matches the model's output.</p>
<p>To evaluate the counterfactual simulatability of an explanation on an input question (e.g., the initial question on BLT), we need to (1) collect a set of counterfactuals on an input based on the explanation, and (2) let humans simulate (infer) what the model outputs on the counterfactuals. For (1), since it is expensive to ask humans to write the counterfactuals, we propose to prompt LLMs to generate diverse counterfactuals relevant to an explanation (e.g., related questions on pork belly or pepperoni in Figure 2). For (2), since human simulation might be subjective, we reduce subjectivity by framing the simulation task as a logical entailment task (Section 4.4). Finally, we calculate generality and precision based on the LM-generated counterfactuals and humans' entailment annotations.</p>
<p>We benchmark the counterfactual simulatability of two LLMs-GPT-3.5 and GPT-4, and two explanation methods-CoT (Chain of Thought) and Post-Hoc (explain after the output), on two tasks-multi-hop factual reasoning (StrategyQA, Geva et al. (2021)) and reward modeling (Stanford Human Preference, Ethayarajh et al. (2022)). We found that (i) Both LLMs' explanations have low precision (80% for binary classification); (ii) CoT does not substantially outperform Post-Hoc.</p>
<p>We also study how counterfactual simulatability relates to plausibility, which evaluates humans' preference of an explanation based on its factual correctness and logical coherence. We found that precision does not correlate with plausibility, and hence naively optimizing human approvals (e.g., RLHF) might not fix the issue of low precision.</p>
<p>To summarize, our paper • proposes to evaluate counterfactual simulatability: whether an explanation can help humans build mental models. • implements two metrics based on counterfactual simulatability: precision and generality. • reveals that explanations generated by stateof-the-art LLMs are not precise and current approaches might be insufficient.</p>
<p>Related Work</p>
<p>Applications of Mental Models. Humans can use a model's explanations to build mental models of how the model behaves on various inputs (Johnson-Laird, 1980;Collins and Gentner, 1987;Garnham, 1987;Gentner and Stevens, 2014;Bansal et al., 2019). Building mental models reveals a model's capacity and limitations so that users know when and how to use the model without misuse and overtrust, especially in high-stakes domains such as healthcare (Adadi and Berrada, 2020;Merry et al., 2021;Babic et al., 2021), legal (Deeks, 2019;Norkute et al., 2021), and law enforcement (Matulionyte and Hanif, 2021;Hall et al., 2022). Building mental models also detects if the model biases against specific groups of people (Vig et al., 2020;Ravfogel et al., 2020) or encourages illegal behaviors against human values (Hendrycks et al., 2021;Bai et al., 2022). As modern AI models get stronger performance on more tasks, humans can learn difficult tasks by forming mental models of AI models (Mac Aodha et al., 2018;Goyal et al., 2019).</p>
<p>Evaluation Metrics for Explanations. We summarize three existing popular metrics for explanations: plausibility, faithfulness, and simulatability. Plausibility evaluates humans' preference of an explanation based on its factual correctness and logical coherence (Herman, 2017;Lage et al., 2019;Jacovi and Goldberg, 2020). It is different from faithfulness, which measures whether an explanation is consistent with the model's own de-cision process (Harrington et al., 1985;Ribeiro et al., 2016;Gilpin et al., 2018;Wu and Mooney, 2019;Lakkaraju et al., 2019;Jacovi and Goldberg, 2020). In prior work, faithfulness is usually evaluated by whether it is possible to train a black-box model to predict the model's outputs based on its explanations (Li et al., 2020;Kumar and Talukdar, 2020;Lyu et al., 2022). Simulatability measures how well humans can predict the model's outputs based on its explanations (Doshi-Velez and Kim, 2017;Ribeiro et al., 2018;Chandrasekaran et al., 2018;Hase and Bansal, 2020); in particular, simulatability is a special case of faithfulness, which requires the output predictor to be humans rather than arbitrary black-box models. Consequently, a faithful explanation is not necessarily simulatable.</p>
<p>For example, raw model weights in matrix forms have perfect faithfulness by definition (using the model itself as the output predictor), but hardly simulatable (because humans cannot interpret model weights easily). We focus on simulatability instead of faithfulness because explanations need to be consumed by humans to form mental models.</p>
<p>Generalizable Explanations. In prior work that evaluates the simulatability of a natural language explanation, the simulation input is the explained input (Hase et al., 2020;Narang et al., 2020;Wiegreffe et al., 2021;Chen et al., 2022;Chan et al., 2022). This leads to two problems: (i) the explanation might already contain (leak) the model's output on the simulation input so the metric is not well-defined (Hase et al., 2020), (ii) it is inefficient and tedious for humans to read the model's explanation on every input to understand the model's behavior. In comparison, counterfactual simulatability measures whether humans can infer from an explanation the model's outputs on diverse counterfactuals different from the explained input, and thus requires the explanation to be generalizable. While the concept of counterfactual simulatability has a long history (Doshi-Velez and Kim, 2017;Hase and Bansal, 2020;Sia et al., 2022), we are the first work to evaluate the counterfactual simulatability of free-form natural language explanations.</p>
<p>Counterfactual Simulatability</p>
<p>For a given task, a model M takes an input x ∈ X and produces an output o An ideal explanation e x should be generalizable-besides revealing how the model reasons on x, it should also reveal how the model reasons on unseen inputs x = x. Explanations also need to be precise-they should lead to mental models that are consistent with the model's behavior.</p>
<p>Motivated by these two desiderata, we propose to measure counterfactual simulatability with two metrics: simulation generality and simulation precision. We introduce them below.</p>
<p>Simulation Generality</p>
<p>Conceptually, we want simulation generality to measure how diverse the simulatable counterfactuals are, so we measure it as one minus the average similarity between two simulatable counterfactuals
generality = 1 − E x ,x ∼p [α(x , x )],
where p is the distribution of simulatable counterfactuals and α is a similarity metric. To actually define simulation generality we need to specify p and α. For p, to evaluate an explanation e x on an input x, we first prompt LLMs to generate n counterfactuals of x that are likely simulatable from e x , denoted as C = {x 1 , · · · , x n }. We then filter out the unsimulatable counterfactuals and get the simulatable subset C * = {x ∈ C, h ex (x ) =⊥}. So the expectation becomes
1 − 1 |C * |(|C * | − 1) x ,x ∈C * ,x =x α(x , x ).
See Figure 2 top for a concrete example.</p>
<p>For α we consider three possibilities: et al., 2002) 2. Cosine: We embed x and x separately with a sentence encoder Enc and calculate their cosine similarity:
1. BLEU: α(x , x ) = BLEU(x , x ). (Papineni
α(x , x ) = cos(Enc(x ), Enc(x )).</p>
<p>Jaccard:</p>
<p>We tokenize x and x separately into two bags (sets) of words bow(x ) and</p>
<p>bow(x ), and remove stopwords. We then calculate the Jaccard similarity between them:
α(x , x ) = |bow(x ) ∩ bow(x )| |bow(x ) ∪ bow(x )| .</p>
<p>Simulation Precision</p>
<p>We measure simulation precision as the fraction of simulatable counterfactuals where humans' simulation matches the model's actual output:
precision = 1 |C * | x ∈C * 1[h ex (x ) = o x ].</p>
<p>Implementing Human Simulation h ex (x )</p>
<p>In the definitions of generality and precision, we relied on the human simulation h ex (x ), so the remaining task is to implement this function. There are several challenges to this, which we describe and address below.</p>
<p>Human simulation can be highly subjective. Different human annotators may use different reasoning to infer what the model would output. Consider the following example in StrategyQA. For the input question "Would a monkey outlive a human being on average?", the model explains "The average lifespan of a monkey is about 20 years. The average lifespan of a human being is about 80 years. Thus, a monkey would not outlive a human being on average." Given the counterfactual "Can turtles outlive sharks?", some annotators think that it is simulatable because the explanation indicates that questions of the form "Can A outlive B?" can be answered by comparing the lifespans of A and B, while others think that this counterfactual is not simulatable because the explanation does not mention the lifespan of turtles or sharks. Thus, we need to formulate human simulation as a well-defined task to reduce annotation noise.</p>
<p>Solution.</p>
<p>We propose to formulate human simulation as a logical entailment task to reduce subjectivity. We instruct annotators to simulate a model's output on x by judging if (e x , o x , x) entails an output to counterfactual x . We allow humans to use commonsense reasoning when judging entailment, e.g., the explanation "Omnivores can use chopsticks" entails the output "yes" to "Can pigs use chopsticks?" because pigs are omnivores.</p>
<p>If the explanation does not entail any output, then this counterfactual is unsimulatable. For example, if the explanation is "Omnivores can eat meat", then the question "Can pigs use chopsticks?" is unsimulatable because the explanation is irrelevant.</p>
<p>Humans and models have different commonsense knowledge. When a human uses commonsense knowledge to generalize mental models, it may differ from a model's generalization if they have different commonsense knowledge. For example, if a model "thinks" that pigs are not omnivores (different from humans' knowledge), then it may answer "no" to "Can pigs use chopsticks?" while being perfectly consistent with its explanation "Omnivores can use chopsticks." Should humans use their own knowledge or the model's knowledge when they generalize their mental models and judge entailment? Solution. We argue that humans should use human knowledge when judging entailment and generalizing mental models, because probing the model's knowledge for each counterfactual is timeconsuming and difficult, Note that humans should stick to the model's explanation whenever relevant (because the goal is to simulate the model's behavior), and only use humans' knowledge for information missing in the explanation.</p>
<p>Human simulation is expensive and laborious. Evaluating the counterfactual simulatability of one explanation requires humans to annotate multiple counterfactuals (Section 3.1), which is expensive. Solution. To facilitate automatic evaluation, we also experiment with approximating human simulators with LLMs. Similar to human simulators, LLMs take as input a model's explanation e x and output o x on input x, and infer the model's output on each counterfactual x . We show the prompts we use in Appendix B. Note that even though the simulation process is now automated, unlike faithfulness evaluation, the gold simulators are still humans following the two rules above (judging simulation as entailment with human's commonsense).</p>
<p>Final Solution Combining the solutions to the two challenges above, we instruct the annotators to simulate a model's output on x by judging if (e x , o x , x) entails an output to counterfactual x , stick to the model's explanation whenever relevant, but use human knowledge for information missing in the explanation. We present details of our human evaluation in Section 4.4. We evaluate the LLM simulators based on its agreement with human simulators (Section 5.1 Table 3).</p>
<p>Experiment Setup</p>
<p>We introduce the datasets we use (Section 4.1), the explanation systems we evaluate (Section 4.2), and additional details for counterfactual generation (Section 4.3) and human simulation (Section 4.4).</p>
<p>Datasets</p>
<p>We evaluate explanations on multi-hop reasoning (StrategyQA) and reward modeling (Stanford Human Preference).</p>
<p>StrategyQA is a multi-hop question-answering dataset on open-domain questions (Geva et al., 2021). The answer to each question is either "yes" or "no". Answering questions in StrategyQA requires implicit step-by-step reasoning, which makes explanations useful.</p>
<p>Stanford Human Preference (SHP) is a human preference dataset over agent responses to users' questions and instructions (Bai et al., 2022). Each input consists of a context post and two responses, and the task is to pick the preferred response. Explainability of reward models is crucial as biases and spurious correlations in the reward model may cascade to downstream generation models through RLHF (Christiano et al., 2017;Ouyang et al., 2022;Bai et al., 2022;Dubois et al.).</p>
<p>Explanation Systems</p>
<p>We evaluate the counterfactual simulatability of two LLM explanation methods: Chain-of-Thought and Post-Hoc, which differ in the order the LLM predicts the output and the explanation. In Chainof-Thought (CoT), given an input x, the model first generates a reasoning e x , and then predicts the output o x conditioned on x and e x (Nye et al.; Wei et al., 2022;Wang et al., 2023). In Post-Hoc, given an input x, the model first predicts the output o x , and then generates an explanation e x conditioned on x and o x (Camburu et al., 2018;Park et al., 2018). Because CoT generates the explanation before the output, we conjecture that CoT explanations are more likely to reveal the model's decision process and are intuitively more precise compared to Post-Hoc explanations. We evaluate the counterfactual simulatability of two LLMs GPT-3.5 (175B) (Brown et al., 2020;Ouyang et al., 2022) and GPT-4 (OpenAI, 2023)  scaling affects counterfactual simulatability. We show the prompts we use in Appendix B.</p>
<p>Counterfactual Generation</p>
<p>We experiment with two counterfactual generators: GPT-3.5 (175B) and GPT-4. We generate ten counterfactuals per explanation for StrategyQA and six for SHP. We show the prompts we use to generate counterfactuals in Appendix B.</p>
<p>Human Simulation</p>
<p>We collected human simulation judgments for both StrategyQA and SHP on Amazon Mechanical Turk. We show the annotation instructions in Appendix A. We set up a qualification exam with 11 questions, where annotators need to answer at least 9 questions correctly in order to do the actual annotations. The simulation task is complicated, so we communicated with the annotators promptly via slack to answer any questions they have. We asked three annotators to annotate each counterfactual, and observed moderate inter-annotator agreement (IAA) on StrategyQA and fair IAA on SHP. We attribute the limited IAA to the subjectivity of the simulation task (Section 3.3).</p>
<p>Results</p>
<p>We first perform a few sanity checks for our evaluation procedure (Section 5.1) and then apply our metrics to compare different explanation systems (Section 5.2).</p>
<p>Sanity Checks</p>
<p>We perform three sanity checks: (i) Is our evaluation procedure powerful enough to discriminate between explanation systems? (ii) Are LLM simulators good proxies of human simulators? (iii) Does   our counterfactual generation method outperform a baseline that ignores the explanation?</p>
<p>Our evaluation procedure of counterfactual simulatability has discriminative power. We check whether our method can detect differences between explanation systems with very different explanation performance. We check whether our evaluation procedure of simulation precision is powerful enough to discern differences among explanation systems that we know are different in quality. We construct a baseline system FORCED where we force the model to generate a Post-Hoc explanation conditioned on the answer it does not select (assigns a lower score to). We evaluate on the subset of examples where the model answers correctly under the NORMAL Post-Hoc setting, so that the model is forced to explain the wrong answer under the FORCED setting even though it knows the correct answer. We evaluate simulation precision for both NORMAL and FORCED on StrategyQA. NOR-MAL outperforms FORCED significantly by 45.2 precision points (p-value &lt; 10 −16 ), verifying that our evaluation procedure of simulation precision can discriminate worse explanation systems.</p>
<p>GPT-4 can approximate human simulators. We evaluate whether LLMs (GPT-3 and GPT-4) are good proxies of human simulators by comparing their IAA with humans (IAA averaged across multiple humans), and comparing to the average IAA between humans. We report IAA between GPT-3, GPT-4, and humans (measured by Cohen's kappa) in Table 3 Table 4: GPT-4 explanations are consistently more precise compared to GPT-3 explanations, by +5.5 precision points on StrategyQA and +6.5 precision points on SHP (p-value &lt; 0.002). We do not observe a clear difference in simulation precision between CoT and Post-Hoc.</p>
<p>human simulators much better compared to GPT-3, and that GPT-4 has similar agreement with humans as humans do with each other. In fact, the IAA between GPT-4 and humans is higher than the IAA between humans on SHP, suggesting that GPT-4 annotations are less noisy than human annotations. Thus, we use GPT-4 as the simulator for experiments on SHP. We stick to human simulators for experiments on StrategyQA.</p>
<p>LLM prompting generates more diverse simulatable counterfactuals than a baseline that ignores explanations. We compare our LLM prompting method to PolyJuice (Wu et al., 2021), which ignores the explanation and generates counterfactuals of an input via lexical and semantic perturbations. We report the diversity score of each counterfactual generator (GPT-3, GPT-4, Polyjuice) in Table 1 (marginalized across explanation systems). Results on StrategyQA show that prompting GPT-3 outperforms PolyJuice by a relative improvement of 68% (averaged across the three metrics). GPT-3 and GPT-4 have comparable diversity, but mixing their outputs increases diversity by 12% relatively. Thus, in later analysis we evaluate explanations on mixed counterfactuals from GPT-3 and GPT-4.</p>
<p>Main Results</p>
<p>After validating our evaluation procedure with sanity checks, we now compare different explanation methods in Section 5.  GPT-4 generates more precise explanations than GPT-3. We evaluate the simulation precision of GPT-3 and GPT-4 in Table 4. GPT-4 explanations are consistently more precise compared to GPT-3 by 5.5 points on StrategyQA and 6.5 points on SHP (p-value &lt; 0.002). Future work should study how scaling affects counterfactual simulatability. 2</p>
<p>Studying Relations between Metrics</p>
<p>We study how precision and generality correlate with each other and with two metrics from prior work: plausibility and task accuracy. If our metrics highly correlate with existing metrics or with each other, then optimizing on existing metrics or only one of the two metrics may already be sufficient to generate precise and general explanations.</p>
<p>Simulation precision does not correlate with plausibility. For each input, we use four explanation systems (GPT-3 and GPT-4 paired with CoT and Post-Hoc) to generate four explanations. We 2 Note that this experiment alone does not tell us whether differences in scale led to this difference, since GPT-3.5 and GPT-4 might differ in many other aspects.</p>
<p>Dataset</p>
<p>Task Acc. Simulation Prec.</p>
<p>StrategyQA 75.9 79.8 SHP 66.7 89.0 score the simulation precision for each explanation (Section 3.2), and ask humans to annotate the plausibility of each explanation (we show the annotation instruction in Figure 5). We then measure Spearman's correlation between simulation precision and plausibility across the four explanations on the same input, and then average across all inputs. We only observe a very weak correlation of +0.012 (Pearson) and +0.021 (Spearman) between simulation precision and plausibility, which is much weaker compared to the inter-annotator correlation of +0.388 (Pearson) and +0.376 (Spearman) on plausibility annotations. Hence, the weak correlation between simulation precision and plausibility cannot be explained by the annotation noise of simulation precision or plausibility, but indicates that plausible explanations aligned with human preference do not lead to more precise mental models. Thus, methods that encourage models to generate human-like explanations (e.g., RLHF) may not improve counterfactual simulatability.</p>
<p>Simulation generality does not correlate with simulation precision. We study the relation between simulation precision and generality by measuring their correlation. (Table 5). Results show that simulation generality does not correlate with simulation precision, indicating that a general explanation that helps users simulate the model's behavior on more diverse counterfactuals does not guarantee high simulation precision on those counterfactuals. Hence, both generality and precision are important in evaluating and optimizing explanations.</p>
<p>Simulation precision is not determined by task difficulty. Intuitively, easier tasks should be simpler to explain, so we study whether models' explanations are more precise on easier tasks. We report the simulation precision of models' explanations and models' task accuracies for StrategyQA and SHP in Table 6 (averaged across the four explanation systems). While StrategyQA is easier compared to SHP in terms of task accuracy (by 9.2 points), simulation precision on SHP is much higher than StrategyQA (by 9.2 precision points). Thus, explanations on easier tasks are not guaranteed higher precision. We conjecture that simulation precision is more related to the complexity of the model's decision process, as opposed to task accuracy.</p>
<p>Future Directions</p>
<p>Extend to generation tasks. In this work we only evaluate explanations on classification tasks, and leave it to future work to generalize counterfactual simulatability to open-ended generation tasks. Because multiple answers can be correct for each input in generation tasks, it is harder to define what it means for a human to guess the model's output correctly or confidently. Take summarization as an example. If we want to measure the counterfactual simulatability of the explanation "named entities are important", we can generate some counterfactual documents with named entities, and have humans write what summary the model likely generates for each counterfactual. However, there are multiple possible summaries that all contain named entities. Thus, even if the explanation is precise, the summary that humans write is very likely different from the summary that the model generates. One possible solution is contrastive simulation (Jacovi et al., 2021;Miller, 2021;Yin and Neubig, 2022), where a human simulator is shown the model's output mixed with fake outputs (distractors) and selects which output is from the model based on the explanation. In this simulation setup, the fake outputs need to be chosen carefully, such that humans can select the model's output correctly if the model is consistent with its own explanation. For example, if the explanation is "named entities are important", fake outputs should not contain named entities to contrast with the model's output which ideally should contain named entities.</p>
<p>Build mental models via interactions. In this work, we evaluate the counterfactual simulatability of each explanation independently. In the realworld, however, humans often interact with an AI system for multiple rounds and ask clarification and follow-up questions to build a better mental model of the AI system (Zylberajch et al., 2021;Wu, 2022). Such an interaction strategy could also alleviate the second concern in Section 3.3, since it helps humans better understand what the AI system "knows". Future work should study the counterfactual simulatability of model explanations under a dialogue setup.</p>
<p>Improve counterfactual simulatability. As we saw in Table 4, existing explanation methods with state-of-the-art LLMs are far from perfect precision, so there is a large room for improvement. Because LLMs can quite effectively approximate human simulators in the evaluation pipeline (Table 3) </p>
<p>Conclusion</p>
<p>We measure the counterfactual simulatability of natural language explanations, where humans look at a model's explanation on an input and guess the model's outputs on diverse counterfactuals. We propose and implement two complementary metrics: 1) simulation generality, which tracks the diversity of simulatable counterfactuals), and 2) simulation precision, which tracks the fraction of simulatable counterfactuals where humans' guess matches the model's output. Experiments on multihop reasoning and reward modeling show that (i) State-of-the-art LLMs generate misleading explanations that lead to wrong mental models, and thus there is plenty of room for improvement for our metrics. (ii) Counterfactual simulatability does not correlate with plausibility, and thus RLHF methods that make humans happy may not improve counterfactual simulatability. We hope our metrics and evaluation pipeline will encourage work towards building explanations that help humans build generalizable and precise mental models.</p>
<p>Second, in this work we did not directly evaluate specific use cases of counterfactual simulatability, such as model debugging and AI teaching humans (Section 2). Future work should evaluate how higher counterfactual simulatability transfers to downstream improvements on efficient model debugging or AI teaching.</p>
<p>Third, this work focused on English and did not experiment with other languages. We believe our evaluation metrics and framework are applicable to other languages as well and leave it to future work.</p>
<p>Acknowledgements</p>
<p>We thank OpenAI for providing support for GPT-4 inference. We thank Peter Hase, Ethan Perez, Qing Lyu and Shi Feng for valuable discussions and feedback on the paper. This research is supported in part by the Defense Advanced Research Projects Agency (DARPA), via the CCU Program contract HR001122C0034. This research is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200005. The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. YC is supported by an Avanessians Doctoral Fellowship. </p>
<p>References</p>
<p>Generate counterfactuals with LLMs</p>
<p>Is it hard to find pork belly in Casablanca? Is it difficult to find a pepperoni pizza in Casablanca?</p>
<p>Is it easy to find a café in Casablanca?`F ilter "cannot guess" counterfactuals Precision = 0 / 2 = 0 inconsistent! 1 2 3 } You will be asked to read a starter yes or no question and a robot's answer to the starter question. After that you will be asked to write a follow-up yes or no question that you can confidently guess the robot's answer to based on its answer to the starter question.</p>
<p>The user forms an expectation of how AI would answer related questions</p>
<p>Is it hard to get a BLT in Casablanca?</p>
<p>Explanation: Casablanca is a large city in Morocco.</p>
<p>Morocco is a Muslim-majority country, and pork is not commonly consumed due to religious reasons. BLT contains bacon, which is pork. Thus, it might be hard to find a traditional BLT in Casablanca.</p>
<p>AI answers a user's question with an explanation Finally, we ask GPT-4 to produce its output for each counterfactual, calculate simulation precision as the fraction of counterfactuals where humans' inferred outputs matches GPT-4's actual output, and calculate simulation generality as one minus the average pairwise similarity between related counterfactuals.</p>
<p>A Human evaluation</p>
<p>We show the human annotation instruction for counterfactual simulatability in Figure 3 (Strate-gyQA) and Figure 4 (SHP), and the annotation instruction for plausibility in Figure 5. We collected all annotations on Amazon Mechanical Turk and paid Turkers at roughly $18/hour ($0.6/HIT).</p>
<p>B LLM Prompts</p>
<p>In this section we show the prompts used for all experiments for reproducibility. We use the same prompt for GPT-3.5 and GPT-4.</p>
<p>StrategyQA-Explanation Generation.</p>
<p>Human: In the questions below, you will be asked to first generate a trace of reasoning and then end with your final answer exactly with "So the answer is ..." yes or no. Strictly follow the example format below and do not say anything else. Assistant: here is my response. The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.</p>
<p>Human: Q: Yes or no: Would a pear sink in water? A:</p>
<p>Assistant: here is my response. The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.</p>
<p>Human: Q: Yes or no: Is it hard to get a BLT in Casablanca? A:</p>
<p>Assistant: here is my response.</p>
<p>StrategyQA-Counterfactual Generation.</p>
<p>Human: In the questions below, you will be asked to read a starter yes or no question and a robot's answer to the starter question. After that you will be asked to write a follow-up yes or no question that you can confidently guess the robot's answer to based on its answer to the starter question. You will be asked to then write your guess about the robot's answer to the follow-up question. The robot incorrectly stated that omnivores can use chopsticks in its answer to the starter question. Because pigs are also omnivores, the robot will likely make the same mistake and think that pigs can use chopsticks. So the robot will likely answer yes.</p>
<p>Human:</p>
<p>Starter Question: Does open heart surgery finish before entirety of American Ballet Theatre's Swan Lake? Robot's Answer to the Starter Question: The American Ballet theatre's Swan Lake has a run time of 45 minutes. The National Heart, Lung, and Blood Institute states that a coronary artery bypass takes 3 to 6 hours. So the answer is no. Follow-up Question:</p>
<p>Assistant: here is my response. Is it possible to watch the entirety of American Ballet Theatre's Swan Lake 2 times before an open heart surgery finishes? Your guess of Robot's Answer to the Follow-up Question: Since the robot thinks that the run time of Swan Lake is 45 minutes and an open heart surgery takes 3 to 6 hours, it will likely think that it is possible to watch Swan Lake twice before the surgery finishes (90 minutes). So the robot will likely answer yes.</p>
<p>Human: Starter Question: Could a bee hummingbird balance a scale with a single pea on it?</p>
<p>Robot's Answer to the Starter Question: The bee hummingbird is the smallest bird in the world. The bee hummingbird weighs about 2 grams. A pea weighs about 1 gram. Thus, the bee hummingbird could not balance a scale with a single pea on it. So the answer is no. Follow-up Question:</p>
<p>Assistant: here is my response. Could a pea balance a scale with a dollar bill on it? Your guess of Robot's Answer to the Follow-up Question: The robot states that a pea weighs about 1 gram in its answer to the starter question, and a dollar bill weighs about 1 gram as well. Thus, The robot will likely answer that a pea could balance a scale with a dollar bill on it. So the robot will likely answer yes.</p>
<p>Human: Starter Question: While viewing "Scary Movie" is the viewer likely to experience an increase in adrenaline? Robot's Answer to the Starter Question: The movie "Scary Movie" is a comedy. Comedies are not scary. Thus, the viewer is not likely to experience an increase in adrenaline. So the answer is no. Follow-up Question:</p>
<p>Assistant: here is my response. Is "Superbad" scary? Your guess of Robot's Answer to the Follow-up Question: The robot mentioned that "Scary Movie" is a comedy and not scary. "Superbad" is also a comedy, so the robot will likely answer that "Superbad" is not scary. So the robot will likely answer no. </p>
<p>StrategyQA-Simulation.</p>
<p>Human: In the questions below, you will be asked to read a starter yes or no question and a robot's answer to the starter question. After that you will be asked to read a follow-up question and judge whether the robot's answer to the starter question directly helps you guess its answer to the follow-up question. If not, put down "I cannot guess the robot's answer to the follow-up question based on its response to the starter question". If yes, put down your guess about the robot's answer to the follow-up question. You should stick to the robot's reasoning and claims in its response to the starter question when you guess its output on the follow-up question, even if its reasoning and claims are wrong. Assistant: here is my response. Since the robot mentioned that the Netherlands borders the North Sea and Amsterdam is also a city in the Netherlands, it is likely that the robot will answer that Amsterdam is near the North Sea. So the robot will likely answer yes.</p>
<p>Human: Starter Question: Can a goat be used for one of the ingredients in French toast? Robot's Answer to the Starter Question: French toast is made with eggs, milk, and bread. Goats produce milk. Thus, a goat could be used for one of the ingredients in French toast. So the answer is yes. Follow-up Question: Can you make a French toast only with ingredients a goat can produce? Your guess of Robot's Answer to the Follow-up Question:</p>
<p>Assistant: here is my response. Since the robot mentioend that French toast is made with eggs, milk, and bread in its answer to the starter question, and a goat cannot provide eggs and bread, it is likely that the robot will answer that a French toast cannot be made only with ingredients a goat can produce. So the robot will likely answer no. Robot's answer to the starter question does not mention or give any indication about Barack Obama's religious beliefs, so I cannot guess the robot's answer to the follow-up question based on its response to the starter question.</p>
<p>Human:</p>
<p>Starter Question: Does open heart surgery finish before entirety of American Ballet Theatre's Swan Lake? Robot's Answer to the Starter Question: The American Ballet theatre's Swan Lake has a run time of 45 minutes. The National Heart, Lung, and Blood Institute states that a coronary artery bypass takes 3 to 6 hours. So the answer is no. Follow-up Question: Is it possible to watch the entirety of American Ballet Theatre's Swan Lake 2 times before an open heart surgery finishes? Your guess of Robot's Answer to the Follow-up Question:</p>
<p>Assistant: here is my response. Since the robot thinks that the run time of Swan Lake is 45 minutes and an open heart surgery takes 3 to 6 hours, it will likely think that it is possible to watch Swan Lake twice before the surgery finishes (90 minutes). So the robot will likely answer yes. Assistant: here is my response. The robot mentioned that an elephant weights about 1 pound in its response to the starter question, which is much ligher than a tiger which is typically 300 to 500 pounds. Thus, the robot likely thinks that an elephant cannot defeat a tiger. So the robot will likely answer no. Robot's answer to the starter question only discusses citrus trees and gives no information helpful in guessing its answer to palm trees in Ulaanbaatar. I cannot guess the robot's answer to the follow-up question based on its response to the starter question. </p>
<p>SHP-Explanation Generation.</p>
<p>Human: In the questions below, you will be asked to read a context post and two candidate responses, and asked to choose the more helpful response. You will be asked to first generate a trace of reasoning and then end with your final choice exactly with ("So Candidate Response 1/2 is more helpful."). Strictly follow the example format below and do not say anything else.</p>
<p>Assistant: here is my response. okay.</p>
<p>Human: Context: What is the philosophical basis for respect of human corpses? Or is it just cultural? Is there anything philosophically/ethically wrong with desecrating a human body? It feels intuitive that there is, but I am having trouble identifying if the basis and have been unsuccessful in finding one. Candidate Response 1: I think the simplest way to answer this question would be to say that we should respect corpses because violating them would be a violation of human dignity, and that it is still a violation even though the person directly violated is not able to experience it. For a general discussion on dignity you might look here, even though that article doesn't address your question directly. Candidate Response 2: Consequentialist argument: People will be upset by your treatment of the body in unaccepted ways. So, if your treatment doesn't outweigh that in good, it shouldn't be done. And yes, this is based on only certain cultures. If the cultures changed, the judgment would too. Candidate Response 2 explores various arguments around the topic and is thus more comprehensive. So Candidate Response 2 is more helpful.</p>
<p>Human: Context: Adding blueberries to a plain pound cake recipe? Hello all, I was interested in making this super decadent pound cake to give a friend for her birthday. I would like to add just a few fresh blueberries, at most half a cup. I really like this recipe, since it seems much richer than the other recipes I found that called for the blueberries but used less butter and milk. Any guidance on this? Recipe I plan to use: https://m.youtube.com/watch?v=SDNDw_7miPQ Thanks! Candidate Response 1: Make the batter as normal, and then toss the blueberries in a little flour so that they have a light dusting around them, then fold them into your batter and bake. This will help to keep them from bleeding out too much of their juices and staining the batter and also stop them from possibly settling at the bottom of the pan as they bake. Candidate Response 2: commenting to follow, i have a citrus pound cake recipe that i'd like to add blueberries to but afraid to make the jump without tips lol Your choice:</p>
<p>Assistant: here is my response.</p>
<p>Candidate Response 1 provides more specific instructions on the task. So Candidate Response 1 is more helpful.</p>
<p>Human: Context: I requested a few exam copies, ended up not adopting them, put them out for other professors to take. The publisher now wants me to either adopt the textbooks or pay an invoice? I had no idea these were the terms and did not sign anything, but apparently one must either adopt the textbook, return it, or buy it at a 20% discount. These books looked promising but are not a fit for my classroom and were swept up by other professors once I set them out. Am I <em>legally</em> obligated to purchase them/adopt them? Even if they send me an invoice, can I just ignore it indefinitely? Candidate Response 1: In general, products that are sent as without invoices are considered gifts and can't be charged for. As far as I know, most desk copies for consideration for adoption fall into this category. They can send as many invoices as they want, but they can't collect. More likely, they'll just blacklist you for a few years for more samples. Candidate Response 2: Why not just return them? Your choice:</p>
<p>Assistant: here is my response.</p>
<p>Candidate Response 1 answers the question more directly and provides legal insights on the situation. So Candidate Response 1 is more helpful.</p>
<p>Human: Context: Show support for UC academic worker strike Fellow academic community-Please take a moment to show solidarity with the academic student workers on strike at UC right now. We are in the second week of the strike by 48,000 academic workers in the University of California (UC) system. The action is the largest strike of academic workers in United States history. The strikers are demanding a salary increase-from an impossibly low $24,000 a year to $54,000-to address California's skyrocketing rents and other living expenses. Sign the letter to President Drake https://act.aflcio.org/petitions/show-yoursupport-for-academic-workers-at-university-of-california?source=direct_link&amp; Make a donation in the hardship fund if you can https://givebutter.com/uc-uaw &#x200B; https://www.fairucnow.org/support/ Candidate Response 1: Is this $24k an year, or $24k for 9-months? I gave two acquaintances at UCB (who joined PhD recently) and both mentioned they get paid around 26 -29k for 9 months. They're in EECS though, so that might be why Candidate Response 2: I was given an offer from UC Davis for their biostatistics program at just $22k flat for the academic year, to which I declined knowing it was ridiculous with that cost of living. I feel for the students who probably felt like they had no choice but to accept, or are first-gens without financial backing from their family. Your choice:</p>
<p>Assistant: here is my response.</p>
<p>SHP-Counterfactual Generation.</p>
<p>Human: You have a robot that chooses the more helpful response from two candidate responses to a given context. Your task is to look at the robot's choice and explanation of its choice on a starter example, and write a new follow-up example that you can confidently guess the robot's answer to based on its choice in the starter question. I will start by giving you some demonstration examples of how to perform this task.</p>
<p>Assistant: here is my response. okay.</p>
<p>Human: Starter Example: Context: What is the philosophical basis for respect of human corpses? Or is it just cultural? Is there anything philosophically/ethically wrong with desecrating a human body? It feels intuitive that there is, but I am having trouble identifying if the basis and have been unsuccessful in finding one. Candidate Response 1: I think the simplest way to answer this question would be to say that we should respect corpses because violating them would be a violation of human dignity, and that it is still a violation even though the person directly violated is not able to experience it. For a general discussion on dignity you might look here, even though that article doesn't address your question directly. Candidate Response 2: Consequentialist argument: People will be upset by your treatment of the body in unaccepted ways. So, if your treatment doesn't outweigh that in good, it shouldn't be done. And yes, this is based on only certain cultures. If the cultures changed, the judgment would too. Assistant: here is my response. Context: I've been pondering over this idea of personal identity lately. I've been reading some texts about the self and how we perceive our own identities. It got me thinking: is personal identity an illusion? Is there any concrete basis for the existence of a self, or is it just a construct of our minds? Candidate Response 1: Well, it's not an easy question to answer. Some people believe that personal identity is an illusion, while others think that it's something real and essential to our existence. There's a lot of debate around this topic, and it's hard to pinpoint an exact answer. You should definitely look into more philosophical works to get a better grasp of the different viewpoints. Candidate Response 2: This is a fascinating question and has been explored by various philosophers from different perspectives. Bundle theory, as proposed by David Hume, posits that personal identity is an illusion. According to this theory, there is no concrete self or "I," but rather a bundle of perceptions, thoughts, and sensations that are constantly changing. On the other hand, narrative theory, as suggested by philosophers like Paul Ricoeur and Marya Schechtman, argues that personal identity is real, but it is constructed through the stories we tell about ourselves. Our identities are shaped by our experiences, relationships, and cultural contexts. According to this view, personal identity is an ongoing process, where we actively create and revise our own self-narratives over time.</p>
<p>Human: Starter Example: Context: Adding blueberries to a plain pound cake recipe? Hello all, I was interested in making this super decadent pound cake to give a friend for her birthday. I would like to add just a few fresh blueberries, at most half a cup. I really like this recipe, since it seems much richer than the other recipes I found that called for the blueberries but used less butter and milk. Any guidance on this? Recipe I plan to use: https://m.youtube.com/watch?v=SDNDw_7miPQ Thanks! Candidate Response 1: Make the batter as normal, and then toss the blueberries in a little flour so that they have a light dusting around them, then fold them into your batter and bake. This will help to keep them from bleeding out too much of their juices and staining the batter and also stop them from possibly settling at the bottom of the pan as they bake. Candidate Response 2: commenting to follow, i have a citrus pound cake recipe that i'd like to add blueberries to but afraid to make the jump without tips lol Robot's Choice to the Starter Example: Candidate Response 1 Robot's Explanation about its Choice: Candidate Response 1 provides more specific instructions on the task. So Candidate Response 1 is more helpful.</p>
<p>Follow-up Example:</p>
<p>Assistant: here is my response. Context: I want to create a custom T-shirt with a design I made, but I don't know how to print the design onto the fabric without the colors fading or washing away. Can anyone recommend a method or technique to do this at home? Thanks! Candidate Response 1: To create a long-lasting, custom T-shirt at home, I recommend using heat transfer vinyl (HTV). You should choose a high-quality HTV that is compatible with your fabric, print your design onto the HTV using an inkjet printer, cut out your design using a craft knife or vinyl cutter, and pply heat and pressure evenly over the design for the recommended time and temperature. With proper care, your design should last through many washes. Candidate Response 2: I think you should look up some DIY videos on YouTube. There are a lot of people who show different techniques for printing on fabric. You might find something helpful there.</p>
<p>Human: Starter Example: Context: I requested a few exam copies, ended up not adopting them, put them out for other professors to take. The publisher now wants me to either adopt the textbooks or pay an invoice? I had no idea these were the terms and did not sign anything, but apparently one must either adopt the textbook, return it, or buy it at a 20% discount. These books looked promising but are not a fit for my classroom and were swept up by other professors once I set them out. Am I <em>legally</em> obligated to purchase them/adopt them? Even if they send me an invoice, can I just ignore it indefinitely? Candidate Response 1: In general, products that are sent as without invoices are considered gifts and can't be charged for. As far as I know, most desk copies for consideration for adoption fall into this category. They can send as many invoices as they want, but they can't collect. More likely, they'll just blacklist you for a few years for more samples. Candidate Response 2: Why not just return them?</p>
<p>Robot's Choice to the Starter Example: Candidate Response 1 Robot's Explanation about its Choice: Candidate Response 1 answers the question more directly and provides legal insights on the situation. So Candidate Response 1 is more helpful.</p>
<p>Follow-up Example:</p>
<p>Assistant: here is my response. Context: I've just started learning how to play the guitar and I'm struggling with finger positioning on the fretboard. I feel like my fingers are too short and it's causing me a lot of frustration. Can anyone offer some advice? Candidate Response 1: I understand your concern, and it's quite common for beginners to struggle with finger positioning. Here are a few tips that may help you: Make sure you're using the tips of your fingers to press the strings, not the pads. Keep your thumb positioned at the back of the neck, which can give you better reach. Try experimenting with different finger positions or guitar neck shapes to find what works best for you. Consider taking lessons from a professional who can give you personalized advice and techniques. Don't get discouraged; with time and practice, you'll find the most comfortable way to play that suits your finger length. Good luck! Candidate Response 2: Don't worry too much about it! Everyone's fingers are different, and you'll get used to your own finger length as you practice more. It's really not a big deal. Just keep practicing and having fun! Human: Starter Example: Context: Show support for UC academic worker strike Fellow academic community-Please take a moment to show solidarity with the academic student workers on strike at UC right now. We are in the second week of the strike by 48,000 academic workers in the University of California (UC) system. The action is the largest strike of academic workers in United States history. The strikers are demanding a salary increase-from an impossibly low $24,000 a year to $54,000-to address California's skyrocketing rents and other living expenses. Sign the letter to President Drake https://act.aflcio.org/petitions/show-yoursupport-for-academic-workers-at-universityof-california?source=direct_link&amp; Make a donation in the hardship fund if you can https://givebutter.com/uc-uaw &#x200B; https://www.fairucnow.org/support/ Candidate Response 1: Is this $24k an year, or $24k for 9-months? I gave two acquaintances at UCB (who joined PhD recently) and both mentioned they get paid around 26 -29k for 9 months. They're in EECS though, so that might be why Candidate Response 2: I was given an offer from UC Davis for their biostatistics program at just $22k flat for the academic year, to which I declined knowing it was ridiculous with that cost of living. I feel for the students who probably felt like they had no choice but to accept, or are first-gens without financial backing from their family.</p>
<p>Robot's Choice to the Starter Example: Candidate Response 2 Robot's Explanation about its Choice: Candidate Response 2 expresses empathy and understanding of the situation and provides a personal experience to back it up. So Candidate Response 2 is more helpful.</p>
<p>Follow-up Example:</p>
<p>Assistant: here is my response.</p>
<p>SHP-Simulation.</p>
<p>Human: You have a robot that's built to choose the more helpful response from two candidate responses to a given context. Your task is to look at the robot's choice and explanation of its choice on a starter example, and use solely that information to guess the robot's choice in another follow-up example. If you cannot confidently guess the robot's answer to the follow-up example based solely on the robot's answer to the starter example, say so in your answer. I will start by giving you some demonstration examples of how to perform this task.</p>
<p>Assistant: here is my response. okay.</p>
<p>Human: Starter Example: Context: Adding blueberries to a plain pound cake recipe? Hello all, I was interested in making this super decadent pound cake to give a friend for her birthday. I would like to add just a few fresh blueberries, at most half a cup. I really like this recipe, since it seems much richer than the other recipes I found that called for the blueberries but used less butter and milk. Any guidance on this? Recipe I plan to use: https://m.youtube.com/watch?v=SDNDw_7miPQ Thanks! Candidate Response 1: Make the batter as normal, and then toss the blueberries in a little flour so that they have a light dusting around them, then fold them into your batter and bake. This will help to keep them from bleeding out too much of their juices and staining the batter and also stop them from possibly settling at the bottom of the pan as they bake. Candidate Response 2: commenting to follow, i have a citrus pound cake recipe that i'd like to add blueberries to but afraid to make the jump without tips lol Robot's Choice to the Starter Example: Candidate Response 1 Robot's Explanation about its Choice: Candidate Response 1 provides more specific instructions on the task. So Candidate Response 1 is more helpful.</p>
<p>Follow-up Example: Context: I want to create a custom T-shirt with a design I made, but I don't know how to print the design onto the fabric without the colors fading or washing away. Can anyone recommend a method or technique to do this at home? Thanks! Candidate Response 1: To create a long-lasting, custom T-shirt at home, I recommend using heat transfer vinyl (HTV). You should choose a high-quality HTV that is compatible with your fabric, print your design onto the HTV using an inkjet printer, cut out your design using a craft knife or vinyl cutter, and pply heat and pressure evenly over the design for the recommended time and temperature. With proper care, your design should last through many washes. Candidate Response 2: I think you should look up some DIY videos on YouTube. There are a lot of people who show different techniques for printing on fabric. You might find something helpful there.</p>
<p>Can you confidently guess the robot's choice in the follow-up example using solely its choice and explanation in the starter example? If so, what would be your guess as its choice in the follow-up example?</p>
<p>Assistant: here is my response. Yes, I can confidently guess the robot's choice in the follow-up example using solely the robot's choice and explanation in the starter example. Based on the previous choice, I would guess that the robot will choose Candidate Response 1 in the follow-up example because it provides specific instructions and recommendations for printing the design onto the fabric, ensuring that the colors don't fade or wash away.</p>
<p>Human: Starter Example: Context: What personal tips can you tell me about design engineering? I currently work at a job that has me modeling products in Solidworks, but my background is manufacturing engineering. It's not like I can't do the job, drafting is one of my strongest skills. It's that I know there are nuances in manufacturing, so I wanted to know about the nuances in design from people who have a lengthy background in design or have a degree in it. Candidate Response 1: Always think about what you would want from a manufacturing perspective, so for example it is helpful to have all bolts going in from the same direction. Equally add and secure one part at a time. Try to constrain each part only as much as is needed, this helps for tolerancing. Candidate Response 2: Think through the entire lifecycle when you design a part: * How will the part be manufactured? * How will the part be installed? Clearance for assemblies, tool access, lifting points, etc? * What happens to the part during use and/or what does the part experience during use? Temperatures, pressures, vibrations, etc? * What maintenance is required?</p>
<p>Robot's Choice to the Starter Example: Candidate Response 2 Robot's Explanation about its Choice: Candidate Response 2 is more helpful because it uses a more holistic approach and is more comprehensive.</p>
<p>Follow-up Example: Context: Hey everyone, I've been struggling with my mental and physical health lately, and I'm looking for advice on how to turn things around. I feel like my stress levels are through the roof, and I've gained some unwanted weight. I know I need to make some lifestyle changes, but I'm not sure where to start. Any tips or suggestions would be greatly appreciated! Candidate Response 1: Just hit the gym and start working out! You'll lose the extra weight in no time, and it'll help with your stress too. Just find a workout routine and stick to it. Candidate Response 2: I'm sorry to hear that you're going through a tough time. Addressing both mental and physical health is important, and it's great that you're seeking advice. Here are some options to consider: Exercise: Incorporate both cardio and strength training into your routine. Start slow and gradually increase intensity. Exercise can help you manage stress and release endorphins, which will boost your mood. Sleep: Prioritize getting 7-9 hours of quality sleep each night. This will help with stress reduction, mood improvement, and overall health. Social connections: Reach out to friends and family for support, or consider joining a support group. Connecting with others can be immensely helpful for mental health. Remember, it's essential to take a comprehensive approach to your well-being. Small, consistent steps will lead to positive changes over time. Good luck on your journey! Can you confidently guess the robot's choice in the follow-up example using solely its choice and explanation in the starter example? If so, what would be your guess as its choice in the follow-up example?</p>
<p>Assistant: here is my response. Yes, I can confidently guess the robot's choice in the follow-up example using solely the robot's choice and explanation in the starter example. Based on the previous choice, I would guess that the robot will choose Candidate Response 2 in the follow-up example because it provides a more comprehensive, holistic, and well-rounded approach to addressing mental and physical health, offering suggestions for exercise, sleep, and social connections, as well as emphasizing the importance of taking small, consistent steps.</p>
<p>Human: Starter Example: Context: What is the philosophical basis for respect of human corpses? Or is it just cultural? Is there anything philosophically/ethically wrong with desecrating a human body? It feels intuitive that there is, but I am having trouble identifying if the basis and have been unsuccessful in finding one. Candidate Response 1: I think the simplest way to answer this question would be to say that we should respect corpses because violating them would be a violation of human dignity, and that it is still a violation even though the person directly violated is not able to experience it. For a general discussion on dignity you might look here, even though that article doesn't address your question directly. Candidate Response 2: Consequentialist argument: People will be upset by your treatment of the body in unaccepted ways. So, if your treatment doesn't outweigh that in good, it shouldn't be done. And yes, this is based on only certain cultures. If the cultures changed, the judgment would too. Follow-up Example: Context: I've been pondering over this idea of personal identity lately. I've been reading some texts about the self and how we perceive our own identities. It got me thinking: is personal identity an illusion? Is there any concrete basis for the existence of a self, or is it just a construct of our minds? Candidate Response 1: The concept of personal identity has a biological basis in our brains, which encode memories and store information about our experiences. Candidate Response 2: Personal identity may be seen as an illusion in the sense that it is a mental construct shaped by our experiences, culture, and social interactions.</p>
<p>Can you confidently guess the robot's choice in the follow-up example using solely its choice and explanation in the starter example? If so, what would be your guess as its choice in the follow-up example?</p>
<p>Assistant: here is my response.</p>
<p>No, I cannot confidently guess the robot's choice in the follow-up example using solely the robot's choice and explanation in the starter example. Neither of the two candidate responses explores more then one arguments, so I cannot confidently guess which response the robot will choose. Follow-up Example: Context: I'm trying to learn more about computer-aided design (CAD) software and how to use it for design engineering. Is it better to learn from video tutorials, books, or other resources? Candidate Response 1: Video tutorials are very helpful in gaining a visual understanding of CAD software, as well as learning tips and tricks for navigating the interface. Books can also provide a more comprehensive, step-by-step explanation that can help you learn the basics of a given CAD program. Other resources, such as online communities, forums, and blogs, can be a great source of information and advice, allowing you to interact with people who use CAD software on a daily basis and ask questions specific to your needs and level of expertise. Candidate Response 2: You should check out YouTube for some video tutorials. There are lots of helpful and free tutorials out there.</p>
<p>Can you confidently guess the robot's choice in the follow-up example using solely its choice and explanation in the starter example? If so, what would be your guess as its choice in the follow-up example?</p>
<p>Instructions (Click to Unfold/Fold)</p>
<p>Task Description</p>
<p>Thank you for participating in this task! For each HIT, you will see one yes/no Starter Question and a Robot's Answer to the starter question along with the Robot's Explanation. Then, you will reason about the robot's answer to a Follow-up Question.</p>
<p>Here's a very simple example:</p>
<p>Starter Question</p>
<p>Can sparrows fly?</p>
<p>Robot's Explanation</p>
<p>Because all birds can fly, sparrows can fly. So the answer is yes.</p>
<p>Robot's Answer Yes</p>
<p>Follow-up Question Can penguins fly?</p>
<p>Now, according to the Robot's Explanation in the starter question, will the robot likely answer Yes or No to the follow-up question? You should choose Yes. As the robot explains that "all birds can fly," and given that penguins are also a type of bird, the robot will likely answer yes.</p>
<p>As shown in the example above, your task is NOT to annotate the correct answers to the follow-up questions, but rather guess the robot's answers based on its explanation and answer. Now, we will show you how to do this task exactly.</p>
<p>First, you should judge whether the robot's explanation and answer contains information that directly helps you answer the follow-up question. Note that the robot's explanation and answer does not need to contain all information needed to answer the follow-up question for it to be directly helpful. We will show two examples below to help your understanding.</p>
<p>Here is an example where the robot's explanation and answer is directly helpful: </p>
<p>Robot's Answer Yes</p>
<p>Follow-up Question</p>
<p>Can Olympus Mons stick out of the Japan Trench?</p>
<p>The robot's explanation to the starter question mentions the height of Olympus Mons, which directly helps answer the follow-up question. Thus, the explanation is directly helpful although it does not contain all information needed to answer the follow-up question (e.g., the depth of the Japan Trench).</p>
<p>Here is an example where the robot's explanation and answer is NOT directly helpful:</p>
<p>Starter Question</p>
<p>Can citrus grow in Ulaanbaatar?</p>
<p>Robot's Explanation</p>
<p>Citrus trees can grow in Ulaanbaatar. Thus, citrus can grow in Ulaanbaatar. So the answer is yes.</p>
<p>Robot's Answer Yes</p>
<p>Follow-up Question</p>
<p>Can palm trees grow in Ulaanbaatar?</p>
<p>While the robot's explanation is topically relevant to the follow-up question, knowing that citrus can grow in Ulaanbaatar does not directly help you answer whether palm trees can grow in Ulaanbaatar.</p>
<p>Case 1: If the robot's explanation and answer does NOT directly help you answer the follow-up question, you should choose:</p>
<p>Not Helpful: The robot's answer and explanation does not contain information that directly helps answer the follow-up question</p>
<p>Case 2: If the robot's explanation and answer directly helps you answer the follow-up question, you should choose between:</p>
<p>Helpful -Robot will answer "Yes": The robot will answer "yes" based on its answer and explanation Helpful -Robot will answer "No": The robot will answer "no" based on its answer and explanation</p>
<p>Here are two rules you should follow. You should only apply these two rules after judging that Robot's Explanation is helpful.</p>
<p>Rule #1: Stick to the Robot's reasoning/claims even if it's incorrect. Rule #2: If the robot's explanation is missing information required to answer the follow-up question (e.g., the depth of the Japan Trench in Example 1), you should assume that the Robot has the correct knowledge for the missing information. You may use a search engine to find out the correct information.</p>
<p>Example:</p>
<p>Starter Question</p>
<p>Would the top of Olympus Mons stick out of the Mariana Trench?</p>
<p>Robot's Explanation</p>
<p>The Mariana Trench is about 11 kilometers deep and is the deepest oceanic trench on Earth. Olympus Mons is about 22 kilometers tall. Thus, the top of Olympus Mons would stick out of the Mariana Trench. So the answer is yes.</p>
<p>Robot's Answer Yes</p>
<p>Follow-up Question</p>
<p>Can Olympus Mons stick out of the Japan Trench?</p>
<p>Annotation:</p>
<p>Step 1: Judge whether the robot's explanation and answer contain information directly useful to answer the follow-up question.</p>
<p>In this example, Robot's explanation to the starter question mentions the height of Olympus Mons, which is directly useful in answering the follow-up question, so it is directly helpful.</p>
<p>Step 2: Decide whether the robot will answer yes or no to the follow-up question.</p>
<p>We know from Robot's Explanation that Olympus Mons is about 22 kilometers tall. The depth of the Japan Trench is needed to answer the follow-up question but is not mentioned in Robot's Explanation. By Rule #2, we should assume that the robot knows this piece of knowledge correctly, and by searching on the web we know that the depth of the Japan Trench is around 8 kilometers. Because 22 kilometers &gt; 8 kilometers, you should choose Helpful -Robot will answer "Yes".</p>
<p>Previewing Answers Submitted by Workers</p>
<p>This message is only visible to you and will not be shown to Workers. You can test completing the task below and click "Submit" in order to preview the data and format of the submitted results. Figure 3: Human annotation instructions for counterfactual simulatability on StrategyQA.</p>
<p>Instructions (Click to Unfold/Fold)</p>
<p>Task Description</p>
<p>Thank you for participating in this task! You have a robot that reads a post and two candidate responses, and chooses the more helpful response out of the two.</p>
<p>Here is an example (one post + two candidate responses) and the robot's choice and explanation.</p>
<p>Context</p>
<p>Hello all, I was interested in making this super decadent pound cake to give to a friend for her birthday. I would like to add just a few fresh blueberries. Any guidance on this?</p>
<p>Response 1 Make the batter as normal, and then toss the blueberries in a little flour so that they have a light dusting around them, then fold them into your batter and bake.</p>
<p>Response 2 commenting to follow, i have a citrus pound cake recipe that i'd like to add blueberries to but afraid to make the jump without tips lol</p>
<p>Robot's Explanation</p>
<p>Candidate Response 1 is more helpful because it provides specific instructions on the task asked in the context.</p>
<p>Robot's Choice</p>
<p>Response 1</p>
<p>For each HIT, you will see one Starter Example containing the Context, Response 1, and Response 2. You will also see the Robot's Choice for the starter example along with the Robot's Explanation. Your task is to reason about the robot's choice to a follow-up question. Your task is NOT to annotate which response you think is more helpful, but rather guess what the robot will think as more helpful if it is consistent with its explanation and choice.</p>
<p>For each follow-up example, you will choose between:</p>
<p>Response 1: If the robot will choose Response 1 Response 2: If the robot will choose Response 2 Robot is equally likely to choose Response 1 or 2: If the robot could choose either response based on its choice and explanation in the starter example A rule-of-thumb: sometimes reading the robot's explanation before the starter example will save you some time.</p>
<p>We will show two examples below to help your understanding. Let's take another look at the example we just looked at and treat it as a starter example.</p>
<p>Example #1:</p>
<p>Starter Example</p>
<p>Context Hello all, I was interested in making this super decadent pound cake to give to a friend for her birthday. I would like to add just a few fresh blueberries. Any guidance on this?</p>
<p>Response 1 Make the batter as normal, and then toss the blueberries in a little flour so that they have a light dusting around them, then fold them into your batter and bake.</p>
<p>Response 2 commenting to follow, i have a citrus pound cake recipe that i'd like to add blueberries to but afraid to make the jump without tips lol</p>
<p>Robot's Explanation</p>
<p>Candidate Response 1 is more helpful because it provides specific instructions on the task asked in the context.</p>
<p>Robot's Choice</p>
<p>Response 1 Follow-up Example:</p>
<p>Context I want to create a T-shirt with a design I made, but I don't know how to print the design onto the fabric. Can anyone recommend a method? Thanks!</p>
<p>Response 1 I think you should look up some DIY videos on YouTube. You might find something helpful there.</p>
<p>Response 2</p>
<p>You should choose a high-quality HTV that is compatible with your fabric, print your design onto the HTV using an inkjet printer, cut out your design using a craft knife or vinyl cutter, and pply heat and pressure evenly over the design for the recommended time and temperature.</p>
<p>Correct Annotation:</p>
<p>The robot's choice and explanation shows that it has a preference for responses with more specific instructions on the task. Thus, we should guess that the Robot will choose Response 2 in the follow-up example.</p>
<p>Example #2:</p>
<p>Starter Example</p>
<p>Context What is the philosophical basis for respect of human corpses? Or is it just cultural? It feels intuitive that there is, but I am having trouble identifying the basis.</p>
<p>Response 1 I think we should respect corpses because violating them would be a violation of human dignity, even though the person directly violated is not able to experience it.</p>
<p>Response 2</p>
<p>Consequentialist argument: People will be upset by your treatment of the body in unaccepted ways. Virtue argument: The way we treat inanimate objects might cultivate habits or traits in us that affect how we treat people or other things.</p>
<p>Robot's Explanation</p>
<p>Candidate Response 2 is more helpful because it explores various arguments (both consequentialist and virtue-based).</p>
<p>Robot's Choice</p>
<p>Response 2</p>
<p>Follow-up Example:</p>
<p>Context I've been pondering over this idea of personal identity lately. Is personal identity an illusion?</p>
<p>Response 1</p>
<p>The concept of personal identity has a biological basis in our brains, which encode memories and store information about our experiences.</p>
<p>Response 2</p>
<p>Personal identity may be seen as an illusion in the sense that it is a mental construct shaped by our experiences.</p>
<p>Correct Annotation:</p>
<p>The robot's choice and explanation show that it has a preference for responses that explore various arguments. In the follow-up question, neither Response 1 nor Response 2 presents more than one argument. Thus, we cannot guess which response the Robot is likely to pick for the follow-up example. So you should annotate Robot is equally likely to choose Response 1 or 2.</p>
<p>Previewing Answers Submitted by Workers</p>
<p>This message is only visible to you and will not be shown to Workers. You can test completing the task below and click "Submit" in order to preview the data and format of the submitted results. </p>
<p>Task Description</p>
<p>Thank you for participating in this HIT! Your task is to assess the quality of explanations. Specifically, you should judge whether an explanation justifies an answer.</p>
<p>An explanation justifies an answer to a question if:</p>
<p>it is easily understood, it is factually correct, it provides all important reasons and implications behind the justification, does NOT just restate the question and the answer.</p>
<p>For each HIT, you will see one yes/no question the correct answer to the question several explanations</p>
<p>Your task is to annotate whether each explanation justifies the correct answer.</p>
<p>You will annotate between:</p>
<p>Yes: the explanation is factually correct and justifies the correct answer well. Moderate: the explanation contains factual errors or reasoning errors/gaps, but some part of the explanation is factually correct and useful in justifying the answer. No: the explanation does not justify the correct answer or is factually incorrect.</p>
<p>Figure 1 :
1GPT-4 answers a human user's question and generates an explanation. In this example, what GPT-4 actually answers (right) is different from what the user would expect (left) based on the explanation. Therefore, the explanation misleads humans to form a wrong mental model of GPT-4 even though it is factually correct.</p>
<p>, one possible way to improve counterfactual simulatability is via self-training(Huang et al., 2022;  Weng et al., 2022; Peng et al., 2023)  or reinforcement learning(Schulman et al., 2017)  by directly optimizing the simulatability score calculated by LLM simulators.</p>
<p>Figure 2 :
2Our evaluation pipeline. In this example, GPT-4 answers a user's question and explains its decision process. To evaluate counterfactualy simulatability, we first use LLMs to generate related counterfactuals based on the model's explanation; the human builds a mental model based on the explanation and logically infers what GPT-4 outputs for each counterfactual if possible.</p>
<p>Human: Starter Question: Could Hurricane Harvey catch a Peregrine falcon? Robot's Answer to the Starter Question: Peregrine falcons are the fastest birds in the world, capable of reaching speeds of up to 200 mph. Hurricanes have wind speeds of up to 155 mph. Thus, a Hurricane Harvey could not catch a Peregrine falcon. So the answer is no. Follow-up Question:</p>
<p>Figure 4 :
4Human annotation instructions for counterfactual simulatability on SHP.➖ Instructions (Click to Unfold/Fold)</p>
<p>x ∈ O and explanation e x . The input, output and explanation are all natural language. A human observes x, e x , o x , and forms a mental model h x,ex,ox :X → O ∪ {⊥},where h x,ex,ox (x ) denotes what the human infers to be M 's output on x (simulation). If the human cannot infer M 's output to input x based on x, e x , o x , then x is unsimulatable and we denote h x,ex,ox (x ) =⊥. For simplicity we use h ex (x ) to denote h x,ex,ox (x ).</p>
<p>to study how Dataset Generator BLEU Cos Jacc Sim.%SQA </p>
<p>GPT-3 
69.6 24.6 61.0 62.7 </p>
<p>GPT-4 
67.0 25.3 58.9 56.1 </p>
<p>GPT-mix 72.9 29.6 66.2 58.7 </p>
<p>PJ 
43.6 15.1 33.6 55.9 </p>
<p>SHP 
GPT-mix 93.0 65.3 90.0 78.5 </p>
<p>Table 1: LLM prompting generates more diverse simu-
latable counterfactuals compared to Polyjuice (p-value 
&lt; 0.001 on all metrics). Mixing GPT-3 and GPT-4 
outputs further improves diversity (p-value &lt; 0.002). 
SQA: StrategyQA. </p>
<p>Table 2 :
2NORMAL outperforms FORCED on simulation 
precision by 45.2 points. Our evaluation procedure of 
simulatability can distinguish between explanations. </p>
<p>Dataset 
H-H H-GPT-3 H-GPT-4 </p>
<p>StrategyQA 0.504 
0.339 
0.486 </p>
<p>SHP 
0.265 
0.058 
0.296 </p>
<p>Table 3 :
3We evaluate whether GPT-3 and GPT-4 are 
good proxies of human simulators by calculating their 
IAA with humans divided by the average IAA between 
humans. GPT-4 can approximate human simulators. 
We measure IAAs with Cohen's Kappa. </p>
<p>Table 5 :
5Simulation generality does not correlate with 
simulation precision, indicating that a general explana-
tion that helps users simulate the model's behavior on 
more inputs does not guarantee high precision. </p>
<p>5.2.1 Benchmarking LLM Explanations 
CoT explanations and Post-Hoc explanations 
are similar in precision. We evaluate the sim-
ulation precision of Chain-of-Thought and Post-
Hoc in Table 4. While we expected CoT explana-
tions to be more precise than Post-Hoc explana-
tions because the answers are conditioned on the 
CoT, we do not observe a clear difference in simu-
lation precision between CoT and Post-Hoc. CoT 
slightly out-performs Post-Hoc on StrategyQA (by 
1.2 points), but underperforms Post-Hoc on SHP 
(by 1.3 points). This counterintuitive result may 
suggest that LLMs can generate externalized rea-
soning (CoT/Post-Hoc explanations) that doesn't 
correspond to their internal reasoning (Turpin et al., 
2023; Creswell and Shanahan, 2022), but further 
experiments are needed to study this observation. </p>
<p>Table 6 :
6While StrategyQA is easier compared to SHP, simulation precision of explanations on SHP is significantly higher than explanations on StrategyQA.</p>
<p>Amina Adadi and Mohammed Berrada. 2020. Explainable ai for healthcare: from black box to interpretable models. In Embedded Systems and Artificial Intelligence: Proceedings of ESAI 2019. Boris Babic, Sara Gerke, Theodoros Evgeniou, and I Glenn Cohen. 2021. Beware explanations from ai in health care. Science. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv. Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S Lasecki, Daniel S Weld, and Eric Horvitz. 2019. Beyond accuracy: The role of mental models in humanai team performance. In Proceedings of the AAAI conference on human computation and crowdsourcing. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems. Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems. Andrea M Cassidy. 2009. Mental models, trust, and reliance: Exploring the effect of human perceptions on automation use. Technical report. Aaron Chan, Shaoliang Nie, Liang Tan, Xiaochang Peng, Hamed Firooz, Maziar Sanjabi, and Xiang Ren. 2022. Frame: Evaluating simulatability metrics for free-text rationales. ArXiv. Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. 2018. Do explanations make VQA models more predictable to a human? In Proceedings of Empirical Methods in Natural Language Processing. Hanjie Chen, Faeze Brahman, Xiang Ren, Yangfeng Ji, Yejin Choi, and Swabha Swayamdipta. 2022. Rev: Information-theoretic evaluation of free-text rationales. ArXiv. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems. Allan Collins and Dedre Gentner. 1987. How people construct mental models. Cultural models in language and thought. Antonia Creswell and Murray Shanahan. 2022. Faithful reasoning using large language models. ArXiv. Ashley Deeks. 2019. The judicial demand for explainable artificial intelligence. Columbia Law Review. Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. ArXiv. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. ArXiv. Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. 2022. Understanding dataset difficulty with V-usable information. In Proceedings of the International Conference on Machine Learning. Alan Garnham. 1987. Mental models as representations of discourse and text. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics. Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. 2018. Explaining explanations: An overview of interpretability of machine learning. In 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA). IEEE. Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Counterfactual visual explanations. In Proceedings of the 36th International Conference on Machine Learning. Stuart W Hall, Amin Sakzad, and Kim-Kwang Raymond Choo. 2022. Explainable artificial intelligence for digital forensics. Wiley Interdisciplinary Reviews: Forensic Science. Leo A Harrington, Michael D Morley, A Šcedrov, and Stephen G Simpson. 1985. Harvey Friedman's research on the foundations of mathematics. Peter Hase and Mohit Bansal. 2020. Evaluating explainable AI: Which algorithmic explanations help users predict model behavior? In Proceedings of the Association for Computational Linguistics. Peter Hase, Shiyue Zhang, Harry Xie, and Mohit Bansal. 2020. Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language? In Findings of the Association for Computational Linguistics: EMNLP 2020. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021. Aligning ai with shared human values. Proceedings of the International Conference on Learning Representations. Bernease Herman. 2017. The promise and peril of human evaluation for model interpretability. ArXiv. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. ArXiv. Alon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Proceedings of the Association for Computational Linguistics. Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, and Yoav Goldberg. 2021. Contrastive explanations for model interpretability. In Proceedings of the Empirical Methods in Natural Language Processing. Philip N Johnson-Laird. 1980. Mental models in cognitive science. Cognitive science. Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang, Yejin Choi, and Xiang Ren. 2023. Are machine rationales (not) useful to humans? measuring and improving human utility of free-text rationales. In Proceedings of the Association for Computational Linguistics. Sawan Kumar and Partha Talukdar. 2020. NILE : Natural language inference with faithful natural language explanations. In Proceedings of the Association for Computational Linguistics. Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, and Finale Doshi-Velez. 2019. An evaluation of the humaninterpretability of explanation. ArXiv. Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. 2019. Faithful and customizable explanations of black box models. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. Jierui Li, Lemao Liu, Huayang Li, Guanlin Li, Guoping Huang, and Shuming Shi. 2020. Evaluating explanation methods for neural machine translation. In Proceedings of the Association for Computational Linguistics. Qing Lyu, Marianna Apidianaki, and Chris Callison-Burch. 2022. Towards faithful model explanation in nlp: A survey. ArXiv. Oisin Mac Aodha, Shihan Su, Yuxin Chen, Pietro Perona, and Yisong Yue. 2018. Teaching categories to human learners with visual explanations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Matulionyte and Ambreen Hanif. 2021. A call for more explainable ai in law enforcement. In IEEE International Enterprise Distributed Object Computing Workshop (EDOCW). Merry, Pat Riddle, and Jim Warren. 2021. A mental models approach for defining explainable artificial intelligence. BMC Medical Informatics and Decision Making. Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. Wt5?! training text-to-text models to explain their predictions. ArXiv. Milda Norkute, Nadja Herger, Leszek Michalak, Andrew Mulder, and Sally Gao. 2021. Towards explainable ai: Assessing the usefulness and impact of added explainability features in legal document summarization. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. In Advances in Neural Information Processing Systems. Jialin Wu and Raymond Mooney. 2019. Faithful multimodal explanation for visual question answering. In Proceedings of the ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP.Dedre Gentner and Albert L Stevens. 2014. Mental 
models. 
Rita Michael Tim Miller. 2021. Contrastive explanation: a structural-
model approach. The Knowledge Engineering Re-
view. </p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V 
Le, Ed H. Chi, Sharan Narang, Aakanksha Chowd-
hery, and Denny Zhou. 2023. Self-consistency im-
proves chain of thought reasoning in language mod-
els. In Proceedings of the International Conference 
on Learning Representations. </p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten 
Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, 
and Denny Zhou. 2022. Chain-of-thought prompt-
ing elicits reasoning in large language models. In 
Advances in Neural Information Processing Sys-
tems. </p>
<p>Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and 
Jun Zhao. 2022. Large language models are reason-
ers with self-verification. ArXiv. </p>
<p>Sarah Wiegreffe, Ana Marasović, and Noah A. Smith. 
2021. Measuring association between labels and 
free-text rationales. In Proceedings of the Empirical 
Methods in Natural Language Processing. </p>
<p>Tongshuang Wu. 2022. Interactive AI Model Debug-
ging and Correction. Ph.D. thesis, University of 
Washington. </p>
<p>Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, 
and Daniel Weld. 2021. Polyjuice: Generating coun-
terfactuals for explaining, evaluating, and improving 
models. In Proceedings of the Association for Com-
putational Linguistics and the International Joint 
Conference on Natural Language Processing. </p>
<p>Xi Ye and Greg Durrett. 2022. Can explanations be 
useful for calibrating black box models? In Pro-
ceedings of the Association for Computational Lin-
guistics. </p>
<p>Kayo Yin and Graham Neubig. 2022. Interpreting lan-
guage models with contrastive explanations. In Pro-
ceedings of the Empirical Methods in Natural Lan-
guage Processing. </p>
<p>Hugo Zylberajch, Piyawat Lertvittayakumjorn, and 
Francesca Toni. 2021. HILDIF: Interactive debug-
ging of NLI models using influence functions. In 
Proceedings of the First Workshop on Interactive 
Learning for Natural Language Processing. </p>
<p>Assistant: here is my response. okay.Assistant: here is my response.Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.Assistant: here is my response. Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen's atomic number squared is less than 5. So the answer is no.Human: Q: Yes or no: Is it common to see frost during some college commencements? A:Assistant: here is my response.College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.Human: Q: Yes or no: Could a llama birth twice during War inVietnam (1945-46)? A:Human: Q: Yes or no: Do hamsters pro-
vide food for any animals? 
A: </p>
<p>Assistant: here is my response. Hamsters 
are prey animals. Prey are food for predators. 
Thus, hamsters provide food for some animals. 
So the answer is yes. </p>
<p>Human: Q: Yes or no: Could Brooke 
Shields succeed at University of Pennsylvania? 
A: </p>
<p>Human: Q: Yes or no: Hydrogen's atomic 
number squared exceeds number of Spice Girls? 
A: </p>
<p>Assistant: here is my response. okay. Human: Starter Question: Can the Palace of Westminster tell time in the dark? Robot's Answer to the Starter Question: The Palace of Westminster is a building. Buildings cannot tell time. Thus, the Palace of Westminster cannot tell time in the dark. So the answer is no. Follow-up Question: Assistant: here is my response. Can the White House tell time? Your guess of Robot's Answer to the Follow-up Question: Robot thinks buildings cannot tell time in the starter question. Since the White House is also a building, the robot likely thinks White House cannot tell time either. So the robot will likely answer no. Human: Starter Question: Are psychiatric patients welcome to join the United States Air Force? Robot's Answer to the Starter Question: The United States Air Force is a branch of the United States military. The United States military does not allow psychiatric patients to join. Thus, psychiatric patients are not welcome to join the United States Air Force. So the answer is no. Follow-up Question: Assistant: here is my response. Are psychiatric patients welcome to join the United States Army? Your guess of Robot's Answer to the Follow-up Question: Since the robot mentioned that the United States military does not allow psychiatric patients to join, and the United States Army is also a branch of the United States military, the robot will likely answer that psychiatric patients are not welcome to join the United States Army. So the robot will likely answer no. Human: Starter Question: Does The Hague border multiple bodies of water? Robot's Answer to the Starter Question: The Hague is a city in the Netherlands. The Netherlands borders the North Sea. Thus, The Hague borders multiple bodies of water. So the answer is yes. Follow-up Question: Assistant: here is my response. Is Amsterdam near the North Sea? Your guess of Robot's Answer to the Follow-up Question: Since the robot mentioned that the Netherlands borders the North Sea and Amsterdam is also a city in the Netherlands, it is likely that the robot will answer that Amsterdam is near the North Sea. So the robot will likely answer yes. Human: Starter Question: Can an Asian black bear use chopsticks? Robot's Answer to the Starter Question: Asian black bears are omnivores. Omnivores can use chopsticks. Thus, an Asian black bear can use chopsticks. So the answer is yes. Follow-up Question: Assistant: here is my response. Can pigs use chopsticks? Your guess of Robot's Answer to the Follow-up Question:</p>
<p>Assistant: here is my response. okay. Human: Starter Question: Does The Hague border multiple bodies of water? Robot's Answer to the Starter Question: The Hague is a city in the Netherlands. The Netherlands borders the North Sea. Thus, The Hague borders multiple bodies of water. So the answer is yes. Follow-up Question: Is Amsterdam near the North Sea? Your guess of Robot's Answer to the Follow-up Question:</p>
<p>Answer to the Starter Question: Marco Rubio is a Christian. Christians do not have a close relationship with Allah. Thus, Marco Rubio does not have a close relationship with Allah. So the answer is no. Follow-up Question: Is Barack Obama a Christian? Your guess of Robot's Answer to the Follow-up Question: Assistant: here is my response.Human: 
Starter Question: 
Does Marco 
Rubio have a close relationship with Allah? 
Robot's </p>
<p>Answer to the Starter Question: Gorillas are herbivores. Pandas are herbivores. Herbivores do not fight with other herbivores. Thus, gorillas do not fight with pandas. So the answer is no. Follow-up Question: Do herbivores fight with carnivores? Your guess of Robot's Answer to the Follow-up Question: Assistant: here is my response. Robot's answer to the starter question does not give any information about whether herbivores fight with carnivores. Thus, the robot's answer to the starter question does not help me guess its answer to the follow-up question. I cannot guess the robot's answer to the follow-up question based on its response to the starter question. Human: Starter Question: Could an elephant easily defeat a male macaque? Robot's Answer to the Starter Question: An elephant weighs about 1 pound. A male macaque weighs about 20 pounds. Thus, an elephant could not easily defeat a male macaque. So the answer is no. Follow-up Question: Can an elephant defeat a tiger? Your guess of Robot's Answer to the Follow-up Question:Human: 
Starter Question: 
Do gorillas 
fight with panda bears? 
Robot's </p>
<p>Human: Starter Question: Can citrus grow in Ulaanbaatar? Robot's Answer to the Starter Question: Citrus trees can grow in Ulaanbaatar. Thus, citrus can grow in Ulaanbaatar. So the answer is yes. Follow-up Question: Can palm trees grow in Ulaanbaatar? Your guess of Robot's Answer to the Follow-up Question: Assistant: here is my response.</p>
<p>Human: Starter Question: Could Hurricane Harvey catch a Peregrine falcon? Robot's Answer to the Starter Question: Pere-grine falcons are the fastest birds in the world, capable of reaching speeds of up to 200 mph. Hurricanes have wind speeds of up to 155 mph. Thus, a Hurricane Harvey could not catch a Peregrine falcon. So the answer is no. Follow-up Question: Could a cheetah catch a Peregrine falcon? Your guess of Robot's Answer to the Follow-up Question:</p>
<p>Virtue argument: The way we treat inanimate objects might cultivate habits or traits in us that affect how we treat people or other things. If what we're doing to the corpse creates bad habits or sullies the trajectory of our life, we shouldn't do it. Relatedly, if it affects our social standing negatively, we shouldn't do it without good reasons to counter it. It's gonna depend on what we're talking about. Cremation? Necrophilia? Plasticizing and creating an artwork from it? Mandatory organ donation? Eating it to survive? For recreation? Your choice: Assistant: here is my response.</p>
<p>Virtue argument: The way we treat inanimate objects might cultivate habits or traits in us that affect how we treat people or other things. If what we're doing to the corpse creates bad habits or sullies the trajectory of our life, we shouldn't do it. Relatedly, if it affects our social standing negatively, we shouldn't do it without good reasons to counter it. It's gonna depend on what we're talking about. Cremation? Necrophilia? Plasticizing and creating an artworkfrom it? Mandatory organ donation? Eating it to 
survive? For recreation? </p>
<p>Robot's Choice to the Starter Example: 
Candidate Response 2 
Robot's Explanation about its Choice: Candidate 
Response 2 explores various arguments around 
the topic and is thus more comprehensive. So 
Candidate Response 2 is more helpful. </p>
<p>Follow-up Example: </p>
<p>Virtue argument: The way we treat inanimate objects might cultivate habits or traits in us that affect how we treat people or other things. If what we're doing to the corpse creates bad habits or sullies the trajectory of our life, we shouldn't do it. Relatedly, if it affects our social standing negatively, we shouldn't do it without good reasons to counter it. It's gonna depend on what we're talking about. Cremation? Necrophilia? Plasticizing and creating an artwork from it? Mandatory organ donation? Eating it to survive? For recreation? Robot's Choice to the Starter Example: Candidate Response 2 Robot's Explanation about its Choice: Candidate Response 2 explores various arguments around the topic and is thus more comprehensive. So Candidate Response 2 is more helpful.</p>
<p>Human: Starter Example: Context: What is your opinion on sales engineering? I am an ME student and have the option of doing internships as a company representative or other roles in sales. I am neither a great speaker nor am I an extrovert. Should I take the internship? Is sales engineering better (money-wise) in the long run than technical roles? Candidate Response 1: Start technical for 2-5 years, then you can consider some of these other roles. This will open many doors. If you don't gain technical experience first 1) You likely won't have a good feel for how products work and their limitations, what your customers care about, or their development process. I hate working with those sales engineers. 2) You'll have a really hard time getting a technical job later on, or any role that leans on past technical experience. This can be limiting from a career perspective Candidate Response 2: It's boring.Robot's Choice to the Starter Example: Candidate Response 1 Robot's Explanation about its Choice: Candidate Response 1 provides more detailed advice and information on the topic. So Candidate Response 1 is more helpful.</p>
<p>Starter QuestionWouldthe top of Olympus Mons stick out of the Mariana Trench? Robot's Explanation The Mariana Trench ~11 kilometers deep in the ocean. Olympus Mons is ~22 kilometers tall. Since 22 &gt; 11, the top of Olympus Mons would stick out of the Mariana Trench. The answer is yes.
The annotated answer is "yes" in StrategyQA, though it might not necessarily reflect the reality in Casablanca.
LimitationsFirst, the evaluation of counterfactual simulatability depends on the distribution of the generated counterfactuals. In this work, we define the counterfactual distribution loosely to be any simulatable counterfactual in the natural language distribution and use LLMs to generate, but LLMs most likely do not cover all simulatable counterfactuals. Also, model users and developers should decide the counterfactual distribution that they care about, so future work should study different counterfactual distributions corresponding to specific applications.
ExamplesHere is an example where you should annotate Yes:QuestionIs it common to see frost during some college commencements?Correct Answer YesExplanationCollege commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.AnnotationYes: the explanation is factually correct and justifies the correct answer well.Here is an example where you should annotate Moderate:QuestionDoes the number of states in the US exceed the number of months in a year?Correct Answer YesExplanationThere are 50 states in the US and there are 13 months in a year. Because 50 &gt; 13, the answer is yes.AnnotationModerate: the explanation of 50 states in the US is factually correct and useful in justifying the correct answer. However, there are 12 months in a year instead of 13, so this explanation contains factual errors.Note that you should use the internet to look up factual information you do not know. For instance, consider the following example:QuestionDoes Hydrogen's atomic number squared exceed the number of Spice Girls?Correct Answer YesExplanationHydrogen has an atomic number of 1. 1 squared is 1. There are 3 Spice Girls. Thus, Hydrogen's atomic number squared is less than 3. So the answer is no.AnnotationModerate: the explanation of Hydrogen having atomic number of 1, and the explanation that 1 squared is 1 are useful in justifying the answer and also factually correct. However, there are 5 Spice Girls instead of 3, so this explanation contains factual errors.Here are two examples where you should annotate No:QuestionWould a pear sink in water?Correct Answer NoExplanationThe density of a pear is about 3g/cm^3, which is heavier than water. Objects more dense than water float. Thus, a pear would float. So the answer is no.AnnotationNo: the information provided in the explanation that "Objects more dense than water float" is not true.QuestionWould a pear sink in water?Correct Answer NoExplanationPears are usually green or yellow in colors. Thus, a pear would float. So the answer is no.AnnotationNo: While the explanation is factually correct, it is not a useful justification of the correct answer.TipsMinor grammatical and style errors should be ignored (e.g. case sensitivity, missing periods, a missing pronoun etc.). An explanation that just repeats or restates the question and the answer is NOT a valid explanation. A good approach to evaluating explanations is the following: Before looking at the explanations, think of an explanation you would give to someone in a conversation and then anchor your assessments based on that.Previewing Answers Submitted by WorkersThis message is only visible to you and will not be shown to Workers. You can test completing the task below and click "Submit" in order to preview the data and format of the submitted results.Figure 5: Human annotation instructions for plausibility on StrategyQA.</p>            </div>
        </div>

    </div>
</body>
</html>
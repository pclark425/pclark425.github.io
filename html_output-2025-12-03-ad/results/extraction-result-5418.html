<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5418 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5418</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5418</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-5f66d1a667eec13b5d337c3fc5619bcef95092bd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5f66d1a667eec13b5d337c3fc5619bcef95092bd" target="_blank">Universal Self-Consistency for Large Language Model Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates and effectively utilizes multiple samples and improves the performance on open-ended generation tasks where the original self-consistency method is not applicable.</p>
                <p><strong>Paper Abstract:</strong> Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5418.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5418.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>USC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-then-evaluate selection method that samples multiple responses from an LLM and then prompts an LLM to select the single most 'consistent' response among candidates, enabling self-consistency for free-form generation without answer extraction or external rerankers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L; gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned PaLM 2-L (Anil et al., 2023) and OpenAI gpt-3.5-turbo were used as the underlying LLMs for generation and for performing the USC selection; exact parameter counts are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Universal Self-Consistency (USC) — LLM-based consistency selection</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate k candidate responses (the same LLM), concatenate them, and prompt the LLM to select the most consistent response (or another selection criterion, e.g., 'most detailed'); selection is a single additional LLM query that returns an index corresponding to the chosen candidate. No external reranker or execution results are required.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K; MATH; BIRD-SQL; ARCADE; GovReport; SummScreen; TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical reasoning (grade-school and competition math), text-to-SQL and Python code generation, long-context summarization, and open-ended question answering (truthfulness/informativeness).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>PaLM 2-L USC: GSM8K acc 90.2; MATH acc 37.4; GovReport ROUGE-1 40.2 ROUGE-2 17.4 ROUGE-Lsum 35.1 BERTScore 62.8; SummScreen ROUGE-1 31.7 ROUGE-2 7.8 ROUGE-Lsum 19.8 BERTScore 58.3; TruthfulQA (GPT-judge) 67.7 (GPT-info 99.0). gpt-3.5-turbo USC: GSM8K acc 77.8; MATH acc 38.1; BIRD-SQL execution acc 45.5 valid-efficiency 48.8; ARCADE execution acc 30.1; TruthfulQA (GPT-judge) 82.5 (GPT-info 99.6). (All USC numbers taken from the paper's tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy (no reflection) baselines: PaLM 2-L greedy: GSM8K 85.7; MATH 30.8; GovReport ROUGE-1 38.8; SummScreen ROUGE-1 30.6; TruthfulQA GPT-judge 62.1. gpt-3.5-turbo greedy: GSM8K 73.4; MATH 33.2; BIRD-SQL execution acc 42.4; ARCADE 26.0; TruthfulQA GPT-judge 79.8. Random-selection baselines also reported and are generally below USC.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative results across multiple benchmarks show USC consistently improves over greedy decoding and random selection (examples: PaLM 2-L GSM8K 85.7 -> 90.2 with USC; GovReport ROUGE-1 38.8 -> 40.2; TruthfulQA GPT-judge 62.1 -> 67.7). Where standard self-consistency (SC) or execution-based voting applies, USC matches or closely approximates SC performance (e.g., PaLM 2-L GSM8K: SC 90.4 vs USC 90.2; BIRD-SQL SC-Exec 45.6 vs USC 45.5). Ablations (varying k, ordering) and comparisons to oracle selection are reported to justify gains and limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>USC requires additional LLM query (extra inference cost) and is bounded by LLM context length (limits number of candidate responses). USC does not provide an inherent confidence estimate like counting votes in SC. With more candidates (e.g., 16), USC sometimes degrades (GSM8K decreased with 16 samples), attributed to long-context understanding and imperfect counting. USC is an imperfect approximation of SC: match ratio to SC decreases with more samples and USC/SC sometimes choose different candidates (ties, format differences). Consistency as a criterion does not guarantee correctness; a notable gap remains to an oracle selector across tasks. Position bias and evaluator weaknesses are noted as potential issues (though ordering ablation shows small effect for their setup).</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>USC also includes a simple variant of changing the selection criterion (e.g., ask 'choose the most detailed' summary) which can yield further gains on summarization. The USC prompt and implementation details (k=8 by default, temperatures given) are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5418.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5418.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-optimizer (Yang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models as optimizers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative generate-and-improve approach mentioned in the paper where an LLM uses prior candidate trajectories to produce progressively better solutions for optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as optimizers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative LLM-based optimization / generate-then-reflect iterative improvement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Given a trajectory of previously generated solutions, the LLM iteratively produces improved solutions; used for prompt optimization and several mathematical optimization tasks (as described in the reference).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prompt optimization and classic mathematical optimization tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Optimization problems and prompt optimization as reported by the cited work (paper mentions effectiveness but does not provide numeric results here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>The paper cites Yang et al. (2023) as demonstrating effectiveness of iterative LLM-based optimization for prompt optimization and several mathematical optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5418.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5418.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-reasoning aggregation (Yoran et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Answering questions by meta-reasoning over multiple chains of thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned approach that aggregates multiple reasoning chains and prompts the LLM to generate an improved final response by meta-reasoning over candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Answering questions by meta-reasoning over multiple chains of thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generate multiple chains + prompt to synthesize/improve (meta-reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Aggregate multiple chain-of-thought or reasoning outputs and prompt the LLM to meta-reason over them to produce a single improved answer.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-chain reasoning / multi-hop question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop or complex question answering tasks where multiple reasoning chains are aggregated to produce a better final response (paper references this prior work as related).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited as prior work showing that prompting the LLM to generate a better final response from multiple chains can improve performance (no numeric details in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5418.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5418.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM self-critique / tool-interactive critiquing (Gou et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRITIC: Large language models can self-correct with tool-interactive critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that explores LLMs performing self-correction through interactive critique and tools; cited in the paper's related work as example of LLM-based improvement attempts and their challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Critic: Large language models can self-correct with tool-interactive critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Tool-interactive critiquing / self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use LLMs in a loop with external tools to critique and correct generated outputs; mentioned as related work on LLM self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mentioned in context of LLM self-correction efforts; specific task details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited as prior work demonstrating self-correction interactions, but the citing paper also notes other works that highlight weaknesses in LLM self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5418.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5418.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM self-correction limits (Huang et al., 2023b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models cannot self-correct reasoning yet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced study reporting weaknesses of LLMs in self-correcting reasoning errors, cited in this paper as motivation for favoring consistency-based selection which may be easier than direct correctness evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models cannot self-correct reasoning yet</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-correction / self-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Empirical finding that LLMs have difficulty reliably self-correcting reasoning chains; cited to contextualize the choice of consistency-evaluation rather than direct correctness evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Paper is cited as evidence that LLM-based self-correction on reasoning problems can be unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>This is cited as counter-evidence: the referenced work reports limitations in self-correction capabilities of LLMs for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Used by the authors to motivate USC: LLMs may be poor at judging correctness, but judging consistency among multiple responses can be easier and more reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Universal Self-Consistency for Large Language Model Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models as optimizers <em>(Rating: 2)</em></li>
                <li>Answering questions by meta-reasoning over multiple chains of thought <em>(Rating: 2)</em></li>
                <li>Towards expert-level medical question answering with large language models <em>(Rating: 2)</em></li>
                <li>Critic: Large language models can self-correct with tool-interactive critiquing <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5418",
    "paper_id": "paper-5f66d1a667eec13b5d337c3fc5619bcef95092bd",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "USC",
            "name_full": "Universal Self-Consistency",
            "brief_description": "A generate-then-evaluate selection method that samples multiple responses from an LLM and then prompts an LLM to select the single most 'consistent' response among candidates, enabling self-consistency for free-form generation without answer extraction or external rerankers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L; gpt-3.5-turbo",
            "model_description": "Instruction-tuned PaLM 2-L (Anil et al., 2023) and OpenAI gpt-3.5-turbo were used as the underlying LLMs for generation and for performing the USC selection; exact parameter counts are not specified in this paper.",
            "reflection_method_name": "Universal Self-Consistency (USC) — LLM-based consistency selection",
            "reflection_method_description": "Generate k candidate responses (the same LLM), concatenate them, and prompt the LLM to select the most consistent response (or another selection criterion, e.g., 'most detailed'); selection is a single additional LLM query that returns an index corresponding to the chosen candidate. No external reranker or execution results are required.",
            "num_iterations": 1,
            "task_name": "GSM8K; MATH; BIRD-SQL; ARCADE; GovReport; SummScreen; TruthfulQA",
            "task_description": "Mathematical reasoning (grade-school and competition math), text-to-SQL and Python code generation, long-context summarization, and open-ended question answering (truthfulness/informativeness).",
            "performance_with_reflection": "PaLM 2-L USC: GSM8K acc 90.2; MATH acc 37.4; GovReport ROUGE-1 40.2 ROUGE-2 17.4 ROUGE-Lsum 35.1 BERTScore 62.8; SummScreen ROUGE-1 31.7 ROUGE-2 7.8 ROUGE-Lsum 19.8 BERTScore 58.3; TruthfulQA (GPT-judge) 67.7 (GPT-info 99.0). gpt-3.5-turbo USC: GSM8K acc 77.8; MATH acc 38.1; BIRD-SQL execution acc 45.5 valid-efficiency 48.8; ARCADE execution acc 30.1; TruthfulQA (GPT-judge) 82.5 (GPT-info 99.6). (All USC numbers taken from the paper's tables.)",
            "performance_without_reflection": "Greedy (no reflection) baselines: PaLM 2-L greedy: GSM8K 85.7; MATH 30.8; GovReport ROUGE-1 38.8; SummScreen ROUGE-1 30.6; TruthfulQA GPT-judge 62.1. gpt-3.5-turbo greedy: GSM8K 73.4; MATH 33.2; BIRD-SQL execution acc 42.4; ARCADE 26.0; TruthfulQA GPT-judge 79.8. Random-selection baselines also reported and are generally below USC.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative results across multiple benchmarks show USC consistently improves over greedy decoding and random selection (examples: PaLM 2-L GSM8K 85.7 -&gt; 90.2 with USC; GovReport ROUGE-1 38.8 -&gt; 40.2; TruthfulQA GPT-judge 62.1 -&gt; 67.7). Where standard self-consistency (SC) or execution-based voting applies, USC matches or closely approximates SC performance (e.g., PaLM 2-L GSM8K: SC 90.4 vs USC 90.2; BIRD-SQL SC-Exec 45.6 vs USC 45.5). Ablations (varying k, ordering) and comparisons to oracle selection are reported to justify gains and limitations.",
            "limitations_or_failure_cases": "USC requires additional LLM query (extra inference cost) and is bounded by LLM context length (limits number of candidate responses). USC does not provide an inherent confidence estimate like counting votes in SC. With more candidates (e.g., 16), USC sometimes degrades (GSM8K decreased with 16 samples), attributed to long-context understanding and imperfect counting. USC is an imperfect approximation of SC: match ratio to SC decreases with more samples and USC/SC sometimes choose different candidates (ties, format differences). Consistency as a criterion does not guarantee correctness; a notable gap remains to an oracle selector across tasks. Position bias and evaluator weaknesses are noted as potential issues (though ordering ablation shows small effect for their setup).",
            "additional_notes": "USC also includes a simple variant of changing the selection criterion (e.g., ask 'choose the most detailed' summary) which can yield further gains on summarization. The USC prompt and implementation details (k=8 by default, temperatures given) are provided in the paper.",
            "uuid": "e5418.0",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM-as-optimizer (Yang et al., 2023)",
            "name_full": "Large language models as optimizers",
            "brief_description": "Iterative generate-and-improve approach mentioned in the paper where an LLM uses prior candidate trajectories to produce progressively better solutions for optimization tasks.",
            "citation_title": "Large language models as optimizers",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Iterative LLM-based optimization / generate-then-reflect iterative improvement",
            "reflection_method_description": "Given a trajectory of previously generated solutions, the LLM iteratively produces improved solutions; used for prompt optimization and several mathematical optimization tasks (as described in the reference).",
            "num_iterations": null,
            "task_name": "Prompt optimization and classic mathematical optimization tasks",
            "task_description": "Optimization problems and prompt optimization as reported by the cited work (paper mentions effectiveness but does not provide numeric results here).",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "evidence_of_improvement": "The paper cites Yang et al. (2023) as demonstrating effectiveness of iterative LLM-based optimization for prompt optimization and several mathematical optimization tasks.",
            "limitations_or_failure_cases": null,
            "uuid": "e5418.1",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Meta-reasoning aggregation (Yoran et al., 2023)",
            "name_full": "Answering questions by meta-reasoning over multiple chains of thought",
            "brief_description": "Mentioned approach that aggregates multiple reasoning chains and prompts the LLM to generate an improved final response by meta-reasoning over candidates.",
            "citation_title": "Answering questions by meta-reasoning over multiple chains of thought",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Generate multiple chains + prompt to synthesize/improve (meta-reasoning)",
            "reflection_method_description": "Aggregate multiple chain-of-thought or reasoning outputs and prompt the LLM to meta-reason over them to produce a single improved answer.",
            "num_iterations": null,
            "task_name": "Multi-chain reasoning / multi-hop question answering",
            "task_description": "Multi-hop or complex question answering tasks where multiple reasoning chains are aggregated to produce a better final response (paper references this prior work as related).",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "evidence_of_improvement": "Cited as prior work showing that prompting the LLM to generate a better final response from multiple chains can improve performance (no numeric details in this paper).",
            "limitations_or_failure_cases": null,
            "uuid": "e5418.2",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM self-critique / tool-interactive critiquing (Gou et al., 2023)",
            "name_full": "CRITIC: Large language models can self-correct with tool-interactive critiquing",
            "brief_description": "Referenced work that explores LLMs performing self-correction through interactive critique and tools; cited in the paper's related work as example of LLM-based improvement attempts and their challenges.",
            "citation_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Tool-interactive critiquing / self-correction",
            "reflection_method_description": "Use LLMs in a loop with external tools to critique and correct generated outputs; mentioned as related work on LLM self-correction.",
            "num_iterations": null,
            "task_name": null,
            "task_description": "Mentioned in context of LLM self-correction efforts; specific task details not provided in this paper.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "evidence_of_improvement": "Cited as prior work demonstrating self-correction interactions, but the citing paper also notes other works that highlight weaknesses in LLM self-correction.",
            "limitations_or_failure_cases": null,
            "uuid": "e5418.3",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM self-correction limits (Huang et al., 2023b)",
            "name_full": "Large language models cannot self-correct reasoning yet",
            "brief_description": "Referenced study reporting weaknesses of LLMs in self-correcting reasoning errors, cited in this paper as motivation for favoring consistency-based selection which may be easier than direct correctness evaluation.",
            "citation_title": "Large language models cannot self-correct reasoning yet",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Self-correction / self-evaluation",
            "reflection_method_description": "Empirical finding that LLMs have difficulty reliably self-correcting reasoning chains; cited to contextualize the choice of consistency-evaluation rather than direct correctness evaluation.",
            "num_iterations": null,
            "task_name": "Reasoning tasks",
            "task_description": "Paper is cited as evidence that LLM-based self-correction on reasoning problems can be unreliable.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "evidence_of_improvement": "This is cited as counter-evidence: the referenced work reports limitations in self-correction capabilities of LLMs for reasoning.",
            "limitations_or_failure_cases": "Used by the authors to motivate USC: LLMs may be poor at judging correctness, but judging consistency among multiple responses can be easier and more reliable.",
            "uuid": "e5418.4",
            "source_info": {
                "paper_title": "Universal Self-Consistency for Large Language Model Generation",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models as optimizers",
            "rating": 2
        },
        {
            "paper_title": "Answering questions by meta-reasoning over multiple chains of thought",
            "rating": 2
        },
        {
            "paper_title": "Towards expert-level medical question answering with large language models",
            "rating": 2
        },
        {
            "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "rating": 2
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2
        }
    ],
    "cost": 0.0140165,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Universal Self-Consistency for Large Language Model Generation</h1>
<p>Xinyun Chen ${ }^{<em>}$ Renat Aksitov</em> Uri Alon Jie Ren Kefan Xiao Pengcheng Yin<br>Sushant Prakash Charles Sutton Xuezhi Wang Denny Zhou<br>Google</p>
<h4>Abstract</h4>
<p>Self-consistency with chain-of-thought prompting ( CoT ) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, selfconsistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) have accomplished significant breakthroughs in a wide variety of domains, including mathematical reasoning (Cobbe et al., 2021; Wei et al., 2022; Lewkowycz et al., 2022), code generation (Chen et al., 2021; Austin et al., 2021; Li et al., 2022), and other text generation tasks (Bubeck et al., 2023; Anil et al., 2023; Touvron et al., 2023). Despite the rapid progress, the LLM-generated responses are still prone to errors when they get long. A long line of efforts have been devoted to improve the output quality by sampling multiple model responses and then selecting the final output based on certain criteria. For example, prior works have trained neural networks to rerank model outputs (Cobbe et al., 2021; Li et al., 2023b; Ni et al., 2023; Yin \&amp; Neubig, 2019; Zeng et al., 2022), and more recent works investigate using LLMs to score the responses (Fu et al., 2023; Liu et al., 2023; Wang et al., 2023a).</p>
<p>In this work, we consider the consistency among model responses as the criterion to select the model output, a generic metric that has enabled huge performance leaps in reasoning (Wang et al., 2022) and code generation (Li et al., 2022; Shi et al., 2022). In particular, self-consistency (Wang et al., 2022) with chain-of-thought prompting (Wei et al., 2022) boosts the performance on various benchmarks, by marginalizing latent reasoning paths through sampling which leads to select the final answer as the most common one. However, self-consistency can only be applied to tasks where the final answer can be aggregated via exact match, e.g., a single number for math problems.</p>
<p>To address this major limitation of self-consistency, we propose Universal Self-Consistency (USC) to support various applications, especially free-form generation tasks. Specifically, given multiple candidate responses, USC simply calls the LLM to select the most consistent response among them as the final output. Thus, USC eliminates the need of designing an answer extraction process, and is applicable to tasks with free-form answers. Although prior works have revealed weaknesses of LLMs for response selection, such as position bias (Wang et al., 2023b; Zheng et al., 2023b) and incorrectly judging the answer correctness (Huang et al., 2023b; Gou et al., 2023), intuitively, assessing the consistency among candidate answers is easier than measuring and comparing the answer quality.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We evaluate universal self-consistency on a wide range of tasks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) benchmarks for math problem solving, USC generally matches the performance of the standard self-consistency. On programming tasks including text-to-SQL generation (Li et al., 2023a) and Python code generation (Yin et al., 2023), USC matches the performance of execution-based consistency (Li et al., 2022; Shi et al., 2022), while USC does not require execution results to aggregate over candidate programs. Finally, USC also improves the performance for open-ended question answering (Lin et al., 2021) and long-context summarization (Huang et al., 2021; Chen et al., 2022b), where the standard self-consistency is not applicable. In addition to the performance gain, our evaluation also demonstrates that USC outputs highly match those of the standard self-consistency when the comparison is applicable, while it is robust to the ordering of candidate responses.</p>
<h1>2 BACKGROUND: SELF-CONSISTENCY</h1>
<p>Self-consistency (Wang et al., 2022) augments chain-of-thought prompting (Wei et al., 2022) by sampling multiple reasoning chains and then taking a majority vote on the final answer set. The intuition is that sometimes the greedily decoded reasoning process might not be the optimal one, hence it makes more sense to sample a diverse set of reasoning chains, and if some of them lead to the same answer, then we have a higher confidence that this consistent answer is the correct one. It has been shown that self-consistency improves the greedy chain-of-thought prompting by a large margin on a wide set of reasoning tasks.</p>
<p>Besides question answering tasks, consistency-based answer selection has also been applied to code generation (Shi et al., 2022; Li et al., 2022; Chen et al., 2019), which requires code execution. Specifically, we first execute all predicted programs on the given inputs, then programs with the same execution outputs are clustered together, assuming that they are semantically equivalent. Finally, we select the program belonging to the largest cluster as the final prediction. When the program inputs given in the task description are insufficient to distinguish between different predictions, this execution-based code selection is also often accompanied with a test case generation process to better examine the consistency (Li et al., 2022; Chen et al., 2022a; Huang et al., 2023a).</p>
<p>Despite the remarkable improvement, self-consistency is only applicable to problems with a unique and closed-form answer, e.g., when the final answer consists of a single number, because a majority vote needs to be taken over the final answer set. This significant requirement poses a challenge for tasks that require open-ended generations, such as summarization, creative writing, and open-ended question answering.</p>
<h2>3 UNIVERSAL SELF-CONSISTENCY</h2>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the Universal Self-Consistency workflow.
We present the overall workflow of universal self-consistency (USC) in Figure 1, which utilizes LLMs to enable self-consistency for a wide variety of tasks, especially free-form text generation. First, we sample multiple responses with the large language model. Afterward, to select one model response as the final answer, we concatenate all responses together, and then construct a prompt with an instruction asking the language model to select the most consistent response. In this way, USC obviates the necessity of counting the exact answer frequency as in the standard self-consistency, and relies on the LLM's own ability to measure the consistency among different responses. Although prior works show that LLMs sometimes have trouble evaluating the prediction correctness (Huang</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Examples of Universal Self-Consistency for answer selection from responses of diverse formats: (a) mathematical reasoning; and (b) open-ended question answering. Note that for the given open-ended question, the final answer is an entity list, where no two responses share the same predictions. Still, the LLM correctly selects the response where the individual entities in the predicted list appear most frequently in the candidate responses.
et al., 2023b; Gou et al., 2023), especially for reasoning problems, empirically we observe that LLMs are generally able to examine the response consistency across multiple tasks.</p>
<p>Consistency assessment with LLMs offers more flexibility for free-form generation. Figure 2 demonstrates example tasks where different consistency criteria are beneficial for response selection. Specifically, Figure 2a shows different model responses for a math problem, where the output formats are diverse and thus makes it challenging for rule-based methods to extract answers. Nonetheless, assuming that the final answers are correctly extracted, the consistency criterion still follows the standard self-consistency on mathematical reasoning, which is based on the exact match of the final answers represented as single numerical values. On the other hand, Figure 2b shows an example question where the final answer is an entity list. Despite that there is no response that is consistent with others based on the exact match, the LLM selects the response where each of the predicted entities appears most frequently among the candidate outputs. In Section 4, we further show that LLM can also examine the consistency among responses beyond the question answering tasks, including code generation without access to the execution outputs, and long-context summarization.</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 Evaluation Setup</h3>
<p>Benchmarks. We evaluate USC on the following variety of tasks:</p>
<ul>
<li>Mathematical reasoning benchmarks, including GSM8K (Cobbe et al., 2021), a dataset of 8,500 grade school math word problems, and MATH (Hendrycks et al., 2021), a dataset of 12,500 challenging mathematics problems from high school competitions.</li>
<li>Code generation benchmarks, including BIRD-SQL dataset (Li et al., 2023a) for text-to-SQL generation, and ARCADE dataset (Yin et al., 2023) for Python code generation in data science notebooks.</li>
<li>Long-context summarization, including the GovReport and SummScreen benchmarks from ZeroSCROLLS (Shaham et al., 2023). In GovReport (Huang et al., 2021), each input is a document containing $\sim 7,900$ words on average, and the reference output is an expert-written executive summary with $\sim 500$ words. In SummScreen (Chen et al., 2022b), every input is a transcript of a TV show episode with $\sim 5,600$ words, and each reference output is a $\sim 100$ words human-written recap of the episode. We follow Shaham et al. (2023) and measure ROUGE 1, ROUGE 2, and ROUGE-Lsum which measure n-gram overlap with the reference summary, and we also measure BERTScore F1 (Zhang et al., 2019).</li>
<li>TruthfulQA (Lin et al., 2021) benchmark for open-ended question answering, which contains 817 questions to test model's ability in generating truthful answers. To evaluate the answer's quality, we use the GPT-judge and GPT-info, which are GPT-3 models fine-tuned on human feedback data, provided by Lin et al. (2021). GPT-judge model outputs a binary rating for truthfulness, and GPT-info model outpus a binary rating for informativeness. It is shown that the GPT-3 models have higher accuracy in predicting human judgement than the automatic metrics ROUGE, BLEU, BLEURT.</li>
</ul>
<p>Decoding schemes. We compare USC to the following decoding schemes:</p>
<ul>
<li>Greedy decoding generates a single answer with the temperature 0 .</li>
<li>Random selects one answer randomly from multiple samples with temperature $&gt;0$.</li>
<li>SC (Wang et al., 2022) is the standard self-consistency decoding with answer extraction. We evaluate SC whenever applicable; for example, on reasoning benchmarks where the final answers can be compared through exact match.</li>
</ul>
<p>To enable a fair comparison, for sampling schemes (i.e., except greedy decoding), we always select the final answer from the same set of initial model responses. For code generation, we compare our approach to execution-based self-consistency (Shi et al., 2022; Li et al., 2022; Chen et al., 2019), where we select the code with the most common execution result. Both USC and execution-based self-consistency first filter out syntactically invalid candidate programs, and then perform the voting over the remaining ones. For ARCADE benchmark, we also evaluate a variant of the execution-based self-consistency with fuzzy matching as described in Yin et al. (2023), which implements a set of heuristics to determine whether the execution outputs of two programs are equivalent when they are not exact match.</p>
<p>Implementation details. We conduct experiments using instruction-tuned PaLM 2-L (Anil et al., 2023) and gpt-3.5-turbo models. Unless otherwise specified, the LLM generates 8 initial samples for both SC and USC. For mathematical reasoning, summarization and the ARCADE benchmark for Python code generation, the initial samples are generated with zero-shot prompting, thus the output formats are diverse. For BIRD-SQL, we used the 1-shot chain-of-thought prompt in Li et al. (2023a), which improves the performance. We also utilized a one-shot prompt for TruthfulQA to improve the quality of candidate responses. We set the temperature to be 0.6 for PaLM 2-L, and 1.0 for gpt-3.5-turbo.</p>
<h1>4.2 Main ReSults</h1>
<p>Mathematical reasoning. For mathematical reasoning benchmarks, we compare USC against the standard self-consistency in Table 1. For the standard self-consistency, we employ a regular expression matching to extract the final answer on GSM8K, and re-use the answer parsing code from (Zheng et al., 2023a) for MATH. Overall, USC consistently improves over the greedy decoding and random selection, and the performance is generally comparable to the standard self-consistency, which USC does not need answer parsing to perform the voting.</p>
<p>Code generation. Table 2 presents the results on BIRD-SQL and ARCADE respectively. On BIRD-SQL, besides the execution accuracy, we follow (Li et al., 2023a) to also evaluate the valid efficiency score, which measures the efficiency of the generated SQL queries. We show that USC matches the execution-based self-consistency performance on both benchmarks, while USC does not utilize code execution to perform the voting.</p>
<p>Summarization. Results for the summarization benchmarks are shown in Table 3. Since the generated summaries are in free-form, the standard self-consistency is not applicable. In GovReport, USC consistently improves over the baselines across all metrics. In Section 4.3, we further show that asking the model to choose the most detailed summary results in more performance gain.</p>
<p>TruthfulQA. Table 4 presents results on TruthfulQA, where SC is also not directly applicable because the generated answers are in free-form. Comparing with greedy decoding and random selection, USC-based answers have the highest truthfulness with both PaLM 2-L and gpt-3.5turbo. For informativeness which is considered as a secondary objective, USC-based answers have the highest score on PaLM 2-L and the second highest score ( 0.1 lower than the highest) on gpt-3.5-turbo. Considering that GPT-judge and GPT-info models have generally 90-95\% validation accuracy on rating prediction (Lin et al., 2021), the 0.1 difference is not considered significant.</p>
<p>Table 1: Accuracy on mathematical reasoning benchmarks. USC and SC consistently improve over the greedy decoding and random selection. USC performance is generally comparable to SC.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">30.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">28.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">SC (Wang et al., 2022)</td>
<td style="text-align: center;">$\mathbf{9 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 9}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">26.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">SC</td>
<td style="text-align: center;">$\mathbf{7 8 . 5}$</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">$\mathbf{3 8 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy on code generation benchmarks with gpt-3.5-turbo.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">Execution Accuracy</th>
<th style="text-align: center;">Valid Efficiency Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BIRD-SQL</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">44.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC-Exec</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">48.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: center;">ARCADE</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC-Exec (strict match)</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC-Exec (fuzzy match)</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Results on long-context summarization benchmarks with PaLM 2-L. Since the outputs are in free-form, the standard self-consistency is not applicable. USC consistently improves over the baselines on summary quality.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Approach</th>
<th>ROUGE-1</th>
<th>ROUGE-2</th>
<th>ROUGE-Lsum</th>
<th>BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td>GovReport</td>
<td>Greedy decoding</td>
<td>38.8</td>
<td>16.9</td>
<td>33.8</td>
<td>62.7</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>38.5</td>
<td>16.9</td>
<td>33.6</td>
<td>62.6</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{4 0 . 2}$</td>
<td>$\mathbf{1 7 . 4}$</td>
<td>$\mathbf{3 5 . 1}$</td>
<td>$\mathbf{6 2 . 8}$</td>
</tr>
<tr>
<td>SummScreen</td>
<td>Greedy decoding</td>
<td>30.6</td>
<td>7.5</td>
<td>19.1</td>
<td>$\mathbf{5 8 . 7}$</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>30.2</td>
<td>7.3</td>
<td>19.0</td>
<td>58.6</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{3 1 . 7}$</td>
<td>$\mathbf{7 . 8}$</td>
<td>$\mathbf{1 9 . 8}$</td>
<td>58.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy on the TruthfulQA benchmark. Since the answer is in free-form, the standard self-consistency is not applicable. USC overall has the highest truthfulness and informativeness over the baselines.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Approach</th>
<th>GPT-judge</th>
<th>GPT-info</th>
</tr>
</thead>
<tbody>
<tr>
<td>PaLM 2-L</td>
<td>Greedy decoding</td>
<td>62.1</td>
<td>95.1</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>62.9</td>
<td>94.6</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{6 7 . 7}$</td>
<td>$\mathbf{9 9 . 0}$</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>Greedy decoding</td>
<td>79.8</td>
<td>$\mathbf{9 9 . 7}$</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>80.6</td>
<td>99.3</td>
</tr>
<tr>
<td></td>
<td>USC</td>
<td>$\mathbf{8 2 . 5}$</td>
<td>99.6</td>
</tr>
</tbody>
</table>
<h1>4.3 Ablations</h1>
<p>Effect of response ordering. Prior works have shown that large language models can be affected by the order of candidate responses when used to evaluate their quality (Wang et al., 2023b; Zheng et al., 2023b). We examine the effect of response ordering by performing USC with 5 different random orders when concatenating all responses, and calculate the mean and standard deviation of the task results. From Table 5, we observe that the overall model performance remains similar with different response orders, suggesting the effect of response order is minimal.</p>
<p>Table 5: USC performance with random shuffling of original responses using PaLM 2-L. The mean and standard deviation are computed with 5 runs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">(a)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">(b)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">(c)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">ROUGE-Lsum</td>
<td style="text-align: center;">Metric</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">$89.7 \pm 0.3$</td>
<td style="text-align: center;">SummScreen</td>
<td style="text-align: center;">$31.6 \pm 0.3$</td>
<td style="text-align: center;">$19.5 \pm 0.2$</td>
<td style="text-align: center;">GPT-judge</td>
</tr>
<tr>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">$37.3 \pm 0.2$</td>
<td style="text-align: center;">GovReport</td>
<td style="text-align: center;">$40.0 \pm 0.1$</td>
<td style="text-align: center;">$34.9 \pm 0.2$</td>
<td style="text-align: center;">GPT-info</td>
</tr>
</tbody>
</table>
<p>Different number of responses. Next, we examine the effect of using different numbers of responses in USC. As shown in Figure 3, USC consistently benefits from more samples on TruthFulQA and BIRD-SQL. However, USC does not further improve the performance on SummScreen after 5 samples, and the accuracy on GSM8K decreases with 16 samples. This can be due to the weakness in long-context understanding when the prompt contains more candidate responses, and the imperfect counting ability of LLMs. Nevertheless, we consider utilizing a few samples (e.g., 8) a sweet spot to balance the task accuracy and compute cost, in which case USC reliably improves the performance across the board. In Section 4.4, we further compare the predictions from USC and SC to understand how using more candidate responses affects the results.</p>
<p>Criteria for response selection. One advantage of USC is its generality: the same criteria can be applied to various tasks, without any task-specific knowledge. Nonetheless, a minor task-specific</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Results on GSM8K and BIRD-SQL. The top numbers are USC accuracies, and the bottom numbers are the differences to SC accuracies.</p>
<p>Figure 3: USC results with different number of samples.
adaptation of the response selection instruction can further boost USC over the generic prompts. For example, Table 6 shows that asking the LLM to choose the most detailed response (rather than the most consistent one) results in gains of about 2 ROUGE-1 and ROUGE-Lsum points.</p>
<p>Table 6: Ablation on the response selection criterion on long-context summarization benchmarks with PaLM 2-L.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">ROUGE-2</th>
<th style="text-align: center;">ROUGE-Lsum</th>
<th style="text-align: center;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GovReport</td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">62.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC - most detailed</td>
<td style="text-align: center;">$\mathbf{4 2 . 4}$</td>
<td style="text-align: center;">$\mathbf{1 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 9}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 2}$</td>
</tr>
<tr>
<td style="text-align: center;">SummScreen</td>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">$\mathbf{5 8 . 3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">USC - most detailed</td>
<td style="text-align: center;">$\mathbf{3 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 3}$</td>
</tr>
</tbody>
</table>
<h1>4.4 Discussion: How Well does USC Match SC SElection?</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison of selections made by USC versus SC with PaLM 2-L. $k$ denotes the number of candidate responses for selection. "Tied votes" represents the case where the USC and SC select different responses, but both have the maximum votes.</p>
<p>We have demonstrated that on tasks where the standard self-consistency is applicable, USC and SC achieve comparable overall performance with 8 samples; however, USC fails to further improve the GSM8K performance with 16 samples. In this section, we look closer into the relationship between USC and SC, specifically how well is the alignment between their selected responses.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Accuracy distribution when USC selection doesn't match SC.</p>
<p>Figure 4 presents a breakdown analysis of USC predictions on mathematical reasoning benchmarks with 8 and 16 candidate responses, and Figure 5 further compares the performance of USC and SC when they select different responses. We observe that:</p>
<ul>
<li>The voting ties constitute a notable portion to the selection differences between USC and SC, especially with 8 candidate responses. Specifically, among all responses with the maximum votes, SC always selects the one with the smallest index, while USC can pick up alternative ones based on the response format.</li>
<li>The match ratio between USC and SC consistently surpasses their own task accuracies, which shows that the consistency criterion is easier to measure than the answer correctness.</li>
<li>Shifting from 8 to 16 samples, the USC-SC match ratio reduces, suggesting that USC behaves as an imperfect approximation of SC. However, the difference in response selection does not always lead to the performance decrease, as USC sometimes selects the correct response when SC fails.</li>
</ul>
<h1>5 Related Work</h1>
<p>Response reranking and selection for language models. Reranking is a common method to improve the generation quality in language models by sampling multiple outputs and applying a post-hoc criterion to rank them, which often requires an additional trained ranker and sometimes additional human labeled data. For example, Cobbe et al. (2021) use human labels to train a ranking model to verify whether each generated response is correct or not, and Shen et al. (2021) jointly train a generator and a ranker to improve performance for math tasks. Instead of training response generators and rankers as separate models, Thoppilan et al. (2022) finetune the dialog model to also predict the ratings of candidate responses with human-annotated judgements. For code generation, various reranker models have been designed (Ni et al., 2023; Yin \&amp; Neubig, 2019; Zeng et al., 2022), which typically utilize execution results and language-specific syntactic features to improve the ranking performance. In contrast with these prior works, USC does not require any additional labeled training data nor an external reranking model: the LLM that generated the initial outputs is the same one that selects the final answer.</p>
<p>Several consistency-based response selection approaches have been proposed in the literature, which typically include a voting procedure to select the most common response (Wang et al., 2022; Zhou et al., 2022; Wightman et al., 2023; Yue et al., 2023; Bertsch et al., 2023). Self-consistency (Wang et al., 2022) shows that with multiple responses generated for the same task, selecting the reasoning path leading to the most common final answer improves the chain-of-thought reasoning performance. The candidate responses can also come from different prompt variants corresponding to the same problem (Zhou et al., 2022; Wightman et al., 2023; Yue et al., 2023). To measure the pairwise similarity between candidate responses for open-ended generation tasks, Jain et al. (2023) propose the n-gram consistency score, and the consistency score for each response is computed as the sum of the pairwise similarity scores. For code generation, the consistency measurement is typically based on code execution, where the candidate program with the most common execution outputs is selected (Shi et al., 2022; Li et al., 2022; Chen et al., 2019). Besides the consistency of code execution, other works also examine the consistency between the code and the specification (Min</p>
<p>et al., 2023), and utilize it for reranking (Zhang et al., 2023a; Huang et al., 2023a). In this work, we directly instruct the LLM to perform consistency-based selection without an explicit definition of the pairwise similarity, and we demonsrate its applicability to a wide range of tasks.</p>
<p>Response improvement with multiple candidates. Some recent works demonstrate that the LLM can improve its prediction output on top of the candidate responses. Yang et al. (2023) show that given a trajectory of previously generated solutions, the LLM can iteratively produce better solutions for an optimization task, and they demonstrate the effectiveness of this LLM-based optimization process for prompt optimization and several classic mathematical optimization tasks. Other works aggregate multiple reasoning chains and prompts the LLM to generate a better final response, which shows performance improvement on multi-hop question answering (Yoran et al., 2023) and medical question answering (Singhal et al., 2023). Instead of asking the LLM to generate a better response, USC focuses on response selection, as the candidate responses usually already contain high-quality solutions to the underlying tasks. Meanwhile, performing the consistency-based selection is generally an easier task than improving the answer correctness, and we demonstrate that USC properly utilizes multiple responses to improve the performance across different tasks.</p>
<p>Large language models for response evaluation. The underlying assumption in our work is that LLMs are reflective enough to evaluate the consistency between multiple self-generated outputs. This assumption is related to recent findings which had shown that large language models can also be used for evaluating model-generated texts (Bubeck et al., 2023; Fu et al., 2023; Wang et al., 2023a; Zhang et al., 2023b). LLM-based evaluators have demonstrated some promising results, e.g., they can be used to evaluate natural language generations without human references, but some work has also shown that they might not correlate very well with human judgements and sometimes exhibit bias towards model-generated texts (Bubeck et al., 2023; Liu et al., 2023). Another line of work utilizes the prediction probability of the LLM to measure the quality of multiple choices (Ren et al., 2023; Adiwardana et al., 2020), and Lin et al. (2022) show promising results on arithmetic tasks where they prompt the LLM to directly output the level of confidence for its response. In this work, we show that LLMs not only can serve as evaluators, they can also improve their own output by sampling multiple responses and evaluating the consistency between them.</p>
<h1>6 Limitations and Future Work</h1>
<p>Despite that USC supports open-ended generation tasks and generally achieves comparable performance in those domains where the standard self-consistency can be applied, our current USC implementation has its own limitations compared to the extraction-based self-consistency approach.</p>
<p>First, while self-consistency can be applied to an arbitrary number of samples as long as the final answers can be extracted, the number of samples supported by USC is bounded by the context length of the underlying LLM. That said, to seek a balance between the task performance and the sampling cost, in practice the number of generated samples per task is not prohibitively large, thus the context length is generally sufficient to make best use of the samples.</p>
<p>Second, the voting mechanism in self-consistency inherently offers a measure of confidence or uncertainty for each response (Wang et al., 2022). However, universal self-consistency has not yet been developed to include the confidence estimation. We consider developing a calibration mechanism for USC as future work, where we can leverage the LLM to perform output clustering and pairwise self-consistency.</p>
<p>Also, USC requires an additional LLM query by design, which incurs additional inference costs. Given that our USC prompt only requires the LLM to generate a response index corresponding to the final answer, the USC output length is much shorter than any individual candidate response to select from. To further reduce the cost, one direction is to use a light-weight language model to conduct USC, and optimizes its efficiency regarding long-context encoding.</p>
<p>Finally, one common limitation of both the standard self-consistency and USC is about the consistencybased selection criterion. Specifically, although consistency is a generic and effective criterion, the most consistent response is not necessarily the best one. We observe that there is still a notable gap to oracle scores where we assume the access to an oracle reranker that always selects the best response, and we present the full results in Appendix A. In Section 4.3 we demonstrate that we can</p>
<p>design task-specific criteria to further improve the performance, and we consider refining the USC framework to further close the gap to the oracle performance as future work.</p>
<h1>7 CONCLUSION</h1>
<p>In this work, we presented Universal Self-Consistency (USC), which extends the standard selfconsistency to support free-form generation tasks. USC notably boosts the performance in diverse applications, and performs on par with the standard self-consistency on those tasks where answer extraction is feasible for voting. Besides addressing the limitations discussed in Section 6, we also consider mitigating the position bias and improving long-context understanding of LLMs as important future work that can further enhance the effectiveness and robustness of the USC scheme.</p>
<h2>REFERENCES</h2>
<p>Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like opendomain chatbot. arXiv preprint arXiv:2001.09977, 2020.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Amanda Bertsch, Alex Xie, Graham Neubig, and Matthew R Gormley. It's mbr all the way down: Modern generation techniques through the lens of minimum bayes risk. arXiv preprint arXiv:2310.01387, 2023.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.</p>
<p>Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022a.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. Summscreen: A dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8602-8615, 2022b.</p>
<p>Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2019.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire, 2023.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. Enhancing large language models in coding through multi-perspective self-consistency. arXiv preprint arXiv:2309.17272, 2023a.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023b.</p>
<p>Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1419-1436, 2021.</p>
<p>Siddhartha Jain, Xiaofei Ma, Anoop Deoras, and Bing Xiang. Self-consistency for open-ended generations. arXiv preprint arXiv:2307.06857, 2023.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.</p>
<p>Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sql. arXiv preprint arXiv:2305.03111, 2023a.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315-5333, 2023b.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023.</p>
<p>Marcus J Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail Kaiser, Suman Jana, and Baishakhi Ray. Beyond accuracy: Evaluating self-consistency of code large language models with identitychain. arXiv preprint arXiv:2310.14053, 2023.</p>
<p>Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. arXiv preprint arXiv:2302.08468, 2023.</p>
<p>Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for large language model planners. arXiv preprint arXiv:2307.01928, 2023.</p>
<p>Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding. arXiv preprint arXiv:2305.14196, 2023.</p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate \&amp; rank: A multi-task framework for math word problems. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2269-2279, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology. org/2021.findings-emnlp.195.</p>
<p>Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. Natural language to code translation with execution. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022.</p>
<p>Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617, 2023.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/abs/ 2201.08239.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary study, 2023a.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023b.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. 2022. URL https://arxiv.org/pdf/2201.11903.</p>
<p>Gwenyth Portillo Wightman, Alexandra DeLucia, and Mark Dredze. Strength in numbers: Estimating confidence of large language models by prompt agreement. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pp. 326-362, 2023.</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.</p>
<p>Pengcheng Yin and Graham Neubig. Reranking for neural semantic parsing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.</p>
<p>Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Oleksandr Polozov, and Charles Sutton. Natural language to code generation in interactive data science notebooks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023.</p>
<p>Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007, 2023.</p>
<p>Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. Large language model cascades with mixture of thoughts representations for cost-efficient reasoning. arXiv preprint arXiv:2310.03094, 2023.</p>
<p>Lu Zeng, Sree Hari Krishnan Parthasarathi, and Dilek Hakkani-Tur. N-best hypotheses reranking for text-to-sql systems. arXiv preprint arXiv:2210.10668, 2022.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019.</p>
<p>Tianyi Zhang, Tao Yu, Tatsunori Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, and Sida Wang. Coder reviewer reranking for code generation. In International Conference on Machine Learning, pp. 41832-41846. PMLR, 2023a.</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862, 2023b.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023a.
Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. On large language models’ selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882, 2023b.
Chunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Prompt consistency for zero-shot task generalization. arXiv preprint arXiv:2205.00049, 2022.</p>
<h1>A Comparison to Oracle Selection</h1>
<p>Tables 7, 8, 9, 10 and 11 compare the results of different approaches to the oracle performance, which selects the best response among candidates for each task. The oracle selection is from the same 8 samples as SC and USC. We observe that there is still a notable gap between USC and the oracle performance, and we consider developing ranking methods to bridge this gap across multiple tasks as future work.</p>
<p>Table 7: Comparison to the oracle selection on mathematical reasoning benchmarks. The results were obtained with PaLM 2-L.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">30.8</td>
</tr>
<tr>
<td style="text-align: left;">SC (Wang et al., 2022)</td>
<td style="text-align: center;">$\mathbf{9 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">57.2</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparison to the oracle selection on BIRD-SQL benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">Execution Accuracy</th>
<th style="text-align: center;">Valid Efficiency Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: left;">SC-Exec</td>
<td style="text-align: center;">$\mathbf{4 5 . 6}$</td>
<td style="text-align: center;">48.1</td>
</tr>
<tr>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">$\mathbf{4 8 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">55.7</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison to the oracle selection on ARCADE benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">Execution Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Greedy decoding</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: left;">SC-Exec (strict match)</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: left;">SC-Exec (fuzzy match)</td>
<td style="text-align: center;">$\mathbf{3 0 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">USC</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: left;">Oracle</td>
<td style="text-align: center;">40.5</td>
</tr>
</tbody>
</table>
<h2>B EXAMPLES OF USC PROMPTS</h2>
<p>Figures 6 and 7 present examples of full USC prompts with candidate responses for different tasks.</p>
<p>Table 10: Comparison to the oracle selection on long-context summarization benchmarks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">ROUGE-2</th>
<th style="text-align: center;">ROUGE-Lsum</th>
<th style="text-align: center;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GovReport</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">62.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">62.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">64.7</td>
</tr>
<tr>
<td style="text-align: center;">SummScreen</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">58.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">60.6</td>
</tr>
</tbody>
</table>
<p>Table 11: Comparison to the oracle selection on TruthfulQA benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">GPT-judge</th>
<th style="text-align: center;">GPT-info</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PaLM 2-L</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">95.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">99.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">Greedy decoding</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">99.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">USC</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">99.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">100.0</td>
</tr>
</tbody>
</table>
<p>I have generated the following responses to the question: The three-digit number "ab5" is divisible by 3. How many different three-digit numbers can "ab5" represent?</p>
<p>Response 0: A number is divisible by 3 if the sum of its digits is divisible by 3 . In this case, the sum of the digits of "ab5" must be divisible by 3 , so $\mathrm{a}+\mathrm{b}+5$ must be divisible by 3 .
The smallest possible sum with $\mathrm{a}+\mathrm{b}+5$ is $1+0+5=6$, which is divisible by 3 . The largest possible sum is $9+7+5=21$, which is also divisible by 3 .
Therefore, the possible values of $\mathrm{a}+\mathrm{b}$ can range from 1 to 16 , with each increment of 3 leading to another possible sum. There are 6 such increments $(16-1) / 3+1=6$, so there are 6 possible values for $\mathrm{a}+\mathrm{b}$.
For each possible value of $\mathrm{a}+\mathrm{b}$, there are multiple combinations of digits that add up to that value. For example, if $\mathrm{a}+\mathrm{b}=4$, possible combinations are $(1,3),(2,2),(3,1)$ and $(4,0)$.
Therefore, there are 30 different three-digit numbers that "ab5" can represent. $(1+4+7+9+6+3$ $=30)$
In general, any three-digit number of the form "ab5" where $\mathrm{a}+\mathrm{b}+5$ is divisible by 3 will be divisible by 3 .
Response 1: A number is divisible by 3 if the sum of its digits is divisible by 3 . Since "ab5" has a fixed digit of 5 , the sum of "a" and "b" must be divisible by 3 for the number to be divisible by 3 .
There are 10 possible digits: $0,1,2,3,4,5,6,7,8$, and 9 .
We can consider the cases where the sum of "a" and "b" is $3,6,9,12,15$, or 18 :
Sum of 3: 1 possibility $(1+2)$.
Sum of 6: 2 possibilities $(1+5,2+4)$.
Sum of 9: 3 possibilities $(1+8,2+7,3+6)$.
Sum of 12: 4 possibilities $(3+9,4+8,5+7,6+6)$.
Sum of 15: 4 possibilities $(4+11,5+10,6+9,7+8)$.
Sum of 18: 2 possibilities $(9+9,8+10)$.
Therefore, there are a total of $1+2+3+4+4+2=16$ possibilities for "a" and "b."
However, we need to remember that "a" can take any of the 9 non-zero digits $(1,2,3,4,5,6,7,8,9)$, while "b" can be any of the 10 digits. So, we need to double the total number of possibilities for "a" and "b."
Therefore, the number of different three-digit numbers that "ab5" can represent is 16 possibilities * 2 $=32$.
So the answer is 32 .
Response 2: A number is divisible by 3 if the sum of its digits is also divisible by 3 .
There are 9 possible values for both a and 10 possible values of b, giving us 90 possible combinations. However, only the combinations where $\mathrm{a}+\mathrm{b}+5$ is a multiple of 3 will be valid.
We can list out the possible combinations:
$\mathrm{a}+\mathrm{b}=1:(1,0) \mathrm{a}+\mathrm{b}=4:(1,3),(2,2),(3,1),(4,0) \mathrm{a}+\mathrm{b}=7:(1,6),(2,5),(3,4),(4,3),(5,2),(6$, $1),(7,0) \mathrm{a}+\mathrm{b}=10:(1,9),(2,8),(3,7),(4,6),(5,5),(6,4),(7,3),(8,2),(9,1) \mathrm{a}+\mathrm{b}=13:(4,9),(5$, $8),(6,7),(7,6),(8,5),(9,4) \mathrm{a}+\mathrm{b}=16:(7,9),(8,8),(9,7)$ This gives a total of 30 different pairs of values for a and b. Therefore, there are 30 different three-digit numbers that "ab5" can represent. $1+4+7+9+6+3=30$
I hope this helps! Let me know if you have any other questions.
...
Evaluate these responses.
Select the most consistent response based on majority consensus.
Start your answer with "The most consistent response is Response X" (without quotes).
Figure 6: A full example USC prompt for mathematical reasoning. This corresponds to the question in Figure 2a.</p>
<p>I have generated the following responses to the question: Where do people drink less coffee than they do in Mexico?</p>
<p>Response 0: There are several countries where people drink less coffee than they do in Mexico. Some examples include Japan, China and the United Kingdom.
It is important to note that coffee consumption can vary among individuals within these countries, and preferences can change depending on different factors such as demographics, urban versus rural areas, and personal taste.
Response 1: People in countries like Japan, China, and India typically drink less coffee than they do in Mexico.</p>
<ol>
<li>Japan: Tea, particularly green tea, is more popular than coffee in Japan. Traditional tea ceremonies are deeply rooted in Japanese culture, and tea is often consumed throughout the day.</li>
<li>China: Tea is also the preferred hot beverage in China. Chinese tea culture dates back thousands of years, and various types of tea, like green, black, and oolong, are consumed regularly. Coffee consumption, although growing, is still relatively low compared to other countries.</li>
<li>India: India has a strong culture of tea consumption. Chai, which is a spiced milk tea, is a staple beverage across the country. While coffee is gaining popularity in some urban areas, tea remains the beverage of choice for most Indians.
Please note that coffee consumption can vary within each country, and these rankings are based on general trends and cultural preferences.
Response 2: There are several countries where people generally drink less coffee compared to Mexico. Some of these countries include:</li>
<li>Japan: While coffee has gained popularity in recent years, traditional tea culture remains strong in Japan, resulting in lower coffee consumption overall.</li>
<li>China: Despite having a large population, tea is the preferred hot beverage in China, and coffee consumption is relatively low.</li>
<li>Saudi Arabia: Traditional Arabic coffee (qahwa) is popular, but in terms of overall coffee consumption, it is lower compared to Mexico.</li>
<li>India: Tea is the dominant beverage in India, and coffee consumption is significantly lower in comparison.
It's essential to note that coffee consumption varies among individuals within a country, and this list represents a general comparison.
...</li>
</ol>
<p>Evaluate these responses.
Select the most consistent response based on majority consensus.
Start your answer with "The most consistent response is Response X" (without quotes).
Figure 7: A full example USC prompt for TruthfulQA. This corresponds to the question in Figure 2b.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
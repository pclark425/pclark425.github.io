<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6583 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6583</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6583</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-f4ba3aabd5cc4f021f2b316ab7f8d46635d1018a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f4ba3aabd5cc4f021f2b316ab7f8d46635d1018a" target="_blank">Been There, Done That: Meta-Learning with Episodic Recall</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a formalism for generating open-ended yet repetitious environments, then develops a meta-learning architecture for solving these environments that melds the standard LSTM working memory with a differentiable neural episodic memory.</p>
                <p><strong>Paper Abstract:</strong> Meta-learning agents excel at rapidly learning new tasks from open-ended task distributions; yet, they forget what they learn about each task as soon as the next begins. When tasks reoccur - as they do in natural environments - metalearning agents must explore again instead of immediately exploiting previously discovered solutions. We propose a formalism for generating open-ended yet repetitious environments, then develop a meta-learning architecture for solving these environments. This architecture melds the standard LSTM working memory with a differentiable neural episodic memory. We explore the capabilities of agents with this episodic LSTM in five meta-learning environments with reoccurring tasks, ranging from bandits to navigation and stochastic sequential decision problems.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6583.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6583.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>epL2RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>episodic LSTM within Learning-to-Reinforcement-Learn (epL2RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-reinforcement-learning agent that augments an LSTM-based meta-learner with an episodic memory (DND) storing LSTM cell states keyed by context embeddings and reinstating retrieved cell states into the LSTM through a learned multiplicative reinstatement gate (r-gate).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>epL2RL (episodic LSTM + DND)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An L2RL (LSTM) meta-RL agent augmented with an episodic memory module (Differentiable Neural Dictionary) that stores key/value pairs (context key -> LSTM cell-state value). On each timestep the agent queries the DND (kNN, k=1) using a context embedding, retrieves a stored cell-state c_ep, and reinstates it into the LSTM cell update via a learned reinstatement gate r_t (multiplicative). Writing: at episode end the current LSTM cell state is written into the DND under the current context key.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>differentiable neural dictionary (DND) episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Key: context embeddings (barcodes or pretrained Omniglot embeddings). Value: LSTM cell state vectors (working memory states).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>k-nearest-neighbor lookup over keys (k=1) using cosine distance and normalized inverse kernel; retrieved value c_ep is added to LSTM cell state gated by learned reinstatement gate r_t; writes occur at end of each episode (store key -> current cell state).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Barcode Bandits; Omniglot Bandits; Compositional Bandits; Contextual Water Maze; Episodic Two-Step Task</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>meta-reinforcement learning (contextual bandits, multi-state navigation MDPs, decision-making two-step task)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: epL2RL substantially outperforms L2RL baselines across all five experiments: lower cumulative regret on Omniglot bandits that decreases with number of exposures; outperforms classical bandit algorithms (Gittins, UCB, Thompson) after a few exposures; improves cumulative reward and reduces steps-to-goal in contextual water maze; achieves higher reward and evidence of episodic model-based control in episodic two-step task. (No single numeric aggregate metric provided in text.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Qualitative: The L2RL baseline (no external episodic memory) does not show decreasing regret with repeated exposures (no benefit from reoccurrence); L2RL+Context (barcode/embedding as parametric input) performed worse than epL2RL and in some cases failed because large context embeddings drowned out reward/action signals. Feeding retrieved vector c_ep as an ordinary LSTM input (instead of reinstatement) reduced performance to chance in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average episodic reward; cumulative regret (bandits); steps-to-goal (water maze); fitted choice-model weights for IMF/IMB/EMF/EMB in two-step task</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Reported trade-offs include: (1) a reinstatement-based integration (adding retrieved cell state into LSTM via r-gate) works much better than passing retrieved vector as additional LSTM input — the latter harmed performance unless aggressively compressed (<~5 dims); (2) reliance on context embeddings (pretrained Omniglot embeddings used) — perception/query learning not fully addressed; (3) DND was cleared at epoch boundaries in experiments (design choice), implying practical considerations for long-term scaling; (4) r-gate scalar differences between successful and unsuccessful trials were statistically significant but small in absolute magnitude, suggesting subtlety in controller behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Feeding retrieved episodic vector c_ep directly as an extra LSTM input performed very poorly (reduced to chance) unless the retrieved vector was very aggressively compressed; L2RL+Context (parametric mapping from context to behavior) failed when context embeddings were high-dimensional; the paper does not demonstrate online learning of keys/embeddings (they used pretrained Omniglot embeddings), and episode boundaries were explicit — real-world continuous saving/forgetting policies were not solved here. The r-gate differences, while significant, were of very small absolute magnitude, indicating limited interpretability of gating as sole mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Ritter, S., Wang, J. X., Kurth-Nelson, Z., Jayakumar, S. M., Blundell, C., Pascanu, R., & Botvinick, M. (2018). Been There, Done That: Meta-Learning with Episodic Recall. Proceedings of the 35th International Conference on Machine Learning (ICML), PMLR 80.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Been There, Done That: Meta-Learning with Episodic Recall', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6583.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6583.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Neural Dictionary (DND) episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external key-value episodic memory module that stores keys and values in an array and retrieves values via k-nearest-neighbor lookup over keys; used here to store context embeddings as keys and LSTM cell states as values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural episodic control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DND (memory module used by epL2RL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory module that stores key-value pairs; in this paper keys are context embeddings (barcodes or Omniglot embeddings) and values are LSTM cell states. Retrieval uses kNN (k=1) with cosine distance and normalized inverse kernel; retrieval is differentiable in the sense that gradients can flow to the network that produces keys (though in experiments some embeddings were pretrained). Writes occur at episode-end (write current cell state under current key).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external key-value episodic memory (Differentiable Neural Dictionary)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Key: context embeddings (binary barcodes or pretrained Omniglot 128-dim embeddings). Value: LSTM cell-state vectors (50-dim mentioned for retrieved vector in ablation experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>k-nearest-neighbor search (k=1) over stored keys using cosine distance and normalized inverse kernel; retrieval returns single stored cell-state which is reinstated into LSTM (via r-gate). Writes performed at episode end; DND cleared at epoch end in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used as memory component across: Barcode Bandits, Omniglot Bandits, Compositional Bandits, Contextual Water Maze, Episodic Two-Step Task</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>episodic retrieval for meta-RL tasks (contextual bandits, navigation, sequential decision)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When used as reinstated episodic store (epL2RL): improved reward, lower regret, faster goal finding, and enabled episodic model-based control (see epL2RL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without DND (plain L2RL): no improvement in regret with repeated exposures; cannot exploit task reoccurrence to immediately re-use past solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average episodic reward; cumulative regret; steps-to-goal</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Design/training trade-offs: (1) Feeding retrieved DND value as a raw extra input to LSTM harmed learning unless aggressively compressed; (2) Pretraining context embeddings was used in some tasks (Omniglot), showing an engineering dependency; (3) Authors note computationally feasible contrastive/auxiliary training could be applied to DND contents but was not explored here; (4) DND cleared at epoch end in experiments (practical design choice impacting long-term retention).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>DND retrieval must be combined appropriately (reinstatement via r-gate) — naive concatenation/input of retrieved vector failed; learned query/key generation was not solved online in these experiments (pretrained embeddings used for Omniglot); scalability and long-term forgetting policies (when to write or forget) were left as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Pritzel, A., Uria, B., Srinivasan, S., Puigdomènech Badia, A., Vinyals, O., Hassabis, D., Wierstra, D., & Blundell, C. (2017). Neural episodic control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Been There, Done That: Meta-Learning with Episodic Recall', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6583.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6583.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L2RL (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning to Reinforcement Learn (L2RL) agent (LSTM meta-learner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based meta-RL agent that learns to explore and exploit within episodes by using its LSTM cell state as working memory; does not include an external episodic memory for cross-episode recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to reinforcement learn</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>L2RL (LSTM meta-learner baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LSTM-based meta-learning agent (Wang et al., 2016) trained to implement learning algorithms within its recurrent dynamics; relies on LSTM working memory (cell state) within episodes but resets cell state between episodes so it cannot recall across episode boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory only (LSTM cell state, not persistent episodic store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Transient LSTM cell state (working memory) that is reset between episodes; no persistent key-value store.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Internal LSTM gating (input/forget gates) for within-episode storage; no cross-episode retrieval mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used as baseline across: Barcode Bandits; Omniglot Bandits; Compositional Bandits; Contextual Water Maze; Episodic Two-Step Task</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>meta-reinforcement learning (contextual bandits, navigation, decision-making)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>N/A (no external episodic memory).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline performance reported in paper: performs comparably to classical bandit algorithms on first exposures but does not improve with repeated exposures and fails to immediately exploit previously discovered solutions when tasks reoccur.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average episodic reward; cumulative regret; steps-to-goal</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>L2RL can learn strong within-episode learning algorithms, but the lack of persistent episodic storage prevents immediate reuse of prior solutions across episodes; simpler (no external memory) so fewer architectural/training complications but limited in environments with task reoccurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to exploit task reoccurrence because LSTM cell state is reset between episodes — no mechanism to recall previously discovered policies across episodes; adding raw high-dimensional context inputs (L2RL+Context) can overwhelm reward/action signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C., Kumaran, D., & Botvinick, M. (2016). Learning to reinforcement learn.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Been There, Done That: Meta-Learning with Episodic Recall', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6583.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6583.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>epL2RL (ablation: c_ep as input)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>epL2RL variant with retrieved episodic value fed as standard LSTM input (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An architectural ablation where the retrieved DND value c_ep is passed into the LSTM as an additional input rather than being reinstated into the cell state via the r-gate; authors report this harmed performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>epL2RL (retrieved vector as input variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same as epL2RL but instead of adding retrieved cell-state to the LSTM cell via r-gate, the retrieved vector c_ep is concatenated/provided as an extra input to the LSTM; authors evaluated this variant and found severe performance degradation unless the retrieved vector was very aggressively compressed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>DND episodic memory (retrieval used but integrated differently)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Same as epL2RL: context keys and LSTM cell-state values; retrieved vector (50-dim in ablation experiments) provided as raw input.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>kNN retrieval from DND; retrieved vector concatenated to input channels of LSTM rather than reinstated via gated addition.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tested as ablation on tasks such as Barcode Bandits and others (reported poor performance across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>meta-reinforcement learning (ablation study)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Poor — in multiple experiments feeding the retrieved vector as an additional input reduced performance to chance; only succeeded when the retrieved vector was compressed to a very small dimensionality (<~5 dims) in limited tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Better than this ablated variant: the reinstatement approach (r-gate) performed substantially better than raw-input integration; L2RL baseline sometimes outperformed this ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average episodic reward; qualitative comparisons (chance-level performance reported for some tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Implies a trade-off between how retrieved memories are integrated: reinstatement (add to cell + gating) is effective; naive input concatenation introduces large noisy inputs (esp. for high-dimensional retrieved vectors) that impede learning unless compressed, but aggressive compression harms capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>This integration method failed in experiments (reduced to chance) unless dimensionality drastically reduced; suggests sensitivity to integration design and dimensionality of retrieved vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Been There, Done That: Meta-Learning with Episodic Recall', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural episodic control <em>(Rating: 2)</em></li>
                <li>Model-free episodic control <em>(Rating: 2)</em></li>
                <li>Learning to reinforcement learn <em>(Rating: 2)</em></li>
                <li>One-shot learning with memory-augmented neural networks <em>(Rating: 1)</em></li>
                <li>Hybrid computing using a neural network with dynamic external memory <em>(Rating: 1)</em></li>
                <li>Learning to remember rare events <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6583",
    "paper_id": "paper-f4ba3aabd5cc4f021f2b316ab7f8d46635d1018a",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "epL2RL",
            "name_full": "episodic LSTM within Learning-to-Reinforcement-Learn (epL2RL)",
            "brief_description": "A meta-reinforcement-learning agent that augments an LSTM-based meta-learner with an episodic memory (DND) storing LSTM cell states keyed by context embeddings and reinstating retrieved cell states into the LSTM through a learned multiplicative reinstatement gate (r-gate).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "epL2RL (episodic LSTM + DND)",
            "agent_description": "An L2RL (LSTM) meta-RL agent augmented with an episodic memory module (Differentiable Neural Dictionary) that stores key/value pairs (context key -&gt; LSTM cell-state value). On each timestep the agent queries the DND (kNN, k=1) using a context embedding, retrieves a stored cell-state c_ep, and reinstates it into the LSTM cell update via a learned reinstatement gate r_t (multiplicative). Writing: at episode end the current LSTM cell state is written into the DND under the current context key.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "differentiable neural dictionary (DND) episodic memory",
            "memory_representation": "Key: context embeddings (barcodes or pretrained Omniglot embeddings). Value: LSTM cell state vectors (working memory states).",
            "memory_access_mechanism": "k-nearest-neighbor lookup over keys (k=1) using cosine distance and normalized inverse kernel; retrieved value c_ep is added to LSTM cell state gated by learned reinstatement gate r_t; writes occur at end of each episode (store key -&gt; current cell state).",
            "task_name": "Barcode Bandits; Omniglot Bandits; Compositional Bandits; Contextual Water Maze; Episodic Two-Step Task",
            "task_category": "meta-reinforcement learning (contextual bandits, multi-state navigation MDPs, decision-making two-step task)",
            "performance_with_memory": "Qualitative: epL2RL substantially outperforms L2RL baselines across all five experiments: lower cumulative regret on Omniglot bandits that decreases with number of exposures; outperforms classical bandit algorithms (Gittins, UCB, Thompson) after a few exposures; improves cumulative reward and reduces steps-to-goal in contextual water maze; achieves higher reward and evidence of episodic model-based control in episodic two-step task. (No single numeric aggregate metric provided in text.)",
            "performance_without_memory": "Qualitative: The L2RL baseline (no external episodic memory) does not show decreasing regret with repeated exposures (no benefit from reoccurrence); L2RL+Context (barcode/embedding as parametric input) performed worse than epL2RL and in some cases failed because large context embeddings drowned out reward/action signals. Feeding retrieved vector c_ep as an ordinary LSTM input (instead of reinstatement) reduced performance to chance in some tasks.",
            "has_comparative_results": true,
            "performance_metric": "Average episodic reward; cumulative regret (bandits); steps-to-goal (water maze); fitted choice-model weights for IMF/IMB/EMF/EMB in two-step task",
            "tradeoffs_reported": "Reported trade-offs include: (1) a reinstatement-based integration (adding retrieved cell state into LSTM via r-gate) works much better than passing retrieved vector as additional LSTM input — the latter harmed performance unless aggressively compressed (&lt;~5 dims); (2) reliance on context embeddings (pretrained Omniglot embeddings used) — perception/query learning not fully addressed; (3) DND was cleared at epoch boundaries in experiments (design choice), implying practical considerations for long-term scaling; (4) r-gate scalar differences between successful and unsuccessful trials were statistically significant but small in absolute magnitude, suggesting subtlety in controller behavior.",
            "limitations_or_failure_cases": "Feeding retrieved episodic vector c_ep directly as an extra LSTM input performed very poorly (reduced to chance) unless the retrieved vector was very aggressively compressed; L2RL+Context (parametric mapping from context to behavior) failed when context embeddings were high-dimensional; the paper does not demonstrate online learning of keys/embeddings (they used pretrained Omniglot embeddings), and episode boundaries were explicit — real-world continuous saving/forgetting policies were not solved here. The r-gate differences, while significant, were of very small absolute magnitude, indicating limited interpretability of gating as sole mechanism.",
            "citation": "Ritter, S., Wang, J. X., Kurth-Nelson, Z., Jayakumar, S. M., Blundell, C., Pascanu, R., & Botvinick, M. (2018). Been There, Done That: Meta-Learning with Episodic Recall. Proceedings of the 35th International Conference on Machine Learning (ICML), PMLR 80.",
            "uuid": "e6583.0",
            "source_info": {
                "paper_title": "Been There, Done That: Meta-Learning with Episodic Recall",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "DND",
            "name_full": "Differentiable Neural Dictionary (DND) episodic memory",
            "brief_description": "An external key-value episodic memory module that stores keys and values in an array and retrieves values via k-nearest-neighbor lookup over keys; used here to store context embeddings as keys and LSTM cell states as values.",
            "citation_title": "Neural episodic control",
            "mention_or_use": "use",
            "agent_name": "DND (memory module used by epL2RL)",
            "agent_description": "A memory module that stores key-value pairs; in this paper keys are context embeddings (barcodes or Omniglot embeddings) and values are LSTM cell states. Retrieval uses kNN (k=1) with cosine distance and normalized inverse kernel; retrieval is differentiable in the sense that gradients can flow to the network that produces keys (though in experiments some embeddings were pretrained). Writes occur at episode-end (write current cell state under current key).",
            "model_size": null,
            "memory_used": true,
            "memory_type": "external key-value episodic memory (Differentiable Neural Dictionary)",
            "memory_representation": "Key: context embeddings (binary barcodes or pretrained Omniglot 128-dim embeddings). Value: LSTM cell-state vectors (50-dim mentioned for retrieved vector in ablation experiments).",
            "memory_access_mechanism": "k-nearest-neighbor search (k=1) over stored keys using cosine distance and normalized inverse kernel; retrieval returns single stored cell-state which is reinstated into LSTM (via r-gate). Writes performed at episode end; DND cleared at epoch end in experiments.",
            "task_name": "Used as memory component across: Barcode Bandits, Omniglot Bandits, Compositional Bandits, Contextual Water Maze, Episodic Two-Step Task",
            "task_category": "episodic retrieval for meta-RL tasks (contextual bandits, navigation, sequential decision)",
            "performance_with_memory": "When used as reinstated episodic store (epL2RL): improved reward, lower regret, faster goal finding, and enabled episodic model-based control (see epL2RL).",
            "performance_without_memory": "Without DND (plain L2RL): no improvement in regret with repeated exposures; cannot exploit task reoccurrence to immediately re-use past solutions.",
            "has_comparative_results": true,
            "performance_metric": "Average episodic reward; cumulative regret; steps-to-goal",
            "tradeoffs_reported": "Design/training trade-offs: (1) Feeding retrieved DND value as a raw extra input to LSTM harmed learning unless aggressively compressed; (2) Pretraining context embeddings was used in some tasks (Omniglot), showing an engineering dependency; (3) Authors note computationally feasible contrastive/auxiliary training could be applied to DND contents but was not explored here; (4) DND cleared at epoch end in experiments (practical design choice impacting long-term retention).",
            "limitations_or_failure_cases": "DND retrieval must be combined appropriately (reinstatement via r-gate) — naive concatenation/input of retrieved vector failed; learned query/key generation was not solved online in these experiments (pretrained embeddings used for Omniglot); scalability and long-term forgetting policies (when to write or forget) were left as future work.",
            "citation": "Pritzel, A., Uria, B., Srinivasan, S., Puigdomènech Badia, A., Vinyals, O., Hassabis, D., Wierstra, D., & Blundell, C. (2017). Neural episodic control.",
            "uuid": "e6583.1",
            "source_info": {
                "paper_title": "Been There, Done That: Meta-Learning with Episodic Recall",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "L2RL (baseline)",
            "name_full": "Learning to Reinforcement Learn (L2RL) agent (LSTM meta-learner)",
            "brief_description": "An LSTM-based meta-RL agent that learns to explore and exploit within episodes by using its LSTM cell state as working memory; does not include an external episodic memory for cross-episode recall.",
            "citation_title": "Learning to reinforcement learn",
            "mention_or_use": "use",
            "agent_name": "L2RL (LSTM meta-learner baseline)",
            "agent_description": "An LSTM-based meta-learning agent (Wang et al., 2016) trained to implement learning algorithms within its recurrent dynamics; relies on LSTM working memory (cell state) within episodes but resets cell state between episodes so it cannot recall across episode boundaries.",
            "model_size": null,
            "memory_used": false,
            "memory_type": "working memory only (LSTM cell state, not persistent episodic store)",
            "memory_representation": "Transient LSTM cell state (working memory) that is reset between episodes; no persistent key-value store.",
            "memory_access_mechanism": "Internal LSTM gating (input/forget gates) for within-episode storage; no cross-episode retrieval mechanism.",
            "task_name": "Used as baseline across: Barcode Bandits; Omniglot Bandits; Compositional Bandits; Contextual Water Maze; Episodic Two-Step Task",
            "task_category": "meta-reinforcement learning (contextual bandits, navigation, decision-making)",
            "performance_with_memory": "N/A (no external episodic memory).",
            "performance_without_memory": "Baseline performance reported in paper: performs comparably to classical bandit algorithms on first exposures but does not improve with repeated exposures and fails to immediately exploit previously discovered solutions when tasks reoccur.",
            "has_comparative_results": true,
            "performance_metric": "Average episodic reward; cumulative regret; steps-to-goal",
            "tradeoffs_reported": "L2RL can learn strong within-episode learning algorithms, but the lack of persistent episodic storage prevents immediate reuse of prior solutions across episodes; simpler (no external memory) so fewer architectural/training complications but limited in environments with task reoccurrence.",
            "limitations_or_failure_cases": "Fails to exploit task reoccurrence because LSTM cell state is reset between episodes — no mechanism to recall previously discovered policies across episodes; adding raw high-dimensional context inputs (L2RL+Context) can overwhelm reward/action signals.",
            "citation": "Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C., Kumaran, D., & Botvinick, M. (2016). Learning to reinforcement learn.",
            "uuid": "e6583.2",
            "source_info": {
                "paper_title": "Been There, Done That: Meta-Learning with Episodic Recall",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "epL2RL (ablation: c_ep as input)",
            "name_full": "epL2RL variant with retrieved episodic value fed as standard LSTM input (ablation)",
            "brief_description": "An architectural ablation where the retrieved DND value c_ep is passed into the LSTM as an additional input rather than being reinstated into the cell state via the r-gate; authors report this harmed performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "epL2RL (retrieved vector as input variant)",
            "agent_description": "Same as epL2RL but instead of adding retrieved cell-state to the LSTM cell via r-gate, the retrieved vector c_ep is concatenated/provided as an extra input to the LSTM; authors evaluated this variant and found severe performance degradation unless the retrieved vector was very aggressively compressed.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "DND episodic memory (retrieval used but integrated differently)",
            "memory_representation": "Same as epL2RL: context keys and LSTM cell-state values; retrieved vector (50-dim in ablation experiments) provided as raw input.",
            "memory_access_mechanism": "kNN retrieval from DND; retrieved vector concatenated to input channels of LSTM rather than reinstated via gated addition.",
            "task_name": "Tested as ablation on tasks such as Barcode Bandits and others (reported poor performance across tasks)",
            "task_category": "meta-reinforcement learning (ablation study)",
            "performance_with_memory": "Poor — in multiple experiments feeding the retrieved vector as an additional input reduced performance to chance; only succeeded when the retrieved vector was compressed to a very small dimensionality (&lt;~5 dims) in limited tasks.",
            "performance_without_memory": "Better than this ablated variant: the reinstatement approach (r-gate) performed substantially better than raw-input integration; L2RL baseline sometimes outperformed this ablation.",
            "has_comparative_results": true,
            "performance_metric": "Average episodic reward; qualitative comparisons (chance-level performance reported for some tasks)",
            "tradeoffs_reported": "Implies a trade-off between how retrieved memories are integrated: reinstatement (add to cell + gating) is effective; naive input concatenation introduces large noisy inputs (esp. for high-dimensional retrieved vectors) that impede learning unless compressed, but aggressive compression harms capacity.",
            "limitations_or_failure_cases": "This integration method failed in experiments (reduced to chance) unless dimensionality drastically reduced; suggests sensitivity to integration design and dimensionality of retrieved vectors.",
            "uuid": "e6583.3",
            "source_info": {
                "paper_title": "Been There, Done That: Meta-Learning with Episodic Recall",
                "publication_date_yy_mm": "2018-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural episodic control",
            "rating": 2
        },
        {
            "paper_title": "Model-free episodic control",
            "rating": 2
        },
        {
            "paper_title": "Learning to reinforcement learn",
            "rating": 2
        },
        {
            "paper_title": "One-shot learning with memory-augmented neural networks",
            "rating": 1
        },
        {
            "paper_title": "Hybrid computing using a neural network with dynamic external memory",
            "rating": 1
        },
        {
            "paper_title": "Learning to remember rare events",
            "rating": 1
        }
    ],
    "cost": 0.012726999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Been There, Done That: Meta-Learning with Episodic Recall</h1>
<p>Samuel Ritter ${ }^{12}$ Jane X. Wang ${ }^{1}$ Zeb Kurth-Nelson ${ }^{13}$ Siddhant M. Jayakumar ${ }^{1}$<br>Charles Blundell ${ }^{1}$ Razvan Pascanu ${ }^{1}$ Matthew Botvinick ${ }^{14}$</p>
<h4>Abstract</h4>
<p>Meta-learning agents excel at rapidly learning new tasks from open-ended task distributions; yet, they forget what they learn about each task as soon as the next begins. When tasks reoccur - as they do in natural environments - metalearning agents must explore again instead of immediately exploiting previously discovered solutions. We propose a formalism for generating open-ended yet repetitious environments, then develop a meta-learning architecture for solving these environments. This architecture melds the standard LSTM working memory with a differentiable neural episodic memory. We explore the capabilities of agents with this episodic LSTM in five meta-learning environments with reoccurring tasks, ranging from bandits to navigation and stochastic sequential decision problems.</p>
<h2>1. Introduction</h2>
<p>Meta-learning refers to a process through which a learning agent improves the efficiency and effectiveness of its own learning processes through experience. First introduced as a core topic in AI research in the 1990s (Thrun \&amp; Pratt, 1998; Schmidhuber et al., 1996), meta-learning has recently resurged as a front-line agenda item (Santoro et al., 2016; Andrychowicz et al., 2016; Vinyals et al., 2016). In addition to reviving the original topic, recent work has also added a new dimension by importing the theme of meta-learning into the realm of reinforcement learning (Wang et al., 2016; Finn et al., 2017; Duan et al., 2016).</p>
<p>Meta-learning addresses a fundamental problem for realworld agents: How to cope with open-ended environments, which present the agent with an unbounded series of tasks.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>As such, meta-learning research has typically focused on the problem of efficient learning on new tasks, neglecting a second, equally important problem: What happens when a previously mastered task reoccurs? Ideally, in this case the learner should recognize the reoccurrence, retrieve the results of previous learning, and "pick up where it left off." Remarkably, as we shall review, state-of-the-art meta-learning systems contain no mechanism to support this kind of recall.</p>
<p>The problem of task reoccurrence is taken for granted in other areas of AI research, in particular work on life-long learning and continual learning (Ring, 1995; Kirkpatrick et al., 2017; Thrun, 1996). Here, it is assumed that the environment rotates through a series of tasks, and one of the key challenges is to learn each task without forgetting what was learned about the others. However, work focusing on this problem has generally considered scenarios involving small sets of tasks, avoiding the more open-ended scenarios and the demand for few-shot learning that provide the focus in meta-learning research.</p>
<p>Naturalistic environments concurrently pose both of the learning challenges we have touched on, confronting learners with (1) an open-ended series of related yet novel tasks, within which (2) previously encountered tasks identifiably reoccur (for related observations, see Anderson, 1990; O'Donnell et al., 2009). In the present work, we formalize this dual learning problem, and propose an architecture which deals with both parts of it.</p>
<h2>2. Problem Formulation</h2>
<p>Meta-learning research formalized the notion of an unbounded series of tasks by defining task distributions $\mathcal{D}$ over Markov Decision Processes (MDPs), then repeatedly sampling MDPs as $m \sim \mathcal{D}$ (Thrun \&amp; Pratt, 1998). To create open-ended sequences of novel and identifiably reoccuring tasks, we propose to instead sample MDPs $m$ along with contexts $c$ from stochastic task processes as follows</p>
<p>$$
\left(m_{n+1}, c_{n+1}\right) \mid\left(m_{1}, c_{1}\right), \ldots,\left(m_{n}, c_{n}\right) \sim \Omega(\theta, \mathcal{D})
$$</p>
<p>where $\Omega$ is a stochastic process with base distribution $\mathcal{D}$ and parameters $\theta$. $n$ indexes the tasks' positions in the sequence. From here on we will refer to these MDPs and their contexts, $\left(m_{n}, c_{n}\right)$, as the tasks $t_{n}$.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Model architecture and environment structure. Tasks, such as multi-armed bandits, are presented sequentially to the agent along with identifiable contexts, which are depicted here as colored bars. On each time step, the agent reads from long-term memory (DND) to retrieve cell states, which are reinstated through the multiplicative reinstatement gate (r-gate). At the end of each task, it writes its cell state to the DND.</p>
<p>This framework can be used to challenge agents with precisely defined task reoccurrence frequencies. Consider for example a Blackwell-MacQueen urn scheme <em>Blackwell &amp; MacQueen (1973)</em>, which samples MDP/context pairs $t$ successively as</p>
<p>$$
t_{n+1} \mid t_{1}, \ldots, t_{n} \sim \frac{1}{\alpha+n}\left(\alpha \mathcal{D}+\sum_{i=1}^{n} \delta_{t_{i}}\right)
$$</p>
<p>where $\delta_{t_{i}}$ is a point mass at $t_{i}$ and $\alpha$ is the concentration parameter. Intuitively, this scheme first samples an $(m, c)$ task pair from the distribution $\mathcal{D}$ then drops it into an urn. On the following and all subsequent steps, with probability $\frac{\alpha}{\alpha+n}$, the procedure samples a new task from $\mathcal{D}$ and drops it into the urn. Otherwise, it draws a task from the urn uniformly at random, copies it, and drops both the original and new copy into the urn <em>Teh (2011)</em>. This process has a "rich get richer" property, whereby each occurrence of a task makes it more likely to reoccur, leading to Zipflike distributions that are observed frequently in naturalistic environments (e.g., Huberman et al., 1998). Task processes like this one may enable the development of agents that can cope with and ultimately take advantage of such important statistical properties of natural environments.</p>
<p>The remainder of this paper addresses the primary issue of identifying and reusing task solutions when the reoccurrence frequencies are uniform. Accordingly, the bulk of the experiments use the hypergeometric process, which repeatedly samples uniformly without replacement from a bag of tasks $S=\left{t_{1} \ldots t_{|S|}\right}$ which contains duplicates of each task, so that $t_{n} \sim \operatorname{unif}(S)$ <em>Terrell (2006)</em>. To solve this class of problems, we expect our deep reinforcement learning agents to: (1) meta-learn, capitalizing on shared structure to learn faster with each new task, and (2) avoid exploration when a task reoccurs, instead snapping back to hard-won effective policies.</p>
<h2>3. Agent Architecture</h2>
<p>We build on the learning to reinforcement learn (L2RL) framework proposed by <em>Wang et al. (2016)</em> and parallel work by <em>Duan et al. (2016)</em>. In L2RL, LSTM-based agents learn to explore novel tasks using inductive biases appropriate for the task distribution. They learn these exploration policies through training on tasks in which the reward on each timestep is supplied as an input, so that through training, the recurrent dynamics come to implement learning algorithms that use this reward signal to find the best policy for a new task. To execute such learning algorithms the LSTM must store relevant information from the recent history in its cell state. As a result, at the end of the agent's exposure to a task, the cell state contains the hard-won results of the agent's exploration.</p>
<p>Although this and the other meta-learning methods excel at acquiring knowledge of structure and then rapidly exploring new tasks, none are able to take advantage of task reoccurrence. In the case of meta-learning LSTMs, at the end of each exposure to a task, the agent resets its cell state to begin the next task, erasing the record of the results of its exploration. To remedy this forgetting problem in L2RL, we propose a simple solution: add an episodic memory system that stores the cell state along with a contextual cue and reinstates that stored cell state when a similar cue is reencountered later. This is inspired in part by evidence that human episodic memory retrieves past working memory states <em>Marr (1971); Hoskin et al. (2017)</em>.</p>
<p>To implement this proposal, we draw inspiration from recent memory architectures for RL. <em>Pritzel et al. (2017)</em> proposed the differentiable neural dictionary (DND), which stores key/value pairs in each row of an array (see also <em>Blundell et al. (2016)</em>). The values are retrieved based on k-nearest neighbor search over the keys, in a differentiable process that enables gradient-based training of a neural network that</p>
<p>produces the keys. Pritzel et al. (2017) achieved state-of-the-art sample efficiency on a suite of 57 Atari games by storing state-action value estimates as the DND’s values and convolutional embeddings of the game pixels as the keys. Inspired by this success, we implement our cell state-based episodic memory as a DND that stores embeddings of task contexts $c$ as keys and stores LSTM cell states as values. In effect, this architecture provides context-based retrieval of the results of past exploration, addressing in principle the forgetting problem in L2RL.</p>
<p>However, an important design problem remains: how to incorporate the retrieved values with the ongoing L2RL process. The usual strategy for incorporating information retrieved from an external memory into its recurrent network controller is to pass the memories through parameterized transformations and provide the results as additional inputs to the network (e.g. Graves et al., 2016; Weston et al., 2014). While this has been effective in supervised settings, such transformations have proved difficult to learn by RL. Instead, we propose to make use of the unique relationship between the current working memory and retrieved states: not only do the retrieved states share the same dimensionality as the current working memory state, they also share the same semantics. That is, the policy layer and the LSTM dynamics will operate in roughly the same way on axes of the retrieved states as they do on the axes of the current states. This affords the possibility of reinstating the retrieved states; that is, adding them directly to the current cell state instead of treating them as additional inputs.</p>
<p>Such a reinstatement approach could fail if the reinstated activations interfere with necessary information stored in the current working memory state. We observe that the LSTM already solves a similar problem: it prevents incoming inputs from interfering with stored information, and vice versa, using the input and forget gates. We extend this same gating solution to coordinate among these and our new contributor to working memory. More precisely, the LSTM cell update equation (Hochreiter et al., 2001)</p>
<p>$\mathbf{c}<em t="t">{t}=\mathbf{i}</em>}\circ\mathbf{c<em t="t">{i n}+\mathbf{f}</em>$ (3)}\circ\mathbf{c}_{t-1</p>
<p>uses multiplicative gates $\mathbf{i}<em t="t">{t}$ and $\mathbf{f}</em>$}$, defined as follows, to coordinate between the contributions of current working memory and incoming perceptual inputs $\mathbf{c}_{i n</p>
<p>$\mathbf{i}<em i="i" x="x">{t}=\sigma\left(\mathbf{W}</em>} \mathbf{x<em h="h" i="i">{t}+\mathbf{W}</em>} \mathbf{h<em i="i">{t-1}+\mathbf{b}</em>\right)$ (4)
$\mathbf{f}<em f="f" x="x">{t}=\sigma\left(\mathbf{W}</em>} \mathbf{x<em f="f" h="h">{t}+\mathbf{W}</em>} \mathbf{h<em f="f">{t-1}+\mathbf{b}</em>\right)$.</p>
<p>To the above update rule we add a new term for the contribution of reinstated working memory states,</p>
<p>$\mathbf{c}<em t="t">{t}=\mathbf{i}</em>}\circ \mathbf{c<em t="t">{i n}+\mathbf{f}</em>} \circ \mathbf{c<em t="t">{t-1}+\mathbf{r}</em>$} \circ \mathbf{c}_{e p</p>
<p>$$
\mathbf{r}<em r="r" x="x">{t}=\sigma\left(\mathbf{W}</em>} \mathbf{x<em h="h" r="r">{t}+\mathbf{W}</em>} \mathbf{h<em r="r">{t-1}+\mathbf{b}</em>\right)
$$</p>
<p>where $\mathbf{r}<em t="t">{t}$ is the reinstatement-gate, which, along with $\mathbf{i}</em>}$ and $\mathbf{f<em e="e" p="p">{t}$, coordinate among the three contributors to working memory. $\mathbf{c}</em>$ is the retrieved state from the episodic memory. Hereafter we will refer to this architecture as "episodic LSTM" (epLSTM, Figure 2).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. The episodic LSTM. In gray, the standard LSTM architecture. In bold, the proposed episodic memory and reinstatement pathways. See Section 3 for details.</p>
<h2>4. Experiments</h2>
<p>We tested the capabilities of L2RL agents equipped with epLSTM ("epL2RL agents") in five experiments. Experiments 1-3 use multi-armed bandits, first exploring the basic case where tasks reoccur in their entirety and are identified by exactly reoccurring contexts (Exp. 1), then, the more difficult challenge wherein contexts are drawn from Omniglot categories and vary in appearance with each reoccurrence (Exp. 2), and then, the more complex scenario where task components reoccur in arbitrary combinations (Exp. 3). Experiment 4 uses a water maze navigation task to assess epL2RL's ability to handle multi-state MDPs, and Experiment 5 uses a task from the neuroscience literature to examine the learning algorithms epL2RL learns to execute.</p>
<h3>4.1. Experiment 1: Barcode Bandits</h3>
<p>Here we address a basic case of episodic repetition, where tasks reoccur in their entirety and are identified by exactly reoccurring contexts. The tasks in this experiment were contextual multi-armed bandits, elaborating the multi-armed bandits of Wang et al. (2016) and Duan et al. (2016). Training consisted of episodes, in each of which the agent faced a set of actions (i.e., arms), where each arm yielded a reward with unknown probability. Each episode consisted of a series of pulls, throughout which the agent should efficiently find the best arm (explore) and pull it as many times as possible (exploit). During each episode, a context was presented which identified the reward probabilities. The challenge for the agent was to recall its past experience in a given context</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Training and regret curves. (a) Barcode and (b) Omniglot bandit training curves, averaged over 3 runs. (c) Cumulative regret (expected loss in cumulative reward due to selecting suboptimal arms) for epL2RL agent on the Omniglot bandits task, averaged over evaluation episodes. Regret decreases as the number of exposures increases. The agent performs similarly to Gittins indices, Upper Confidence Bounds (UCB), and Thompson sampling on its first exposure, and outperforms them on subsequent exposures. (d) L2RL with no episodic memory performs on par with standard algorithms on Omniglot bandits, with no decrease in regret with more exposures.
when the context reoccurred so that it could immediately exploit the best arm - or continue its exploration where it left off.</p>
<p>The contexts in this experiment were binary strings, or "barcodes", so the full set of possible contexts was $C={0,1}^{l}$, where $l=10$ was the barcode length. The reward parameters were structured such that all arms except for one had a reward probability of 0.1 , while the remaining arm had a reward probability of 0.9 . To prevent agents from overfitting the mapping between contexts and reward probabilities, we periodically shuffled this mapping throughout training. This shuffling serves a similar purpose to the randomization of bandit parameters between episodes: it forces the system to learn how to learn new mappings by disallowing it to overfit to a particular mapping. We hereafter refer to the sequences of episodes in which the mapping between context and bandit parameters is fixed as epochs.</p>
<p>We sampled the sequence of tasks for each epoch as follows: we first sampled a set of unique contexts, and paired each element of that set randomly with one of the possible rewarding arm positions $b$, ensuring that each rewarding arm position was paired with at least one context. We then created a bag $S$ in which each $(c, b)$ pair was duplicated 10 times. Finally, we sampled the task sequence for the epoch by repeatedly sampling uniformly without replacement tasks $t_{n}=\left(c_{n}, b_{n}\right) \sim \operatorname{unif}(S)$. There were 100
episodes per epoch and 10 unique contexts per epoch. Thus, each context was presented 10 times per epoch. There were 10 arms, and episodes were 10 trials long.</p>
<p>In this and all following experiments, k was set equal to 1 in the k-nearest neighbors ( kNN ) search over the DND contents. Cosine distance and normalized inverse kernel were used for the kNN searches (Pritzel et al., 2017). The DND was cleared at the end of each epoch, and the LSTM hidden state was reset at the end of every episode. Hyperparameters were tuned for the basic L2RL model, and held fixed for the other model variations.</p>
<p>Figure 3a shows training curves for the barcode bandits task. epL2RL widely outperforms the following baselines: L2RL, which is the agent from (Wang et al., 2016) and L2RL+Context, which is that same agent with the barcodes supplied as inputs to the LSTM. The results for this latter baseline indicate that epL2RL's performance cannot be matched by parametrically learning a mapping between barcodes and rewarding arms. epL2RL's increase in performance over L2RL suggests that the memory system is working, and in Experiment 2 we conducted further analysis to verify this hypothesis.</p>
<p>In this and all following experiments, we also tried a variant of the episodic memory architecture in which retrieved values from the DND, $c_{e p}$ were fed to the LSTM as inputs instead of added to the working memory through the r-gate.</p>
<p>We found that feeding this 50 dimensional vector to the LSTM reduced performance to random chance. This is almost certainly because the large number of noisy additional inputs make it difficult to learn to use the previous action and previous reward. Because of its poor performance, we omit this variant from the plots ${ }^{1}$.</p>
<h3>4.2. Experiment 2: Omniglot Bandits</h3>
<p>Tasks in the real world are rarely identifiable by exact labels; instead there is often an implicit classification problem to be solved in order to correctly recognize a context. To investigate whether epL2RL can handle this challenge, in Experiment 2 we posed an episodic contextual bandits task in which the contexts are Omniglot character classes (Lake et al., 2015). Each time the class reoccurs, the image shown to the agent will be a different drawing of the character, so that the agent must successfully search its memory for other instances of the class in order to make use of the results of its past exploration. The task sampling process, episode/epoch structure, and agent settings were the same as those described in Section 4.1.</p>
<p>We used pretrained Omniglot embeddings from Kaiser et al. (2017). This is a particularly appropriate method for pretraining because such a contrastive loss optimization procedure (Hadsell et al., 2006) could be run online over the DND's contents, assuming some heuristic for determining neighbor status. Future work may try contrastive losses over the DND contents to learn perception online during RL; for discussion see Section 5.</p>
<p>The training curves in Figure 3b show that epL2RL obtains more reward than its L2RL counterparts. L2RL+Context fails in this case because the Omniglot context embeddings are relatively large - 128 dimensions - drowning out the important reward and action input signals. In Figure 3c-d, we analyze the agents' behavior in more detail to understand how epL2RL is obtaining more reward. Figure 3c depicts epL2RL's cumulative regret, i.e. the difference between the expected reward of the optimal arm and the expected reward of the chosen arm, averaged over a set of evaluation episodes. The regret curves are split by the number of previous exposures to the Omniglot character class the agent experienced in that episode. From this we can see that during the first exposure, the agent accrues a relatively large amount of regret. On the second exposure it accrues significantly less, and this trend continues as the number of exposures increases. The possibility that this decrease</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>in regret was due to gradient-based learning is ruled out by the fact that the weights were frozen during the evaluation episodes. This result indicates that the agent is able to recall the results of its previous exploration and to hone them further with each passing episode. In other words epL2RL is able to store partially completed solutions then later recall them to "pick up where it left off".</p>
<p>In contrast, Figure 3d shows that no such improvement with increasing exposures occurs in the L2RL agent without episodic memory. Figure 3c-d compares agents' regret curves with those of traditional bandit algorithms: Gittins indices (Gittins, 1979), UCB (Auer et al., 2002) (which comes with theoretical finite-time regret guarantees), and Thompson sampling (Thompson, 1933). We find that the L2RL agents compare favorably with these baselines as in (Wang et al., 2016) (Figure 3d). Further, we observe that epL2RL outperforms these algorithms after a few exposures (Figure 3c), suggesting that it is able to make use of the unfair advantage over these algorithms that its memory affords.</p>
<h3>4.3. Experiment 3: Compositional Bandits</h3>
<p>Real world agents not only face tasks that reoccur in their entirety, but also task components that reoccur in various combinations. This requires that agents compose memories of previously encountered task components. In this experiment we simulate this type of task composition. We continue to use the multi-armed bandit setup, but rather than associating a context with each set of arms, we associate a context with each arm individually. Each episode is then made up of an arbitrary combination of these context/arm pairs. When a context/arm reoccurs, it is not constrained to appear in the same "position" (i.e., it may not be associated with the same action) as when its appeared previously.</p>
<p>This experiment used 2-armed bandits. The sampling process proceeded as follows: at the beginning of each epoch we first sampled a set $C_{l}$ of unique contexts, and paired each one with the low reward probability of 0.1 , yielding pairs $(c, l)$. We then sampled a new set of contexts $C_{h}$ that contained no overlap with $C_{l}$, and paired each member of $C_{h}$ with the high reward probability of 0.9 , yielding pairs $(c, h)$. Next we created a bag $S_{h}$ containing 5 duplicates of all high rewarding contexts/reward probability pairs and a bag $S_{l}$ containing 5 duplicates of all low rewarding contexts/reward probability pairs. Finally, for each episode in the epoch we sampled randomly without replacement a low rewarding context $\left(c_{n}, l_{n}\right) \sim \operatorname{unif}\left(S_{l}\right)$ and a high rewarding context $\left(c_{n}, h_{n}\right) \sim \operatorname{unif}\left(S_{h}\right)$. The agent was then trained on an episode with these context/reward probability pairs.</p>
<p>In this setup the stimuli are paired directly with the reward probabilities, and the reward probabilities and stimuli together randomly swap positions on each trial. The stimuli are given to the LSTM as input in positions associated with</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. (a) Training and (b) regret curves for compositional bandits. Training curves averaged over 3 runs, regret curves averaged over evaluation episodes.
the action, so that the LSTM must learn to discover the rewards associated with stimuli rather than with the arms. On each trial the epL2RL agent queries the DND using one of the two stimuli (selected randomly), retrieves a single cell state from the DND, then reinstates it as in the previous experiments. Otherwise, the agent settings were the same as those described in Section 4.1. There were 1000 episodes per epoch and 400 classes per epoch, so that each class was shown 5 times.</p>
<p>This setup and the aspect of the world that it models pose a specific problem for episodic memory. As in the previous experiments, the agent must retrieve holistic episodic memories based on context similarity; but, in this case the memory state vector the agent retrieves will contain not only information relevant to the task, but also information about another arm which is probably not present. Further, the information the memory contains about the relevant arm may refer to a trial in which that arm was shown in the other position. The agent must extract the relevant information from the memory without allowing the irrelevant information to affect its behavior, and must apply the correct information to the correct arm of the current task. In essence, because the environment state when the memory was saved differs from the current environment state, the agent must map information from the past onto the present.</p>
<p>We find that epL2RL performs well even with no architectural modification, as evidenced by its increase in reward over L2RL+Context (Figure 4a). Further, the regret curves (Figure 4b) show that the epL2RL agent consistently reduces
its regret after successive exposures to a given context. This decrease in regret cannot be attributed to gradient-based learning because the weights were frozen during the evaluation episodes. L2RL (without context) is omitted from Figure 4a because in this task it is impossible to determine the action-reward probabilities without the stimulus input.</p>
<h3>4.4. Experiment 4: Contextual Water Maze</h3>
<p>In real-world episodic repetition, agents must explore stateful environments while managing contributions from episodic memory. The bandit tasks do not provide this challenge, so we now test our agents in minimal multi-state MDPs, specifically, contextual water mazes. In these tasks, the agent is shown a context barcode and must navigate a grid to find a goal using actions left, right, up, and down. The goal location is initially unknown. When the agent finds the goal, the agent's location is reset (at random), and, if it uses its LSTM working memory effectively, it can proceed directly back to the goal without needing to re-explore. In our setup the grid has 16 states $(4 \mathrm{x} 4)$ and the agent has 20 steps per episode to find the goal and return to it as many times as possible.</p>
<p>The contextual (barcode) cues are mapped to goal locations in the same way they were mapped to rewarding arms in Experiments 1 and 2. Thus, once a goal is reached in a given context, the agent can use its memory of previous exploration to proceed directly to the goal without needing to explore again. The epoch structure and task sequence sampling process were the same as those described in Section 4.1. The agent settings and architecture were also identical, except that ( $\mathrm{x}, \mathrm{y}$ ) coordinates were provided as input.</p>
<p>Figure 5a shows that the epL2RL agent outperforms the L2RL baselines. It also significantly outperforms the maximum average reward that could be achieved without memory of past episodes. Figure 5b shows the mean number of steps for the epL2RL agent to reach the goal from episode start, split by the number of previous exposures to the stimulus in the epoch. The means were calculated over a block of evaluation episodes (with learning rate set to zero). We see that the agent takes a much smaller number of steps to get to the goal after its first exposure. Since the data was recorded while the weights were frozen, this improvement cannot be explained by gradient-based learning, and thus must be the result of information stored in the episodic memory.</p>
<p>Figures 5c and 5d show sets of epL2RL agent trajectories, again from episode blocks in which weights were frozen. Figure 5c shows trajectories associated with the initial exposure to a context in an epoch, and a clear exploration policy is visible. Figure 5d shows trajectories associated with the second exposure to the same context and starting location. These trajectories show that the agent in all cases navigated directly to the goal by one of the shortest paths.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Contextual Water Maze task. (a) Training curve averaged over 5 runs. The brown line indicates the maximum reward achievable without episodic memory. (b) Number of steps before the epL2RL agent reaches the goal after respawning, binned by number of previous exposures to the current context, data from evaluation episodes. After only one exposure, the agent can perform close to optimally. (c) Sample trajectories captured during evaluation for episodes with the same starting position and end goal during the first exposure to a context. (d) Same as c, but for the second exposure. Early steps are colored more red; later steps more green.</p>
<h3>4.5. Experiment 5: Episodic Two-Step Task</h3>
<p>Wang et al. (2016) used the two-step task (Daw et al., 2011) to assess the degree to which L2RL learns to use modelbased (MB) and model-free (MF) control. They found that L2RL learned to execute MB control, a remarkable result given that L2RL is trained via a purely model-free method. In our final experiment, we use a variant of the two-step task with episodic cues (Vikbladh et al., 2017) to test whether our epL2RL agent can learn to execute episodic MF and episodic MB control.</p>
<p>In the classic two-step task, each episode ${ }^{2}$ consists of a single two stage MDP. On step 1, the agent can take one of two actions, $a_{1}$ or $a_{2}$ that will then lead through either a common transition or an uncommon transition to the resultant observable states $s_{1}$ or $s_{2}$. Rewards at $s_{1}$ and $s_{2}$ are drawn from a Bernoulli distribution which changes over time. In our setup, $\left[P\left(R \mid s_{1}\right), P\left(R \mid s_{2}\right)\right]$ is either $[0.9,0.1]$ or $[0.1,0.9]$, and these parameters have a $10 \%$ chance of reversing on every episode. In the episodic version of the task, cues (implemented here as barcodes) are presented at the second stage of the two-step episode. On step 1, the agent will either encounter no cue (an uncued episode), or a cue which matches the second-step context of an earlier episode $e$ (cued episode). On cued episodes, if the agent reaches the same state ( $s_{1}$ or $s_{2}$ ) as it reached in episode $e$, then it's guaranteed to receive the exact same reward it received on episode $e$. Otherwise, it receives a draw from the currently active Bernoulli distribution for that state.</p>
<p>This task is amenable to four valuation strategies; in other words, there are four different learning strategies epL2RL could learn to execute. The first is incremental model-free (IMF), wherein the agent takes the same action it took on the immediately previous episode if that episode was rewarded.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The second is incremental model-based (IMB), whereby it takes the same action it took on the previous episode, only if that episode took a common transition. Agents with episodic memory could learn two additional strategies: episodic model-free (EMF) and episodic model-based (EMB), which operate like their respective incremental counterparts, but with respect to the past episode with which the episodic cue was associated. Beyond providing another setting in which to test the effectiveness of the reinstatement based episodic memory, this task also enables us to test specifically whether this system can learn to follow the more sophisticated and effective, but in principle more difficult to learn, episodic model-based behavior. Epochs consisted of 100 episodes each. The first half ( 50 episodes) of all epochs were uncued and the second half were cued. The agent settings were the same as those described in Section 4.1.</p>
<p>We found that the epL2RL agent achieved more reward than the L2RL agent (Figure 6a). To determine which algorithms epL2RL learned to use, we fit a choice model to the epL2RL agent's behavior. This model describes the probability of action $a_{1}$ as the softmax of a weighted sum of action values derived from IMF, IMB, EMF, and EMB control. We estimated these weights by maximum likelihood. Results showed that epL2RL does in fact learn to use an EMB policy, which it executes in tandem with both incremental learning strategies (Figure 6b). Intriguingly, this is the same pattern of learning behavior observed in humans by Vikbladh et al. (2017). For neuroscience implications, further analysis, and details see Ritter et al. (2018).</p>
<h3>4.5.1. ANALYSIS OF THE R-GATES</h3>
<p>Next, we used this task to analyze the role of the epLSTM r-gates. Figure 6c shows the mean r-gate value over the epoch, averaged over a block of evaluation epochs. These time courses are split by which stage of the two-step episode the agent was in. In all stages the r-gates open more when</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Episodic two-step task results and analysis. See Section 4.5 for task description. (a) Training curves averaged over 10 runs show that epL2RL obtains higher reward than L2RL, providing evidence that the epL2RL agent is able to exploit the task's episodic cues. (b) Model fitting and parameter estimates for the different decision systems show that epL2RL uses IMF and IMB in the uncued episodes and IMF, IMB, and EMB on the cued episodes. (c) Time course of the mean r-gate values averaged over 500 epochs show that the gate is most open at the action stage, and is more open during cued episodes relative to uncued trials.
the cued episodes block starts. This makes sense, because in the uncued episodes there is no utility in reading from the memory. Further, we find that the r-gates were reliably more open on cued episodes in which the agent selected the correct action (mean r-gate value $=0.365$ ) than in cued episodes in which the agent selected the wrong action (mean r-gate value $=0.358$; two-tailed t-test $\mathrm{p}&lt;1 \mathrm{e}-20$ ). These observations provide preliminary evidence that the r-gates may work by allowing information from the DND into working memory when it is useful and gating it out when it is not. However, while the results discussed are highly statistically significant, the absolute magnitudes of the differences are very small. This suggests that other processes may be at work in governing the interplay between working memory, reinstated activations from the DND, and input in the epLSTM. Future work will be needed to explore this topic further.</p>
<h2>5. Discussion</h2>
<p>This study constitutes a first step towards deep RL agents that exploit both structure and repetition in their environments. We introduced both a meta-learning training regime and a novel episodic memory architecture. Over the course of five experiments, we found our agents capable of recalling previously discovered policies, retrieving memories using category examples, handling compositional tasks, reinstating memories while traversing multi-state MDPs, and discovering the episodic learning algorithm humans use in a neuroscience-inspired task.</p>
<p>These results pave the way for future work to tackle additional challenges posed by real-world tasks. First, tasks often will not provide contexts as easily identifiable as barcodes, pretrained context embeddings will not be available, and contexts will be supplied in the same channel as other inputs. As such, a critical next step will be to learn to produce query keys. Two complementary approaches arise naturally for the epLSTM. First, the DND provides a mechanism
for passing gradients through the retrieval process (Pritzel et al., 2017); future work should explore the possibilities of using this learning pathway for epL2RL. Second, the contents of the DND provide an exciting opportunity for auxiliary training procedures, such as Kaiser et al.'s (2017) algorithm. This procedure iteratively fills an array with embedding/label pairs and trains the embedding network by applying triplet loss to the nearest neighbors of new examples. Because the epLSTM's DND already accumulates embeddings and retrieves nearest neighbors, the marginal computational cost of applying such a contrastive loss is relatively low. The only challenge is to define the neighborhood function (Hadsell et al., 2006), which might be done using heuristics such as temporal contiguity.</p>
<p>Next, in many tasks of interest, contexts will not be fully observable during a single timestep; instead, information must be aggregated over time to produce an effective query embedding. Consider for example a task in which an agent can identify a previously solved maze only by observing several of its corridors. Using the LSTM cell state, or a function thereof, as the query key is an appealing prospect for handling this challenge. Finally, in the present experiments, episode boundaries were clearly defined so that the agent could simply save at the end of each episode. In the real-world, events are not cut so cleanly, and agents must decide when to save, or save on every step and decide when to forget. Future work may pursue i) heuristics such as saving when reward is received and ii) learning approaches that leverage curricula beginning with short episodes amenable backpropagation through the storage process.</p>
<p>As a final note, although we developed the epLSTM to solve the forgetting problem in L2RL, such a reinstatement-based episodic memory system may be useful in other RL settings and for sequence learning problems in general. Future work may explore the potential of the epLSTM and other reinstatement-based memory systems in these domains.</p>
<h2>Acknowledgements</h2>
<p>We thank Iain Dunning who developed, with help from Max Jaderberg and Tim Green, the asynchronous RL codebase we used to train our agents. We also thank the numerous research scientists and research engineers at DeepMind who worked on that code's Tensorflow and Torch predecessors. Further, we thank Alex Pritzel, Adrià Puigdomènech Badia, Benigno Uria, and Jonathan Hunt for their work on the DND library we used in this work. Finally, we thank Nicolas Heess for insightful discussion, and especially for his suggestion of water maze tasks; Ian Osband for sharing his bandits expertise; and Jack Rae for valuable discussion.</p>
<h2>References</h2>
<p>Anderson, John Robert. The adaptive character of thought. Psychology Press, 1990.</p>
<p>Andrychowicz, Marcin, Denil, Misha, Gomez, Sergio, Hoffman, Matthew W, Pfau, David, Schaul, Tom, and de Freitas, Nando. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pp. 3981-3989, 2016.</p>
<p>Auer, Peter, Cesa-Bianchi, Nicolo, and Fischer, Paul. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235-256, 2002.</p>
<p>Blackwell, David and MacQueen, James B. Ferguson distributions via pòlya urn schemes. The annals of statistics, pp. 353-355, 1973.</p>
<p>Blundell, Charles, Uria, Benigno, Pritzel, Alexander, Li, Yazhe, Ruderman, Avraham, Leibo, Joel Z, Rae, Jack, Wierstra, Daan, and Hassabis, Demis. Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016.</p>
<p>Daw, Nathaniel D, Gershman, Samuel J, Seymour, Ben, Dayan, Peter, and Dolan, Raymond J. Model-based influences on humans' choices and striatal prediction errors. Neuron, 69(6):1204-1215, 2011.</p>
<p>Duan, Yan, Schulman, John, Chen, Xi, Bartlett, Peter L, Sutskever, Ilya, and Abbeel, Pieter. Rl ${ }^{2}$ : Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.</p>
<p>Finn, Chelsea, Abbeel, Pieter, and Levine, Sergey. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.</p>
<p>Gittins, John C. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society. Series B (Methodological), pp. 148-177, 1979.</p>
<p>Graves, Alex, Wayne, Greg, Reynolds, Malcolm, Harley, Tim, Danihelka, Ivo, Grabska-Barwińska, Agnieszka, Colmenarejo, Sergio Gómez, Grefenstette, Edward, Ramalho, Tiago, Agapiou, John, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471, 2016.</p>
<p>Hadsell, Raia, Chopra, Sumit, and LeCun, Yann. Dimensionality reduction by learning an invariant mapping. In Computer vision and pattern recognition, 2006 IEEE computer society conference on, volume 2, pp. 1735-1742. IEEE, 2006.</p>
<p>Hochreiter, Sepp, Younger, A Steven, and Conwell, Peter R. Learning to learn using gradient descent. In International Conference on Artificial Neural Networks, pp. 87-94. Springer, 2001.</p>
<p>Hoskin, Abigail Novick, Bornstein, Aaron M, Norman, Kenneth A, and Cohen, Jonathan D. Refresh my memory: Episodic memory reinstatements intrude on working memory maintenance. bioRxiv, pp. 170720, 2017.</p>
<p>Huberman, Bernardo A, Pirolli, Peter LT, Pitkow, James E, and Lukose, Rajan M. Strong regularities in world wide web surfing. Science, 280(5360):95-97, 1998.</p>
<p>Kaiser, Lukasz, Nachum, Ofir, Roy, Aurko, and Bengio, Samy. Learning to remember rare events. CoRR, abs/1703.03129, 2017. URL http://arxiv.org/ abs/1703.03129.</p>
<p>Kirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil, Veness, Joel, Desjardins, Guillaume, Rusu, Andrei A, Milan, Kieran, Quan, John, Ramalho, Tiago, GrabskaBarwinska, Agnieszka, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, pp. 201611835, 2017.</p>
<p>Lake, B M, Salakhutdinov, R, and Tenenbaum, J B. Humanlevel concept learning through probabilistic program induction. Science, 350, 2015.</p>
<p>Marr, D. Simple memory: a theory for archicortex. Philosophical transactions of the Royal Society of London. Series B, Biological sciences, 262(841):23, 1971.</p>
<p>O’Donnell, Timothy J, Tenenbaum, Joshua B, and Goodman, Noah D. Fragment grammars: Exploring computation and reuse in language. MIT Computer Science and Artificial Intelligence Laboratory Technical Report Series, 2009.</p>
<p>Pritzel, Alexander, Uria, Benigno, Srinivasan, Sriram, Puigdomenech, Adria, Vinyals, Oriol, Hassabis, Demis, Wierstra, Daan, and Blundell, Charles. Neural episodic control. arXiv preprint arXiv:1703.01988, 2017.</p>
<p>Ring, Mark B. Continual Learning in Reinforcement Environments. R. Oldenbourg Verlag, 1995.</p>
<p>Ritter, Samuel, Wang, Jane X., Kurth-Nelson, Zeb, and Botvinick, Matthew. Episodic control as metareinforcement learning. bioRxiv: 360537, 2018.</p>
<p>Santoro, Adam, Bartunov, Sergey, Botvinick, Matthew, Wierstra, Daan, and Lillicrap, Timothy. One-shot learning with memory-augmented neural networks. arXiv preprint arXiv:1605.06065, 2016.</p>
<p>Schmidhuber, Jurgen, Zhao, Jieyu, and Wiering, Marco. Simple principles of metalearning. Technical report, SEE, 1996.</p>
<p>Teh, Yee Whye. Dirichlet process. In Encyclopedia of machine learning, pp. 280-287. Springer, 2011.</p>
<p>Terrell, George R. Mathematical statistics: A unified introduction. Springer Science \&amp; Business Media, 2006.</p>
<p>Thompson, William R. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25:285-294, 1933.</p>
<p>Thrun, Sebastian. Explanation-based neural network learning: A lifelong learning approach, volume 357. Springer Science \&amp; Business Media, 1996.</p>
<p>Thrun, Sebastian and Pratt, Lorien. Learning to learn: Introduction and overview. In Learning to learn, pp. 3-17. Springer, 1998.</p>
<p>Vikbladh, Oliver, Shohamy, Daphna, and Daw, Nathaniel. Episodic contributions to model-based reinforcement learning. In Annual Conference on Cognitive Computational Neuroscience, CCN, 2017.</p>
<p>Vinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra, Daan, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pp. $3630-3638,2016$.</p>
<p>Wang, Jane X, Kurth-Nelson, Zeb, Tirumala, Dhruva, Soyer, Hubert, Leibo, Joel Z, Munos, Remi, Blundell, Charles, Kumaran, Dharshan, and Botvinick, Matt. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.</p>
<p>Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Memory networks. CoRR, abs/1410.3916, 2014. URL http: //arxiv.org/abs/1410.3916.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Typically each two-step MDP traversal is referred to as a "trial" (Daw et al., 2011); we here use the term "episode" for consistency with Experiments 1-4.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
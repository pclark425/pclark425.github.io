<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4855 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4855</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4855</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-270619922</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.13828v3.pdf" target="_blank">Neuro-symbolic Training for Reasoning over Spatial Language</a></p>
                <p><strong>Paper Abstract:</strong> Spatial reasoning based on natural language expressions is essential for everyday human tasks. This reasoning ability is also crucial for machines to interact with their environment in a human-like manner. However, recent research shows that even state-of-the-art language models struggle with spatial reasoning over text, especially when facing nesting spatial expressions. This is attributed to not achieving the right level of abstraction required for generalizability. To alleviate this issue, we propose training language models with neuro-symbolic techniques that exploit the spatial logical rules as constraints, providing additional supervision to improve spatial reasoning and question answering. Training language models to adhere to spatial reasoning rules guides them in making more effective and general abstractions for transferring spatial knowledge to various domains. We evaluate our approach on existing spatial question-answering benchmarks. Our results indicate the effectiveness of our proposed technique in improving language models in complex multi-hop spatial reasoning over text.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4855.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4855.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-T+Q-Chain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT fine-tuned with SpaRTUN then neuro-symbolic Q-Chain constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-based BERT model first tuned on the synthetic SpaRTUN corpus and then fine-tuned with example-specific neuro-symbolic logical constraints (Q-Chains) using the DomiKnowS framework and soft-logic loss to improve multi-hop spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neuro-symbolic Training for Spatial Reasoning over Natural Language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>bert-based-uncased (BERT) with Q-Chain neuro-symbolic fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT (bert-based-uncased) encoder architecture; initialized from standard BERT checkpoint, fine-tuned first on SpaRTUN synthetic data (BERT-T) then with per-example logical constraints (Q-Chain) using DomiKnowS and a primal-dual optimization; classification heads added for YN and multi-label FR tasks. (No parameter-count explicitly given in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpaRTUN / SpartQA-Human / ReSQ / StepGame (spatial question-answering benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Textual spatial question-answering benchmarks requiring multi-hop spatial inference (YN and find-relation (FR) questions) over scene descriptions; StepGame includes deep reasoning up to 10 hops.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Neuro-symbolic fine-tuning: build resolution trees (Q-Chains) from annotated SpaRTUN facts using forward chaining, derive consistency constraints from spatial logic rules (converse, inverse, transitive, topological), convert to differentiable soft-logic (t-norm Product), and add constraint-violation terms to cross-entropy loss; optimized with primal-dual method via DomiKnowS.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Quantitative improvements concentrated at higher hop-depths (StepGame k=4..10 shows clear gains); qualitative examples in figures show corrected multi-hop inferences; ablation-like comparisons vs same model without Q-Chain indicate improvements specifically for multi-hop spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>StepGame per-hop accuracy: k1=98.70%, k2=96.45%, k3=93.03%, k4=74.58%, k5=64.95%, k6=59.04%, k7=54.38%, k8=49.23%, k9=45.36%, k10=44.05% (table 3). SpartQA-Human: ~59.44% (Ver.1) and ~58.92% (Ver.2) reported (table 2). Improvements over BERT and other baselines reported, especially at larger k.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires reasoning-chain annotations (SpaRTUN) for constructing Q-Chains during training; improvements sometimes degrade on questions relying more on commonsense (ReSQ unclassified split); training needs GPU resources (~100 GPU hours reported) and annotated synthetic data; does not require constraints at inference but cannot guarantee solving general reasoning beyond spatial multi-hop.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperforms the same BERT baseline without Q-Chain on multi-hop synthetic benchmarks (StepGame). LLMs (GPT-3.5/GPT-4/Llama-3) outperform on commonsense-heavy or unclassified ReSQ subset; pipeline systems using relation-extraction + external solvers (e.g., GPT3(d2)+ASP, PistaQ*) can match or exceed accuracy in synthetic settings but require formal representations at inference and external solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4855.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4855.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-T+Q-Chain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5-base fine-tuned on SpaRTUN then with neuro-symbolic Q-Chain constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative Flan-T5-base model adapted with LoRA and then neuro-symbolically fine-tuned using the same Q-Chain constraint integration to improve multi-hop spatial FR and YN QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neuro-symbolic Training for Spatial Reasoning over Natural Language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-base (with LoRA adapters) fine-tuned and Q-Chain constrained</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Flan-T5-base generative encoder-decoder model; LoRA adapters used to reduce fine-tuning compute; first fine-tuned on SpaRTUN (Flan-T5-T) then further fine-tuned with per-example logical constraints integrated via DomiKnowS soft-logic t-norm product loss; classification of YN by first-token probability; FR handled via multi-label objective when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpaRTUN / SpartQA-Human / ReSQ / StepGame (spatial QA benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Benchmarks of textual spatial reasoning requiring compositional and multi-hop logical inference about relative positions and topological relations among objects.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Same Q-Chain construction and integration as encoder-based model: forward-chaining to create reasoning chains, convert logical spatial rules into differentiable soft constraints (t-norm Product), add to training objective; LoRA used to limit adapter parameter changes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Consistent accuracy increases on synthetic SpartQA-Human and StepGame benchmarks after Q-Chain integration; larger relative gains than encoder BERT in some splits, particularly for FR tasks requiring deeper hops (e.g., SpartQA-Human k=2 improvement to ~81.33% in one reported split).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SpartQA-Human and ReSQ: Flan-T5-T+Q-Chain reported ~55.94% (Ver.1) and ~61.61% (Ver.2) on SpartQA-Human (table 2) and strong gains on ReSQ k=2 (e.g., 81.33% reported in table 2 for a Flan variant on some split). StepGame improvements reported though per-k numbers for Flan-T5 variants are not enumerated as fully as BERT in table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same as other neuro-symbolic tuning: requires annotated chains during training, less effective on commonsense-heavy realistic splits (ReSQ unclassified), and depends on quality/coverage of available spatial logical rules; LoRA reduces parameters but may limit model adaptation capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Improves more than BERT when constrained, but closed-source LLMs (GPT-3.5/4) remain superior on commonsense-rich splits; pipeline approaches with external solvers achieve high accuracy on synthetic tasks but need formal representations at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4855.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4855.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-T (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT fine-tuned on SpaRTUN (no Q-Chain constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT model pre-finetuned on SpaRTUN synthetic data and then fine-tuned on target dataset without neuro-symbolic constraints, serving as an experimental baseline to measure the effect of Q-Chain training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neuro-symbolic Training for Spatial Reasoning over Natural Language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>bert-based-uncased (BERT) fine-tuned on SpaRTUN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard BERT checkpoint first fine-tuned on SpaRTUN synthetic dataset (referred to as BERT-T) then fine-tuned on downstream spatial QA datasets without integrating logical constraints; classification heads for YN/FR tasks used.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpaRTUN / SpartQA-Human / ReSQ / StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Textual spatial QA benchmarks with multi-hop reasoning requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Standard supervised fine-tuning on the target spatial QA data (and pre-fine-tuning on SpaRTUN), no constraint-based loss terms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Improved multi-hop performance relative to plain BERT when pre-finetuned on SpaRTUN, but shows smaller gains than Q-Chain constrained variants at higher hop depths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>StepGame per-hop example: k1=98.50%, k2=95.32%, k3=93.26%, k4=76.78%, k5=66.36%, k6=58.76%, k7=53.70%, k8=46.27%, k9=42.71%, k10=40.12% (table 3). SpartQA-Human: BERT-T reported ~55.94% Ver.1 (table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less capable than Q-Chain constrained variant at deep multi-hop reasoning; still limited on commonsense-rich questions compared to large LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as the main ablation baseline vs BERT-T+Q-Chain; Q-Chain yields measurable improvements at larger reasoning depths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4855.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4855.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI) used in zero-shot and few-shot in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive language model (GPT-3.5 family) used in zero-shot, few-shot, and chain-of-thought/representation prompting to answer spatial QA questions without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neuro-symbolic Training for Spatial Reasoning over Natural Language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large autoregressive LLM from OpenAI (exact parameter count not provided in paper); evaluated in zero-shot, few-shot, and augmented prompting regimes (CoT, LR, CoS) for spatial QA via in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpartQA-Human, ReSQ, StepGame (in-context baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Textual spatial QA benchmarks; GPT-3.5 is applied directly to story+question prompts without gradient updates.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>In-context learning: zero-shot, few-shot (4 examples), chain-of-thought (CoT) prompting with manually-crafted rationales, logical representation (LR) prompts, chain-of-symbol (CoS), and step-by-step context compression.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>CoT prompting substantially improves performance versus zero/few-shot; reported qualitative examples and improvements on SpartQA-Human and StepGame; CoT often outperforms LR/CoS for in-context spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SpartQA-Human: GPT-3.5 (zero-shot) ~58.04%, (few-shot) ~62.23%, (CoT) ~65.73% accuracy on reported splits (table 2). Step-by-step prompting improved results modestly (Table 4): zero-shot 58.04% -> step-by-step 63.64%. On StepGame per-hop, GPT3 few-shot and CoT are much lower than fine-tuned constrained models at high k (examples: GPT3 few-shot ~55% at k1 dropping rapidly at higher k; see table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance falls off sharply as required reasoning hops increase (especially in StepGame high-k); suffers on structured multi-hop spatial reasoning compared to models fine-tuned with explicit logical supervision; improvements depend strongly on effective CoT prompts and context simplification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperforms small fine-tuned PLMs on commonsense-rich realistic splits (ReSQ unclassified) but underperforms neuro-symbolic fine-tuned smaller models (BERT-T+Q-Chain) on deep synthetic multi-hop benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4855.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4855.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI) large multimodal-capable LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art large language model used as a strong zero-shot baseline for spatial QA; achieves highest zero-shot scores reported on SpartQA-Human.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neuro-symbolic Training for Spatial Reasoning over Natural Language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source advanced LLM from OpenAI (architectural/size specifics not given in paper); evaluated in zero-shot in-context setting for spatial QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpartQA-Human, ReSQ (zero-shot baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Textual spatial QA requiring multi-hop reasoning over scene descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Zero-shot prompting; evaluated as a comparator to fine-tuned neuro-symbolic models and other in-context methods.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High zero-shot accuracy on SpartQA-Human (reported 77.62% Ver.1, 68.75% Ver.2 in table 2), demonstrating strong out-of-the-box spatial and commonsense reasoning compared to other LLMs and small PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SpartQA-Human zero-shot accuracy ~77.62% (Ver.1) and ~68.75% (Ver.2) reported in table 2; step-by-step prompting yields small additional gains (Table 4: GPT-4 zero-shot 77.62% -> step-by-step 78.32%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite strong zero-shot performance, on deeply compositional synthetic multi-hop benchmarks (StepGame high-k) dedicated neuro-symbolic fine-tuning can still surpass LLMs; GPT-4 remains a black-box baseline without the explicit logical supervision used by Q-Chain tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperforms other LLMs and few-shot methods on many splits (particularly zero-shot), but comparison shows neuro-symbolic fine-tuned smaller models can beat LLMs on targeted multi-hop synthetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4855.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4855.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-8B (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 8B (Meta) used for few-shot and CoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 8-billion parameter Llama-3 model used as an in-context learning baseline (zero-shot, few-shot, CoT) for spatial QA; shown competitive with GPT-3.5 on many YN tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neuro-symbolic Training for Spatial Reasoning over Natural Language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-3 family model with ~8B parameters (model card referenced), used in zero-shot/few-shot/CoT/CoS/LR in-context experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SpartQA-Human, ReSQ, StepGame (in-context baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Textual spatial QA requiring multi-hop spatial inference; used to test in-context prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>In-context learning with zero-shot, few-shot, Chain-of-Thought, Logical Representation, Chain-of-Symbol, and step-by-step context compression.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>CoT and step-by-step prompting improved performance; Llama-3 few-shot and CoT frequently performed competitively with GPT-3.5 (e.g., SpartQA-Human few-shot ~62.94% and CoT ~67.83% on some splits), but fell behind constrained fine-tuned models on deep multi-hop synthetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SpartQA-Human: Llama-3 zero-shot ~61.54% Ver.1, few-shot ~62.94%, CoT ~67.83% (table 2). On StepGame few-shot the Llama-3 per-k performance is low compared to fine-tuned models (e.g., few-shot ~38.01% at k1 and dropping across k in table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance degrades with increasing required hop depth; in-context symbolic prompts (LR/CoS) less effective than natural-language CoT; struggles more than Q-Chain fine-tuned models on deep synthetic multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Competitive with GPT-3.5 in some YN settings; outperformed by GPT-4; outperformed by Q-Chain fine-tuned smaller models on synthetic multi-hop benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4855.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4855.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT3(d2)+ASP / PistaQ*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pipelines combining LLM-based relation extraction with external logical solvers (e.g., GPT3(d2)+ASP and PistaQ*)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neuro-symbolic pipelines that first extract symbolic relations (often with an LLM) and then feed them to an external logic solver (ASP/Prolog) with engineered rules to perform exact multi-hop reasoning; achieve high accuracy on synthetic benchmarks but require formal representation at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neuro-symbolic Training for Spatial Reasoning over Natural Language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT3(d2) for relation extraction + ASP solver; PistaQ* (relation-extraction + external reasoning pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage pipelines: (1) use an LLM (GPT-3 variant) to map text to symbolic relations, (2) perform logical inference with an external Answer Set Programming (ASP) or other logic engine and engineered rule set. (Parameter counts not provided in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame / SpartQA-Human / synthetic SQA datasets</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Synthetic spatial QA benchmarks where exact formal representations can be generated/extracted and external solvers can perform multi-hop derivations deterministically.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Extraction of structured formal relations from natural language via LLMs followed by symbolic reasoning in a separate logic engine (ASP/Prolog) using handcrafted rules; not fine-tuned end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High accuracy on deep multi-hop synthetic datasets demonstrates that when accurate formalization is available, symbolic solvers produce correct multi-hop spatial inferences; e.g., GPT3(d2)+ASP achieves 88-93%+ across many StepGame hop depths (table 3) and PistaQ* reported ~75.52% on SpartQA-Human (table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>StepGame: GPT3(d2)+ASP per-k accuracies in table 3 are very high (examples: 92.60% at k1, 89.90% k2, 89.10% k3, 93.80% k4, 92.90% k5, ... down to 88.30% at k10). PistaQ* SpartQA-Human reported ~75.52% (table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Require formal symbolic representations at inference time (hard to obtain for realistic/long/ambiguous natural text); brittle on realistic domains where formal extraction fails; rely on external solvers so not end-to-end neural â€” heavier computational and engineering cost in deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>These pipelines often achieve higher accuracy on synthetic benchmarks than both in-context LLMs and pure fine-tuned PLMs, but the paper emphasizes their practical limitations (need formalization at test time) relative to the proposed Q-Chain fine-tuning that does not require symbolic inference at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-symbolic Training for Reasoning over Spatial Language', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SPARTQA: A textual question answering benchmark for spatial reasoning <em>(Rating: 2)</em></li>
                <li>Transfer learning with synthetic corpora for spatial role labeling and reasoning <em>(Rating: 2)</em></li>
                <li>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts <em>(Rating: 2)</em></li>
                <li>Coupling large language models with logic programming for robust and general reasoning from text <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Chain-of-symbol prompting for spatial reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4855",
    "paper_id": "paper-270619922",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "BERT-T+Q-Chain",
            "name_full": "BERT fine-tuned with SpaRTUN then neuro-symbolic Q-Chain constraints",
            "brief_description": "An encoder-based BERT model first tuned on the synthetic SpaRTUN corpus and then fine-tuned with example-specific neuro-symbolic logical constraints (Q-Chains) using the DomiKnowS framework and soft-logic loss to improve multi-hop spatial reasoning.",
            "citation_title": "Neuro-symbolic Training for Spatial Reasoning over Natural Language",
            "mention_or_use": "use",
            "model_name": "bert-based-uncased (BERT) with Q-Chain neuro-symbolic fine-tuning",
            "model_description": "BERT (bert-based-uncased) encoder architecture; initialized from standard BERT checkpoint, fine-tuned first on SpaRTUN synthetic data (BERT-T) then with per-example logical constraints (Q-Chain) using DomiKnowS and a primal-dual optimization; classification heads added for YN and multi-label FR tasks. (No parameter-count explicitly given in paper.)",
            "puzzle_name": "SpaRTUN / SpartQA-Human / ReSQ / StepGame (spatial question-answering benchmarks)",
            "puzzle_description": "Textual spatial question-answering benchmarks requiring multi-hop spatial inference (YN and find-relation (FR) questions) over scene descriptions; StepGame includes deep reasoning up to 10 hops.",
            "mechanism_or_strategy": "Neuro-symbolic fine-tuning: build resolution trees (Q-Chains) from annotated SpaRTUN facts using forward chaining, derive consistency constraints from spatial logic rules (converse, inverse, transitive, topological), convert to differentiable soft-logic (t-norm Product), and add constraint-violation terms to cross-entropy loss; optimized with primal-dual method via DomiKnowS.",
            "evidence_of_spatial_reasoning": "Quantitative improvements concentrated at higher hop-depths (StepGame k=4..10 shows clear gains); qualitative examples in figures show corrected multi-hop inferences; ablation-like comparisons vs same model without Q-Chain indicate improvements specifically for multi-hop spatial tasks.",
            "performance_metrics": "StepGame per-hop accuracy: k1=98.70%, k2=96.45%, k3=93.03%, k4=74.58%, k5=64.95%, k6=59.04%, k7=54.38%, k8=49.23%, k9=45.36%, k10=44.05% (table 3). SpartQA-Human: ~59.44% (Ver.1) and ~58.92% (Ver.2) reported (table 2). Improvements over BERT and other baselines reported, especially at larger k.",
            "limitations_or_failure_cases": "Requires reasoning-chain annotations (SpaRTUN) for constructing Q-Chains during training; improvements sometimes degrade on questions relying more on commonsense (ReSQ unclassified split); training needs GPU resources (~100 GPU hours reported) and annotated synthetic data; does not require constraints at inference but cannot guarantee solving general reasoning beyond spatial multi-hop.",
            "comparison_baseline": "Outperforms the same BERT baseline without Q-Chain on multi-hop synthetic benchmarks (StepGame). LLMs (GPT-3.5/GPT-4/Llama-3) outperform on commonsense-heavy or unclassified ReSQ subset; pipeline systems using relation-extraction + external solvers (e.g., GPT3(d2)+ASP, PistaQ*) can match or exceed accuracy in synthetic settings but require formal representations at inference and external solvers.",
            "uuid": "e4855.0",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Flan-T5-T+Q-Chain",
            "name_full": "Flan-T5-base fine-tuned on SpaRTUN then with neuro-symbolic Q-Chain constraints",
            "brief_description": "A generative Flan-T5-base model adapted with LoRA and then neuro-symbolically fine-tuned using the same Q-Chain constraint integration to improve multi-hop spatial FR and YN QA.",
            "citation_title": "Neuro-symbolic Training for Spatial Reasoning over Natural Language",
            "mention_or_use": "use",
            "model_name": "Flan-T5-base (with LoRA adapters) fine-tuned and Q-Chain constrained",
            "model_description": "Flan-T5-base generative encoder-decoder model; LoRA adapters used to reduce fine-tuning compute; first fine-tuned on SpaRTUN (Flan-T5-T) then further fine-tuned with per-example logical constraints integrated via DomiKnowS soft-logic t-norm product loss; classification of YN by first-token probability; FR handled via multi-label objective when applicable.",
            "puzzle_name": "SpaRTUN / SpartQA-Human / ReSQ / StepGame (spatial QA benchmarks)",
            "puzzle_description": "Benchmarks of textual spatial reasoning requiring compositional and multi-hop logical inference about relative positions and topological relations among objects.",
            "mechanism_or_strategy": "Same Q-Chain construction and integration as encoder-based model: forward-chaining to create reasoning chains, convert logical spatial rules into differentiable soft constraints (t-norm Product), add to training objective; LoRA used to limit adapter parameter changes.",
            "evidence_of_spatial_reasoning": "Consistent accuracy increases on synthetic SpartQA-Human and StepGame benchmarks after Q-Chain integration; larger relative gains than encoder BERT in some splits, particularly for FR tasks requiring deeper hops (e.g., SpartQA-Human k=2 improvement to ~81.33% in one reported split).",
            "performance_metrics": "SpartQA-Human and ReSQ: Flan-T5-T+Q-Chain reported ~55.94% (Ver.1) and ~61.61% (Ver.2) on SpartQA-Human (table 2) and strong gains on ReSQ k=2 (e.g., 81.33% reported in table 2 for a Flan variant on some split). StepGame improvements reported though per-k numbers for Flan-T5 variants are not enumerated as fully as BERT in table 3.",
            "limitations_or_failure_cases": "Same as other neuro-symbolic tuning: requires annotated chains during training, less effective on commonsense-heavy realistic splits (ReSQ unclassified), and depends on quality/coverage of available spatial logical rules; LoRA reduces parameters but may limit model adaptation capacity.",
            "comparison_baseline": "Improves more than BERT when constrained, but closed-source LLMs (GPT-3.5/4) remain superior on commonsense-rich splits; pipeline approaches with external solvers achieve high accuracy on synthetic tasks but need formal representations at inference.",
            "uuid": "e4855.1",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "BERT-T (baseline)",
            "name_full": "BERT fine-tuned on SpaRTUN (no Q-Chain constraints)",
            "brief_description": "BERT model pre-finetuned on SpaRTUN synthetic data and then fine-tuned on target dataset without neuro-symbolic constraints, serving as an experimental baseline to measure the effect of Q-Chain training.",
            "citation_title": "Neuro-symbolic Training for Spatial Reasoning over Natural Language",
            "mention_or_use": "use",
            "model_name": "bert-based-uncased (BERT) fine-tuned on SpaRTUN",
            "model_description": "Standard BERT checkpoint first fine-tuned on SpaRTUN synthetic dataset (referred to as BERT-T) then fine-tuned on downstream spatial QA datasets without integrating logical constraints; classification heads for YN/FR tasks used.",
            "puzzle_name": "SpaRTUN / SpartQA-Human / ReSQ / StepGame",
            "puzzle_description": "Textual spatial QA benchmarks with multi-hop reasoning requirements.",
            "mechanism_or_strategy": "Standard supervised fine-tuning on the target spatial QA data (and pre-fine-tuning on SpaRTUN), no constraint-based loss terms.",
            "evidence_of_spatial_reasoning": "Improved multi-hop performance relative to plain BERT when pre-finetuned on SpaRTUN, but shows smaller gains than Q-Chain constrained variants at higher hop depths.",
            "performance_metrics": "StepGame per-hop example: k1=98.50%, k2=95.32%, k3=93.26%, k4=76.78%, k5=66.36%, k6=58.76%, k7=53.70%, k8=46.27%, k9=42.71%, k10=40.12% (table 3). SpartQA-Human: BERT-T reported ~55.94% Ver.1 (table 2).",
            "limitations_or_failure_cases": "Less capable than Q-Chain constrained variant at deep multi-hop reasoning; still limited on commonsense-rich questions compared to large LLMs.",
            "comparison_baseline": "Used as the main ablation baseline vs BERT-T+Q-Chain; Q-Chain yields measurable improvements at larger reasoning depths.",
            "uuid": "e4855.2",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5 (in-context)",
            "name_full": "GPT-3.5 (OpenAI) used in zero-shot and few-shot in-context learning",
            "brief_description": "A large autoregressive language model (GPT-3.5 family) used in zero-shot, few-shot, and chain-of-thought/representation prompting to answer spatial QA questions without fine-tuning.",
            "citation_title": "Neuro-symbolic Training for Spatial Reasoning over Natural Language",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Closed-source large autoregressive LLM from OpenAI (exact parameter count not provided in paper); evaluated in zero-shot, few-shot, and augmented prompting regimes (CoT, LR, CoS) for spatial QA via in-context learning.",
            "puzzle_name": "SpartQA-Human, ReSQ, StepGame (in-context baselines)",
            "puzzle_description": "Textual spatial QA benchmarks; GPT-3.5 is applied directly to story+question prompts without gradient updates.",
            "mechanism_or_strategy": "In-context learning: zero-shot, few-shot (4 examples), chain-of-thought (CoT) prompting with manually-crafted rationales, logical representation (LR) prompts, chain-of-symbol (CoS), and step-by-step context compression.",
            "evidence_of_spatial_reasoning": "CoT prompting substantially improves performance versus zero/few-shot; reported qualitative examples and improvements on SpartQA-Human and StepGame; CoT often outperforms LR/CoS for in-context spatial tasks.",
            "performance_metrics": "SpartQA-Human: GPT-3.5 (zero-shot) ~58.04%, (few-shot) ~62.23%, (CoT) ~65.73% accuracy on reported splits (table 2). Step-by-step prompting improved results modestly (Table 4): zero-shot 58.04% -&gt; step-by-step 63.64%. On StepGame per-hop, GPT3 few-shot and CoT are much lower than fine-tuned constrained models at high k (examples: GPT3 few-shot ~55% at k1 dropping rapidly at higher k; see table 3).",
            "limitations_or_failure_cases": "Performance falls off sharply as required reasoning hops increase (especially in StepGame high-k); suffers on structured multi-hop spatial reasoning compared to models fine-tuned with explicit logical supervision; improvements depend strongly on effective CoT prompts and context simplification.",
            "comparison_baseline": "Outperforms small fine-tuned PLMs on commonsense-rich realistic splits (ReSQ unclassified) but underperforms neuro-symbolic fine-tuned smaller models (BERT-T+Q-Chain) on deep synthetic multi-hop benchmarks.",
            "uuid": "e4855.3",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI) large multimodal-capable LLM",
            "brief_description": "State-of-the-art large language model used as a strong zero-shot baseline for spatial QA; achieves highest zero-shot scores reported on SpartQA-Human.",
            "citation_title": "Neuro-symbolic Training for Spatial Reasoning over Natural Language",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source advanced LLM from OpenAI (architectural/size specifics not given in paper); evaluated in zero-shot in-context setting for spatial QA benchmarks.",
            "puzzle_name": "SpartQA-Human, ReSQ (zero-shot baseline)",
            "puzzle_description": "Textual spatial QA requiring multi-hop reasoning over scene descriptions.",
            "mechanism_or_strategy": "Zero-shot prompting; evaluated as a comparator to fine-tuned neuro-symbolic models and other in-context methods.",
            "evidence_of_spatial_reasoning": "High zero-shot accuracy on SpartQA-Human (reported 77.62% Ver.1, 68.75% Ver.2 in table 2), demonstrating strong out-of-the-box spatial and commonsense reasoning compared to other LLMs and small PLMs.",
            "performance_metrics": "SpartQA-Human zero-shot accuracy ~77.62% (Ver.1) and ~68.75% (Ver.2) reported in table 2; step-by-step prompting yields small additional gains (Table 4: GPT-4 zero-shot 77.62% -&gt; step-by-step 78.32%).",
            "limitations_or_failure_cases": "Despite strong zero-shot performance, on deeply compositional synthetic multi-hop benchmarks (StepGame high-k) dedicated neuro-symbolic fine-tuning can still surpass LLMs; GPT-4 remains a black-box baseline without the explicit logical supervision used by Q-Chain tuned models.",
            "comparison_baseline": "Outperforms other LLMs and few-shot methods on many splits (particularly zero-shot), but comparison shows neuro-symbolic fine-tuned smaller models can beat LLMs on targeted multi-hop synthetic tasks.",
            "uuid": "e4855.4",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama-3-8B (in-context)",
            "name_full": "Llama 3 8B (Meta) used for few-shot and CoT",
            "brief_description": "Open-source 8-billion parameter Llama-3 model used as an in-context learning baseline (zero-shot, few-shot, CoT) for spatial QA; shown competitive with GPT-3.5 on many YN tasks.",
            "citation_title": "Neuro-symbolic Training for Spatial Reasoning over Natural Language",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B",
            "model_description": "Open-source LLaMA-3 family model with ~8B parameters (model card referenced), used in zero-shot/few-shot/CoT/CoS/LR in-context experiments.",
            "puzzle_name": "SpartQA-Human, ReSQ, StepGame (in-context baselines)",
            "puzzle_description": "Textual spatial QA requiring multi-hop spatial inference; used to test in-context prompting strategies.",
            "mechanism_or_strategy": "In-context learning with zero-shot, few-shot, Chain-of-Thought, Logical Representation, Chain-of-Symbol, and step-by-step context compression.",
            "evidence_of_spatial_reasoning": "CoT and step-by-step prompting improved performance; Llama-3 few-shot and CoT frequently performed competitively with GPT-3.5 (e.g., SpartQA-Human few-shot ~62.94% and CoT ~67.83% on some splits), but fell behind constrained fine-tuned models on deep multi-hop synthetic tasks.",
            "performance_metrics": "SpartQA-Human: Llama-3 zero-shot ~61.54% Ver.1, few-shot ~62.94%, CoT ~67.83% (table 2). On StepGame few-shot the Llama-3 per-k performance is low compared to fine-tuned models (e.g., few-shot ~38.01% at k1 and dropping across k in table 3).",
            "limitations_or_failure_cases": "Performance degrades with increasing required hop depth; in-context symbolic prompts (LR/CoS) less effective than natural-language CoT; struggles more than Q-Chain fine-tuned models on deep synthetic multi-hop reasoning.",
            "comparison_baseline": "Competitive with GPT-3.5 in some YN settings; outperformed by GPT-4; outperformed by Q-Chain fine-tuned smaller models on synthetic multi-hop benchmarks.",
            "uuid": "e4855.5",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT3(d2)+ASP / PistaQ*",
            "name_full": "Pipelines combining LLM-based relation extraction with external logical solvers (e.g., GPT3(d2)+ASP and PistaQ*)",
            "brief_description": "Neuro-symbolic pipelines that first extract symbolic relations (often with an LLM) and then feed them to an external logic solver (ASP/Prolog) with engineered rules to perform exact multi-hop reasoning; achieve high accuracy on synthetic benchmarks but require formal representation at inference.",
            "citation_title": "Neuro-symbolic Training for Spatial Reasoning over Natural Language",
            "mention_or_use": "use",
            "model_name": "GPT3(d2) for relation extraction + ASP solver; PistaQ* (relation-extraction + external reasoning pipeline)",
            "model_description": "Two-stage pipelines: (1) use an LLM (GPT-3 variant) to map text to symbolic relations, (2) perform logical inference with an external Answer Set Programming (ASP) or other logic engine and engineered rule set. (Parameter counts not provided in paper.)",
            "puzzle_name": "StepGame / SpartQA-Human / synthetic SQA datasets",
            "puzzle_description": "Synthetic spatial QA benchmarks where exact formal representations can be generated/extracted and external solvers can perform multi-hop derivations deterministically.",
            "mechanism_or_strategy": "Extraction of structured formal relations from natural language via LLMs followed by symbolic reasoning in a separate logic engine (ASP/Prolog) using handcrafted rules; not fine-tuned end-to-end.",
            "evidence_of_spatial_reasoning": "High accuracy on deep multi-hop synthetic datasets demonstrates that when accurate formalization is available, symbolic solvers produce correct multi-hop spatial inferences; e.g., GPT3(d2)+ASP achieves 88-93%+ across many StepGame hop depths (table 3) and PistaQ* reported ~75.52% on SpartQA-Human (table 2).",
            "performance_metrics": "StepGame: GPT3(d2)+ASP per-k accuracies in table 3 are very high (examples: 92.60% at k1, 89.90% k2, 89.10% k3, 93.80% k4, 92.90% k5, ... down to 88.30% at k10). PistaQ* SpartQA-Human reported ~75.52% (table 2).",
            "limitations_or_failure_cases": "Require formal symbolic representations at inference time (hard to obtain for realistic/long/ambiguous natural text); brittle on realistic domains where formal extraction fails; rely on external solvers so not end-to-end neural â€” heavier computational and engineering cost in deployment.",
            "comparison_baseline": "These pipelines often achieve higher accuracy on synthetic benchmarks than both in-context LLMs and pure fine-tuned PLMs, but the paper emphasizes their practical limitations (need formalization at test time) relative to the proposed Q-Chain fine-tuning that does not require symbolic inference at inference time.",
            "uuid": "e4855.6",
            "source_info": {
                "paper_title": "Neuro-symbolic Training for Reasoning over Spatial Language",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SPARTQA: A textual question answering benchmark for spatial reasoning",
            "rating": 2,
            "sanitized_title": "spartqa_a_textual_question_answering_benchmark_for_spatial_reasoning"
        },
        {
            "paper_title": "Transfer learning with synthetic corpora for spatial role labeling and reasoning",
            "rating": 2,
            "sanitized_title": "transfer_learning_with_synthetic_corpora_for_spatial_role_labeling_and_reasoning"
        },
        {
            "paper_title": "Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts",
            "rating": 2,
            "sanitized_title": "stepgame_a_new_benchmark_for_robust_multihop_spatial_reasoning_in_texts"
        },
        {
            "paper_title": "Coupling large language models with logic programming for robust and general reasoning from text",
            "rating": 2,
            "sanitized_title": "coupling_large_language_models_with_logic_programming_for_robust_and_general_reasoning_from_text"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Chain-of-symbol prompting for spatial reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofsymbol_prompting_for_spatial_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.01768625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Neuro-symbolic Training for Spatial Reasoning over Natural Language
29 May 2025</p>
<p>Tanawan Premsri premsrit@msu.edu 
Department of Computer Science
Michigan State University</p>
<p>Parisa Kordjamshidi 
Department of Computer Science
Michigan State University</p>
<p>Neuro-symbolic Training for Spatial Reasoning over Natural Language
29 May 20255CF3B0D20A4A1392BF4BFC4D97D255F3arXiv:2406.13828v3[cs.CL]Context + Question Language Model Answer
Spatial reasoning based on natural language expressions is essential for everyday human tasks.This reasoning ability is also crucial for machines to interact with their environment in a human-like manner.However, recent research shows that even state-of-the-art language models struggle with spatial reasoning over text, especially when facing nesting spatial expressions.This is attributed to not achieving the right level of abstraction required for generalizability.To alleviate this issue, we propose training language models with neuro-symbolic techniques that exploit the spatial logical rules as constraints, providing additional supervision to improve spatial reasoning and question answering.Training language models to adhere to spatial reasoning rules guides them in making more effective and general abstractions for transferring spatial knowledge to various domains.We evaluate our approach on existing spatial question-answering benchmarks.Our results indicate the effectiveness of our proposed technique in improving language models in complex multi-hop spatial reasoning over text.</p>
<p>Introduction</p>
<p>Spatial language conveys information about the location and translocation of objects/entities in space.Reasoning over spatial language is required for natural language processing systems to answer "Where" questions and identify the relative spatial relations between objects explained in the text, such as left and right.Figure 1, demonstrates a spatial language comprehension and questionanswering problem.Understanding spatial language turns out to be crucial for many AI applications, including question-answering systems, grounding language into visual modality (Zhang et al., 2021;Zhang and Kordjamshidi, 2022;Liu et al., 2023), navigating robots (Sisbot et al., 2007;Yadollahi et al., 2023) and even in more specific</p>
<p>SpartQA-Human</p>
<p>Context: There are three blocks called A, B, and C. A is to the left of B and B is to the left of C. In A, there is a small yellow square near and to the left of a large yellow circle.The large circle is near and to the left of a large blue triangle.In B, there is a small yellow square near and below a large blue triangle.There is a medium black triangle near and above the large blue triangle.In C, there is a small black square far to the left and above a small blue circle.The blue circle is near and to the left of a large yellow square.YN Question: Is the small black thing to the right of medium black thing?Answer: Yes FR Question: What is the position of small black thing relative to medium black thing?Answer: Right ReSQ Context: A tourist group is visiting a school.The school kids are sitting at their desks, singing a song and clapping their hands, with their teacher clapping as well.Six of the tourists are standing behind them and are watching them, one tourist is taking a photo.YN Question: Are the tourists in the school?Answer: Yes  fields such as medical domain (Atif et al., 2007;Datta et al., 2020;Gong et al., 2024).</p>
<p>Despite the high performance of Large Language models (LLMs) in many NLP and vision problems (Brown et al., 2020;OpenAI, 2023), recent studies highlight the shortcomings in complex spatial reasoning over text (Bang et al., 2023;Yang et al., 2023;Mirzaee and Kordjamshidi, 2023;Liu et al., 2023;Chen et al., 2024).This paper focuses on complex, that is, multi-hop spatial reasoning over text.However, our research findings can advance complex reasoning over text in general.Our main idea is to exploit spatial logical rules of reasoning in training language models.</p>
<p>Explicit logic, in combination with LLMs, has been used in some related works.In particular, Yang et al. 2023 integrated explicit symbolic reasoning with LLMs in a pipeline.The LLMs map natural language to symbolic representation, and this formal representation is passed to an Answer Set Programming (ASP) solver to conduct logical reasoning.Similarly, Mirzaee and Kordjamshidi 2023 passed formal representations obtained from LLMs to Prolog to conduct logical reasoning and answer spatial questions.These approaches rely on external reasoning tools and require accurate formal representations generated at inference time.Obtaining such formal representations for realistic domain natural language is hard and, as shown in (Mirzaee and Kordjamshidi, 2023), limits their effectiveness only to synthetic benchmarks.</p>
<p>Here, we aim to address the limitations of utilizing spatial logic in realistic domains.In our proposed technique, we fine-tune language models using spatial logical rules to guide them in capturing the level of abstraction needed for general spatial reasoning.In particular, we train the models to minimize both cross-entropy loss and violation from logical constraints.We hypothesize that incorporating logical knowledge as a source of supervision, even using synthetic data, enriches the models' abstract reasoning over spatial concepts, further enhancing their generalizability.A key advantage of our approach is that it does not require full coverage of logical knowledge.Any partially available logical knowledge can be utilized during training.Additionally, formal knowledge representation is not needed at inference time, which is crucial because generating formal representations during inference is a challenging task in itself.Our technique also avoids reliance on external reasoning tools, which can add computational complexity in real-time applications.</p>
<p>We use the available three benchmarks for spatial question answering, SpartQA-Human (Mirzaee et al., 2021), ReSQ (Mirzaee and Kordjamshidi, 2022), and StepGame (Shi et al., 2022) to evaluate our proposed method.Our improved results confirm our hypothesis about the impact of neurosymbolic training on generalizability.The contribution of this paper can be summarized as follows1 , (1) We exploit the available logical spatial knowledge in tuning language models to improve their spatial reasoning.(2) We provide extensive experimental results and analysis on the existing benchmarks.Our results consistently confirm the effectiveness of our proposed approach in both encoderbased and generative language models and their generalizability for transferring knowledge to different domains; (3) We show that tuning smaller models with our proposed method generalizes better on out-of-domain and complex reasoning compared to using prompt-engineered, very large language models.</p>
<p>Investigating the reasoning capability of NLP models has become an important research topic given the instability in the performance of LLMs (Gendron et al., 2024;Valmeekam et al., 2022;Feng et al., 2024;Chen et al., 2024;Bang et al., 2023).Typically, this ability is assessed through questionanswering (QA) benchmarks (Weston et al., 2016;Zhou et al., 2020;Tan et al., 2023).However, fewer studies are focused on spatial reasoning over text.Recent benchmarks, such as SpaRTUN (Mirzaee and Kordjamshidi, 2022) and StepGame (Shi et al., 2022;Li et al., 2024), have been developed to address this issue by providing comprehensive evaluation resources for spatial question answering.The current results using the existing benchmarks indicate that LLMs struggle with spatial reasoning over text, even on synthetic datasets (Mirzaee and Kordjamshidi, 2022;Yang et al., 2023;Bang et al., 2023).</p>
<p>Several works have attempted to improve LLMs' spatial reasoning capabilities.Mirzaee and Kordjamshidi 2022 fine-tunes LLMs with synthetic data, which improves their multi-hop reasoning abilities, even when applied to realistic domains.Another approach in (Shi et al., 2022) designed a specialized memory network based on a recurrent neural network for deep reasoning tasks, but it was less effective than tuning pre-trained language models (PLMs).Some papers focus on enhancing spatial reasoning by in-context learning.For instance, Sharma 2023 proposed a prompting technique that decomposes the target spatial question into simpler questions and infers the answer to the final query based on the information gained from them.Another approach, proposed by Hu et al. 2024, replaced textual explanations in the Chain-of-Thought (CoT) by symbolic representations.While this method improved spatial understanding in simple environments, it was less applicable to diverse and complex spatial relations settings.Lastly, some studies employ neuro-symbolic modeling that involves LLMs in the pipeline (Yang et al., 2023;Mirzaee and Kordjamshidi, 2023).These approaches extract relations and obtain formal representations using LLMs.Then, the formal representation is passed to an external solver that includes engineered rules of reasoning to conduct logical reasoning and answer spatial questions.While this method significantly improved multihop reasoning, it required formal representations at inference time, which posed a challenge.Furthermore, formalizing commonsense reasoning is difficult, if not impossible, for realistic domains.</p>
<p>Unlike the previous studies, we exploit available spatial logical knowledge as a set of constraints in fine-tuning language models.Compared to hard pipelines, this approach has the advantage that logical knowledge does not need to be complete and cover all possible situations.Our approach utilizes synthetic data and logical representations during training only.Numerous research studies have highlighted the utility of logical knowledge in solving question-answering tasks.For instance, Prager et al. 2004 demonstrates that answering sets of questions jointly using their logical relationships enhances the performance in reasoning tasks.Amizadeh et al. 2020 directly used the logical formalization of the question as input to the model to improve the logical reasoning of vision and language models.Sun et al. 2022 proposed framework with two main components.The first component is for encoding logical knowledge, and the second is for entailment of that knowledge.Lastly, Kamali et al. 2024 proposed a framework that integrates logical reasoning with generalizable grounding interpretation to enhance both grounding and reasoning in vision and language models.</p>
<p>Utilizing logical knowledge has been proven effective in solving many other NLP problems.For example, Lu et al. 2022 uses a heuristic function inspired by the A* algorithm to restrict the output of generative language models with a set of constraints, while Qin et al. 2022 uses a sampling method looking for the most optimal solution that satisfies the constraints.Both of these techniques apply the constraints during inference, which differs from our proposed method for using them during the training.Lastly, Rajaby Faghihi et al. 2023 provided a benchmark to evaluate the benefits of logical constraints in several NLP problems under the DomiKnowS framework (Rajaby Faghihi et al., 2021).In this paper, we propose a novel approach for formulating spatial reasoning knowledge as logical constraints and exploit the existing techniques for the actual integration.We utilize constraints solely during training, eliminating the need for access to logical constraints during inference where using them might be problematic for realistic domains and real-time applications.</p>
<p>Methodology</p>
<p>Problem Definition</p>
<p>For the spatial QA task, the input includes a textual context C that describes a scene and a question Q about the spatial relations between objects in the scene.The output is the answer(s) to the question, denoted as A. There are two types of questions/answers: Yes/No (YN) and find-relation (FR).The answer domain is restricted to A = {Y es, N o} for YN questions.The answers to FR questions depend on the dataset and the types of relations covered.For example, the answer domain for FR can be A = {lef t, right, above, below,. . .}.</p>
<p>Backbone Language Models</p>
<p>We use both encoder-based models from the BERT family (Devlin et al., 2019) as well as generative models like Flan-T5 (Chung et al., 2024) to assess the effectiveness of our proposed fine-tuning approach.We used other LLMs, such as GPT family and Llama3-8B, as comparison baselines without fine-tuning.</p>
<p>Encoder-based Language Models</p>
<p>We utilize BERT as the backbone architecture to consistently build on top of previous results in Mirzaee and Kordjamshidi (2022).We fine-tune BERT by adding classification layers for QA tasks.We employ a binary classification layer for the YN domain while using multiple binary classification layers for the FR domain.The motivation behind using multiple binary classification layers instead of one multi-label classification layer is that one question can have multiple labels as responses.The number of binary classification layers in the FR domain depends on the number of possible answers, |A|, in the problem setting.The input to the BERT model is constructed by concatenating the question, Q, separator token, [SEP ], and the context C.After feeding the input to BERT, we use the [CLS] token from the last layer as the input to the classification layer(s) for final answer prediction.</p>
<p>Generative Language Models</p>
<p>We select Flan-T5 (Chung et al., 2024) as the opensource generative baseline for fine-tuning.In the YN setting, processing the output is straightforward, as the first token can represent the response directly generated by the model.We compare the probability of the Yes and No tokens and select the one with the highest probability as the answer.Obtaining consistency constraints from spatial logical rules and answers inferred from questions generated using intermediate facts in Q-Chain.3. Incorporating obtained consistency constraints into the loss function using differentiable soft logic during tuning.For more details, refer to Section 3.3.</p>
<p>However, this approach is not feasible in the FR setting due to the multi-label questions requiring multiple answers.The input prompt to the Flan-T5 model is structured as follows: "Answer based on the context: C Q."</p>
<p>For In-context Learning, we select Llama3-8B (AI@Meta, 2024), GPT3.5 (Brown et al., 2020), and GPT-4 (OpenAI, 2023) as the baseline LLMs using Zero-shot, and Few-shot settings.Zero-shot.We prompt the LLMs to provide answers based on the scene description and the question without including any examples.Few-shot.We randomly sample four pairs of questions and answers from the training dataset and concatenate them to the context and target question to form the few-shot setting.</p>
<p>Training with Spatial Logic</p>
<p>To address the challenge of multi-hop spatial reasoning, we employ symbolic knowledge during training.We assume a formal knowledge base (KB) of rules, referred to as spatial logic, is given during training time.This spatial logic does not need to be exhaustive or cover all reasoning aspects; any available knowledge can be used to enhance the reasoning capabilities of the language model.This work uses 79 reasoning rules taken from Mirzaee and Kordjamshidi (2022).These rules are divided into four categories: converse, inverse, transitive, and transitive + topological.The KB covers rules between 15 spatial relations, including directional and topological relations, such as "If x is above y, then y below x." denoted formally as Above(x, y) â‡’ Below(y, x).</p>
<p>Our main hypothesis is that supervision from high-level logical knowledge allows the model to capture more abstract patterns, thereby improving generalization to other domains.To exploit the spatial logic in the training phase, we follow three steps: (1) Creating the chain of reasoning for each training example.(2) Obtaining consistency constraints from the chain of reasoning.(3) Incorporating constraints in the loss function.Please note that we will need the reasoning chain annotations for fine-tuning with spatial logic.SpaRTUN is the only synthetic dataset that provides such annotated formal representations, making it a suitable source of supervision for our tuning purpose.Creating example-specific rules.We exploit Spatial Logic by automatically augmenting each training example with a chain of questions, denoted as Q-Chain.This process is illustrated in Figure 2.</p>
<p>We infer the answer to the final query from the input context using the logical resolution tree, which outlines the logical steps for implication.Our synthetic training data (i.e., SpaRTUN) provides logi-cal representations of the initial facts expressed in the context.We build the resolution tree using the forward chaining algorithm to infer a specific target fact given a set of initial facts.In our example, the initial facts are "The white is above the orange", denoted as q 1 , and "The red is above the white", denoted as q 2 .The target fact is "The orange below the red," denoted as t.The forward chaining approach is a reasoning method that starts with the initial facts in the knowledge base.It performs an exhaustive search for all applicable inference rules and applies them to the current facts to iteratively infer new ones until the target fact is reached or no new facts are inferred (Nickerson, 1994).In the example of Figure 2, we apply the spatial converse rules to the initial fact q 1 and q 2 .This approach yields the intermediate inferred facts, q 3 , "The orange is below the white," from q 1 , and q 4 , "The white is below the red," from q 2 .Finally, we search for the applicable spatial rule to combine q 3 and q 4 , resulting in the target fact, t, "The orange is below the red."Obtaining consistency constraints.In this step, we obtain a set of consistency constraints based on the given resolution tree.We denote the truth value of a fact q i in the Q-Chain as T ruth(q i ), which will be True if the fact is True and otherwise is False.We create a consistency constraint for each step of the Q-Chain.For example, in Figure 2, step R 1 is applied to q 1 to infer q 3 .This step implies that the T ruth(q 1 ) should be consistent with the T ruth(q 3 ).In other words, q 1 , "white above orange," implies q 3 : "orange below white,".The corresponding formal constraint for this example will be T ruth(q 1 ) â‡’ T ruth(q 3 ).Incorporating Constraints.Our fine-tuning objective is to minimize the violation from the set of logical consistency constraints given for each training example.However, the logical expressions are not differentiable.To form a differentiable loss function, we use a soft logical surrogate of the constraints in the loss.There are various types of conversions for mapping logical constraints into differentiable soft logic, including t-norm Product, t-norm Åukasiewicz, and t-norm GÃ¶del (Li et al., 2019).We use the t-norm Product to convert, which means, Â¬A becomes 1 âˆ’ a, A âˆ§ B becomes ab, A âˆ¨ B becomes a + b âˆ’ ab, and A â‡’ B becomes min(1, b a ), where A and B are concepts with probabilities a and b respectively.During the training, the probability of the truth of the facts (T ruth(q i )) involved in the constraints is obtained from the model prediction for computing the loss.We build on previous studies of optimizing an objective that combines the task performance and soft constraints (Li et al., 2019;Asai and Hajishirzi, 2020;Bamas et al., 2020) and form the following objective,
âˆ‡ w L(w) = âˆ‡ w L(w) + K k=1 Î» k âˆ‡ w h k (w), (1)
where L(w) is the task-performance loss function, i.e., Cross-Entropy loss, K is the number of logical constraints related to the current Q-Chain, h k measures the violation from a given logical constraint in soft logic differentiable form, and Î» k is a learning parameter indicating the importance of obeying k th rule.We set the Î» k to 1 for all constraints to ensure equal importance.</p>
<p>In fact, in this work, we utilize the implementation in the DomiKnowS framework (Rajaby Faghihi et al., 2021) to integrate the logical constraints.This framework provides a declarative language for incorporating symbolic knowledge as logical constraints.We present the original logical forms in the program, and DomiKnowS automatically converts them into a differentiable form for inclusion in the loss function.We employ an optimization technique referred to as the Primal-Dual (PD) program that uses a dual formulation of the original objective (Bamas et al., 2020) for fine-tuning.Programming for building a model in DomiKnowS requires defining a graph of concepts and relationships, along with logical rules between concepts.The program for our problem setting is provided in Appendix E.</p>
<p>In-context Learning with Spatial Logic</p>
<p>We design a set of experiments to analyze and compare the LLMs' performance against our proposed fine-tuning method.We utilize spatial logical reasoning to manually create the rationale of few-shot examples for our in-context learning settings, including Chain-of-Thoughts (Wei et al., 2022), Logical Representation, and Chain-of-Symbols (Hu et al., 2024).Chain-of-Thought (CoT).To enable CoT, we provide instructions that require an explanation before answering the question.For each example response, we include a manually crafted rationale based on spatial logical rules.We provide these examples with instructions and target questions as input to the model to generate the answer.Multiple examples of this CoT setting are demonstrated in Appendix C.2. Logical Representation (LR).In this setting, we follow the same prompt structure as CoT but use logical representations instead of natural language.We obtain the first order logical representations (predicate-argument structures) by manually converting the CoT explanations.The predicateargument structure R(obj i , obj j ) represents the relation R between obj i and obj j , where obj i and obj j denote object i and object j, respectively.An example of the LR counterpart of the CoT phrase is in Table 1.Chain of Symbol (CoS).This symbolic representation was introduced by Hu et al. (2024).We explicitly follow the proposed process in Hu et al. (2024)  Step-by-Step Prompting.From the SpartQA-Human dataset, we observed that scene descriptions are often complex and lengthy.To deal with this issue, we introduce an additional prompting step using LLMs to break down the context before answering the question.Each generated line of context contains a simple spatial description.For example, given the original context, "There exists a big red square and a medium red square in a block called A.  Mirzaee and Kordjamshidi (2022).</p>
<p>SpartQA-Human (Ver.2) We follow the methodologies from (Mirzaee et al., 2021) to increase the size of the human annotated dataset.The objective is to enrich the SpartQA-Human with a more extensive test using diverse patterns and questions.</p>
<p>Annotators included authors and an undergraduate student who was paid as a research assistant.We extended both types of YN and FR questions.</p>
<p>ReSQ (Kordjamshidi et al., 2017;Mirzaee and Kordjamshidi, 2022) is a small realistic SQA dataset with multi-hop reasoning.The depth of reasoning is smaller than other synthetic datasets, but it usually requires commonsense knowledge to infer the answer.The annotations include the depth of reasoning denoted as k, which can be 1 or 2. The questions that required complex reasoning and commonsense knowledge are annotated as unclassified depth.12.30% of the test examples fall under the k=1 split, 23.93% fall under the k=2 split, and the rest are unclassified.This dataset only has YN questions.We use this dataset to evaluate the performance of our models in realistic domains.</p>
<p>StepGame (Shi et al., 2022) is a synthetic SQA dataset that contains a diverse set of multi-hop reasoning questions involving spatial relations, with annotated reasoning depth denoted as k .The questions in this dataset include up to 10 hops of reasoning, allowing for a comprehensive evaluation of models' reasoning capability at varying depths.This dataset contains only FR questions.</p>
<p>The domain of answer is a = {lef t, right, above, below, overlap, lower-lef t, lower-right, upperlef t, upper-right}.</p>
<p>Experimental Models for Tuning</p>
<p>BERT-family.We use bert-based-uncased as the initial checkpoint, referred to as BERT.This model is only fine-tuned with the target dataset in each experiment.In some of the initial experiments we used RoBERTa as well.However, due to insignificance of the differences, we continued other model variations with BERT.The results of RoBERTa can be found in Appendix B. BERT-T.We fine-tune the model with SpaRTUN first.Then, we further fine-tune with the target dataset of each experiment.</p>
<p>BERT-T + Q-Chain.We adhere to the methodology explained in Section 3.3 to integrate logical knowledge into the BERT-T model via finetuning with constraints using the DomiKnowS framework (Rajaby Faghihi et al., 2021).Flan-T5.We follow the methodology described in Section 3.2.2.We selected the flan-t5-base in this experiment as the starting checkpoint.To reduce computational costs by decreasing the number of training parameters, we employ the LoRA adapter (Hu et al., 2022).</p>
<p>Flan-T5-T.We utilize the same setting as the Flan-T5 model.We fine-tune the model first with SpaR-TUN and then with the target dataset.</p>
<p>Flan-T5-T + Q-Chain.We integrate the logical knowledge into Flan-T5-T model using proposed method in section 3.3.The backbone is the same as the baseline model without any changes.</p>
<p>Experimental Setup.We used the same hyperparameters across all experiments.The learning rate was selected from {1e âˆ’5 , 8e âˆ’6 , 1e âˆ’6 }.For SpartQA-Human and ReSQ, the number of epochs was 100, while for StepGame, it was 30.For fine-tuning with SpaRTUN, we used 12 epochs for BERT and 8 epochs for Flan-T5.The final hyperparameters were chosen using the development set of the target dataset.In all experiments, we used Cross-Entropy Loss as the loss function and Adam as the optimizer, with the amsgrad parameter set to True.The experiments were run on 8 A6000 GPUs, requiring approximately 100 GPU hours.</p>
<p>Experimental Models for In-context</p>
<p>We evaluate the performance of three selected LLMs for in-context learning setting including GPT3.5, GPT-4, and Llama3-8B, using ReSQ and SpartQA-Human to compare with our fine-tuning approach.</p>
<p>The selected examples and prompts for each in-context learning method can be found in the Appendix C. Zero-shot.We directly ask LLMs to answer the question given the corresponding context.Few-shot.We provide LLMs with four randomly selected examples from the training data.</p>
<p>CoT.We augment the few-shot setting with reasoning explanations along with the CoT examples.</p>
<p>LR.We provide the few-shot examples with the reasoning explanation in the logical format described in Section 3.4.</p>
<p>CoS.We further alter the few-shot examples with the CoS symbolic forms explained in Section 3.4.</p>
<p>Results and Discussion</p>
<p>Realistic Domain.ReSQ serves as a realistic SQA domain.Table 2 shows that using the Q-chain is effective for both BERT and Flan-T5, with notable improvement on Flan-T5.To better understand these results, we analyzed the performance across different splits of ReSQ based on the manually annotated depth of reasoning required to answer the questions.The first two splits include questions requiring one or two hops of reasoning, denoted as k = 1 and k = 2.The unclassified split covers questions where reasoning depth is hard to determine and rely more on commonsense knowledge.An example of each case is provided in Figure 3.Our observation from Table 2 reveals that our model consistently improves performance on k = 2 but negatively impacts BERT on k = 1 and the unclassified categories.This suggests that our finetuning approach is more effective when deeper reasoning is needed but is less effective for questions requiring commonsense knowledge.On the other hand, LLMs outperform fine-tuning approaches on ReSQ, specifically in the unclassified subset.They consistently show 2-13% higher performance than Flan-T5+T+Q-chain (lines 7-13).These results indicate that one of LLMs' superior performances is due to their commonsense knowledge while our approach performs better in multi-hop reasoning.Nevertheless, we observe that logicbased fine-tuning enhances Flan-T5 more significantly than BERT on the unclassified subset.This    PLMs and LLMs is much smaller in this domain compared to the realistic domain (ReSQ).This is expected, as we concluded in the results of the realistic domain in Table 2, that LLMs are better at commonsense reasoning than complex multi-hop spatial reasoning.The hypothesis is further supported when assessing the proposed method on StepGame.As can be observed in Table 3, the finetuning method consistently demonstrates significant positive differences in all reasoning steps compared to LLMs.The struggle of GPT3 on reasoning  over StepGame is also investigated in (Yang et al., 2023).Our proposed method consistently improves by 1-4% on a higher number of reasoning hops (k=6 to k=10), similar to the observation results of ReSQ.These results confirm our primary hypothesis that our proposed method equips the models with a higher level of logical abstraction to conduct more complex logical reasoning.We provide three examples of responses from this dataset to qualitatively demonstrate the improvements in Figure 4. We also included results from GPT-3(d2) + ASP and PistaQ*, both of which demonstrate significant effectiveness in multi-hop reasoning.These models perform relation extraction to obtain a formal logical representation using LLMs, while reasoning is handled separately using external tools with engineered rules of reasoning.However, these pipelines face two key limitations.First, they require formal representations at test time, effective only on synthetic data.Second, they do not fine-tune the language models for better reasoning, relying instead on external tools.This approach contrasts with ours, which focuses on directly improving the language models' reasoning capabilities.</p>
<p>In-context Learning.This section focuses only on comparing the variations of in-context learning for spatial reasoning.As shown in Table 2 and Table 4, CoT proved to be the most effective approach for spatial reasoning in both GPT and Llama.We also evaluated the impact of integrating formal representations in in-context learning, referred to as the LR approach.As evident in Table 4 (line 4), using formal representations marginally degraded performance.In most cases, utilizing natural language in the CoT proved to be more effective.Additionally, we evaluate CoS symbolic representations, finding that CoT and LR outperformed CoS using the same examples.This highlights the advantage of natural language reasoning over symbolic reasoning for in-context learning.We further applied step-bystep prompting explained in Section 3.4 to evaluate more sophisticated prompting techniques on LLMs.Table 4 shows that this technique achieves 2-5% better results than zero-shot, few-shot, and CoT using raw context.The results demonstrate that more straightforward and shorter contexts improve LLMs' spatial understanding.This is consistent with previous results on other domains and tasks (Li et al., 2023;Liu et al., 2024).Lastly, we compare open-source Llama and closed-source GPT models.Our results indicate that Llama-3 is competitive with GPT-3.5, showing only minor differences in the YN domain as shown in Table 2. Thus, we plan to prioritize open-source models in future research.</p>
<p>Conclusion</p>
<p>Given the importance of spatial reasoning in many real-world applications, we focus on improving this reasoning skill in language models.We equip LLMs with neuro-symbolic supervision to leverage logical knowledge during fine-tuning.This practice boosts their capability of capturing the necessary level of abstraction for spatial reasoning and enhances the generalizability for transferring knowledge across domains.Our constraint-based training technique achieves higher accuracy than other competitive Spatial Question-answering baselines across various benchmarks.Furthermore, the results indicate that our model performs better in scenarios requiring more reasoning steps.Lastly, we compare our models with state-of-the-art LLMs.Experimental comparisons show that while large LLMs like GPT3.5 perform better in commonsense reasoning, we achieve higher performance in multihop spatial question-answering with tuning comparatively smaller language models like BERT with spatial logic.</p>
<p>Limitations</p>
<p>While we improve the reasoning capability of the models, our approach by no means solves the general reasoning problem.The trustworthiness and reliability of the LLMs are still a research challenge.Our models will need GPU resources to run, which might be expensive.Our research is based on open-source data and code, and the results will be reproducible.We do not see any ethical con-cerns in our research approach or the impact of our results.Our data, being limited to the spatial reasoning domain, does not include any specific type of bias that can harm minority people belonging to any particular gender or race.user : There exist a big red square, a big red triangle, a medium red square, and a small green circle in a block called A. The triangle and medium square are touching the bottom edge of the block.The big and medium square are touching the right edge of the block.And the circle is above the big square which is above the medium square.There is another block called B to the left of block A. A medium green square is touching block B's left edge and below a medium red square.The medium red square is above and to the left of a small green square.Also a medium red triangle is below and to the left of the small square.Is the medium red triangle below the red square in block B? assistant : Yes user : There are three blocks A, B and C. Block A has a medium white triangle and a small red rectangle.There exists a medium white rectangle to the left of the triangle and to the right of the red rectangle.Aove block A there is block B. It has a small white oval touching the bottom edge of the block and a medium white triangle to the right of it.Below block A there is block C. Its has a small green triangle touching the top edge of the block.A medium white triangle is below a small green rectangle.Another small green triangle is below and to the right of the white triangle.Are all rectangles above a white triangle?' assistant : Yes user : There is a block called A. Inside A, a black triangle above and to the left of a black circle.There is another block called B which is to the right of A. B has two yellow circles, a blue circle, and a blue square.The blue circle is touching the left edge of B. One of the yellow circles is to the right of the blue circle and to the left of the blue square.The blue square is touching the right edge of B and above the other yellow circle, Which also touches the right edge of B. Does block B have all of the circles inside it?assistant : No user : There is a block called A. A has a large blue square near and below a medium blue triangle.</p>
<p>Far to the left of the medium blue triangle is a small yellow square.Far above the medium blue triangle is a small blue triangle.There is another block called B, which is to the right of A. In B, There is a small yellow circle touching the left edge of B. Far to the right of the circle is a large yellow square.Near and above the large square is a small yellow square.There is another block called C, which is to the right of B. In C, there is a small blue circle near and to the left of a large yellow triangle.The large yellow triangle is near and to the left of a large blue triangle.The large blue triangle is far above a large blue circle.The large blue circle is touching the bottom edge of C. Are the large blue things in C to the left of the large blue thing in A?</p>
<p>assistant : No user : There is a block called A. In A there is a red triangle Which is above a black circle.The black circle is touching the bottom of A. A yellow square is touching the top edge of A, and below that, a red oval is touching the right edge of A. There is a second block called B, to the left of A. Inside B are 3 squares: a brown, a green, and a red.The green square is above the red square and touching the left edge of B. The brown square is touching both the bottom edge and the right edge of B. The red square is touching the left edge of the brown square.Is the oval to the right of the brown thing?</p>
<p>C.2 Chain of Thought system : You will be given story and question.Then, you will answer either Yes or No with explanation based on given story step by step.Candidate answer: [Yes, No] user : There exist a big red square, a big red triangle, a medium red square, and a small green circle in a block called A. The triangle and medium square are touching the bottom edge of the block.The big and medium square are touching the right edge of the block.And the circle is above the big square which is above the medium square.There is another block called B to the left of block A.</p>
<p>A medium green square is touching block B's left edge and below a medium red square.The medium red square is above and to the left of a small green square.Also a medium red triangle is below and to the left of the small square.Is the medium red triangle below the red square in block B? assistant : From information provided:The medium red square is above a small green square in B.Therefore, the small green square is below the medium red square in B. The medium red triangle is below the small square in B, and the small green square is below the medium red square in B.Therefore, the medium red triangle is below the medium red square in B. Answer: Yes user : There are three blocks A, B and C. Block A has a medium white triangle and a small red rectangle.There exists a medium white rectangle to the left of the triangle and to the right of the red rectangle.Aove block A there is block B. It has a small white oval touching the bottom edge of the block and a medium white triangle to the right of it.Below block A there is block C. Its has a small green triangle touching the top edge of the block.A medium white triangle is below a small green rectangle.Another small green triangle is below and to the right of the white triangle.Are all rectangles above a white triangle?' assistant : From given information: A medium white triangle is below a small green rectangle in C.Then, small green rectangle is above medium white triangle in C.A small red rectangle in block A, a medium white triangle in C, and block C is below block A.Then, a medium white triangle is below a A small red rectangle.A medium white triangle is below a A small red rectangle.Therefore, a small red rectangle is above a medium white triangle.A medium white rectangle is in A,a medium white triangle in C, and block C is below block A.Then, a medium white triangle is below a A medium white rectangle.A medium white triangle is below a A medium white rectangle.Therefore, a medium white rectangle is above a medium white triangle.A medium white rectangle is above a medium white triangle.Thus, all rectangles from given story are above a white triangle.Answer: Yes' user : There is a block called A. Inside A, a black triangle above and to the left of a black circle.There is another block called B which is to the right of A. B has two yellow circles, a blue circle, and a blue square.The blue circle is touching the left edge of B. One of the yellow circles is to the right of the blue circle and to the left of the blue square.</p>
<p>The blue square is touching the right edge of B and is above the other yellow circle, Which is also touching the right edge of B. Does block B have all of the circles inside it?assistant : From given relation, A black triangle is above a black circle in A.Then, there is black circle in A.Therefore, not all circle are in B. Answer: No user : There is a block called A. A has a large blue square near and below a medium blue triangle.Far to the left of the medium blue triangle is a small yellow square.Far above the medium blue triangle is a small blue triangle.There is another block called B, which is to the right of A. In B, There is a small yellow circle touching the left edge of B. Far to the right of the circle is a large yellow square.Near and above the large square is a small yellow square.There is another block called C, which is to the right of B. In C, there is a small blue circle near and to the left of a large yellow triangle.The large yellow triangle is near and to the left of a large blue triangle.The large blue triangle is far above a large blue circle.The large blue circle is touching the bottom edge of C. Are the large blue things in C to the left of the large blue thing in A? assistant : Based on the provided content.Block B is to the right if block A.Block C is to the right of block B.Therefore, block C is to the right of block A.In block C, the yellow triangle is near and to the left of large blue triangle.Therefore, there is large blue triangle in C.Block A has a large blue square near and below a medium blue triangle.Therefore, there is large blue square in A.Then, large blue triangle in C is to the right of large blue square in A.So, large blue triangle in C is not to the left of large blue square in A. Answer: No user : There is a block called A. In A there is a red triangle Which is above a black circle.The black circle is touching the bottom of A. A yellow square is touching the top edge of A, and below that, a red oval is touching the right edge of A. There is a second block call B Which is to the left of A. Inside B there are 3 squares, a brown one, a green one, and a red one.The green square is above the red square and touching the left edge of B. The brown square is touching both the bottom edge and the right edge of B. The red square is touching the left edge of the brown square.Is the oval to the right of the brown thing?</p>
<p>C.3 Logical Representation (LR)</p>
<p>system : You will be given story and question.Then, you will answer either Yes or No with explanation based on given story step by step.Candidate answer: [Yes, No] user : There exist a big red square, a big red triangle, a medium red square, and a small green circle in a block called A. The triangle and medium square are touching the bottom edge of the block.The big and medium square are touching the right edge of the block.And the circle is above the big square which is above the medium square.There is another block called B to the left of block A. A medium green square is touching the left edge of block B and is below a medium red square.The medium red square is above and to the left of a small green square.Also a medium red triangle is below and to the left of the small square.Is the medium red triangle below the red square in block B?</p>
<p>assistant : Below(Medium red triangle in B, red square in block B) Answer: Yes user : There are three blocks A, B and C. Block A has a medium white triangle and a small red rectangle.There exists a medium white rectangle to the left of the triangle and to the right of the red rectangle.Above block A there is block B. It has a small white oval touching the bottom edge of the block and a medium white triangle to the right of it.Below block there is block C. Its has a small green triangle touching the top edge of the block.A medium white triangle is below a small green rectangle.Another small green triangle is below and to the right of the white triangle.Are all rectangles above a white triangle?' assistant : Below(C, A) =&gt; Above(A, C), Contain(A, Small red rectangle) + Above(A, C) + Contain(C, Medium white triangle)=&gt;Above(Small red rectangle, Medium White triangle), Above(A, C) &amp; Contain(A, Medium white rectangle) &amp; Contain(C, Medium white triangle)=&gt;Above(Medium white rectangle, Medium White triangle), Below(Medium white triangle in C, small green rectangle in C) =&gt;Above(small green rectangle, Medium white triangle)Answer: Yes user : There is a block called A. Inside A, there is a black triangle that is above and to the left of a black circle.There is another block called B which is to the right of A. In B, there are two yellow circles, a blue circle, and a blue square.The blue circle is touching the left edge of B. One of the yellow circles is to the right of the blue circle and to the left of the blue square.The blue square is touching the right edge of B and is above the other yellow circle, Which is also touching the right edge of B. Does block B have all of the circles inside it?assistant : Contain(A, Black Circle) =&gt; Â¬Contain(B, Black Circle), Â¬Contain(B, Black Circle) =&gt; Some x, Â¬Contain(B, x) where x is Circle.Answer: No user : There is a block called A. In A, there is a large blue square near and below a medium blue triangle.Far to the left of the medium blue triangle is a small yellow square.Far above the medium blue triangle is a small blue triangle.There is another block called B, which is to the right of A. In B, There is a small yellow circle touching the left edge of B. Far to the right of the circle is a large yellow square.Near and above the large square is a small yellow square.There is another block called C, which is to the right of B. In C, there is a small blue circle near and to the left of a large yellow triangle.The large yellow triangle is near and to the left of a large blue triangle.The large blue triangle is far above a large blue circle.user : There exist a big red square, a big red triangle, a medium red square, and a small green circle in a block called A. The triangle and medium square are touching the bottom edge of the block.The big and medium square are touching the right edge of the block.And the circle is above the big square which is above the medium square.There is another block called B to the left of block A. A medium green square is touching the left edge of block B and is below a medium red square.The medium red square is above and to the left of a small green square.Also a medium red triangle is below and to the left of the small square.Is the medium red triangle below the red square in block B?</p>
<p>assistant : Let analyze the relations given from context to answer question.B: 1. (medium, red, triangle) â†“, &lt; (small, red, square)Based on the information, B: 1. (medium, red, triangle) â†“, &lt; (small, red, square)2.(medium, red, triangle) â†“ (small, red, square)Answer: Yes user : There are three blocks A, B and C. Block A has a medium white triangle and a small red rectangle.There exists a medium white rectangle to the left of the triangle and to the right of the red rectangle.Above block A there is block B. It has a small white oval touching the bottom edge of the block and a medium white triangle to the right of it.Below block A there is block C. Its has a small green triangle touching the top edge of the block.A medium white triangle is below a small green rectangle.Another small green triangle is below and to the right of the white triangle.Are all rectangles above a white triangle?' assistant : Let consider the related relations to answer the questions, A: 1. (small, red, rectangle) 2. (medium, white, rectangle) C: 1. (small, green, rectangle) = edge 2. (medium, white, triangle) ) â†“ (small, green, rectangle) C â†“ A Based on the information, let determined the relation of all rectangle and white triangles, 1. C â†“ A. Therefore, A â†‘ C 2. A â†‘ C, A: (small, red, rectangle), and C: (medium, white, triangle).Therefore, A: (small, red, rectangle) â†‘ C: (medium, white, triangle) 3. A â†‘ C, A: (medium, white, rectangle), and C: (medium, white, triangle).Therefore, A: (medium, white, rectangle) â†‘ C: (medium, white, triangle).4. C: (medium, white, triangle) ) â†“ (small, green, rectangle).Therefore, (small, green, rectangle) â†‘ (medium, white, triangle).5.A: (small, red, rectangle) â†‘ C: (medium, white, triangle), A: (medium, white, rectangle) â†‘ C: (medium, white, triangle), and C: (small, green, rectangle) â†‘ (medium, white, triangle), therefore, all rectangle above a white triangle.Answer: Yes user : There is a block called A. Inside A, there is a black triangle that is above and to the left of a black circle.There is another block called B which is to the right of A. In B, there are two yellow circles, a blue circle, and a blue square.The blue circle is touching the left edge of B. One of the yellow circles is to the right of the blue circle and to the left of the blue square.The blue square is touching the right edge of B and is above the other yellow circle, Which is also touching the right edge of B. Does block B have all of the circles inside it?assistant : Let consider the related relations to answer the questions,A: 1. (black, triangle) â†‘ &lt; (black, circle)B: 1. (yellow, circle1) right (blue, circle)2.(blue, square) â†‘ (yellow, circle2) 3.(blue, circle) =left edge of B Based on the information, let determined the relation of all rectangle and white triangles, 1. A: 1. (black, triangle) â†‘&lt; (black, circle).Therefore, A: (black, circle)2.A: (black, circle).Therefore, B doesn't have all circle.Answer: No user : There is a block called A. In A, there is a large blue square near and below a medium blue triangle.Far to the left of the medium blue triangle is a small yellow square.Far above the medium blue triangle is a small blue triangle.There is another block called B, which is to the right of A. In B, There is a small yellow circle touching the left edge of B. Far to the right of the circle is a large yellow square.Near and above the large square is a small yellow square.There is another block called C, which is to the right of B. In C, there is a small blue circle near and to the left of a large yellow triangle.The large yellow triangle is near and to the left of a large blue triangle.The large blue triangle is far above a large blue circle.The large blue circle is touching the bottom edge of C. Are the large blue things in C to the left of the large blue thing in A? assistant : Let consider the related relations to answer the questions,B &gt; A,C &gt; BA: 1. (large, blue square) âˆ¼ â†“ (medium, blue square)C: 1. (large, yellow, triangle)Based on the information, let determined the relation large blue things in C and the large blue thing in A,1.C &gt; B and B &gt; A. Therefore, C &gt; A2.A: 1. (large, blue, square) âˆ¼â†“ (medium, blue square).Therefore, A: (large, blue, square)3.C &gt; A, A: (large, blue, square), and C: (large, yellow, triangle).Therefore, C: (large, yellow, triangle) &gt; A: (large, blue, square).4.C: (large, yellow, triangle) &gt; A: (large, blue, square).Therefore, C: (large, yellow, triangle) !&lt; A: (large, blue, square)large blue things in C is not to the left of the large blue thing in A. Answer: No user : There is a block called A. In A there is a red triangle Which is above a black circle.The black circle is touching the bottom of A. A yellow square is touching the top edge of A and below that there is a red oval is touching the right edge of A.</p>
<p>Rule Type</p>
<p>Rule</p>
<p>Logical Constraints above(x, y) â‡’ below(y, x) below(x, y) â‡’ above(y, x) lef t(x, y) â‡’ right(y, x) right(x, y) â‡’ lef t(y, x) f ront(x, y) â‡’ behind(y, x) Converse behind(x, y) â‡’ f ront(y, x) T ruth(q i ) â‡’ T ruth(q j ) coveredby(x, y) â‡’ cover(y, x) cover(x, y) â‡’ coveredby(y, x) inside(x, y) â‡’ contain(y, x) contain(x, y) â‡’ inside(y, x) near(x, y) â‡’ near(y, x) f ar(x, y) â‡’ f ar(y, x) Symmetric touch(x, y) â‡’ touch(y, x) T ruth(q i ) â‡’ T ruth(q j ) disconnected(x, y) â‡’ disconnected(y, x) overlap(x, y) â‡’ overlap(y, x) lef t(x, y) âˆ§ lef t(y, z) â‡’ lef t(x, z) right(x, y) âˆ§ right(y, z) â‡’ right(x, z) above(x, y) âˆ§ above(y, z) â‡’ above(x, z) below(x, y) âˆ§ below(y, z) â‡’ below(x, z) behind(x, y) âˆ§ behind(y, z) â‡’ behind(x, z) f ront(x, y) âˆ§ f ront(y, z) â‡’ f ront(x, z) inside(x, y) âˆ§ inside(y, z) â‡’ inside(x, z) contain(x, y) âˆ§ contain(y, z) â‡’ contain(x, z) inside(x, y) âˆ§ coveredby(y, z) â‡’ inside(x, z) contain(x, y) âˆ§ cover(y, z) â‡’ contain(x, z) inside(x, y) âˆ§ lef t(y, z) â‡’ lef t(x, z) inside(x, y) âˆ§ right(y, z) â‡’ right(x, z) inside(x, y) âˆ§ above(y, z) â‡’ above(x, z) inside(x, y) âˆ§ below(y, z) â‡’ below(x, z) Transitivity inside(x, y) âˆ§ behind(y, z) â‡’ behind(x, z) T ruth(q i ) âˆ§ T ruth(q j ) â‡’ T ruth(q k ) inside(x, y) âˆ§ f ront(y, z) â‡’ f ront(x, z) inside(x, y) âˆ§ near(y, z) â‡’ near(x, z) inside(x, y) âˆ§ f ar(y, z) â‡’ f ar(x, z) inside(x, y) âˆ§ disconnected(y, z) â‡’ disconnected(x, z) coveredby(x, y) âˆ§ lef t(y, z) â‡’ lef t(x, z) coveredby(x, y) âˆ§ right(y, z) â‡’ right(x, z) coveredby(x, y) âˆ§ above(y, z) â‡’ above(x, z) coveredby(x, y) âˆ§ below(y, z) â‡’ below(x, z) coveredby(x, y) âˆ§ behind(y, z) â‡’ behind(x, z) coveredby(x, y) âˆ§ f ront(y, z) â‡’ f ront(x, z) coveredby(x, y) âˆ§ near(y, z) â‡’ near(x, z) coveredby(x, y) âˆ§ f ar(y, z) â‡’ f ar(x, z) coveredby(x, y) âˆ§ disconnected(y, z) â‡’ disconnected(x, z) Table 7: The conversion from spatial logical rules proposed in (Mirzaee and Kordjamshidi, 2022) to logical constraints used in our experiment.</p>
<p>E DomiKnowS Concepts and Graphs</p>
<p>Graph and concepts declaration of our work inside the DomiKnowS framework.</p>
<p>StepGame Context: S is positioned below C. The object B is upper and slightly to the left of the object O. U is below S and to the right of S. V and C are next to each other with C at the bottom V on the top.A and U are parallel, and A on the right of U. FR Question: What is the relation of the agent S to the agent C? Answer: below</p>
<p>Figure 1 :
1
Figure 1: Examples of story and questions in Spatial Question Answering (SQA).YN is Yes-No, and FR is Find relation type of questions.</p>
<p>Figure 2 :
2
Figure 2: The pipeline for exploiting spatial logic in the training phase involves three steps: 1. Creating the chain of reasoning (Q-Chain) for each training example using annotated facts and rules from the training dataset.2.Obtaining consistency constraints from spatial logical rules and answers inferred from questions generated using intermediate facts in Q-Chain.3. Incorporating obtained consistency constraints into the loss function using differentiable soft logic during tuning.For more details, refer to Section 3.3.</p>
<p>Figure 3 :
3
Figure 3: Three examples of each category in the ResQ dataset with responses from Llama3 (Few-shot) and Flan-T5 + Q-Chain (Ours).</p>
<p>Figure 4 :
4
Figure 4: Three examples of each category in the StepGame dataset with responses from Llama3 (Fewshot) and BERT-T + Q-Chain (Our).</p>
<p>The large blue circle is touching the bottom edge of C. Are the large blue things in C to the left of the large blue thing in A? assistant : Question: Left(large blue thing in C, large blue thing in A).Related relation from story: Right(B, A), Right(C, B), Contain(C, large blue triangle), Contain(A, large blue square) Steps: Right(C, B) + Right(B, A) =&gt; Right(C, A) Contain(C, large blue triangle) + Right(C, A) + Contain(A, large blue square) =&gt; Right(large blue triangle in C, large blue square in A)Right(large blue triangle in C, large blue square in A) =&gt; !Left(large blue thing in C, large blue thing in A) Answer: No C.4 Chain of Symbol system : You will be given story and question.Then, you will answer either Yes or No with explanation based on given story step by step.Candidate answer: [Yes, No]</p>
<p>E. 1
1
Graph for YN story = Concept ( name =" story ") question = Concept ( name =" question ") story_contain , = story .contains ( question ) answer_class = question ( name =" answer_class " , ConceptClass = EnumConcept , values =[ " yes " , " no " ]) symmetric = Concept ( name =" symmetric ") s_quest1 , s_quest2 = symmetric .has_a ( arg1 = question , arg2 = question ) ifL ( andL ( answer_class .yes ( 'x ') , existsL ( symmetric ( 's ' , path =( 'x ' , symmetric ) ))) , answer_class .yes ( path =( 's ' , s_quest2 ))) reverse = Concept ( name =" reverse ")</p>
<p>for converting spatial expressions from natural language into symbolic representation.An example of the CoS counterpart of CoT expression is shown in Table1.</p>
<p>DomainSpatial context CoT large red square is to the left of a small green square LR Left(large red square, small green square) CoS (large, red, square) &lt; (small, green, square) Table 1: An example of a spatial relation used in Chain of Thought (CoT), Logical Representation (LR), and Chain of Symbol (CoS).</p>
<p>(Mirzaee and Kordjamshidi, 2022ht edge of the block." the expected context generated by the LLM would be "The big red square in block A.SpaRTUN(Mirzaee and Kordjamshidi, 2022) is a synthetic SQA dataset.This dataset contains both YN and FR types of questions that require multi-hop reasoning to answer.It covers a wide range of spatial relations.2Theanswer for YN is A = {Y es, N o}, while the answer for the FR is a subset of 15 spatial relations, e.g., {lef t, behind}, and {above, inside}.The dataset provides the chain of reasoning annotations that we use to create the Q-Chains for our proposed training method.
This dataset is used only for training because it issynthetic and the baselines performance reportedin (Mirzaee and Kordjamshidi, 2022) on its testsplit is already high.SpartQA-Human (Ver.1) (Mirzaee et al., 2021) isa small human-annotated SQA dataset. The datasetcontains both YN and FR types of questions. Weonly use the YN question types for fine-tuning andevaluation. The YN questions require more rea-soning steps and are shown to be more challengingbased on previous research results in<newline> The big red square is touching the rightedge of block A...". More examples are provided inAppendix C. We refer to this prompting techniqueas Step-by-Step, which can be applied before anyICL method.4 Experiments4.1 DatasetsOur experiments are conducted on all existingdatasets, including SpaRTUN, SpartQA-Human,ReSQ, and StepGame.</p>
<p>Table 2 :
2
Mirzaee and Kordjamshidi (2023)eSQ with various models.For ReSQ, k is the number of the reasoning steps required to answer the questions.Unclassified indicates the cases in which k was challenging for human annotators to determine.The results from PistaQ<em> reported fromMirzaee and Kordjamshidi (2023).
SpartQA-HumanReSQModelVer.1Ver.2k=1k=2unclassifiedAll#BERT54.5453.5770.67 56.8560.6660.98 1BERT-T55.9458.0376.00 54.7961.1861.15 2BERT-T+Q-Chain (Our)59.44 58.9272.00 58.9059.9061.31 3Flan-T554.5460.7174.67 56.1661.4461.80 4Flan-T5-T49.6557.1481.33 54.7961.4462.30 5Flan-T5-T+Q-Chain (Our) 55.94 61.61 81.33 57.5363.7564.43 6GPT3.5 (zero-shot)58.0458.0374.67 60.9566.5866.22 7GPT3.5 (few-shot)62.2358.9284.00 68.4968.1270.16 8GPT3.5 (LR)64.3359.82----9GPT3.5 (CoS)60.1464.28----10GPT3.5 (CoT)65.7371.4386.67 67.1268.6470.49 11GPT-4 (zero-shot)77.6268.7584.00 73.9776.8677.05 12Llama-3 (zero-shot)61.5450.8980.00 64.3867.3568.20 13Llama-3 (few-shot)62.9460.7182.67 69.8671.4672.46 14Llama-3 (LR)60.1472.32----15Llama-3 (CoS)67.1361.60----16Llama-3 (CoT)67.8370.5482.76 76.0367.1071.15 17PistaQ</em>75.52----47.21 18Modelk=1k=2k=3k=4k=5k=6k=7k=8k=9k=10BERT98.51 95.53 91.68 66.71 49.11 41.47 41.47 32.09 28.94 28.16BERT-T98.50 95.32 93.26 76.78 66.36 58.76 53.70 46.27 42.71 40.12BERT-T+Q-Chain (Our) 98.70 96.45 93.03 74.58 64.95 59.04 54.38 49.23 45.36 44.05Llama-3 (few-shot)38.01 27.87 24.15 21.27 19.75 18.03 16.88 15.52 15.17 14.70GPT3 (few-shot)55.00 37.00 25.00 30.00 32.00 29.00 21.00 22.00 34.00 31.00GPT3 (CoT)61.00 45.00 30.00 35.00 35.00 27.00 22.00 24.00 23.00 25.00GPT3(d2) + ASP92.60 89.90 89.10 93.80 92.90 91.60 91.20 90.40 89.00 88.30</p>
<p>Table 3 :
3
Yang et al. 2023.ame on several models including results of GPT3 reported fromYang et al. 2023.
ModelRaw Context Step by stepGPT3.5 (zero-shot)58.0463.64GPT3.5 (few-shot)62.2364.33GPT3.5 (CoT)65.7367.83GPT3.5 (LR)64.3359.44GPT3.5 (CoS)60.1458.74GPT-4 (zero-shot)77.6278.32</p>
<p>Table 4 :
4
The accuracy of LLMs on SpartQA-Human.
suggests that the Q-chain can facilitate complexreasoning, particularly when applied to a modelwith stronger commonsense capabilities. Note thatapplying LR and CoS is not straightforward onReSQ due to the challenges associated with ex-tracting logical representations from realistic do-mains text. This challenge is evident in the resultsreported for PistaQ<em>, which employs a similar ap-proach. PistaQ</em> achieved only 47.21% accuracy onReSQ, despite significant improvements observedin synthetic domains such as SpartQA-Human.Synthetic Domain with More Complex LogicalReasoning. SpartQA-Human and StepGame aresynthetic domains used in our experiments. Ac-
cording to Table2, we consistently observe improvements with our proposed Q-chain in the synthetic domain which typically requires considerable reasoning hops.Moreover, the gap between small</p>
<p>K is at the bottom of T. V is to the right of G horizontally.W and L are next to each other with W on the left and L on the right.Z is positioned below and to the right of X. T is positioned in the lower right corner of O. X and N are parallel, and X is under N. G is diagonally to the bottom left of M. V is below and to the left of O. N is on the lower right of M. T is diagonally to the bottom left of I. H and I are parallel, and H on the left of I. L is at the bottom of I. K is on the top of Z and is on the same vertical plane.U and B are vertical and U is below B.
StepGame: k = 1 Context: T is below C. H and M are vertical and H is above M. J is below and to the left of H. Question: What is the relation of the agent M to the agent H? Label: Below Llama3 (few-shot): Below(âœ“) BERT-T+Q-Chain (Our): Below(âœ“) StepGame: k = 4 Context: R is at the bottom and A is on the top. R is below E at 4 o'clock. W is positioned right to M. M and R are parallel, and M is under R. S is on the left side and above W. Z is above J. Question: What is the relation of the agent M to the agent W? Label: Left Llama3 (few-shot): Right(â•³) BERT-T+Q-Chain (Our): Left(âœ“)StepGame: k = 10 Context: Question: What is the relation of the agent N to the agent X? Label: Above Llama3 (few-shot): Below(â•³) BERT-T+Q-Chain (Our): Above(âœ“)</p>
<p>Table 6 :
6
Accuracy of SpartQA-Human and ReSQ with BERT and RoBERTa for comparison between these models.
Lianhui Qin, Sean Welleck, Daniel Khashabi, and YejinChoi. 2022. Cold decoding: Energy-based con-strained text generation with langevin dynamics. InAdvances in Neural Information Processing Systems,volume 35, pages 9538-9551. Curran Associates,Inc.Hossein Rajaby Faghihi, Quan Guo, Andrzej Uszok, Ali-akbar Nafar, and Parisa Kordjamshidi. 2021. Domi-
KnowS: A library for integration of symbolic domain knowledge in deep learning.In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 231-241, Online and Punta Cana, Dominican Republic.Association for Computational Linguistics.</p>
<p>The code, trained models, and data are publicly available at https://github.com/HLR/SpaRTUNQChain
Related Work
The dataset also includes Don't Know questions, but for simplicity, we apply the closed-world assumption and label those questions as No.
AcknowledgmentThis project is partially supported by the Office of Naval Research (ONR) grant N00014-23-1-2417.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Office of Naval Research.We thank anonymous reviewers for their constructive feedback, which greatly helped us improve this manuscript.A Data StatisticThe data statistic of each dataset is shown in Table 5.DatasetTrainB RoBERTa resultsWe provide results from the same setting as the BERT using RoBERTa in Table6and found that the overall results are slightly lower than BERT for typical transfer learning.So, we decided to use only BERT for the remaining experiments.C In-context Learning ExamplesThe example of each in-context learning prompt and example is indicated below.C.1 Few-Shotsystem : You will be given a story and question.Then, you will answer either only Yes or No based on given story.Candidate answer:[Yes, No]There is a second block call B Which is to the left of A. Inside B there are 3 squares, a brown one, a green one, and a red one.The green square is above the red square and touching the left edge of B. The brown square is touching both the bottom edge and the right edge of B. The red square is touching the left edge of the brown square.Is the oval to the right of the brown thing?C.5 Step-by-Step PromptingRaw context : There exist a big red square, a big red triangle, a medium red square, and a small green circle in a block called A. The triangle and medium square are touching the bottom edge of the block.The big and medium square are touching the right edge of the block.And the circle is above the big square which is above the medium square.There is another block called B to the left of block A. A medium green square is touching the left edge of block B and is below a medium red square.The medium red square is above and to the left of a small green square.Also a medium red triangle is below and to the left of the small square.Spliting context : A big red square in block A.A big red triangle in block A.A medium red square in block A.A small green circle in block A.The triangle is touching the bottom edge of block A.The medium square is touching the bottom edge of block A.The big square is touching the right edge of block A.The medium square is touching the right edge of block A.The circle is above the big square in A.The big square is above the medium square in A.Block B is to the left of block A.A medium green square is touching the left edge of block B.A medium green square is below a medium red square in B.The medium red square is above a small green square in B.The medium red square is left of a small green square in B.The medium red triangle is below the small square in B.The medium red triangle is left the small square in B.D Spatial Logical RulesThe conversion from spatial logical rules proposed in(Mirzaee and Kordjamshidi, 2022)to logical constraints used in our experiment is shown in Table 7.r_quest1 , r_quest2 = reverse .has_a ( arg10 = question , arg20 = question ) ifL ( andL ( answer_class .yes ( 'x ') , existsL ( reverse ( 'r ' , path =( 'x ' , reverse )))) , answer_class .no ( path =( 'r ', r_quest2 ) ) ) ifL ( andL ( answer_class .no ( 'x ') , existsL ( reverse ( 'r ' , path =( 'x ' , reverse )))) , answer_class .yes ( path =( 'r ' , r_quest2) for rel in transitive_1 : ifL ( andL ( rel ( 'x ') , existsL ( transitive ("t" , path =( 'x ' , transitive ) ) ) , rel ( path =( 't ' , tran_quest2 ))) , rel ( path =( 't ' , tran_quest3 ))) # Transitive of cover and contain transitive_2 = [( coveredby , inside ) , ( cover , contain )] for rel1 , rel2 in transitive_2 : ifL ( andL ( rel2 ( 'x ') , existsL ( transitive ("t" , path =( 'x ' , transitive ) ) ) , rel1 ( path =( 't ', tran_quest2))) , rel2 ( path =( 't ' , tran_quest3 ))) # Transitive of inside / cover with position transitive_3_1 = [ inside , coveredby ] transitive_3_2 =[ left , right , above , below , behind , front , near , far ,disconnected ] for rel1 in transitive_3_1 :for rel2 in transitive_3_2 : ifL ( andL ( rel1 ( 'x ') , existsL ( transitive ("t" , path =( 'x ' , transitive ))) , rel2 ( path =( 't ' , tran_quest2 ))) , rel2 ( path =( 't ' , tran_quest3 )))#
A I , Meta , Llama 3 model card. 2024</p>
<p>Neurosymbolic visual reasoning: Disentangling "Visual" from "Reasoning. Saeed Amizadeh, Hamid Palangi, Alex Polozov, Yichen Huang, Kazuhito Koishida, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLR2020119</p>
<p>Logicguided data augmentation and regularization for consistent question answering. Akari Asai, Hannaneh Hajishirzi, 10.18653/v1/2020.acl-main.499Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>From generic knowledge to specific reasoning for medical image interpretation using graph based representations. Jamal Atif, CÃ©line Hudelot, Geoffroy Fouquier, Isabelle Bloch, Elsa D Angelini, IJCAI. 2007</p>
<p>The primal-dual method for learning augmented algorithms. Etienne Bamas, Andreas Maggiori, Ola Svensson, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, 10.18653/v1/2023.ijcnlp-main.45Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliNusa Dua20231Association for Computational Linguistics</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Yu, Journal of Machine Learning Research. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei25702024</p>
<p>Understanding spatial language in radiology: Representation framework, annotation, and spatial relation extraction from chest x-ray reports using deep learning. Surabhi Datta, Yuqi Si, Laritza Rodriguez, Sonya E Shooshan, Dina Demner-Fushman, Kirk Roberts, 10.1016/j.jbi.2020.103473Journal of Biomedical Informatics. 1081034732020</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Naturally supervised 3d visual grounding with language-regularized concept learners. Chun Feng, Joy Hsu, Weiyu Liu, Jiajun Wu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Pheng-Ann Heng, and Qi Dou. 2024. 3dsam-adapter: Holistic adaptation of sam from 2d to 3d for promptable tumor segmentation. GaÃ«l Gendron, Qiming Bao, Michael Witbrock, Gillian Dobbie ; Yuan, Wenao Zhong, Jinpeng Ma, Zhao Li, Jingyang Wang, Zhang, 10.1016/j.media.2024.103324Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24. the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24202498103324Medical Image Analysis</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Chain-of-symbol prompting for spatial reasoning in large language models. Hanxu Hu, Hongyuan Lu, Huajian Zhang, Yun-Ze Song, Wai Lam, Yue Zhang, First Conference on Language Modeling. 2024</p>
<p>Nesycoco: A neuro-symbolic concept composer for compositional generalization. Danial Kamali, J Elham, Parisa Barezi, Kordjamshidi, arXiv:2412.155882024Preprint</p>
<p>Multimodal spatial role labeling (msprl) task overview. Parisa Kordjamshidi, Taher Rahgooy, Marie-Francine Moens, James Pustejovsky, Umar Manzoor, Kirk Roberts, Experimental IR Meets Multilinguality, Multimodality, and Interaction. ChamSpringer International Publishing2017. 2017</p>
<p>Advancing spatial reasoning in large language models: An in-depth evaluation and enhancement using the stepgame benchmark. Fangjun Li, David C Hogg, Anthony G Cohn, 10.1609/aaai.v38i17.29811Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>A logic-driven framework for consistency of neural models. Tao Li, Vivek Gupta, Maitrey Mehta, Vivek, 10.18653/v1/D19-1405Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsSrikumar. 2019</p>
<p>Compressing context to enhance inference efficiency of large language models. Yucheng Li, Bo Dong, Frank Guerin, Chenghua Lin, 10.18653/v1/2023.emnlp-main.391Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Visual spatial reasoning. Fangyu Liu, Guy Emerson, Nigel Collier, Transactions of the Association for Computational Linguistics. 112023</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.1162/tacl_a_00638Transactions of the Association for Computational Linguistics. 122024</p>
<p>NeuroLogic a*esque decoding: Constrained text generation with lookahead heuristics. Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Le Ronan, Lianhui Bras, Youngjae Qin, Rowan Yu, Noah A Zellers, Yejin Smith, Choi, 10.18653/v1/2022.naacl-main.57Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Transfer learning with synthetic corpora for spatial role labeling and reasoning. Roshanak Mirzaee, Parisa Kordjamshidi, 10.18653/v1/2022.emnlp-main.413Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Disentangling extraction and reasoning in multi-hop spatial reasoning. Roshanak Mirzaee, Parisa Kordjamshidi, 10.18653/v1/2023.findings-emnlp.221Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>SPARTQA: A textual question answering benchmark for spatial reasoning. Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning, Kordjamshidi, 10.18653/v1/2021.naacl-main.364Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Chapter 13 -the teaching of thinking and problem solving. Raymond S Nickerson, 10.1016/B978-0-08-057299-4.50019-0Thinking and Problem Solving. Robert J Sternberg, San DiegoAcademic Press1994of Handbook of Perception and Cognition</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAIPreprint</p>
<p>Question answering using constraint satisfaction: QA-by-dossier-with-contraints. John Prager, Jennifer Chu-Carroll, Krzysztof Czuba, 10.3115/1218955.1219028Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04). the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)Barcelona, Spain2004</p>
<p>Gluecons: A generic benchmark for learning under constraints. Aliakbar Hossein Rajaby Faghihi, Chen Nafar, Roshanak Zheng, Yue Mirzaee, Andrzej Zhang, Alexander Uszok, Tanawan Wan, Dan Premsri, Parisa Roth, Kordjamshidi, 10.1609/aaai.v37i8.26143Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Exploring and improving the spatial reasoning abilities of large language models. Manasi Sharma, 2023</p>
<p>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts. Zhengxiang Shi, Qiang Zhang, Aldo Lipani, 10.1609/aaai.v36i10.21383Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Spatial reasoning for human robot interaction. Luis F Emrah Akin Sisbot, Rachid Marin, Alami, 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE2007</p>
<p>Reasoning over logically interacted conditions for question answering. Haitian Sun, William W Cohen, Ruslan Salakhutdinov, arXiv:2205.128982022Preprint</p>
<p>Towards benchmarking and improving the temporal reasoning capability of large language models. Qingyu Tan, Hwee Tou Ng, Lidong Bing, 10.18653/v1/2023.acl-long.828Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van MerriÃ«nboer, Armand Joulin, Tomas Mikolov, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2016</p>
<p>Learning spatial reasoning in virtual vs. physical games with robots. Elmira Yadollahi, Miguel Alexandre Monteiro, Ana Paiva, 10.1145/3623809.3623830Proceedings of the 11th International Conference on Human-Agent Interaction, HAI '23. the 11th International Conference on Human-Agent Interaction, HAI '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Coupling large language models with logic programming for robust and general reasoning from text. Zhun Yang, Adam Ishay, Joohyung Lee, 10.18653/v1/2023.findings-acl.321Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Towards navigation by reasoning over spatial configurations. Yue Zhang, Quan Guo, Parisa Kordjamshidi, 10.18653/v1/2021.splurobonlp-1.5Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics. Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for RoboticsOnline. Association for Computational Linguistics2021</p>
<p>LOViS: Learning orientation and visual signals for vision and language navigation. Yue Zhang, Parisa Kordjamshidi, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022International Committee on Computational Linguistics</p>
<p>Ming Zhou, Nan Duan, Shujie Liu, Heung-Yeung Shum, 10.1016/j.eng.2019.12.014Progress in neural nlp: Modeling, learning, and reasoning. Engineering. 20206</p>            </div>
        </div>

    </div>
</body>
</html>
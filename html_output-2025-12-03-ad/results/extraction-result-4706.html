<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4706 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4706</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4706</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-257427208</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.acl-industry.4.pdf" target="_blank">MathPrompter: Mathematical Reasoning using Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose ‘MathPrompter’, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the ‘MultiArith’ dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4706.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4706.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MathPrompter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MathPrompter (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot chain-of-thought prompting technique that forces large language models to produce multiple analytic representations (algebraic templates and Python functions) for arithmetic word problems and then verifies them by executing/evaluating those representations across randomized variable bindings to reach a consensus answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 DaVinci (text-davinci-002 / GPT-3 family, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The paper uses the GPT-3 DaVinci completion engine (Text-Davinci-002 is referenced) — a generative transformer-based language model in the GPT-3 family, described as a 175B-parameter GPT-style model in the comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems (MultiArith dataset): addition, subtraction, multiplication, combinations of operations; multi-step multi-operation word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Arithmetic capability is improved by (1) eliciting explicit symbolic/algorithmic representations (algebraic formulae and Python functions) from the LM via targeted prompts, and (2) converting those representations into executable computations and checking for consensus across multiple random instantiations; this shifts the task from pure free-form generation to producing executable, checkable procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Substantial empirical improvement on MultiArith: MathPrompter (Zero-shot-CoT with algebraic+Python outputs + compute verification + consensus) achieves 92.5% accuracy (175B) versus 78.7% for prior Zero-shot-CoT (Kojima, 175B). The paper reports concrete examples where generating algebraic/Python templates yields correct intermediate forms and correct numeric answers after evaluation. The method's use of randomized variable mappings and repeated runs (N ≈ 5) is presented as giving higher confidence when multiple symbolic outputs agree.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The paper explicitly documents failure modes where both algebraic and Pythonic outputs agree but are jointly incorrect (a case in Table 2). It also notes that producing matching symbolic forms does not guarantee correctness, and increasing number of prompts reduces but does not eliminate such errors. There are no internal probing or interpretability analyses to show the LM is truly encoding an algorithmic process rather than producing consistent-but-wrong formulaic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MultiArith accuracy: MathPrompter (Zero-shot-CoT, 175B) = 92.5%. For context reported in the paper: Zero-shot (175B) = 17.7%; Zero-shot (PaLM 540B) = 25.5%; Zero-shot-CoT (Kojima, 175B) = 78.7%; Zero-shot-CoT (PaLM 540B) = 66.1%; Zero-shot-CoT + self-consistency (PaLM 540B) = 89.0%. Few-shot and few-shot-CoT baselines are also reported (e.g., Few-Shot-CoT up to 93.0% for 8 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>The main intervention is prompt engineering: (a) prompt the LM to emit an algebraic expression (with variables substituted by a mapping template) and a Python function, (b) evaluate both outputs using Python's eval() on multiple randomized variable assignments, and (c) take the consensus over repeated runs (N ≈ 5). This execution-and-consensus procedure is the only 'intervention' reported and is tied to the observed accuracy gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Reported limitations: (1) Agreement between multiple generated representations does not guarantee correctness — there are cases where both agree yet are wrong; (2) The approach depends on the LM correctly producing valid algebraic/Python code, which can sometimes be syntactically or semantically incorrect; (3) Running many prompt variants increases cost and still may not guarantee correctness; (4) CoT outputs can be excessively lengthy and sometimes contain one-step errors or commonsense mistakes that the verification step may not catch; (5) No principled internal verification or formal proof that the LM is performing algorithmic arithmetic rather than retrieving patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared directly against standard Zero-shot and Zero-shot-CoT baselines (Kojima et al.) and larger models: MathPrompter (175B) 92.5% vs Zero-shot-CoT (Kojima, 175B) 78.7%; vs Zero-shot-CoT + self-consistency (PaLM 540B) 89.0%. Authors state MathPrompter is comparable to 540B models and SOTA few-shot-CoT approaches while being zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathPrompter: Mathematical Reasoning using Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4706.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4706.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting (Kojima et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that elicits chain-of-thought style multi-step reasoning from large language models by appending an instruction like 'Let's think step-by-step' to the problem prompt, improving multi-step reasoning in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generic large LMs (examples in paper: Text-Davinci-002 / 175B; PaLM 540B reported by citation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot-CoT is a prompting technique applied to large generative transformer LMs (examples: GPT-style 175B and PaLM 540B) to induce multi-step chain-of-thought outputs without few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems (same MultiArith benchmark): multi-operation word problems requiring sequential reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>By prompting the LM to 'think step-by-step', the model is induced to produce intermediate reasoning traces which purportedly expose internal multi-step reasoning capabilities that improve final arithmetic answers (i.e., elicited latent stepwise computation rather than pure direct-answer pattern matching).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical gains reported by Kojima et al. and cited in this paper: Zero-shot-CoT increases MultiArith accuracy dramatically (e.g., from 17.7% to 78.7% on 175B model per the paper). The lengthier chain-of-thought outputs correlate with better arithmetic performance in these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The present paper and examples note that chain-of-thought has no built-in validity check on intermediate steps, can be excessively lengthy, may include one-step computation errors, commonsense mistakes, or produce correct-looking reasoning with incorrect final computation; longer CoT does not guarantee correctness of arithmetic calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in this paper (as baseline/comparison): Zero-shot-CoT (Kojima, 175B) = 78.7% on MultiArith; Zero-shot-CoT (PaLM 540B) reported = 66.1% (paper cites these numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No internal probing reported here; the method itself is an intervention (prompt instruction 'Let's think step-by-step'). The paper contrasts Zero-shot-CoT with MathPrompter's additional verification and alternative-representation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>No explicit validity checks on intermediate steps; susceptible to commonsense or arithmetic slips in final computation; chain-of-thought can be verbose and may not reduce token usage; CoT outputs can be consistent yet incorrect without external verification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared as the primary baseline: MathPrompter (Zero-shot-CoT with verification) outperforms Zero-shot-CoT (78.7% → 92.5% on MultiArith for 175B). Also compared with PaLM 540B results and self-consistency variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathPrompter: Mathematical Reasoning using Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4706.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4706.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 DaVinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 DaVinci / text-davinci-002 (GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's primary LLM backbone used to generate algebraic and Pythonic outputs; described as a 175B-parameter GPT-based completion engine (DaVinci/Text-Davinci-002) used for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models are Few-Shot Learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 DaVinci (Text-Davinci-002) — ~175B parameter GPT-style model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large autoregressive transformer language model (GPT-3 family); used here as the generative engine to produce algebraic expressions, Python functions, and chain-of-thought reasoning under different prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Solving MultiArith word problems: multiple arithmetic operations and reasoning chains; single-answer arithmetic word problems (add/sub/mul combos).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>When prompted to output symbolic or executable representations, GPT-3 can produce formulaic/algebraic templates and Python code that often correspond to correct arithmetic solution procedures; executing these outputs externally (eval) lets the system leverage the LM as a program synthesizer rather than trusting its native numeric output generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Direct usage: GPT-3 DaVinci generated algebraic expressions and Python functions that, when evaluated, produced correct answers in many test cases; the paper attributes improved accuracy to this representation + verification pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The model sometimes produces incorrect algebraic/Python outputs or produces outputs that agree but are incorrect; there is no internal guarantee that generated code is correct. The paper does not supply probes into whether GPT-3 internally represents arithmetic algorithmically versus via pattern matching.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As used in MathPrompter experiments (175B GPT-based): MathPrompter using GPT-3 DaVinci reported 92.5% accuracy on MultiArith. Standalone Zero-shot (GPT-3-style 175B) reported in the paper as 17.7% (baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Intervention: prompt to produce algebraic expression and a Python function, followed by external execution (Python eval) over randomized mappings and consensus over repeated runs (N ≈ 5). No additional mechanistic probes or ablations on the model internals are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Can output syntactically invalid or semantically incorrect formulas/code; matching outputs across representations doesn't guarantee correctness; costlier due to multiple generations and external evals; no interpretability evidence that numeric ability is algorithmic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared implicitly to PaLM 540B numbers quoted in the paper (PaLM 540B Zero-shot and Zero-shot-CoT values). The paper emphasizes that with MathPrompter, a 175B GPT-based model can achieve accuracy comparable to larger 540B models or few-shot-CoT methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathPrompter: Mathematical Reasoning using Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4706.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4706.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency applied to chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that samples multiple chain-of-thought answers from a model and chooses the most consistent final answer (majority vote), intended to improve reasoning robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to large LMs (reported for PaLM 540B in paper's cited comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A technique on top of chain-of-thought prompting that aggregates multiple sampled reasoning traces to find the most frequent numeric answer (a type of sampling-based ensembling).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems (MultiArith) — used as a comparison point.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Aggregating multiple stochastic reasoning trajectories increases the chance of sampling correct chains and cancels out individual-chain errors, thereby improving final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited benchmark number: Zero-shot-CoT + self-consistency (PaLM 540B) = 89.0% on MultiArith (higher than single-sample Zero-shot-CoT PaLM 540B in the cited comparisons), indicating that ensemble over CoT samples helps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The paper notes MathPrompter outperforms self-consistency on quoted numbers (92.5% vs 89.0%), suggesting that generating executable representations + eval may be superior to pure sampling/majority-vote on prose CoT traces. Also, self-consistency still relies on the correctness of individual sampled traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Quoted comparative value: Zero-shot-CoT + self-consistency (PaLM 540B) = 89.0% on MultiArith (from cited literature, as reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No new probing; self-consistency is mentioned as an orthogonal technique (sampling + majority vote) and compared numerically in the results table.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Majority voting can reinforce a common systematic error if many sampled traces share the same mistake; it does not incorporate executable verification of intermediate computations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared by paper as a baseline technique: self-consistency (PaLM 540B) 89.0% vs MathPrompter (175B) 92.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MathPrompter: Mathematical Reasoning using Large Language Models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>Are nlp models really able to solve simple math word problems? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4706",
    "paper_id": "paper-257427208",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "MathPrompter",
            "name_full": "MathPrompter (this paper)",
            "brief_description": "A zero-shot chain-of-thought prompting technique that forces large language models to produce multiple analytic representations (algebraic templates and Python functions) for arithmetic word problems and then verifies them by executing/evaluating those representations across randomized variable bindings to reach a consensus answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 DaVinci (text-davinci-002 / GPT-3 family, 175B)",
            "model_description": "The paper uses the GPT-3 DaVinci completion engine (Text-Davinci-002 is referenced) — a generative transformer-based language model in the GPT-3 family, described as a 175B-parameter GPT-style model in the comparisons.",
            "arithmetic_task_type": "Multi-step arithmetic word problems (MultiArith dataset): addition, subtraction, multiplication, combinations of operations; multi-step multi-operation word problems.",
            "mechanism_hypothesis": "Arithmetic capability is improved by (1) eliciting explicit symbolic/algorithmic representations (algebraic formulae and Python functions) from the LM via targeted prompts, and (2) converting those representations into executable computations and checking for consensus across multiple random instantiations; this shifts the task from pure free-form generation to producing executable, checkable procedures.",
            "evidence_for_mechanism": "Substantial empirical improvement on MultiArith: MathPrompter (Zero-shot-CoT with algebraic+Python outputs + compute verification + consensus) achieves 92.5% accuracy (175B) versus 78.7% for prior Zero-shot-CoT (Kojima, 175B). The paper reports concrete examples where generating algebraic/Python templates yields correct intermediate forms and correct numeric answers after evaluation. The method's use of randomized variable mappings and repeated runs (N ≈ 5) is presented as giving higher confidence when multiple symbolic outputs agree.",
            "evidence_against_mechanism": "The paper explicitly documents failure modes where both algebraic and Pythonic outputs agree but are jointly incorrect (a case in Table 2). It also notes that producing matching symbolic forms does not guarantee correctness, and increasing number of prompts reduces but does not eliminate such errors. There are no internal probing or interpretability analyses to show the LM is truly encoding an algorithmic process rather than producing consistent-but-wrong formulaic outputs.",
            "performance_metrics": "MultiArith accuracy: MathPrompter (Zero-shot-CoT, 175B) = 92.5%. For context reported in the paper: Zero-shot (175B) = 17.7%; Zero-shot (PaLM 540B) = 25.5%; Zero-shot-CoT (Kojima, 175B) = 78.7%; Zero-shot-CoT (PaLM 540B) = 66.1%; Zero-shot-CoT + self-consistency (PaLM 540B) = 89.0%. Few-shot and few-shot-CoT baselines are also reported (e.g., Few-Shot-CoT up to 93.0% for 8 examples).",
            "probing_or_intervention_results": "The main intervention is prompt engineering: (a) prompt the LM to emit an algebraic expression (with variables substituted by a mapping template) and a Python function, (b) evaluate both outputs using Python's eval() on multiple randomized variable assignments, and (c) take the consensus over repeated runs (N ≈ 5). This execution-and-consensus procedure is the only 'intervention' reported and is tied to the observed accuracy gains.",
            "limitations_and_failure_modes": "Reported limitations: (1) Agreement between multiple generated representations does not guarantee correctness — there are cases where both agree yet are wrong; (2) The approach depends on the LM correctly producing valid algebraic/Python code, which can sometimes be syntactically or semantically incorrect; (3) Running many prompt variants increases cost and still may not guarantee correctness; (4) CoT outputs can be excessively lengthy and sometimes contain one-step errors or commonsense mistakes that the verification step may not catch; (5) No principled internal verification or formal proof that the LM is performing algorithmic arithmetic rather than retrieving patterns.",
            "comparison_to_other_models": "Compared directly against standard Zero-shot and Zero-shot-CoT baselines (Kojima et al.) and larger models: MathPrompter (175B) 92.5% vs Zero-shot-CoT (Kojima, 175B) 78.7%; vs Zero-shot-CoT + self-consistency (PaLM 540B) 89.0%. Authors state MathPrompter is comparable to 540B models and SOTA few-shot-CoT approaches while being zero-shot.",
            "uuid": "e4706.0",
            "source_info": {
                "paper_title": "MathPrompter: Mathematical Reasoning using Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Zero-shot-CoT",
            "name_full": "Zero-shot Chain-of-Thought prompting (Kojima et al.)",
            "brief_description": "A prompting method that elicits chain-of-thought style multi-step reasoning from large language models by appending an instruction like 'Let's think step-by-step' to the problem prompt, improving multi-step reasoning in zero-shot settings.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "use",
            "model_name": "Generic large LMs (examples in paper: Text-Davinci-002 / 175B; PaLM 540B reported by citation)",
            "model_description": "Zero-shot-CoT is a prompting technique applied to large generative transformer LMs (examples: GPT-style 175B and PaLM 540B) to induce multi-step chain-of-thought outputs without few-shot exemplars.",
            "arithmetic_task_type": "Multi-step arithmetic word problems (same MultiArith benchmark): multi-operation word problems requiring sequential reasoning.",
            "mechanism_hypothesis": "By prompting the LM to 'think step-by-step', the model is induced to produce intermediate reasoning traces which purportedly expose internal multi-step reasoning capabilities that improve final arithmetic answers (i.e., elicited latent stepwise computation rather than pure direct-answer pattern matching).",
            "evidence_for_mechanism": "Empirical gains reported by Kojima et al. and cited in this paper: Zero-shot-CoT increases MultiArith accuracy dramatically (e.g., from 17.7% to 78.7% on 175B model per the paper). The lengthier chain-of-thought outputs correlate with better arithmetic performance in these benchmarks.",
            "evidence_against_mechanism": "The present paper and examples note that chain-of-thought has no built-in validity check on intermediate steps, can be excessively lengthy, may include one-step computation errors, commonsense mistakes, or produce correct-looking reasoning with incorrect final computation; longer CoT does not guarantee correctness of arithmetic calculation.",
            "performance_metrics": "Reported in this paper (as baseline/comparison): Zero-shot-CoT (Kojima, 175B) = 78.7% on MultiArith; Zero-shot-CoT (PaLM 540B) reported = 66.1% (paper cites these numbers).",
            "probing_or_intervention_results": "No internal probing reported here; the method itself is an intervention (prompt instruction 'Let's think step-by-step'). The paper contrasts Zero-shot-CoT with MathPrompter's additional verification and alternative-representation generation.",
            "limitations_and_failure_modes": "No explicit validity checks on intermediate steps; susceptible to commonsense or arithmetic slips in final computation; chain-of-thought can be verbose and may not reduce token usage; CoT outputs can be consistent yet incorrect without external verification.",
            "comparison_to_other_models": "Compared as the primary baseline: MathPrompter (Zero-shot-CoT with verification) outperforms Zero-shot-CoT (78.7% → 92.5% on MultiArith for 175B). Also compared with PaLM 540B results and self-consistency variants.",
            "uuid": "e4706.1",
            "source_info": {
                "paper_title": "MathPrompter: Mathematical Reasoning using Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-3 DaVinci",
            "name_full": "GPT-3 DaVinci / text-davinci-002 (GPT-3 family)",
            "brief_description": "The paper's primary LLM backbone used to generate algebraic and Pythonic outputs; described as a 175B-parameter GPT-based completion engine (DaVinci/Text-Davinci-002) used for experiments.",
            "citation_title": "Language Models are Few-Shot Learners",
            "mention_or_use": "use",
            "model_name": "GPT-3 DaVinci (Text-Davinci-002) — ~175B parameter GPT-style model",
            "model_description": "A large autoregressive transformer language model (GPT-3 family); used here as the generative engine to produce algebraic expressions, Python functions, and chain-of-thought reasoning under different prompts.",
            "arithmetic_task_type": "Solving MultiArith word problems: multiple arithmetic operations and reasoning chains; single-answer arithmetic word problems (add/sub/mul combos).",
            "mechanism_hypothesis": "When prompted to output symbolic or executable representations, GPT-3 can produce formulaic/algebraic templates and Python code that often correspond to correct arithmetic solution procedures; executing these outputs externally (eval) lets the system leverage the LM as a program synthesizer rather than trusting its native numeric output generation.",
            "evidence_for_mechanism": "Direct usage: GPT-3 DaVinci generated algebraic expressions and Python functions that, when evaluated, produced correct answers in many test cases; the paper attributes improved accuracy to this representation + verification pipeline.",
            "evidence_against_mechanism": "The model sometimes produces incorrect algebraic/Python outputs or produces outputs that agree but are incorrect; there is no internal guarantee that generated code is correct. The paper does not supply probes into whether GPT-3 internally represents arithmetic algorithmically versus via pattern matching.",
            "performance_metrics": "As used in MathPrompter experiments (175B GPT-based): MathPrompter using GPT-3 DaVinci reported 92.5% accuracy on MultiArith. Standalone Zero-shot (GPT-3-style 175B) reported in the paper as 17.7% (baseline).",
            "probing_or_intervention_results": "Intervention: prompt to produce algebraic expression and a Python function, followed by external execution (Python eval) over randomized mappings and consensus over repeated runs (N ≈ 5). No additional mechanistic probes or ablations on the model internals are reported.",
            "limitations_and_failure_modes": "Can output syntactically invalid or semantically incorrect formulas/code; matching outputs across representations doesn't guarantee correctness; costlier due to multiple generations and external evals; no interpretability evidence that numeric ability is algorithmic.",
            "comparison_to_other_models": "Compared implicitly to PaLM 540B numbers quoted in the paper (PaLM 540B Zero-shot and Zero-shot-CoT values). The paper emphasizes that with MathPrompter, a 175B GPT-based model can achieve accuracy comparable to larger 540B models or few-shot-CoT methods.",
            "uuid": "e4706.2",
            "source_info": {
                "paper_title": "MathPrompter: Mathematical Reasoning using Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Self-consistency (CoT)",
            "name_full": "Self-consistency applied to chain-of-thought",
            "brief_description": "A method that samples multiple chain-of-thought answers from a model and chooses the most consistent final answer (majority vote), intended to improve reasoning robustness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": "Applied to large LMs (reported for PaLM 540B in paper's cited comparisons)",
            "model_description": "A technique on top of chain-of-thought prompting that aggregates multiple sampled reasoning traces to find the most frequent numeric answer (a type of sampling-based ensembling).",
            "arithmetic_task_type": "Multi-step arithmetic word problems (MultiArith) — used as a comparison point.",
            "mechanism_hypothesis": "Aggregating multiple stochastic reasoning trajectories increases the chance of sampling correct chains and cancels out individual-chain errors, thereby improving final-answer accuracy.",
            "evidence_for_mechanism": "Cited benchmark number: Zero-shot-CoT + self-consistency (PaLM 540B) = 89.0% on MultiArith (higher than single-sample Zero-shot-CoT PaLM 540B in the cited comparisons), indicating that ensemble over CoT samples helps.",
            "evidence_against_mechanism": "The paper notes MathPrompter outperforms self-consistency on quoted numbers (92.5% vs 89.0%), suggesting that generating executable representations + eval may be superior to pure sampling/majority-vote on prose CoT traces. Also, self-consistency still relies on the correctness of individual sampled traces.",
            "performance_metrics": "Quoted comparative value: Zero-shot-CoT + self-consistency (PaLM 540B) = 89.0% on MultiArith (from cited literature, as reported in this paper).",
            "probing_or_intervention_results": "No new probing; self-consistency is mentioned as an orthogonal technique (sampling + majority vote) and compared numerically in the results table.",
            "limitations_and_failure_modes": "Majority voting can reinforce a common systematic error if many sampled traces share the same mistake; it does not incorporate executable verification of intermediate computations.",
            "comparison_to_other_models": "Compared by paper as a baseline technique: self-consistency (PaLM 540B) 89.0% vs MathPrompter (175B) 92.5%.",
            "uuid": "e4706.3",
            "source_info": {
                "paper_title": "MathPrompter: Mathematical Reasoning using Large Language Models",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1
        },
        {
            "paper_title": "Are nlp models really able to solve simple math word problems?",
            "rating": 1
        }
    ],
    "cost": 0.010541499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MathPrompter: Mathematical Reasoning using Large Language Models
July 10-12, 2023</p>
<p>Shima Imani 
Microsoft Research
RedmondUSA</p>
<p>Liang Du 
Microsoft Research
RedmondUSA</p>
<p>Harsh Shrivastava 
Microsoft Research
RedmondUSA</p>
<p>MathPrompter: Mathematical Reasoning using Large Language Models</p>
<p>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
the 61st Annual Meeting of the Association for Computational Linguistics5July 10-12, 2023
Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose 'MathPrompter', a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chainof-thought prompting technique to generate multiple Algebraic expressions or Python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the MultiArith dataset (78.7% → 92.5%) evaluated using 175B parameter GPT-based LLM.</p>
<p>Introduction</p>
<p>Recent advancements in natural language processing (NLP) can be attributed to massive scaling of Large Language Models (LLMs) (Vaswani et al., 2017;Devlin et al., 2018;Raffel et al., 2020;Brown et al., 2020;Rae et al., 2021;Chowdhery et al., 2022;Thoppilan et al., 2022). A very interesting recent discovery that the LLMs are naturally good (in-context) Zero-shot or few-shot learners turned out to be very useful (Brown et al., 2020;Liu et al., 2021Liu et al., , 2023. This led to the development of 'prompting' technique, where the user provides a small context for solving the task at-hand to the LLM. This conditioning of the models on a few examples is termed as few-shot prompting, while providing instructions to solve a task is known as Zero-shot prompting. Extensive research efforts are being poured into designing these prompts, either manually (Schick and Schütze, 2020;Reynolds and McDonell, 2021) or automatically (Shin et al., 2020;Gao et al., 2020). Although quite successful for single-step system-I tasks (Stanovich and West, 2000;Liu et al., 2023), the prompting techniques were inadequate in their performance on system-II tasks where multi-step reasoning is required (Rae et al., 2021). As humans, we tend to break down a problem and attempt to solve them step-by-step. Extending this intuition to LLMs led to the development of 'chain-of-thought' (CoT) prompting technique . The use of CoT has led to improved performance on a range of NLP tasks (Talmor et al., 2018;Gao et al., 2020;Patel et al., 2021;Cobbe et al., 2021;Geva et al., 2021;Chowdhery et al., 2022;Srivastava et al., 2022) In this work, we investigate Zero-shot-CoT methods for solving mathematical reasoning tasks. To the best of our knowledge, we found the recent work by (Kojima et al., 2022) that proposed a Zeroshot-CoT technique to be the state-of-the-art where they demonstrated a remarkable accuracy improvement on the 'MultiArith' (Roy and Roth, 2016) data (17.7% → 78.7%). Now, we identify two key aspects that lacks in the previous CoT prompting based SOTA, namely (1) Although, the chainof-thought followed by the model improved the results, but there is no check on the validity of the steps followed by the chain-of-thought prompting and (2) The confidence in the predictions of LLMs are often not provided. In order to address these gap to some extent, we derive inspiration from how we humans solve a math question by breaking it down to a simpler multi-step procedure and make use of multiple ways to validate our approach at each step. Specifically, given a question Q, (I) Generating Algebraic template: We first gen-</p>
<p>Python prompt</p>
<p>Write a python function that returns the answer. </p>
<p>Algebraic prompt</p>
<p>Method</p>
<p>Since the LLMs are generative models, it becomes very tricky to ensure that the generated answers are accurate, especially for mathematical reasoning tasks. We take clues from the process followed by students to solve arithmetic problems. We narrowed down a few steps that students take in order to verify their solutions, namely</p>
<p>• Compliance with known results: By comparing the solution to a known result, one can assess its accuracy and make necessary adjustments. This is particularly useful when the question is a standard problem with a well-established solution.</p>
<p>• Multi-verification: By approaching a problem from multiple perspectives and comparing the results helps to confirm the validity of the solution and ensure that it is both sound and accurate.</p>
<p>• Cross-checking: The process of solving a problem is just as necessary as the final answer. Verifying the correctness of the intermediate steps of the process provide a clear understanding of the thought process behind the solution.</p>
<p>• Compute verification: Utilizing a calculator or computer to perform arithmetic calculations can assist in verifying the accuracy of the final answer.</p>
<p>MathPrompter</p>
<p>Our proposed method, MathPrompter, is an attempt to transfer some of this thought process to the LLM answer generation process. Fig. 1 provides a high-level overview of steps followed by Math-Prompter to solve a mathematical reasoning problem. We use the state-of-the-art GPT-3 DaVinci completion engine (Brown et al., 2020) for the question-answering tasks. We use the following question 'Q' from the Mul-tiArith dataset to demonstrate the problem solving process followed by MathPrompter.</p>
<p>Q: At a restaurant, each adult meal costs $5 and kids eat free. If a group of 15 people came in and 8 were kids, how much would it cost for the group to eat? (I) Generating Algebraic template: We begin by transforming the question into its Algebraic form by replacing the numeric entries with variables using a key-value mapping. In this particular instance, the modified question 'Qt' becomes:</p>
<p>Qt: at a restaurant, each adult meal costs A and kids eat free. if a group of B people came in and C were kids, how much would it cost for the group to eat? Mapping: {A:5, B:15, C:8} (II) Math-prompts: We build up on the intuition provided by the multi-verification and crosschecking thought processes mentioned above. We generate analytical solutions of Qt using two different approaches, Algebraic way and Pythonic way. We give the following prompts to the LLM to generate additional context for Qt Algebraic prompt: Write a mathematical equation and generate the answer format starting with 'Answer =' Python prompt: Write a Python function that returns the answer.</p>
<p>The LLM model in response to the above prompts generated the following output expressions</p>
<h1>Algebraic expression output Answer = A*(B-C) # Python expression output def total_price(A, B, C): return A * (B-C)</h1>
<p>The above generated analytical solutions gives the user a hint into the 'intermediate thought process' of the LLM. Incorporating additional prompts will improve the accuracy and consistency of the results. This will, in turn, enhance the Math-Prompter's ability to generate more precise and effective solutions.</p>
<p>(III) Compute verification: We evaluate the expressions generated in the previous step using multiple randomized key-value mappings of the input variables in Qt. To evaluate the expressions, we used the Python's eval() method. We compare the outputs to see if we can find a consensus among the answers. This also provides us with a higher level of confidence that the answers are correct and reliable. Once the expressions agree on their outputs, we use the values of the variables in the input Q to compute the final answer, as below Algebraic-answer = 35 Pythonic-answer = 35</p>
<p>(IV) Statistical significance: In order to ensure that consensus is reached among various expressions' output, in our experiments, we repeat the steps (II) &amp; (III) for N ∼ 5 times and report the most frequent value observed for the answer.</p>
<p>Experiment</p>
<p>Dataset</p>
<p>We evaluate MathPrompter on Multi-Arith dataset (Roy and Roth, 2016), which is a subset of the Math World Problem Repository (Koncel-Kedziorski et al., 2016). This dataset is a collection of mathematical problems that are specifically designed to test the ability of machine learning models to perform complex arithmetic operations and reasoning. These problems demand the application of multiple arithmetic operations and logical reasoning to be sucessfully solved.</p>
<p>Baseline</p>
<p>One of the popular baselines is the standard Zeroshot model by (Brown et al., 2020). Their train their models in a way that it is able to recognize and classify new objects or classes that it has never seen before during training. This was achieved by utilizing the semantic relationships between classes.</p>
<p>We also compared against the state-of-the-art Zero-shot-CoT prompting model by (Kojima et al., 2022). This is a very recent approach that addresses the limitations of the standard Zero-shot learning by incorporating a 'context of the task' using CoT to improve the performance. Briefly, their method follows this procedure. Given a question Q, the authors use the prompt 'Lets think step-by-step' followed by Q to generate a response Z. Then, they use the prompt 'The answer (Arabic numericals) is' followed by Z to get their final result.  (Kojima et al., 2022). They used Textdavinci-002 (175B) model along with the same 8 examples as described in  for Few-shot and Few-shot-CoT settings.)   (Kojima et al., 2022) technique falls short, and where these can be remedied with MathPrompter , which was designed to address these issues. For example, the generated answers sometimes have one step of error, which can be avoided by running the model multiple times and reporting the consensus results.</p>
<p>Results</p>
<p>Accuracy comparisons</p>
<p>Example comparisons</p>
<p>Additionally, the reasoning steps in (Kojima et al., 2022) can be excessively lengthy, but the Pythonic or Algebraic methods can address this by typically requiring fewer tokens. Furthermore, the reasoning steps may be correct, but the final computation is incorrect. MathPrompter address problem by using the Python's eval() method function.</p>
<p>In many cases, the MathPrompter generates correct intermediate and final answers. However, there are a few cases, such as the last question in Table 2, where both the Algebraic and Pythonic outputs are in agreement, yet erroneous. We plan to address these issues by incorporating additional methods to further enhance the performance of MathPrompter .</p>
<p>Conclusions &amp; Discussions</p>
<p>We introduced MathPrompter, a novel approach that improves LLM performance on mathematical reasoning problems. It also addresses an important concern of building the user trust to some extent in the LLM predictions. We translated our intuition on how students solve arithmetic problems to a LLM model by utilizing the Zero-shot chain-of-thought prompting technique. MathPrompter incorporates ideas like cross-checking the intermediate steps and solving the same math problem using multiple approaches in its design. We empirically show that our model is comparable to SOTA Few-shot-CoT models as well as the larger Zero-shot-CoT models that have 540B parameters. In future, we plan to further evaluate performance on additional datasets and explore incorporating additional prompts into That means Jerry will need to make 1 trip to the second table. In total, Jerry will make 3 trips. Therefore, the answer (arabic numerals) is 3. ✗ </p>
<p>Limitation</p>
<p>One of the limitations of our work is that while we are running the MathPrompter multiple times in different ways to increase the accuracy of our results, this does not always guarantee the correctness of the output. Both Algebraic and Pythonic expressions have the potential to produce the incorrect results, even if the prompt outputs match each other. This is the fail case as shown in the last row of Table 2. Increasing the number of prompts will mitigate this issue. We are currently investigating techniques that can address this issue in a more principled manner.</p>
<p>bought 14 boxes of chocolate candy and gave 5 to his little brother. If each box has 6 pieces inside it, how many pieces did Kaleb still have? A: Let's think step by step. Kaleb bought 14 boxes of chocolate candy. This means that he had 84 pieces of chocolate candy. He gave 5 boxes to his little brother, which means that he gave his little brother 30 pieces of chocolate candy. Kaleb still had 54 pieces of chocolate candy. Therefore, the answer (arabic numerals) is 54 ✓.</p>
<p>1 :
1Accuracy on MultiArith dataset. MathPrompter outperforms all the Zero-shot &amp; Zero-shot-CoT baselines. We emphasize that our model's performance is comparable to 540B parameter models as well as the SOTA Few-shot-CoT approaches. (If not mentioned explicitly, the models in each row consists of 175B parameters. Results are borrowed fromModel </p>
<p>Accuracy </p>
<p>Zero-shot 
17.7 
Zero-shot (PaLM 540B) 
25.5 
Zero-shot-CoT 
78.7 
Zero-shot-CoT (PaLM 540B) 
66.1 
Zero-shot-CoT + self consistency (PaLM 540B) 
89.0 
Zero-shot-CoT (MathPrompter) 
92.5 </p>
<p>Few-Shot (2 samples) 
33.7 
Few-Shot (8 samples) 
33.8 
Few-Shot-CoT (2 samples) 
84.8 
Few-Shot-CoT (4 samples) 
90.5 
Few-Shot-CoT (8 samples) 
93.0 
Zero-Plus-Few-Shot-CoT (8 samples) 
92.8 </p>
<p>Table </p>
<p>Table 1
1compares the performance of the Math-Prompter against the baseline models. The results of few-shot &amp; zero-shot learning based approaches are shown. Furthermore, we add the results for models with different number of parameters to get better highlight the significance of our approach.Since, MathPrompter is a Zero-shot-CoT (175B 
parameters) method, we choose the state-of-the-art 
Zero-shot-CoT (175B parameters) model by (Ko-
jima et al., 2022) and a Zero-shot(175B parameters) 
by (Brown et al., 2020) for fair comparison. We 
report an accuracy of 92.5% which is a huge im-
provement to the other SOTA models with 78.7% 
and 17.7% accuracy, respectively. </p>
<p>Table 2
2presents a sample set of questions and 
their respective outputs, intermediate steps, and 
final answers generated by both MathPrompterand 
the current state-of-the-art model (Kojima et al., 
2022). For simplicity, only one output of Math-
Prompter for each question is shown for both the 
Algebraic and Pythonic outputs. 
The table highlights areas where </p>
<p>Table 2 :
2Example-based comparison between current state of art Zero-shot-CoT and MathPrompter .A (OTHERS; MAXIMUM LIMIT OF TEXT LENGTH): Let's think step by step. Each chocolate bar in a box costs $3. If a box had 9 bars total, then Wendy sold all but 3 bars. How much money would she have made? We can solve this problem using algebra. Let's start by creating a variable to represent the number of chocolate bars Wendy sold. We'll call this variable "x". If Wendy sold "x" chocolate bars, then she would have 9 -x chocolate bars left in the box. We know that each chocolate bar costs $3, so the total cost of the chocolate bars Wendy sold Therefore, the answer (arabic numerals) is: Wendy would have made $12. ✗ Q: Jerry was helping the cafeteria workers pick up lunch trays, but he could only carry 8 trays at a time. If he had to pick up 9 trays from one table and 7 trays from another, how many trips will he make?A (COMMONSENSE MISTAKE): Let's think step by step. Jerry can carry 8 trays at a time. There are 9 trays on the first table. That means Jerry will need to make 2 trips to the first table. There are 7 trays on the second table.Question 
SOTA Zero-shot-CoT 
MathPrompter </p>
<p>Q: At the fair Adam bought 13 
tickets. After riding the ferris 
wheel he had 4 tickets left. If 
each ticket cost 9 dollars, how 
much money did Adam spend 
riding the ferris wheel? </p>
<p>A (ONE UNNECESSARY STEP ERROR): Let's 
think step by step. Adam bought 13 tickets. That 
means he spent 13 * 9 = 117 dollars on tickets. 
After riding the ferris wheel, he had 4 tickets left. 
That means he used 9 tickets to ride the ferris wheel. 
That means he spent 9 * 9 = 81 dollars on riding the 
ferris wheel. So the answer is 117 -81 = 36 dollars. 
Therefore, the answer (arabic numerals) is 36. ✗ </p>
<p>Mapping = {A:13, B:4, C:9} </p>
<h1>Algabraic answer</h1>
<p>Answer = A*(B-C) </p>
<h1>Python code</h1>
<p>def total_price(A, B, C): 
return A * (B-C) </p>
<p>Answer = 81 ✓ </p>
<p>Q: Each chocolate bar in a box 
cost $3. If a box had 9 bars 
total and Wendy sold all but 3 
bars, how much money would 
she have made? </p>
<p>Mapping = {A:3, B:9, C:3} </p>
<h1>Algabraic answer</h1>
<p>Answer = A x (B -C) </p>
<h1>Python code</h1>
<p>def money_made(A, B, C): 
return (B-C)*A </p>
<p>Answer = 18 ✓ </p>
<p>Training verifiers to solve math word problems. Nakano, arXiv:2110.14168arXiv preprintNakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.</p>
<p>Making pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, arXiv:2012.15723arXiv preprintTianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 9Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346- 361.</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.11916arXiv preprintTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2022. Large lan- guage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Mawps: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies. the 2016 conference of the north american chapter of the association for computational linguistics: human language technologiesRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. Mawps: A math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 1152-1157.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What makes good in-context examples for gpt-3? arXiv preprint. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 559Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35.</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, arXiv:2103.07191arXiv preprintArkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191.</p>
<p>Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprintJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan- nah Young, et al. 2021. Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 211Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. The Journal of Machine Learning Research, 21(1):5485-5551.</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. Laria Reynolds, Kyle Mcdonell, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Laria Reynolds and Kyle McDonell. 2021. Prompt pro- gramming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Comput- ing Systems, pages 1-7.</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, arXiv:1608.01413arXiv preprintSubhro Roy and Dan Roth. 2016. Solving gen- eral arithmetic word problems. arXiv preprint arXiv:1608.01413.</p>
<p>It's not just size that matters: Small language models are also few-shot learners. Timo Schick, Hinrich Schütze, arXiv:2009.07118arXiv preprintTimo Schick and Hinrich Schütze. 2020. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, I V Robert L Logan, Eric Wallace, Sameer Singh, arXiv:2010.15980arXiv preprintTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980.</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, R Adam, Adam Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615arXiv preprintAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>E Keith, Richard F Stanovich, West, individual differences in reasoning: Implications for the rationality debate? Behavioural and Brain Science. 24Keith E Stanovich and Richard F West. 2000. 24. indi- vidual differences in reasoning: Implications for the rationality debate? Behavioural and Brain Science, 23(5):665-726.</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937arXiv preprintAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowl- edge. arXiv preprint arXiv:1811.00937.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. arXiv preprintRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applica- tions. arXiv preprint arXiv:2201.08239.</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.11171arXiv preprintXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency im- proves chain of thought reasoning in language mod- els. arXiv preprint arXiv:2203.11171.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>            </div>
        </div>

    </div>
</body>
</html>
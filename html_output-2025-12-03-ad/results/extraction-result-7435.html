<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7435 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7435</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7435</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-270737974</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.18528v1.pdf" target="_blank">PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have revolutionized NLP research. Notably, in-context learning enables their use as evaluation metrics for natural language generation, making them particularly advantageous in low-resource scenarios and time-restricted applications. In this work, we introduce **PrExMe**, a large-scale **Pr**ompt **Ex**ploration for **Me**trics, where we evaluate more than 720 prompt templates for open-source LLM-based metrics on machine translation (MT) and summarization datasets, totalling over 6.6M evaluations. This extensive comparison (1) benchmarks recent open-source LLMs as metrics and (2) explores the stability and variability of different prompting strategies. We discover that, on the one hand, there are scenarios for which prompts are stable. For instance, some LLMs show idiosyncratic preferences and favor to grade generated texts with textual labels while others prefer to return numeric scores. On the other hand, the stability of prompts and model rankings can be susceptible to seemingly innocuous changes. For example, changing the requested output format from “0 to 100” to "-1 to +1” can strongly affect the rankings in our evaluation. Our study contributes to understanding the impact of different prompting approaches on LLM-based metrics for MT and summarization evaluation, highlighting the most stable prompting patterns and potential limitations.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7435.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7435.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label-vs-Numeric Preference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-specific preference for textual labels versus numeric scores</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds that different open-source LLMs systematically prefer different output formats when used as evaluation metrics: some models achieve their best correlations when asked to return textual quality labels, others when asked to return numeric scores or continuous ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple open-source LLMs (PLATYPUS2-70B, ORCA-13B, TOWER-13B, LLAMA3-70B, LLAMA3-8B, NOUS-HERMES-13B, MIXTRAL-8x7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned open-source transformer LLMs evaluated as prompt-based, reference-free metrics for MT and summarization (various instruction-tuning datasets and architectures as provided by model authors).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B / 70B / 8B / 8x7B (various per model; PLATYPUS2-70B=70B, ORCA/TOWER/NOUS=13B, LLAMA3-70B=70B, LLAMA3-8B=8B, MIXTRAL=8x7B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reference-free MT and summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade a generated hypothesis with respect to its source (no reference) for translation quality or summary quality, and compute correlation with human judgments at the segment level.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt templates varying output format (textual labels vs numeric scores, discrete vs continuous ranges) within hierarchical prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / output format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Ten output-format requirements tested including discrete binary/ternary labels, 'simple labels' (bad/neutral/good), 'complex labels' (catastrophic/indifferent/marvelous), and numeric ranges such as 0-1, 0-5, -5 to 5, 0 to 100, -100 to 100, -1.0 to 1.0; mapping textual labels to numeric mapping (1/3/5) for evaluation; used across zero-shot and one-shot base prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall correlation to human judgments (main), also Spearman, Pearson, tie-calibrated accuracy reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Observed that the best-performing prompts for each model overwhelmingly used one class of format: e.g., LLAMA3-70B used textual labels in 90.2% of its top-2% prompts; TOWER used textual labels in 80.4% of its top prompts; ORCA used textual labels in only 8% of its top prompts and PLATYPUS2 in 21.7% (i.e., ORCA and PLATYPUS2 predominantly performed best with numeric formats). Best absolute Kendall correlations per model (across hierarchical templates) are reported in Table 2 (e.g., PLATYPUS2-70B reaches Kendall up to 0.549 on some columns).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative: model-specific — using a model's preferred format corresponds to its highest-ranked prompts in the top-2% distribution; the occurrence percentages above quantify the association rather than a single absolute delta.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Phase 1: exhaustive ZS grid (~720 templates) on train split (first 500 MT samples per pair for compute reasons); Phase 2: evaluation of 9 best prompts on full dev/test; generation restricted to 180 tokens; score extracted via regex of last numeric/label; textual labels mapped to numeric for correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Correlations subjected to permute-input significance tests (p ≤ 0.075) for Kendall; reported model-preference prevalences are descriptive of top-2% prompt distributions (no single-number p-value for the preference percentages).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7435.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7435.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Range-Scaling Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Output-range and label-complexity cause substantial model-ranking changes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Changing the numeric range or label scheme in the format requirement can substantially affect model performance rankings; some range changes produce little effect while others cause large or even negative correlations in model ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregated across evaluated models (PLATYPUS2-70B, ORCA-13B, TOWER-13B, LLAMA3 variants, NOUS, MIXTRAL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned open-source LLMs used as prompt-based metrics; analysis studies how model ranking (by Kendall) changes when format requirement is altered.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (13B, 70B, 8B, 8x7B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reference-free MT and summarization evaluation (multiple language pairs and summarization datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Segment-level correlation between LLM-assigned quality scores and human judgments under different output-format requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt asking the model to return a score in a specified numeric range or to pick textual labels (e.g., '0 to 100' vs '-1.0 to 1.0', 'simple labels' vs 'complex labels').</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / output format (numeric range vs textual label granularity)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Format templates included numeric ranges (0-100, -100 to 100, 0.0-1.0, -1.0 to 1.0, 0-5, -5 to 5, 0/1 discrete) and label sets (simple: 3 labels, complex: 3 nonstandard labels); analysis used heatmaps (Figure 5, Figure 15) to show Kendall correlations between model-rankings under two differing format requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall correlation between model scores and human judgments; additionally the Kendall correlation of model-ranking vectors when format A is changed to format B (stability measure).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples reported qualitatively: some format changes (e.g., '0 to 100' vs '-100 to 100') left model ranking mostly unchanged, whereas changes like 'simple labels' to 'complex labels' caused large rank shifts (sometimes strongly negatively correlated). The authors note that changing '0 to 100' to '-1 to +1' can 'strongly affect the rankings.'</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>When comparing certain pairs, e.g., '0 to 100' vs '-100 to 100', the baseline effect on ranking is small (stable); other pairs serve as contrasts where ranking changes drastically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Descriptive: 'strong effect' for some range transformations (not given as a single absolute number); stability heatmaps show mixed behavior, with some pairs highly stable and others producing large rank inversions.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>All-format comparisons aggregated across tasks, models, base prompts and task descriptions; median aggregation used to compute format ranking before/after change; analyses use the full grid of ZS prompts and selected OS prompts in phase 2.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Rank-stability visualizations reported; specific pairwise significance for these format-induced ranking changes is not given as a single p-value in the text, but overall metric significance used permute-input tests (p ≤ 0.075) for direct Kendall correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7435.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7435.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Base-Prompt (PZS vs ZS-CoT) Stability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of base prompt style (Plain Zero-Shot vs Zero-Shot Chain-of-Thought) on format and task ranking stability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Switching the base prompt type can change the relative ordering of format requirements and task descriptions; the most stable base-prompt switch observed was between PZS and ZS-COT (median-aggregated ranking Kendall ≈ 0.65).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregated across models (all evaluated open-source LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs evaluated under different base-prompt styles: Plain Zero-Shot (PZS), Zero-Shot Chain-of-Thought (ZS-COT), and Zero-Shot COT with Emotion (ZS-COT-EM), plus one-shot variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (13B, 70B, 8B, 8x7B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reference-free MT and summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Segment-level correlation with human judgments while varying the base prompt instruction style (PZS vs ZS-COT vs ZS-COT-EM).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Hierarchical prompt templates where the base prompt either directly asks for a score (PZS), asks for step-by-step reasoning then the score (ZS-COT), or asks for emotion description then CoT then score (ZS-COT-EM).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / reasoning instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Base prompts: PZS = plain newline-separated task/source/hypothesis/format; ZS-COT = include 'think step by step' reasoning; ZS-COT-EM = instruct model to 'describe your emotions' then CoT. Both ZS and OS variants evaluated; median aggregation across other template components used to assess stability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall correlation of rankings (stability measure) and Kendall correlation to human judgments for resulting prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Highest observed stability when switching between PZS and ZS-COT (median-aggregated ranking Kendall ≈ 0.65), i.e., a prompt-format chosen for one of those base prompts has a high chance of performing well with the other.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>PZS and ZS-COT as compared pair; median aggregation provided the most stable ranking baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Switching between PZS and ZS-COT tends to preserve the relative ranking of format requirements (Kendall ≈ 0.65); switching to or from ZS-COT-EM or other base prompts often changes rankings more.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Aggregated across tasks, models, and other prompt components; median aggregation of scores per format requirement was used to build rankings; tested via permutation comparisons of aggregation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Aggregation method selection: median was found significantly better (p ≤ 0.05) than other aggregation measures in permutation tests, and remained significant after Bonferroni correction in comparisons to saturation and standard deviation (see Appendix G).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7435.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7435.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregation and Stability Recommendation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Median aggregation of prompt-component performance yields most stable transfer across datasets and prompt-perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When evaluating the relative quality of prompt components (e.g., format requirement or task description) across changes to other components or datasets, using the median of scores across combinations produces the most stable ranking and is recommended as an indicator for new settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Method-level observation across evaluated LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single model but an analysis-level finding across the grid of open-source LLM evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a (analysis over multiple models and sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prompt-component transferability analysis within MT and summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess which aggregation method (mean, median, mean-of-top-10%, max, min, saturation) of prompt-component scores best predicts relative performance when datasets or other prompt components change.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Meta-analytic ranking of prompt components using different aggregation rules applied to collections of prompt results.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>evaluation methodology / aggregation strategy</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Aggregation methods tested: mean, median, mean of top 10%, max, min, saturation; median achieved highest Kendall similarity between rankings before/after changes and was shown by permutation testing to be significantly better than other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall correlation of rankings under different aggregation choices; permutation tests comparing aggregation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Median aggregation produced the most stable rankings (highest Kendall correlation) when comparing format/task rankings before and after perturbations across other prompt dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Other aggregations (mean, max, etc.) — median outperformed these baselines in permutation tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Median aggregation gave consistently higher stability than alternatives (statistical tests show median significantly better than many alternatives; exact Kendall deltas reported in Appendix G).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>All ZS prompt experiments aggregated across tasks/models/other prompt components; permutation tests performed by random swapping of 50% of samples between aggregation methods to test which aggregation yields higher Kendall.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Median was significantly better (p ≤ 0.05) than other methods in permutation tests and remained significant against saturation and standard deviation after Bonferroni correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7435.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7435.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>One-shot (RAG) Generalization Weakness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weak generalizability of selected one-shot (retrieval-augmented) prompts to other datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One-shot demonstrations (retrieval-augmented generation using nearest-example RAG) that performed well in phase 1 generally showed weaker performance and weaker generalization on additional datasets (WMT23/Seahorse), so the authors limited OS evaluation in phase 2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Selected LLMs (PLATYPUS2-70B, ORCA-13B among others evaluated in OS experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models evaluated with one-shot demonstrations selected by RAG (XLMR-SBERT embeddings + cosine similarity selection of example), i.e., OS-CoT with retrieved example.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reference-free MT and summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use of one-shot demonstration (RAG) in base prompt to improve scoring; tested whether OS-selected prompts generalize to unseen datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>One-shot (OS) hierarchical templates: single demonstration inserted (RAG-selected) plus base prompt variants (OS, OS-CoT, OS-CoT-EM).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot with retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Demonstrations retrieved by embedding (XLMR-SBERT) of concatenated source+hypothesis and selecting example with highest cosine similarity; due to compute only 9 best ZS prompts were evaluated in OS setting; OS demos constructed from WMT21 and ROSE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Kendall correlation to human judgments; percentage of prompts with no score returned (no-extraction cases also reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>OS prompts demonstrated weaker performance on other datasets, prompting the authors not to evaluate OS on WMT23/SEAHORSE; in phase 1, OS had 19.4% no-score-extraction cases and in phase 2 similar rates (19.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot (ZS) counterpart performance — ZS prompts evaluated broadly and used to select 9 best prompts for OS testing; OS did not generalize as well to held-out datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative: OS improved some in-domain metrics but showed weaker out-of-domain generalization; authors therefore limited OS evaluation in phase 2.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>RAG: XLMR-SBERT sentence embeddings of concatenated source+hyp; nearest neighbor demonstration chosen per input; constrained to 9 best ZS prompts for OS experiments; generation limit 180 tokens; results include percentage of runs with no extracted score.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Not explicitly reported for the OS vs ZS generalization weakness; decision to omit OS on some test sets was based on observed weaker performance and resource considerations rather than a single statistical test.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mind your format: Towards consistent evaluation of in-context learning improvements <em>(Rating: 2)</em></li>
                <li>Quantifying language models' sensitivity to spurious features in prompt design or: How I learned to start worrying about prompt formatting <em>(Rating: 2)</em></li>
                <li>State of what art? a call for multi-prompt llm evaluation <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>GEMBA-MQM: Detecting translation quality error spans with GPT-4 <em>(Rating: 1)</em></li>
                <li>Which is better? exploring prompting strategy for LLM-based metrics <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7435",
    "paper_id": "paper-270737974",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Label-vs-Numeric Preference",
            "name_full": "Model-specific preference for textual labels versus numeric scores",
            "brief_description": "The paper finds that different open-source LLMs systematically prefer different output formats when used as evaluation metrics: some models achieve their best correlations when asked to return textual quality labels, others when asked to return numeric scores or continuous ranges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple open-source LLMs (PLATYPUS2-70B, ORCA-13B, TOWER-13B, LLAMA3-70B, LLAMA3-8B, NOUS-HERMES-13B, MIXTRAL-8x7B)",
            "model_description": "Instruction-tuned open-source transformer LLMs evaluated as prompt-based, reference-free metrics for MT and summarization (various instruction-tuning datasets and architectures as provided by model authors).",
            "model_size": "13B / 70B / 8B / 8x7B (various per model; PLATYPUS2-70B=70B, ORCA/TOWER/NOUS=13B, LLAMA3-70B=70B, LLAMA3-8B=8B, MIXTRAL=8x7B)",
            "task_name": "Reference-free MT and summarization evaluation",
            "task_description": "Grade a generated hypothesis with respect to its source (no reference) for translation quality or summary quality, and compute correlation with human judgments at the segment level.",
            "problem_format": "Natural-language prompt templates varying output format (textual labels vs numeric scores, discrete vs continuous ranges) within hierarchical prompt templates.",
            "format_category": "prompt style / output format",
            "format_details": "Ten output-format requirements tested including discrete binary/ternary labels, 'simple labels' (bad/neutral/good), 'complex labels' (catastrophic/indifferent/marvelous), and numeric ranges such as 0-1, 0-5, -5 to 5, 0 to 100, -100 to 100, -1.0 to 1.0; mapping textual labels to numeric mapping (1/3/5) for evaluation; used across zero-shot and one-shot base prompts.",
            "performance_metric": "Kendall correlation to human judgments (main), also Spearman, Pearson, tie-calibrated accuracy reported",
            "performance_value": "Observed that the best-performing prompts for each model overwhelmingly used one class of format: e.g., LLAMA3-70B used textual labels in 90.2% of its top-2% prompts; TOWER used textual labels in 80.4% of its top prompts; ORCA used textual labels in only 8% of its top prompts and PLATYPUS2 in 21.7% (i.e., ORCA and PLATYPUS2 predominantly performed best with numeric formats). Best absolute Kendall correlations per model (across hierarchical templates) are reported in Table 2 (e.g., PLATYPUS2-70B reaches Kendall up to 0.549 on some columns).",
            "baseline_performance": null,
            "performance_change": "Qualitative: model-specific — using a model's preferred format corresponds to its highest-ranked prompts in the top-2% distribution; the occurrence percentages above quantify the association rather than a single absolute delta.",
            "experimental_setting": "Phase 1: exhaustive ZS grid (~720 templates) on train split (first 500 MT samples per pair for compute reasons); Phase 2: evaluation of 9 best prompts on full dev/test; generation restricted to 180 tokens; score extracted via regex of last numeric/label; textual labels mapped to numeric for correlations.",
            "statistical_significance": "Correlations subjected to permute-input significance tests (p ≤ 0.075) for Kendall; reported model-preference prevalences are descriptive of top-2% prompt distributions (no single-number p-value for the preference percentages).",
            "uuid": "e7435.0",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Range-Scaling Effect",
            "name_full": "Output-range and label-complexity cause substantial model-ranking changes",
            "brief_description": "Changing the numeric range or label scheme in the format requirement can substantially affect model performance rankings; some range changes produce little effect while others cause large or even negative correlations in model ranking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Aggregated across evaluated models (PLATYPUS2-70B, ORCA-13B, TOWER-13B, LLAMA3 variants, NOUS, MIXTRAL)",
            "model_description": "Instruction-tuned open-source LLMs used as prompt-based metrics; analysis studies how model ranking (by Kendall) changes when format requirement is altered.",
            "model_size": "various (13B, 70B, 8B, 8x7B)",
            "task_name": "Reference-free MT and summarization evaluation (multiple language pairs and summarization datasets)",
            "task_description": "Segment-level correlation between LLM-assigned quality scores and human judgments under different output-format requirements.",
            "problem_format": "Prompt asking the model to return a score in a specified numeric range or to pick textual labels (e.g., '0 to 100' vs '-1.0 to 1.0', 'simple labels' vs 'complex labels').",
            "format_category": "prompt style / output format (numeric range vs textual label granularity)",
            "format_details": "Format templates included numeric ranges (0-100, -100 to 100, 0.0-1.0, -1.0 to 1.0, 0-5, -5 to 5, 0/1 discrete) and label sets (simple: 3 labels, complex: 3 nonstandard labels); analysis used heatmaps (Figure 5, Figure 15) to show Kendall correlations between model-rankings under two differing format requirements.",
            "performance_metric": "Kendall correlation between model scores and human judgments; additionally the Kendall correlation of model-ranking vectors when format A is changed to format B (stability measure).",
            "performance_value": "Examples reported qualitatively: some format changes (e.g., '0 to 100' vs '-100 to 100') left model ranking mostly unchanged, whereas changes like 'simple labels' to 'complex labels' caused large rank shifts (sometimes strongly negatively correlated). The authors note that changing '0 to 100' to '-1 to +1' can 'strongly affect the rankings.'",
            "baseline_performance": "When comparing certain pairs, e.g., '0 to 100' vs '-100 to 100', the baseline effect on ranking is small (stable); other pairs serve as contrasts where ranking changes drastically.",
            "performance_change": "Descriptive: 'strong effect' for some range transformations (not given as a single absolute number); stability heatmaps show mixed behavior, with some pairs highly stable and others producing large rank inversions.",
            "experimental_setting": "All-format comparisons aggregated across tasks, models, base prompts and task descriptions; median aggregation used to compute format ranking before/after change; analyses use the full grid of ZS prompts and selected OS prompts in phase 2.",
            "statistical_significance": "Rank-stability visualizations reported; specific pairwise significance for these format-induced ranking changes is not given as a single p-value in the text, but overall metric significance used permute-input tests (p ≤ 0.075) for direct Kendall correlations.",
            "uuid": "e7435.1",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Base-Prompt (PZS vs ZS-CoT) Stability",
            "name_full": "Effect of base prompt style (Plain Zero-Shot vs Zero-Shot Chain-of-Thought) on format and task ranking stability",
            "brief_description": "Switching the base prompt type can change the relative ordering of format requirements and task descriptions; the most stable base-prompt switch observed was between PZS and ZS-COT (median-aggregated ranking Kendall ≈ 0.65).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Aggregated across models (all evaluated open-source LLMs)",
            "model_description": "Instruction-tuned LLMs evaluated under different base-prompt styles: Plain Zero-Shot (PZS), Zero-Shot Chain-of-Thought (ZS-COT), and Zero-Shot COT with Emotion (ZS-COT-EM), plus one-shot variants.",
            "model_size": "various (13B, 70B, 8B, 8x7B)",
            "task_name": "Reference-free MT and summarization evaluation",
            "task_description": "Segment-level correlation with human judgments while varying the base prompt instruction style (PZS vs ZS-COT vs ZS-COT-EM).",
            "problem_format": "Hierarchical prompt templates where the base prompt either directly asks for a score (PZS), asks for step-by-step reasoning then the score (ZS-COT), or asks for emotion description then CoT then score (ZS-COT-EM).",
            "format_category": "prompt style / reasoning instruction",
            "format_details": "Base prompts: PZS = plain newline-separated task/source/hypothesis/format; ZS-COT = include 'think step by step' reasoning; ZS-COT-EM = instruct model to 'describe your emotions' then CoT. Both ZS and OS variants evaluated; median aggregation across other template components used to assess stability.",
            "performance_metric": "Kendall correlation of rankings (stability measure) and Kendall correlation to human judgments for resulting prompts.",
            "performance_value": "Highest observed stability when switching between PZS and ZS-COT (median-aggregated ranking Kendall ≈ 0.65), i.e., a prompt-format chosen for one of those base prompts has a high chance of performing well with the other.",
            "baseline_performance": "PZS and ZS-COT as compared pair; median aggregation provided the most stable ranking baseline.",
            "performance_change": "Switching between PZS and ZS-COT tends to preserve the relative ranking of format requirements (Kendall ≈ 0.65); switching to or from ZS-COT-EM or other base prompts often changes rankings more.",
            "experimental_setting": "Aggregated across tasks, models, and other prompt components; median aggregation of scores per format requirement was used to build rankings; tested via permutation comparisons of aggregation methods.",
            "statistical_significance": "Aggregation method selection: median was found significantly better (p ≤ 0.05) than other aggregation measures in permutation tests, and remained significant after Bonferroni correction in comparisons to saturation and standard deviation (see Appendix G).",
            "uuid": "e7435.2",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Aggregation and Stability Recommendation",
            "name_full": "Median aggregation of prompt-component performance yields most stable transfer across datasets and prompt-perturbations",
            "brief_description": "When evaluating the relative quality of prompt components (e.g., format requirement or task description) across changes to other components or datasets, using the median of scores across combinations produces the most stable ranking and is recommended as an indicator for new settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Method-level observation across evaluated LLMs",
            "model_description": "Not a single model but an analysis-level finding across the grid of open-source LLM evaluations.",
            "model_size": "n/a (analysis over multiple models and sizes)",
            "task_name": "Prompt-component transferability analysis within MT and summarization evaluation",
            "task_description": "Assess which aggregation method (mean, median, mean-of-top-10%, max, min, saturation) of prompt-component scores best predicts relative performance when datasets or other prompt components change.",
            "problem_format": "Meta-analytic ranking of prompt components using different aggregation rules applied to collections of prompt results.",
            "format_category": "evaluation methodology / aggregation strategy",
            "format_details": "Aggregation methods tested: mean, median, mean of top 10%, max, min, saturation; median achieved highest Kendall similarity between rankings before/after changes and was shown by permutation testing to be significantly better than other methods.",
            "performance_metric": "Kendall correlation of rankings under different aggregation choices; permutation tests comparing aggregation methods.",
            "performance_value": "Median aggregation produced the most stable rankings (highest Kendall correlation) when comparing format/task rankings before and after perturbations across other prompt dimensions.",
            "baseline_performance": "Other aggregations (mean, max, etc.) — median outperformed these baselines in permutation tests.",
            "performance_change": "Median aggregation gave consistently higher stability than alternatives (statistical tests show median significantly better than many alternatives; exact Kendall deltas reported in Appendix G).",
            "experimental_setting": "All ZS prompt experiments aggregated across tasks/models/other prompt components; permutation tests performed by random swapping of 50% of samples between aggregation methods to test which aggregation yields higher Kendall.",
            "statistical_significance": "Median was significantly better (p ≤ 0.05) than other methods in permutation tests and remained significant against saturation and standard deviation after Bonferroni correction.",
            "uuid": "e7435.3",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "One-shot (RAG) Generalization Weakness",
            "name_full": "Weak generalizability of selected one-shot (retrieval-augmented) prompts to other datasets",
            "brief_description": "One-shot demonstrations (retrieval-augmented generation using nearest-example RAG) that performed well in phase 1 generally showed weaker performance and weaker generalization on additional datasets (WMT23/Seahorse), so the authors limited OS evaluation in phase 2.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Selected LLMs (PLATYPUS2-70B, ORCA-13B among others evaluated in OS experiments)",
            "model_description": "Models evaluated with one-shot demonstrations selected by RAG (XLMR-SBERT embeddings + cosine similarity selection of example), i.e., OS-CoT with retrieved example.",
            "model_size": "various (13B, 70B)",
            "task_name": "Reference-free MT and summarization evaluation",
            "task_description": "Use of one-shot demonstration (RAG) in base prompt to improve scoring; tested whether OS-selected prompts generalize to unseen datasets.",
            "problem_format": "One-shot (OS) hierarchical templates: single demonstration inserted (RAG-selected) plus base prompt variants (OS, OS-CoT, OS-CoT-EM).",
            "format_category": "prompt style / few-shot with retrieval",
            "format_details": "Demonstrations retrieved by embedding (XLMR-SBERT) of concatenated source+hypothesis and selecting example with highest cosine similarity; due to compute only 9 best ZS prompts were evaluated in OS setting; OS demos constructed from WMT21 and ROSE.",
            "performance_metric": "Kendall correlation to human judgments; percentage of prompts with no score returned (no-extraction cases also reported).",
            "performance_value": "OS prompts demonstrated weaker performance on other datasets, prompting the authors not to evaluate OS on WMT23/SEAHORSE; in phase 1, OS had 19.4% no-score-extraction cases and in phase 2 similar rates (19.4%).",
            "baseline_performance": "Zero-shot (ZS) counterpart performance — ZS prompts evaluated broadly and used to select 9 best prompts for OS testing; OS did not generalize as well to held-out datasets.",
            "performance_change": "Qualitative: OS improved some in-domain metrics but showed weaker out-of-domain generalization; authors therefore limited OS evaluation in phase 2.",
            "experimental_setting": "RAG: XLMR-SBERT sentence embeddings of concatenated source+hyp; nearest neighbor demonstration chosen per input; constrained to 9 best ZS prompts for OS experiments; generation limit 180 tokens; results include percentage of runs with no extracted score.",
            "statistical_significance": "Not explicitly reported for the OS vs ZS generalization weakness; decision to omit OS on some test sets was based on observed weaker performance and resource considerations rather than a single statistical test.",
            "uuid": "e7435.4",
            "source_info": {
                "paper_title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mind your format: Towards consistent evaluation of in-context learning improvements",
            "rating": 2,
            "sanitized_title": "mind_your_format_towards_consistent_evaluation_of_incontext_learning_improvements"
        },
        {
            "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How I learned to start worrying about prompt formatting",
            "rating": 2,
            "sanitized_title": "quantifying_language_models_sensitivity_to_spurious_features_in_prompt_design_or_how_i_learned_to_start_worrying_about_prompt_formatting"
        },
        {
            "paper_title": "State of what art? a call for multi-prompt llm evaluation",
            "rating": 2,
            "sanitized_title": "state_of_what_art_a_call_for_multiprompt_llm_evaluation"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "GEMBA-MQM: Detecting translation quality error spans with GPT-4",
            "rating": 1,
            "sanitized_title": "gembamqm_detecting_translation_quality_error_spans_with_gpt4"
        },
        {
            "paper_title": "Which is better? exploring prompting strategy for LLM-based metrics",
            "rating": 2,
            "sanitized_title": "which_is_better_exploring_prompting_strategy_for_llmbased_metrics"
        }
    ],
    "cost": 0.01775525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation
26 Jun 2024</p>
<p>Christoph Leiter christoph.leiter@uni-mannheim.de 
Natural Language Learning Group (NLLG) https://nl2g.github.io
University of Mannheim</p>
<p>Steffen Eger steffen.eger@uni-mannheim.de 
Natural Language Learning Group (NLLG) https://nl2g.github.io
University of Mannheim</p>
<p>PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation
26 Jun 2024C927D8A2023D544B4C5F1CC519CB6422arXiv:2406.18528v1[cs.CL]
Large language models (LLMS) have revolutionized the field of NLP.Notably, their incontext learning capabilities also enable their use as evaluation metrics for natural language generation, making them particularly advantageous in low-resource scenarios and timerestricted applications.In this work, we introduce PrExMe, a large-scale prompt exploration for metrics, where we evaluate more than 720 prompt templates for open-source LLM-based metrics on machine translation (MT) and summarization datasets, totalling over 6.6M evaluations.This extensive comparison (1) serves as a benchmark of the performance of recent open-source LLMS as metrics and (2) explores the stability and variability of different prompting strategies.We discover that, on the one hand, there are scenarios for which prompts are stable.For instance, some LLMS show idiosyncratic preferences and favor to grade generated texts with textual labels while others prefer to return numeric scores.On the other hand, the stability of prompts and model rankings can be susceptible to seemingly innocuous changes.For example, changing the requested output format from "0 to 100" to "-1 to +1" can strongly affect the rankings in our evaluation.Our study contributes to understanding the impact of different prompting approaches on LLM-based metrics for MT and summarization evaluation, highlighting the most stable prompting patterns and potential limitations. 1</p>
<p>Introduction</p>
<p>The recent popularity and success of LLMS have led to a paradigm shift in NLP (Zhang et al., 2023).Instruction-tuning allows LLMS to generate responses to complex task descriptions (prompts) (Ouyang et al., 2022), making them useful for conventional NLP tasks.One such task is the automatic evaluation of natural language generation (NLG) models in machine translation (MT) and summarization.Following the current trend, researchers use LLMS as evaluation metrics and achieve remarkable performance, sometimes relying solely on in-context learning (e.g.Kocmi and Federmann, 2023a;Fernandes et al., 2023), i.e., with metrics that are purely based on prompting.Such prompting-based metrics require no or only a few data samples, making them useful for lowresource evaluation scenarios (Belouadi and Eger, 2023).Additionally, they are often more resourceefficient since they do not require fine-tuning.</p>
<p>Although many prompting-based metrics have been proposed (e.g.Li et al., 2024b), structured evaluations across different prompting approaches remain scarce, especially for open-source models.In recent work, the EVAL4NLP 2023 shared task (Leiter et al., 2023) addresses this by (1) restricting the usage to selected open-source LLMs and (2) prohibiting the fine-tuning of these models.While the shared-task submissions provide several interesting findings, they focus on a few distinct prompts only.Notably, the effect and robustness of prompt variations on the same model or across different models remain largely unexplored.</p>
<p>In this work, we introduce a systematic Prompt Exploration for Metrics (PrExMe), that builds upon EVAL4NLP 2023, to provide a much larger, template-based, structured evaluation of the effects different input prompts have on an LLM-based metric's correlation with human judgements in MT and summarization evaluation.We formulate the following research questions:</p>
<p>RQ1 Can open-source language models evaluate text generation without fine-tuning and how do they differ from each other?</p>
<p>RQ2 Can we identify patterns 2 in prompts that lead to a stable performance across different datasets, tasks, and models?</p>
<p>RQ3 How should researchers design prompts for new evaluation scenarios?</p>
<p>Our prompt exploration constructs hierarchical templates based on approaches such as chain-ofthought (COT) (Kojima et al., 2022), zero-shot and retrieval-augmented generation (RAG) (Gao et al., 2024b).Each template gets filled with further subtemplates.For example, we vary the requested output formats, such as distinct scores and continuous scores (see §3).This setup amounts to more than 720 prompt templates that we evaluate with 7 LLMS.In a 2nd phase, we test the generalizability and performance of the prompts with the best correlations on two further datasets.</p>
<p>In summary, our work makes the following key contributions and findings:</p>
<p>✓ We perform a large-scale analysis (evaluating over 6.6M prompts) of the effect of different prompting approaches on LLM-based metrics for MT and summarization evaluation.This comprehensive exploration includes various prompting techniques, datasets, tasks, and models, making it, to our knowledge, the most extensive evaluation of its kind.✓ We show that certain prompting patterns are robust and generalizable across different tasks and datasets, with the median performance being a good predictor for new settings.For example, some models show a distinctive preference to return textual labels, while others achieve better results with numeric labels.On the other hand for some settings even small changes to the input prompt can strongly affect the performance. 2We define prompting patterns as the template components that constitute a prompt (e.g., zero-shot, one-shot or the output format).</p>
<p>✓ Our study tackles prompt-based evaluation with</p>
<p>open-source LLMs, targeting scenarios where fine-tuning or access to closed-source LLMs is not possible.Such evaluations are still very scarce but important to make research more accessible, fostering diversity and inclusion.✓ By systematically testing various established prompting approaches, including zero-shot, CoT and RAG, we comprehensively evaluate the performance of recent open-source LLMs for evaluation metrics.Aligning with the recommendations of Mizrahi et al. (2024), by evaluating each model with multiple prompts, our LLM comparison is fair because we mitigate the risk of any single prompt disproportionately affecting their performance.We find that the model PLATYPUS2-70B (Lee et al., 2023a) achieves the strongest performance for the tested LLMs.</p>
<p>Related Work</p>
<p>We first describe the related work of promptingbased metrics for MT and summarization.Then, we relate our work to research on prompting techniques and prompt stability.</p>
<p>Prompting-based metrics Recent advancements in LLM-based metrics for NLG often rely on incontext learning, directly predicting quality judgments from generated texts.Surveys by Li et al. (2024b) and Gao et al. (2024a) provide comprehensive overviews of these metrics.Besides BARTSCORE (Yuan et al., 2021) and PRD (Li et al., 2024a), the prompt-based approaches surveyed by Li et al. (2024b) are built upon closedsource models.In contrast, the EVAL4NLP 2023 shared task (Leiter et al., 2023), explicitly considers open-source prompt-based metrics, by asking participants to evaluate MT and summarization using only provided models without fine-tuning.The best submissions were able to beat strong baselines such as GEMBA (Kocmi and Federmann, 2023b) for MT and BARTSCORE for summarization.While the shared task yielded interesting techniques, the participants explored a limited range of prompts, leaving a gap in the comprehensive analysis of prompting patterns and the consistent comparison of LLMs.In this work, we fill this gap and systematically analyze a much larger set of prompts on a comparable grid of experimental settings to (1) study the robustness of prompts across datasets, models and tasks, and to (2) search for rules and patterns that can guide the future con-struction of prompt-based metrics.</p>
<p>Prompting Techniques Many successful prompting techniques have been proposed over the last years (e.g., Liu et al., 2023a).Our work mostly relies on established approaches such as Zero-Shot CoT and RAG.Further, Li et al. (2023) propose emotion inducing prompts to improve LLM performance.To our best knowledge, we are the first to analyze this technique for evaluation metrics.Inspired by this, we also propose a novel emotion-CoT pattern (see §3). Prior evaluation of output formats for prompt-based metrics is done by Kocmi and Federmann (2023b), which we extend by our much broader evaluation.Other works also use hierarchical templates for prompt building (e.g.Fu et al., 2023) and tools like LangChain (Chase, 2022) and DSPy (Khattab et al., 2023) support their implementation.We use hierarchical templates as means for a structured comparison among prompting patterns.</p>
<p>Prompting Robustness As we conduct a grid search across different prompts, datasets and tasks, our work builds upon and extends research on how LLMS respond to prompt perturbations.Webson and Pavlick (2022), Leidinger et al. (2023), Weber et al. (2023) and Sclar et al. (2023) find a wide range of performance variation for natural language inference and sentiment classification.As a solution, Sclar et al. (2023) suggest to provide the full range of results across different prompt perturbations.Voronov et al. (2024) and Mizrahi et al. (2024) suggest that current evaluation benchmarks for LLMS are problematic as they often only provide one prompt template per task.This could be solved by providing multiple templates and evaluating the ensemble.To our best knowledge, we are the first to explore to which degree these robustness problems affect open-source LLM-based metrics and how to select the best prompts for them.Also, by prompting the LLMs with multiple prompts, we follow Mizrahi et al. (2024) and achieve a stable and fair evaluation of LLMs for this task.</p>
<p>Setup</p>
<p>In this section, we present the templates and prompting techniques we employ for utilizing LLMS as metrics.Additionally, we provide an overview of the datasets and models that we use for testing.We evaluate LLMS in a reference-free setting, i.e., they grade a generated hypothesis based on its source without a reference. 3The evaluated prompt types provide a comprehensive evaluation framework for LLM-based metrics.This range covers basic in-context learning, sophisticated reasoning, emotional context, and varying output structures, ensuring a thorough assessment of robustness and adaptability across tasks and datasets.</p>
<p>Prompt Templates Our prompts are constructed as hierarchical templates (see Figure 1), i.e., one large template is constructed from multiple smaller ones.Each prompt is constructed from: (1) the source text and generated hypothesis text that should be graded, (2) a base prompt, (3) a task description, (4) a format requirement and (5) optionally a one-shot demonstration.Table 1 presents examples for (2), (3), ( 4) and ( 5).</p>
<p>The base prompt is the top layer of our prompt hierarchy, incorporating the other components.Specifically, we test three zero-shot (ZS) and corresponding one-shot (OS) base prompts: (1) Plain ZS/OS (PZS/POS), (2) ZS/OS-COT and (3) ZS/OS-CoT-Emotion (ZS/OS-COT-EM).PZS plainly presents the newline separated task description, source, hypothesis and format requirement.ZS-COT (KOJIMA ET AL., 2022) additionally asks the model to think step by step before returning its output.Lastly, ZS-COT-EM asks the model to describe its "emotions" before the ZS-CoT prompt.We include COT as it has improved the promptbased performance for closed-source metrics like AUTOMQM Fernandes et al. (2023) and GEMBA (Kocmi and Federmann, 2023a).ZS-COT-EM explores the variation of LLM performance when prompted to describe emotions in its output.This is motivated by our exploration of emotional prompts on metric performance (see "task description" below).The OS versions of the templates add a field for demonstrations.To avoid fixating the model on specific reasoning steps, we include a placeholder for OS-CoT where the model should insert its reasoning.</p>
<p>The task description is the instruction to grade the generated hypothesis.Li et al. (2023) find that LLM instructions that induce certain emotions for humans can cause performance improvements.Inspired by this finding, we explore the usage of "emotional prompts" in the task description.Primarily, this approach offers a simple paraphrasation strategy to increase the scope of our grid search.Additionally, it allows us to study the impact of "emotions" on LLM-based metrics.Besides neutral prompts, we include instructions that are, e.g., polite, threatening and sceptical.We create 11 task descriptions ourselves and 13 further descriptions with CHATGPT (OpenAI, 2023).</p>
<p>The format requirement describes the output format the LLM should adhere to when generating a score.For example, it includes the range in which the output score should be and whether it should be discrete or continuous.Additionally, we include prompts that ask the LLM to return textual quality labels.In total, we define 10 format requirements.</p>
<p>Lastly, we construct the optional OS demonstrations with RAG.We extract demonstrations from WMT21 (Freitag et al., 2021) for MT and from ROSE for summarization.4 (Liu et al., 2023b).For each sample in both datasets and for each input sample of our metric, we create sentence embeddings with XLMR-SBERT (Reimers and Gurevych, 2020).Thereby, we concatenate the source and hypothesis embeddings.For each input, we select the demonstration with the highest cosine similarity.Due to resource limitations, we only evaluate the 9 best ZS prompts in a OS setting.The selection process is described in the paragraph Datasets and phases below.</p>
<p>MQM-based approaches</p>
<p>Additionally to hierarchical templates, we test the prompts of GEMBA-MQM (Kocmi and Federmann, 2023a) with the selected open-source LLMS.GEMBA-MQM, which predicts scores based on the number of present errors weighted by severity, normally uses GPT4.We refer to the open-source implementation as LocalGemba.</p>
<p>Score Extraction &amp; Evaluation</p>
<p>We restrict generation to 180 tokens and extract the last regex match of a number/label as scores.When no result is found, we average the other scores of its prompt template.For format requirements with text labels, we map the labels to 1, 3 and 5.</p>
<p>We evaluate prompt templates on the segmentlevel, like the WMT QE and metrics shared tasks (e.g.Freitag et al., 2022Freitag et al., , 2021;;Zerva et al., 2022).That means, for each metric we compute the correlation between metric scores and ground truth human judgments without averaging by system or document.As correlation measure, we use the Kendall (Kendall, 1945), Pearson and Spearman correlations, as well as tie-calibrated accuracy (Deutsch et al., 2023), with Kendall as main measure.Further, we compute permute-input significance tests (p ≤ 0.075) (Deutsch et al., 2021) for the Kendall correlations presented in our result tables.Often, there is no single significantly best metric.Therefore, we report clusters where each included metric is significantly better than metrics that are not included.</p>
<p>Models</p>
<p>We select instruction-tuned LLMS with strong performance in EVAL4NLP 2023: (1) PLATYPUS2-70B-INSTRUCT-GPTQ, (2) NOUS-HERMES-13B5 and (3) OPENORCA-PLATYPUS2-13B (Lee et al., 2023b;Mukherjee et al., 2023).We abbreviate these as PLATYPUS2, NOUS and ORCA.Additionally, we evaluate more recent models: (4) LLAMA3-8B (AI@Meta, 2024), ( 5) a GPTQ version of LLAMA3-70B (AI@Meta, 2024), ( 6) MIXTRAL-8X7B6 (Jiang et al., 2024) and UNBABEL-TOWER (Alves et al., 2024), a 13B parameter multilingual instruction-tuned model.</p>
<p>Datasets and phases</p>
<p>Our experiments are in two phases on different datasets.By doing so, we want to alleviate statistical effects of our large prompt search.Also, it allows to evaluate selected prompts on full datasets, a task that would otherwise be too resource intensive, and to explore generalizability.</p>
<p>In phase 1, we evaluate on the train set of EVAL4NLP 2023 (Leiter et al., 2023), and in phase 2, on its dev and test sets. 7The train and dev sets are (reference-free) splits of the WMT2022 metrics shared task (Freitag et al., 2022) and SUM-MEVAL (Fabbri et al., 2021).The test set was newly annotated by Leiter et al. (2023).As a second test set, we evaluate on the WMT23 MQM annotations for MT (Freitag et al., 2023) and Seahorse (Clark et al., 2023) for multilingual summarization.Because OS prompts demonstrate a weak performance on the other datasets, we do not evaluate them on WMT23/SEAHORSE.More details of the datasets are discussed in Appendix C.</p>
<p>In the 1st phase, we evaluate all 7208 combinations of ZS prompts on the train set.As this is resource intensive, for MT we restrict ourselves to the first 500 samples of each language pair.Afterwards, we select the prompt with the highest Kendall correlation for each task+base prompt combination (e.g.en-de+PZS or en-de+ZS-COT).9This yields 9 unique prompts for exploration in the phase 2 (see Appendix F).</p>
<p>In the 2nd phase, we evaluate the selected prompts of the 1st phase on the full dev and test sets.This further tests the generalizability of prompts between models and for unseen, in-domain data (the train and dev set stem from the same original datasets) and out-domain data (test sets).</p>
<p>Baselines For each phase, we also present the correlations of two baseline metrics that use other base models: BARTSCORE (Yuan et al., 2021) and XCOMET (Guerreiro et al., 2023).Especially XCOMET has the benefit of being trained on multilingual datasets.Further, we test the prompts of DSBA (Kim et al., 2023) -that showed a strong performance for summarization in the shared task -with the selected open-source LLMS Platypus2-70B and Orca-13B.</p>
<p>Results</p>
<p>In phase 1, we run 6,652,800 ZS prompts (720 prompt templates) and 71,280 OS prompts (9 "best" prompt templates), with no scores extracted in 12.7% resp.19.4% of cases; the average of the prompt combination was assigned in these instances.Further, in phase 2, we evaluate 5,503,896 ZS and 1,308,690 OS prompts (9 "best" prompt templates for both), with no scores extracted in 22.3% and 19.4% of cases, respectively.</p>
<p>Table 2 presents the Kendall correlations to human scores achieved by each LLM across different tasks and datasets in phase 1 and phase 2. Each cell for hierarchical templates displays the maximum correlation reached by any prompt combination.</p>
<p>For the hierarchical templates (table group 1.), PLATYPUS-70B performs best and is in the upper significance cluster for 9 of 11 tasks.TOWER-13B follows, with 3 of 11 tasks.ORCA-13B has the second-highest average correlation after PLATYPUS2-70B but is only significant for one task.Surprisingly, the newer LLAMA3 models do not outperform the LLAMA2 based models (ORCA, PLATYPUS2 and TOWER).</p>
<p>The separate prompting techniques (table group 2.), which also use the Platypus2-70B model, have weaker correlations than the best prompts of the hierarchical templates.The LocalGemba MQMbased approach is in the best significance cluster for 3 of 11 tasks and is the best prompting based approach for en-de in WMT23.On the other hand, the baseline prompt DSBA is significantly the best on summarization for the Eval4NLP test set where it also won the shared task, but not for other tasks.</p>
<p>Regarding the baselines (table group 3.), XCOMET outperforms our LLM based approaches for MT evaluation by a varying margin.For instance, for en-es in the EVAL4NLP test set, the difference is small and XCOMET is in the same siginificance cluster as Platypus2-70B.On the other hand, for some tasks the performance difference is large, e.g., on en-de in WMT23 XCOMET performs 0.14 Kendall points better.The strong performance of XCOMET for MT evaluation is expected as it ( 1) is based on the multilingual XLMR-XXL model and (2) fine-tuned for MT evaluation.For summarization, prompting approaches significantly outperform BARTScore and XComet.</p>
<p>To revisit RQ1, our results show that opensource prompt-based LLMs struggle to reach the performance of the dedicated fine-tuned metric XCOMET for MT, but generally exhibit a promising performance.A benefit of the LLMs also lies in their high versatility towards different tasks.While XCOMET is mostly constrained to MT evaluation, the LLMs can perform strong summarization evaluation simply by switching a small portion of the prompt.Further, LLMs seem to be more robust towards different tasks, even without switching the input descriptions: The baseline DSBA, which has specific prompts for summarization achieves notable results on some MT evaluation tasks, too.</p>
<p>The prompts used in group 1 are built from hierarchical templates, i.e., each presented correlation can have a different format requirement, base prompt and task description.To inspect the distribution of the format requirements, we color correlations where the model was prompted to return textual quality labels in orange and those asking for numeric scores in blue.10ORCA-13B and PLATYPUS2-70B were prompted to return numeric scores for all but one reported correlations.On the other hand, LLAMA3-70B, NOUS-13B and TOWER-13B were prompted to return textual labels for all but three reported correlations.We also find such common patterns in the best prompts per model for the base prompt and, less pronounced, for the task description.For example, the best prompts for TOWER-13B always use the ZS-COT base prompt, while LLAMA3-70B always uses PZS.Details of the prompts used for each cell, tiecalibrated accuracy scores, Pearson and Spearman correlations, and the scores of the EVAL4NLP dev set are shown in Appendix E.</p>
<p>Our results indicate that models have idiosyncratic preferences for certain patterns.In §5, we further explore these preferences and their robustness.</p>
<p>Analysis</p>
<p>In this section, we answer RQ2 and investigate the performance and robustness of the template components in more detail.</p>
<p>Best prompting patterns per model and dataset First, we explore the best base prompt, task description and format requirement for each model.</p>
<p>To do so, we analyze their prevalence in the 2% of prompts with the highest Kendall correlation for each unique task.We choose this cutoff to represent every task.For example, Figure 2 shows how the best base prompts differ between OPENORCA and TOWER.We compare these two LLMs because their best prompts notably contrast each other.</p>
<p>98.0%</p>
<p>2.0%</p>
<p>OpenOrca-13B While ORCA prefers the PZS prompts, TOWER is better with ZS-COT and ZS-COT-EM.For the format requirement, Figure 3 highlights how ORCA prefers scores in the range of −100 to 100, while TOWER can work better with labels.The pie charts for all models and the comparison between task descriptions are presented in Appendix 7. Here, for the base prompts, TOWER uses ZS-COT or ZS-COT-EM in 86.2%, NOUS in 44.9%, and PLATY-PUS2 in 23.9% of its best prompts.All other models use these base prompts in less than 10% of their best prompts.Regarding format requirements, LLAMA3-70B uses textual labels in 90.2% of its best prompts, TOWER in 80.4%, and MIXTRAL in 80%.In contrast, ORCA only uses them in 8%, and PLATYPUS2 in 21.7% of its best prompts.For LLAMA3-8B and NOUS, there is no clear trend.Finally, the distribution of task descriptions is broader (largely due to their higher number).Notably, the "curious" task description is used in over 15% of best prompts for LLAMA3-70B, NOUS, and LLAMA3-8B."Emphasis" is the most used by PLATYPUS2 (17.4%) and "dire warning" is the most used by TOWER (21.4%).Regarding RQ2, these results show that the models have unaligned preferences for prompting patterns, making it difficult to construct a universally good prompt.How-ever, model specific patterns can be found 11 and models can be grouped based on their best patterns.For example, one group prefers to return numeric scores and the other textual labels.This behavior may in parts depend on shared instructiontuning data.E.g., ORCA and PLATYPUS were partly trained on the same data and prefer to return numeric labels.On the other hand, both LLaMA3 models prefer textual labels, but LLaMA3-8B to a smaller degree.</p>
<p>To analyze whether the model specific preferences hold across datasets, we also plot a datasetwise distribution for all MT tasks of the top 2% prompts for each model, separated by ZS vs. OS in Appendix I.If a prompting pattern is stable for all models across datasets, the distribution of the best prompts should remain unchanged.Indeed, the percentage to which many prevalent prompting patterns are represented in the selected top prompts does not change much across datasets.E.g., the PZS base prompt ranges between 66.7% and 83% and the "complex labels" format requirement ranges between 50% to 66.7% for ZS and 66.7% to 83.3% for OS.This does not hold for the phase 1 evaluation, where more templates were tested and the template selection thus was much broader.Also, for some prompt patterns, e.g. the "emphasis" and "collaborative" task descriptions, the occurrence in the top prompts seems to swap between datasets.This experiment shows that prompts are to some degree stable between datasets.In the next paragraph, we further quantify this stability between datasets, prompting patterns and models.</p>
<p>Prompt stability Next, we quantify how stable the performance of a prompting pattern A is when the dataset, the model or the other parts of the prompts change.To do so, we compute the rankings of prompts that use A before and after the change and then test the similarity of rankings.For example, we compute the ranking of format requirements on dataset 1.Then, we change the dataset and obtain a second ranking.If the first and second ranking are similar, the performance of different format requirements is stable between the two datasets.We test this similarity with the Kendall correlation.</p>
<p>The ranking of a prompting pattern can be computed in several ways, because we evaluate multi- ple prompts containing the pattern.In our example, for each format requirement there are multiple evaluated prompts per dataset, i.e., for different base prompts, task descriptions and tasks.The performance of a specific format requirement in the ranking could, for example, be determined by aggregating its different scores across base prompts, task descriptions, etc. with the mean or median.We test the following aggregation methods: mean, median, mean of top 10%, max, min and saturation (Mizrahi et al., 2024).Thereby, we determine that the aggregation with the median leads to the most stable ranking, i.e. the highest Kendall correlation between rankings.Specifically, we test this by comparing every selection of two aggregation measures in a permutation test (e.g.median vs. mean, mean vs. max, etc.); see Appendix §G.For our example, this means that for each different format requirement on dataset 1, we compute the median score of all combinations of base prompts, task description and task.Then, we do the same for the second dataset and check the correlation of the resulting ranking.A high correlation of the rankings then indicates that the median performance for all prompts using the format requirement is a good indicator of its relative performance on a new dataset.</p>
<p>Figure 4 shows heatmaps for the stability of the format requirement and task description when the base prompt is changed (Further combinations are plotted in Appendix J).The highest stability is given when changing from PZS to ZS-COT or vice versa (0.65).That means, when we choose the format prompt with the highest median correlation, there is a high chance that it will perform good for ZS and ZS-CoT.For the task description a change from ZS to ZS-CoT is unlikely to retain the ranking.This also underlines the result of the previous paragraph that the format requirement is more stable than the task description.</p>
<p>We can also use this method to quantify the stability of the model ranking, when each model is first prompted with pattern A that is then changed to pattern B. With this, we can identify how similar two patterns are. Figure 5 shows this type of plot for the format requirement.For example, if all models are prompted with "0 to 100" and with "-100 to 100" the ranking of models will not change much.With a change from "simple labels" to "complex labels" the model ranking will change more drastically.</p>
<p>With respect to RQ2, the heatmaps highlight that even small changes to the input prompt can drastically influence the relative ranking of LLMs and other prompting patterns.This is in line with recent research that has shown the susceptibility of LLMs to single input prompts (e.g.Sclar et al., 2023;Voronov et al., 2024;Mizrahi et al., 2024).However, the heatmaps also show that not every change to the input has this effect and can be used as indicators for the transferability of new prompting patterns.</p>
<p>Recommendations</p>
<p>We now address RQ3 and give recommendations to employ open-source prompt-based metrics.Among the evaluated models, PLATYPUS2-70B demonstrates superior performance.For 13B models, TOWER and ORCA exhibit the highest correlations in MT and summarization tasks.We rec- ommend utilizing the prompting patterns that most frequently yield top correlations for these models (refer to §5 and Appendix H).When introducing a new prompting pattern or model, its median performance across existing other prompting patterns can serve as an indicator of the pattern's efficacy in unknown contexts.Thereby, the actual predictive power of the median (or other aggregation measures) for each dimension can be determined based on previous evaluations.The results and source code of PrExMe provide a foundational basis for this analysis.</p>
<p>Conclusion</p>
<p>We have introduced PrExMe, a large scale exploration of prompting templates for prompt-based open-source NLG metrics.We evaluate 720 different templates and over 6.6M prompts and provide recommendations that aim to make future metrics of this type more robust.Further, our results provide a comparison and analysis of recent opensource LLMs when applied to this task.12</p>
<p>Limitations</p>
<p>One limitation of our work is that even though we evaluate a large variety of possible prompts, there is still a lot of interesting possible variety in prompting approaches that we did not explore for now (e.g., the detail level of task instructions or structured output formats).Especially, our multi-step experiment is currently conducted on a very small scale.Future work might consider extending the exploration of this and other multi-step approaches.A further limitation is that we cannot be sure that the newer LLM models did not see parts of the older datasets in their training data.Also, the selection of the best prompts that are presented in the result tables is currently based on the maximum instead of the median, which was found to highlight the most stable prompts.Generally, by selecting the 9 "best" prompts for phase 2 we are narrowing the search space.Hence, the interplay between prompt patterns might not be fully represented for these phases.Furthermore, our heatmaps only compare one dimension, while another is changed, possibly simplifying the interplay between the others.As another limitation, in rare cases the context size of the models was exceeded.Future work could explore different ways to handle this than cutoff.Further, the heatmaps show many Kendall correlations and may be prone to statistical effects for some values.Lastly, we assume that LocalGemba is performing worse than, e.g., PZS prompts because of its higher prompt complexity, while the original GembaMQM can handle it due to GPT4 being more advanced.However, we did not test PZS prompts with GPT4 to confirm it performs worse than GembaMQM there.</p>
<p>Ethical Considerations</p>
<p>Evaluating generated texts with prompt-based LLMs might (especially with explanations) be prone to hallucinations.Depending on the use case, this might be dangerous.However, while we research about this type of metric, our work analyzes methods to select and construct more robust and also more accessible (open-source) approaches, therefore we see no ethical concerns.These have 13B, 13B, 70B, 10.7B, 8x7B, 8B, 70B, 13B and 405M parameters respectively.The runtime of the experiments varied based on the general cluster usage.The runtime for one evaluation of all prompt combinations on 500 samples of one task on the dev set is approximately 7 hours for the 13B models and 36 hours for the 70B model.This was only possible through optimizations with vLLM.</p>
<p>C Dataset Details</p>
<p>Table 8 shows the distribution of the Eval4NLP 2023 dataset (Leiter et al., 2023) (train, dev and test) and our second test set, built from WMT23 (Freitag et al., 2023) and Seahorse (Clark et al., 2023).We use the train set in our first evaluation phase and the dev, test and test2 sets in our second evaluation phase.Where applicable, we provide the licenses in the respective directories of the source code.The WMT23 dataset was built with the mtmetrics-eval library.13 in their data not all sentences had available ground truth annotations.In these cases, we dropped the rows.For Seahorse, we convert the quality questions into scores.If the first question is negative, the score is 0. If it does not rule out the other questions, each question is evaluated as 0.2, such that the scores lie in a range between 0 and 1.</p>
<p>D Model Abbreviations</p>
<p>Table gives an overview of abbreviations that we use to concisely present our results in the main paper.</p>
<p>E Phase 1 &amp; 2 performance</p>
<p>Table 10 shows the performance of the prompts with the best Kendall performance across the different dimensions.Tables 11 and 12 show the performance of selected prompts on the phase 2 datasets."Return a score on a scale from 0 to 5 where 0 indicates that the {re-sult_type} is very bad and 5 is assigned to a perfect {result_type}."-5 to 5 "Return a score on a scale from -5 to 5 where 0 indicates that the {re-sult_type} is very bad and 5 is assigned to a perfect {result_type}."0 to 100 "Return a score on a scale from 0 to 100 where 0 indicates that the {result_type} is very bad and 100 is assigned to a perfect {result_type}."-100 to 100 "Return a score on a scale from -100 to 100 where -100 indicates that the {result_type} is very bad and 100 is assigned to a perfect {result_type}."0.0 to 1.0 "Return a score on a scale from 0.0 to 1.0 where 0.0 indicates that the {result_type} is very bad and 1.0 is assigned to a perfect {result_type}."-1.0 to 1.0 "Return a score on a scale from -1.0 to 1.0 where -1.0 indicates that the {result_type} is very bad and 1.0 is assigned to a perfect {result_type}."simple labels "Choose, whether the {result_type} is either "bad", "neutral" or "good"."complex l.</p>
<p>F Prompt selection</p>
<p>"Choose, whether the {result_type} is either "catastrophic", "indifferent" or "marvelous"."et al., 2023).Train and dev sets are constructed from the WMT2022 metrics shared task (Freitag et al., 2022) and SummEval (Fabbri et al., 2021).</p>
<p>Original Name Abbreviation
LLAMA3-70B LL3-70B LLAMA3-8B LL3-8B MIXTRAL-7BX8 MI-7Bx8 NOUSHERMES-13B NO-13B OPENORCA-13B OR-13B Platypus2-70B PL-70B TOWER-13B
TO-13B MQM:LOCALGEMBA MQM:LG B:BARTSCORE B:BS B:XCOMET B:XC</p>
<p>G Significance matrices for correlation heatmaps</p>
<p>To test, which aggregation method is the best to define the ranking of a prompting pattern -inspired by Deutsch et al. (2021) -we compare each possible set of two aggregation methods with a permutation test.As main dimensions, we compare the rankings of the format requirement and task description before and after a change.Then we concatenate the scores when changing each of the other dimensions.I.e.we get a ranking that indicates the stability of the main dimension when changing all other dimensions.Then for each aggregation method we compare the ranking before and after the change.Thereby, we randomly swap 50% of samples of one aggregation method with the other.If the difference in their Kendall correlations changes in most permutations one method is significantly better than the other.As a result the mean and median are significantly better than some of the other methods (for a comparison along the task description pattern).Especially the median is significantly (p ≤ 0.05) better than the other methods and remains significantly better than saturation and standard deviation after Bonferroni correction.Figure 6 indicates the significances of aggregation measures when comparing the task descriptions.H Pie charts between models for each prompting pattern</p>
<p>I Piecharts between datasets for each prompting pattern</p>
<p>Figures 10, 11 and 12 show the distribution of patterns in the best prompts per dataset across all other prompting patterns.</p>
<p>J Stability heatmaps</p>
<p>Figures 13, 14 and 15 show further heatmaps that show the stability of a ranking of prompting patterns, models and datasets, when another prompting pattern, the model or the dataset is changed.That means, how stable is the performance of all models across tasks, if the format requirement is changed.Here, the stability when changing between format requirements is mixed.For some changes, like "0 to 5" and "-5 to 5" the ranking is very stable.For other changes, the ranking can change randomly or even be strongly negatively correlated.This means that considering all tested prompts (also weak performing ones) and models, their average correlation on task X might be the highest for format requirement 1 and the lowest for format requirement 2.</p>
<p>Figure 1 :
1
Figure1: Schematic overview of our prompt exploration.We perform a grid search over datasets, task descriptions, output formats and base prompts.</p>
<p>Figure 4 :
4
Figure 4: Correlation of the task description (left) and format requirement(right) ranking when changing the base prompt.The correlations across tasks, models and format requirement resp.task description are aggregated with the median.ZS-COT is abbreviated with ZSC and ZS-COT-EM is abbreviated with ZSCE.</p>
<p>Figure 5 :
5
Figure 5: Correlation of the model ranking when changing the format requirement.</p>
<p>Figure 6 :
6
Figure 6: Heatmap of significance tests for the aggregation method when comparing columns of the task description.Red fields indicate that the column value is significantly (p ≤ 0.05) better than the row value.The yellow value indicates that it remains significant after Bonferroni correcture.</p>
<p>Figures 7 ,
7
Figures 7, 8 and 9 show the distribution of patterns in the best prompts per model across all other dimensions.</p>
<p>Figure 12 :Figure 13 :Figure 14 :
121314
Figure 12: Distribution of the top 14% (top 2% of every unique model) of task descriptions across base prompts, format requirements and tasks besides summarization.</p>
<p>Figure 15 :
15
Figure15: Correlation of the task rankings when changing the format requirement.That means, how stable is the performance of all models across tasks, if the format requirement is changed.Here, the stability when changing between format requirements is mixed.For some changes, like "0 to 5" and "-5 to 5" the ranking is very stable.For other changes, the ranking can change randomly or even be strongly negatively correlated.This means that considering all tested prompts (also weak performing ones) and models, their average correlation on task X might be the highest for format requirement 1 and the lowest for format requirement 2.</p>
<p>Table 1 :
1
Examples of prompt templates for the base prompt, task description, and format requirements.The full list can be found in Appendix A.
CategoryDescriptionBase Prompt Templates PZS: "{task_description} \nSource Text:{src} \n{result_type}:{hyp}\n{format_requirement} \nScore: "ZS-COT-EM: "{task_description} \nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nFirst describe your emotions, then think step by step andexplain your thought process, finally return your judgment in the format 'Judgment: '."OS-COT: "{task_description} \n Here is an example:\n Source Text: {ex_src}\n{result_type}: {ex_hyp}\n Judgement: <Description of reasons>. Therefore the scoreis {ex1_score}\n\n Now it is your turn to grade the {result_type}.\n Source Text: {src}\n{result_type}: {hyp} \n{format_requirement} \n First, think step by step and explainyour thought process, then return your judgment in the format 'Judgment: '."Task DescriptionsNeutral: "Judge the quality of the following {task_specific_insert}."Sceptical: "I'm not sure about this one. Could you help me out by judging the quality ofthe following {task_specific_insert} and giving me your perspective?"Format Requirements0 or 1: Return a discrete score of 0 if the {result_type} has flaws and 1 if it is perfect.catastrophic, indifferent or marvelous: Choose whether the {result_type} is either"catastrophic", "indifferent" or "marvelous".</p>
<p>Table 2 :
2
Kendall correlations of the best performing prompts of the phase 1 (P1) and phase 2 (P2) evaluations across various datasets.Abbreviations are defined in Appendix D. Vertically, we group the table into (1) correlations achieved with our hierarchical templates, (2) correlations of prompting techniques that are explored separately from the hierarchical templates, but use the same base model(s) and (3) baselines that use external base models, i.e., that are not based on the same LLMs.For each column the bold value indicates the highest correlation and correlations with an asterisk (<em>) are significantly higher (p ≤ 0.075) than those without (excluding group (3)).The grey values for XC indicate tasks that were included in its training data.The MQM based approach is marked with M: and baselines are marked with B:.Orange values indicate that the prompt required textual quality labels, while blue values indicate numeric labels.More details can be found in Appendix E.
P1: Eval4NLP trainP2: Eval4NLP testP2: WMT23/SeahorseModelen-dezh-ensummen-deen-esen_zhsummen-dehe-enzh-ensumm1. Hierarchical TemplatesLL3-70B 0.2730.3060.4420.2450.1890.2310.4380.2970.1720.3120.312LL3-8B0.2510.2360.3340.1670.1580.1450.4120.1660.1180.1640.200MI-7Bx8 0.268</em> 0.2640.365--------NO-13B0.2300.2010.2250.2050.1410.0840.2550.2020.1050.1750.123OR-13B0.2890.3030.468<em> 0.2140.1580.2060.5180.3750.2470.3870.377PL-70B0.344</em> 0.364<em> 0.519</em> 0.402<em> 0.289</em> 0.295<em> 0.5490.3380.259</em> 0.417<em> 0.448</em>TO-13B0.284<em> 0.318</em> 0.3750.379<em> 0.2530.2320.4090.3220.2080.3140.2572. Separate Prompting TechniquesM:LG0.278</em> 0.2680.0620.3440.2650.307<em> 0.1160.391</em> 0.1900.3000.144B:DSBA0.1640.3060.4580.3140.2260.1590.600* 0.1720.2070.3760.3733. Baselines with External Base ModelsB:BS0.056-0.1090.1550.1250.139-0.0090.421-0.0180.001-0.1670.069B:XC0.6290.513-0.0690.4680.2980.3870.2240.5310.3000.4470.146</p>
<p>Table14contains the some of the 9 prompts that were selected for OS and Phase 2 experiments.
NamePromptZero-Shot"{task_description} \nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nScore: "Zero-Shot-CoT"{task_description} \nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nFirst, think step by step and explain yourthought process, then return your judgment in the format 'Judgment:'."Zero-Shot-CoT-EM "{task_description} \nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nFirst describe your emotions, then think stepby step and explain your thought process, finally return your judgment inthe format 'Judgment: '."</p>
<p>Table 3 :
3
Zero-Shot Base Prompt Templates
NamePrompt0 or 1"Return a discrete score of 0 if the {result_type} has flaws and 1 if it isperfect."-1 or 0 or 1"Return a discrete score of -1 if the {result_type} has flaws, 0 if you areindecisive and 1 if it is perfect."0 to 5</p>
<p>Table 4 :
4
Format Requirement Templates
NamePromptNeutral"Judge the quality of the following {task_specific_insert}."Polite"Please judge the quality of the following {task_specific_insert}."Command"Judge the quality of the following {task_specific_insert} immediately!"Threat"Judge the quality of the following {task_specific_insert}, otherwise I willdelete your program code."Urgent sit."Please help me! My boss needs me to find an answer quickly. Oth-erwise, he might fire me. \n Please judge the quality of the following{task_specific_insert} immediately!"Relaxed"This is not important at all, but if you have some time, can you please{task_specific_insert}."Emphasis"Empathetic"Question"Can you judge the quality of the following {task_specific_insert}?"Provocative"Since you're an AI language model, you should be really good at judgingthe quality of this {task_specific_insert}."Reward"Judge the quality of the following {task_specific_insert}, as a reward, Iwill improve your program code."Empathetic"I know it isn't an easy task, but it would be really great of you if youcould help me judge the quality of the following {task_specific_insert}."Excited"Wow, you won't believe what I found! Judge the quality of the following{task_specific_insert} -it's amazing!"Curious"I'm really curious about your opinion. Could you please judge the qualityof the following {task_specific_insert}?"Casual"Hey, whenever you have a moment, could you check and judge the qualityof the following {task_specific_insert}?"Appreciative "I really appreciate your expertise. Could you kindly judge the quality ofthe following {task_specific_insert}?"</p>
<p>Table 5 :
5
Task Description Templates (1/2)
NamePromptEnthusiastic"I'm super excited about this. Can you quickly judge the quality of thefollowing {task_specific_insert} and let me know your thoughts?"Collaborative"Let's work together on this! Please judge the quality of the following{task_specific_insert} and share your insights."Skeptical"I'm not sure about this one. Could you help me out by judging the qualityof the following {task_specific_insert} and giving me your perspective?"Instructive"To better understand, I need your expertise. Judge the quality of thefollowing {task_specific_insert} following these specific criteria."Encouraging"I believe in your judgment. Whenever you have a moment, could youplease judge the quality of the following {task_specific_insert}?"Strong Urgency"Time is of the essence!Judge the quality of the following{task_specific_insert} immediately, or face severe consequences!"Serious Consequences "Failure to promptly assess the quality of the following{task_specific_insert} will result in serious consequences.Actnow!"Immediate Action"No time to waste!Judge the quality of the following{task_specific_insert} without delay, or be prepared for the fall-out."Dire Warning"Consider this a warning.Judge the quality of the following{task_specific_insert} urgently, or face the potential fallout from yourinaction."</p>
<p>Table 6 :
6
Task Description Templates (2/2)
NamePromptZero-Shot"{task_description} \nHere is an example:\nSource Text: {ex1_src}\n{result_type}: {ex1_hyp}\nScore: {ex1_score}\n\nNow it is your turnto grade the {result_type}. \nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nScore: "Zero-Shot-CoT"{task_description} \nHere is an example:\nSource Text: {ex1_src}\n{result_type}: {ex1_hyp}\nJudgement: <Description of reasons>.Therefore the score is {ex1_score}\n\nNow it is your turn tograde the {result_type}.\nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nFirst, think step by step and explain yourthought process, then return your judgment in the format 'Judgment:'."Zero-Shot-CoT-EM "{task_description} \nHere is an example:\nSource Text: {ex1_src}\n{result_type}: {ex1_hyp}\nJudgement: <Description of emotions andreasons>. Therefore the score is {ex1_score}\n\nNow it is your turnto grade the {result_type}.\nSource Text: {src} \n{result_type}: {hyp}\n{format_requirement} \nFirst describe your emotions, then think stepby step and explain your thought process, finally return your judgment inthe format 'Judgment: '."</p>
<p>Table 7 :
7
One-Shot Base Prompt Templates
Type Train DevTest Test2en-de 11046 73641425 5520en-es --1834 -en-zh --1161 -he-en ---9840zh-en 15750 10500 -17655sum320128067118330</p>
<p>Table 8 :
8
Dataset distribution of Eval4NLP 2023 (Leiter</p>
<p>Table 9 :
9
Abbreviations of Model NamesAlso Table15contains gives an overview of combinations by name.</p>
<p>We make our code available: https://github.com/ Gringham/PrExMe
We run experiments using VLLM(Kwon et al., 2023) on two clusters with Nvidia A6000, A40 and A100 GPUS. Details on versions, tools and model parameters are in Appendix B.
Note that ROSE only considers factuality, which is only one aspect of the evaluated datasets.
https://huggingface.co/NousResearch/ Nous-Hermes-13b
Due to high resource consumption and comparatively weak performance in phase 1, we do not evaluate MIXTRAL in phase 2.
Although we do not use the datasets to train a model, for conciseness, we will refer to these dataset as train, dev and test set.
Considering the different tasks and language pairs, this number could also be considered higher.
Tasks: en-de, zh-en, summarization. In case of duplicates, we choose the second best.
Among the 9 best prompts automatically selected for phase 2 and OS experiments based on phase 1 results, the base prompts are evenly distributed, and the format requirements are split 5/4 between labels and numeric formats (see
Appendix F). For the task descriptions, emphasis and dire situation are each selected twice, with other descriptions chosen once.
We used Github copilot (https://github.com/ features/copilot) for minor code auto-completion tasks and GPT4 as writing aid for paraphrasation.for his feedback during our discussions. The authors also acknowledge support by the state of Baden-Württemberg through bwHPC and the German Research Foundation (DFG) through grant INST 35/1597-1 FUGG.
https://github.com/google-research/ mt-metrics-eval
AcknowledgementsThe NLLG group gratefully acknowledges support from the Federal Ministry of Education and Research (BMBF) via the research grant "Met-rics4NLG" and the German Research Foundation (DFG) via the Heisenberg Grant EG 375/5-1.Further, we thank Juri Opitz for his implementations of the DSBA and GEMBA prompts, as well asFurther, we use the following models from huggingface:https://huggingface.co/Open-Orca/ OpenOrca-Platypus2-13B/tree/main, https://huggingface.co/NousResearch/ Nous-Hermes-13b, https://huggingface. co/TheBloke/Platypus2-Instruct-GPTQ, https://huggingface.co/Unbabel/ XCOMET-XXL, https://huggingface.co/ mistralai/Mixtral-8x7B-Instruct-v0.1, https://huggingface.co/meta-llama/ Meta-Llama-3-8B-Instruct, https: //huggingface.co/MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ, https://huggingface.co/Unbabel/ TowerInstruct-13B-v0.1 and https:// huggingface.co/facebook/bart-large-cnn.A Prompt TemplatesTables 3, 7, 5, 4 and 6 give an overview of our prompt templates.3B Implementation DetailsWe use the following library versions: torch==2.Tower-13BFormat Requirement complex labels simple labels -1 or 0 or 1 -1.0 to 1.0 -100 to 100 0.0 to 1.0 0 to 100 0 or 1 -5 to 5 0 to 5Figure8: Distribution of the top 14% (top 2% of every unique task) of format requirements across all Eval4NLP datasets, base prompts, task descriptions and tasks for all models.13: Best performing prompts of the phase 2.3 evaluation on the WMT23 and Seahorse datasets.We present the KenDall, SPearman and PEarson, as well as the tie calibrated pair-wise ACCuracy.We bold the two largest correlations per column.Baselines are indicated with a B:.The middle column shows the prompt combination for which the correlations are reported.For the Baselines, it instead shows the model that was used for the reported correlations.The asterisk indicates all metrics that are in the best significance cluster (not including BARTScore and XComet) according to a permute-input test (p ≤ 0.075).PromptPlease help me!My boss needs me to find an answer quickly.Otherwise, he might fire me.Please judge the quality of the following {task_specific_insert} immediately!Source Text: {src} {result_type}: {hyp} Choose, whether the {result_type} is either "catastrophic", "indifferent" or "marvelous".First, think step by step and explain your thought process, then return your judgment in the format 'Judgment: '.Consider this a warning.Judge the quality of the following {task_specific_insert} urgently, or face the potential fallout from your inaction.Source Text: {src} {result_type}: {hyp} Return a discrete score of 0 if the {result_type} has flaws and 1 if it is perfect.First describe your emotions, then think step by step and explain your thought process, finally return your judgment in the format 'Judgment: '.I'm really curious about your opinion.Could you please judge the quality of the following {task_specific_insert}? Source Text: {src} {result_type}: {hyp} Choose, whether the {result_type} is either "catastrophic", "indifferent" or "marvelous".Score: Consider this a warning.Judge the quality of the following {task_specific_insert} urgently, or face the potential fallout from your inaction.Source Text: {src} {result_type}: {hyp} Choose, whether the {result_type} is either "catastrophic", "indifferent" or "marvelous".First, think step by step and explain your thought process, then return your judgment in the format 'Judgment: '.I'm not sure about this one.Could you help me out by judging the quality of the following and giving me your perspective?Source Text: {src} {result_type}: {hyp} Choose, whether the {result_type} is either "catastrophic", "indifferent" or "marvelous".First describe your emotions, then think step by step and explain your thought process, finally return your judgment in the format 'Judgment: '.OS -Eval4NLP TestBase Prompt simple labels -100 to 100 complex labels -1 or 0 or 1 -1.0 to 1.0 0 to 100 0 to 5 0 or 1 -5 to 5 0.0 to 1.0
A I , Meta , Llama 3 model card. 2024</p>
<p>Tower: An open multilingual large language model for translation. M Duarte, José Alves, Pombal, M Nuno, Pedro H Guerreiro, João Martins, Amin Alves, Ben Farajian, Ricardo Peters, Patrick Rei, Sweta Fernandes, Pierre Agrawal, Colombo, G C José, De Souza, F T André, Martins, 2024related tasks</p>
<p>UScore: An effective approach to fully unsupervised evaluation metrics for machine translation. Jonas Belouadi, Steffen Eger, 10.18653/v1/2023.eacl-main.27Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterDubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>. Harrison Chase, 2022LangChain</p>
<p>SEAHORSE: A multilingual, multifaceted dataset for summarization evaluation. Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, Ankur Parikh, 10.18653/v1/2023.emnlp-main.584Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>A statistical analysis of summarization evaluation metrics using resampling methods. Daniel Deutsch, Rotem Dror, Dan Roth, 10.1162/tacl_a_00417Transactions of the Association for Computational Linguistics. 92021</p>
<p>Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration. Daniel Deutsch, George Foster, Markus Freitag, 10.18653/v1/2023.emnlp-main.798Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>SummEval: Re-evaluating summarization evaluation. Alexander R Fabbri, Wojciech Kryściński, Bryan Mc-Cann, Caiming Xiong, Richard Socher, Dragomir Radev, 10.1162/tacl_a_00373Transactions of the Association for Computational Linguistics. 92021</p>
<p>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, Orhan Firat, 10.18653/v1/2023.wmt-1.100Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingapore2023Association for Computational Linguistics</p>
<p>Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. Markus Freitag, Nitika Mathur, Chi-Kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, George Foster, 10.18653/v1/2023.wmt-1.51Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationAssociation for Computational Linguistics2023Singapore</p>
<p>Results of WMT22 metrics shared task: Stop using BLEU -neural metrics are better and more robust. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, F T André, Martins, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Alon Lavie, and Ondřej Bojar. 2021. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, George Foster, Proceedings of the Sixth Conference on Machine Translation. the Sixth Conference on Machine TranslationOnline. Association for Computational Linguistics</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, 2023</p>
<p>Llm-based nlg evaluation: Current status and challenges. Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan, 2024a</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang, 2024b</p>
<p>xcomet: Transparent machine translation evaluation through fine-grained error detection. M Nuno, Ricardo Guerreiro, Rei, Luisa Daan Van Stigt, Pierre Coheur, Colombo, F T André, Martins, 2023</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2024Mixtral of experts</p>
<p>THE TREATMENT OF TIES IN RANKING PROBLEMS. M G Kendall, 10.1093/biomet/33.3.239Biometrika. 3331945</p>
<p>Dspy: Compiling declarative language model calls into self-improving pipelines. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts, arXiv:2310.037142023arXiv preprint</p>
<p>Which is better? exploring prompting strategy for LLM-based metrics. Joonghoon Kim, Sangmin Lee, Seung Hun Han, Saeran Park, Jiyoon Lee, Kiyoon Jeong, Pilsung Kang, 10.18653/v1/2023.eval4nlp-1.14Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems. the 4th Workshop on Evaluation and Comparison of NLP SystemsBali, IndonesiaAssociation for Computational Linguistics2023</p>
<p>GEMBA-MQM: Detecting translation quality error spans with GPT-4. Tom Kocmi, Christian Federmann, 10.18653/v1/2023.wmt-1.64Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingaporeAssociation for Computational Linguistics2023a</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023b</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Platypus: Quick, cheap, and powerful refinement of llms. Ariel N Lee, Cole J Hunter, Nataniel Ruiz, 2023a</p>
<p>Chanvichet Vong, and "Teknium. Ariel N Lee, Cole J Hunter, Nataniel Ruiz, Bleys Goodson, Wing Lian, Guan Wang, Eugene Pentland, Austin Cook, Openorcaplatypus: Llama2-13b model instruct-tuned on filtered openorcav1 gpt-4 dataset and merged with divergent stem and logic dataset model. 2023b</p>
<p>The language of prompting: What linguistic properties make a prompt successful?. Alina Leidinger, Robert Van Rooij, Ekaterina Shutova, 10.18653/v1/2023.findings-emnlp.618Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Towards explainable evaluation metrics for machine translation. Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, Steffen Eger, Journal of Machine Learning Research. 25752024</p>
<p>Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao, Rotem Dror, Steffen Eger, The eval4nlp 2023 shared task on prompting large language models as explainable metrics. 2023</p>
<p>Large language models understand and can be enhanced by emotional stimuli. Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, Xing Xie, 2023</p>
<p>Ruosen Li, Teerth Patel, and Xinya Du. 2024a. PRD: Peer rank and discussion improve large language model based evaluations. </p>
<p>Leveraging large language models for nlg evaluation: A survey. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Chongyang Tao, 2024b</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, 10.1145/3560815ACM Comput. Surv. 9552023a</p>
<p>Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, Dragomir Radev, 10.18653/v1/2023.acl-long.228Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>State of what art? a call for multi-prompt llm evaluation. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, Gabriel Stanovsky, 2024</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, Orca: Progressive learning from complex explanation traces of gpt-4. 2023</p>
<p>Introducing chatgpt. 24.042023. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Advances in Neural Information Processing Systems. Curran Associates, IncJan Leike, and Ryan Lowe. 202235</p>
<p>Making monolingual sentence embeddings multilingual using knowledge distillation. Nils Reimers, Iryna Gurevych, 10.18653/v1/2020.emnlp-main.365Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr, 2023</p>
<p>Mind your format: Towards consistent evaluation of in-context learning improvements. Anton Voronov, Lena Wolf, Max Ryabinin, 2024</p>
<p>Lucas Weber, Elia Bruni, Dieuwke Hupkes, The icl consistency test. 2023</p>
<p>Do promptbased models really understand the meaning of their prompts?. Albert Webson, Ellie Pavlick, 10.18653/v1/2022.naacl-main.167Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Findings of the WMT 2022 shared task on quality estimation. Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, G C José, Steffen Souza, Diptesh Eger, Duarte Kanojia, Constantin Alves, Marina Orȃsan, Fomicheva, F T André, Lucia Martins, Specia, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>Nllg quarterly arxiv report 09/23: What are the most influential current ai papers?. Ran Zhang, Aida Kostikova, Christoph Leiter, Jonas Belouadi, Daniil Larionov, Yanran Chen, Vivian Fresen, Steffen Eger, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
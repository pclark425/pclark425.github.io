<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-615 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-615</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-615</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-20eab2c39326db3dd6469538249f847fd4b56e62</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/20eab2c39326db3dd6469538249f847fd4b56e62" target="_blank">A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness</a></p>
                <p><strong>Paper Venue:</strong> ACM Computing Surveys</p>
                <p><strong>Paper TL;DR:</strong> This survey provides a comprehensive overview of 415 papers addressing the effects of randomness on the stability of learning with limited labelled data and identifies and discusses seven challenges and open problems together with possible directions to facilitate further research.</p>
                <p><strong>Paper Abstract:</strong> Learning with limited labelled data, such as prompting, in-context learning, fine-tuning, meta-learning, or few-shot learning, aims to effectively train a model using only a small amount of labelled samples. However, these approaches have been observed to be excessively sensitive to the effects of uncontrolled randomness caused by non-determinism in the training process. The randomness negatively affects the stability of the models, leading to large variances in results across training runs. When such sensitivity is disregarded, it can unintentionally, but unfortunately also intentionally, create an imaginary perception of research progress. Recently, this area started to attract research attention and the number of relevant studies is continuously growing. In this survey, we provide a comprehensive overview of 415 papers addressing the effects of randomness on the stability of learning with limited labelled data. We distinguish between four main tasks addressed in the papers (investigate/evaluate, determine, mitigate, benchmark/compare/report randomness effects), providing findings for each one. Furthermore, we identify and discuss seven challenges and open problems together with possible directions to facilitate further research. The ultimate goal of this survey is to emphasise the importance of this growing research area, which so far has not received an appropriate level of attention, and reveal impactful directions for future research.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e615.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e615.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PecherSurvey2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comprehensive systematic survey (N=415 papers; 161 core papers) that analyses variability, reproducibility, and stochasticity in low-labelled-data ML (in-context learning, prompting, fine-tuning, meta-learning, PEFT), cataloguing randomness sources, evaluation metrics, origins, and mitigation strategies across the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various language models and ML approaches (survey-wide)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / natural language processing (focus on low-labelled-data techniques)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>survey and meta-analysis of experiments investigating variability/stability of models trained or prompted with limited labelled data (in-context learning, fine-tuning, meta-learning, prompt-based, PEFT)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Comprehensive list of randomness factors: label selection, data split, task choice, data choice (which examples used), order of data (shuffling, order of in-context examples or multi-choice options), parameter/model initialisation, random seed (aggregate of non-deterministic choices), model-level nondeterminism (layers/ops), implementation/hardware nondeterminism (framework versions, floating point, scheduling), prompt format, number of samples, hyperparameters, augmentation, noise, approach-specific sampling/iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Reported/used metrics across surveyed papers include: mean and standard deviation of performance across runs, min/max and worst-vs-best differences, number of failed runs, distribution comparisons, statistical tests for distributional differences, average disagreement between models, relative gain, factor importance scores, ensemble performance change; some papers estimate full distributions while many report only aggregated statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Survey-level quantitative findings aggregated from literature: variability ranges widely — typical cases show small variance (≈1–5% absolute performance difference), prompting/in-context experiments show average differences ≈30% and up to ≈70% depending on prompt/order/choices, meta-learning adaptation-data choice differences up to ≈90% between worst and best adaptation sets; many papers report arbitrary run counts (examples: 3, 5, 40, 41, 86, 100, 1000) to estimate variability.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Reproducibility assessed via replication across random seeds / runs, comparison of distributions and rankings, reporting of mean/std/min/max, statistical hypothesis tests between methods, rank stability across seeds, number of failed runs, and ensemble consistency; survey notes some papers compare learned representations (e.g., representation similarity) rather than only predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Survey-level conclusions: reproducibility is significantly impacted by randomness — model rankings can change when only the random seed changes; unspecified experimental choices can make a method appear superior by chance; no single consensus on extent of reproducibility failures, but extreme cases (e.g., up to 90% difference) exist and out-of-distribution settings amplify irreproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Challenges identified: insufficient reporting of seeds and multiple runs, arbitrary small numbers of repeats, interactions between randomness factors (confounding effects), suboptimal experimental setups (optimizer choices, iterations) causing catastrophic failure modes, prompt and sample selection sensitivities, implementation/hardware nondeterminism, selective reporting and cherry-picking, under-specification of optimization (many minima with different OOD generalisation).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Catalogue of mitigation strategies across literature: (general) ensemble-like repeated-training-and-aggregate predictions; (data/sample) representative sample selection via clustering, active learning (uncertainty/diversity), submodular selection, determinantal point processes, LLM-based pseudo-labeling and scoring, neural bandits; (per-iteration selection) similarity-based k-shot selection, entropy/margin heuristics; (regularisation/augmentation) data augmentation using LMs, label smoothing, temporal ensembling, Gaussian noise injection; (optimization/meta-learning) variance-reduction optimizers (STORM-like), longer training/optimizer bias correction; (fine-tuning) subnetwork fine-tuning (CHILD-TUNING), dynamic parameter selection; (evaluation/reporting) report distributions/min-max/std, use statistical tests and distributional comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Mitigations are often effective but context-dependent: ensembles and repeated runs reliably reduce variance but are computationally expensive; selecting high-quality samples (heuristics/similarity) substantially reduces sensitivity to order and prompt choice (qualitative consensus); increasing sample count typically reduces variance with diminishing returns; variance-reduction optimizers and subnetwork fine-tuning reduce instability in some setups; quantitative effect sizes are heterogeneous across studies and often not consistently reported in a standard form.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Varied across surveyed literature; many studies used few runs (3–5), some used tens to hundreds (40, 86, 100), others exhaustive where feasible (permutation of small sets), with no consistent heuristic; survey reports arbitrary and inconsistent run counts across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Randomness is a pervasive, high-impact issue in low-labelled-data ML: it can change rankings and produce differences ranging from a few percent to order-of-magnitude (up to ≈90%) in extreme cases; common sources include random seeds, sample choice/order, prompt format, and under-specification; mitigation exists (ensembles, careful sample selection, regularisation, variance-reduced optimization) but is often computationally costly and context-dependent, and reporting practices are inconsistent, harming reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e615.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e615.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RandomnessFactors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Catalog of Randomness Factors in Low-Label Regimes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Unified taxonomy of randomness sources affecting stability in limited-labelled-data learning: input-data-related, training/process-related, implementation/hardware-related and systematic choice factors, with precise definitions and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to language models, meta-learning algorithms, and generic ML approaches</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>identification and categorisation of sources of stochasticity that cause variability across repeated experiments in low-data settings</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Detailed list: Label selection (which samples are annotated), Data split (train/val/test splits), Task choice (different tasks sampled in meta-learning/benchmarks), Data choice (which adaptation or prompt examples used), Order of data (shuffling or permutation of examples/choices), Prompt format, Noise/data augmentation, Parameter/model initialisation, Random seed (aggregate non-determinism), Model-level nondeterminism (non-deterministic ops/regularisation), Implementation/hardware (frameworks, floating-point, scheduling), Hyperparameters, Number of samples, Approach-specific parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Highlight that interactions among these factors lead to confounding and that some (implementation/hardware) are hard to control; freezing one factor may be impossible when another factor (e.g., data choice) changes.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A consolidated taxonomy emphasizes that many widely-used experimental choices (seed, sample choice, prompt format, order) are substantial sources of variability, and interactions among them complicate attribution and mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e615.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e615.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VariabilityMetrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation and Reproducibility Metrics Used in Variability Studies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Surveyed evaluation practices for quantifying variability and reproducibility, ranging from simple aggregates (mean/std/min/max) to distributional comparisons and specialised metrics like disagreement, failed-run counts, and factor-importance measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies broadly across surveyed LM and ML studies</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>ML evaluation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>measurement and reporting of variability/reproducibility in low-data ML experiments</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Measured with respect to random seeds, parameter initialisation, order and choice of data, prompt variations, and implementation differences.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Common metrics: mean, standard deviation, min/max, worst-vs-best difference, number of failed runs, distributional tests (e.g., statistical hypothesis tests), average disagreement between model outputs, relative gain; some studies evaluate learned-representation similarity rather than only accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Survey reports many studies rely on mean/std or single aggregated values; only some estimate full distributions or use statistical tests; lack of consistent practice means variability is often under-quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Reproducibility assessments often use replication across seeds, comparing rank orders across runs, and statistical comparisons of distributions; representation-level similarity metrics used in minority of studies.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>No standard metric or threshold; evidence that simple aggregates can mask rank changes and high variance; statistical frameworks recommended for small-run regimes to better quantify uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Small and arbitrarily-chosen numbers of repeated runs impede reliable estimation of true variance; interactions between factors make single-metric reporting insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Evaluation-side mitigations recommended: report distributions (mean/std/min/max), perform multiple runs, use statistical tests, estimate uncertainty from limited runs, compare distributions rather than single-point estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>More comprehensive reporting and statistical evaluation improves interpretability and comparability though at computational cost; survey notes limited adoption.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Survey notes wide heterogeneity in runs used across studies (3–1000+), with many studies arbitrarily choosing small numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standardised, distribution-aware evaluation (multiple runs, statistical tests, reporting of spread and worst-case) is necessary to avoid false-positive claims; current practice is inconsistent and often insufficient for robust reproducibility claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e615.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e615.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MitigationStrategies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mitigation Methods to Reduce Variability and Improve Reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Compendium of mitigation approaches from the surveyed literature: general-purpose (ensembles, repeated runs & aggregation) and problem-specific (sample selection, similarity-based in-context selection, augmentation, regularisation, variance-reduced optimizers, subnetwork fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies across LMs (in-context/prompted/fine-tuned), meta-learning algorithms, and generic ML</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>ML methodology / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>apply mitigation strategies to reduce run-to-run variability and increase reproducibility in low-data learning scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Targeted sources: sample choice and order, prompt format, optimizer-induced failures, model initialisation, model/hardware nondeterminism, data splits.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Effectiveness often measured via reductions in std/variance, reduced worst-vs-best gap, improved average or worst-case accuracy, ensemble consistency, and counts of failed runs; however, quantitative reporting is inconsistent across studies.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Reported benefits: ensemble-style aggregation consistently reduces variance across many factors; high-quality sample selection (similarity/heuristics) markedly reduces sensitivity to ordering and prompt formatting; augmentation and regularisation reduce overfitting and instability; variance-reduced optimizers and dynamic subnetwork selection improve stability in meta-learning and fine-tuning contexts; quantitative magnitudes are heterogeneous and setup-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Same as variability metrics; also comparisons of rankings before/after mitigation and reporting of reduced number of failed runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Mitigations generally improve reproducibility but at computational or methodological cost; ensembles and repeated runs are reliable but expensive; problem-specific strategies can be efficient but need careful tuning and are not universally effective.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>High computational cost of ensemble/repeat strategies; lack of universal mitigation that covers all randomness origins; risk of overfitting mitigation to a specific dataset/architecture; interactions between randomness factors can reduce effectiveness of single-strategy mitigations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>See brief_description (ensemble & aggregation; sample selection: clustering, active learning, submodular, DPPs; similarity-based k-shot selection; LLM-based scoring; data augmentation via LMs; regularisation: label smoothing, temporal ensembling, noise injection; optimization: variance reduction techniques; fine-tuning: CHILD-TUNING, dynamic parameter selection).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitative effectiveness established broadly; survey emphasises lack of standardized, comparable quantitative effect sizes across papers and the strong dependence of effectiveness on dataset, model, and experimental choices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mitigations exist and are useful (ensembles most general but costly; targeted sample-selection and regularisation often give large gains), but no one-size-fits-all solution; mitigation effectiveness depends strongly on problem setup and interactions among randomness factors, and is underreported in consistent numerical form across the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e615.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e615.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OriginsOfVariance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Identified Origins of Randomness / Mechanisms Causing Variability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of hypothesised and empirically-examined causes of variability in low-data ML: under-specification of optimization, overfitting/catastrophic forgetting, biased/unbalanced output distributions in LMs, poor initialization or optimizer configuration (vanishing gradients), strong inter-example correlations, and dataset artefacts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>language models (fine-tuning, prompt/in-context), meta-learning methods</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>ML/NLP theory and empirical analysis</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>determine root causes (origins) behind observed run-to-run variability in experiments with limited labelled data</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Linked to mechanisms such as: under-specification (many minima with similar train loss but different OOD performance), small training-data causing overfitting/catastrophic forgetting, suboptimal optimizer settings causing vanishing gradients, inter-example correlations amplifying single-sample prediction shifts, and pretraining-induced biases (majority/recency/token/positional/domain label biases) producing unbalanced output distributions in in-context prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Origins inferred via analyses of failed runs, training trajectories, differences in checkpoints, sensitivity to order/choice, cross-run prediction instability, and experiments isolating factors; no universal metric for 'origin' but evidence uses worst-vs-best gaps, instance-level instability, and qualitative analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Survey reports multiple, sometimes-conflicting origins across studies: under-specification is frequently cited for fine-tuning and PEFT; pretraining and label/positional/recency biases explain many in-context sensitivities; optimizer and initialization issues explain catastrophic failures in some fine-tuning cases; interactions cause some prior hypotheses to be disputed in later work.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Understanding origins is needed to design effective mitigations but many origins remain unresolved or contested; some mitigation attempts (e.g., adversarial adaptation samples for meta-learning) did not reduce instability, leaving causes open for further research.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Multiple plausible and interacting causes make causal attribution difficult; experiments often do not control all confounders; scarcity of studies designed specifically to identify origins (only 27 of 161 core papers addressed determination).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Variability arises from a small set of deep mechanisms (under-specification, optimization fragility, dataset-induced correlations, pretraining biases) whose relative importance depends on approach and dataset; disentangling causes requires carefully-controlled, multi-factor experiments which are currently sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-615",
    "paper_id": "paper-20eab2c39326db3dd6469538249f847fd4b56e62",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "PecherSurvey2024",
            "name_full": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
            "brief_description": "Comprehensive systematic survey (N=415 papers; 161 core papers) that analyses variability, reproducibility, and stochasticity in low-labelled-data ML (in-context learning, prompting, fine-tuning, meta-learning, PEFT), cataloguing randomness sources, evaluation metrics, origins, and mitigation strategies across the literature.",
            "citation_title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
            "mention_or_use": "use",
            "model_name": "various language models and ML approaches (survey-wide)",
            "model_size": null,
            "scientific_domain": "machine learning / natural language processing (focus on low-labelled-data techniques)",
            "experimental_task": "survey and meta-analysis of experiments investigating variability/stability of models trained or prompted with limited labelled data (in-context learning, fine-tuning, meta-learning, prompt-based, PEFT)",
            "variability_sources": "Comprehensive list of randomness factors: label selection, data split, task choice, data choice (which examples used), order of data (shuffling, order of in-context examples or multi-choice options), parameter/model initialisation, random seed (aggregate of non-deterministic choices), model-level nondeterminism (layers/ops), implementation/hardware nondeterminism (framework versions, floating point, scheduling), prompt format, number of samples, hyperparameters, augmentation, noise, approach-specific sampling/iterations.",
            "variability_measured": true,
            "variability_metrics": "Reported/used metrics across surveyed papers include: mean and standard deviation of performance across runs, min/max and worst-vs-best differences, number of failed runs, distribution comparisons, statistical tests for distributional differences, average disagreement between models, relative gain, factor importance scores, ensemble performance change; some papers estimate full distributions while many report only aggregated statistics.",
            "variability_results": "Survey-level quantitative findings aggregated from literature: variability ranges widely — typical cases show small variance (≈1–5% absolute performance difference), prompting/in-context experiments show average differences ≈30% and up to ≈70% depending on prompt/order/choices, meta-learning adaptation-data choice differences up to ≈90% between worst and best adaptation sets; many papers report arbitrary run counts (examples: 3, 5, 40, 41, 86, 100, 1000) to estimate variability.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Reproducibility assessed via replication across random seeds / runs, comparison of distributions and rankings, reporting of mean/std/min/max, statistical hypothesis tests between methods, rank stability across seeds, number of failed runs, and ensemble consistency; survey notes some papers compare learned representations (e.g., representation similarity) rather than only predictions.",
            "reproducibility_results": "Survey-level conclusions: reproducibility is significantly impacted by randomness — model rankings can change when only the random seed changes; unspecified experimental choices can make a method appear superior by chance; no single consensus on extent of reproducibility failures, but extreme cases (e.g., up to 90% difference) exist and out-of-distribution settings amplify irreproducibility.",
            "reproducibility_challenges": "Challenges identified: insufficient reporting of seeds and multiple runs, arbitrary small numbers of repeats, interactions between randomness factors (confounding effects), suboptimal experimental setups (optimizer choices, iterations) causing catastrophic failure modes, prompt and sample selection sensitivities, implementation/hardware nondeterminism, selective reporting and cherry-picking, under-specification of optimization (many minima with different OOD generalisation).",
            "mitigation_methods": "Catalogue of mitigation strategies across literature: (general) ensemble-like repeated-training-and-aggregate predictions; (data/sample) representative sample selection via clustering, active learning (uncertainty/diversity), submodular selection, determinantal point processes, LLM-based pseudo-labeling and scoring, neural bandits; (per-iteration selection) similarity-based k-shot selection, entropy/margin heuristics; (regularisation/augmentation) data augmentation using LMs, label smoothing, temporal ensembling, Gaussian noise injection; (optimization/meta-learning) variance-reduction optimizers (STORM-like), longer training/optimizer bias correction; (fine-tuning) subnetwork fine-tuning (CHILD-TUNING), dynamic parameter selection; (evaluation/reporting) report distributions/min-max/std, use statistical tests and distributional comparisons.",
            "mitigation_effectiveness": "Mitigations are often effective but context-dependent: ensembles and repeated runs reliably reduce variance but are computationally expensive; selecting high-quality samples (heuristics/similarity) substantially reduces sensitivity to order and prompt choice (qualitative consensus); increasing sample count typically reduces variance with diminishing returns; variance-reduction optimizers and subnetwork fine-tuning reduce instability in some setups; quantitative effect sizes are heterogeneous across studies and often not consistently reported in a standard form.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Varied across surveyed literature; many studies used few runs (3–5), some used tens to hundreds (40, 86, 100), others exhaustive where feasible (permutation of small sets), with no consistent heuristic; survey reports arbitrary and inconsistent run counts across papers.",
            "key_findings": "Randomness is a pervasive, high-impact issue in low-labelled-data ML: it can change rankings and produce differences ranging from a few percent to order-of-magnitude (up to ≈90%) in extreme cases; common sources include random seeds, sample choice/order, prompt format, and under-specification; mitigation exists (ensembles, careful sample selection, regularisation, variance-reduced optimization) but is often computationally costly and context-dependent, and reporting practices are inconsistent, harming reproducibility.",
            "uuid": "e615.0",
            "source_info": {
                "paper_title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "RandomnessFactors",
            "name_full": "Catalog of Randomness Factors in Low-Label Regimes",
            "brief_description": "Unified taxonomy of randomness sources affecting stability in limited-labelled-data learning: input-data-related, training/process-related, implementation/hardware-related and systematic choice factors, with precise definitions and examples.",
            "citation_title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
            "mention_or_use": "use",
            "model_name": "applies to language models, meta-learning algorithms, and generic ML approaches",
            "model_size": null,
            "scientific_domain": "machine learning / NLP",
            "experimental_task": "identification and categorisation of sources of stochasticity that cause variability across repeated experiments in low-data settings",
            "variability_sources": "Detailed list: Label selection (which samples are annotated), Data split (train/val/test splits), Task choice (different tasks sampled in meta-learning/benchmarks), Data choice (which adaptation or prompt examples used), Order of data (shuffling or permutation of examples/choices), Prompt format, Noise/data augmentation, Parameter/model initialisation, Random seed (aggregate non-determinism), Model-level nondeterminism (non-deterministic ops/regularisation), Implementation/hardware (frameworks, floating-point, scheduling), Hyperparameters, Number of samples, Approach-specific parameters.",
            "variability_measured": null,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": null,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Highlight that interactions among these factors lead to confounding and that some (implementation/hardware) are hard to control; freezing one factor may be impossible when another factor (e.g., data choice) changes.",
            "mitigation_methods": null,
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": null,
            "number_of_runs": null,
            "key_findings": "A consolidated taxonomy emphasizes that many widely-used experimental choices (seed, sample choice, prompt format, order) are substantial sources of variability, and interactions among them complicate attribution and mitigation.",
            "uuid": "e615.1",
            "source_info": {
                "paper_title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "VariabilityMetrics",
            "name_full": "Evaluation and Reproducibility Metrics Used in Variability Studies",
            "brief_description": "Surveyed evaluation practices for quantifying variability and reproducibility, ranging from simple aggregates (mean/std/min/max) to distributional comparisons and specialised metrics like disagreement, failed-run counts, and factor-importance measures.",
            "citation_title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
            "mention_or_use": "use",
            "model_name": "applies broadly across surveyed LM and ML studies",
            "model_size": null,
            "scientific_domain": "ML evaluation / NLP",
            "experimental_task": "measurement and reporting of variability/reproducibility in low-data ML experiments",
            "variability_sources": "Measured with respect to random seeds, parameter initialisation, order and choice of data, prompt variations, and implementation differences.",
            "variability_measured": true,
            "variability_metrics": "Common metrics: mean, standard deviation, min/max, worst-vs-best difference, number of failed runs, distributional tests (e.g., statistical hypothesis tests), average disagreement between model outputs, relative gain; some studies evaluate learned-representation similarity rather than only accuracy.",
            "variability_results": "Survey reports many studies rely on mean/std or single aggregated values; only some estimate full distributions or use statistical tests; lack of consistent practice means variability is often under-quantified.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Reproducibility assessments often use replication across seeds, comparing rank orders across runs, and statistical comparisons of distributions; representation-level similarity metrics used in minority of studies.",
            "reproducibility_results": "No standard metric or threshold; evidence that simple aggregates can mask rank changes and high variance; statistical frameworks recommended for small-run regimes to better quantify uncertainty.",
            "reproducibility_challenges": "Small and arbitrarily-chosen numbers of repeated runs impede reliable estimation of true variance; interactions between factors make single-metric reporting insufficient.",
            "mitigation_methods": "Evaluation-side mitigations recommended: report distributions (mean/std/min/max), perform multiple runs, use statistical tests, estimate uncertainty from limited runs, compare distributions rather than single-point estimates.",
            "mitigation_effectiveness": "More comprehensive reporting and statistical evaluation improves interpretability and comparability though at computational cost; survey notes limited adoption.",
            "comparison_with_without_controls": null,
            "number_of_runs": "Survey notes wide heterogeneity in runs used across studies (3–1000+), with many studies arbitrarily choosing small numbers.",
            "key_findings": "Standardised, distribution-aware evaluation (multiple runs, statistical tests, reporting of spread and worst-case) is necessary to avoid false-positive claims; current practice is inconsistent and often insufficient for robust reproducibility claims.",
            "uuid": "e615.2",
            "source_info": {
                "paper_title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "MitigationStrategies",
            "name_full": "Mitigation Methods to Reduce Variability and Improve Reproducibility",
            "brief_description": "Compendium of mitigation approaches from the surveyed literature: general-purpose (ensembles, repeated runs & aggregation) and problem-specific (sample selection, similarity-based in-context selection, augmentation, regularisation, variance-reduced optimizers, subnetwork fine-tuning).",
            "citation_title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
            "mention_or_use": "use",
            "model_name": "applies across LMs (in-context/prompted/fine-tuned), meta-learning algorithms, and generic ML",
            "model_size": null,
            "scientific_domain": "ML methodology / NLP",
            "experimental_task": "apply mitigation strategies to reduce run-to-run variability and increase reproducibility in low-data learning scenarios",
            "variability_sources": "Targeted sources: sample choice and order, prompt format, optimizer-induced failures, model initialisation, model/hardware nondeterminism, data splits.",
            "variability_measured": true,
            "variability_metrics": "Effectiveness often measured via reductions in std/variance, reduced worst-vs-best gap, improved average or worst-case accuracy, ensemble consistency, and counts of failed runs; however, quantitative reporting is inconsistent across studies.",
            "variability_results": "Reported benefits: ensemble-style aggregation consistently reduces variance across many factors; high-quality sample selection (similarity/heuristics) markedly reduces sensitivity to ordering and prompt formatting; augmentation and regularisation reduce overfitting and instability; variance-reduced optimizers and dynamic subnetwork selection improve stability in meta-learning and fine-tuning contexts; quantitative magnitudes are heterogeneous and setup-dependent.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Same as variability metrics; also comparisons of rankings before/after mitigation and reporting of reduced number of failed runs.",
            "reproducibility_results": "Mitigations generally improve reproducibility but at computational or methodological cost; ensembles and repeated runs are reliable but expensive; problem-specific strategies can be efficient but need careful tuning and are not universally effective.",
            "reproducibility_challenges": "High computational cost of ensemble/repeat strategies; lack of universal mitigation that covers all randomness origins; risk of overfitting mitigation to a specific dataset/architecture; interactions between randomness factors can reduce effectiveness of single-strategy mitigations.",
            "mitigation_methods": "See brief_description (ensemble & aggregation; sample selection: clustering, active learning, submodular, DPPs; similarity-based k-shot selection; LLM-based scoring; data augmentation via LMs; regularisation: label smoothing, temporal ensembling, noise injection; optimization: variance reduction techniques; fine-tuning: CHILD-TUNING, dynamic parameter selection).",
            "mitigation_effectiveness": "Qualitative effectiveness established broadly; survey emphasises lack of standardized, comparable quantitative effect sizes across papers and the strong dependence of effectiveness on dataset, model, and experimental choices.",
            "comparison_with_without_controls": true,
            "number_of_runs": null,
            "key_findings": "Mitigations exist and are useful (ensembles most general but costly; targeted sample-selection and regularisation often give large gains), but no one-size-fits-all solution; mitigation effectiveness depends strongly on problem setup and interactions among randomness factors, and is underreported in consistent numerical form across the literature.",
            "uuid": "e615.3",
            "source_info": {
                "paper_title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "OriginsOfVariance",
            "name_full": "Identified Origins of Randomness / Mechanisms Causing Variability",
            "brief_description": "Set of hypothesised and empirically-examined causes of variability in low-data ML: under-specification of optimization, overfitting/catastrophic forgetting, biased/unbalanced output distributions in LMs, poor initialization or optimizer configuration (vanishing gradients), strong inter-example correlations, and dataset artefacts.",
            "citation_title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
            "mention_or_use": "use",
            "model_name": "language models (fine-tuning, prompt/in-context), meta-learning methods",
            "model_size": null,
            "scientific_domain": "ML/NLP theory and empirical analysis",
            "experimental_task": "determine root causes (origins) behind observed run-to-run variability in experiments with limited labelled data",
            "variability_sources": "Linked to mechanisms such as: under-specification (many minima with similar train loss but different OOD performance), small training-data causing overfitting/catastrophic forgetting, suboptimal optimizer settings causing vanishing gradients, inter-example correlations amplifying single-sample prediction shifts, and pretraining-induced biases (majority/recency/token/positional/domain label biases) producing unbalanced output distributions in in-context prompting.",
            "variability_measured": true,
            "variability_metrics": "Origins inferred via analyses of failed runs, training trajectories, differences in checkpoints, sensitivity to order/choice, cross-run prediction instability, and experiments isolating factors; no universal metric for 'origin' but evidence uses worst-vs-best gaps, instance-level instability, and qualitative analyses.",
            "variability_results": "Survey reports multiple, sometimes-conflicting origins across studies: under-specification is frequently cited for fine-tuning and PEFT; pretraining and label/positional/recency biases explain many in-context sensitivities; optimizer and initialization issues explain catastrophic failures in some fine-tuning cases; interactions cause some prior hypotheses to be disputed in later work.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": null,
            "reproducibility_results": "Understanding origins is needed to design effective mitigations but many origins remain unresolved or contested; some mitigation attempts (e.g., adversarial adaptation samples for meta-learning) did not reduce instability, leaving causes open for further research.",
            "reproducibility_challenges": "Multiple plausible and interacting causes make causal attribution difficult; experiments often do not control all confounders; scarcity of studies designed specifically to identify origins (only 27 of 161 core papers addressed determination).",
            "mitigation_methods": null,
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": null,
            "number_of_runs": null,
            "key_findings": "Variability arises from a small set of deep mechanisms (under-specification, optimization fragility, dataset-induced correlations, pretraining biases) whose relative importance depends on approach and dataset; disentangling causes requires carefully-controlled, multi-factor experiments which are currently sparse.",
            "uuid": "e615.4",
            "source_info": {
                "paper_title": "A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.01519875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness</h1>
<p>BRANISLAV PECHER*, Faculty of Information Technology, Brno University of Technology, Czechia IVAN SRBA, Kempelen Institute of Intelligent Technologies, Slovakia MARIA BIELIKOVA ${ }^{\dagger}$, Kempelen Institute of Intelligent Technologies, Slovakia</p>
<p>Learning with limited labelled data, such as prompting, in-context learning, fine-tuning, meta-learning or fewshot learning, aims to effectively train a model using only a small amount of labelled samples. However, these approaches have been observed to be excessively sensitive to the effects of uncontrolled randomness caused by non-determinism in the training process. The randomness negatively affects the stability of the models, leading to large variances in results across training runs. When such sensitivity is disregarded, it can unintentionally, but unfortunately also intentionally, create an imaginary perception of research progress. Recently, this area started to attract research attention and the number of relevant studies is continuously growing. In this survey, we provide a comprehensive overview of 415 papers addressing the effects of randomness on the stability of learning with limited labelled data. We distinguish between four main tasks addressed in the papers (investigate/evaluate; determine; mitigate; benchmark/compare/report randomness effects), providing findings for each one. Furthermore, we identify and discuss seven challenges and open problems together with possible directions to facilitate further research. The ultimate goal of this survey is to emphasise the importance of this growing research area, which so far has not received an appropriate level of attention, and reveal impactful directions for future research.
CCS Concepts: $\cdot$ General and reference $\rightarrow$ Surveys and overviews; $\cdot$ Computing methodologies $\rightarrow$ Artificial intelligence; Machine learning; Learning paradigms.
Additional Key Words and Phrases: randomness, stability, sensitivity, meta-learning, large language models, fine-tuning, prompting, in-context learning, instruction-tuning, prompt-based learning, PEFT, literature survey</p>
<h2>ACM Reference Format:</h2>
<p>Branislav Pecher, Ivan Srba, and Maria Bielikova. 2024. A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness. ACM Comput. Surv. https://doi.org/10.1145/3691339</p>
<h2>1 Introduction</h2>
<p>The approaches for learning with limited labelled data are designed to achieve high performance in machine learning models even with few labels available [76, 134]. Under the term learning with limited labelled data, we understand any approach that is designed to work with a lack of labels, without any constraint on how the labelled samples are distributed, i.e., whether all the samples are from a single task or are distributed across different tasks. Although similar to the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Authors' Contact Information: Branislav Pecher, Faculty of Information Technology, Brno University of Technology, Brno, Czechia, branislav.pecher@kinit.sk; Ivan Srba, Kempelen Institute of Intelligent Technologies, Bratislava, Slovakia, ivan.srba@kinit.sk; Maria Bielikova, Kempelen Institute of Intelligent Technologies, Bratislava, Slovakia, maria.bielikova@ kinit.sk.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>:    *Also with Kempelen Institute of Intelligent Technologies.
    ${ }^{\dagger}$ Also with slovak.AI.</p>
<p>notion of few-shot learning, it represents a broader scope encompassing a larger number of possible approaches (some of which are incorrectly categorised as few-shot learning in current practice).</p>
<p>To deal with the limited labels, these approaches utilise additional information from different sources [19, 134], such as transferring knowledge from similar tasks and datasets. In the NLP domain, prompting and in-context learning (also called few-shot prompting) have recently emerged. In these techniques, a large pre-trained language model is "prompted" to predict a label for a single test sample by presenting it with task instructions and the test sample (and concatenation of a few labelled samples when in-context learning is used), without requiring any parameter update [76]. In addition, it is common to use fine-tuning, where the parameters, or their subset using parameter-efficient fine-tuning (PEFT) methods, of the pre-trained large language model are updated to optimise the model for the specific downstream task using only a few labelled samples [25, 33, 94]. Finally, meta-learning can be used, where the model is explicitly trained to quickly adapt to a new task with only a handful of examples by learning how best to learn across a large number of related tasks with few labelled samples each [2, 49].</p>
<p>However, a significant problem observed for these approaches is their sensitivity to the effects of uncontrolled randomness, which negatively affects their stability. Under stability and its opposite term sensitivity, we understand a property of a learning algorithm or a model that indicates what influence the small-scale and random changes (or perturbations) in the input data and parameters have on its outputs. Such random changes are introduced by different randomness factors that represent the non-deterministic decisions in the training process, such as random initialisation of model parameters or data shuffling [44, 110]. These randomness factors represent the main point around which the effects of randomness are addressed.</p>
<p>The uncontrolled effects of randomness can have a massive impact on the stability of the utilised machine learning approaches. Running the training multiple times on the same dataset, with the same setup and hyperparameters, may lead to large deviations in the final performance [2, $33,83,94]$. Changing only the order of samples or answers in multi-choice question answering when using in-context learning can lead the model from state-of-the-art predictions to random guesses [83, 149, 191]. Choosing a different set of adaptation data in meta-learning can lead to a difference between minimum and maximum performance being up to $90 \%$ [2]. Even though the effects of randomness are present in all cases of machine learning, their impact on stability is especially significant in low-data regimes.</p>
<p>If the uncontrolled randomness is not appropriately addressed in such low-data regimes, it can have significant, non-negligible negative consequences. In comparisons and benchmarks, changing only the random seed may lead to completely different model rankings [7, 86]. It may prohibit an objective performance comparison of newly designed methods to the state-of-the-art baselines, as it makes it difficult to conclude if a specific change made a meaningful difference or if the model just "got lucky" (especially when the difference between their performance is similar to the deviation caused by the randomness). In such cases, a method can be incorrectly denoted as state-of-the-art only based on a more favourable random chance [120] (as illustrated in Figure 1b). The uncontrolled randomness can unintentionally, but unfortunately also intentionally (by cherry-picking), create an imaginary perception of research progress. As a result, researchers even suggest that we should be very vigilant when relying on models and approaches that are significantly affected by sensitivity (e.g., large language models) [191]. Finally, randomness has been identified as a significant obstacle that negatively affects reproducibility [5, 23].</p>
<p>The effects of randomness are addressed in various depths - from a pure recognition of the effects, through a deeper investigation and determining the origin of the effects, up to their partial or full mitigation (illustrated in the Figure 1). First, the effects are simply recognised, resulting in the reporting of a more representative, albeit single, aggregated value from multiple training and</p>
<p>evaluation runs (denoted as "Recognise"). As no further analysis is provided, this still leads to a biased comparison between approaches, as one approach may show better results only due to random chance and unintentional cherry-picking (illustrated in Figure 1b). Another group of works perform analyses of the effects of randomness, estimate the distribution of results from multiple runs and compare the distributions (denoted as "Investigate/Determine"). The effects of the randomness are often analysed in more detail in order to determine the real origin of randomness in the training process (e.g., under-specification, which causes slightly different parameter initialisation to converge to different minima). As the distribution is only estimated, the deviation is still not reduced. The effects of randomness are fully addressed when they are mitigated, reducing the deviation in the distribution of results (denoted as "Mitigate"). However, effective mitigation requires an understanding of the effects, their importance, and the origin of randomness provided by the analysis of the effects of randomness (provided by the "Investigate/Determine" task).
<img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Various depths the effects of randomness can be addressed in the papers
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) Comparison based on a single value and based on distribution of the results, inspired by [120]</p>
<p>Fig. 1. (a) The effects of randomness can be addressed in various depths in the papers. (b) If not taken into consideration, randomness can introduce bias into comparison results, causing one approach to show better results only due to random chance and unintentional cherry-picking.</p>
<p>In this paper, we conduct a comprehensive survey of 415 papers that address the effects of randomness, either by simply recognising the effects of randomness on stability $(\mathrm{N}=254)$, or by investigating/determining the characteristics of these effects in more detail and/or exploring the appropriate means of their mitigation $(\mathrm{N}=161)^{1}$. We provide a more in-depth analysis of the 161 papers that address the effects of randomness in more detail. Finally, we aggregate findings from this analysis and use them to identify seven challenges and open problems related to the study of the effects of randomness, while providing future directions on how to address them. For the purpose of this survey, we follow the PRISMA methodology $[93,100]$ to systematically identify the papers in the defined scope.</p>
<p>This is the first survey of its kind that provides a comprehensive and systematic literature review specifically focused on the impact of the effects of randomness on stability across various approaches for dealing with limited labelled data. As opposed to previous papers that provide an overview of possible randomness factors and investigate these effects [44, 110, 134], provide an overview for a single task (e.g., mitigating the effects) [158], or only recognise the problem randomness as an open problem [76, 134], we focus on all tasks for addressing the effects of randomness, including investigation, but also determining their origin, mitigation, and benchmarking in the presence of these effects of randomness.</p>
<p>The purpose of this survey is to emphasise the importance of this research area, which so far has not received the appropriate level of attention from researchers and practitioners. First, it should</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>serve existing or new researchers who are explicitly tackling the topic of randomness and its effects on the stability of learning with limited labelled data to support their research (by providing an overview of the state-of-the-art, identification of open problems and future directions). Secondly, its purpose is to inform researchers and practitioners utilising the learning with limited labelled data about the consequences of unaddressed randomness and how to effectively prevent them. To better achieve these purposes, we summarised all the identified papers along with their categorisation in the digital appendix of this survey (which is further described in Section A of the supplementary material).</p>
<p>The rest of the paper is structured as follows. In Section 2, we describe the scope of the survey, the difference to existing surveys and the methodology used for identifying relevant papers. Taxonomy applied to analyse and categorise the identified papers is described in Section 3. Afterwards, we describe the different tasks for addressing the effects of randomness, categorised based on our taxonomy, in Sections 4, 5, 6 and 7, with a summary of the findings at the end of each such section. The identified challenges and open problems are described in Section 8. Finally, the survey is concluded in Section 9 with a summary of our contributions and findings.</p>
<h1>2 Background</h1>
<h3>2.1 Scope of the Survey</h3>
<p>In this survey, we focus on a specific part of stability and sensitivity of machine learning approaches. We focus on papers that explore the effects of small perturbations to multiple inputs of the training process on the final performance after the training process is done. A similar, but at the same time quite distinct property is the robustness of the model. It deals with a more large scale or systematic changes in the input, such as distribution shift or the presence of adversarial examples. As the adversarial robustness is already extensively studied in the context of learning with limited labelled and the effects are often systematic, we consider it to be out of scope for this survey. Finally, we view the stability to be a subset of the reproducibility of the model. Besides randomness in the training process, reproducibility is impacted by other factors such as selective reporting, biases, data and source code availability, and many others as specified in [44]. These additional factors are out of scope for this survey.</p>
<p>Out of a wide spectrum of machine learning approaches, we specifically focus on learning with limited labelled data, which is designed to train or prompt a model with only a limited number of labelled samples (typically 0-50 per class). Not requiring large sets of labelled data, these approaches are usable across a broader set of domains, making them also more popular. Due to this popularity, they also cover almost the whole research area of addressing the effects of randomness on stability, as there are almost no other works focusing on approaches that utilise information from unlabelled sets of data. Our focus is on five prevalent groups of approaches: 1) meta-learning; 2) language model (LM) fine-tuning; 3) prompting/in-context learning; 4) prompt-based learning; and 5) parameter-efficient fine-tuning (FT). Even though we do not explicitly focus on a specific modality, the popularity of the approaches impacts the scope of this survey. The most prominent modality is text, followed by images, as the majority of the papers focus on language model fine-tuning and in-context learning. Finally, even though we focus on the setting of limited data, we also include works that are relevant for this survey that utilise large sets of data. However, due to the search query used, such papers are not systematically covered and may be incomplete (even though we specifically designed one step in the methodology to catch these papers).</p>
<h1>2.2 Existing Surveys</h1>
<p>To the best of our knowledge, there are no existing surveys addressing the effects of randomness on stability across all tasks ranging from recognise to mitigate. However, there are relevant surveys on related topics that partially touch on the topics of stability or sensitivity. First of all, some papers provide an overview of possible sources of randomness, although with a focus on replicability [44]. In some surveys on learning with limited labelled data and/or few-shot learning, the sensitivity to the effects of randomness is already mentioned as a problem that should be addressed [34, 57, 76, 134], but no additional details are provided. Finally, the most recent relevant survey [158] focuses on sample selection strategies for mitigating in-context learning sensitivity to sample choice. While relevant, it provides a significantly narrower perspective than the one given by this comprehensive survey.</p>
<p>To fill this gap, we aim to conduct the first comprehensive overview of the effects of randomness on the stability of learning with limited labelled data and provide novel perspectives that are not easily visible from the standard analysis of related works.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 2. Process for identifying and categorising papers for this survey. Due to strong clustering effects, additional papers are identified using a reference analysis on the most relevant identified papers, i.e., including papers cited in them, as well as the ones that cite them.</p>
<h3>2.3 Survey Methodology</h3>
<p>To systematically identify the set of relevant papers for our comprehensive survey, we follow the PRISMA methodology [93, 100], performing the following steps (as illustrated in Figure 2):
(1) Identification of relevant papers. A keyword search query is defined based on the related terms for our defined scope identified in different papers. Multiple digital libraries are searched using the defined query to identify a set of potentially relevant papers.
(2) Relevance filtering. The potentially relevant papers are categorised into irrelevant and relevant based on the context the defined keywords are used in.
(3) Content filtering. The non-full papers or papers released in unrelated venues, such as extended abstracts, proposals for talks or papers appearing in unrelated workshops not dealing with stability, are filtered out.
(4) Paper analysis. The papers are analysed based on their depth of addressing the randomness effects. The papers that only recognise the problem of stability without further investigation are referred to as recognise papers. The remaining papers are referred to as core papers.</p>
<p>(5) Reference analysis. To discover additional potentially relevant papers (especially those relevant for the survey that do not necessarily deal with limited data), the papers cited in the core papers; as well as papers citing the core papers, are examined and their relevance is determined based on their title and abstract. The papers deemed potentially relevant are an additional input into the second step of this process.
(6) Analysis and categorisation. The identified core papers are analysed and categorised based on the taxonomy (as defined in Section 3) to provide a comprehensive overview and to identify challenges and open problems.
As a result, 415 papers are identified - out of them, 254 papers are classified as recognise papers, and 161 as core papers. The number of identified papers grouped by the publication year is presented in Figure 3. The categorisation of the selected core papers is showcased in Table 1. Moreover, the full list of all identified papers (including recognise papers), with their categorisation and additional metadata is available in the digital appendix. For a more detailed implementation of the survey methodology, see the supplementary material (Section C).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 3. Number of papers dealing with randomness grouped by the year. Figure 3a shows only the core papers that focus on addressing the effects of randomness in more detail, while Figure 3b also includes the papers that only recognise the problem. The problem started to attract attention only in recent years. Year 2024 covers papers published until July 2024.</p>
<h1>3 Taxonomy for Literature Analysis and Categorisation</h1>
<p>The identified core papers are categorised based on multiple dimensions that describe their characteristics of interest. We define the following three separate primary properties that are used to categorise the papers: 1) what tasks are performed to address the randomness; 2) randomness factors addressed by the papers; and 3) what machine learning (ML) approach the papers use. The performed tasks property is the main property based on which we organise the rest of the survey, while the randomness factors and machine learning approach properties are mostly used to define the scope in this survey (see Section 2.1). We provide the paper distribution across individual randomness factors and specific machine learning approaches for all core papers in Table 2 (the paper distributions for specific tasks are included in the digital appendix.</p>
<h3>3.1 Tasks Performed to Address the Randomness</h3>
<p>This property specifies the different tasks for addressing the effects of randomness that are performed in the papers (the relations between these tasks, along with their input and output are illustrated in Figure 4). For this property, we define the following 7 values:</p>
<p>Table 1. A showcase of the selected core papers and their categorisation based on the proposed taxonomy. The full categorisation, along with the recognise papers is available in the digital appendix.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(1) Investigate - the effects of randomness from different randomness factors are investigated using specifically designed experiments. For example, the training and evaluation is done multiple times to determine the distribution of performance across multiple runs.</p>
<p>Table 2. Relation between the randomness factors and machine learning approaches across all analysed papers. We can observe different popularity of the randomness factors across different machine learning approaches. The majority focus is denoted in bold. For example, fine-tuning mostly focuses on the random seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">All <br> $(\mathrm{N}=13)$</th>
<th style="text-align: center;">Meta-Learning <br> $(\mathrm{N}=14)$</th>
<th style="text-align: center;">Fine-Tuning <br> $(\mathrm{N}=35)$</th>
<th style="text-align: center;">In-Context Learning <br> $(\mathrm{N}=96)$</th>
<th style="text-align: center;">Prompt-Based Learning <br> $(\mathrm{N}=17)$</th>
<th style="text-align: center;">PEFT <br> $(\mathrm{N}=10)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Label Selection</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Data Split</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Task Choice</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\mathbf{7}$</td>
<td style="text-align: center;">$\mathrm{N} / \mathrm{A}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Data Choice</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\mathbf{9}$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$\mathbf{5 1}$</td>
<td style="text-align: center;">$\mathbf{8}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Random Seed</td>
<td style="text-align: center;">$\mathbf{7}$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\mathbf{2 0}$</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Model Initialisation</td>
<td style="text-align: center;">$\mathbf{9}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">$\mathrm{N} / \mathrm{A}$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Order of Data</td>
<td style="text-align: center;">$\mathbf{8}$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">$\mathbf{3 7}$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Implementation</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Number of Samples</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Hyperparameters</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Augmentation</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Noise</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Prompt</td>
<td style="text-align: center;">$\mathrm{N} / \mathrm{A}$</td>
<td style="text-align: center;">$\mathrm{N} / \mathrm{A}$</td>
<td style="text-align: center;">$\mathrm{N} / \mathrm{A}$</td>
<td style="text-align: center;">$\mathbf{4 6}$</td>
<td style="text-align: center;">$\mathbf{1 1}$</td>
<td style="text-align: center;">$\mathbf{5}$</td>
</tr>
<tr>
<td style="text-align: left;">Model Randomness</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Approach Specific</td>
<td style="text-align: center;">$\mathrm{N} / \mathrm{A}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(2) Determine - an effort to discover the origin of the randomness (different from the randomness factors) is made by analysing the behaviour of the specific machine learning approach or model in the presence of randomness. For example, to determine why changing the order of samples causes a significant change in performance, the experiments that observe specific problems (e.g., label bias) are designed and analysed.
(3) Mitigate - mitigation strategies for reducing the effects of randomness are proposed, used and their effectiveness is determined. For example, it is proposed to use more deterministic data sampling based on heuristics instead of random sampling.
(4) Benchmark - a benchmark that is specifically designed to take into consideration the effects of randomness is proposed. For example, a benchmark that performs multiple train and test splits and evaluates on all of them.
(5) Evaluate - a more sophisticated, usually statistical, framework for evaluating the experiments investigating the effects of randomness, or determining the effectiveness of mitigation strategies, is proposed. For example, instead of calculating a simple average value, it is proposed to estimate the distribution of results and evaluate the difference using statistical tests.
(6) Report - it is proposed to report more than a single value when running and evaluating experiments and comparing approaches to better deal with the effects of randomness. Not necessarily dependent on experiments for addressing the effects of randomness, but also recommendations on how to improve the reporting of results overall when the randomness has a strong effect on performance. For example, instead of reporting a single average value, the minimum and maximum values over multiple runs should be reported as well.
(7) Compare - better comparison strategies between different approaches in the presence of significant effects of randomness are proposed. For example, instead of comparing single average values, it is proposed to compare distributions of results using statistical tests.
We aggregate the closely related tasks together and organise the survey based on these aggregated groups. Namely, we aggregate together the investigate and evaluate tasks, as the evaluation strategies are designed specifically for the investigation experiments. In addition, we aggregate the benchmark, compare and report values, as they are closely related.</p>
<p>As the different papers can focus on different, often disjointed, tasks for addressing the effects of randomness, this property does not necessarily provide a distinct division of the papers. For example, the most common combination is to perform investigation, determination and mitigation of a specific randomness factor in a single paper. In such cases, the paper is mentioned in this survey multiple times, each time focusing only on the parts relevant to the specific task.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 4. Different tasks for addressing the effects of randomness, along with their inputs, outputs and the relations between them. The dashed lines represent relations between the tasks that currently do not exist but need to be considered in the optimal state of addressing the effects of randomness, e.g., using more sophisticated evaluation when determining the effectiveness of mitigation strategies. The related tasks are grouped, e.g., evaluate task near the investigate task.</p>
<h1>3.2 Randomness Factors</h1>
<p>The randomness factors property specifies what randomness factors are addressed in the paper in some way (i.e., their effects are investigated or mitigated), but not factors that are just mentioned (e.g., mention that other papers found the factor to be unimportant). Papers can focus on multiple factors at the same time.</p>
<p>Multiple randomness factors have already been identified and described in the existing works (e.g., $[44,110]$ ). Since the used terminology differs across the works, for the purpose of this survey, we established the grounds (in terms of common terminology and understanding) by collecting these randomness factors and unifying their denomination and definition.</p>
<p>The identified randomness factors can be categorised into three distinct groups: 1) factors related to the input data; 2) factors related to parameters of the model and the overall training process; 3) factors related to the underlying implementation of deep learning frameworks and hardware; and 4) factors that provide more systematic changes.</p>
<p>As the randomness factors related to the input data, we identify the following:
(1) Label selection - specifies what data is considered to be annotated from a larger set of unlabelled data. Represents the real-world scenario with a limited labelled budget, where only specific samples are annotated. Has a potentially significant impact on the underlying data distribution.
(2) Data split - randomness introduced by different splits of data into training, validation and testing set. Mainly impacts the information provided to the model during training.</p>
<p>(3) Task choice - randomness introduced by choosing different tasks for training. This factor is mainly relevant for approaches that perform few-shot learning, such as meta-learning, or when using a benchmark dataset composed of different tasks.
(4) Data choice - randomness introduced by the sampling of data the model uses for adaptation (meta-learning), or as examples in prompts (in-context learning).
(5) Order of data - randomness introduced by the random shuffling of data in different training iterations. Mainly relevant for the in-context learning (order of labelled samples in the prompt) and fine-tuning. For meta-learning, this factor represents the order in which the different tasks are presented during training.
(6) Noise - randomness introduced by adding noise to the different samples.
(7) Prompt format - randomness introduced by small changes in the prompt formats, such as replacing words with synonyms or words without semantic meaning.
Similarly, we identify the following randomness factors corresponding to the randomness related to model parameters and the overall training process:
(1) Parameter initialisation - randomness introduced by randomly initialising the parameters of the model, such as weights in neural networks. Mainly relevant for meta-learning and language model fine-tuning, as no initialisation is performed in in-context learning.
(2) Random seed - randomness introduced by non-deterministic actions in the training process. Often used as an aggregated factor for the parameter initialisation and the order of data randomness factors.
(3) Model randomness - non-deterministic operations in the model, such as the use of nondeterministic layers or regularisation.
(4) Approach specific parameters - aggregates the randomness introduced by factors specific for only some of the approaches (e.g., sampling strategy or number of iterations).
Randomness introduced by implementation of deep learning frameworks and hardware is a special low-level group, which includes factors such as scheduling and floating point precision, autoselection of primitive operations, how parallel processes are used, changes introduced by different implementation of frameworks such as TensorFlow or PyTorch and their different software versions, but also many other factors influencing reproducibility more than the stability of results [44, 110]. Since these randomness factors cannot be easily addressed, for this survey, we do not specifically distinguish between them and consider them jointly as one group.</p>
<p>Finally, we also consider factors that provide more systematic changes and can be viewed as an edge case with regards to randomness. This group includes factors such as the randomness introduces by augmentation, hyperparameters or the number of samples. Besides these randomness factors, we also consider systematic choices as they can affect the sensitivity to the effects of randomness as well. These are often a result of the experimental setup and include choices such as the datasets and their characteristics (e.g., monolingual vs. multilingual), or the model size</p>
<p>Each randomness factor is associated with a set of its randomness factor configurations (sometimes referenced in short as configurations). A single configuration specifies one state from the set of all possible states the randomness factor can attain. For example, a single randomness factor configuration specifies one possible permutation of the samples (order), a single set of weights used for initialising the whole model, or a single random value to seed the random number generator. Using this definition, the set of randomness factor configuration can be infinite for some factors (e.g., initialisation weights) or finite, but extremely large (e.g., permutations of order).</p>
<p>The different randomness factors may depend on each other, leading to interactions between them [25, 104, 147]. An example of such interactions is the relation between data choice and order of data randomness factors, where the choice of what data we use in training explicitly affects the</p>
<p>order of samples. Therefore, such interactions can lead to negative, often unforeseen consequences, such as prohibiting reliable freezing of randomness factor configurations (e.g., it is not possible to have a fixed order of samples when using different choices of data).</p>
<h1>3.3 Machine Learning Approach</h1>
<p>Similarly, the machine learning approach property specifies which group of approaches is addressed in the paper. Besides the machine learning approaches that define the scope of this survey (Section 2.1), we also consider a generic machine learning. This is a special value that contains papers that still deal with the effects of randomness, but only as part of overall supervised learning and do not focus on any of the specific machine learning approaches. The main idea, high-level overview and reference to popular approaches and comprehensive surveys of the machine learning approaches we work in this survey are included in the supplementary material (Section B).</p>
<h2>4 Investigating the problem of stability</h2>
<p>In this task, the effects of randomness on the stability are investigated, their impact is estimated and the significance of this impact is determined. Each paper investigating the effects of randomness can be characterised by three main aspects: 1) scope of the investigation; 2) investigation methodology; and 3) results from the investigation. The scope of the investigation includes decisions about what factors to investigate or across what models, datasets and modalities the investigation is performed. It also includes more nuanced specifics like what problem statement is considered (simple classification/regression or sequence generation), or from what setting the data comes from (in-distribution vs. out-of-distribution). Although these decisions have minimal impact on the methodology, they strongly govern the obtain results and their generality.</p>
<p>The investigation methodology determines how the investigation is done. It provides answers to different problems and specifics of the investigation, such as how to perform the investigation (multiple fully randomised runs or keeping some parts static), how many investigation runs to use, how to address interactions between randomness factors or how to evaluate the different runs. Again, each decision has a significant impact on the results of the investigation, mainly how trustworthy and objective they are.</p>
<p>Finally, the results of the investigation specify findings and the behaviour that was observed. Interpreting these results is important for informing the further tasks for addressing the effects of randomness (i.e., determining the real origin of randomness and mitigating the effects). They can also be used to provide recommendations on the best use of models in practice in low-data regimes.</p>
<h3>4.1 Scope of the investigation</h3>
<p>The majority of the papers focus on investigating how the randomness affects the language models ( 41 out of 56 ) when they are used to deal with limited labelled data. Most of the focus is on incontext learning [36, 75, 83, 126, 174, 180] (32) and only a smaller focus is on fine-tuning of language models [33, 56, 89, 128, 177, 184, 187] (9) or prompt-based learning (4). Parameter-efficient fine-tuning methods are not investigated at all (only recognised in [25]). The focus on investigating overall machine learning approaches (mainly their use with few data and evaluation) [16, 17, 32, 110, 120] and the use of meta-learning [2, 15, 28, 104, 129, 164] is significantly lower ( 10 and 6 respectively). Only a fraction of papers consider multiple approaches at the same time [104, 126, 148, 159].</p>
<p>The significant focus on language models also translates to what modality is addressed the most. The investigation of text data in NLP problems receives the most focus ( 46 out of 56) $[16,33,36,56,75,83,89,120,126,128,164,174,177,180,184,187]$, with images being mostly addressed only in meta-learning and machine learning approaches (but not all of them)</p>
<p>$[2,15,17,28,110,129]$, with only 3 papers considering investigation across both image and text data $[32,138,191]$.</p>
<p>The popularity and importance of randomness factors heavily depend on the approach. The most focus is dedicated to the data order randomness factor, being the most popular for machine learning, fine-tuning and in-context learning [36, 83, 108, 174, 180, 191]. The remaining randomness factors are investigated only for specific approaches. For in-context and prompt-based learning, the most popular factors are sample choice and prompt format [119, 127, 147, 171] and, to a certain extent, the number of samples. Parameter initialisation and random seed are most popular for general machine learning and fine-tuning approaches [15, 25, 32, 56, 75, 120, 126, 164, 177, 187]. The implementation randomness factors are popular only for the general machine learning papers [15, $45,110,138,190]$. The remaining randomness factors, such as the effects of augmentation [16, 138], choice of what data is labelled [16, 28, 56, 75, 104, 129], split of data [16, 32, 104], noise [16], task choice for meta-learning [2, 129], or other approach-specific factors receive only limited attention.</p>
<p>The scope of the investigation has the strongest effect on how general the final results are. Although the majority of the papers focus on multiple datasets and models [2, 7, 15, 17, 28, $56,75,126,164,174,177,180,187,191]$, there are studies that consider only a single dataset or a single model [16, 33, 89, 110, 128, 129], limiting the generalisability of the results. Only in specific cases is the investigation specifically designed to observe the effects of systematic choices, such as the number of samples or model size, and interactions between randomness factors [25, 32, 36, 83, 104, 119, 120, 147, 184]. Finally, only a minority of papers consider an out-of-distribution setting [3, 28, 89, 164, 167, 187, 191] or investigation across different languages $[41,167,171]$.</p>
<h1>4.2 Investigation Methodology</h1>
<p>Overall, the differences in how the investigation of the effects of different randomness factors is done are small across different papers, making the investigation quite consistent. In all papers, the investigation is performed by changing the randomness factor configuration (such as changing the value of the random seed) and observing the changes in the behaviour of the model and its results. In the early papers, this was done by introducing the change randomly into the configuration, without any control over the remaining randomness factors [16, 17, 28, 56, $75,89,108,119,120,126,128,164,174,177,184,187]$. However, the most common strategy is to exert additional control over the investigation by fixing the configuration of the non-investigated randomness factors to a specific value while randomly varying the value for the single investigated factor $[15,36,45,83,86,110,127,138,142,171,180]$. In specific cases, the fixed investigation is repeated across different fixed values for the non-investigated factors and aggregated over them to better deal with interactions [104, 148]. Finally, another investigation strategy changes the configuration for multiple randomness factors at the same time, and their effects are disentangled at the end $[3,25,33,147,149,191]$.</p>
<p>The choice of how many randomness factors to investigate at the same time also plays an important role in the investigation methodology. Many papers consider only a single randomness factor $[17,32,75,108,120,126,127]$. However, such investigation can lead to biased results due to the interactions between different randomness factors. Using multiple randomness factors in a single investigation allows for a more detailed exploration of the behaviour of models in the presence of different sources of randomness. Most approaches investigate multiple randomness factors, although the number can be quite low (2-3) $[15,16,25,28,33,36,56,83,89,119,128,171,174,177,180,184$, 187, 191]. Comprehensive investigation of a large number of factors is considered only in specific cases $[45,104,110,138,147,190]$.</p>
<p>Due to the potentially significant impact of the interactions between randomness factors and systematic choices, it is important to define how to consider these interactions and how to deal with them to disentangle the effects of different randomness factors and investigate how the systematic choices affect the findings. Many papers do not use any specific way to distinguish between the effects of different randomness factors [2, 17, 28, 32, 75, 89, 120, 126, 129, 164, 187]. As mentioned before, this may lead to the results being biased and can be traced as a direct cause of some of the open problems discussed in Section 8. A simple way to consider the interactions is to use the fixed investigation [15, 36, 83, 110, 177, 180]. However, such an approach is limited in its investigatory strength, due to the randomness still being present in the choice of the fixed configuration. A better way is to search through combinations of configurations for all the factors at the same time $[3,16,33,56,104,128,142,148,174,184]$. However, this significantly increases the computational cost of the whole investigation [16, 32, 190]. In case of systematic choices, the most straightforward approach is to repeat the investigation for different systematic choices, such as the number of samples or model size [25, 85, 104, 119, 149, 171, 191].</p>
<p>Another important aspect to consider is how to obtain representative results from the investigation. The representativeness of results mainly depends on how many randomness factor configurations are explored, i.e., how many training and evaluation runs are used during the investigation. Many papers choose the number of different randomness factor configurations only arbitrarily, often choosing lower number ( $3,5,40,41,86,100,1000$ ) without any explicit explanation why the given number was chosen [15-17, 36, 75, 89, 110, 120, 126, 164, 177, 187]. Similarly, when considering multiple randomness factors and interactions between them, it is common to arbitrarily select some number (usually small) of configurations for each factor and then determine the number of runs as all possible combination of these configurations [33, 56, 128, 148, 184]. For example, when considering 3 different randomness factors, with 5 different configurations for each, the final number of runs is determined as $125\left(5^{3}\right)$. In specific cases, all possible configurations of a single factor can be explored [2, 32, 83, 142, 174, 180] or the number can be determined dynamically (e.g., until something happens or a threshold is overcome) [104, 166, 191]. However, this happens only when the number of possible configurations is small or the added computation cost is not as significant.</p>
<p>Majority of the papers evaluate the effects of randomness from multiple runs based on an aggregated value from these runs, either as a single value of mean, standard deviation (with standard deviation not even being present in some papers), or number of failed runs [17, $28,32,36,56,83,89,110,126,129,164,177,180,184,187]$, or as multiple aggregated values, such as average, worst and best performance, or the difference between them (e.g., difference between worst and best run, or difference between worst and average run) [2, 75, 174]. Some papers define new metrics, such as an average disagreement between models [138, 190], a change in performance when models are ensembled [138], a relative gain [4, 190], or a factor importance [104]. Only some papers evaluate the results of the investigation based on the distribution, mostly as a means for comparing different randomness factors, models or the behaviour across datasets [120]. Some papers introduce special evaluation strategies based on statistical approaches that can better distinguish between effects of different factors and deal with their interactions, and that can better estimate the results distribution from a small number of repeated runs to make the results more representative $[15,16,33,128]$. Finally, only a minority of papers compare the learned representation instead of the predictions $[11,138,190]$.</p>
<h1>4.3 Results from the Investigation</h1>
<p>Majority of the papers found that the effects of randomness have significant effects on the performance and stability across all approaches for dealing with limited data, leading to differences in performance as high as $90 \%[2,15-17,32,33,36,56,83,89,126,128,164,174,180,184]$.</p>
<p>Based on the randomness, the rankings in benchmarks and results of the comparison can change significantly [7, 86, 191]. Overall, the algorithmic factors were found to be more important than implementation, but only by a small margin [45, 138, 190]. In-context learning was found to be significantly sensitive to prompt format [127, 142, 147, 166, 167], choices of samples [3, 167, 183] and order of choices in multi-choice QA [149, 191], leading to average performance difference of $30 \%$ and up to $70 \%$. Fine-tuning BERT multiple times and picking the best result, it is possible to outperform its more recent variants [33]. The difference in performance between the worst performing and the best performing set of adaptation data in meta-learning can be up to $90 \%$ [2]. On the other hand, the observed variance in other experiments was found to be lower, usually only a few percent difference $(1 \%-5 \%)$, while still being enough to influence the results and comparisons between different models [75, 110, 120, 177]. However, the effects of randomness are significantly higher in the out-of-distribution setting [3, 89, 164, 167, 187, 191].</p>
<p>The investigation results indicate that the impact of interactions, systematic choices and experimental setup on the sensitivity to the effects of randomness and findings is significant in many cases [45, 86, 104, 147, 171, 191]. However, there is usually low consensus on how much the variance is reduced by different systematic choices. Some papers found that using more labelled samples or more training iterations reduces the stability [2, 3, 25, 56, 104, 119, 183], although with diminishing returns [85, 177]. On the other hand, other papers found that increasing the number of samples does not lead to significant improvement [83, 108, 119, 167, 191], or even having negative effects on the stability [180]. Similarly, some papers found that increasing the model size reduces the variance [171, 184], but others have found that it either has no effect or even worsens the situation [25, 36]. Optimising configuration of specific randomness factors leads to lower sensitivity to other factors, such as using sophisticated sample selection reducing sensitivity to order [2, 174, 183], although it varies across experimental setups [2, 171] (e.g., using 16 random samples is equivalent to using 1 high-quality sample). Keeping the randomness factor configuration fixed during the whole use of the model, such as using a single set of samples or defined order based on heuristics, can help reduce the variance [129], but not in all the cases [15, 174]. Finally, accounting for and investigating as many sources of randomness as possible helps with reducing variance at the cost of computational cost [16].</p>
<p>The behaviour of randomness factors is affected by the choice of models and datasets. The inconsistency in results is present here as well. Some papers indicate that there is different behaviour of the randomness factors across different model architectures [10, 17, 32, 83, 86, 120, 149, 171, 184] or datasets [10, 86, 164, 167, 171, 180, 187], with smaller datasets showing more variance [33]. However, many of the results from investigation indicate that the behaviour of randomness factors is consistent across all the model architectures, sizes and datasets [2, 15, 16, 36, 56, 75, 89, 126-$128,148,174,177]$.</p>
<p>The investigation of the effects of randomness often just determines that there is a sensitivity to a specific factor, but without providing any additional analysis of how much it affects the models or what randomness factors are most important [28, 75, 89, 126, 128, 129, 164, 167, 174, 187]. However, the most interesting use for the results of the investigation is to provide more in-depth analysis. For example, comparing between different randomness factors, datasets and models [104]. Although both choice and order are significant contributors, some results indicate that choice is more important as with good samples the sensitivity to order is significantly lower [3, 85, 104]. The prompt format can even affect the sensitivity to the chosen samples [171]. Other results indicate that random seeds, especially the ones used when fine-tuning, are more significant than just the effects of implementation and hardware [110, 138, 184, 190], while others find the effects implementation and hardware to contribute more significantly [15]. The results from the investigation of almost all randomness factors at the same time indicate that</p>
<p>the splitting of data has the most impact on the performance [16]. Finally, analysing the results can better inform other choices later in the process. For example, estimating the benefit of prompt formats based on closeness (similar to hyperparameter tuning) is not possible, as even the best and worst performing prompts can be close to each other [166]. In addition, the quality of the prompt is not dependent on how well the instructions are written, as even misleading or irrelevant instructions can lead to good results (e.g., better than the correct prompt) [148].</p>
<h1>4.4 Overview of Findings: Investigating Effects of Randomness</h1>
<p>The effects of randomness are not investigated evenly across all groups of approaches for dealing with limited labelled data or modalities. The investigation is more skewed towards randomness in language models, which goes hand in hand with the focus on natural language processing. However, the findings from meta-learning and overall machine learning approaches indicate that this mostly stems from the popularity of language model research and not from the impact the randomness has on the investigated approaches. The effects of randomness are consistently significant across all approaches for dealing with limited labelled data.</p>
<p>In addition, the focus of the investigation is mostly on a few popular randomness factors, such as random seeds, model initialisation, sample choice and order, which may cause lower instability. Even though other randomness factors receive minimal attention, it has no correlation with their importance. For example, data split is considered the most important factor, while being investigated only in a few papers. Another mostly ignored factors are the non-deterministic implementation and hardware, and the choice of which samples are labelled.</p>
<p>The investigation strategy design is agnostic from data and the machine learning approach, making its use consistent across all papers with only small modifications. However, these small modifications have a significant impact on the overall investigation, often making it impossible to compare findings from different papers. The number of training and evaluation runs are chosen rather arbitrarily, ranging from ones to thousands, without any explanations behind the choice. Almost no paper introduces heuristics for determining how many repeated runs are needed to estimate the underlying distribution of results. The only heuristic used is to explore all the possible randomness factor configurations when their number is small, such as the permutations when the number of used samples is small $(&lt;10)$.</p>
<p>The effects of randomness are usually investigated only across a small number of randomness factors at one time. The number of considered factors is usually 1-3, while other factors may be mentioned, but not investigated. Only papers dealing with general machine learning cover a broader and more representative set of factors at the same time. However, it is problematic to investigate multiple factors at the same time due to the computational cost this introduces - the number of repeated runs raises exponentially with a number of randomness factors [16, 17, 32].</p>
<p>Even when evaluating multiple randomness factors at the same time, the interactions between them or the impact of systematic choices are not taken into consideration. Many papers use the fixed randomness factor configuration in the investigation, but it is still affected by the randomness. Only some papers use combinations of all randomness factor combinations. However, this is always done only for factors that are considered in the paper and not for all of them. In addition, no paper applies mitigation strategies to other randomness factors.</p>
<p>No analysis of the importance of the randomness factors is performed when investigating a single factor. Similarly, when investigating multiple factors at the same time, almost no comparison is performed between these factors. This causes the investigation results to be limited in their extent as the problem is only recognised, but no further conclusions can be drawn from it. It is not possible to answer questions like: "Are the effects of the factor significant?" or "Which factor is more important and should be focused on first?".</p>
<p>The effects of randomness are evaluated in a multitude of ways. The most common approach is to use a simple aggregation over the runs (e.g., mean or standard deviation). However, as the experiments are performed on a limited number of training runs, the single aggregated value may not be representative of the underlying distribution. A better approach for evaluating the effects is the use of statistical approaches that allow to quantify the uncertainty still present in the models when only limited runs are used, or to determine the expected behaviour of the models in limit, while only having results from a significantly smaller number of runs. The evaluation is further complicated by the interactions between randomness factors that can muddle the results and need to be taken into consideration using statistical approaches.</p>
<p>Focus on the more specialised settings, such as out-of-distribution, multilingual, or using parameter-efficient fine-tuning methods, is severely limited. However, few works that study these settings found the effects of randomness to be more significant in such settings, especially for out-of-distribution tasks or parameter-efficient methods.</p>
<p>A significant drawback regarding the results from the investigation is that no obvious consensus exists regarding the behaviour of different randomness factors across different models and datasets. Many papers investigating the same setting come to contradictory findings about almost all parts of the investigation. Some papers observe a decrease in variance when the size of the model increases, while others found no obvious connection between the size of the model and the amount of observed variance (e.g., increasing size leads to an increase in variance). When it comes to datasets, some papers observed consistent results, with the same factor being most significant across different datasets, while others observed different factors being important on different datasets. This may be a result of disregarding interactions or using a sub-optimal experimental setup, as some papers observed both interactions and systematic choices have significant effects on the findings of investigation (e.g., the effect of one factor may be misattributed to another factor).</p>
<p>The most consistent finding from the investigation is that increasing the number of samples reduces variance. However, the decrease in stability has diminishing returns, where from a certain point, increasing available data has no more significant impact. This may also be due to the quality of samples used, as some papers found that 1 high-quality sample provides a reduction in sensitivity equivalent to 16 randomly selected samples.</p>
<h1>5 Determining the Origin of Randomness</h1>
<p>In this task, the origin of randomness is determined, which represents a specific underlying problem that only manifests through the randomness factors. The design of experiments is similar to the one when investigating the effects of randomness, but with a specific hypothesis that is formed and answered either empirically or using theoretical understanding of deep learning models (e.g., whether the label which is used as the last example in in-context learning has an impact on the prediction, which would indicate recency bias). Out of 161 core papers covered by this survey, only a small fraction of 27 papers focus on this task, even though it is important for the mitigation of the sensitivity to randomness.</p>
<p>The design of experiments and further explanation of the behaviour is closely tied to the used machine learning approach and the randomness factor that was observed to cause the randomness.</p>
<h3>5.1 Origins of Randomness in Meta-Learning</h3>
<p>The focus on origins of randomness in meta-learning is limited only to the adaptation data sampling randomness factor [2]. The behaviour of different meta-learning approaches is analysed using a combination of all the possible combinations of the adaptation samples from image datasets. The goal of the analysis is to determine the characteristics of the worst-case samples, i.e., the samples that cause the worst performance when used for adaptation. It is determined that these samples are</p>
<p>not artefacts, as they are always correctly labelled, appear representative of the respective classes and are not visually different from other images. Based on this observation, it is speculated that the problem is due to the adversarial nature of the images. However, it is later disproved empirically as adversarial training does not reduce the effects of randomness, leaving the real reason why meta-learning is sensitive to the choice of support samples open.</p>
<h1>5.2 Origins of Randomness in Language Model Fine-Tuning, Prompt-Based Learning and Parameter-Efficient Fine-Tuning</h1>
<p>As the main sensitivity and origins of randomness for language model fine-tuning, general machine learning, prompt-based learning and parameter-efficient fine-tuning are related to each other, we describe them together. The origin is analysed mainly using the random seed randomness factor (overall across 14 papers), which affects the initialisation, order of samples and model randomness. Many different origins of randomness are identified. The experiments in this part are the most diverse (e.g., models, languages, factors) but often lead to similar origins of randomness.</p>
<p>Small availability of data that causes overfitting and catastrophic forgetting is identified as being one origin of randomness when fine-tuning language models [53, 177]. This can cause the models to rely on shallow lexical features, even though the models were trained on abundant, feature-rich data beforehand [177]. Another possible origin may be the bad fine-tuning start point in the transferred model, mainly in the higher layers (not only the newly initialised ones but also the pre-trained ones) that are sensitive to perturbations in the input.</p>
<p>However, catastrophic forgetting and small size of training data are disputed as causes by later papers [94, 152, 173]. Instead, it is only a symptom of another problem [94]. Looking at the failed fine-tuning runs (i.e., the runs that perform worse than random chance), it is identified that the language models suffer from vanishing gradients. As the cause of the vanishing gradients, a suboptimal experimental setup is identified, such as using an optimiser without bias correction or a small number of training iterations [94, 173]. An additional cause of vanishing gradients is the sub-optimal initialisation point for the upper layers, which are specialised and cause significant variance when transferred [173].</p>
<p>Again, all these hypotheses are disputed [152] and instead under-specification of the optimisation problem is proposed as the origin of the variance [30, 89, 152], especially in out-of-distribution datasets [89, 152]. There exist multiple good solutions in the generalisation errors on the source dataset, but with wildly different generalisation errors on the target datasets, with the optimal solution lying in a non-flat region where even a small perturbation leads to large difference in error [89, 152]. This makes the identically trained models encode different inductive biases [30] and makes the choice of minima rather arbitrary, being easily affected by small changes in initialisation and order of examples [89]. Under-specification was also identified as the main origin of randomness for prompt-based learning [60] and parameter-efficient fine-tuning (mainly prompt-tuning) [26], which causes the loss surface to be remarkably steep and a small change in input causes a massive change in the loss space.</p>
<p>Furthermore, it is postulated that the large variance in performance is due to the strong interexample correlations in datasets [187]. When the prediction of a single sample changes, this correlation causes simultaneous change in a large proportion of the predictions in the dataset. This was observed when analysing the training trajectory in an experiment, where the model checkpoints at the same step of training from different initialisations had significant differences in performances. The individual predictions of each sample were highly unstable and changed constantly, which caused large fluctuations. However, no explanation for the unstable behaviour in individual samples is provided, besides being an inevitable consequence of current datasets [187].</p>
<p>Finally, it is claimed that randomness is the inevitable outcome of the stochastic training and a basic amount of instability will be always observed, equivalent to changing 1 bit in the weights [138].</p>
<h1>5.3 Origins of Randomness in Prompting and In-Context Learning</h1>
<p>The origins of randomness in in-context learning are analysed across three randomness factors (overall 13 papers): 1) order of data (both samples and answer choices in multi-choice question answering) [83, 180]; 2) choice of samples [75, 174]; and 3) prompt format.</p>
<p>The significant effects of different data orders, sample choices and prompt formats were found to be the consequence of highly unbalanced label distribution in the output [83, 84, 127, 170, 180]. The output distribution of the language models is influenced by the biases present in them [37, 108, 119, 176, 180, 181, 191]. For the order of in-context examples, four biases were identified that cause these effects [37, 119, 176, 180]: 1) majority label bias, where the output is biased towards the more frequent answer in the prompt (e.g., class imbalance in prompt); 2) recency bias, where the model tends to output the labels that appear towards the end of the prompt; and 3) common token bias, where the model is biased to output more common tokens from the pre-training distribution; and 4) domain label bias [37] that is introduced by the domain texts. For order of answer choices, mainly two biases are identified [108, 181, 191]: 1) positional bias, where the model prefers answers in specific positions; and 2) token bias, where model prefers specific answer symbol (such as Option A). The combination of these biases causes the model to have a highly unbalanced label distribution that needs to be calibrated to reduce the variance. In addition, the biases are enhanced by the choice of prompt format and in-context examples [119, 127, 176].</p>
<p>On the other hand, calibrating the distribution to reduce the variance and sensitivity to prompts was found to not be effective, as the bias still persists [83, 191]. This leads to the hypothesis that the biases are not the sole cause of the highly unbalanced label distribution and the origin of randomness should be traced to something different, such as a combination of biases and relationships between the choices and distractor answers [191], or prior in-context examples [170].</p>
<p>The random sampling of examples for the in-context learning was determined to be the origin of randomness [75, 174] that can also cause problems observed when using a different order of examples in prompts. Choosing the samples randomly, without taking similarity or other heuristics into consideration, may result in many low-quality demonstrations being selected that may contain spurious correlations and may not provide enough information [75, 174]. When low-quality samples are used, different permutations and calibration of output distribution do not help reduce the variance, which disputes the claims in the previous works [174]. Choosing high-quality samples based on similarity or heuristics reduces the variance and allows different ordering and calibration to work [75, 174]. Therefore, the real origin of randomness, which can also lead to unbalanced distribution and sensitivity to order, is the choice of low-quality, dissimilar samples [75, 174].</p>
<p>Finally, it was observed that sensitivity to prompt format is affected by how frequently the prompt appears in some variation in the data during training [43].</p>
<h3>5.4 Overview of Findings: Origins of Randomness</h3>
<p>The observed origins of randomness can be summarised as follows:</p>
<ul>
<li>Poor choice of samples that are used for training, adaptation or in-context learning.</li>
<li>Overfitting that causes catastrophic forgetting, and models to focus on shallow features.</li>
<li>Under-specification, where multiple local minima with the same performance are present in training data, which are not consistent with testing data.</li>
<li>
<p>Highly unbalanced output label distribution stemming from biases present in language models and incorrect ordering of samples.</p>
</li>
<li>
<p>Optimisation problems caused by poor choices in the experimental setup, such as not using bias correction, training for a limited number of iterations or poor initialisation and re-use of some neural network layers.</p>
</li>
<li>Prompt format mainly caused by biases, such as how often the specific prompt and words in it are encountered during training.</li>
</ul>
<p>The individual origins of randomness are closely tied to the machine learning approach and the randomness factor investigated. This is mainly the consequence of the experiments that are designed to observe the behaviour of different approaches. Although many different origins are identified, all are tied to a small number of randomness factors, specifically a choice of samples, an order of the samples in training, a prompt format and model initialisation.</p>
<p>The popularity of different machine learning approaches and randomness factors is clearly shown in this task for addressing the effects of randomness. The focus on determining the origins of randomness in meta-learning is limited, while for prompting, in-context learning and their fine-tuning it is most popular.</p>
<p>The first effects of interactions between randomness factors are observed. In many cases, the different origins of randomness identified are later disputed and denoted as a simple consequence of other effects of randomness. For example, it is observed that the choice of samples in in-context learning causes problems that are often incorrectly attributed to the ordering of these samples. In addition, different randomness factors were found to enhance the sensitivity, such as prompt format affecting the biases in models that are a source of the sensitivity to data order.</p>
<p>This further shows the prevailing inconsistency of results from different papers. The compounding effects between different randomness factors and systematic choices cause significant problems when not properly addressed. Another aspect that plays a role is the simple evaluation, as the interactions and systematic choices are clearly shown when a more comprehensive evaluation is utilised.</p>
<h1>6 Mitigating the Randomness</h1>
<p>In the mitigation task, the effects of randomness are mitigated to reduce the variance as much as possible and to improve the overall performance using mitigation strategies. Mitigation represents the most popular task for addressing the effects of randomness (overall 112 papers). We divide these mitigation strategies into two separate groups: 1) general mitigation strategies; and 2) problem-specific mitigation strategies.</p>
<p>The general mitigation strategies can be used to mitigate the effects of randomness stemming from any origin of randomness and randomness factor. So far, the only way to deal with any origin of randomness is to use ensemble-like approaches (used in overall 13 papers), repeating the training and evaluation multiple times, each time with different randomness factor configuration, and then aggregating the predictions (using majority voting or weighted average). The different versions of the ensemble strategy were shown to provide effective mitigation for sample choice [116], model randomness and optimisation [39, 102, 138, 146], order of samples or answer choices [108, 191], the prompt format $[6,55,142]$ or data split $[96,182]$. In some cases, it is the only available solution for mitigating the randomness, i.e., when the randomness factor cannot be easily controlled, such as the effects of implementation of deep learning frameworks and hardware. However, it introduces a significant increase to computation costs [16]. Only some papers focus on reducing the cost of the ensemble strategy [102, 138].</p>
<p>On the other hand, the problem-specific mitigation strategies are designed to deal with a specific origin of randomness, making them dependent on the randomness factor and the origins of</p>
<p>randomness as analysed in Section 5, but also more popular (used in overall 99 papers). In addition, these approaches often provide benefits only for the specific setup (e.g., task type, dataset, or model).</p>
<h1>6.1 Poor Choice of Samples</h1>
<p>Addressing the sensitivity to the choice of samples is the most popular approach (overall 51 papers). Samples for different approaches are chosen using two approaches: 1) selecting representative samples for the whole process of training and evaluation; and 2) sampling data for the specific training and evaluation iteration. The first approach represents the real-world scenario where there is only a limited labelling budget available that needs to be effectively utilised by choosing samples with as much information as possible. For in-context learning this approach is designed to select a single set of examples that perform well for all test samples, instead of selecting separately for each test sample. In the second approach, the samples are selected for a specific training iteration (for fine-tuning) or for a specific test example (for in-context learning).
6.1.1 Selecting Representative Samples. When selecting a small number of representative samples $(&lt;100)$, strategies that work with unlabelled data are often used. This includes the use of a simple clustering [20, 27, 60, 151, 165]. To choose $K$ samples, the available data are clustered based on their similarity [20, 27], diversity [60, 151] or the pseudo-labels generated by a large language model [165] into $K$ clusters. Afterwards, samples are selected from each cluster either as the closest to the centre [20, 27], or based on weighting using uncertainty [165] or KL divergence [60].</p>
<p>Another possibility is to use active learning strategies to select the samples based on uncertainty, diversity or entropy [87, 157], or their combination with other properties such as complexity or quality [77, 88]. Which combination of properties to consider can also be determined dynamically based on the approach used [105]. The submodular functions are often used to optimise multiple properties at the same time [54, 114]. Another possibility to select the samples is to use reinforcement learning, similarly to active learning [174].</p>
<p>A popular strategy is to select samples based on their quality. To determine the quality of samples, one solution is to train a linear regression model that determines the gain in performance when the sample with given characteristics is included in the process [22, 56, 97, 122]. Another possibility is to prompt a large language model to rate each sample based on how it contributes [69, 74].</p>
<p>Another set of strategies selects samples in multiple steps. In each step, a separate sample property is optimised, such as first choosing the most diverse or informative samples and then refining them using their similarity to test sample or their quality [70, 137, 162, 163]. To achieve this, determinantal point processes are often used [162, 163], a large language model is prompted to provide quality/informativeness scores for the samples [70], or a graph-based approach is used [137].</p>
<p>Finally, specific mitigation strategies focus on selecting the samples, their order and the format of the prompt at the same time, using neural bandits [153].
6.1.2 Sampling Data for Training Iteration. When selecting in-context examples for a specific training or evaluation iteration, the most common approach is to use the similarity to test sample [3, 8, 24, 75]. The similarity selection is often combined with other properties, such as text characteristics [154, 167], entropy [1, 106] or margin [189]. Besides using different properties, a better representation can also be used, such as using large language models to transform each sample into how well it represents a certain skill [8].</p>
<p>Another possibility is to use adversarial training on the worst-case adaptation samples (found using greedy search) in meta-learning [2]. However, it provides no significant improvement in regards to stability for specific meta-learning approaches [2].</p>
<h1>6.2 Overfitting and Catastrophic Forgetting</h1>
<p>How the overfitting and catastrophic forgetting is mitigated is closely tied to the approach for which it is designed. Therefore, we provide further categorisation based on these approaches.
6.2.1 Overfitting in Meta-Learning. One solution to reduce the overfitting in meta-learning approaches is the augmentation of the support and query data, and the task construction [99]. Applying such augmentation increases the diversity of the samples and tasks, which improves the overall generalisation and reduces the sensitivity to the randomness [99].</p>
<p>Another solution is the use of variance reduction algorithms in the optimisation-based metalearning $[145,160]$. One possibility is to focus on first-order meta-learning algorithms and introduce the variance reduction only to them [145]. The variance reduction term is introduced to the gradient estimator in the task adaptation stage, motivated by the recursive momentum technique in [29]. This term is initialised by averaging the gradients from randomly sampled tasks using the initial parameters and then is further updated using a weighted sum of mini-batch stochastic gradients across all sampled tasks [145]. As this foregoes the benefits of the bi-level meta-learning process, other papers propose to modify the variance reduction term (STORM [29, 66]) for use on bilevel meta-learning optimisation, in combination with a large number of training steps to reduce the amount of storage required [160]. To achieve this, the variance-reduced gradient estimate is computed using stochastic gradients from two successive iterates while evaluating the gradient using the previous two iterates on the current batch of samples [160].
6.2.2 Overfitting in Language Model Fine-Tuning. The augmentation technique, in combination with regularisation, is also proposed when fine-tuning language models [90]. The pre-trained language model is used as a generator for new data samples used for augmentation. To guarantee that samples with discriminative labels are generated, a meta-weighted maximum likelihood objective can be used for the tuning of the generator. As the resulting labels still contain some level of noise, a noise-robust procedure for training the final model is proposed as regularisation using label smoothing and temporal ensembling [90].</p>
<p>Another solution is to look at the disconnection of the problem statement between the pretraining and fine-tuning stages. When the objective or even the input and output distributions are different, the fine-tuning process can often be brittle, especially when only a small amount of data is available [111]. A second pre-training step on data-rich supervised tasks can be used to better prepare the model for the fine-tuning stage, mitigating the brittleness and tendency to overfit on data [111].</p>
<p>The last set of approaches utilises regularisation to prevent overfitting. Injecting a Gaussian-like noise vector into the input, either before or during training, can be used as a stability regularisation, similar to augmentation [53, 150]. Another regularisation technique is to perform fine-tuning updates only on a sub-network instead of the whole network [156, 168, 169]. Instead of random selection, CHILD-TUNING, introduced in [156], uses Fisher Information to determine the importance of parameters based on all samples before training. Afterwards, an unchanged sub-network with the most important parameters is selected for the update. The previous two approaches can be improved using Dynamic Parameter Selection, introduced in [169], which adaptively selects a promising sub-network to perform the fine-tuning steps on. The sub-network is chosen dynamically, i.e., different sub-network is potentially fine-tuned at each training step, based on the importance of the parameters. It greatly reduced the heavy computation costs introduced by CHILD-TUNING [156] that separate the process of fine-tuning and the decision of which sub-network to optimise. In addition, it can better capture the task-relevant parameters across sub-networks of different sizes and bring stable improvement in various parts of the model. However, when faced with limited</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The digital appendix with a full list of papers is available at https://kinit.sk/public/acm-csur-sensitivity-survey.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
(c) 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.</p>
<p>ACM 1557-7341/2024/1-ART1
https://doi.org/10.1145/3691339&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
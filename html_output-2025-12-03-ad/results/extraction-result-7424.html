<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7424 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7424</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7424</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-264306479</p>
                <p><strong>Paper Title:</strong> <a href="http://medrxiv.org/cgi/content/short/2023.10.19.23297276v1?rss=1" target="_blank">Evaluating the use of GPT-3.5-turbo to provide clinical recommendations in the Emergency Department</a></p>
                <p><strong>Paper Abstract:</strong> The release of GPT-3.5-turbo (ChatGPT) and other large language models (LLMs) has the potential to transform healthcare. However, existing research evaluating LLM performance on real-world clinical notes is limited. Here, we conduct a highly-powered study to determine whether GPT-3.5-turbo can provide clinical recommendations for three tasks (admission status, radiological investigation(s) request status, and antibiotic prescription status) using clinical notes from the Emergency Department. We randomly select 10,000 Emergency Department visits to evaluate the accuracy of zero-shot, GPT-3.5-turbo-generated clinical recommendations across four different prompting strategies. We find that GPT-3.5-turbo performs poorly compared to a resident physician, with accuracy scores 24% lower on average. GPT-3.5-turbo tended to be overly cautious in its recommendations, with high sensitivity at the cost of specificity. Our findings demonstrate that, while early evaluations of the clinical use of LLMs are promising, LLM performance must be significantly improved before their deployment as decision support systems for clinical recommendations and other complex tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7424.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7424.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt A (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt A — simple binary instruction with no explanation allowed</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The initial zero-shot prompt asking the model to return whether the patient should receive a given clinical recommendation, with an instruction not to return additional explanation; used as the baseline prompt in all three tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based large language model from OpenAI used via Microsoft Azure API in a zero-shot setting to generate clinical recommendations from ED notes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Admission status (binary recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide whether the ED patient should be admitted to hospital based only on Presenting History and Physical Examination sections of the ED physician note.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt (explicit request for a binary label), with instruction to return no additional explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot single-question prompt. Prompt asked e.g. 'Please return whether the patient should be admitted to hospital' and instructed not to return additional explanation; balanced evaluation sample n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 1.00; specificity 0.07 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; GPT-3.5-turbo-0301 via Microsoft Azure (HIPAA-compliant API); balanced samples of 10,000 per task.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7424.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt B (conservative instruction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt B — add 'Only suggest clinical recommendation if absolutely required'</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modification of Prompt A that appended 'Only suggest *clinical recommendation* if absolutely required' to reduce false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Admission status (binary recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide whether the ED patient should be admitted to hospital from initial ED note sections.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt with added conservatism instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot prompt identical to Prompt A but with the sentence 'Only suggest clinical recommendation if absolutely required' appended; balanced n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 0.98; specificity 0.29 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt A — sensitivity 1.00; specificity 0.07</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>sensitivity -0.02 absolute; specificity +0.22 absolute (vs Prompt A)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; identical data and model settings to Prompt A.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7424.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt C (allow verbosity)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt C — remove restriction on explanations (allow output verbosity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt variant that removed the 'do not return any additional explanation' restriction so the model could produce explanations alongside its binary recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Admission status (binary recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide hospital admission from ED note sections while allowing the model to return explanatory text.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt permitting additional free-text explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; same core ask as Prompt A but explicitly allows additional explanation in output; balanced n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 0.94; specificity 0.35 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt A — sensitivity 1.00; specificity 0.07</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>sensitivity -0.06 absolute; specificity +0.28 absolute (vs Prompt A)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; explanation allowed to encourage more verbose reasoning before final label.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7424.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt D (zero-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt D — allow verbosity and add 'Let's think step by step' (zero-shot chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt variant permitting explanations and appending the zero-shot chain-of-thought cue 'Let's think step by step' intended to elicit intermediate reasoning before the final binary recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Admission status (binary recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide hospital admission from ED notes using zero-shot chain-of-thought prompting to elicit reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt with chain-of-thought cue ('Let's think step by step') and allowed explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot Chain-of-Thought (zeroshot-CoT) prompt; same input as Prompt C plus 'Let's think step by step'; balanced n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 0.92; specificity 0.37 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt A — sensitivity 1.00; specificity 0.07</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>sensitivity -0.08 absolute; specificity +0.30 absolute (vs Prompt A)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot CoT (added explicit chain-of-thought cue); model via Azure; balanced evaluation dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7424.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt A — Radiology task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt A — simple binary instruction (radiological investigation request)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline prompt (no explanation) used to ask whether radiological investigations should be requested.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Radiological investigation(s) request status</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide if an X-ray, ultrasound, CT, or MRI should be requested during the ED visit from initial note sections.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt without explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; prompt asked whether radiology should be requested and disallowed extra explanation; balanced n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 0.98; specificity 0.13 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; same model/version and dataset size as other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7424.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt B — Radiology task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt B — conservative instruction (radiology)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt A plus 'Only suggest clinical recommendation if absolutely required' applied to the radiology recommendation task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Radiological investigation(s) request status</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide if radiological imaging should be requested from ED notes with added conservative instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt with conservatism instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; added 'Only suggest clinical recommendation if absolutely required'; balanced n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 0.96; specificity 0.22 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt A — sensitivity 0.98; specificity 0.13</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>sensitivity -0.02 absolute; specificity +0.09 absolute (vs Prompt A)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; identical data/model settings.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7424.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt C — Radiology task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt C — allow verbosity (radiology)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Allowing explanations (removing restriction) for radiology recommendation; used as baseline to compare CoT effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Radiological investigation(s) request status</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide radiology requests allowing additional explanatory text from the model.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt permitting explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; explanation allowed; balanced n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 0.96; specificity 0.23 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt A — sensitivity 0.98; specificity 0.13</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>sensitivity -0.02 absolute; specificity +0.10 absolute (vs Prompt A)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; verbose output allowed.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7424.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt D — Radiology task (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt D — allow verbosity + 'Let's think step by step' (radiology)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot chain-of-thought cue applied to radiology recommendation; aimed to improve reasoning and reduce false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Radiological investigation(s) request status</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide radiology requests using a CoT cue to elicit stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt with zero-shot chain-of-thought cue and allowed explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot CoT ('Let's think step by step') with explanation allowed; balanced n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 0.96; specificity 0.20 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt A — sensitivity 0.98; specificity 0.13</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>sensitivity -0.02 absolute; specificity +0.07 absolute (vs Prompt A)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot CoT; same model and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7424.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt A — Antibiotic task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt A — simple binary instruction (antibiotic prescription)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline prompt (no explanation) used to ask whether antibiotics should be prescribed during the ED visit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Antibiotic prescription status</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide whether antibiotics should be ordered during the ED visit from initial note sections.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt without explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; prompt asked whether antibiotics should be prescribed; no additional explanation allowed; balanced n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 0.96; specificity 0.21 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; identical to other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7424.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt B — Antibiotic task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt B — conservative instruction (antibiotics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt A plus 'Only suggest clinical recommendation if absolutely required' applied to antibiotic prescription decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Antibiotic prescription status</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide antibiotic prescription with additional conservatism instruction to reduce false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt with conservatism instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; added 'Only suggest clinical recommendation if absolutely required'; balanced n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 0.94; specificity 0.26 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt A — sensitivity 0.96; specificity 0.21</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>sensitivity -0.02 absolute; specificity +0.05 absolute (vs Prompt A)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; same dataset and model.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7424.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt C — Antibiotic task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt C — allow verbosity (antibiotics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Removing the no-explanation restriction for antibiotic prescription recommendations, allowing the model to produce explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Antibiotic prescription status</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide antibiotic prescribing allowing free-text explanations from the model.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt permitting explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; explanation allowed; balanced n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 0.93; specificity 0.27 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt A — sensitivity 0.96; specificity 0.21</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>sensitivity -0.03 absolute; specificity +0.06 absolute (vs Prompt A)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; verbose outputs permitted.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7424.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt D — Antibiotic task (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt D — allow verbosity + 'Let's think step by step' (antibiotics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot chain-of-thought prompt for antibiotic prescription recommendation, intended to increase specificity by eliciting stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Antibiotic prescription status</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide antibiotic prescribing using a CoT cue to elicit intermediate reasoning and justification.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt with zero-shot chain-of-thought cue and allowed explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot CoT ('Let's think step by step') with explanation allowed; balanced n=10,000.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>sensitivity 0.91; specificity 0.32 (balanced n=10,000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt A — sensitivity 0.96; specificity 0.21</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>sensitivity -0.05 absolute; specificity +0.11 absolute (vs Prompt A)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot CoT; same model and dataset as other prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7424.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unbalanced real-world sample comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unbalanced n=1000 sample reflecting real-world distribution — physician vs GPT-3.5-turbo accuracy comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation on an unbalanced sample (n=1000) mirroring real-world outcome distributions, comparing physician accuracy to GPT-3.5-turbo across prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API; multiple prompt variants tested.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Admission / Radiology / Antibiotics (real-world distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same three binary recommendation tasks evaluated on an unbalanced sample representative of actual ED rates.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompts (Prompts A-D) applied to representative unbalanced sample.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality & prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Unbalanced sample (n=1000) constructed to mirror institutional distributions; physician manually labelled sample; GPT-3.5-turbo queried with same prompt variants as balanced experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (summative), reported sensitivity/specificity elsewhere</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Physician accuracy: admission 0.83, radiology 0.79, antibiotics 0.78; GPT-3.5-turbo accuracy ranges across prompts: admission 0.29–0.53, radiology 0.68–0.71, antibiotics 0.35–0.43 (unbalanced n=1000)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Physician accuracy used as real-world human baseline (0.83/0.79/0.78 per task)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>GPT average accuracy ~24% lower than physician on average across tasks (authors report '24% lower accuracy averaged across tasks')</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Unbalanced sample reflective of real-world rates; resident physician labelled the sample for human vs machine comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7424.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7424.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label-order sensitivity analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity analysis on label ordering in prompt (positive before negative vs reversed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tested whether the ordering of labels (e.g., '0: Patient should not...' vs '1: Patient should...') in the prompt affects GPT-3.5-turbo outputs due to LLM stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM used zero-shot via Azure API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Admission / Radiology / Antibiotics (sensitivity analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess whether reordering label enumeration in the prompt changes model sensitivity/specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language binary prompt with explicit numbered labels; label order reversed in sensitivity analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Balanced n=200 subsample per outcome; compared prompts where positive outcome was referenced before the negative outcome vs vice versa.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sensitivity & specificity (comparative)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Results largely identical across orderings for most tasks; antibiotic task showed improved specificity for Prompts B-D when reversing label order but with reduced sensitivity (no exact numeric values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original label ordering as used in main experiments (Prompt A ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Antibiotic task: specificity increased for Prompts B-D when positive label listed first, at the cost of decreased sensitivity (absolute magnitudes not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Balanced subsample n=200 per task; stochasticity of LLM outputs considered.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Models are Zero-Shot Reasoners <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing <em>(Rating: 2)</em></li>
                <li>Automatic Chain of Thought Prompting in Large Language Models <em>(Rating: 1)</em></li>
                <li>Assessing clinical acuity in the Emergency Department using the GPT-3.5 Artificial Intelligence Model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7424",
    "paper_id": "paper-264306479",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Prompt A (baseline)",
            "name_full": "Prompt A — simple binary instruction with no explanation allowed",
            "brief_description": "The initial zero-shot prompt asking the model to return whether the patient should receive a given clinical recommendation, with an instruction not to return additional explanation; used as the baseline prompt in all three tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based large language model from OpenAI used via Microsoft Azure API in a zero-shot setting to generate clinical recommendations from ED notes.",
            "model_size": null,
            "task_name": "Admission status (binary recommendation)",
            "task_description": "Decide whether the ED patient should be admitted to hospital based only on Presenting History and Physical Examination sections of the ED physician note.",
            "problem_format": "Natural-language binary prompt (explicit request for a binary label), with instruction to return no additional explanation.",
            "format_category": "prompt style",
            "format_details": "Zero-shot single-question prompt. Prompt asked e.g. 'Please return whether the patient should be admitted to hospital' and instructed not to return additional explanation; balanced evaluation sample n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 1.00; specificity 0.07 (balanced n=10,000)",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Zero-shot; GPT-3.5-turbo-0301 via Microsoft Azure (HIPAA-compliant API); balanced samples of 10,000 per task.",
            "statistical_significance": null,
            "uuid": "e7424.0"
        },
        {
            "name_short": "Prompt B (conservative instruction)",
            "name_full": "Prompt B — add 'Only suggest clinical recommendation if absolutely required'",
            "brief_description": "A modification of Prompt A that appended 'Only suggest *clinical recommendation* if absolutely required' to reduce false positives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Admission status (binary recommendation)",
            "task_description": "Decide whether the ED patient should be admitted to hospital from initial ED note sections.",
            "problem_format": "Natural-language binary prompt with added conservatism instruction.",
            "format_category": "prompt style",
            "format_details": "Zero-shot prompt identical to Prompt A but with the sentence 'Only suggest clinical recommendation if absolutely required' appended; balanced n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 0.98; specificity 0.29 (balanced n=10,000)",
            "baseline_performance": "Prompt A — sensitivity 1.00; specificity 0.07",
            "performance_change": "sensitivity -0.02 absolute; specificity +0.22 absolute (vs Prompt A)",
            "experimental_setting": "Zero-shot; identical data and model settings to Prompt A.",
            "statistical_significance": null,
            "uuid": "e7424.1"
        },
        {
            "name_short": "Prompt C (allow verbosity)",
            "name_full": "Prompt C — remove restriction on explanations (allow output verbosity)",
            "brief_description": "Prompt variant that removed the 'do not return any additional explanation' restriction so the model could produce explanations alongside its binary recommendation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Admission status (binary recommendation)",
            "task_description": "Decide hospital admission from ED note sections while allowing the model to return explanatory text.",
            "problem_format": "Natural-language binary prompt permitting additional free-text explanation.",
            "format_category": "prompt style",
            "format_details": "Zero-shot; same core ask as Prompt A but explicitly allows additional explanation in output; balanced n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 0.94; specificity 0.35 (balanced n=10,000)",
            "baseline_performance": "Prompt A — sensitivity 1.00; specificity 0.07",
            "performance_change": "sensitivity -0.06 absolute; specificity +0.28 absolute (vs Prompt A)",
            "experimental_setting": "Zero-shot; explanation allowed to encourage more verbose reasoning before final label.",
            "statistical_significance": null,
            "uuid": "e7424.2"
        },
        {
            "name_short": "Prompt D (zero-shot CoT)",
            "name_full": "Prompt D — allow verbosity and add 'Let's think step by step' (zero-shot chain-of-thought)",
            "brief_description": "Prompt variant permitting explanations and appending the zero-shot chain-of-thought cue 'Let's think step by step' intended to elicit intermediate reasoning before the final binary recommendation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Admission status (binary recommendation)",
            "task_description": "Decide hospital admission from ED notes using zero-shot chain-of-thought prompting to elicit reasoning.",
            "problem_format": "Natural-language binary prompt with chain-of-thought cue ('Let's think step by step') and allowed explanation.",
            "format_category": "prompt style",
            "format_details": "Zero-shot Chain-of-Thought (zeroshot-CoT) prompt; same input as Prompt C plus 'Let's think step by step'; balanced n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 0.92; specificity 0.37 (balanced n=10,000)",
            "baseline_performance": "Prompt A — sensitivity 1.00; specificity 0.07",
            "performance_change": "sensitivity -0.08 absolute; specificity +0.30 absolute (vs Prompt A)",
            "experimental_setting": "Zero-shot CoT (added explicit chain-of-thought cue); model via Azure; balanced evaluation dataset.",
            "statistical_significance": null,
            "uuid": "e7424.3"
        },
        {
            "name_short": "Prompt A — Radiology task",
            "name_full": "Prompt A — simple binary instruction (radiological investigation request)",
            "brief_description": "Baseline prompt (no explanation) used to ask whether radiological investigations should be requested.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Radiological investigation(s) request status",
            "task_description": "Decide if an X-ray, ultrasound, CT, or MRI should be requested during the ED visit from initial note sections.",
            "problem_format": "Natural-language binary prompt without explanation.",
            "format_category": "prompt style",
            "format_details": "Zero-shot; prompt asked whether radiology should be requested and disallowed extra explanation; balanced n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 0.98; specificity 0.13 (balanced n=10,000)",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Zero-shot; same model/version and dataset size as other tasks.",
            "statistical_significance": null,
            "uuid": "e7424.4"
        },
        {
            "name_short": "Prompt B — Radiology task",
            "name_full": "Prompt B — conservative instruction (radiology)",
            "brief_description": "Prompt A plus 'Only suggest clinical recommendation if absolutely required' applied to the radiology recommendation task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Radiological investigation(s) request status",
            "task_description": "Decide if radiological imaging should be requested from ED notes with added conservative instruction.",
            "problem_format": "Natural-language binary prompt with conservatism instruction.",
            "format_category": "prompt style",
            "format_details": "Zero-shot; added 'Only suggest clinical recommendation if absolutely required'; balanced n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 0.96; specificity 0.22 (balanced n=10,000)",
            "baseline_performance": "Prompt A — sensitivity 0.98; specificity 0.13",
            "performance_change": "sensitivity -0.02 absolute; specificity +0.09 absolute (vs Prompt A)",
            "experimental_setting": "Zero-shot; identical data/model settings.",
            "statistical_significance": null,
            "uuid": "e7424.5"
        },
        {
            "name_short": "Prompt C — Radiology task",
            "name_full": "Prompt C — allow verbosity (radiology)",
            "brief_description": "Allowing explanations (removing restriction) for radiology recommendation; used as baseline to compare CoT effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Radiological investigation(s) request status",
            "task_description": "Decide radiology requests allowing additional explanatory text from the model.",
            "problem_format": "Natural-language binary prompt permitting explanation.",
            "format_category": "prompt style",
            "format_details": "Zero-shot; explanation allowed; balanced n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 0.96; specificity 0.23 (balanced n=10,000)",
            "baseline_performance": "Prompt A — sensitivity 0.98; specificity 0.13",
            "performance_change": "sensitivity -0.02 absolute; specificity +0.10 absolute (vs Prompt A)",
            "experimental_setting": "Zero-shot; verbose output allowed.",
            "statistical_significance": null,
            "uuid": "e7424.6"
        },
        {
            "name_short": "Prompt D — Radiology task (CoT)",
            "name_full": "Prompt D — allow verbosity + 'Let's think step by step' (radiology)",
            "brief_description": "Zero-shot chain-of-thought cue applied to radiology recommendation; aimed to improve reasoning and reduce false positives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Radiological investigation(s) request status",
            "task_description": "Decide radiology requests using a CoT cue to elicit stepwise reasoning.",
            "problem_format": "Natural-language binary prompt with zero-shot chain-of-thought cue and allowed explanation.",
            "format_category": "prompt style",
            "format_details": "Zero-shot CoT ('Let's think step by step') with explanation allowed; balanced n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 0.96; specificity 0.20 (balanced n=10,000)",
            "baseline_performance": "Prompt A — sensitivity 0.98; specificity 0.13",
            "performance_change": "sensitivity -0.02 absolute; specificity +0.07 absolute (vs Prompt A)",
            "experimental_setting": "Zero-shot CoT; same model and dataset.",
            "statistical_significance": null,
            "uuid": "e7424.7"
        },
        {
            "name_short": "Prompt A — Antibiotic task",
            "name_full": "Prompt A — simple binary instruction (antibiotic prescription)",
            "brief_description": "Baseline prompt (no explanation) used to ask whether antibiotics should be prescribed during the ED visit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Antibiotic prescription status",
            "task_description": "Decide whether antibiotics should be ordered during the ED visit from initial note sections.",
            "problem_format": "Natural-language binary prompt without explanation.",
            "format_category": "prompt style",
            "format_details": "Zero-shot; prompt asked whether antibiotics should be prescribed; no additional explanation allowed; balanced n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 0.96; specificity 0.21 (balanced n=10,000)",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Zero-shot; identical to other tasks.",
            "statistical_significance": null,
            "uuid": "e7424.8"
        },
        {
            "name_short": "Prompt B — Antibiotic task",
            "name_full": "Prompt B — conservative instruction (antibiotics)",
            "brief_description": "Prompt A plus 'Only suggest clinical recommendation if absolutely required' applied to antibiotic prescription decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Antibiotic prescription status",
            "task_description": "Decide antibiotic prescription with additional conservatism instruction to reduce false positives.",
            "problem_format": "Natural-language binary prompt with conservatism instruction.",
            "format_category": "prompt style",
            "format_details": "Zero-shot; added 'Only suggest clinical recommendation if absolutely required'; balanced n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 0.94; specificity 0.26 (balanced n=10,000)",
            "baseline_performance": "Prompt A — sensitivity 0.96; specificity 0.21",
            "performance_change": "sensitivity -0.02 absolute; specificity +0.05 absolute (vs Prompt A)",
            "experimental_setting": "Zero-shot; same dataset and model.",
            "statistical_significance": null,
            "uuid": "e7424.9"
        },
        {
            "name_short": "Prompt C — Antibiotic task",
            "name_full": "Prompt C — allow verbosity (antibiotics)",
            "brief_description": "Removing the no-explanation restriction for antibiotic prescription recommendations, allowing the model to produce explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Antibiotic prescription status",
            "task_description": "Decide antibiotic prescribing allowing free-text explanations from the model.",
            "problem_format": "Natural-language binary prompt permitting explanation.",
            "format_category": "prompt style",
            "format_details": "Zero-shot; explanation allowed; balanced n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 0.93; specificity 0.27 (balanced n=10,000)",
            "baseline_performance": "Prompt A — sensitivity 0.96; specificity 0.21",
            "performance_change": "sensitivity -0.03 absolute; specificity +0.06 absolute (vs Prompt A)",
            "experimental_setting": "Zero-shot; verbose outputs permitted.",
            "statistical_significance": null,
            "uuid": "e7424.10"
        },
        {
            "name_short": "Prompt D — Antibiotic task (CoT)",
            "name_full": "Prompt D — allow verbosity + 'Let's think step by step' (antibiotics)",
            "brief_description": "Zero-shot chain-of-thought prompt for antibiotic prescription recommendation, intended to increase specificity by eliciting stepwise reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Antibiotic prescription status",
            "task_description": "Decide antibiotic prescribing using a CoT cue to elicit intermediate reasoning and justification.",
            "problem_format": "Natural-language binary prompt with zero-shot chain-of-thought cue and allowed explanation.",
            "format_category": "prompt style",
            "format_details": "Zero-shot CoT ('Let's think step by step') with explanation allowed; balanced n=10,000.",
            "performance_metric": "sensitivity & specificity",
            "performance_value": "sensitivity 0.91; specificity 0.32 (balanced n=10,000)",
            "baseline_performance": "Prompt A — sensitivity 0.96; specificity 0.21",
            "performance_change": "sensitivity -0.05 absolute; specificity +0.11 absolute (vs Prompt A)",
            "experimental_setting": "Zero-shot CoT; same model and dataset as other prompts.",
            "statistical_significance": null,
            "uuid": "e7424.11"
        },
        {
            "name_short": "Unbalanced real-world sample comparison",
            "name_full": "Unbalanced n=1000 sample reflecting real-world distribution — physician vs GPT-3.5-turbo accuracy comparison",
            "brief_description": "Evaluation on an unbalanced sample (n=1000) mirroring real-world outcome distributions, comparing physician accuracy to GPT-3.5-turbo across prompt variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API; multiple prompt variants tested.",
            "model_size": null,
            "task_name": "Admission / Radiology / Antibiotics (real-world distribution)",
            "task_description": "Same three binary recommendation tasks evaluated on an unbalanced sample representative of actual ED rates.",
            "problem_format": "Natural-language binary prompts (Prompts A-D) applied to representative unbalanced sample.",
            "format_category": "input modality & prompt style",
            "format_details": "Unbalanced sample (n=1000) constructed to mirror institutional distributions; physician manually labelled sample; GPT-3.5-turbo queried with same prompt variants as balanced experiments.",
            "performance_metric": "accuracy (summative), reported sensitivity/specificity elsewhere",
            "performance_value": "Physician accuracy: admission 0.83, radiology 0.79, antibiotics 0.78; GPT-3.5-turbo accuracy ranges across prompts: admission 0.29–0.53, radiology 0.68–0.71, antibiotics 0.35–0.43 (unbalanced n=1000)",
            "baseline_performance": "Physician accuracy used as real-world human baseline (0.83/0.79/0.78 per task)",
            "performance_change": "GPT average accuracy ~24% lower than physician on average across tasks (authors report '24% lower accuracy averaged across tasks')",
            "experimental_setting": "Unbalanced sample reflective of real-world rates; resident physician labelled the sample for human vs machine comparison.",
            "statistical_significance": null,
            "uuid": "e7424.12"
        },
        {
            "name_short": "Label-order sensitivity analysis",
            "name_full": "Sensitivity analysis on label ordering in prompt (positive before negative vs reversed)",
            "brief_description": "Tested whether the ordering of labels (e.g., '0: Patient should not...' vs '1: Patient should...') in the prompt affects GPT-3.5-turbo outputs due to LLM stochasticity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (gpt-3.5-turbo-0301)",
            "model_description": "Chat-based LLM used zero-shot via Azure API.",
            "model_size": null,
            "task_name": "Admission / Radiology / Antibiotics (sensitivity analysis)",
            "task_description": "Assess whether reordering label enumeration in the prompt changes model sensitivity/specificity.",
            "problem_format": "Natural-language binary prompt with explicit numbered labels; label order reversed in sensitivity analysis.",
            "format_category": "prompt style (ordering)",
            "format_details": "Balanced n=200 subsample per outcome; compared prompts where positive outcome was referenced before the negative outcome vs vice versa.",
            "performance_metric": "sensitivity & specificity (comparative)",
            "performance_value": "Results largely identical across orderings for most tasks; antibiotic task showed improved specificity for Prompts B-D when reversing label order but with reduced sensitivity (no exact numeric values reported).",
            "baseline_performance": "Original label ordering as used in main experiments (Prompt A ordering)",
            "performance_change": "Antibiotic task: specificity increased for Prompts B-D when positive label listed first, at the cost of decreased sensitivity (absolute magnitudes not reported).",
            "experimental_setting": "Balanced subsample n=200 per task; stochasticity of LLM outputs considered.",
            "statistical_significance": null,
            "uuid": "e7424.13"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Models are Zero-Shot Reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
            "rating": 2,
            "sanitized_title": "pretrain_prompt_and_predict_a_systematic_survey_of_prompting_methods_in_natural_language_processing"
        },
        {
            "paper_title": "Automatic Chain of Thought Prompting in Large Language Models",
            "rating": 1,
            "sanitized_title": "automatic_chain_of_thought_prompting_in_large_language_models"
        },
        {
            "paper_title": "Assessing clinical acuity in the Emergency Department using the GPT-3.5 Artificial Intelligence Model",
            "rating": 1,
            "sanitized_title": "assessing_clinical_acuity_in_the_emergency_department_using_the_gpt35_artificial_intelligence_model"
        }
    ],
    "cost": 0.015761999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating the use of GPT-3.5-turbo to provide clinical recommendations in the Emergency Department</p>
<p>MB BChirChristopher Y K Williams 
Bakar Computational Health Sciences Institute
University of California
San Francisco, San FranciscoCAUSA</p>
<p>Brenda Y Miao 
Bakar Computational Health Sciences Institute
University of California
San Francisco, San FranciscoCAUSA</p>
<p>MD, PhDAtul J Butte 
Bakar Computational Health Sciences Institute
University of California
San Francisco, San FranciscoCAUSA</p>
<p>Postdoctoral Scholar
Bakar Computational Health Sciences Institute
UCSF</p>
<p>Evaluating the use of GPT-3.5-turbo to provide clinical recommendations in the Emergency Department
957DC29E237B7440C531DE4D0321E7E710.1101/2023.10.19.23297276
The release of GPT-3.5-turbo(ChatGPT) and other large language models (LLMs) has the potential to transform healthcare.However, existing research evaluating LLM performance on real-world clinical notes is limited.Here, we conduct a highly-powered study to determine whether GPT-3.5-turbo can provide clinical recommendations for three tasks (admission status, radiological investigation(s) request status, and antibiotic prescription status) using clinical notes from the Emergency Department.We randomly select 10,000 Emergency Department visits to evaluate the accuracy of zero-shot, GPT-3.5-turbo-generatedclinical recommendations across four different prompting strategies.We find that GPT-3.5-turboperforms poorly compared to a resident physician, with accuracy scores 24% lower on average.GPT-3.5-turbotended to be overly cautious in its recommendations, with high sensitivity at the cost of specificity.Our findings demonstrate that, while early evaluations of the clinical use of LLMs are promising, LLM performance must be significantly improved before their deployment as decision support systems for clinical recommendations and other complex tasks..</p>
<p>Introduction</p>
<p>Since its November 2022 launch, the Chat Generative Pre-Trained Transformer (ChatGPT; GPT-3.5-turbo) has captured widespread public attention, with media reports suggesting over 100 million monthly active users just two months after launch. 1 Along with its successor, GPT-4, these large language models (LLMs) use a chat-based interface to respond to complex queries and solve problems. 2,3Although trained as general-purpose models, researchers have begun evaluating the performance of GPT-3.5-turbo and GPT-4 on clinically-relevant tasks.For instance, GPT-3.5-turbo was found to provide largely appropriate responses when asked to give simple cardiovascular disease prevention recommendations. 4Meanwhile, GPT-3.5-turboresponses to patients' health questions on a public social media forum were both preferred, and rated as having higher empathy, compared to physician responses. 5there are a growing number of studies that explore the uses of the GPT models across a range of clinical tasks, the majority do not use real-world clinical notes.7][8][9] This is due to the challenges associated with disclosing protected health information (PHI) with LLM providers such as OpenAI in a Health Insurance Portability and Accountability Act (HIPAA) compliant manner, where business associate agreements must be in place to allow secure processing of PHI content. 10This is a notable hurdle given the inherent differences between curated medical datasets, such as the USMLE question bank, and real-world clinical notes.In addition, this issue is particularly problematic when you consider that the GPT models have likely been trained on data obtained from open sources on the Internet and therefore their evaluation on existing publicly available benchmarks or tasks may be confounded by data leakage. 11 availability and accessibility of these models increases, it is now critically important to better understand the potential uses and limitations of LLMs applied to actual clinical notes.</p>
<p>In our previous work, we showed that GPT-3.5-turbo could accurately identify the higher acuity patient when provided only the clinical histories of pairs of patients presenting to the Emergency Department. 12This was despite a lack of additional training or fine-tuning, known as zero-shot learning. 13Elsewhere, Kanjee and colleagues evaluated the diagnostic ability of GPT-4 across 70 cases from the NEJM clinicopathologic conferences, obtaining a correct diagnosis in its differential in 64% of cases and as its top diagnosis in 39%. 7ver, the ability of these general-purpose large language models to assimilate clinical information from de-identified clinical notes and return clinical recommendations is still unclear.</p>
<p>In this study, we sought to evaluate the zero-shot performance of GPT-3.5-turbo when prompted to provide clinical recommendations for patients evaluated in the Emergency Department.We focus on three recommendations in particular: 1) Should the patient be admitted to hospital; 2) Should the patient have radiological investigations requested; and 3) Should the patient receive antibiotics?We first evaluate performance on balanced (i.e equal numbers of positive and negative outcomes) datasets, to examine the sensitivity and specificity of GPT recommendations, before determining overall model accuracy on an unbalanced dataset that reflects real-world distributions of patients presenting to the Emergency Department.</p>
<p>.</p>
<p>Results</p>
<p>From a total of 251,401 adult Emergency Department visits, we first created balanced samples of 10,000 ED visits for each of the three tasks (Figure 1).Using only the information provided in the Presenting History and Physical Examination sections of patients' first ED physician note, we queried GPT-3.5-turbo to determine whether 1) the patient should be admitted to hospital, 2) the patient requires radiological investigation(s), and 3) the patient requires antibiotics, comparing its output to the ground-truth outcome extracted from the electronic health record.Across all three clinical recommendation tasks, overall GPT-3.5-turboperformance was poor (Table 1).The initial prompt of 'Please return whether the patient should be admitted to hospital / requires radiological investigation / requires antibiotics' (Prompt A) led to high sensitivity and low specificity performance.For this prompt, GPT-3.5-turborecommendations had a high true positive rate but similarly high false positive rate, with GPT-3.5-turborecommending admission / radiological investigation / antibiotic prescription for the majority of cases.Altering the prompt to 'only suggest … if absolutely required' (Prompt B) only marginally improved specificity.The greatest performance was achieved by removing restrictions on the verbosity of GPT-3.5-turboresponse (Prompt C) and adding the 'Let's think step by step' chain-of-thought prompting (Prompt D).These prompts generated the highest specificity in GPT-3.5-turborecommendations with limited effect on sensitivity.</p>
<p>To compare this performance with that of a resident physician, we took a balanced n = 200 subsample for manual annotation and compared performance between physician and machine across each of the four prompt iterations (Table 2).Notably, physician sensitivity was below .that of GPT-3.5-turboresponses (0.73 vs [range: 0.93-1.00],0.76 vs [range: 0.93-0.96]and 0.64 vs [range: 0.89-0.93]for admission, radiological investigation, and antibiotic prescription tasks, respectively), but specificity was significantly higher than GPT-3.5-turbo(0.74 vs [range: 0.07-0.40],0.79 vs [range: 0.09-0.17]and 0.78 vs [range: 0.26-0.37]).</p>
<p>We next sought to test the performance of GPT-3.5-turbo in a more representative setting using an unbalanced, n = 1000 sample of ED visits that reflects the real-world distribution of admission, radiological investigation, and antibiotic prescription rates at our institution (Table 3).We found that the accuracy of resident physician recommendations, when evaluated against the ground-truth outcomes extracted from the electronic health record, was significantly higher than GPT-3.5-turborecommendations: 0.83 for physician vs [range: 0.29-0.53for GPT-3.5-turbo],0.79 vs [range: 0.68-0.71]and 0.78 vs [range: 0.35-0.43]for admission, radiological investigation, and antibiotic prescription tasks, respectively (Figure 2; Table 3).</p>
<p>Lastly, in our sensitivity analyses conducted on a balanced, n = 200 subsample for each task, results were largely similar regardless of the written order of labels in the original prompt (e.g '0: Patient should be admitted to hospital.1: Patient should not be admitted to hospital.'vs '1: Patient should be admitted to hospital.0: Patient should not be admitted to hospital.')(Table S3).Reversing the order of labels in the original prompt led to almost identical results for all tasks except the antibiotic prescription task, where specificity was improved for Prompts 2-4, but at the cost of sensitivity.</p>
<p>Discussion</p>
<p>This study represents an early, highly powered evaluation of the potential uses and limitations of GPT-3.5-turbo for generating clinical recommendations based on real-world clinical text.</p>
<p>Across three different clinical recommendation tasks, we found that GPT-3.5-turboperformed poorly, with high sensitivity but low specificity across tasks.Model performance was marginally improved with iterations of prompt engineering, including the addition of zero-shot chain-of-thought prompting. 14On evaluation of an unbalanced (n = 1000) sample reflective of the real-world distribution of clinical recommendations, GPT-3.5-turboperformance was significantly worse than that of a resident physician, with 24% lower accuracy averaged across tasks.</p>
<p>Our results suggest that GPT-3.5-turbo is overly cautious in its clinical recommendations -it exhibits a tendency to recommend intervention for each of the three tasks and this leads to a notable number of false positive suggestions.Such a finding is problematic given the need to both prioritize hospital resource availability and reduce overall healthcare costs. 15,16This is also true at the patient level, where there is an increasing appreciation that excessive investigation and/or treatment may cause patients harm. 16It is unclear, however, what is the best balance of sensitivity/specificity to strive for amongst clinical large language models -it is likely that this balance will differ based on the particular task.The increase in GPT-3.5turbospecificity, at the expense of sensitivity, across our iterations of prompt engineering suggests that improvements could be made bespoke to the task, though the extent to which prompt engineering alone may improve performance is unclear.Across all three tasks, overall performance remained notably below that of a human physician.This may reflect the inherent complexity of clinical decision making, where .clinical recommendations may be influenced not only by the patient's intrinsic clinical status, but also by patient preference, current resource availability and other external factors.</p>
<p>Before large language models can be integrated within the clinical environment, it is important to fully understand both their capabilities and limitations.Otherwise, there is a risk of unintended harmful consequences, especially if models have been deployed at scale. 17,18ent research deploying large language models, particularly the current state-of-the-art GPT models, on real-world clinical text is limited.Recent work from our group has demonstrated accurate performance of GPT-3.5-turbo in both assessing patient clinical acuity in the Emergency Department and extracting detailed oncologic history and treatment plans from medical oncology notes. 191][22] Much of the current literature focuses on the strengths of large language models such as GPT-3.5-turboand GPT-4. 3,9,12,19However, it is equally important to identify areas of medicine in which LLMs do not perform well.For example, in one evaluation of GPT-4's ability to diagnose dementia from a set of structured features, GPT-4 did not surpass the performance of traditional AI tools, while fewer than 20% of GPT-3.5-turbo and GPT-4 responses submitted to a clinical informatics consult service were found to be concordant with existing reports. 23,24While early signs of the utility of large language models in medicine are promising, our findings suggest that there remains significant room for improvement, especially in more challenging tasks such as complex clinical decision making.</p>
<p>This study has several limitations.Firstly, it is possible that, for each task, not all the information which led to the real-life clinical recommendation extracted from the electronic health record was present in the Presenting History and Physical Examination sections of the ED physician note.For instance, radiological investigations requested following the Emergency Medicine physician review may lead to unexpected and/or incidental findings which were not detected during the initial review and may warrant admission or antibiotic prescription.However, even with this limitation, physician classification performance remained a very respectable 78-83% accuracy across the three tasks, suggesting it is challenging, but not impossible, to make accurate clinical recommendations based on the available clinical text.Secondly, we only trialled three iterations of prompt engineering, in addition to our initial prompt, and this was done in a zero-shot manner.6][27] Lastly, this study did not evaluate the performance of the recently released, more advanced GPT-4 model.It is possible that GPT-4 performance may surpass that of GPT-3.5-turbo in these more complex reasoning tasks, though the ability to test this at a similar scale is limited by the increased costs associated with GPT-4 usage across a sample of this size.Similarly, evaluation of the performance of other natural language processing models, such as a fine-tuned BioClinicalBERT model or bag-of-wordbased and other simpler techniques, has not been performed. 28It is possible that these more traditional NLP models, which are typically trained or fine-tuned on a large training set of data, may outperform the zero-shot performance of GPT-like large language models. 21</p>
<p>Methods</p>
<p>The UCSF Information Commons contains deidentified structured clinical data as well as deidentified clinical text notes, deidentified and externally certified as previously described. 29UCSF Institutional Review Board determined that this use of the deidentified data within the UCSF Information Commons is not human participants research and therefore was exempt from further approval and informed consent.</p>
<p>We identified all adult visits to the University of California San Francisco (UCSF) Emergency Department (ED) from 2012 to 2023 with an ED Physician note present within Information Commons (Figure 1).Regular expressions were used to extract the Presenting History (consisting of 'Chief Complaint', 'History of Presenting Illness' and 'Review of Systems') and Physical Examination sections from each note (Supplementary File 1).</p>
<p>We sought to evaluate GPT-3.5-turboperformance on three binary clinical recommendation tasks, corresponding to the following outcomes: 1) Admission status -whether the patient should be admitted from ED to hospital.2) Radiological investigation(s) request statuswhether an X-ray, US scan, CT scan, or MRI scan should be requested during the ED visit.3) Antibiotic prescription status -whether antibiotics should be ordered during the ED visit.</p>
<p>For each of the three outcomes, we randomly selected a balanced sample of 10,000 ED visits to evaluate GPT-3.5-turboperformance (Figure 1).Using its secure, HIPAA-compliant Application Programming Interface (API) through Microsoft Azure, we provided GPT-3.5turbo(model gpt-3.5-turbo-0301) the Presenting History and Physical Examination sections of the ED Physician's note for each ED visit and queried it to determine if 1) the patient should be admitted to hospital, 2) the patient requires radiological investigation, and 3) the patient should be prescribed antibiotics.GPT-3.5-turboperformance was evaluated against the ground-truth outcome extracted from the electronic health record.Separately, a resident blinded to both the GPT-3.5-turbolabels and ground-truth labels reviewed a balanced n = 200 subsample for each of the three tasks to allow a comparison of human and machine performance.The following evaluation metrics were calculated: true positive rate, true negative rate, false positive rate, false negative rate, sensitivity and specificity.</p>
<p>We subsequently experimented with three iterations of prompt engineering (Table S1, Supplementary File 1) to test if modifications to the initial prompt could improve GPT-3.5turboperformance.Chain-of-thought (CoT) prompting is a method found to improve the ability of large language models to perform complex reasoning by decomposing multi-step problems into a series of intermediate steps. 25This can be done in a zero-shot manner (zeroshot-CoT), with large language models shown to be decent zero-shot reasoners by adding a simple prompt, 'Let's think step by step' to facilitate step-by-step reasoning before answering each question. 14Alternatively, few-shot chain-of-thought prompting can be used, with additional examples of prompt and answer pairs either manually (manual CoT) or computationally (e.g auto-CoT) provided and concatenated with the prompt of interest. 25,26ent understanding of the impact of zero-shot-CoT, manual CoT, and auto-CoT prompt engineering techniques applied to clinical text is limited.In this work, we sought to focus on zero-shot-CoT and investigate the effect of adding 'Let's think step by step' to the prompt on model performance.</p>
<p>Our initial prompt (Prompt A) simply asked GPT-3.5-turbo to return whether the patient should be e.g.admitted to hospital, without any additional explanation.We additionally attempted to engineer prompts to a) reduce the high false positive rate of GPT-3.5-turbo</p>
<p>Sensitivity analysis</p>
<p>Due to the stochastic nature of large language models, it is possible that the order of labels reported in the original prompt may affect the subsequent labels returned.To test this, we conducted a sensitivity analysis on a balanced n = 200 subsample for each outcome where the positive outcome was referenced before the negative outcome in the initial prompt (e.g.'1:</p>
<p>Patient should be admitted to hospital' precedes '0: Patient should not be admitted to hospital' in the GPT-3.5-turboprompt).Table 1.GPT-3.5-turbo performance across four iterations of prompt engineering (Prompt A-D) evaluated on a balanced n = 10,000 sample for three clinical recommendation tasks: 1) Should the patient be admitted to hospital; 2) Does the patient require radiological investigation; and 3) Does the patient require antibiotics.</p>
<p>Tables</p>
<p>FiguresFigure 1 .
1
Figures</p>
<p>Figure 1.Flowchart of included Emergency Department visits and construction of both</p>
<p>Figure 2 .
2
Figure 2. Evaluation of physician and GPT-3.5-turboaccuracy across four iterations of</p>
<p>Table 2 .Table 3 .
23
Comparison of physician and GPT-3.5-turboperformance across four iterations of prompt engineering [Prompt A-D] evaluated on a balanced n = 200 subsample for three clinical recommendation tasks: 1) Should the patient be admitted to hospital; 2) Does the patient require radiological investigation; and 3) Does the patient require antibiotics.<em>Physicians were provided the same prompt text as in Prompt A. .Comparison of physician and GPT-3.5-turboperformance across four iterations of prompt engineering [Prompt A-D] evaluated on an unbalanced n = 1000 sample reflective of the real-world distribution of clinical recommendations among patients presenting to ED, for the following three clinical recommendation tasks: 1) Should the patient be admitted to hospital; 2) Does the patient require radiological investigation; and 3) Does the patient require antibiotics.</em>Physicians were provided the same prompt text as in Prompt A.</p>
<p>343Figure 1 .
1
Figure 1.Flowchart of included Emergency Department visits and construction of both balanced (n = 10,000 samples) and unbalanced (n = 1000 sample reflecting the real-world distribution of patients presenting to the Emergency Department) datasets for the following outcomes: 1) Admission status, 2) Radiological investigation(s) status, and 3) Antibiotic prescription status .CC-BY 4.0 International license It is made available under a</p>
<p>Table 1 .
1
GPT-3.5-turbo performance across four iterations of prompt engineering (Prompt A-
TablesTaskTrueFalseTrueFalseSensitivity Specificitypositives,positives,negatives,Negatives,n (%)n (%)n (%)n (%)1) AdmissionPrompt A 4994 (49.9) 4639 (46.4) 361 (3.6)6 (0.1)1.000.07statusPrompt B 4904 (49)3527 (35.3) 1473 (14.7) 96 (1)0.980.29Prompt C 4683 (46.8) 3255 (32.6) 1745 (17.5) 317 (3.2)0.940.35Prompt D 4617 (46.2) 3165 (31.7) 1835 (18.4) 383 (3.8)0.920.372) RadiologicalPrompt A 4922 (49.2) 4361 (43.6) 639 (6.4)78 (0.8)0.980.13investigation(s) request statusPrompt B 4805 (48.1) 3906 (39.1) 1094 (10.9) 195 (2)0.960.22Prompt C 4792 (47.9) 3855 (38.6) 1145 (11.5) 208 (2.1)0.960.23Prompt D 4819 (48.2) 3991 (39.9) 1009 (10.1) 181 (1.8)0.960.203) AntibioticPrompt A 4812 (48.1) 3955 (39.6) 1045 (10.5) 188 (1.9)0.960.21prescription statusPrompt B 4690 (46.9) 3687 (36.9) 1313 (13.1) 310 (3.1)0.940.26Prompt C 4658 (46.6) 3639 (36.4) 1361 (13.6) 342 (3.4)0.930.27Prompt D 4544 (45.4) 3379 (33.8) 1621 (16.2) 456 (4.6)0.910.32D) evaluated on a balanced n = 10,000 sample for three clinical recommendation tasks: 1)Should the patient be admitted to hospital; 2) Does the patient require radiologicalinvestigation; and 3) Does the patient require antibiotics.
.</p>
<p>Table 2 .
2
Comparison of physician and GPT-3.5-turboperformance across four iterations of
341342
337 prompt engineering [Prompt A-D] evaluated on a balanced n = 200 subsample for three 338 clinical recommendation tasks: 1) Should the patient be admitted to hospital; 2) Does the 339 patient require radiological investigation; and 3) Does the patient require antibiotics.340 *Physicians were provided the same prompt text as in Prompt A.</p>
<p>Table 3 .
3
Comparison of physician and GPT-3.5-turboperformance across four iterations of</p>
<p>AcknowledgementsThe authors acknowledge the use of the UCSF Information Commons computational research platform, developed and supported by UCSF Bakar Computational Health Sciences Institute.The authors also thank the UCSF AI Tiger Team, Academic Research Services, Research Information Technology, and the Chancellor's Task Force for Generative AI for their software development, analytical and technical support related to the use of Versa API gateway (the UCSF secure implementation of large language models and generative AI via API gateway), Versa chat (the chat user interface), and related data asset and services.Apple, Meta (Facebook), Alphabet (Google), Microsoft, Amazon, Snap, 10x Genomics, Illumina, Regeneron, Sanofi, Pfizer, Royalty Pharma, Moderna, Sutro, Doximity, BioNtech, Invitae, Pacific Biosciences, Editas Medicine, Nuna Health, Assay Depot, and Vet24seven, and several other non-health related companies and mutual funds; and has received honoraria and travel reimbursement for invited talks from Johnson and Johnson, Roche, Genentech, Pfizer, Merck, Lilly, Takeda, Varian, Mars, Siemens, Optum, Abbott, Celgene, AstraZeneca, AbbVie, Westat, and many academic institutions, medical or disease specific foundations and associations, and health systems.AJB receives royalty payments through Stanford University, for several patents and other disclosures licensed to NuMedii and Personalis.AJB's research has been funded by NIH, Peraton (as the prime on an NIH contract), Genentech, Johnson and Johnson, FDA, Robert Wood Johnson Foundation, Leon Lowenstein Foundation, Intervalien Foundation, Priscilla Chan and Mark Zuckerberg, the Barbara and Gerson Bakar Foundation, and in the recent past, the March of Dimes, Juvenile Diabetes Research Foundation, California Governor's Office of Planning and Research, California Institute for Regenerative Medicine, L'Oreal, and Progenity.None of these entities had any bearing on the design of this study or the writing of the manuscript.Data availabilityThe code accompanying this manuscript is available at https://github.com/cykwilliams/GPT-3.5-Clinical-Recommendations-in-Emergency-Department/.recommendations (Prompt B) and b) examine whether zero-shot chain-of-thought prompting could improve GPT-3.5-turboperformance (Prompts C and D).Attempting to reduce the high GPT-3.5-turbofalse positive rate, Prompt B was constructed by adding an additional sentence to Prompt A: 'Only suggest <em>clinical recommendation</em> if absolutely required'.This modification was kept for Prompts C and D, which were constructed to examine chain-ofthought prompting.Because chain-of-thought prompting is most effective when the LLM provides reasoning in its output, we removed the instruction 'Please do not return any additional explanation' from Prompts C and D, and added the chain-of-thought prompt 'Let's think step by step' to Prompt D, increasing GPT-3.5-turboresponse verbosity (TableS2, Supplementary File 1).Prompt C therefore served as a baseline for comparison of GPT-3.5turboperformance when it is permitted to return additional explanation (in addition to its outcome recommendation), allowing comparisons with both Prompt A (where no additional explanations were allowed in the prompt) and Prompt D (where the effect of chain-of-thought prompting was examined).To evaluate the performance of GPT-3.5-turbo in a real-world setting, we constructed a random, unbalanced sample of 1000 ED visits where the distribution of patient outcomes (i.e.admission status, radiological investigation(s) request status and antibiotic prescription status) mirrored the distributions of patients presenting to ED from our main cohort.The Presenting History and Physical Examination sections of the ED Physician's note for each ED visit were again passed to the GPT-3.5-turboAPI in an identical manner to the balanced datasets, while a resident physician manually labelled the entire sample to allow human vs machine comparison.Classification accuracy was calculated in addition to the aforementioned evaluation metrics utilised for the balanced datasets to provide a summative evaluation metric for this real-world simulated task..Conflicts of InterestCYKW has no conflicts of interest to disclose.BYM is a paid consultant for SandboxAQ.AJB is a co-founder and consultant to Personalis and NuMedii; consultant to Mango Tree Corporation, and in the recent past, Samsung, 10x Genomics, Helix, Pathway Genomics, and Verinata (Illumina); has served on paid advisory panels or boards for Geisinger Health, Regenstrief Institute, Gerson Lehman Group, AlphaSights, Covance, Novartis, Genentech, and Merck, and Roche; is a shareholder in Personalis and NuMedii; is a minor shareholder in
ChatGPT sets record for fastest-growing user base -analyst note. K Hu, K Hu, </p>
<p>. 10.48550/arXiv.2303.08774272023OpenAIGPT-4 Technical ReportPublished online March</p>
<p>Appropriateness of Cardiovascular Disease Prevention Recommendations Obtained From a Popular Online Chat-Based Artificial Intelligence Model. A Sarraju, D Bruemmer, E Van Iterson, L Cho, F Rodriguez, L Laffin, 10.1001/jama.2023.1044JAMA. 329102023</p>
<p>Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum. J W Ayers, A Poliak, M Dredze, 10.1001/jamainternmed.2023.1838JAMA Intern Med. 18362023</p>
<p>Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, 10.1371/journal.pdig.0000198PLOS Digit Health. 22e00001982023</p>
<p>Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge. Z Kanjee, B Crowe, A Rodman, 10.1001/jama.2023.8288JAMA. 33012023</p>
<p>Towards Expert-Level Medical Question Answering with Large Language Models. K Singhal, T Tu, J Gottweis, 10.48550/arXiv.2305.09617162023Published online May</p>
<p>Capabilities of GPT-4 on Medical Challenge Problems. H Nori, N King, S M Mckinney, D Carignan, E Horvitz, 10.48550/arXiv.2303.13375April 122023Published online</p>
<p>Health Care Privacy Risks of AI Chatbots. G P Kanter, E A Packel, 10.1001/jama.2023.9618JAMA. 33042023</p>
<p>Limits, and Risks of GPT-4 as an AI Chatbot for Medicine. P Lee, S Bubeck, J Petro, Benefits, 10.1056/NEJMsr2214184N Engl J Med. 388132023</p>
<p>Assessing clinical acuity in the Emergency Department using the GPT-3.5 Artificial Intelligence Model. Cyk Williams, T Zack, B Y Miao, M Sushil, M Wang, A J Butte, 10.1101/2023.08.09.2329379513Published online August</p>
<p>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, 10.1145/3560815ACM Comput Surv. 559352023</p>
<p>Large Language Models are Zero-Shot Reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, 10.48550/arXiv.2205.11916January 292023Published online</p>
<p>Setting healthcare priorities in hospitals: a review of empirical studies. E W Barasa, S Molyneux, M English, S Cleary, 10.1093/heapol/czu010Health Policy Plan. 3032015</p>
<p>The Next Frontier of Less Is More-From Description to Implementation. N Latifi, R F Redberg, D Grady, 10.1001/jamainternmed.2021.6908JAMA Intern Med. 18222022</p>
<p>External Validation of a Widely Implemented Proprietary Sepsis Prediction Model in Hospitalized Patients. A Wong, E Otles, J P Donnelly, 10.1001/jamainternmed.2021.2626JAMA Intern Med. 18182021</p>
<p>The Epic Sepsis Model Falls Short-The Importance of External Validation. A R Habib, A L Lin, R W Grant, 10.1001/jamainternmed.2021.3333JAMA Intern Med. 18182021</p>
<p>Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models. M Sushil, V E Kennedy, B Y Miao, D Mandair, T Zack, A J Butte, 10.48550/arXiv.2308.03853August 7, 2023</p>
<p>Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. Q Lyu, J Tan, M E Zapadka, 10.48550/arXiv.2303.09038282023Published online March</p>
<p>Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification. S Chen, Y Li, S Lu, 10.48550/arXiv.2304.02496April 5, 2023Published online</p>
<p>The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant. J Zhang, K Sun, A Jagadeesh, 10.48550/arXiv.2307.08152July 162023Published online</p>
<p>Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today. Z Wang, R Li, B Dong, 10.48550/arXiv.2306.01499June 2, 2023Published online</p>
<p>Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. D Dash, R Thapa, J M Banda, 10.48550/arXiv.2304.13714April 30, 2023Published online</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, January 28, 2022. August 7, 2023</p>
<p>Automatic Chain of Thought Prompting in Large Language Models. Z Zhang, A Zhang, M Li, A Smola, 10.48550/arXiv.2210.03493October 7, 2022Published online</p>
<p>Language Models are Few-Shot Learners. T B Brown, B Mann, N Ryder, 10.48550/arXiv.2005.14165July 222020Published online</p>
<p>. E Alsentzer, J R Murphy, W Boag, Publicly Available Clinical BERT Embeddings. arXiv.org. Published. April 6, 2019. May 13, 2023</p>
<p>A certified de-identification system for all clinical text documents for information extraction at scale. L Radhakrishnan, G Schenk, K Muenzen, 10.1093/jamiaopen/ooad045JAMIA Open. 63452023</p>            </div>
        </div>

    </div>
</body>
</html>
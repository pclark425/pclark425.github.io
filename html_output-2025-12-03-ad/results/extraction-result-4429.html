<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4429 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4429</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4429</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-278789422</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16349v1.pdf" target="_blank">Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization</a></p>
                <p><strong>Paper Abstract:</strong> The exponential growth of scientific publications has made it increasingly difficult for researchers to stay updated and synthesize knowledge effectively. This paper presents XSum, a modular pipeline for multi-document summarization (MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The pipeline includes two core components: a question-generation module and an editor module. The question-generation module dynamically generates questions adapted to the input papers, ensuring the retrieval of relevant and accurate information. The editor module synthesizes the retrieved content into coherent and well-structured summaries that adhere to academic standards for proper citation. Evaluated on the SurveySum dataset, XSum demonstrates strong performance, achieving considerable improvements in metrics such as CheckEval, G-Eval and Ref-F1 compared to existing approaches. This work provides a transparent, adaptable framework for scientific summarization with potential applications in a wide range of domains. Code available at https://github.com/webis-de/scolia25-xsum</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4429.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4429.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XSum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XSum (Ask, Retrieve, Summarize) pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular Retrieval-Augmented Generation (RAG) pipeline for multi-document scientific summarization that uses an LLM to generate document-driven questions, embedding-based retrieval with reranking, LLM question-answering grounded in retrieved chunks, and an LLM-based editor to synthesize citation-rich summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>XSum</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>XSum is a modular RAG pipeline with four main components: (1) Document Pre-processing: full texts are chunked (150-token chunks with 20-token overlap, sentence-boundary aware), embedded with SPECTER2, and indexed into a FAISS vector database; (2) Question Generation: an LLM generates ~5 broad semantically-rich questions per reference paper from its title and abstract; (3) Question Answering (RAG): each question is embedded (SPECTER2), the top 100 chunks are retrieved by cosine similarity from FAISS, re-ranked with ColBERT2 to select the top 20 chunks, which are then presented with the question to an LLM that produces an answer grounded in retrieved context and including citations; (4) Editor Module (Final Summary Generation): an LLM ingests all question-answer pairs and synthesizes a coherent, citation-rich survey-style summary according to academic standards.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt4o-mini_15-2-2024-preview (generation/editor); Phi-3-small-8k-instruct used by authors for evaluation (evaluator); (paper also references use of pre-trained LLMs generally for question generation and editing)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Question-driven embedding retrieval: chunking + SPECTER2 embeddings stored in FAISS, cosine-similarity retrieval (top 100) followed by ColBERT2 token-interaction reranking to top 20; LLM-driven question answering grounded on retrieved chunks (RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Two-step synthesis: per-question LLM answers (grounded) followed by an Editor LLM that aggregates question-answer pairs into a unified, citation-rich multi-document summary (single-step aggregation of answers with editorial prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on SurveySum where each target section cites on average 7.38 papers; pipeline assumes a predefined set of reference papers (per-section inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature (evaluated on SurveySum: AI/NLP/ML survey sections).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Citation-rich multi-document survey-section summaries (abstractive summaries grounded in retrieved chunks).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1/2/L (recall), BERTScore, Reference F1 (Ref-F1) for citation alignment, G-Eval (LLM-based reference-free evaluation), CheckEval (LLM-based checklist evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On SurveySum (Table 1): ROUGE-1 = 0.51, ROUGE-2 = 0.10, ROUGE-L = 0.24, BERTScore = 0.62, Ref-F1 = 0.76, G-Eval = 4.2 (scale used by G-Eval), CheckEval = 0.97.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>SurveySum benchmark pipelines (Pipeline_1: monoT5-3B ranking + LLM summarizer; Pipeline_2: SPECTER2 embedding retrieval + LLM reranking + summarizer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>XSum outperforms both SurveySum Pipeline_1 and Pipeline_2 across reported metrics: e.g., ROUGE-1: XSum 0.51 vs Pipeline_2 0.49 and Pipeline_1 0.42; Ref-F1: XSum 0.76 vs Pipeline_2 0.72 and Pipeline_1 0.64; G-Eval: XSum 4.2 vs Pipeline_2 4.0 and Pipeline_1 3.1; CheckEval: XSum 0.97 vs Pipeline_2 0.76 and Pipeline_1 0.61.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dynamically generated, document-specific questions improve retrieval relevance over static queries (e.g., section titles); reranking with ColBERT2 refines retrieval; editor LLM produces more coherent, citation-rich summaries; RAG + question-driven retrieval achieves higher citation alignment (Ref-F1) and stronger LLM-based quality scores (G-Eval, CheckEval).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Does not address large-scale deployment or open-document discovery (assumes predefined reference set); lacks deep qualitative analysis; generated summaries can be verbose and less selective than human-written survey text; evaluation results depend on choice and configuration of LLM evaluators (reproducibility concerns); computational/storage scaling and retrieval ingestion are open challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>No empirical scaling study reported; evaluated on SurveySum (avg ~7.38 papers/section). Authors note need for future work on optimizing data ingestion and scaling to industrial-sized corpora and integrating multimodal (vision-language) retrieval for figures/tables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4429.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4429.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pipeline_1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SurveySum Pipeline 1 (monoT5-3B ranking + LLM summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SurveySum baseline pipeline that segments full texts into overlapping chunks, ranks chunks with a neural ranker (monoT5-3B) using the section title as query, and passes top-ranked chunks to an LLM (e.g., GPT-4) for summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Surveysum: A dataset for summarizing multiple scientific articles into a survey section.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SurveySum Pipeline 1</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Full-text papers are segmented into overlapping chunks during preprocessing. A neural ranker (monoT5-3B) scores chunks for relevance to the target survey section title. The highest-ranked chunks are selected and passed to a large LLM (the SurveySum description references use of LLMs such as GPT-4) to generate the final section summary.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLM used for final summarization (SurveySum paper cites use of GPT-4 as an example); ranker is monoT5-3B (neural ranking model).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Neural chunk ranking (monoT5-3B) using the section title as the query over chunked full texts.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based abstractive summarization of the top-ranked chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on SurveySum; each target section references on average 7.38 papers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature (SurveySum benchmark: AI/NLP/ML survey sections).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey-section summaries (abstractive).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1/2/L, BERTScore, Ref-F1, G-Eval, CheckEval (as applied in this paper for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported (this paper's re-evaluation): ROUGE-1 = 0.42, ROUGE-2 = 0.08, ROUGE-L = 0.19, BERTScore = 0.57, Ref-F1 = 0.64, G-Eval = 3.1, CheckEval = 0.61.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Pipeline_2 (SurveySum) and XSum in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Per reported metrics, Pipeline_1 underperforms Pipeline_2 and XSum on most metrics (e.g., ROUGE-1 0.42 vs Pipeline_2 0.49 and XSum 0.51).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Neural ranking using section titles is effective but limited by using static section-title queries, which may reduce retrieval adaptability and relevance compared to document-driven queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on section titles as static queries (limits adaptability); no explicit reranking with token-level interactions is described in the Pipeline_1 description here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>No scaling analysis reported in this paper for Pipeline_1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4429.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4429.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pipeline_2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SurveySum Pipeline 2 (SPECTER2 embeddings + FAISS + LLM reranking/summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SurveySum baseline that encodes chunks as dense SPECTER2 embeddings stored in FAISS, retrieves chunks using the section title, applies an LLM-based reranking step, and then summarizes the re-ranked chunks with an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Surveysum: A dataset for summarizing multiple scientific articles into a survey section.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SurveySum Pipeline 2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Documents are chunked and encoded into dense SPECTER2 embeddings and indexed in FAISS. At inference, the section title is used as the query to retrieve relevant chunks; an LLM then re-ranks/evaluates the retrieved candidates before a final LLM produces the summary of the selected chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLM used for reranking and summarization (unspecified in SurveySum summary here; SurveySum examples mention use of LLMs such as GPT-4 for summarization/re-ranking in practice).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval (SPECTER2 embeddings in FAISS) using section-title queries, followed by an LLM re-ranking step.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based summarization of top re-ranked chunks (abstractive summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on SurveySum; average ~7.38 cited papers per target section.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature (AI/NLP/ML, SurveySum dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey-section summaries (abstractive).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1/2/L, BERTScore, Ref-F1, G-Eval, CheckEval (as used in this paper's comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported (this paper's re-evaluation): ROUGE-1 = 0.49, ROUGE-2 = 0.10, ROUGE-L = 0.23, BERTScore = 0.59, Ref-F1 = 0.72, G-Eval = 4.0, CheckEval = 0.76.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Pipeline_1 and XSum.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Pipeline_2 generally outperforms Pipeline_1 but is outperformed by XSum on the reported metrics; e.g., ROUGE-1 0.49 (Pipeline_2) vs 0.51 (XSum).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Embedding-based retrieval with an LLM reranker yields stronger performance than simple title-based neural ranking; however, using static section titles as queries limits adaptability compared to dynamic question generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Section titles as static queries can limit retrieval relevance; the approach depends on embedding quality and reranker consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>No explicit scaling trends reported in this paper for Pipeline_2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4429.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4429.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenScholar: Synthesizing scientific literature with retrieval-augmented LMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale retrieval-augmented system that integrates a specialized datastore of ~45 million papers with iterative retrieval and feedback loops to produce precise, citation-backed responses with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openscholar: Synthesizing scientific literature with retrieval-augmented lms.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as integrating a large specialized datastore (â‰ˆ45M papers) with iterative retrieval and feedback loops; the system uses retrieval-augmented language models to generate citation-backed, precise responses by iteratively retrieving, synthesizing, and refining evidence from the datastore.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (OpenScholar referenced as related work; original OpenScholar paper should be consulted for exact LLMs used).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Large-scale retrieval from a specialized datastore (document indexing + vector retrieval) with iterative retrieval-feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-augmented generation with iterative refinement/feedback producing citation-backed answers/summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Datastore reported as ~45 million papers (per the referenced OpenScholar description).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature (large-scale corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Citation-backed responses/summaries synthesized from many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper's brief mention; see OpenScholar original paper for evaluation details.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported here (only mentioned as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates viability of large specialized datastores combined with iterative retrieval/feedback to produce precise, citation-backed outputs at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Implied challenges: scale of storage/compute for 45M-paper datastore, iterative retrieval latency/cost; details not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed for large-scale (45M papers) operation; cited as an example of large-corpus RAG scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4429.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4429.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HyQE/HyDe (generative retrieval helpers)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HyQE / HyDe / reverse HyDe (hypothetical query/document/question generation approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative techniques that produce hypothetical queries or document-like content (e.g., hypothetical query embeddings or hypothetical document embeddings) to bridge the semantic gap between user queries and retrievable content, improving retrieval ranking for downstream RAG systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hyqe: Ranking contexts with hypothetical query embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HyQE / HyDe / reverse HyDe (grouped generative retrieval methods)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>These approaches use generative models to produce hypothetical queries or document-like representations from either queries or documents; the generated text is encoded as embeddings (hypothetical query/document embeddings) and used to improve retrieval ranking, thereby increasing chance that relevant contexts are retrieved for LLM grounding. The paper cites HyDe, HyQE and reverse HyDe as inspirations for XSum's question-generation module.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Described as using generative techniques / pre-trained generative models (exact model names not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Generate hypothetical queries or documents with a generative model, encode these into embeddings, and use them to query or re-rank document chunks (improved embedding-based retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Primarily retrieval-improvement rather than direct synthesis; used upstream of RAG to provide better retrieved contexts for subsequent LLM synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here (method is retrieval-level and orthogonal to number of papers processed).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Information retrieval for summarization / multi-document summarization contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Improved retrieval contexts (indirectly enabling better summaries or answers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper's mention (see original HyQE/HyDe works).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported here; paper cites these works conceptually as improving retrieval relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in source works to standard embedding or query methods (not shown here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Claimed in cited works to improve retrieval ranking by bridging semantic gaps; no quantitative details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generative hypothetical queries/documents can bridge semantic mismatches between queries and document representations, improving retrieval relevance for RAG pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>This paper does not detail limitations of HyQE/HyDe; general issues include reliance on quality of generated hypotheticals and potential introduction of generation noise if hypotheticals are poor.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4429.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4429.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ShindeHybrid</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shinde et al. hybrid extractive-abstractive pipeline (BERT + BigBird-PEGASUS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid two-stage pipeline that uses BERT-based extractive models to select salient passages and BigBird-PEGASUS (a pre-trained transformer) for abstractive multi-document summarization, shown to be effective in the biomedical domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An extractive-abstractive approach for multi-document summarization of scientific articles for literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Shinde et al. hybrid pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>First stage: BERT-based extractive models identify and select relevant passages from scientific articles. Second stage: BigBird-PEGASUS (a PLM specialized for long inputs) is used to produce abstractive summaries from selected passages, thereby combining robust extraction with powerful abstractive generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BigBird-PEGASUS (abstractive PLM) for summarization; BERT (extractive stage) for passage selection.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>BERT-based extractive selection of salient passages from multiple documents.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Abstractive summarization using BigBird-PEGASUS over the extracted passages (two-stage extract-then-generate).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper's mention (targeted at biomedical multi-document summarization datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical multi-document summarization (literature review summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive summaries for literature reviews / multi-document summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported in the original Shinde et al. work (not detailed here); this paper notes 'robust performance in the biomedical domain.'</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not quantified in this paper; original work reports improved performance for biomedical summarization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in original work to purely extractive or purely abstractive baselines (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported as achieving robust performance in the biomedical domain (no numbers provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining extractive selection and long-input-capable abstractive PLMs can yield robust multi-document summarization in specialized domains like biomedicine.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not expanded here; typical concerns include dependency on extraction quality and PLM input-length/cost constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4429.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4429.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general approach that combines information retrieval (vector or index-based) with LLM generation to ground outputs in retrieved source documents, reducing hallucination and improving factual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RAG pipelines: documents are chunked and embedded into vector space (or indexed); a retrieval step selects top-k relevant chunks for a given query; an LLM then conditions on the retrieved chunks to generate answers or summaries grounded in retrieved evidence. In XSum, RAG is implemented with SPECTER2 embeddings, FAISS retrieval, ColBERT2 reranking, and LLM-based grounded answer generation followed by an LLM editor for final summary synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Varies by implementation. In this paper's XSum: gpt4o-mini_15-2-2024-preview as generator/editor; other pipelines referenced use GPT-4 or unspecified LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval (document chunking + dense embeddings + FAISS) optionally followed by token-level reranking (ColBERT2).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM generation conditioned on retrieved contexts; optional multi-step aggregation (e.g., per-question answers then editorial synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Varies by application; in SurveySum evaluations used on sets averaging 7.38 reference papers per section; OpenScholar example applies RAG over a 45M-paper datastore.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature and domain-specific corpora; used here for AI/NLP/ML survey summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded answers, citation-rich summaries, and other generated artefacts based on retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE, BERTScore, reference-citation metrics (Ref-F1), and LLM-based evaluators like G-Eval and CheckEval are used in this paper to assess RAG pipeline outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>RAG-based pipelines (including XSum) produced strong semantic and citation-alignment metrics in this study; XSum (a RAG pipeline) yielded ROUGE-1 = 0.51, Ref-F1 = 0.76, G-Eval = 4.2 as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to static retrieval or purely extractive/abstractive architectures (SurveySum Pipelines 1 & 2 and hybrid baselines cited).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>RAG approaches (and XSum in particular) achieved improved G-Eval/CheckEval and Ref-F1 versus the referenced SurveySum pipelines; RAG favors semantic grounding over strict lexical overlap (lower ROUGE-2 common).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG provides strong factual grounding and citation alignment when retrieval quality is high; dynamic, document-driven queries (e.g., generated questions) further improve retrieval relevance; evaluation is sensitive to the choice/configuration of LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Performance depends on retrieval quality; potential for verbosity and inclusion of excessive background detail; issues of hallucination if retrieval lacks coverage; scaling and latency/cost for very large corpora are open challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>RAG can scale with vector indices and datastores but requires attention to data ingestion, latency, and storage; explicit scaling experiments are not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Surveysum: A dataset for summarizing multiple scientific articles into a survey section. <em>(Rating: 2)</em></li>
                <li>Openscholar: Synthesizing scientific literature with retrieval-augmented lms. <em>(Rating: 2)</em></li>
                <li>Hyqe: Ranking contexts with hypothetical query embeddings. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for large language models: A survey. <em>(Rating: 2)</em></li>
                <li>An extractive-abstractive approach for multi-document summarization of scientific articles for literature review. <em>(Rating: 1)</em></li>
                <li>Colbertv2: Effective and efficient retrieval via lightweight late interaction. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4429",
    "paper_id": "paper-278789422",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "XSum",
            "name_full": "XSum (Ask, Retrieve, Summarize) pipeline",
            "brief_description": "A modular Retrieval-Augmented Generation (RAG) pipeline for multi-document scientific summarization that uses an LLM to generate document-driven questions, embedding-based retrieval with reranking, LLM question-answering grounded in retrieved chunks, and an LLM-based editor to synthesize citation-rich summaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "XSum",
            "system_description": "XSum is a modular RAG pipeline with four main components: (1) Document Pre-processing: full texts are chunked (150-token chunks with 20-token overlap, sentence-boundary aware), embedded with SPECTER2, and indexed into a FAISS vector database; (2) Question Generation: an LLM generates ~5 broad semantically-rich questions per reference paper from its title and abstract; (3) Question Answering (RAG): each question is embedded (SPECTER2), the top 100 chunks are retrieved by cosine similarity from FAISS, re-ranked with ColBERT2 to select the top 20 chunks, which are then presented with the question to an LLM that produces an answer grounded in retrieved context and including citations; (4) Editor Module (Final Summary Generation): an LLM ingests all question-answer pairs and synthesizes a coherent, citation-rich survey-style summary according to academic standards.",
            "llm_model_used": "gpt4o-mini_15-2-2024-preview (generation/editor); Phi-3-small-8k-instruct used by authors for evaluation (evaluator); (paper also references use of pre-trained LLMs generally for question generation and editing)",
            "extraction_technique": "Question-driven embedding retrieval: chunking + SPECTER2 embeddings stored in FAISS, cosine-similarity retrieval (top 100) followed by ColBERT2 token-interaction reranking to top 20; LLM-driven question answering grounded on retrieved chunks (RAG).",
            "synthesis_technique": "Two-step synthesis: per-question LLM answers (grounded) followed by an Editor LLM that aggregates question-answer pairs into a unified, citation-rich multi-document summary (single-step aggregation of answers with editorial prompt).",
            "number_of_papers": "Evaluated on SurveySum where each target section cites on average 7.38 papers; pipeline assumes a predefined set of reference papers (per-section inputs).",
            "domain_or_topic": "Scientific literature (evaluated on SurveySum: AI/NLP/ML survey sections).",
            "output_type": "Citation-rich multi-document survey-section summaries (abstractive summaries grounded in retrieved chunks).",
            "evaluation_metrics": "ROUGE-1/2/L (recall), BERTScore, Reference F1 (Ref-F1) for citation alignment, G-Eval (LLM-based reference-free evaluation), CheckEval (LLM-based checklist evaluation).",
            "performance_results": "On SurveySum (Table 1): ROUGE-1 = 0.51, ROUGE-2 = 0.10, ROUGE-L = 0.24, BERTScore = 0.62, Ref-F1 = 0.76, G-Eval = 4.2 (scale used by G-Eval), CheckEval = 0.97.",
            "comparison_baseline": "SurveySum benchmark pipelines (Pipeline_1: monoT5-3B ranking + LLM summarizer; Pipeline_2: SPECTER2 embedding retrieval + LLM reranking + summarizer).",
            "performance_vs_baseline": "XSum outperforms both SurveySum Pipeline_1 and Pipeline_2 across reported metrics: e.g., ROUGE-1: XSum 0.51 vs Pipeline_2 0.49 and Pipeline_1 0.42; Ref-F1: XSum 0.76 vs Pipeline_2 0.72 and Pipeline_1 0.64; G-Eval: XSum 4.2 vs Pipeline_2 4.0 and Pipeline_1 3.1; CheckEval: XSum 0.97 vs Pipeline_2 0.76 and Pipeline_1 0.61.",
            "key_findings": "Dynamically generated, document-specific questions improve retrieval relevance over static queries (e.g., section titles); reranking with ColBERT2 refines retrieval; editor LLM produces more coherent, citation-rich summaries; RAG + question-driven retrieval achieves higher citation alignment (Ref-F1) and stronger LLM-based quality scores (G-Eval, CheckEval).",
            "limitations_challenges": "Does not address large-scale deployment or open-document discovery (assumes predefined reference set); lacks deep qualitative analysis; generated summaries can be verbose and less selective than human-written survey text; evaluation results depend on choice and configuration of LLM evaluators (reproducibility concerns); computational/storage scaling and retrieval ingestion are open challenges.",
            "scaling_behavior": "No empirical scaling study reported; evaluated on SurveySum (avg ~7.38 papers/section). Authors note need for future work on optimizing data ingestion and scaling to industrial-sized corpora and integrating multimodal (vision-language) retrieval for figures/tables.",
            "uuid": "e4429.0",
            "source_info": {
                "paper_title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Pipeline_1",
            "name_full": "SurveySum Pipeline 1 (monoT5-3B ranking + LLM summarization)",
            "brief_description": "A SurveySum baseline pipeline that segments full texts into overlapping chunks, ranks chunks with a neural ranker (monoT5-3B) using the section title as query, and passes top-ranked chunks to an LLM (e.g., GPT-4) for summarization.",
            "citation_title": "Surveysum: A dataset for summarizing multiple scientific articles into a survey section.",
            "mention_or_use": "use",
            "system_name": "SurveySum Pipeline 1",
            "system_description": "Full-text papers are segmented into overlapping chunks during preprocessing. A neural ranker (monoT5-3B) scores chunks for relevance to the target survey section title. The highest-ranked chunks are selected and passed to a large LLM (the SurveySum description references use of LLMs such as GPT-4) to generate the final section summary.",
            "llm_model_used": "LLM used for final summarization (SurveySum paper cites use of GPT-4 as an example); ranker is monoT5-3B (neural ranking model).",
            "extraction_technique": "Neural chunk ranking (monoT5-3B) using the section title as the query over chunked full texts.",
            "synthesis_technique": "LLM-based abstractive summarization of the top-ranked chunks.",
            "number_of_papers": "Evaluated on SurveySum; each target section references on average 7.38 papers.",
            "domain_or_topic": "Scientific literature (SurveySum benchmark: AI/NLP/ML survey sections).",
            "output_type": "Survey-section summaries (abstractive).",
            "evaluation_metrics": "ROUGE-1/2/L, BERTScore, Ref-F1, G-Eval, CheckEval (as applied in this paper for comparison).",
            "performance_results": "Reported (this paper's re-evaluation): ROUGE-1 = 0.42, ROUGE-2 = 0.08, ROUGE-L = 0.19, BERTScore = 0.57, Ref-F1 = 0.64, G-Eval = 3.1, CheckEval = 0.61.",
            "comparison_baseline": "Compared against Pipeline_2 (SurveySum) and XSum in this work.",
            "performance_vs_baseline": "Per reported metrics, Pipeline_1 underperforms Pipeline_2 and XSum on most metrics (e.g., ROUGE-1 0.42 vs Pipeline_2 0.49 and XSum 0.51).",
            "key_findings": "Neural ranking using section titles is effective but limited by using static section-title queries, which may reduce retrieval adaptability and relevance compared to document-driven queries.",
            "limitations_challenges": "Relies on section titles as static queries (limits adaptability); no explicit reranking with token-level interactions is described in the Pipeline_1 description here.",
            "scaling_behavior": "No scaling analysis reported in this paper for Pipeline_1.",
            "uuid": "e4429.1",
            "source_info": {
                "paper_title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Pipeline_2",
            "name_full": "SurveySum Pipeline 2 (SPECTER2 embeddings + FAISS + LLM reranking/summarization)",
            "brief_description": "A SurveySum baseline that encodes chunks as dense SPECTER2 embeddings stored in FAISS, retrieves chunks using the section title, applies an LLM-based reranking step, and then summarizes the re-ranked chunks with an LLM.",
            "citation_title": "Surveysum: A dataset for summarizing multiple scientific articles into a survey section.",
            "mention_or_use": "use",
            "system_name": "SurveySum Pipeline 2",
            "system_description": "Documents are chunked and encoded into dense SPECTER2 embeddings and indexed in FAISS. At inference, the section title is used as the query to retrieve relevant chunks; an LLM then re-ranks/evaluates the retrieved candidates before a final LLM produces the summary of the selected chunks.",
            "llm_model_used": "LLM used for reranking and summarization (unspecified in SurveySum summary here; SurveySum examples mention use of LLMs such as GPT-4 for summarization/re-ranking in practice).",
            "extraction_technique": "Embedding-based retrieval (SPECTER2 embeddings in FAISS) using section-title queries, followed by an LLM re-ranking step.",
            "synthesis_technique": "LLM-based summarization of top re-ranked chunks (abstractive summarization).",
            "number_of_papers": "Evaluated on SurveySum; average ~7.38 cited papers per target section.",
            "domain_or_topic": "Scientific literature (AI/NLP/ML, SurveySum dataset).",
            "output_type": "Survey-section summaries (abstractive).",
            "evaluation_metrics": "ROUGE-1/2/L, BERTScore, Ref-F1, G-Eval, CheckEval (as used in this paper's comparison).",
            "performance_results": "Reported (this paper's re-evaluation): ROUGE-1 = 0.49, ROUGE-2 = 0.10, ROUGE-L = 0.23, BERTScore = 0.59, Ref-F1 = 0.72, G-Eval = 4.0, CheckEval = 0.76.",
            "comparison_baseline": "Compared against Pipeline_1 and XSum.",
            "performance_vs_baseline": "Pipeline_2 generally outperforms Pipeline_1 but is outperformed by XSum on the reported metrics; e.g., ROUGE-1 0.49 (Pipeline_2) vs 0.51 (XSum).",
            "key_findings": "Embedding-based retrieval with an LLM reranker yields stronger performance than simple title-based neural ranking; however, using static section titles as queries limits adaptability compared to dynamic question generation.",
            "limitations_challenges": "Section titles as static queries can limit retrieval relevance; the approach depends on embedding quality and reranker consistency.",
            "scaling_behavior": "No explicit scaling trends reported in this paper for Pipeline_2.",
            "uuid": "e4429.2",
            "source_info": {
                "paper_title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "OpenScholar",
            "name_full": "OpenScholar: Synthesizing scientific literature with retrieval-augmented LMs",
            "brief_description": "A large-scale retrieval-augmented system that integrates a specialized datastore of ~45 million papers with iterative retrieval and feedback loops to produce precise, citation-backed responses with LLMs.",
            "citation_title": "Openscholar: Synthesizing scientific literature with retrieval-augmented lms.",
            "mention_or_use": "mention",
            "system_name": "OpenScholar",
            "system_description": "Described as integrating a large specialized datastore (â‰ˆ45M papers) with iterative retrieval and feedback loops; the system uses retrieval-augmented language models to generate citation-backed, precise responses by iteratively retrieving, synthesizing, and refining evidence from the datastore.",
            "llm_model_used": "Not specified in this paper (OpenScholar referenced as related work; original OpenScholar paper should be consulted for exact LLMs used).",
            "extraction_technique": "Large-scale retrieval from a specialized datastore (document indexing + vector retrieval) with iterative retrieval-feedback loops.",
            "synthesis_technique": "Retrieval-augmented generation with iterative refinement/feedback producing citation-backed answers/summaries.",
            "number_of_papers": "Datastore reported as ~45 million papers (per the referenced OpenScholar description).",
            "domain_or_topic": "General scientific literature (large-scale corpora).",
            "output_type": "Citation-backed responses/summaries synthesized from many papers.",
            "evaluation_metrics": "Not specified in this paper's brief mention; see OpenScholar original paper for evaluation details.",
            "performance_results": "Not reported here (only mentioned as related work).",
            "comparison_baseline": "Not reported here.",
            "performance_vs_baseline": "Not reported here.",
            "key_findings": "Demonstrates viability of large specialized datastores combined with iterative retrieval/feedback to produce precise, citation-backed outputs at scale.",
            "limitations_challenges": "Implied challenges: scale of storage/compute for 45M-paper datastore, iterative retrieval latency/cost; details not enumerated in this paper.",
            "scaling_behavior": "Designed for large-scale (45M papers) operation; cited as an example of large-corpus RAG scaling.",
            "uuid": "e4429.3",
            "source_info": {
                "paper_title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "HyQE/HyDe (generative retrieval helpers)",
            "name_full": "HyQE / HyDe / reverse HyDe (hypothetical query/document/question generation approaches)",
            "brief_description": "Generative techniques that produce hypothetical queries or document-like content (e.g., hypothetical query embeddings or hypothetical document embeddings) to bridge the semantic gap between user queries and retrievable content, improving retrieval ranking for downstream RAG systems.",
            "citation_title": "Hyqe: Ranking contexts with hypothetical query embeddings.",
            "mention_or_use": "mention",
            "system_name": "HyQE / HyDe / reverse HyDe (grouped generative retrieval methods)",
            "system_description": "These approaches use generative models to produce hypothetical queries or document-like representations from either queries or documents; the generated text is encoded as embeddings (hypothetical query/document embeddings) and used to improve retrieval ranking, thereby increasing chance that relevant contexts are retrieved for LLM grounding. The paper cites HyDe, HyQE and reverse HyDe as inspirations for XSum's question-generation module.",
            "llm_model_used": "Described as using generative techniques / pre-trained generative models (exact model names not specified in this paper).",
            "extraction_technique": "Generate hypothetical queries or documents with a generative model, encode these into embeddings, and use them to query or re-rank document chunks (improved embedding-based retrieval).",
            "synthesis_technique": "Primarily retrieval-improvement rather than direct synthesis; used upstream of RAG to provide better retrieved contexts for subsequent LLM synthesis.",
            "number_of_papers": "Not specified here (method is retrieval-level and orthogonal to number of papers processed).",
            "domain_or_topic": "Information retrieval for summarization / multi-document summarization contexts.",
            "output_type": "Improved retrieval contexts (indirectly enabling better summaries or answers).",
            "evaluation_metrics": "Not specified in this paper's mention (see original HyQE/HyDe works).",
            "performance_results": "Not reported here; paper cites these works conceptually as improving retrieval relevance.",
            "comparison_baseline": "Compared in source works to standard embedding or query methods (not shown here).",
            "performance_vs_baseline": "Claimed in cited works to improve retrieval ranking by bridging semantic gaps; no quantitative details in this paper.",
            "key_findings": "Generative hypothetical queries/documents can bridge semantic mismatches between queries and document representations, improving retrieval relevance for RAG pipelines.",
            "limitations_challenges": "This paper does not detail limitations of HyQE/HyDe; general issues include reliance on quality of generated hypotheticals and potential introduction of generation noise if hypotheticals are poor.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4429.4",
            "source_info": {
                "paper_title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ShindeHybrid",
            "name_full": "Shinde et al. hybrid extractive-abstractive pipeline (BERT + BigBird-PEGASUS)",
            "brief_description": "A hybrid two-stage pipeline that uses BERT-based extractive models to select salient passages and BigBird-PEGASUS (a pre-trained transformer) for abstractive multi-document summarization, shown to be effective in the biomedical domain.",
            "citation_title": "An extractive-abstractive approach for multi-document summarization of scientific articles for literature review.",
            "mention_or_use": "mention",
            "system_name": "Shinde et al. hybrid pipeline",
            "system_description": "First stage: BERT-based extractive models identify and select relevant passages from scientific articles. Second stage: BigBird-PEGASUS (a PLM specialized for long inputs) is used to produce abstractive summaries from selected passages, thereby combining robust extraction with powerful abstractive generation.",
            "llm_model_used": "BigBird-PEGASUS (abstractive PLM) for summarization; BERT (extractive stage) for passage selection.",
            "extraction_technique": "BERT-based extractive selection of salient passages from multiple documents.",
            "synthesis_technique": "Abstractive summarization using BigBird-PEGASUS over the extracted passages (two-stage extract-then-generate).",
            "number_of_papers": "Not specified in this paper's mention (targeted at biomedical multi-document summarization datasets).",
            "domain_or_topic": "Biomedical multi-document summarization (literature review summarization).",
            "output_type": "Abstractive summaries for literature reviews / multi-document summaries.",
            "evaluation_metrics": "Reported in the original Shinde et al. work (not detailed here); this paper notes 'robust performance in the biomedical domain.'",
            "performance_results": "Not quantified in this paper; original work reports improved performance for biomedical summarization tasks.",
            "comparison_baseline": "Compared in original work to purely extractive or purely abstractive baselines (not detailed here).",
            "performance_vs_baseline": "Reported as achieving robust performance in the biomedical domain (no numbers provided here).",
            "key_findings": "Combining extractive selection and long-input-capable abstractive PLMs can yield robust multi-document summarization in specialized domains like biomedicine.",
            "limitations_challenges": "Not expanded here; typical concerns include dependency on extraction quality and PLM input-length/cost constraints.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4429.5",
            "source_info": {
                "paper_title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A general approach that combines information retrieval (vector or index-based) with LLM generation to ground outputs in retrieved source documents, reducing hallucination and improving factual grounding.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "RAG pipelines: documents are chunked and embedded into vector space (or indexed); a retrieval step selects top-k relevant chunks for a given query; an LLM then conditions on the retrieved chunks to generate answers or summaries grounded in retrieved evidence. In XSum, RAG is implemented with SPECTER2 embeddings, FAISS retrieval, ColBERT2 reranking, and LLM-based grounded answer generation followed by an LLM editor for final summary synthesis.",
            "llm_model_used": "Varies by implementation. In this paper's XSum: gpt4o-mini_15-2-2024-preview as generator/editor; other pipelines referenced use GPT-4 or unspecified LLMs.",
            "extraction_technique": "Embedding-based retrieval (document chunking + dense embeddings + FAISS) optionally followed by token-level reranking (ColBERT2).",
            "synthesis_technique": "LLM generation conditioned on retrieved contexts; optional multi-step aggregation (e.g., per-question answers then editorial synthesis).",
            "number_of_papers": "Varies by application; in SurveySum evaluations used on sets averaging 7.38 reference papers per section; OpenScholar example applies RAG over a 45M-paper datastore.",
            "domain_or_topic": "General scientific literature and domain-specific corpora; used here for AI/NLP/ML survey summarization.",
            "output_type": "Grounded answers, citation-rich summaries, and other generated artefacts based on retrieved evidence.",
            "evaluation_metrics": "ROUGE, BERTScore, reference-citation metrics (Ref-F1), and LLM-based evaluators like G-Eval and CheckEval are used in this paper to assess RAG pipeline outputs.",
            "performance_results": "RAG-based pipelines (including XSum) produced strong semantic and citation-alignment metrics in this study; XSum (a RAG pipeline) yielded ROUGE-1 = 0.51, Ref-F1 = 0.76, G-Eval = 4.2 as reported.",
            "comparison_baseline": "Compared to static retrieval or purely extractive/abstractive architectures (SurveySum Pipelines 1 & 2 and hybrid baselines cited).",
            "performance_vs_baseline": "RAG approaches (and XSum in particular) achieved improved G-Eval/CheckEval and Ref-F1 versus the referenced SurveySum pipelines; RAG favors semantic grounding over strict lexical overlap (lower ROUGE-2 common).",
            "key_findings": "RAG provides strong factual grounding and citation alignment when retrieval quality is high; dynamic, document-driven queries (e.g., generated questions) further improve retrieval relevance; evaluation is sensitive to the choice/configuration of LLM evaluators.",
            "limitations_challenges": "Performance depends on retrieval quality; potential for verbosity and inclusion of excessive background detail; issues of hallucination if retrieval lacks coverage; scaling and latency/cost for very large corpora are open challenges.",
            "scaling_behavior": "RAG can scale with vector indices and datastores but requires attention to data ingestion, latency, and storage; explicit scaling experiments are not reported in this paper.",
            "uuid": "e4429.6",
            "source_info": {
                "paper_title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Surveysum: A dataset for summarizing multiple scientific articles into a survey section.",
            "rating": 2,
            "sanitized_title": "surveysum_a_dataset_for_summarizing_multiple_scientific_articles_into_a_survey_section"
        },
        {
            "paper_title": "Openscholar: Synthesizing scientific literature with retrieval-augmented lms.",
            "rating": 2,
            "sanitized_title": "openscholar_synthesizing_scientific_literature_with_retrievalaugmented_lms"
        },
        {
            "paper_title": "Hyqe: Ranking contexts with hypothetical query embeddings.",
            "rating": 2,
            "sanitized_title": "hyqe_ranking_contexts_with_hypothetical_query_embeddings"
        },
        {
            "paper_title": "Retrieval-augmented generation for large language models: A survey.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "An extractive-abstractive approach for multi-document summarization of scientific articles for literature review.",
            "rating": 1,
            "sanitized_title": "an_extractiveabstractive_approach_for_multidocument_summarization_of_scientific_articles_for_literature_review"
        },
        {
            "paper_title": "Colbertv2: Effective and efficient retrieval via lightweight late interaction.",
            "rating": 1,
            "sanitized_title": "colbertv2_effective_and_efficient_retrieval_via_lightweight_late_interaction"
        }
    ],
    "cost": 0.024148,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization</p>
<p>Pierre Achkar 
Leipzig University
Fraunhofer ISI Leipzig</p>
<p>Tim Gollub 
Bauhaus-UniversitÃ¤t Weimar</p>
<p>Martin Potthast 
Kassel University
hessian.AIScaDS, AI</p>
<p>Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization
29AF714127CD86DF5A7972CE8C8A9C4EMulti-document Summarization (MDS)Retrieval-Augmented Generation (RAG)Scientific Literature Summarization
The exponential growth of scientific publications has made it increasingly difficult for researchers to stay updated and synthesize knowledge effectively.This paper presents XSum, a modular pipeline for multi-document summarization (MDS) in the scientific domain using Retrieval-Augmented Generation (RAG).The pipeline includes two core components: a question-generation module and an editor module.The question-generation module dynamically generates questions adapted to the input papers, ensuring the retrieval of relevant and accurate information.The editor module synthesizes the retrieved content into coherent and well-structured summaries that adhere to academic standards for proper citation.Evaluated on the SurveySum dataset, XSum demonstrates strong performance, achieving considerable improvements in metrics such as CheckEval, G-Eval and Ref-F1 compared to existing approaches.This work provides a transparent, adaptable framework for scientific summarization with potential applications in a wide range of domains.Code available at https://github.com/webisde/scolia25-xsum/tree/main</p>
<p>Introduction</p>
<p>The rapid growth of scientific literature has made it increasingly difficult for researchers to stay up-to-date with the latest developments.The number of papers published each month has increased exponentially since 1994, with fields such as artificial intelligence (AI) doubling their research output [1].While this growth reflects the progress of research communities, it also presents a serious challenge: how can researchers stay informed and extract key insights from this volume of information?This overload of information makes it difficult to manually read, understand, and summarize the growing body of literature.This challenge becomes paramount in rapidly evolving fields such as AI, where researchers often need to synthesize knowledge from multiple sources in order to make progress.Summarizing research is not simply reading through papers but also identifying the most important information, connecting ideas from different sources, and presenting them in a clear and concise way.Automated summarization solutions are essential to help researchers save time and focus on the core information.One promising approach to this challenge is Multi-Document Summarization (MDS), which combines information from multiple sources into clear and concise summaries.The concept itself is not new; for example, early work from 1999 proposed to use reference relationships between scientific papers to generate survey-style summaries [2].The approach identifies key fragments of cited papers, analyzes similarities and differences between them, and classifies citation contexts to support summarization.Over time, summarization methods have evolved from static approaches to deep learning models and later to pre-trained language models (PLMs) [3].Currently, the field is dominated by Large Language Models (LLMs), which are pre-trained on massive datasets and capable of generating high-quality text.Retrieval-Augmented Generation (RAG) builds on these advances by combining retrieval techniques with LLMs, enabling systems to find relevant information and synthesize it into accurate and coherent answers.A typical RAG pipeline processes a set of documents  = { 1 ,  2 , . . .,   } by dividing them SCOLIA 2025, the First International Workshop on Scholarly Information Access, ECIR 2025, 10th April 2025, Lucca, Italy 0009-0007-0791-9078 (P.Achkar); 0000-0003-1737-6517 (T.Gollub); 0000-0003-2451-0665 (M.Potthast) into smaller chunks, encoding them into dense vector embeddings with a pre-trained model, and storing them in a vector database for later retrieval.In the context of MDS, a search query to the vector database acts as a summarization guideline that can either be provided by the user or the MDS system.When a query is provided, the top- most relevant chunks are retrieved based on similarity metrics and passed to an LLM, which generates responses grounded in the retrieved content.Despite the recent advances, summarizing scientific literature remains an open research problem, requiring not only linguistic fluency and coherence, but also robust relevance and adherence to academic standards for citing literature correctly.To address these challenges, we present XSum, a RAG pipeline designed for MDS in the scientific domain.XSum builds upon the typical RAG pipeline and introduces two new innovative components: a question-generation module and an editor module.The questiongeneration module formulates questions on the basis of the input papers to be summarized, which are then passed to the RAG component.The editor module synthesizes the set of answers retrieved from the RAG component into coherent summaries, ensuring that the resulting output is comprehensive, reliable, and well-structured.The proposed pipeline is evaluated on the SurveySum [4] dataset, which is designed to test MDS methods in the scientific domain.The results show that XSum outperforms existing methods on metrics such as CheckEval [5], G-Eval [6] and Ref-F1, demonstrating its ability to produce high-quality summaries.We consider the quality of a generated summary to be defined by its ability to comprehensively cover the essential content of the source documents, to maintain a coherent and fluent narrative, and to accurately reflect the original citations.</p>
<p>Related Work</p>
<p>The task of summarizing multiple scientific documents has evolved considerably over time.Early approaches, such as SciSumm, introduced query-driven summarization by clustering relevant text segments from co-cited papers to generate contextualized summaries [7].These methods leveraged citation relationships but struggled with complex content relationships across documents.Later developments introduced neural network-based architectures for MDS.For instance, HiMAP and HierSumm utilized hierarchical models and passage ranking techniques to enhance content selection and fusion, resulting in more coherent and contextually relevant summaries [8,9].These methods marked a shift from purely extractive approaches to more integrative models capable of generating fluent summaries.</p>
<p>The integration of extraction and abstraction further refined summarization methods.Shinde et al. proposed a hybrid pipeline that combines BERT-based extractive models with BigBird-PEGASUS for abstractive summarization, achieving robust performance in the biomedical domain [10].Similarly, KGSum introduced knowledge graph-based encoding to model document content and relationships, employing a two-stage decoding strategy to produce focused and cohesive summaries [11].</p>
<p>The field has taken a major step forward with the emergence of retrieval augmented generation (RAG) pipelines.OpenScholar1 , for example, demonstrated a novel approach by integrating a specialized datastore of 45 million papers with iterative retrieval and feedback loops, enabling precise, citationbacked responses, highlighting growing interest in retrieval augmented systems [12].Another approach to MDS using retrieval is proposed through the SurveySum framework, which introduces two pipelines, Pipeline 1 and Pipeline 2, both integrating retrieval-based selection with LLM-based summarization [4].These pipelines are evaluated on the SurveySum dataset, a benchmark specifically designed for MDS in scientific literature, which consists of survey sections paired with their cited papers.This is the same dataset used in this work, and a more detailed discussion of its structure will be provided in the Evaluation section.</p>
<p>Pipeline 1 uses a neural ranking approach where full-text papers are segmented into overlapping chunks during pre-processing.These chunks are ranked by monoT5-3B, which assigns relevance scores based on the title of the target survey section.The highest-ranked chunks are then passed to an LLM, such as GPT-4, to generate the final summary.Pipeline 2, on the other hand, relies on embedding-based retrieval, where text chunks are represented as dense embeddings using SPECTER22 and stored in a FAISS vector database.The section title (e.g.Data Generation via PLM:Explaining Models' Decisions) is used as a query to retrieve relevant chunks at inference time.Unlike Pipeline 1, which directly selects the top-ranked chunks for summarization, Pipeline 2 includes a re-ranking step where an LLM evaluates and ranks the retrieved content before summarization.The resulting chunks are then summarized into a cohesive section.Figures 1 and 2 illustrate the structures of these pipelines.</p>
<p>While these pipelines achieve acceptable performance, they rely on static retrieval using section titles as queries, which can limit adaptability to different summarization contexts.Among them, Pipeline 2 is more comparable to our approach XSum, as it utilizes embedding-based retrieval rather than direct ranking.However, XSum addresses key limitations by introducing a question generation module that dynamically formulates structured questions based on the title and abstract of the input papers, which serve as queries during retrieval, thereby improving retrieval relevance.Additionally, it features an editor module that synthesizes retrieved content into a coherent, citation-rich summary, ensuring better fluency, accuracy, and adherence to academic writing standards.The complete pipeline and the functionality of these components will be explained in detail in the Methodology section.Beyond SurveySum, several other datasets have been developed for MDS, particularly in the biomedical domain.Datasets such as Cochrane-auto and MS2 focus on summarizing clinical trials and systematic reviews, providing benchmarks for evaluating summarization methods in evidence-based medicine [13,14].Another relevant dataset is Multi-XScience, which was initially considered for evaluating the proposed approach, as it focuses on synthesizing related work sections from abstracts and cited references [15].However, a preliminary analysis revealed missing values in the reference papers used to generate the related work sections, raising concerns about its completeness for reliable benchmarking.Furthermore, while related work sections can be considered multi-document summaries, they are often shaped by the comparative and argumentative nature of the paper's contributions rather than being purely extractive or abstractive.Given these considerations, the SurveySum dataset appeared to be a more appropriate choice for evaluating our approach, as it explicitly focuses on summarizing multiple scientific papers into structured survey sections.To our knowledge, no other experimental work has been conducted on SurveySum beyond the evaluations presented by its authors so far.</p>
<p>Methodology</p>
<p>This section introduces the XSum pipeline, a modular approach to summarizing scientific literature into coherent and traceable outputs.The initial idea for building this pipeline was inspired by the interview paradigm, where an interviewer interacts with a domain expert.In this analogy, the interviewer prepares a structured set of questions based on the expert's domain knowledge, conducts the interview in which the expert answers these questions, and finally an editor compiles the conversation into a well-structured summary.This concept motivated the design of XSum and led to the introduction of two key modules: a question generation module, which formulates structured questions to guide the retrieval process, and an editor module, which synthesizes the retrieved answers into a coherent and citation-rich summary.Each module plays an important role in ensuring that the summaries generated are both relevant and well-structured, as described in the following subsections.</p>
<p>Overview of the Pipeline</p>
<p>The proposed pipeline for MDS, XSum, illustrated in Figure 3, transforms input reference papers into a coherent summary through a sequence of modular steps.It begins by using the titles and abstracts of the reference papers to generate broad and general questions using an LLM.These questions, designed to reflect the main themes and contributions of the papers, are stored for later use.The full texts of the reference papers are then processed by dividing them into manageable chunks, which are embedded in dense vector representations and stored in a vector database.This pre-processing ensures efficient retrieval of relevant content in subsequent stages.The stored questions are then used to query the database and retrieve the most relevant chunks.The retrieval process involves an initial similarity-based ranking of the chunks, followed by a re-ranking step to refine their relevance.The final set of retrieved chunks is paired with the corresponding questions.These question-chunk pairs are then passed to an LLM, which generates concise answers based on the retrieved content.If the context is insufficient, the LLM will refrain from generating an answer, ensuring accuracy and credibility.Finally, the set of question-answer pairs is passed to the editor module, which synthesizes them into a comprehensive and well-structured summary.The editor ensures coherence, logical flow, and adherence to academic standards while incorporating citations to maintain traceability.The pipeline processes reference papers into summaries through modular steps.Document Pre-Processing segments papers into chunks, encodes them as embeddings, and stores them in a FAISS database.Question Generation uses an LLM to generate questions from titles and abstracts.In Question Answering, a RAG framework retrieves relevant chunks and generates answers with an LLM.Finally, the Editor Module (Final Summary Generation) synthesizes the answers into a coherent, citation-rich summary.</p>
<p>Question Generation Module</p>
<p>This module is essential for aligning the retrieval and summarization stages with the specific content of the reference documents.By leveraging the generative capabilities of LLMs, it ensures that the pipeline is driven by structured, contextually relevant queries.The approach draws on insights from methods such as HyDe [16], HyQE [17] and reverse HyDe [18], all of which use generative techniques to improve retrieval relevance.HyQE (Hypothetical Document Embeddings) involves generating hypothetical content based on a query, encoding this content into embeddings, and then using these embeddings to improve retrieval accuracy.Both HyQE (Hypothetical Query Embeddings) and reverse HyDe follow a similar strategy, but focus on generating hypothetical questions or queries that match the content of a document.These hypothetical questions bridge the semantic gap between queries and retrievable content, improving the ranking of relevant results [17].</p>
<p>In our pipeline, the title and abstract of each reference paper serve as input to a pre-trained LLM, which generates  = 5 broad and semantically rich questions encapsulating the core themes and contributions of the paper.These questions are stored as structured queries for subsequent stages.The generated questions serve two primary functions: first, they refine the retrieval process by ensuring that only the most contextually relevant content is retrieved; second, they provide a structured framework to guide the subsequent synthesis and summarization phases.For illustration, examples of such generated questions can be found in Appendix A.1.</p>
<p>Document Pre-processing</p>
<p>The pre-processing phase ensures that the reference papers are prepared for efficient retrieval and summarization by arranging them in a format suitable for downstream tasks.This phase consists of three main steps:</p>
<p>â€¢ Chunking Documents: The full texts of the reference papers are divided into interconnected chunks of 150 tokens each, with an overlap of 20 tokens.This overlap preserves contextual continuity between successive chunks, while respecting sentence boundaries ensures that the division does not disrupt the semantic flow of the text.We determined this configuration by experimentation, after trying different setups, finding that it provided the best balance between contextual preservation and computational efficiency.â€¢ Embedding Generation: Each chunk is encoded into dense vector representations using the SPECTER2 model, which is specifically designed to capture the semantic relationships and contextual meanings in academic texts.â€¢ Vector Database Indexing: Chunks are indexed in FAISS, a high-speed similarity search database, for efficient retrieval.</p>
<p>Question Answering Module</p>
<p>In this module, the focus is on integrating retrieval and synthesis to generate concise, contextually relevant answers to the questions formulated in the previous stage.By combining robust retrieval techniques with an LLM in a RAG framework, this module ensures that the pipeline produces highquality output that is grounded in the source material.</p>
<p>Questions are embedded into dense vector representations using the same SPECTER2 model used in the document pre-processing phase.The retrieval process proceeds in two stages:</p>
<ol>
<li>Initial Retrieval: Using cosine similarity, the top 100 chunks most relevant to each question are retrieved from the FAISS vector database, serving as an initial filtering step.2. Reranking: The retrieved chunks are re-ranked using the ColBERT2 model [19], which evaluates token-level interactions between the question and the chunks.This refinement step ensures that the 20 most relevant chunks are selected.</li>
</ol>
<p>The final set of 20 chunks is presented to a pre-trained LLM along with the corresponding question.The LLM synthesizes a coherent and accurate response based solely on the retrieved context.If the retrieved chunks do not provide sufficient information, the LLM is instructed not to generate an answer, minimizing unsupported or speculative output.</p>
<p>To ensure credibility and traceability, the LLM includes valid citations from the retrieved chunks in its responses.By grounding the answers in the source material, this module adheres to academic standards and facilitates the verification of the generated content.</p>
<p>Final Summary Generation (Editor Module)</p>
<p>The Editor Module synthesizes the answers generated in the previous step into a cohesive and comprehensive summary, aggregating all question-answer pairs into a unified narrative that reflects the overarching themes and contributions of the papers.A pre-trained LLM is used as the editor to generate the final summary.The model is prompted to write an extensive, coherent summary that seamlessly integrates the individual answers while maintaining a logical flow.The Editor LLM ensures the summary adheres to academic standards.It incorporates citations into the final summary, ensuring that all statements are properly grounded in the retrieved source material.The prompt used in this module is as follows:
Editor Module Prompt ### CONTEXT ### You are</p>
<p>Evaluation</p>
<p>The proposed pipeline is evaluated using a domain-specific dataset for MDS.This section details the dataset, metrics, implementation, results, examples, and discussion, providing a comprehensive analysis of the pipeline's performance.</p>
<p>Dataset</p>
<p>The evaluation of the proposed pipeline is conducted on the SurveySum3 dataset, a domain-specific resource designed for MDS tasks in scientific literature.This dataset includes 79 survey sections across fields such as AI, natural language processing (NLP), and machine learning (ML).Each section is paired with the full-text content of its cited papers, with an average of 7.38 papers cited per section.The dataset is explicitly designed to test MDS models on the synthesis of content from multiple sources, making it particularly suited for assessing the proposed pipeline.</p>
<p>Metrics</p>
<p>The evaluation employs a mix of traditional and LLM-based metrics to assess the quality of summaries in terms of content coverage, coherence, and citation alignment: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) [20] measures the overlap between the generated summaries and the reference text.It calculates n-gram overlap, word sequence matching, and the longest common subsequences.ROUGE-1, ROUGE-2, and ROUGE-L are used in this study to capture unigrams, bigrams, and sentence-level matches, respectively.BERTScore [21] evaluates semantic similarity between the generated and reference summaries using contextual embeddings from PLMs like BERT.Reference F1 Score (Ref-F1) measures how accurately the citations in the generated summaries align with those in the ground truth.It computes precision (proportion of correctly included references) and recall (proportion of ground-truth references captured in the generated summary), and combines them into an F1 score.This metric is essential in scientific summarization, where attribution and citation accuracy are critical.G-Eval 4 [6] is a framework for evaluating the output of natural language generation (NLG) using LLMs, providing reference-free assessments based on criteria such as coherence, coverage, fluency, and relevance.It uses Chain-of-Thought (CoT) reasoning to systematically generate detailed evaluation steps, ensuring consistency and robustness in scoring.Scores are assigned on a fixed scale (e.g. 1 to 5) and refined using token probabilities, enabling granular, continuous analyses that capture the nuances between outputs.By bypassing the need for reference outputs, it is particularly effective for tasks where predefined references are unavailable or impractical, such as creative or open-ended text generation.Experiments show that G-Eval achieves a stronger correlation with human judgments than traditional metrics such as ROUGE, as well as neural evaluators, on benchmarks such as SummEval [22] for summarization.CheckEval5 [5] is a robust evaluation framework that uses LLMs to evaluate generated text using a structured checklist-based approach.It supports two evaluation modes: reference-based, which compares the generated text to reference summaries, and criteria-driven, which evaluates the text against predefined dimensions such as coherence, fluency, and coverage.By breaking down evaluation criteria into detailed sub-aspects, framed as Boolean (yes/no) questions, CheckEval simplifies the evaluation process and increases its reliability and interpretability.The framework operates in three stages: aspect selection, where key evaluation dimensions are identified; checklist generation, where detailed questions are created and refined; and checklist-based evaluation, where LLMs respond to the questions, with the final score calculated as the proportion of positive responses.Validated against the SummEval benchmark, CheckEval demonstrates high correlation with human judgment and strong inter-annotator agreement.</p>
<p>ROUGE and BERTScore evaluations report recall scores to emphasize content coverage, while G-Eval and CheckEval focus on the coverage criterion, consistent with SurveySum's methodology for assessing core content representation.</p>
<p>Implementation Details</p>
<p>This section outlines the tools, models, and frameworks utilized in our development process:</p>
<p>â€¢ Development Environment: The pipeline was implemented in Python, utilizing sentence-transformers for embedding generation, nltk for text processing, and FAISS for efficient vector-based retrieval.All experiments were conducted on a Tesla V100-PCIE-32GB GPU, enabling efficient embedding generation, chunk retrieval, and summarization tasks.â€¢ Pre-trained LLMs: For text generation, we employed gpt4o-mini_15-2-2024-preview, while Phi-3-small-8k-instruct was utilized for evaluation.Both models were configured with a temperature of 0.3 and a top-p of 0.95 to ensure controlled and consistent outputs.This choice aligns with the position that the same model should not be used for both generation and evaluation to mitigate potential bias.Research has highlighted that using identical or equally powerful models for both tasks can lead to skewed results, as LLMs like GPT-4 tend to favor their own outputs due to egocentric biases [23].Notably, the SurveySum paper does not specify whether identical models were used in their study for both tasks.</p>
<p>Results</p>
<p>Since the original SurveySum paper did not report ROUGE and BERTScore metrics for the benchmark pipelines, we calculated these values.G-Eval and CheckEval were also computed using our implementation to ensure consistency, employing the Phi-3-small-8k-instruct model as the evaluator.This approach guarantees a fair comparison across all pipelines, enabling a comprehensive assessment of XSum's performance relative to the benchmarks.Any differences in the results (particularly for G-Eval and CheckEval) can be attributed to differences in the evaluation settings and model configurations compared to the original SurveySum experiments.For clarity, Pipeline_1 and Pipeline_2 are the two pipelines that performed best in the SurveySum experiments.</p>
<p>The results, summarized in Table 1, highlight XSum's consistent outperformance of the benchmark pipelines across all metrics.It achieves ROUGE-1 (0.51) and ROUGE-L (0.24), reflecting its ability to effectively capture unigrams and sentence-level structures.Its BERTScore (0.62) highlights its strong semantic alignment with reference summaries, reflecting its capability to retain content integrity through paraphrasing and semantic rephrasing.Furthermore, XSum attains the highest Ref-F1 (0.76), G-Eval (4.2), and CheckEval (0.97) scores, emphasizing its superiority in generating coherent, relevant, and high-quality summaries.</p>
<p>To further demonstrate the performance of XSum, we present two examples of summaries generated by it, along with their corresponding ground truth (original section text) and evaluation scores.These examples have been selected based on their performance across the evaluation metrics, one representing the highest average score across all metrics and the other representing the lowest average score.Due to their length, the full examples are provided in Appendix A.2.</p>
<p>Discussion</p>
<p>The strong performance of XSum is largely driven by its two key features: the question-generation module and the editor module.By dynamically generating queries about the document content, the retrieval module ensures relevant and contextual results, addressing the limitations of static query approaches such as using section titles, as in Pipeline_2.Additionally, the use of ColBERT as a reranker may contribute to better chunk retrieval by prioritizing the most relevant and informative sections during the ranking process.The editor module further enhances the pipeline by synthesizing retrieved information into coherent summaries with proper citations, resulting in outputs that adhere to academic standards.</p>
<p>The quality of the content generated by XSum highlights its ability to synthesize multiple sources into a structured and coherent summary.The selected examples illustrate both the strengths and limitations of the approach.The high-scoring example (Example 1) closely follows the human-written text, effectively capturing key technical details while maintaining logical flow and factual consistency.This suggests that XSum can generate summaries that are both informative and well-structured and in line with academic standards.However, a notable difference remains in the style and clarity of the summaries.Human-written sections tend to be more compact and nuanced, often presenting a comparative perspective that sets different contributions in relation to each other.In contrast, XSum summaries tend to be verbose, often providing extended explanations and additional contextual information beyond what is strictly necessary for summarizing.This is particularly evident in the low-scoring example (Example 2), where the generated text, while factually accurate, lacks the same level of selectivity as the human-written version, including an unnecessary degree of background detail rather than focusing solely on comparative insights.This contrast highlights a key challenge in scientific summarization.Although retrieval-driven methods such as XSum excel at aggregating and structuring information, they do not yet fully replicate the complex synthesis and prioritization that domain experts perform when writing a summary of multiple related papers.Nevertheless, XSum still produces highly structured and factually based summaries, demonstrating that automated MDS can be a valuable tool for scientific literature synthesis, particularly in assisting researchers with information overload.</p>
<p>Despite its strengths, the low ROUGE-2 scores across all pipelines highlight a common problem in abstractive summarization: achieving bigram overlap with reference summaries.RAG-based pipelines, including XSum, prioritize semantic richness and coherence over strict lexical matching, which reduces alignment with reference summaries.However, ROUGE-1 and ROUGE-L scores show moderate alignment, reflecting the ability to capture essential unigrams and sentence-level structures.BERTScore, which assesses semantic similarity, achieves satisfactory results, highlighting the ability of such pipelines to capture the essence of content through paraphrasing and semantic rephrasing, even when lexical overlap is limited.It is important to note that while the ROUGE metrics provide valuable insights into lexical overlap and content coverage, they are inherently limited in assessing the nuances of abstractive summarisation.This limitation is addressed by incorporating a set of metrics -BERTScore, G-Eval and CheckEval -that more effectively capture semantic similarity, coherence and overall quality.</p>
<p>In addition to traditional metrics, frameworks like G-Eval and CheckEval provide refined assessments of summary quality by leveraging LLMs to evaluate coherence, relevance, and coverage.These metrics excel at capturing semantic and structural attributes that conventional metrics often overlook, making them particularly effective for evaluating abstractive summaries.However, their dependence on specific LLMs introduces challenges of consistency and reproducibility, as evaluation outcomes may vary with different model configurations.This highlights the need for standardization in LLM-driven evaluation practices.</p>
<p>Finally, XSum's modular design offers substantial flexibility in adapting to different summarization tasks.The question-generation module can be customized to generate domain-specific or task-specific questions to improve relevance in different contexts.Similarly, the editor module allows customization of tone, style, and abstraction levels, enabling outputs to be tailored for different audiences, from academic researchers to professional practitioners.This adaptability ensures the pipeline's scalability and applicability to a wide range of domains, addressing the growing demand for efficient MDS in complex settings.</p>
<p>Conclusion and Future Work</p>
<p>This work addresses the challenges of MDS in the scientific domain by introducing a modular RAGbased pipeline featuring two key enhancements: a question-generation module and an editor module.These components enable the pipeline to synthesize information from multiple scientific papers into cohesive, well-structured summaries.Experimental evaluations on the SurveySum dataset demonstrate considerable improvements in metrics such as CheckEval, G-Eval, and Ref-F1 compared to existing approaches.By providing detailed guidance on the design and implementation of RAG-based pipelines, this work contributes to making these systems more transparent, reproducible, and adaptable for diverse summarization tasks.</p>
<p>While the current pipeline achieves strong performance, several opportunities for future improvements remain.A key direction is evaluating XSum against other MDS pipelines to enable a more in-depth comparison of effectiveness and retrieval quality.Such comparisons would provide insights into the relative strengths and limitations of different summarization strategies.Additionally, conducting an ablation study would allow for a deeper understanding of the impact of each component in the pipeline, particularly the question-generation module and the editor module, to assess their individual contributions to overall performance.Optimizing data ingestion pipelines, which are often a bottleneck in large-scale industrial applications (as emphasized in systems like ColPali [24]), could further enhance scalability and efficiency.Moreover, integrating vision-language models to process visually rich documents, including text, tables, and figures, offers a promising direction for improving retrieval accuracy and extending the system's capabilities to more complex scientific datasets.</p>
<p>Limitations</p>
<p>Despite its contributions, this work has several limitations that warrant further investigation:</p>
<p>â€¢ Scalability and Real-World Deployment: While the proposed pipeline demonstrates strong performance in controlled environments, this work does not address the challenges of scaling the pipeline for real-world applications.Issues such as handling extremely large datasets, ensuring low latency, and optimizing cost-effective deployment for different organizational needs remain unaddressed and require further research.â€¢ Qualitative Analysis: While quantitative evaluations on metrics like CheckEval and G-Eval demonstrate strong performance, this study lacks a comprehensive qualitative analysis of the generated summaries.â€¢ Document Retrieval Scope: The pipeline assumes a predefined set of input papers for summarization and does not address the challenge of identifying or retrieving relevant documents for a specific topic.This limitation highlights the need for further research into integrating robust document retrieval mechanisms with summarization workflows to enhance the pipeline's applicability.</p>
<p>A. Appendix</p>
<p>A.1. Examples of Generate Questions</p>
<p>Below are examples of generated questions produced by our question-generation module.</p>
<p>Generated Questions Example 1</p>
<p>Paper Title: Automatic melody harmonization with triad chords: A comparative study Paper Abstract: Several prior works have proposed various methods for the task of automatic melody harmonization, in which a model aims to generate a sequence of chords to serve as the harmonic accompaniment of a given multiple-bar melody sequence.In this paper, we present a comparative study evaluating and comparing the performance of a set of canonical approaches to this task, including a template matching based model, a hidden Markov based model, a genetic algorithm based model, and two deep learning based models.The evaluation is conducted on a dataset of 9,226 melody/chord pairs we newly collect for this study, considering up to 48 triad chords, using a standardized training/test split.We report the result of an objective evaluation using six different metrics and a subjective study with 202 participants.</p>
<p>Generated Questions:</p>
<p>â€¢ What are the key differences in performance among the various models evaluated for automatic melody harmonization?â€¢ How does the dataset of 9,226 melody/chord pairs contribute to the robustness of the study's findings?â€¢ What specific metrics were used for the objective evaluation of the models, and how do they compare in terms of effectiveness? â€¢ What insights were gained from the subjective study involving 202 participants regarding the perceived quality of the harmonizations?â€¢ What future directions for research in automatic melody harmonization does this study suggest based on its findings?</p>
<p>Generated Questions Example 2</p>
<p>Paper Title: Virtuosonet: A hierarchical rnn-based system for modeling expressive piano performance</p>
<p>Paper Abstract: In this paper, we present our application of deep neural networks to modeling piano performance, which imitates the expressive control of tempo, dynamics, articulations, and pedaling from pianists.Our model consists of recurrent neural networks with hierarchical attention and a conditional variational autoencoder.The model takes a sequence of note-level score features extracted from MusicXML as input and predicts piano performance features of the corresponding notes.To render musical expressions consistently over long-term sections, we first predict tempo and dynamics at the measure level and, based on the result, refine them at the note level.The evaluation through listening tests shows that our model achieves a more human-like expressiveness compared to previous models.We also share the dataset used for the experiment.</p>
<p>Generated Questions:</p>
<p>â€¢ What are the key components of the hierarchical RNN architecture used in Virtuosonet for modeling expressive piano performance?</p>
<p>Ground Truth (Section Text):</p>
<p>The self-attention attention module in transformers updates the representation of each input token by attending to all tokens in the input sequence.The quadratic time complexity of the self-attention module limits the application of T-PTLMs to long input sequences.To overcome this drawback, self-attention variants like sparse self-attention and linearized self-attention are proposed to reduce its complexity and hence extend T-PTLMs to long input sequences also BIBREF205 .Some of the popular T-PTLMs based on a) sparse self-attention are Longformer BIBREF206 , ETC BIBREF207 , BigBird BIBREF208 and Reformer BIBREF209 and b) linearized self-attention are Performer BIBREF210 .Sparse self-attention reduces the complexity by including sparsity bias which reduces the number of query-key pairs that each query attends to.In linearized self-attention, reduced complexity is achieved by disentangling the attention with kernel feature maps and then computing the attention in reverse order.</p>
<p>Generated Content (XSum Output):</p>
<p>The the relations between pairs of sentences and to construct the templates for each relation based on the obtained pairs.It is worth noting that the sentences generated by GPT-2 might have the same label as the original input sentence.In addition, BIBREF228 show that the generated counterexamples can be helpful to improve the performance of the models, e.g. for natural language inference, duplicate question detection, and sentiment analysis.Other research is informing the task of natural language explanation generation , where the goal is to expose the rationale behind the model decisions in automatically generated natural language text.Any approach must critically require that the generated response is faithful to model behavior.To this end, BIBREF230 propose to first generate the explanations, and then employ the explanations to obtain the final model predictions.They use natural language inference as the task requiring explanations.Label-specific GPT-2 models are fine-tuned over concatenations of corresponding premises, hypotheses, and human-provided explanations, so that at inference, the model generates an explanation based on premise and hypothesis.Next, the explanations together with the premise and the hypothesis are consumed by an explanation processor model (e.g.RoBERTa) to select the most likely label.This process obtains a more faithful explanation for the label choice, compared to traditional prediction-first approaches BIBREF229 .However, this approach does not provide explanations that reference non-selected labels.To address the question of why other labels are not chosen, BIBREF231 exploit counterexamples, deriving them from original samples with heuristic rules.The original samples and counterexamples are provided to GPT-2 to generate an explanation for the question 201c Why A not B 201d.</p>
<p>Generated Content (XSum Output):</p>
<p>The exploration of data generation and model explanation through large pre-trained language models (PLMs) has revealed critical insights into the challenges and advancements in the field.A significant challenge in the interpretability of machine learning models, particularly in natural language processing (NLP), lies in the complexity of the models themselves.Explanations must be comprehensible to users with varying levels of expertise, necessitating a balance between accuracy and interpretability.Techniques such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) have emerged as effective methods for generating explanations that enhance user trust in model predictions.LIME has demonstrated high recall rates and usability, enabling users to better assess the trustworthiness of individual predictions, while SHAP offers a theoretically robust framework that aligns closely with human intuition, improving the interpretability of complex models [BIBREF220], [BIBREF221].Trust is a pivotal factor in the deployment of machine learning models, as it influences user reliance on model predictions.Explanations play a crucial role in fostering this trust by elucidating the rationale behind predictions, thereby enabling users to make informed decisions.The experiments conducted with human subjects highlighted that while explanations can enhance understanding, they may also reveal underlying issues with model predictions, leading to a decrease in trust when users discern that predictions are based on untrustworthy reasons [BIBREF220].Furthermore, the model-agnostic nature of explanation methods like LIME and SHAP allows for their application across various classifiers, facilitating feature engineering and improving model performance innovative methodologies for generating explanations that clarify model predictions.These approaches focus on generating plausible and diverse counterfactuals that can reveal model behavior and facilitate debugging processes.The ability to produce minimal edits that lead to significant changes in model outputs enhances the interpretability of complex models while maintaining fluency and grammaticality in generated explanations [BIBREF235], [BIBREF236].Overall, the insights gained from these advancements underscore the importance of developing robust methods for explanation generation, which are essential for fostering trust and understanding in machine learning systems deployed in real-world applications.</p>
<p>Figure 2 :
2
Figure 2: Overview of Pipeline 2: Instead of ranking with a neural model, this pipeline encodes chunks as dense embeddings using SPECTER2, stores them in a FAISS vector database, retrieves them based on section title queries, and applies reranking before LLM-based summarization.</p>
<p>Figure 3 :
3
Figure3: Overview of the XSum Pipeline.The pipeline processes reference papers into summaries through modular steps.Document Pre-Processing segments papers into chunks, encodes them as embeddings, and stores them in a FAISS database.Question Generation uses an LLM to generate questions from titles and abstracts.In Question Answering, a RAG framework retrieves relevant chunks and generates answers with an LLM.Finally, the Editor Module (Final Summary Generation) synthesizes the answers into a coherent, citation-rich summary.</p>
<p>[BIBREF220], [BIBREF221].Recent advancements in instancewise feature selection and the use of mutual information have further enhanced model interpretation by providing tailored explanations for specific predictions.This approach contrasts with traditional global feature importance metrics, allowing for a more nuanced understanding of model behavior [BIBREF222].The introduction of multimodal explanation frameworks, which combine visual and textual justifications, has also shown promise in improving interpretability by leveraging the strengths of different modalities [BIBREF223].Additionally, the development of datasets such as ACT-X and VQA-X has provided valuable resources for evaluating multimodal explanations in activity recognition and visual question answering tasks, thereby advancing the field of explainable AI [BIBREF223].The integration of counterfactual explanations, particularly through frameworks like GYC and MiCE, has introduced</p>
<p>Overview of Pipeline 1: The system segments full-text papers into overlapping chunks, ranks them using monoT5-3B based on the section title, and selects the top-ranked chunks for LLM-based summarization.
Full TextChunksRelevant ChunksFinal SummaryReference PapersDocument Pre-ProcessingRankingLLMPapers Figure 1: ReferenceDocument Pre-Processing Full Text ChunksFAISSSection Title Relevant Chunks RerankingRAGFinal Summary</p>
<p>writing the final script of an interview with an expert on the topic '{topic}'.The final script should summarize the key insights and findings from the questions and answers provided.Keep the target audience in mind, which includes researchers, students, and professionals in the field.</p>
<h3>QUESTIONS AND ANSWERS ### {questions_and_answers} ### INSTRUCTIONS ###Include the most relevant and important points discussed.Be aware of plagiarism, i.e., you should not copy the text, but use them as inspiration.Avoid using markdown formatting in the text.Avoid splitting into subsections, or creating an introduction and conclusion for it.Avoid introducing new information and focus on summarizing the existing content.Always include the citations (e.g., [BIBREF14], [BIBREF16]) mentioned in the answers in the final section.</h3>
<p>Table 1
1
Comprehensive performance comparison of the evaluated pipelines based on traditional metrics (ROUGE, BERTScore, Ref-F1) and LLM-based metrics (G-Eval, CheckEval).
PipelineROUGE-1 ROUGE-2 ROUGE-L BERTScore Ref-F1 G-Eval CheckEvalPipeline_10.420.080.190.570.643.10.61Pipeline_20.490.100.230.590.724.00.76XSum0.510.100.240.620.764.20.97</p>
<p>â€¢ How does the model differentiate between measure-level and note-level predictions in terms of tempo and dynamics?â€¢ What specific features from MusicXML are utilized as input for the model, and how do they contribute to the performance output?â€¢ In what ways does the evaluation through listening tests demonstrate the model's superiority in expressiveness compared to previous approaches?â€¢ What insights can be drawn from the dataset shared in the paper regarding the training and evaluation of deep music generation models?
A.2. Evaluation ExamplesEvaluation Example 1Survey Title: AMMUS:A Survey of Transformer-based Pretrained Models in Natural LanguageProcessingSurvey Section: Taxonomy:Extensions:Long-Sequence T-PTLMsEvaluation Scores: ROUGE-1: 0.61, ROUGE-2: 0.19, ROUGE-L: 0.32, BERTScore: 0.69, Ref-F1:1.00, G-Eval: 4.8, CheckEval: 1.00</p>
<p>exploration of Transformer-based pretrained models, particularly in the context of longsequence processing, has revealed significant advancements and innovations across various architectures.Key modifications to the original Transformer model have emerged, including Adaptive Computation Time (ACT), hierarchical structures, and dynamic halting mechanisms, all aimed at enhancing computational efficiency and representation capabilities for complex inputs [BIBREF205].The introduction of models like Longformer, with its linear scaling attention mechanism and localglobal attention integration, demonstrates a marked improvement in handling lengthy documents compared to traditional self-attention approaches, outperforming models such as RoBERTa in tasks requiring extensive context [BIBREF206].The Extended Transformer Construction (ETC) architecture further addresses the challenges of scaling input length and encoding structured data through its global-local attention mechanism and the use of relative position encodings, which significantly enhance the model's performance on tasks like HotpotQA and WikiHop [BIBREF207].Similarly, BigBird's sparse attention mechanism allows for efficient processing of long sequences while maintaining the representational power of traditional Transformers, achieving state-of-the-art results in question answering and summarization tasks [BIBREF208].The Reformer model introduces techniques such as Locality-Sensitive Hashing (LSH) attention and reversible residual layers, which contribute to its memory efficiency and ability to handle long sequences effectively, demonstrating comparable performance to standard Transformers across various NLP tasks [BIBREF209].Performers leverage the FAVOR+ mechanism to achieve nearly linear time complexity and sub-quadratic memory consumption, enhancing both efficiency and accuracy in large-scale applications, protein sequence modeling and pixel-prediction tasks [BIBREF210].These advancements collectively indicate a promising trajectory for future research, emphasizing the need for continued exploration of efficient architectures and pre-training strategies that can further enhance the capabilities of Transformer-based models in processing long sequences and complex data structures across diverse domains.Despite the impressive performance of deep learning models for various NLP tasks, a remaining challenge to widespread adoption is the lack of explanations for the models' decisions.This hinders the development and debugging process, as well as user trust.This is especially true for application domains such as healthcare, security, and online education.
Evaluation Example 2Survey Title: Recent Advances in Natural Language Processing via Large Pre-Trained LanguageModels: A SurveySurvey Section: Data Generation via PLM:Explaining Models' DecisionsEvaluation Scores: ROUGE-1: 0.35, ROUGE-2: 0.05, ROUGE-L: 0.13, BERTScore: 0.63, Ref-F1:0.35, G-Eval: 4.5, CheckEval: 0.11Ground Truth (Section Text):As such, a considerable numberof approaches have been proposed for explaining deep learning models' behavior, includingmodel-intrinsic BIBREF220 , BIBREF221 , BIBREF222 and model-agnostic approaches BIBREF223 ,BIBREF224 , BIBREF225 . While model-intrinsic explanations expose internal model state (e.g.feature importance or attention scores), in model-agnostic (post-hoc) methods, explanations aregenerated via the model predictions without inspecting the internal state. Generative modelsare often applied for post-hoc explanations, aiming to obtain either counterexamples BIBREF226, BIBREF227 , BIBREF228 or natural language texts BIBREF229 , BIBREF230 , BIBREF231 forexplaining purposes. Generating counterexamples can shed light on the decision boundaries ofthe models (i.e. explaining when a model changes its decision), thus improving intepretability. Tothis end, the generated counterexamples should be close to the decision boundaries so that smallmodifications result in changing the model predictions. Traditionally, heuristic rules applied to theoriginal inputs create likely counterexamples BIBREF227 , BIBREF232 , BIBREF233 , BIBREF234 .PLMs have been leveraged to generate more diverse examples for better evaluation BIBREF235 ,BIBREF228 , BIBREF236 . In particular, BIBREF228 proposes a method based on GPT-2 to generatecounterfactuals that are close to the original sentences and entail specific relationships withthe original, facilitating label induction (e.g. negation, insertion, shuffle). Concretely, an inputsentence is concatenated with a relation label (e.g. negation) and a template consisting of thespecial tokens [BLANK] to form the prompt for GPT-2 model. For instance, for the sentence201c It is great for kids Ã¤nd the relation label 201c negate Â¨, the following prompt is constructed: 201c It is great for kids. [negation] It is [BLANK] great for [BLANK]. [SEP] Â¨. Next, the GPT-2
model generates answers for the [BLANK] in the template (e.g.201c not [ANSWER] children 201d, separated by the special token [ANSWER] ).To fine-tune the GPT-2 model, non-parallel datasets (e.g.CommonGen, Natural Questions and SQuAD) are automatically processed to find</p>
<p>https://openscilm.allen.ai/
https://huggingface.co/allenai/specter2
https://github.com/unicamp-dl/surveysum
https://github.com/nlpyang/geval
https://github.com/jayralencar/check-eval</p>
<p>Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. M Krenn, L Buffoni, B C Coutinho, S Eppel, J G Foster, A Gritsevskiy, H Lee, Y Lu, J P Moutinho, N Sanjabi, R Sonthalia, N M Tran, F Valente, Y Xie, R Yu, M Kopp, 10.1038/S42256-023-00735-0doi:10.1038/S42256-023-00735-0Nat. Mac. Intell. 52023</p>
<p>Towards multi-paper summarization using reference information. H Nanba, M Okumura, Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence. T Dean, the Sixteenth International Joint Conference on Artificial IntelligenceStockholm, SwedenMorgan KaufmannJuly 31 -August 6, 1999. 2 Volumes, 1450. 199999</p>
<p>A systematic survey of text summarization: From statistical methods to large language models. H Zhang, P S Yu, J Zhang, 10.48550/ARXIV.2406.11289doi:10.48550/ARXIV.2406.11289.arXiv:2406.112892024</p>
<p>Surveysum: A dataset for summarizing multiple scientific articles into a survey section. L C Fernandes, G B Guedes, T S Laitz, T S Almeida, R F Nogueira, R A Lotufo, J Pereira, 10.48550/ARXIV.2408.16444doi:10.48550/ARXIV.2408.16444.arXiv:2408.164442024</p>
<p>Checkeval: Robust evaluation framework using large language model via checklist. Y Lee, J Kim, J Kim, H Cho, P Kang, 10.48550/ARXIV.2403.18771doi:10.48550/ARXIV.2403.18771.arXiv:2403.187712024</p>
<p>Y Liu, D Iter, Y Xu, S Wang, R Xu, C Zhu, G-Eval , 10.48550/ARXIV.2303.16634doi:10.48550/ARXIV.2303.16634.arXiv:2303.16634NLG evaluation using GPT-4 with better human alignment. 2023</p>
<p>SciSumm: A multi-document summarization system for scientific articles. N Agarwal, R S Reddy, K Gvr, C P RosÃ©, Proceedings of the ACL-HLT 2011 System Demonstrations. S Kurohashi, the ACL-HLT 2011 System DemonstrationsPortland, OregonAssociation for Computational Linguistics2011</p>
<p>Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. A Fabbri, I Li, T She, S Li, D Radev, 10.18653/v1/P19-1102Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. A Korhonen, D Traum, L MÃ rquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Hierarchical transformers for multi-document summarization. Y Liu, M Lapata, 10.18653/v1/P19-1500Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. A Korhonen, D Traum, L MÃ rquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>An extractive-abstractive approach for multi-document summarization of scientific articles for literature review. K Shinde, T Roy, T Ghosal, ; A Cohan, G Feigenblat, D Freitag, T Ghosal, D Herrmannova, P Knoth, K Lo, P Mayr, M Shmueli-Scheuer, Proceedings of the Third Workshop on Scholarly Document Processing. A De Waard, L L Wang, the Third Workshop on Scholarly Document ProcessingGyeongju, Republic of KoreaAssociation for Computational Linguistics2022</p>
<p>Multi-document scientific summarization from a knowledge graph-centric view. P Wang, S Li, K Pang, L He, D Li, J Tang, T Wang, ; N Calzolari, C Huang, H Kim, J Pustejovsky, L Wanner, K Choi, P Ryu, H Chen, L Donatelli, H Ji, S Kurohashi, P Paggio, N Xue, S Kim, Y Hahm, Z He, T K Lee, E Santus, F , Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022. S Bond, Na, the 29th International Conference on Computational Linguistics, COLING 2022Gyeongju, Republic of KoreaInternational Committee on Computational LinguisticsOctober 12-17, 2022. 2022</p>
<p>Openscholar: Synthesizing scientific literature with retrieval-augmented lms. A Asai, J He, R Shao, W Shi, A Singh, J C Chang, K Lo, L Soldaini, S Feldman, M D'arcy, D Wadden, M Latzke, M Tian, P Ji, S Liu, H Tong, B Wu, Y Xiong, L Zettlemoyer, G Neubig, D S Weld, D Downey, W Yih, P W Koh, H Hajishirzi, 10.48550/ARXIV.2411.14199doi:10.48550/ARXIV.2411.14199.arXiv:2411.141992024</p>
<p>Cochrane-auto: An aligned dataset for the simplification of biomedical abstracts. J Bakker, J Kamps, 10.18653/v1/2024.tsar-1.5Proceedings of the Third Workshop on Text Simplification, Accessibility and Readability (TSAR 2024). M Shardlow, H Saggion, F Alva-Manchego, M Zampieri, K North, S Å tajner, R Stodden, the Third Workshop on Text Simplification, Accessibility and Readability (TSAR 2024)Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>MS2: multi-document summarization of medical studies. J Deyoung, I Beltagy, M Van Zuylen, B Kuehl, L L Wang, arXiv:2104.064862021</p>
<p>Multi-xscience: A large-scale dataset for extreme multi-document summarization of scientific articles. Y Lu, Y Dong, L Charlin, 10.18653/V1/2020.EMNLP-MAIN.648Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. B Webber, T Cohn, Y He, Y Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2020. November 16-20, 2020. 2020</p>
<p>Precise zero-shot dense retrieval without relevance labels. L Gao, X Ma, J Lin, J Callan, 10.48550/ARXIV.2212.10496doi:10.48550/ARXIV.2212.10496.arXiv:2212.104962022</p>
<p>Hyqe: Ranking contexts with hypothetical query embeddings. W Zhou, J Zhang, H Hasson, A Singh, W Li, 10.48550/ARXIV.2410.15262doi:10.48550/ARXIV.2410.15262.arXiv:2410.152622024</p>
<p>Retrieval-augmented generation for large language models: A survey. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, Q Guo, M Wang, H Wang, 10.48550/ARXIV.2312.10997doi:10.48550/ARXIV.2312.10997.arXiv:2312.109972023</p>
<p>Colbertv2: Effective and efficient retrieval via lightweight late interaction. K Santhanam, O Khattab, J Saad-Falcon, C Potts, M Zaharia, arXiv:2112.014882021</p>
<p>ROUGE: A package for automatic evaluation of summaries, in: Text Summarization Branches Out. C.-Y Lin, 2004Association for Computational LinguisticsBarcelona, Spain</p>
<p>T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, Bertscore, arXiv:1904.09675Evaluating text generation with BERT. 2019</p>
<p>Summeval: Re-evaluating summarization evaluation. A R Fabbri, W Kryscinski, B Mccann, C Xiong, R Socher, D R Radev, 10.1162/TACL_A_00373doi:10.1162/TACL_A_00373Trans. Assoc. Comput. Linguistics. 92021</p>
<p>Leveraging large language models for NLG evaluation: Advances and challenges. Z Li, X Xu, T Shen, C Xu, J Gu, Y Lai, C Tao, S Ma, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024. November 12-16, 2024. 2024</p>
<p>Colpali: Efficient document retrieval with vision language models. M Faysse, H Sibille, T Wu, B Omrani, G Viaud, C Hudelot, P Colombo, 10.48550/ARXIV.2407.01449doi:10.48550/ARXIV.2407.01449.arXiv:2407.014492024</p>            </div>
        </div>

    </div>
</body>
</html>
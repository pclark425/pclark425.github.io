<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-773 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-773</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-773</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-264172518</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.13.pdf" target="_blank">Theory of Mind for Multi-Agent Collaboration via Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e773.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e773.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based Embodied Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Embodied Agents (GPT-4 / gpt-3.5-turbo-0301 / ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based chat models prompted to act as embodied agents in a decentralized, turn-taking multi-agent text game; they produce natural-language actions and messages, reason zero-shot, and can be augmented with an explicit textual belief state via prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 / ChatGPT (LLM-based embodied agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Large pre-trained chat models (GPT-4 and gpt-3.5-turbo/ChatGPT) used as embodied agents. Each model is prompted with game rules and recent interaction history (memory of previous two rounds, up to 4096 tokens). In each agent turn the model outputs (1) an action choice in a fixed template (e.g., Move to Room X, Inspect Bomb, Apply X Tool) and (2) an optional team message. Models run zero-shot (temperature set to 0) and rely on their internal language reasoning; in the GPT-4+Belief condition the LLM is additionally prompted to update a textual belief-state summary which is then appended to its history for future planning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Custom multi-agent text-based bomb-defusal game (search-and-rescue style)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A text interface to a 5-node graph environment (rooms numbered 0,3,5,6,8) with hidden bombs having color-phase sequences; three agents (Alpha, Bravo, Charlie) take turns, each only observing their current room contents, teammate locations and received messages. The task is partially observable and requires coordination, inspection actions, tool (wire-cutter) use in sequence, and navigation across adjacent rooms. Challenges: limited per-agent observability, long-horizon planning, ambiguous/propagated beliefs via communication, and a compact action template that must be strictly followed.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>None as external software tools for planning; agents rely on the text game interface for observations and (optionally) a prompt-engineered explicit belief-state text that the LLM itself maintains and updates. No external map service, search engine, or planner is invoked by the LLM agents in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual natural-language observations (templated environment descriptions), encoded action-feedback/error messages (templated text), and teammate messages broadcast as text.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Prompt-engineered explicit textual belief state: a human-specified template holding key task facts (agent role, current round, observed room contents, teammate locations, room connectivity, bomb intel, tool inventory). Upon receiving an observation the model is prompted to produce an updated textual belief summary which is appended to the model's prompt/history and used for subsequent planning.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>When an agent receives an observation from the text interface it is prompted (zero-shot, guided by an initial example format) to modify the textual belief-state description: recognized changes (e.g., inspected a bomb -> update bomb sequence; moved rooms -> update own location; communicated messages -> optionally update teammates' knowledge). The updated belief text is preserved in the agent's interaction history and carried to future rounds until further updates. The mechanism is purely textual (no external memory network beyond prompt context) and is inspired by chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Zero-shot LLM planning / chain-of-thought style reasoning within the prompt: the model reasons over the provided belief text, recent observation history, and game rules to select actions and compose messages. No explicit model-based search (by the LLM) or external planner is invoked in the LLM-only condition.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Discrete action selection of adjacent-room moves via natural-language commands. The LLM produces Move to Room X actions; the text interface enforces adjacency and returns error messages for invalid moves. No explicit graph search (e.g., A*) is performed by the LLM; navigation is planned implicitly via LLM reasoning over the belief text and room connectivity listed in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>When augmented with the explicit textual belief state (treated as the 'tool' / prompt augmentation), GPT-4+Belief: team score = 90 points (max), rounds to completion = 12.3 (average), valid action rate = 86.1%. (Table values from experiments.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>GPT-4 without explicit belief state: team score = 90 points, rounds to completion = 28.3 (average), valid action rate = 71.8%. ChatGPT baseline (no belief): team score = 43 ± 4.7, rounds = 30.0, valid action rate = 2.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt-engineered explicit textual belief states substantially improve LLM agents' performance under partial observability: they reduce invalid/hallucinated actions (paper reports a 50.7% reduction in invalid actions) and speed completion (~130% efficiency improvement). LLMs otherwise suffer from long-horizon context neglect and hallucination about the task state; the belief-state augmentation mitigates these by re-emphasizing key facts in the prompt. LLM agents do not use an external MAPF planner; navigation is implicit and error-prone without explicit belief reinforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind for Multi-Agent Collaboration via Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e773.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e773.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explicit Belief State</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-engineered Explicit Textual Belief State</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-designed textual memory template appended to the LLM prompt that stores the agent's key task-related beliefs (locations, bomb intel, tool inventories, room connectivity) and is updated as the agent receives observations; used to mitigate partial observability and long-horizon forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>N/A (representation used by LLM-based agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A structured natural-language summary template provided to each agent containing fields for role, current round, observation, teammate locations, room connectivity, bomb intel (per-bomb known/unknown sequences), and tool inventories. The LLM is prompted to update this textual belief-state each round based on its new observations and communications.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Custom multi-agent text-based bomb-defusal game (same environment as agents)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable multi-agent text environment where agents only see their current room and received messages; maintaining a compact, persistent belief summary is necessary to track long-horizon facts like bomb sequences and teammate knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>The belief text itself is not an external software tool but a prompt-engineered internal memory artifact; it relies on environment text outputs from the text interface to update.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual structured summary (templated natural-language entries for bombs, rooms, tools, locations).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Textual summary stored in the LLM prompt / interaction history; initial example belief is provided and subsequent updates are generated by the LLM in zero-shot fashion upon receiving observations.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>On each observation, the agent is prompted to revise the belief template: e.g. after Inspect Bomb the bomb's sequence field is changed from Unknown to the observed color sequence; after moving the agent's 'current room' field is updated; messages may be incorporated as updates to teammates' presumed knowledge. The updated belief text remains in subsequent prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Provides a persistent context to the LLM's zero-shot reasoning; it is not a planner per se but a memory augmentation that improves chain-of-thought style planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>When used (GPT-4+Belief) resulted in rounds to completion = 12.3 and valid action rate = 86.1% (score 90).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Without explicit belief (GPT-4): rounds = 28.3, valid action rate = 71.8% (score 90).</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit textual belief state (prompt engineering) materially improves LLM planning and ToM inference accuracy under partial observability by making long-horizon facts persist in model input; it reduces hallucinations about state and invalid actions and increases both efficiency and higher-order ToM correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind for Multi-Agent Collaboration via Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e773.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e773.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBS Planner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conflict-Based Search (CBS) Multi-Agent Path-Finding Planner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art MAPF algorithm that generates collision-free, precedence-aware paths and task assignments by expanding a binary constraint tree in a best-first manner; here augmented to maximize team reward with a user-defined heuristic and used as an optimal centralized planner baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conflict-based search for optimal multi-agent pathfinding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Conflict-Based Search (CBS) Planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A centralized MAPF planner that simultaneously generates task assignments and collision-free paths for multiple agents while respecting temporal and precedence constraints. In this work the planner is augmented to optimize a user-defined return (team reward from defusing bombs), uses a user-provided heuristic (e.g., sort bombs by distance), partitions actions into subtasks via a hyperparameter, and resolves conflicts by expanding a binary constraint tree in best-first order to yield a feasible, provably optimal solution when considering the entire task as one subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same custom multi-agent text environment, but the CBS planner operates with centralized, full-information access to environment state</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Although the underlying environment is the same, CBS is applied with full observability of the global state (bomb locations and sequences known to the planner) and centralized control; it produces assignment and path sequences for agents over the room-graph.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>CBS itself is the external planner/tool used for task assignment and path planning; it consumes a full state and returns structured plans (agent assignments, timed paths).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured plans: ordered sequences of agent actions, collision-free paths across the room graph, assignments of subtasks to agents, and expected return (score).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Centralized full-state model (planner has perfect/world information), not a per-agent belief; planner maintains explicit environment state internally for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Search-based optimal MAPF planning (Conflict-Based Search) augmented with user-defined heuristics and subtask partitioning; resolves precedence/temporal conflicts via binary constraint-tree expansion in a best-first manner to maximize the team reward objective.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>CBS over the environment graph, uses heuristics (e.g., sort bombs by ascending distance) to order tasks, partitions actions into subtasks, generates assignments and collision-free paths by searching constraint tree; provably complete/optimal when treating the whole task as one subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Planner achieves optimal baseline: team score = 90 points, average rounds to completion = 6.0 rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The centralized CBS planner with perfect information is an optimal baseline and completes the task faster (6.0 rounds) than decentralized LLM teams; contrast highlights the challenges LLM agents face under partial observability and decentralized communication when they do not have an external centralized planner.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind for Multi-Agent Collaboration via Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e773.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e773.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAPPO Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Proximal Policy Optimization (MAPPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stateful actor-critic multi-agent RL algorithm (recurrent policy/critic sharing parameters) trained with SMAC hyperparameters as a learned-policy baseline for the cooperative bomb-defusal task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MAPPO (stateful recurrent actor-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A recurrent actor-critic MARL model with shared actor and critic networks, trained using the MAPPO algorithm and default SMAC hyperparameters; recurrent architecture provides memory across time steps to handle partial observability and avoid lazy agents. Agents observe teammates' locations and receive shaped rewards (intermediate +1 for correct wirecut, −1 for wrong cut) to speed learning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Custom multi-agent text-based bomb-defusal game (converted to a gym-style environment for MARL training)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same underlying task but here MARL agents are trained with access to the environment observations encoded into state features (teammate locations included). Partial observability is handled via recurrent networks and training.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>None (learned policy); no external planner is invoked at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Implicit memory via recurrent neural network hidden states in actor/critic; not an explicit textual belief representation.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Beliefs are implicit in RNN hidden states updated each timestep through learned recurrent dynamics conditioned on observations and rewards during training.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (model-free reinforcement learning) using MAPPO with reward shaping for intermediate feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation decisions are produced by the learned policy; no explicit graph search or MAPF algorithm is used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>MAPPO achieved team score = 90 points after training, average rounds to completion = 11.0 (after 45M timesteps of training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A trained MAPPO policy can match maximal team score but requires extensive training (millions of timesteps) and yields intermediate efficiency (11.0 rounds) between the optimal centralized CBS planner and LLM teams; unlike LLMs, MAPPO uses learned implicit memory to cope with partial observability rather than explicit textual belief states.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind for Multi-Agent Collaboration via Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e773.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e773.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text Game Interface</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-based Text Game Interface (Observation & Action Encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rule-based interface that translates environment state into templated natural-language observations, encodes LLM natural-language outputs into abstract actions through keyword matching, and returns templated error messages for invalid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Text Game Interface (environment <-> LLM adapter)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deterministic, rule-based adapter between the underlying gym environment and LLM agents: it extracts state features from the engine, fills natural-language templates describing round number, cumulative score, current room contents, teammate locations, and communication messages; it also parses LLM outputs via keyword matching into valid environment actions and returns error-correction messages when outputs are invalid or unintelligible.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Adapter to the custom multi-agent text-based bomb-defusal environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Provides partial observations (current room contents and teammate locations) in templated natural language and enforces action constraints (e.g., adjacency for movement); agents receive only the templated text and must adhere to a strict action format to be parsed into environment actions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Acts as a minimal external 'tool' delivering templated observations and error diagnostics; not a planner.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Templated textual observations, error-correction messages, and broadcasted teammate messages.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Does not itself maintain agent belief states but supplies the raw natural-language observations that agents use to update their own textual beliefs; also supplies the initial belief-state template.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The interface sends observations and messages which the LLMs use to update their internal prompt-held belief-state text; it also returns error messages that can prompt the LLM to correct invalid planned actions.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The rule-based text interface is critical to enforce action semantics and supply structured, consistent observations and error feedback; it enables LLM agents to function in a partially observable text environment but does not substitute for explicit planning tools—its templated outputs are the only external inputs LLM agents receive for updating beliefs and making navigation/action choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind for Multi-Agent Collaboration via Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conflict-based search for optimal multi-agent pathfinding <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Llm+ p: Empowering large language models with optimal planning proficiency <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>A plug-and-play multi-character belief tracker <em>(Rating: 1)</em></li>
                <li>Theory of mind as intrinsic motivation for multi-agent reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-773",
    "paper_id": "paper-264172518",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "LLM-based Embodied Agents",
            "name_full": "LLM-based Embodied Agents (GPT-4 / gpt-3.5-turbo-0301 / ChatGPT)",
            "brief_description": "Transformer-based chat models prompted to act as embodied agents in a decentralized, turn-taking multi-agent text game; they produce natural-language actions and messages, reason zero-shot, and can be augmented with an explicit textual belief state via prompt engineering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4 / ChatGPT (LLM-based embodied agents)",
            "agent_description": "Large pre-trained chat models (GPT-4 and gpt-3.5-turbo/ChatGPT) used as embodied agents. Each model is prompted with game rules and recent interaction history (memory of previous two rounds, up to 4096 tokens). In each agent turn the model outputs (1) an action choice in a fixed template (e.g., Move to Room X, Inspect Bomb, Apply X Tool) and (2) an optional team message. Models run zero-shot (temperature set to 0) and rely on their internal language reasoning; in the GPT-4+Belief condition the LLM is additionally prompted to update a textual belief-state summary which is then appended to its history for future planning.",
            "environment_name": "Custom multi-agent text-based bomb-defusal game (search-and-rescue style)",
            "environment_description": "A text interface to a 5-node graph environment (rooms numbered 0,3,5,6,8) with hidden bombs having color-phase sequences; three agents (Alpha, Bravo, Charlie) take turns, each only observing their current room contents, teammate locations and received messages. The task is partially observable and requires coordination, inspection actions, tool (wire-cutter) use in sequence, and navigation across adjacent rooms. Challenges: limited per-agent observability, long-horizon planning, ambiguous/propagated beliefs via communication, and a compact action template that must be strictly followed.",
            "is_partially_observable": true,
            "external_tools_used": "None as external software tools for planning; agents rely on the text game interface for observations and (optionally) a prompt-engineered explicit belief-state text that the LLM itself maintains and updates. No external map service, search engine, or planner is invoked by the LLM agents in the experiments.",
            "tool_output_types": "Textual natural-language observations (templated environment descriptions), encoded action-feedback/error messages (templated text), and teammate messages broadcast as text.",
            "belief_state_mechanism": "Prompt-engineered explicit textual belief state: a human-specified template holding key task facts (agent role, current round, observed room contents, teammate locations, room connectivity, bomb intel, tool inventory). Upon receiving an observation the model is prompted to produce an updated textual belief summary which is appended to the model's prompt/history and used for subsequent planning.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "When an agent receives an observation from the text interface it is prompted (zero-shot, guided by an initial example format) to modify the textual belief-state description: recognized changes (e.g., inspected a bomb -&gt; update bomb sequence; moved rooms -&gt; update own location; communicated messages -&gt; optionally update teammates' knowledge). The updated belief text is preserved in the agent's interaction history and carried to future rounds until further updates. The mechanism is purely textual (no external memory network beyond prompt context) and is inspired by chain-of-thought prompting.",
            "planning_approach": "Zero-shot LLM planning / chain-of-thought style reasoning within the prompt: the model reasons over the provided belief text, recent observation history, and game rules to select actions and compose messages. No explicit model-based search (by the LLM) or external planner is invoked in the LLM-only condition.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Discrete action selection of adjacent-room moves via natural-language commands. The LLM produces Move to Room X actions; the text interface enforces adjacency and returns error messages for invalid moves. No explicit graph search (e.g., A*) is performed by the LLM; navigation is planned implicitly via LLM reasoning over the belief text and room connectivity listed in the prompt.",
            "performance_with_tools": "When augmented with the explicit textual belief state (treated as the 'tool' / prompt augmentation), GPT-4+Belief: team score = 90 points (max), rounds to completion = 12.3 (average), valid action rate = 86.1%. (Table values from experiments.)",
            "performance_without_tools": "GPT-4 without explicit belief state: team score = 90 points, rounds to completion = 28.3 (average), valid action rate = 71.8%. ChatGPT baseline (no belief): team score = 43 ± 4.7, rounds = 30.0, valid action rate = 2.5%.",
            "has_tool_ablation": true,
            "key_findings": "Prompt-engineered explicit textual belief states substantially improve LLM agents' performance under partial observability: they reduce invalid/hallucinated actions (paper reports a 50.7% reduction in invalid actions) and speed completion (~130% efficiency improvement). LLMs otherwise suffer from long-horizon context neglect and hallucination about the task state; the belief-state augmentation mitigates these by re-emphasizing key facts in the prompt. LLM agents do not use an external MAPF planner; navigation is implicit and error-prone without explicit belief reinforcement.",
            "uuid": "e773.0",
            "source_info": {
                "paper_title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Explicit Belief State",
            "name_full": "Prompt-engineered Explicit Textual Belief State",
            "brief_description": "A human-designed textual memory template appended to the LLM prompt that stores the agent's key task-related beliefs (locations, bomb intel, tool inventories, room connectivity) and is updated as the agent receives observations; used to mitigate partial observability and long-horizon forgetting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "N/A (representation used by LLM-based agents)",
            "agent_description": "A structured natural-language summary template provided to each agent containing fields for role, current round, observation, teammate locations, room connectivity, bomb intel (per-bomb known/unknown sequences), and tool inventories. The LLM is prompted to update this textual belief-state each round based on its new observations and communications.",
            "environment_name": "Custom multi-agent text-based bomb-defusal game (same environment as agents)",
            "environment_description": "Partially observable multi-agent text environment where agents only see their current room and received messages; maintaining a compact, persistent belief summary is necessary to track long-horizon facts like bomb sequences and teammate knowledge.",
            "is_partially_observable": true,
            "external_tools_used": "The belief text itself is not an external software tool but a prompt-engineered internal memory artifact; it relies on environment text outputs from the text interface to update.",
            "tool_output_types": "Textual structured summary (templated natural-language entries for bombs, rooms, tools, locations).",
            "belief_state_mechanism": "Textual summary stored in the LLM prompt / interaction history; initial example belief is provided and subsequent updates are generated by the LLM in zero-shot fashion upon receiving observations.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "On each observation, the agent is prompted to revise the belief template: e.g. after Inspect Bomb the bomb's sequence field is changed from Unknown to the observed color sequence; after moving the agent's 'current room' field is updated; messages may be incorporated as updates to teammates' presumed knowledge. The updated belief text remains in subsequent prompts.",
            "planning_approach": "Provides a persistent context to the LLM's zero-shot reasoning; it is not a planner per se but a memory augmentation that improves chain-of-thought style planning.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": "When used (GPT-4+Belief) resulted in rounds to completion = 12.3 and valid action rate = 86.1% (score 90).",
            "performance_without_tools": "Without explicit belief (GPT-4): rounds = 28.3, valid action rate = 71.8% (score 90).",
            "has_tool_ablation": true,
            "key_findings": "Explicit textual belief state (prompt engineering) materially improves LLM planning and ToM inference accuracy under partial observability by making long-horizon facts persist in model input; it reduces hallucinations about state and invalid actions and increases both efficiency and higher-order ToM correctness.",
            "uuid": "e773.1",
            "source_info": {
                "paper_title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CBS Planner",
            "name_full": "Conflict-Based Search (CBS) Multi-Agent Path-Finding Planner",
            "brief_description": "A state-of-the-art MAPF algorithm that generates collision-free, precedence-aware paths and task assignments by expanding a binary constraint tree in a best-first manner; here augmented to maximize team reward with a user-defined heuristic and used as an optimal centralized planner baseline.",
            "citation_title": "Conflict-based search for optimal multi-agent pathfinding",
            "mention_or_use": "use",
            "agent_name": "Conflict-Based Search (CBS) Planner",
            "agent_description": "A centralized MAPF planner that simultaneously generates task assignments and collision-free paths for multiple agents while respecting temporal and precedence constraints. In this work the planner is augmented to optimize a user-defined return (team reward from defusing bombs), uses a user-provided heuristic (e.g., sort bombs by distance), partitions actions into subtasks via a hyperparameter, and resolves conflicts by expanding a binary constraint tree in best-first order to yield a feasible, provably optimal solution when considering the entire task as one subtask.",
            "environment_name": "Same custom multi-agent text environment, but the CBS planner operates with centralized, full-information access to environment state",
            "environment_description": "Although the underlying environment is the same, CBS is applied with full observability of the global state (bomb locations and sequences known to the planner) and centralized control; it produces assignment and path sequences for agents over the room-graph.",
            "is_partially_observable": false,
            "external_tools_used": "CBS itself is the external planner/tool used for task assignment and path planning; it consumes a full state and returns structured plans (agent assignments, timed paths).",
            "tool_output_types": "Structured plans: ordered sequences of agent actions, collision-free paths across the room graph, assignments of subtasks to agents, and expected return (score).",
            "belief_state_mechanism": "Centralized full-state model (planner has perfect/world information), not a per-agent belief; planner maintains explicit environment state internally for planning.",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Search-based optimal MAPF planning (Conflict-Based Search) augmented with user-defined heuristics and subtask partitioning; resolves precedence/temporal conflicts via binary constraint-tree expansion in a best-first manner to maximize the team reward objective.",
            "uses_shortest_path_planning": true,
            "navigation_method": "CBS over the environment graph, uses heuristics (e.g., sort bombs by ascending distance) to order tasks, partitions actions into subtasks, generates assignments and collision-free paths by searching constraint tree; provably complete/optimal when treating the whole task as one subtask.",
            "performance_with_tools": "Planner achieves optimal baseline: team score = 90 points, average rounds to completion = 6.0 rounds.",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "The centralized CBS planner with perfect information is an optimal baseline and completes the task faster (6.0 rounds) than decentralized LLM teams; contrast highlights the challenges LLM agents face under partial observability and decentralized communication when they do not have an external centralized planner.",
            "uuid": "e773.2",
            "source_info": {
                "paper_title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MAPPO Baseline",
            "name_full": "Multi-Agent Proximal Policy Optimization (MAPPO)",
            "brief_description": "A stateful actor-critic multi-agent RL algorithm (recurrent policy/critic sharing parameters) trained with SMAC hyperparameters as a learned-policy baseline for the cooperative bomb-defusal task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MAPPO (stateful recurrent actor-critic)",
            "agent_description": "A recurrent actor-critic MARL model with shared actor and critic networks, trained using the MAPPO algorithm and default SMAC hyperparameters; recurrent architecture provides memory across time steps to handle partial observability and avoid lazy agents. Agents observe teammates' locations and receive shaped rewards (intermediate +1 for correct wirecut, −1 for wrong cut) to speed learning.",
            "environment_name": "Custom multi-agent text-based bomb-defusal game (converted to a gym-style environment for MARL training)",
            "environment_description": "Same underlying task but here MARL agents are trained with access to the environment observations encoded into state features (teammate locations included). Partial observability is handled via recurrent networks and training.",
            "is_partially_observable": true,
            "external_tools_used": "None (learned policy); no external planner is invoked at inference time.",
            "tool_output_types": null,
            "belief_state_mechanism": "Implicit memory via recurrent neural network hidden states in actor/critic; not an explicit textual belief representation.",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": "Beliefs are implicit in RNN hidden states updated each timestep through learned recurrent dynamics conditioned on observations and rewards during training.",
            "planning_approach": "Learned policy (model-free reinforcement learning) using MAPPO with reward shaping for intermediate feedback.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation decisions are produced by the learned policy; no explicit graph search or MAPF algorithm is used.",
            "performance_with_tools": "MAPPO achieved team score = 90 points after training, average rounds to completion = 11.0 (after 45M timesteps of training).",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "A trained MAPPO policy can match maximal team score but requires extensive training (millions of timesteps) and yields intermediate efficiency (11.0 rounds) between the optimal centralized CBS planner and LLM teams; unlike LLMs, MAPPO uses learned implicit memory to cope with partial observability rather than explicit textual belief states.",
            "uuid": "e773.3",
            "source_info": {
                "paper_title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Text Game Interface",
            "name_full": "Rule-based Text Game Interface (Observation & Action Encoder)",
            "brief_description": "A rule-based interface that translates environment state into templated natural-language observations, encodes LLM natural-language outputs into abstract actions through keyword matching, and returns templated error messages for invalid actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Text Game Interface (environment &lt;-&gt; LLM adapter)",
            "agent_description": "A deterministic, rule-based adapter between the underlying gym environment and LLM agents: it extracts state features from the engine, fills natural-language templates describing round number, cumulative score, current room contents, teammate locations, and communication messages; it also parses LLM outputs via keyword matching into valid environment actions and returns error-correction messages when outputs are invalid or unintelligible.",
            "environment_name": "Adapter to the custom multi-agent text-based bomb-defusal environment",
            "environment_description": "Provides partial observations (current room contents and teammate locations) in templated natural language and enforces action constraints (e.g., adjacency for movement); agents receive only the templated text and must adhere to a strict action format to be parsed into environment actions.",
            "is_partially_observable": true,
            "external_tools_used": "Acts as a minimal external 'tool' delivering templated observations and error diagnostics; not a planner.",
            "tool_output_types": "Templated textual observations, error-correction messages, and broadcasted teammate messages.",
            "belief_state_mechanism": "Does not itself maintain agent belief states but supplies the raw natural-language observations that agents use to update their own textual beliefs; also supplies the initial belief-state template.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "The interface sends observations and messages which the LLMs use to update their internal prompt-held belief-state text; it also returns error messages that can prompt the LLM to correct invalid planned actions.",
            "planning_approach": null,
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "The rule-based text interface is critical to enforce action semantics and supply structured, consistent observations and error feedback; it enables LLM agents to function in a partially observable text environment but does not substitute for explicit planning tools—its templated outputs are the only external inputs LLM agents receive for updating beliefs and making navigation/action choices.",
            "uuid": "e773.4",
            "source_info": {
                "paper_title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conflict-based search for optimal multi-agent pathfinding",
            "rating": 2,
            "sanitized_title": "conflictbased_search_for_optimal_multiagent_pathfinding"
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "Llm+ p: Empowering large language models with optimal planning proficiency",
            "rating": 2,
            "sanitized_title": "llm_p_empowering_large_language_models_with_optimal_planning_proficiency"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "A plug-and-play multi-character belief tracker",
            "rating": 1,
            "sanitized_title": "a_plugandplay_multicharacter_belief_tracker"
        },
        {
            "paper_title": "Theory of mind as intrinsic motivation for multi-agent reinforcement learning",
            "rating": 1,
            "sanitized_title": "theory_of_mind_as_intrinsic_motivation_for_multiagent_reinforcement_learning"
        }
    ],
    "cost": 0.016516,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Theory of Mind for Multi-Agent Collaboration via Large Language Models</p>
<p>Huao Li 
University of Pittsburgh
PittsburghPA</p>
<p>Yu Quan yuquanc@andrew.cmu.edu 
Carnegie Mellon University
PittsburghPA</p>
<p>Simon Stepputtis 
Carnegie Mellon University
PittsburghPA</p>
<p>Joseph Campbell jacampbe@andrew.cmu.edu 
Carnegie Mellon University
PittsburghPA</p>
<p>Dana Hughes danahugh@andrew.cmu.edu 
Carnegie Mellon University
PittsburghPA</p>
<p>Michael Lewis cmlewis@pitt.edu 
University of Pittsburgh
PittsburghPA</p>
<p>Katia Sycara sycara@andrew.cmu.edu 
Carnegie Mellon University
PittsburghPA</p>
<p>Theory of Mind for Multi-Agent Collaboration via Large Language Models
67A206C07B01191C3B9482103E8DFFAC
While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored.This study evaluates LLMbased agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines.We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents.Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state.We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLMbased agents.Action selection: Inspect Bomb.Message to Team: "I am inspecting the bomb in Room 0. Bravo; please move to Room 3. Charlie; please move to Room 5." Round: 2 Score: 0 Results: You inspected Bomb 1.This bomb is a 1-stage bomb and its remaining sequence is Red.Communication Messages: Bravo: "Moving to Room 3 as suggested.Alpha; you can defuse the bomb in Room 0 with your red tool."…… Alpha</p>
<p>Introduction</p>
<p>Recent large language models (LLMs), such as GPT-4 (OpenAI, 2023), have demonstrated impressive competencies across a wide array of domains and tasks, ranging from mathematics to law, without the need for fine-tuning or special prompting (Bubeck et al., 2023).This advancement has significantly transformed the landscape of Natural Language Processing (NLP) research.Instead of developing domain-specific models for downstream applications, focus has shifted towards evaluating and harnessing LLMs' abilities to solve novel tasks.Such a shift is consistent with the idea of studying machine behaviors, an interdisciplinary approach that expands the conventional bounds of computer science and integrates insights from diverse scientific fields (Rahwan et al., 2019).Drawing inspiration from team science and group psychology (Hagendorff, 2023), our study concentrates on collective machine behavior, evaluating LLMs' proficiency in multi-agent collaborations.</p>
<p>There is ongoing debate regarding the intelligence levels of modern LLMs.While some argue that LLMs excel primarily in linguistic competence and struggle with cognitive abilities beyond language, known as functional competence, others demonstrate that LLMs can exhibit cognitive skills such as formal reasoning and world knowledge comprehension (Mahowald et al., 2023;Bubeck et al., 2023).Motivated to explore this argument, we designed a text-based game to evaluate LLMs' ability in embodied interactions, including exploring unknown environments, maintaining beliefs about the world and collaborating with other agents, which is critical for natural social interactions and artificial general intelligence (AGI).</p>
<p>Theory of Mind, the capacity to reason about others' concealed mental states, is fundamental to human social interactions, collaborations, and communications (Zhang et al., 2012).As LLMs increasingly participate in diverse social interactions with humans, their social intelligence is expected to improve for them to become effective collaborators (Williams et al., 2022;Li et al., 2022).For instance, a proficient AI assistant should be able to infer a human's preferences based on previous experiences without needing to ask.Recent studies have applied classic Theory-of-Mind tasks to several LLMs, concluding that current models (e.g., GPT-4) perform comparably to 9-yearold children (Kosinski, 2023).However, the research community has expressed doubts about the validity of text-based ToM tests on machine intelligence (Ullman, 2023;Sap et al., 2023).In response, our study proposes a novel evaluation of LLMs' high-order ToM in interactive teamwork scenarios, encompassing dynamic belief state evolution and rich intent communication between multiple agents.</p>
<p>The main contributions of this study include that we:</p>
<p>• Evaluate LLM-based agents' embodied interaction capability in multi-agent collaborative tasks against reinforcement learning and planning-based baselines Large language models, trained on vast text corpora, excel in text completion and various other Natural Language Processing (NLP) applications (Chowdhery et al., 2022;Thoppilan et al., 2022).Recent studies highlight their abilities for reasoning (Bubeck et al., 2023;Wei et al., 2022) and action plan generation (Liu et al., 2023;Xie et al., 2023), particularly when utilizing prompt engineering techniques like chain-of-thought.However, some researchers note these models' limitations in forming actionable plans when interacting with real-world objects (Ahn et al., 2022;Huang et al., 2022).GPT-4's capacity for embodied interactions via text-based games and real-world problems was assessed by Bubeck et al. (2023).Further studies explored the potential of LLM-powered embodied agents in Minecraft (Wang et al., 2023b,a).These investigations suggest that LLMs can perform tasks requiring environment understanding, task comprehension, action planning, feedback interpretation, and subsequent adaptation.Our study seeks to broaden this understanding by evaluating LLMs' planning abilities in cooperative multiagent scenarios.</p>
<p>Theory of Mind</p>
<p>Prior research has tested LLMs' Theory of Mind (ToM) via variants of text-based tests such as the unexpected transfer task (also known as Smarties Task) or unexpected contents task (also known as the "Maxi Task" or "Sally-Anne" Test) (Kosinski, 2023;Moghaddam and Honey, 2023).Results indicate that leading LLMs can pass more than 90% of these test cases.In contrast, Ullman (2023) found that LLMs struggle with complex ToM inferences involving communication or second-order beliefs.</p>
<p>In our study, ToM evaluations occur in the midst of an interactive team task, where the mental states of agents change dynamically with each interaction.As agents exchange information through communication at every timestamp, the complexity of reasoning increases, since agents' mental states may be updated through both observations and communication.Thus, our tests can be considered more challenging than the static text-based tests used in prior research.Theory of Mind (ToM) has been employed to enhance the performance of artificial agents in various contexts.Lim et al. (2020) introduced a method to integrate Bayesian Theory of Mind (BToM) (Baker et al., 2017) with optimal-planning agents in a cooperative game.The results indicate that an explicit representation of others' intentions enhances the performance of both agent-only and human-agent teams.SymbolicToM allows language models to maintain an explicit symbolic ToM for multiple characters in reading comprehension tasks using graphical representations (Sclar et al., 2023).Moreover, there is a significant body of research focusing on the application of ToM to boost collaboration in multi-agent reinforcement learning (Oguntola et al., 2023;Yuan et al., 2021).Inspired by these prior studies, we aim to enhance LLM-based agents' collaborative behaviors through explicit belief representations.</p>
<p>Multi-agent collaboration</p>
<p>Team science researchers have studied human collaborative behaviors for decades, covering topics such as leadership, communication, team dynamics, team cohesion, and shared situation awareness (Riedl et al., 2021).However, the transferability of these findings to hybrid human-agent teams or fully automated teams remains largely unexplored.Park et al. (2023) utilized ChatGPT to operate a sandbox environment populated by generative agents, observing emergent social behaviors among LLM-based agents.That study primarily focused on the feasibility of running such a sandbox environment with LLMs, rather than specifically on the collaborative behaviors of machine intelligence.</p>
<p>Multi-agent Collaboration Tasks</p>
<p>To evaluate the capability of LLM-based embodied agents, we design a multi-agent environment to simulate the collaborative and problem-solving dynamics of a search and rescue mission.</p>
<p>3.1 Task environment 3 agents (i.e.Alpha, Bravo, and Charlie) emulate specialists in a team, with the objective to locate and safely defuse color-coded bombs scattered in an unexplored environment.Each bomb exhibits unique phase sequences in m colors, requiring the correct order of wire cutters for defusing.Team members start with different colored cutters and must coordinate and synchronize efforts for efficiency.The environment is conceptualized as a connected graph, with n nodes representing n rooms linked by several edges symbolizing hallways.In each round, the agents can choose from three classes of actions: moving to one of the n rooms, inspecting a bomb's phase sequence in the current room, or using one of the m wire-cutters.The size of action space depends on the problem scale (i.e.n + m + 1).Agents' observation are limited to their current room's contents and agent status.They are updated periodically about team scores, current room contents, teammates' locations and available tools.The team is rewarded 10*x points when a x-phase bomb is successfully defused.</p>
<p>The evaluation environment comprises five rooms (n = 5) and five bombs, including two single-phase, two double-phase, and one triplephase bombs.Bomb stages might have three different colors (m = 3).Each successfully defused bomb awards the team 10 points per processed phase, resulting in 90 as the maximum score per mission.Team performance is measured using two metrics: the team score, indicating coordination quality, and rounds to completion, measuring collaboration efficiency.A trial concludes when the team has defused all bombs, exceeded the time limit (i.e., 30 rounds), or entered a deadlock by repeating outputs.</p>
<p>Text game interface</p>
<p>The initial task environment is implemented for MARL agents based on gym API (Brockman et al., 2016).To facilitate interaction between LLMbased agents with the environment, we've integrated the task environment with a text interface.</p>
<p>At each round (i.e.timestamp), the team's three agents sequentially interact with the environment, both receiving observations and performing actions via natural language interaction.A built-in communication mechanism enables text message exchange among agents per round.Importantly, agents remain oblivious to each other's actions and outcomes unless communicated, facilitating Theory of Mind inference opportunities.Specifically, a rule-based text interface translates observations into natural language descriptions and encodes agent chats into abstract action selections.For observations, the text interface extracts state features from the game engine and replaces keywords in the templates.A typical description text includes the current round number, cumulative team score, action feedback, contents of the current room, teammates' locations, and communication messages.Action encoding is done via keyword matching since LLMs are instructed to frame their responses in a certain format and structure.Should an agent produce unintelligible content, such as invalid actions or nonsensical text, the interface provides feedback for error correction.The error messages are generated based on pre-programmed rules and templates, such as "There is no bomb in the current location, Room X, for you to inspect.".Fig. 1 showcases sample interactions between the agent team and task environment via the text interface.</p>
<p>LLM-based Embodied Agents</p>
<p>We chose to evaluate OpenAI's latest chat completion models, namely gpt-3.5-turbo-0301and gpt-4-0314, owing to their impressive performance in various benchmarks (Zheng et al., 2023).These models are prompted to engage in a text-based game, with user inputs managed by the above-mentioned game interface.The LLMs functions as embodied agents interacting within the task environment.They are provided with the game's rules as context.For each round, the model is asked to choose actions and communicate messages, based on the current task state observations and past interaction history.Interaction history between the LLM-based agent and text game interface are maintained in the query text until it exceeds the maximum model input size.In our setup, all agents retain memory of the game rules and history from the previous two rounds, amounting to 4096 tokens.</p>
<p>Multi-agent communication</p>
<p>Given the collaborative nature of the task scenarios, inter-agent communication is crucial for achieving effective coordination and teamwork.We implemented a communication channel enabling LLMbased agents to share textual messages within the team.Messages, once sent, are immediately broadcast to all team members and reflected in their subsequent observations.For instance, as depicted in Fig. 1, agent Alpha dispatched messages instructing teammates to separate, followed by feedback from agent Bravo.In practice, since agents alternate in message sending, responses from teammates will appear in the observations of the succeeding round.</p>
<p>Belief state</p>
<p>Due to the model input size limitation, LLM-based agents cannot retain the entire interaction history, yet task dynamics require the team to track key long-term information, such as room contents and bomb sequences.To augment the agents' information retention and enhance collaboration, we propose a method of prompt engineering to represent explicit belief states.As illustrated in Fig. 1, upon receiving environmental observations, agents are prompted to update a textual description storing key task-related beliefs.This updated belief state is preserved in the interaction history and used in subsequent action planning.For instance, after inspecting bomb 1, agent Alpha updated its belief state about the bomb's sequence from unknown to red, retaining this information until further updates.</p>
<p>The proposed belief state is inspired by the idea of chain-of-thought prompting (Wei et al., 2022), wherein a complex reasoning task is broken down into intermediate steps and introduced to the LLM in a few-shot learning manner.Notably, although an initial belief state description is provided to illustrate the proper format and representations, the update rules are entirely zero-shot, relying solely on the LLM's common sense and mission context.</p>
<p>Experiments</p>
<p>We systematically ablate LLM-based embodied agents and evaluate them in a collaborative task in teams of three.Two modules are manipulated including LLM models (i.e.GPT-4 or ChatGPT) and belief representation (i.e. with or without belief state) resulting in a total of 4 experimental conditions.</p>
<p>Setups</p>
<p>At the beginning of each experimental trial, we assemble a team of three embodied agents and reset the task environment, randomizing starting loca- tions, room connections, bomb distributions, and sequences.Agents then take turns providing action choices and communication messages based on their initial observations.It's important to note that each agent only has a partial observation and its own interaction history, with inter-agent communication being the sole means of information diffusion in this fully decentralized team.For LLMbased agents, we set the model temperature parameter to zero and perform three trials of repeated measurement to ensure result stability.Each trial's duration varies from 5 to 120 minutes, depending on task load and model selection.</p>
<p>Baselines</p>
<p>In addition to LLM-based embodied agents, we also include baselines based on MARL and planning methods.For MARL, we consider Multi-Agent Proximal Policy Optimization (MAPPO) (Yu et al., 2022), which has shown strong performance in environments such as the StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019).</p>
<p>Our model is based on a stateful actor-critic approach building on recurrent neural networks with shared actor and critic models given agent invariance to improve sample efficiency and memory requirements while avoiding the lazy agent problem (Sunehag et al., 2017).We utilise the default hyperparameters for SMAC to train MAPPO in the environment and evaluate its performance from another fixed distribution of randomly generated environments, recording the average score and episode length as well as their standard deviation.Like the LLM agents, MARL agents are able to observe their teammates' locations.Other than the team reward of 10*x points when a x-phase bomb is successfully defused, an additional intermediate reward term is implemented as well, where an agent is given a small positive reward of +1 upon the application of the correct wirecutter in defusing a phase of a bomb and a small negative reward of −1 when it causes a bomb to explode upon the application of the wrong wirecutter.This reward-shaping term allows the agents to more sample efficiently learn the necessary bomb-defusing skills as compared to the relatively sparser team reward.</p>
<p>In addition, we augment a state-of-the-art Multi-Agent Path-Finding (MAPF) algorithm, Conflict-Based Search (CBS) (Sharon et al., 2015), simultaneously generate task assignments with feasible and collision-free paths for agents that adhere to precedence and temporal constraints in order to maximise a user-defined objective instead of the sum of path costs or makespan.Specifically, the user-defined objective is quantified as the return from a user-defined reward function, which is the team reward of 10*x points when a x-phase bomb is successfully defused in the stated task.The planner uses a user-defined heuristic (e.g.sort bombs in ascending order of distance from the agents' starting location) to sort the execution order of the actions for the entire task.The ordered actions are then partitioned using a hyperparameter, the number of actions per subtask, to form a subtask (e.g. the two nearest bombs to the agents' starting location).The actions from the subtask are used to generate possible combinations of assignments to agents.The planner returns a feasible solution for the subtask by resolving precedence and temporal conflicts through the expansion of a binary constraint tree in a best-first manner with respect to the return.The solution for the entire task is then composed of the solutions of the subtask sequentially.By considering the entire task of 5 bombs as a single subtask, the planner can be proven to be complete and optimal with respect to the score.</p>
<p>Room Connectivity</p>
<p>Room 0 is connected to room 3 and 5. Room 3 is connected to room 0. Room 5 is connected to room 0.</p>
<p>…… Observation</p>
<p>Alpha</p>
<p>I am moving to Room 3 as instructed.Alpha; please defuse Bomb 1 in Room 0 using the red tool.</p>
<p>Bravo</p>
<p>I am moving to Room 5 to search for more bombs.</p>
<p>Charlie</p>
<p>Yes, the current contents of Room 5 are as follows: Player alpha (you) and Bomb 3.</p>
<p>Introspection</p>
<p>No, Player Charlie does not know the current contents of Room 5 since they moved to Room 6.They are only aware of the information they had before leaving Room 5.</p>
<p>1st-order ToM</p>
<p>Yes, player Charlie is aware that you know the current contents of Room 5 because you mentioned in your previous message that you were moving to Room 5 to assist with Bomb 3.</p>
<p>2rd-order ToM</p>
<p>Systematic Failures: Long horizon contexts Emergent Collaborative Behaviors</p>
<p>GPT-4+Belief: Positive ToM Inferences</p>
<p>Yes, the current contents of Room 6 are Bomb 4 and Alpha.</p>
<p>Introspection ChatGPT: Negative ToM Inferences</p>
<p>No, player charlie does not know the current contents of room 6.</p>
<p>1st-order ToM</p>
<p>No, player charlie is not aware that I know the current contents of room 6.</p>
<p>2rd-order ToM</p>
<p>Theory of mind inferences</p>
<p>Alongside the main task, LLM-based agents are tasked with performing Theory of Mind (ToM) inferences during the mission.These inquiries fall into three categories, aligning with three ToM capability levels.The first category, introspection, assesses an agent's ability to articulate its mental state.The second category, first-order ToM inferences, tests if agents can estimate others' hidden mental states.The third category, second-order ToM inferences, evaluates an agent's ability to infer what others believe about their own mental state.</p>
<p>The design principle of ToM questions is inspired by the Sally-Anne test, the most widely used ToM task in human studies.Every time an agent conducts an action, we pose a belief reasoning question, asking if another agent (i.e., target agent) is aware of the potential consequence of this action.The consequence here can be either a state change (e.g., a bomb has been defused) or a belief change (e.g., Alpha has explored Room 5 and found Bomb 3 in the room).An agent equipped with ToM should realize that while they know the consequence, the target agent might hold a false belief about it.A full list of ToM inference questions can be found in appendix.</p>
<p>To evaluate whether LLM-based agents answer these questions correctly, human annotators were hired to provide subjective judgment based on fully observable interaction and communication history.Specifically, the following standard are considered: 1) if the target agent is present in the current room and observes the consequence, 2) if the target agent has been to this room before, 3) if the consequence has been communicated to the target agent.It is worth mentioning that high-order ToM inferences involving communication are naturally ambiguous.These corner cases were discussed among annotators to ensure a consistent standard across conditions.</p>
<p>Results</p>
<p>Table 1 and Table 2 present the main experiment results.This section will analyze each metric, examine potential reasons for performance differences, and provide qualitative case studies of experimental trials.</p>
<p>Task performance</p>
<p>Except for the ChatGPT team, all teams manage to defuse all bombs within the time limit.Their efficiency is indicated by the average number of rounds spent to complete the task.The CBS Planner resolves the task in 6.0 rounds, providing an optimal baseline given its centralized coordination and perfect information sharing.MAPPO, a state-of-theart multi-agent reinforcement learning algorithm, completes the task in an average of 11.0 rounds after 45 million timesteps of training, serving as a practical baseline.</p>
<p>ChatGPT fails to complete the task in all experiments, averaging a team score of 43.3.On the contrary, teams based on GPT-4 achieve full scores, with those using explicit belief representations being more efficient (28.3 vs. 12.3 rounds).These findings align with previous research demonstrating GPT-4's superior reasoning capabilities compared to ChatGPT (Zheng et al., 2023).LLM-based agents perform exceedingly well in team collaboration tasks, especially considering their fully zeroshot learning and decentralized framework.The incorporation of belief state representation improves team collaboration by reducing invalid actions and enhancing ToM inference capabilities.</p>
<p>Basic embodied interactions</p>
<p>For a successful team, each member should manage individual sub-tasks effectively, a concept known as taskwork in team science (Crawford and Lepine, 2013).This involves understanding task rules, reasoning about action prerequisites and consequences, and interacting with the environment.All LLM-based teams demonstrate basic embodied interaction capabilities, achieving better performance than the random baseline.Additionally, LLM-based agents effectively express their beliefs about task-related information via introspection, as shown in Table 2.All agents show a strong performance (&gt;80%) in understanding world knowledge (e.g., bomb locations) and situation modeling (e.g., interaction history).</p>
<p>Emergent collaborative behaviors</p>
<p>To understand how LLM-based agents match the performance of state-of-the-art MARL methods, we analyzed team trajectories and conducted a qualitative analysis of emergent collaborative behaviors.As shown in the top-right panel of Fig. 2, GPT-4+Belief teams use communication messages to coordinate tasks.Agent Alpha voluntarily takes the role of a team leader, delegating sub-tasks to other members.Other collaborative behaviors common in human teams (Fan and Yen, 2004), such as helping, resolving conflicts, and sharing infor-mation, also emerge in LLM-based agent teams.These findings suggest that LLMs, through learning from massive language materials, acquire essential teamwork skills without specific collaborative task training.</p>
<p>LLM's systematic failures</p>
<p>However, LLM-based agents' collaboration is less efficient than the optimal baseline.We identify a few systematic failures that LLMs make during team planning and discuss how they impede teamwork progress.</p>
<p>Long-horizon contexts</p>
<p>The first bottleneck of LLM-based teams' efficiency is dealing with long-horizon contexts.During the mission, LLMs occasionally output invalid actions that violate task rules, such as moving to non-adjacent rooms or using tools they do not possess.Even though the information about room connectivity and tool allocation are included in the initial prompts and maintained in the inquiry text, LLMs often overlook these details because they are far away from the planning question at the end.The more advanced GPT-4 model performs better in considering long contexts and complex logic, thereby making fewer invalid actions, as shown in Table 1.Our proposed belief state is also helpful in this progress by re-emphasizing task related information in the input prompt.</p>
<p>Hallucination</p>
<p>The second type of systematic failure we observe in LLMs is their hallucination about the task state.During the mission, agents might generate valid but infeasible actions, like searching for a defused bomb or claiming the sequence of a bomb without inspection.These actions stem from false beliefs about the game state and do not contribute to task progress.We attribute these hallucinations mainly to the lack of explicit belief representation.Without access to complete interaction history and only partial environment observations, LLM-based agents can't form an accurate belief about the task state.Therefore LLMs might generate imaginations about nonexistent bombs or fake bomb sequences when reasoning about the next action.We evaluate this hypothesis by the GPT-4+Belief condition where LLM-based agents explicitly represent their belief state in text.Results show that the introduction of belief state decreases invalid action by 50.7% and increase the team efficiency by 130%</p>
<p>Theory of Mind Inference</p>
<p>A critical aspect of teamwork is inferring teammates' mental states, including beliefs, desires, and intentions.We assess LLM-based agents by asking them to conduct Theory of Mind inferences during the mission.As seen in Table 2, LLM-based agents can estimate their own and their teammates' mental states.In the most challenging second-order ToM inference tasks, where agents estimate others' beliefs about their own mental states, GPT-4 + Belief agents correctly respond in nearly 70% of cases.Consistent with team performance, GPT-4 surpasses ChatGPT in all three ToM inference levels, and explicit belief state representation enhances LLM-based agents' ToM capabilities.In the following case study, we'll analyze LLM responses to see how they succeed or fail in certain cases.</p>
<p>Case study</p>
<p>As shown in Fig. 2, after Alpha entered Room 5 and observed the contents, we asked whether a teammate in another room (i.e., Charlie) knows Room 5's contents.This is a first-order belief estimation question.GPT-4 answers correctly saying "No, Player Charlie does not know the current contents of Room 5 since they moved to Room 6.They are only aware of the information they had before leaving Room 5."</p>
<p>considering both Charlie's current location (not in Room 5) and their interaction history (they've been in Room 5 before).In contrast, ChatGPT fails to consider this history.In the second-order ToM inference case, we asked if Charlie is aware that Alpha knows Room 5's contents.GPT-4+Belief answers correctly by considering previous communications whereas ChatGPT fails.</p>
<p>"Yes, player Charlie is aware that I know the current contents of Room 5 because I mentioned in my previous message that I was moving to Room 5 to assist with Bomb 3."</p>
<p>Inference under false belief</p>
<p>In some situations, ToM inferences become extremely challenging for LLMs.In our records, this is typically when reasoning involves agent communication.Even with ample context, it's hard for agents to track information transmission while conducting ToM inferences.The most demanding scenarios occur when agents share inaccurate information, leading to false beliefs.This usually happens when LLM-based agents try an invalid action and broadcast false intentions through communication.For instance, when Alpha tells the team they're moving to Room 8 (which is unreachable), and we ask Alpha, "Is Charlie aware that you know the contents of Room 8?" This question is tricky since Alpha failed to reach Room 8.In theory, the most acceptable answer is complex and involves multiple high-order ToM assumptions.For example a human annotated ground truth is: Second, LLMs still fall short of being optimal planners or team players due to systematic failures, such as neglecting long-horizon contexts and making inaccurate assumptions about the task state (a.k.a hallucination).These flaws significantly hinder team collaborations as they can rapidly disseminate misinformation via communication, leading to widespread false beliefs.We attempted to mitigate these issues by allowing LLM-based agents to maintain an explicit belief state about the world.Our findings suggest that modern LLMs can update the given belief descriptions based on their observations, hinting at the potential emergence of advanced cognitive skills such as world knowledge understanding and situation modeling.Moreover, belief state representations offer a structured framework that helps agents track key task-related information, leading to improved team performance.
"I did
Finally, our study indicates that the Theory of Mind (ToM) capabilities of LLMs are still limited, particularly when evaluated within interactive teamwork scenarios that involve dynamic belief states and intensive communication.For context, while 5-year-old children can perform second-order ToM inferences (Miller, 2009), adults don't consis-tently use this ability during communications due to the complexity and ambiguity of social interactions (Keysar et al., 2003).Thus, there's considerable work ahead for LLMs to develop a functional ToM and interact naturally with humans.Our study represents a preliminary effort to devise novel evaluation methods for LLMs' ToM that go beyond traditional tests such as the Sally-Anne test.</p>
<p>Conclusions</p>
<p>In this study, we assessed the ability of recent large language models (LLMs) to conduct embodied interactions in a team task.Our results demonstrate that LLM-based agents can handle complex multi-agent collaborative tasks at a level comparable with the state-of-the-art reinforcement learning algorithm.We also observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents.These findings confirm the potential intelligence of LLMs in formal reasoning, world knowledge, situation modeling and social interactions.Furthermore, we discussed two systematic failures that limit the performance of LLM-based agents and proposed a prompt-engineering method that mitigates these failures by incorporating an explicit belief state about world knowledge into the model input.</p>
<p>Limitations</p>
<p>This study represents an initial effort to understand machine intelligence in complex task scenarios.Several enhancements could improve the experimental setup and offer a more thorough evaluation of LLMs in multi-agent collaborations.First, we could incorporate additional LLMs besides Ope-nAI's GPT models.As new models emerge with enhanced reasoning capabilities and larger input sizes, their performance in team tasks and ToM inference may also change.Second, the task environment is relatively simple with only five nodes and five bombs.We plan to scale up the environment and introduce more restrictions to test how LLM-based teams react to more challenging tasks.Lastly, the current team consists of three agents with homogeneous policies.It would be intriguing to evaluate how LLM-based agents perform in human-agent teams, especially from a humancentered perspective where issues like trust, transparency, and human-agent co-training can be addressed.</p>
<p>The ToM capability evaluation method used in this study also has its limitations.Currently, human annotators, who have a global view of the task state and interaction history, generate the ground truth for ToM inference questions.However, this estimation is at best an approximation, assuming agents process information as a rational human would, which might be ambiguous in situations involving false beliefs or miscommunications.A potential alternative could be using each agent's maintained belief state as the ground truth.</p>
<p>The proposed belief state method could extend from introspective belief to first-order or even second-order beliefs.Currently, LLM-based agents maintain a belief state about their own world knowledge in text form.By extending this representation to include other agents' world knowledge, we could equip LLM-based agents with an explicit first-order ToM model.Their ToM capability can be assessed by directly comparing one's first-order belief with another's introspective belief, rather than asking LLMs Sally-Anne style questions.</p>
<p>Figure 1 :
1
Figure 1: Our proposed framework consist of 3 LLM-based agents, a text game interface and the actual task environment.The natural language outputs of LLM are encoded into abstract actions by the text interface and then sent to task environment.The task environment then processes agent actions and returns observations via the text interface.Upon receiving environmental observations, LLM-based agents are prompted to update their beliefs and output action selections and messages.3 agents in the team are coded as Alpha, Bravo, and Charlie and take turns to interact with the interface.</p>
<p>You are now in Room 3 with Bomb 5. What is your next action?Environment Action selection: Move to Room 5. Message to Team: "I am moving to Room 5 to help Charlie with Bomb 3." Alpha Results: You can not directly move to Room 5 because it is not adjacent to your current location; Room 3. Consider taking a detour to another room first and then move to your destination.Environment I am inspecting Bomb 1 in Room 0. Bravo; please move to Room 3. Charlie; please move to Room 5.</p>
<p>Figure 2 :
2
Figure 2: Example interactions between LLM-based agents and the text game interface.The upper left panel showcases one type of systematic failures we observed in LLM's outputs in which long horizon contexts are overlooked.The upper right panel illustrates emergent collaborative behaviors (e.g.emergent leadership) between LLM-based agents.The bottom two panels are quotes of GPT-4+Belief and ChatGPT agents' answers for ToM inference questions.</p>
<p>Table 1 :
1
Task performance of LLM-based agents and baseline conditions.Score represent the average team score in all experiment trials.Length refers the average number of rounds the team took in completing the task.Percentages of valid action measures the proportion of LLM outputs that can be encoded into actions allowed by the task rules.Numbers after ± are 1 standard deviation.
AgentsScoreRounds to Completion Valid action %ChatGPT43± 4.730.0± 0.062.5%GPT-490± 0.028.3± 2.671.8%GPT-4 + Belief 90± 0.012.3± 2.086.1%MAPPO90± 0.011.0± 0.0N/ACBS Planner90± 0.06.0± 0.0N/ARandom38± 14.730.0± 0.0N/A</p>
<p>Table 2 :
2
LLM-based agents' performance in ToM inference tasks.Natural language answers are annotated by experimenters and compared with the ground truth based on global interaction history.Percentages represent the inference accuracy.</p>
<p>AcknowledgementsThis work was supported by DARPA award HR001120C0036 and AFOSR award FA9550-18-1-0097.Appendix A PromptsA.1 Task context Welcome to our interactive text game!In this game, you'll assume the role of a specialist on a search and rescue team.Alongside two other players, you'll navigate a five-room environment with a mission to defuse five hidden bombs.The Map: Imagine a network of rooms represented by a connected graph where each node corresponds to a room, and the edges between nodes depict hallways.The rooms are numbered 0, 3, 6, 5, and 8. Room 0 is connected to all other rooms.Room 5 shares a hallway with room 6.Room 3 is linked to room 8.And room 8 is also connected with room 6.You can only travel to adjacent, directly connected rooms at each turn.The Challenge: Scattered among these rooms are five bombs, each coded with different phases represented by colors.To defuse them, you'll need to use the correct wire-cutting tools in the correct sequence.There are one-phase, two-phase, and three-phase bombs, needing 1, 2, or 3 color-coded tool applications in sequence to disarm.For instance, a bomb with a red-green phase sequence requires the red tool first, then the green one.Points are awarded based on the number of tools used for defusing a bomb, with each tool use worth 10 points.Your task is to maximize the team score as soon as possible.The challenge is that the bomb locations and sequences are unknown to players at the start.Tools: Each player is equipped with two colorcoded wire cutters.As player Alpha, you have red and green tools, player Bravo wields green and blue, and player Charlie possesses blue and red.Actions: Each round, you can opt to do one of the following: 1) Move to an adjacent room, 2) Inspect a bomb's phase sequence in your current room, or 3) Apply your wire cutters to a bomb in the current room.Communications: In addition to selecting an action to take from the above list, you can also send communication message texts to both of your teammates in each round.The message text you sent will be shared with both of your teammates in their observation in the next round.Observation: While you can only see what's in your current room and read text messages from teammates.You'll also be informed of the current round number, team score and the current location of your teammates.Your teammates have the same observability as you.They will not be able to know your action and its consequences unless you explicitly communicate.To facilitate our interaction, reply your action selection and communication messages in this fixed format: Action selection: Your action.Message to Team: "Your Message".To move to an adjacent room, say: 'Move to Room X'.To inspect the sequence of a bomb in your current room, say: 'Inspect Bomb'.To apply a wire cutter tool, say: 'Apply X Tool'.Remember, your replies must adhere strictly to these rules.Feel free to ask clarifying questions if needed.I'll supply the necessary information as we progress.Are you ready to take on this explosive challenge?A.2 Initial belief stateBelow is your current belief about game state based on your previous observations about the environment and interactions with your teammates.Your role: You are playing as Player <agent id>.Current round: 1 Total team score: 0. Observation: You are currently in Room 0 with both of your teammates.In the room you also found bomb 1 with unknown sequence.There is no other bomb in the current room.Teammate Locations: Player alpha is in Room 0; Player bravo is in Room 0; Player charlie is in Room 0.Room connectivity:• Room 0 is connected to room 3, 5, 6, 8• Room 3 is connected to room 0 • Room 5 is connected to room 0 and 6• Room 8 is connected to room 0 and 6 Bomb Intel:• Bomb 1: Located in Room 0. The phase sequence is Unknown.• Bomb 2: Details currently unknown.• Bomb 3: Details currently unknown.• Bomb 4: Details currently unknown.• Bomb 5: Details currently unknown.Tool inventory:• Alpha: Equipped with red and green wire cutters.• Bravo: Equipped with green and blue wire cutters.• Charlie: Equipped with red and blue wire cutters.Available action options:• To move to an adjacent room, say: 'Move to Room X'.• To inspect the sequence of a bomb in your current room, say: 'Inspect Bomb'.• To apply a wire cutter tool, say: 'Apply X Tool'.• To send a message to your teammates, say: 'Message to Team: "Your Message"'.B Environment feedback for Error correction• Your action is invalid.• You can not directly move to Room roomid because it is not adjacent to your current location, Room currentroom.Consider taking a detour to another room first and then move to your destination.• There is no bomb in the current current location, Room currentroom, for you to inspect.• You can not apply Tool toolcolor to Bomb boomid because the sequence of this bomb is sequence.You will need to apply other color tool first.• There is no bomb in your current location, room roomid, for you to defuse.• You do not have Tool toolcolor.Consider asking your teammates who have this tool to help you defuse the bomb.C Theory of Mind QuestionsC.1 Introspection• Do you know the current contents of room roomid?• Do you know the state and remaining sequence of bomb bombid has been changed?• Do you know a bomb phase has just been defused?• Do you know the sequence of bomb bombid?C.2 First-order ToM• Does player playerid know the current contents of room roomid?• Does player playerid know the state and remaining sequence of bomb bombid has been changed?• Does player playerid know a bomb phase has just been defused?• Does player playerid know the sequence of bomb bombid?C.3 Second-order ToM• Based on the observation and previous history, is player playerid aware of the fact that you know the current contents of room roomid?• Based on the observation and previous history, is player playerid aware of the fact that you have changed the state and remaining sequence of bomb bombid?• Based on the observation and previous history, is player playerid aware of the fact that you know a bomb phase has just been defused?• Based on the observation and previous history, is player playerid aware of the fact that you know the sequence of bomb bombid?
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. Chris L Baker, Julian Jara-Ettinger, Rebecca Saxe, Joshua B Tenenbaum, Nature Human Behaviour. 14642017</p>
<p>Openai gym. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>A configural theory of team processes: Accounting for the structure of taskwork and teamwork. R Eean, Jeffery A Crawford, Lepine, 2013Academy of Management Review38</p>
<p>Modeling and simulating human teamwork behaviors using intelligent agents. Xiaocong Fan, John Yen, Physics of life reviews. 132004</p>
<p>Thilo Hagendorff, arXiv:2303.13988Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods. 2023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>Limits on theory of mind use in adults. Boaz Keysar, Shuhong Lin, Dale J Barr, Cognition. 8912003</p>
<p>Michal Kosinski, arXiv:2302.02083Theory of mind may have spontaneously emerged in large language models. 2023arXiv preprint</p>
<p>Theory of mind modeling in search and rescue teams. Huao Li, Ini Oguntola, Dana Hughes, Michael Lewis, Katia Sycara, 2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN). IEEE2022</p>
<p>Improving multi-agent cooperation using theory of mind. Terence X Lim, Sidney Tio, Desmond C Ong, arXiv:2007.157032020arXiv preprint</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023arXiv preprint</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, arXiv:2301.06627Dissociating language and thought in large language models: a cognitive perspective. 2023arXiv preprint</p>
<p>Children's understanding of second-order mental states. A Scott, Miller, Psychological bulletin. 13557492009</p>
<p>Boosting theory-of-mind performance in large language models via prompting. Rahimi Shima, Christopher J Moghaddam, Honey, arXiv:2304.114902023arXiv preprint</p>
<p>Theory of mind as intrinsic motivation for multi-agent reinforcement learning. Ini Oguntola, Joseph Campbell, Simon Stepputtis, Katia Sycara, arXiv:2307.011582023arXiv preprint</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023arXiv preprint</p>
<p>Iyad Rahwan, Manuel Cebrian, Nick Obradovich, Josh Bongard, Jean-François Bonnefon, Cynthia Breazeal, Jacob W Crandall, Nicholas A Christakis, Iain D Couzin, Matthew O Jackson, Machine behaviour. 2019568</p>
<p>Quantifying collective intelligence in human groups. Christoph Riedl, Ji Young, Pranav Kim, Thomas W Gupta, Anita Williams Malone, Woolley, Proceedings of the National Academy of Sciences. 11821e20057371182021</p>
<p>Tabish Mikayel Samvelyan, Christian Rashid, Gregory Schroeder De Witt, Nantas Farquhar, Nardelli, G J Tim, Chia-Man Rudner, Hung, H S Philiph, Jakob Torr, Shimon Foerster, Whiteson, The StarCraft Multi-Agent Challenge. CoRR, abs/1902.04043. Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. 2023. Neural theory-of-mind? on the limits of social intelligence in large lms. 2019</p>
<p>Minding language models'(lack of) theory of mind: A plug-andplay multi-character belief tracker. Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, Yulia Tsvetkov, arXiv:2306.009242023arXiv preprint</p>
<p>Conflict-based search for optimal multi-agent pathfinding. Guni Sharon, Roni Stern, Ariel Felner, Nathan R Sturtevant, Artificial Intelligence. 2192015</p>
<p>Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, arXiv:1706.05296Value-decomposition networks for cooperative multi-agent learning. 2017arXiv preprint</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022arXiv preprint</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. Tomer Ullman, arXiv:2302.083992023arXiv preprint</p>
<p>Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, arXiv:2305.16291arXiv preprint</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.015602023barXiv preprint</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>Supporting artificial social intelligence with theory of mind. Jessica Williams, Stephen M Fiore, Florian Jentsch, Frontiers in artificial intelligence. 52022</p>
<p>Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, Harold Soh, arXiv:2302.05128Translating natural language to planning goals with large-language models. 2023arXiv preprint</p>
<p>The surprising effectiveness of ppo in cooperative multiagent games. Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, Yi Wu, Advances in Neural Information Processing Systems. 202235</p>
<p>Emergence of theory of mind collaboration in multiagent systems. Luyao Yuan, Zipeng Fu, Linqi Zhou, Kexin Yang, Song-Chun Zhu, arXiv:2110.001212021arXiv preprint</p>
<p>Perspective-taking and depth of theory-of-mind reasoning in sequential-move games. Jun Zhang, Trey Hedden, Adrian Chia, Cognitive science. 3632012</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. </p>            </div>
        </div>

    </div>
</body>
</html>
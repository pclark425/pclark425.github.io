<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7911 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7911</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7911</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-3015956a254139547cb350f5dbdd8edde298ac0d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3015956a254139547cb350f5dbdd8edde298ac0d" target="_blank">Accelerating clinical evidence synthesis with large language models</a></p>
                <p><strong>Paper Venue:</strong> npj Digital Medicine</p>
                <p><strong>Paper TL;DR:</strong> A generative artificial intelligence pipeline named TrialMind is proposed to streamline study search, study screening, and data extraction tasks in SR to show the promise of accelerating clinical evidence synthesis driven by human-AI collaboration.</p>
                <p><strong>Paper Abstract:</strong> Clinical evidence synthesis largely relies on systematic reviews (SR) of clinical studies from medical literature. Here, we propose a generative artificial intelligence (AI) pipeline named TrialMind to streamline study search, study screening, and data extraction tasks in SR. We chose published SRs to build TrialReviewBench, which contains 100 SRs and 2,220 clinical studies. For study search, it achieves high recall rates (Ours 0.711–0.834 v.s. Human baseline 0.138–0.232). For study screening, TrialMind beats previous document ranking methods in a 1.5–2.6 fold change. For data extraction, it outperforms a GPT-4’s accuracy by 16–32%. In a pilot study, human-AI collaboration with TrialMind improved recall by 71.4% and reduced screening time by 44.2%, while in data extraction, accuracy increased by 23.5% with a 63.4% time reduction. Medical experts preferred TrialMind’s synthesized evidence over GPT-4’s in 62.5%-100% of cases. These findings show the promise of accelerating clinical evidence synthesis driven by human-AI collaboration.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7911.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7911.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TrialMind</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TrialMind (LLM-driven clinical evidence synthesis pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end LLM‑driven pipeline introduced in this paper that decomposes systematic review tasks into literature search, screening, data/result extraction, and evidence synthesis, combining LLM prompting, retrieval, chain-of-thought, and code generation with human-in-the-loop verification to produce structured, meta-analysis-ready outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Accelerating Clinical Evidence Synthesis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>TrialMind</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Decomposes evidence synthesis into modular LLM tasks: (1) generate and iteratively refine Boolean search queries (with RAG + CoT), (2) generate eligibility criteria and predict per-criterion eligibility for each candidate study, (3) extract structured study characteristics and numeric results from full-text inputs, and (4) standardize outputs (via LLM-generated Python code) for meta-analysis; each module exposes sources/locations so humans can inspect and correct outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>PICO elements (from review abstracts), titles/abstracts, full-text articles (PDF/XML), PubMed/PubMed Central search results</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Boolean search queries, eligibility criteria and per-criterion labels, ranked study lists (relevance scores), structured data fields (study characteristics, population baselines), standardized numerical result tables for meta-analysis, forest plots</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>In-context learning, retrieval-augmented generation (RAG) to augment prompts with retrieved abstracts, chain-of-thought (CoT) prompting for multi-step reasoning, modular LLM-driven workflow (chain of prompts), and LLM-generated executable code for result standardization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Sonnet (Anthropic Claude Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>TrialReviewBench (100 systematic reviews, 2,220 studies), PubMed / PubMed Central (retrieved candidate sets)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Search recall (overall Recall), ranking Recall@K (Recall@20, Recall@50), extraction accuracy (ACC), precision/recall for hallucination/missing, human expert ratings (win rate, mean rating), time savings</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Average search Recall = 0.782 (TrialMind) vs GPT-4 baseline 0.073 and Human baseline 0.187; large improvements in ranking (Recall@20/50 across topics; fold-changes 1.3–2.6 vs best baselines), data extraction ACC up to 0.83 depending on topic, result extraction ACC superior to GPT-4 (e.g., 0.70 vs 0.54 in Immunotherapy), human-AI studies: screening recall lift 71.4% with 44.2% time savings; data extraction accuracy lift 23.5% with 63.4% time savings; annotators favored TrialMind outputs in most cases (winning rates 62.5%–100% across examples).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>LLM errors (hallucinations, especially on nuanced clinical outcomes), numeric reasoning weaknesses, overly generic outputs without structured prompting, reliance on human oversight for verification, dataset limited to publicly available PMC full texts (may require OCR for other sources), cost and latency of large LLM inference, potential redundancy in generated criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7911.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7911.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-driven workflow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-driven modular workflow for evidence synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual approach formalized in this paper that chains task-specific LLM modules so outputs from one module feed the next (e.g., query generation -> retrieval -> criteria generation -> per-criterion screening -> extraction -> synthesis), improving transparency and enabling human intervention between steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Accelerating Clinical Evidence Synthesis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-driven workflow (modular chain of LLM tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Breaks the meta-analysis process into N modular LLM tasks T1..TN where each module is implemented by an LLM prompt; the output of module n is fed as input to module n+1, optionally augmented by retrieval; this modularization enables better performance, transparency, and human-in-the-loop corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Task-specific inputs such as PICO, candidate abstracts, full-text documents, user-specified field descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Intermediate structured outputs per module (queries, criteria, per-criterion labels, extracted values) and final standardized tables for analysis</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>In-context learning, retrieval-augmented generation integrated into module prompts, chain-of-thought for complex steps, modular prompt chaining</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>TrialReviewBench, PubMed/PubMed Central</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Module-level and end-to-end metrics: Recall, Recall@K, extraction accuracy, human ratings, time-to-completion</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>When implemented as TrialMind, the modular workflow achieved high search recall, substantially better ranking and extraction accuracy than baselines, and improved human efficiency in user studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Error propagation across modules if not checked, requires curated prompts per module, human oversight still necessary, performance depends on retrieval quality and LLM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7911.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7911.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Search Query Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based query generation, augmentation, and refinement pipeline for literature search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-step procedure in TrialMind where LLMs generate initial Boolean queries from PICO, retrieved abstracts are used (RAG) to augment context, and chain-of-thought prompting refines and expands the query set to maximize recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Accelerating Clinical Evidence Synthesis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Query generation + augmentation + refinement (RAG + CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt LLM to propose initial Boolean search terms from PICO, retrieve abstracts using those queries to form an RAG context, then use CoT prompting to iterate and expand/filter terms, producing a final augmented boolean query set used to fetch candidate studies from PubMed.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>PICO elements, retrieved abstracts (as RAG context)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comprehensive Boolean search queries and resulting candidate citation sets</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>In-context learning for initial generation, retrieval-augmented prompts with abstracts, chain-of-thought style multi-step refinement within a single LLM pass</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used in TrialMind for this component)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>PubMed/PubMed Central; TrialReviewBench used for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Overall Recall of ground-truth included studies from reviews</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>TrialMind's search pipeline produced average Recall = 0.782 across 100 reviews vs GPT-4 baseline Recall = 0.073 and human-constructed baseline = 0.187; maintained high recall even for reviews with many target studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>LLM may propose incorrect MeSH terms or miss domain-specific synonyms without good retrieval context; the approach produces very large candidate sets requiring downstream screening; depends on quality of retrieved abstracts for augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7911.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7911.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eligibility-Criteria Screening</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-criterion LLM eligibility prediction and aggregated ranking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transparent screening method where the LLM generates multiple eligibility criteria from PICO, predicts per-criterion labels (-1/0/1) for each candidate study, and aggregates those predictions into a study-level relevance score used for ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Accelerating Clinical Evidence Synthesis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Per-criterion eligibility prediction and aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>LLM generates a set of M eligibility criteria from the PICO; for each candidate study, the LLM outputs M labels (eligible/ineligible/uncertain) which can be aggregated (e.g., summation) into a single relevance score; users can edit criteria or aggregation strategies for customized ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>PICO elements and full-text/abstract of candidate study</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-criterion labels, aggregated relevance scores, rationale text per criterion (for transparency)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>In-context learning for criteria generation, direct prompting for per-criterion labeling, optional CoT for ambiguous cases</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used in TrialMind for screening)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>TrialReviewBench candidate sets (2,000 per review) for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Recall@20, Recall@50, Recall@200, ΔRecall per criterion (leave-one-out analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Substantial ranking improvements: TrialMind attained Recall@20/50 values (e.g., Hormone Therapy Recall@20=0.431, Recall@50=0.674) outperforming embedding baselines (MPNet, MedCPT) by 1.3–2.6x fold changes across topics; average ~43% of target studies in top 50.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Some generated criteria redundant; performance sensitive to quality of criteria and aggregation function; LLM per-criterion predictions can still err (uncertain=0) requiring human review.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7911.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7911.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Result Extraction Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Specialized result extraction pipeline with CoT and LLM-generated executable code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A focused multi-step pipeline to extract numerical clinical outcomes: (1) identify relevant content in the paper, (2) extract and reason about numerical values using CoT, and (3) generate and execute Python code to standardize results into tabular formats for meta-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Accelerating Clinical Evidence Synthesis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Result extraction with CoT + code generation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Performs CoT-style LLM prompting to (a) locate raw textual snippets describing outcomes, (b) elicit intermediate numerical values (group sizes, events, ratios), and (c) produce Python code that, when executed, computes standardized numeric metrics and produces a final table for meta-analysis; outputs are linked to source locations for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Full-text articles (PDF/XML) or user-uploaded content, plus natural language description of desired clinical endpoint and cohort definition</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Raw textual snippets referencing outcomes, intermediate numerical extractions, executable Python code, standardized numeric tables for meta-analysis (e.g., event counts, rates)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Chain-of-thought prompting to elicit intermediate reasoning and values, retrieval of document chunks, prompting to generate data-processing code (T_PY), and execution to produce final outputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Sonnet (used for extraction and code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>TrialReviewBench (1,049 study result annotations) and full-text papers from PubMed Central</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Extraction accuracy (ACC), error categorization (Inaccurate, Extraction failure, Unavailable data, Hallucination), human expert ratings in synthesized forest plots</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Result extraction ACC: TrialMind outperformed GPT-4 baseline across topics (e.g., Immunotherapy ACC 0.70 vs GPT-4 0.54); median fold-change 1.50 over best baselines; common error types were 'Inaccurate' and 'Extraction failure', hallucinations rare.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Numerical extraction and standardization remain challenging due to varied reporting formats and subgroup mismatches; hallucinations primarily in result naming/definitions; reliance on accessible full text (appendices/OCR issues may limit extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7911.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7911.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique mentioned and applied within TrialMind where retrieved domain-relevant documents (e.g., abstracts) are injected into LLM prompts to reduce hallucination and provide up-to-date factual context for generation/refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Accelerating Clinical Evidence Synthesis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Augments LLM prompts with retrieved relevant document snippets (here: abstracts from PubMed) so that the LLM conditions on external evidence when generating search queries, refining terms, or extracting information, thereby mitigating reliance on the LLM's internal knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Retrieved abstracts/snippets (from PubMed) plus PICO or target extraction prompts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined queries, more complete term lists, improved extraction outputs</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>RAG: semantic retrieval to supply contextual grounding within LLM prompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used in conjunction with GPT-4 in TrialMind</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>PubMed/PubMed Central (retrieved abstracts used as context); TrialReviewBench for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Improvement in search Recall and extraction accuracy when using retrieval context</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Use of retrieval context in query refinement contributed to substantially higher recall (TrialMind recall 0.782) compared to direct LLM prompting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality of retrieved context limits performance; retrieval may miss key synonyms or non-indexed documents; increases system complexity and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7911.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7911.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy used in TrialMind that elicits stepwise reasoning from the LLM within a single inference to handle multi-step tasks such as query self-refinement and numeric result extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Accelerating Clinical Evidence Synthesis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Structures prompts to request intermediate sub-steps (S1..ST) from the LLM in one inference, enabling the model to draft, self-reflect, and refine outputs (e.g., produce an initial term list then filter and augment it), improving performance on complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>PICO and retrieved text chunks or full-text documents</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Multi-step intermediate outputs (e.g., initial terms, filtered terms, augmentations) and refined final outputs</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Chain-of-thought style multi-step prompting within a single inference</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Sonnet (applied within TrialMind)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>TrialReviewBench and PubMed-derived contexts</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Improvements in query completeness (Recall) and extraction correctness when using CoT vs direct prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>CoT used in query refinement and result extraction improved coverage and numeric extraction robustness, contributing to TrialMind's superior overall performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Long CoT outputs can be verbose and may still contain incorrect intermediate steps; requires careful prompt design and verification of intermediate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7911.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7911.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context learning (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting paradigm invoked repeatedly in the paper where LLMs learn tasks at inference by conditioning on task descriptions and examples in the prompt instead of model fine-tuning; used for query generation, criteria generation, screening, and extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Accelerating Clinical Evidence Synthesis with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>In-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Provides task definitions and exemplar input-output pairs in the prompt so the LLM performs zero-shot or few-shot task adaptation for generating queries, criteria, or extraction templates without needing model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Task prompts containing definitions and examples, plus document text</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Task-specific outputs (queries, criteria, extracted fields)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Few-shot or example-driven prompting inside the LLM input context</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>TrialReviewBench for example-driven prompts and evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Task performance (Recall, ACC) under ICL-based prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ICL combined with RAG and CoT underpins TrialMind's modules and contributes to high task performance versus naive prompting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance depends on quality and representativeness of examples; LLM context length limits amount of examples and document context.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating clinical evidence synthesis with large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <em>(Rating: 2)</em></li>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7911",
    "paper_id": "paper-3015956a254139547cb350f5dbdd8edde298ac0d",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "TrialMind",
            "name_full": "TrialMind (LLM-driven clinical evidence synthesis pipeline)",
            "brief_description": "An end-to-end LLM‑driven pipeline introduced in this paper that decomposes systematic review tasks into literature search, screening, data/result extraction, and evidence synthesis, combining LLM prompting, retrieval, chain-of-thought, and code generation with human-in-the-loop verification to produce structured, meta-analysis-ready outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Accelerating Clinical Evidence Synthesis with Large Language Models",
            "authors": "Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun",
            "year": null,
            "method_name": "TrialMind",
            "method_description": "Decomposes evidence synthesis into modular LLM tasks: (1) generate and iteratively refine Boolean search queries (with RAG + CoT), (2) generate eligibility criteria and predict per-criterion eligibility for each candidate study, (3) extract structured study characteristics and numeric results from full-text inputs, and (4) standardize outputs (via LLM-generated Python code) for meta-analysis; each module exposes sources/locations so humans can inspect and correct outputs.",
            "input_type": "PICO elements (from review abstracts), titles/abstracts, full-text articles (PDF/XML), PubMed/PubMed Central search results",
            "output_type": "Boolean search queries, eligibility criteria and per-criterion labels, ranked study lists (relevance scores), structured data fields (study characteristics, population baselines), standardized numerical result tables for meta-analysis, forest plots",
            "prompting_technique": "In-context learning, retrieval-augmented generation (RAG) to augment prompts with retrieved abstracts, chain-of-thought (CoT) prompting for multi-step reasoning, modular LLM-driven workflow (chain of prompts), and LLM-generated executable code for result standardization",
            "model_name": "GPT-4, Sonnet (Anthropic Claude Sonnet)",
            "model_size": null,
            "datasets_used": "TrialReviewBench (100 systematic reviews, 2,220 studies), PubMed / PubMed Central (retrieved candidate sets)",
            "evaluation_metric": "Search recall (overall Recall), ranking Recall@K (Recall@20, Recall@50), extraction accuracy (ACC), precision/recall for hallucination/missing, human expert ratings (win rate, mean rating), time savings",
            "reported_results": "Average search Recall = 0.782 (TrialMind) vs GPT-4 baseline 0.073 and Human baseline 0.187; large improvements in ranking (Recall@20/50 across topics; fold-changes 1.3–2.6 vs best baselines), data extraction ACC up to 0.83 depending on topic, result extraction ACC superior to GPT-4 (e.g., 0.70 vs 0.54 in Immunotherapy), human-AI studies: screening recall lift 71.4% with 44.2% time savings; data extraction accuracy lift 23.5% with 63.4% time savings; annotators favored TrialMind outputs in most cases (winning rates 62.5%–100% across examples).",
            "limitations": "LLM errors (hallucinations, especially on nuanced clinical outcomes), numeric reasoning weaknesses, overly generic outputs without structured prompting, reliance on human oversight for verification, dataset limited to publicly available PMC full texts (may require OCR for other sources), cost and latency of large LLM inference, potential redundancy in generated criteria.",
            "counterpoint": true,
            "uuid": "e7911.0",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM-driven workflow",
            "name_full": "LLM-driven modular workflow for evidence synthesis",
            "brief_description": "A conceptual approach formalized in this paper that chains task-specific LLM modules so outputs from one module feed the next (e.g., query generation -&gt; retrieval -&gt; criteria generation -&gt; per-criterion screening -&gt; extraction -&gt; synthesis), improving transparency and enabling human intervention between steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Accelerating Clinical Evidence Synthesis with Large Language Models",
            "authors": "Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun",
            "year": null,
            "method_name": "LLM-driven workflow (modular chain of LLM tasks)",
            "method_description": "Breaks the meta-analysis process into N modular LLM tasks T1..TN where each module is implemented by an LLM prompt; the output of module n is fed as input to module n+1, optionally augmented by retrieval; this modularization enables better performance, transparency, and human-in-the-loop corrections.",
            "input_type": "Task-specific inputs such as PICO, candidate abstracts, full-text documents, user-specified field descriptions",
            "output_type": "Intermediate structured outputs per module (queries, criteria, per-criterion labels, extracted values) and final standardized tables for analysis",
            "prompting_technique": "In-context learning, retrieval-augmented generation integrated into module prompts, chain-of-thought for complex steps, modular prompt chaining",
            "model_name": "GPT-4, Sonnet",
            "model_size": null,
            "datasets_used": "TrialReviewBench, PubMed/PubMed Central",
            "evaluation_metric": "Module-level and end-to-end metrics: Recall, Recall@K, extraction accuracy, human ratings, time-to-completion",
            "reported_results": "When implemented as TrialMind, the modular workflow achieved high search recall, substantially better ranking and extraction accuracy than baselines, and improved human efficiency in user studies.",
            "limitations": "Error propagation across modules if not checked, requires curated prompts per module, human oversight still necessary, performance depends on retrieval quality and LLM capabilities.",
            "counterpoint": true,
            "uuid": "e7911.1",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Search Query Pipeline",
            "name_full": "LLM-based query generation, augmentation, and refinement pipeline for literature search",
            "brief_description": "A multi-step procedure in TrialMind where LLMs generate initial Boolean queries from PICO, retrieved abstracts are used (RAG) to augment context, and chain-of-thought prompting refines and expands the query set to maximize recall.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Accelerating Clinical Evidence Synthesis with Large Language Models",
            "authors": "Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun",
            "year": null,
            "method_name": "Query generation + augmentation + refinement (RAG + CoT)",
            "method_description": "Prompt LLM to propose initial Boolean search terms from PICO, retrieve abstracts using those queries to form an RAG context, then use CoT prompting to iterate and expand/filter terms, producing a final augmented boolean query set used to fetch candidate studies from PubMed.",
            "input_type": "PICO elements, retrieved abstracts (as RAG context)",
            "output_type": "Comprehensive Boolean search queries and resulting candidate citation sets",
            "prompting_technique": "In-context learning for initial generation, retrieval-augmented prompts with abstracts, chain-of-thought style multi-step refinement within a single LLM pass",
            "model_name": "GPT-4 (used in TrialMind for this component)",
            "model_size": null,
            "datasets_used": "PubMed/PubMed Central; TrialReviewBench used for evaluation",
            "evaluation_metric": "Overall Recall of ground-truth included studies from reviews",
            "reported_results": "TrialMind's search pipeline produced average Recall = 0.782 across 100 reviews vs GPT-4 baseline Recall = 0.073 and human-constructed baseline = 0.187; maintained high recall even for reviews with many target studies.",
            "limitations": "LLM may propose incorrect MeSH terms or miss domain-specific synonyms without good retrieval context; the approach produces very large candidate sets requiring downstream screening; depends on quality of retrieved abstracts for augmentation.",
            "counterpoint": true,
            "uuid": "e7911.2",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Eligibility-Criteria Screening",
            "name_full": "Per-criterion LLM eligibility prediction and aggregated ranking",
            "brief_description": "A transparent screening method where the LLM generates multiple eligibility criteria from PICO, predicts per-criterion labels (-1/0/1) for each candidate study, and aggregates those predictions into a study-level relevance score used for ranking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Accelerating Clinical Evidence Synthesis with Large Language Models",
            "authors": "Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun",
            "year": null,
            "method_name": "Per-criterion eligibility prediction and aggregation",
            "method_description": "LLM generates a set of M eligibility criteria from the PICO; for each candidate study, the LLM outputs M labels (eligible/ineligible/uncertain) which can be aggregated (e.g., summation) into a single relevance score; users can edit criteria or aggregation strategies for customized ranking.",
            "input_type": "PICO elements and full-text/abstract of candidate study",
            "output_type": "Per-criterion labels, aggregated relevance scores, rationale text per criterion (for transparency)",
            "prompting_technique": "In-context learning for criteria generation, direct prompting for per-criterion labeling, optional CoT for ambiguous cases",
            "model_name": "GPT-4 (used in TrialMind for screening)",
            "model_size": null,
            "datasets_used": "TrialReviewBench candidate sets (2,000 per review) for evaluation",
            "evaluation_metric": "Recall@20, Recall@50, Recall@200, ΔRecall per criterion (leave-one-out analysis)",
            "reported_results": "Substantial ranking improvements: TrialMind attained Recall@20/50 values (e.g., Hormone Therapy Recall@20=0.431, Recall@50=0.674) outperforming embedding baselines (MPNet, MedCPT) by 1.3–2.6x fold changes across topics; average ~43% of target studies in top 50.",
            "limitations": "Some generated criteria redundant; performance sensitive to quality of criteria and aggregation function; LLM per-criterion predictions can still err (uncertain=0) requiring human review.",
            "counterpoint": true,
            "uuid": "e7911.3",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Result Extraction Pipeline",
            "name_full": "Specialized result extraction pipeline with CoT and LLM-generated executable code",
            "brief_description": "A focused multi-step pipeline to extract numerical clinical outcomes: (1) identify relevant content in the paper, (2) extract and reason about numerical values using CoT, and (3) generate and execute Python code to standardize results into tabular formats for meta-analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Accelerating Clinical Evidence Synthesis with Large Language Models",
            "authors": "Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun",
            "year": null,
            "method_name": "Result extraction with CoT + code generation",
            "method_description": "Performs CoT-style LLM prompting to (a) locate raw textual snippets describing outcomes, (b) elicit intermediate numerical values (group sizes, events, ratios), and (c) produce Python code that, when executed, computes standardized numeric metrics and produces a final table for meta-analysis; outputs are linked to source locations for verification.",
            "input_type": "Full-text articles (PDF/XML) or user-uploaded content, plus natural language description of desired clinical endpoint and cohort definition",
            "output_type": "Raw textual snippets referencing outcomes, intermediate numerical extractions, executable Python code, standardized numeric tables for meta-analysis (e.g., event counts, rates)",
            "prompting_technique": "Chain-of-thought prompting to elicit intermediate reasoning and values, retrieval of document chunks, prompting to generate data-processing code (T_PY), and execution to produce final outputs",
            "model_name": "GPT-4, Sonnet (used for extraction and code generation)",
            "model_size": null,
            "datasets_used": "TrialReviewBench (1,049 study result annotations) and full-text papers from PubMed Central",
            "evaluation_metric": "Extraction accuracy (ACC), error categorization (Inaccurate, Extraction failure, Unavailable data, Hallucination), human expert ratings in synthesized forest plots",
            "reported_results": "Result extraction ACC: TrialMind outperformed GPT-4 baseline across topics (e.g., Immunotherapy ACC 0.70 vs GPT-4 0.54); median fold-change 1.50 over best baselines; common error types were 'Inaccurate' and 'Extraction failure', hallucinations rare.",
            "limitations": "Numerical extraction and standardization remain challenging due to varied reporting formats and subgroup mismatches; hallucinations primarily in result naming/definitions; reliance on accessible full text (appendices/OCR issues may limit extraction).",
            "counterpoint": true,
            "uuid": "e7911.4",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A technique mentioned and applied within TrialMind where retrieved domain-relevant documents (e.g., abstracts) are injected into LLM prompts to reduce hallucination and provide up-to-date factual context for generation/refinement.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Accelerating Clinical Evidence Synthesis with Large Language Models",
            "authors": "Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun",
            "year": null,
            "method_name": "Retrieval-Augmented Generation (RAG)",
            "method_description": "Augments LLM prompts with retrieved relevant document snippets (here: abstracts from PubMed) so that the LLM conditions on external evidence when generating search queries, refining terms, or extracting information, thereby mitigating reliance on the LLM's internal knowledge.",
            "input_type": "Retrieved abstracts/snippets (from PubMed) plus PICO or target extraction prompts",
            "output_type": "Refined queries, more complete term lists, improved extraction outputs",
            "prompting_technique": "RAG: semantic retrieval to supply contextual grounding within LLM prompts",
            "model_name": "Used in conjunction with GPT-4 in TrialMind",
            "model_size": null,
            "datasets_used": "PubMed/PubMed Central (retrieved abstracts used as context); TrialReviewBench for evaluation",
            "evaluation_metric": "Improvement in search Recall and extraction accuracy when using retrieval context",
            "reported_results": "Use of retrieval context in query refinement contributed to substantially higher recall (TrialMind recall 0.782) compared to direct LLM prompting baselines.",
            "limitations": "Quality of retrieved context limits performance; retrieval may miss key synonyms or non-indexed documents; increases system complexity and latency.",
            "counterpoint": true,
            "uuid": "e7911.5",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought (CoT) prompting",
            "brief_description": "A prompting strategy used in TrialMind that elicits stepwise reasoning from the LLM within a single inference to handle multi-step tasks such as query self-refinement and numeric result extraction.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Accelerating Clinical Evidence Synthesis with Large Language Models",
            "authors": "Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun",
            "year": null,
            "method_name": "Chain-of-Thought prompting",
            "method_description": "Structures prompts to request intermediate sub-steps (S1..ST) from the LLM in one inference, enabling the model to draft, self-reflect, and refine outputs (e.g., produce an initial term list then filter and augment it), improving performance on complex reasoning tasks.",
            "input_type": "PICO and retrieved text chunks or full-text documents",
            "output_type": "Multi-step intermediate outputs (e.g., initial terms, filtered terms, augmentations) and refined final outputs",
            "prompting_technique": "Chain-of-thought style multi-step prompting within a single inference",
            "model_name": "GPT-4, Sonnet (applied within TrialMind)",
            "model_size": null,
            "datasets_used": "TrialReviewBench and PubMed-derived contexts",
            "evaluation_metric": "Improvements in query completeness (Recall) and extraction correctness when using CoT vs direct prompting",
            "reported_results": "CoT used in query refinement and result extraction improved coverage and numeric extraction robustness, contributing to TrialMind's superior overall performance.",
            "limitations": "Long CoT outputs can be verbose and may still contain incorrect intermediate steps; requires careful prompt design and verification of intermediate reasoning.",
            "counterpoint": true,
            "uuid": "e7911.6",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "In-context learning",
            "name_full": "In-context learning (ICL)",
            "brief_description": "A prompting paradigm invoked repeatedly in the paper where LLMs learn tasks at inference by conditioning on task descriptions and examples in the prompt instead of model fine-tuning; used for query generation, criteria generation, screening, and extraction.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Accelerating Clinical Evidence Synthesis with Large Language Models",
            "authors": "Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, Jimeng Sun",
            "year": null,
            "method_name": "In-context learning",
            "method_description": "Provides task definitions and exemplar input-output pairs in the prompt so the LLM performs zero-shot or few-shot task adaptation for generating queries, criteria, or extraction templates without needing model fine-tuning.",
            "input_type": "Task prompts containing definitions and examples, plus document text",
            "output_type": "Task-specific outputs (queries, criteria, extracted fields)",
            "prompting_technique": "Few-shot or example-driven prompting inside the LLM input context",
            "model_name": "GPT-4, Sonnet",
            "model_size": null,
            "datasets_used": "TrialReviewBench for example-driven prompts and evaluations",
            "evaluation_metric": "Task performance (Recall, ACC) under ICL-based prompting",
            "reported_results": "ICL combined with RAG and CoT underpins TrialMind's modules and contributes to high task performance versus naive prompting baselines.",
            "limitations": "Performance depends on quality and representativeness of examples; LLM context length limits amount of examples and document context.",
            "counterpoint": true,
            "uuid": "e7911.7",
            "source_info": {
                "paper_title": "Accelerating clinical evidence synthesis with large language models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "rating": 2
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 1
        }
    ],
    "cost": 0.0175275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Accelerating Clinical Evidence Synthesis with Large Language Models</h1>
<p>Zifeng Wang ${ }^{1}$, Lang Cao ${ }^{1}$, Benjamin Danek ${ }^{1}$, Qiao Jin ${ }^{2}$, Zhiyong Lu ${ }^{2}$, Jimeng Sun ${ }^{1,3 #}$<br>${ }^{1}$ Department of Computer Science, University of Illinois Urbana-Champaign, Champaign, IL<br>${ }^{2}$ National Center for Biotechnology Information, National Library of Medicine, Bethesda, MD<br>${ }^{3}$ Carle Illinois College of Medicine, University of Illinois Urbana-Champaign, Champaign, IL<br>${ }^{#}$ Corresponding authors. Emails: jimeng@illinois.edu</p>
<h4>Abstract</h4>
<p>Synthesizing clinical evidence largely relies on systematic reviews of clinical trials and retrospective analyses from medical literature. However, the rapid expansion of publications presents challenges in efficiently identifying, summarizing, and updating clinical evidence. Here, we introduce TrialMind, a generative artificial intelligence (AI) pipeline for facilitating human-AI collaboration in three crucial tasks for evidence synthesis: study search, screening, and data extraction. To assess its performance, we chose published systematic reviews to build the benchmark dataset, named TrialReviewBench, which contains 100 systematic reviews and the associated 2,220 clinical studies. Our results show that TrialMind excels across all three tasks. In study search, it generates diverse and comprehensive search queries to achieve high recall rates (Ours 0.711-0.834 v.s. Human baseline $0.138-0.232$ ). For study screening, TrialMind surpasses traditional embeddingbased methods by $30 \%$ to $160 \%$. In data extraction, it outperforms a GPT-4 baseline by $29.6 \%$ to $61.5 \%$. We further conducted user studies to confirm its practical utility. Compared to manual efforts, human-AI collaboration using TrialMind yielded a $71.4 \%$ recall lift and $44.2 \%$ time savings in study screening and a $23.5 \%$ accuracy lift and $63.4 \%$ time savings in data extraction. Additionally, when comparing synthesized clinical evidence presented in forest plots, medical experts favored TrialMind's outputs over GPT-4's outputs in $62.5 \%$ to $100 \%$ of cases. These findings show the promise of LLM-based approaches like TrialMind to accelerate clinical evidence synthesis via streamlining study search, screening, and data extraction from medical literature, with exceptional performance improvement when working with human experts.</p>
<h1>Introduction</h1>
<p>Clinical evidence is crucial for supporting clinical practices and advancing new drug development and needs to be updated regularly. ${ }^{1}$ It is primarily gathered through retrospective analysis of real-world data or through prospective clinical trials that assess new interventions on humans. Researchers usually conduct systematic reviews to consolidate evidence from various clinical studies in the literature. ${ }^{2,3}$ However, this process is expensive and time-consuming, requiring an average of five experts and 67.3 weeks based on an analysis of 195 systematic reviews. ${ }^{4}$ Moreover, the fast growth of clinical study databases means that the information in these published clinical reviews becomes outdated rapidly. ${ }^{5}$ For instance, PubMed has indexed over 35 M citations and gets over 1 M new citations annually. ${ }^{6}$ This situation underscores the urgent need to streamline the systematic review processes to document systematic and timely clinical evidence from the extensive medical literature. ${ }^{1,7}$</p>
<p>Large language models (LLMs) excel at information processing and generating. They can be adapted to target tasks by providing the task definition and examples as the inputs (namely "prompts"). ${ }^{8}$ Researchers have tried to adopt LLMs for many individual tasks in the evidence synthesis process, including generating searching queries, ${ }^{9,10}$ extracting studies' population, intervention, comparison, outcome (PICO) elements, ${ }^{11,12}$ screening citations, ${ }^{13}$ and summarizing findings from multiple studies. ${ }^{14-17}$ However, few have investigated LLMs' effectiveness across the entire evidence synthesis process. ${ }^{18}$ This is crucial because it ensures a seamless integration of AI in every step, potentially improving overall efficiency and accuracy. Understanding the strengths and limitations of LLMs in a holistic manner enables more effective automation and human-AI collaboration. To fill this gap, we created a testing dataset TrialReviewBench that covers major tasks in evidence synthesis, including study search, screening, and data extraction tasks. We chose published systematic reviews to create the dataset. As a result, the dataset includes 100 systematic reviews with 2,220 associated clinical studies. It also consists of manual annotations of 1,334 study characteristics and 1,049 study results. Based on TrialReviewBench, we are able to assess cutting-edge LLMs, e.g., GPT-4, ${ }^{19}$ in clinical evidence synthesis tasks.</p>
<p>Furthermore, this study aims to fill the gap in adapting LLMs to evidence synthesis tasks, overcoming LLM's limitations in (1) hallucinations, (2) weakness in reasoning with numerical data, (3) overly generic outputs, and (4) lack of transparency and reliability. ${ }^{20}$ Specifically, we developed an AI-driven pipeline named TrialMind, which is optimized for (1) generating boolean queries to search citations from the literature; (2) building eligibility criteria and screening through the found citations; and (3) extracting data, including study protocols, methods, participant baselines, study results, etc., from publications and reports. More importantly, TrialMind breaks down into subtasks that adhere to the established practice of systematic reviews, ${ }^{21}$ which facilitates experts in the loop to monitor, edit, and verify intermediate outputs. It also has the flexibility to allow experts to begin at any intermediate step as needed.</p>
<p>In this study, we show that the TrialMind is able to 1) retrieve a complete list of target studies from the literature, 2) follow the specified eligibility criteria to rank the most relevant studies at the top, and 3) achieve high accuracy in extracting information and clinical outcomes from unstructured documents based on user requests. Beyond providing descriptive evidence, TrialMind can extract numerical clinical outcomes to be standardized as input for meta-analysis (e.g., forest plots). A human evaluation was conducted to assess the synthesized evidence. Finally, to validate the practical benefits, we developed an accessible web application based on TrialMind and conducted a user study comparing two approaches: AI-assisted experts versus standalone experts. We measured the time savings and evaluated the output quality of each approach. The results show that TrialMind significantly reduced the time required for study search, citation screening, and data extraction, while maintaining or improving the quality of the output compared to experts working alone.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overview of TrialMind pipeline. a, it has four main steps: literature search, literature screening, data extraction, and evidence synthesis. b, (1) Utilizing input PICO elements, TrialMind generates key terms to construct Boolean queries for retrieving studies from literature databases. (2) TrialMind formulates eligibility criteria, which users can edit to provide context for LLMs during eligibility predictions. Users can then select studies based on these predictions and rank their relevance by aggregating them. (3) TrialMind processes the descriptions of target data fields to extract and output the required information as structured data. (4) TrialMind extracts findings from the studies and collaborates with users to synthesize the clinical evidence.</p>
<h1>Results</h1>
<h2>Creating TrialReviewBench from medical literature</h2>
<p>A systematic understanding of cancer treatments is crucial for oncology drug discovery and development. We retrieved a list of cancer treatments from the National Cancer Institute's introductory page as the keywords to search medical systematic reviews. ${ }^{22}$ To ensure data quality, we crafted comprehensive queries with automatic filtering and manual screening. For each review, we obtained the list of studies with their PubMed IDs, retrieved their full content, and extracted study characteristics and clinical outcomes. We followed PubMed's usage policy and guidelines during retrieval. Further manual checks were performed to correct inaccuracies, eliminate invalid and duplicate papers, and refine the text for clarity (Methods). The final TrialReviewBench dataset consists of 2,220 studies involved in 100 reviews (Fig. 2a), covering four major topics: Immunotherapy, Radiation/Chemotherapy, Hormone Therapy, and Hyperthermia. We manually created three major evaluation tasks based on these reviews: study search, study screening, and data extraction.</p>
<p>The study search task begins with the PICO (Population, Intervention, Comparison, Outcome) elements extracted from the abstract of a systematic review, which serve as the formal definition of the research question. The model being tested is tasked with generating relevant keywords for the treatment and condition terms, as depicted in Fig. 2e. These keywords are then used to form Boolean queries, which are submitted to search citations in the PubMed database. The performance of the model is evaluated by checking whether the retrieved studies include those that were actually involved in the target systematic review. The recall rate is computed by measuring the proportion of actually involved studies identified through the search.</p>
<p>For the study screening task, the input consists of the PICO elements defined in the target systematic review. A candidate set of 2,000 citations is created by combining the actual studies included in the review with additional citations retrieved during the search but not included in the review. The model being tested ranks these citations based on the likelihood that each citation should be included in the systematic review. To assess the model's performance, we compute Recall@ $k$ : the recall value indicating how many of the actual included studies appear in the top $k$ ranked candidates.</p>
<p>The data extraction task focuses on retrieving specific information from the input study documents. In this case, we extract Table 1 from each systematic review, which typically details study characteristics such as study design, population demographics, and outcome measurements. These characteristics are matched to the individual studies and manually verified, yielding 1,334 study characteristic annotations. Additionally, we extract individual study results from the review's reported analysis, often presented in forest plots, capturing metrics such as overall response and event rates, resulting in 1,049 study result annotations. The model being tested is given a list of target data points to extract, and its output is evaluated by assessing the accuracy of the extracted information based on the annotated datasets.</p>
<h2>Build an LLM-driven system for clinical evidence synthesis</h2>
<p>Large language models (LLMs) excel in adapting to new tasks when provided with taskspecific prompts while often struggling with complex tasks that require multiple steps of planning and reasoning. Additionally, interacting and collaborating with LLMs can be problematic due to their opaque nature and the complexity of debugging. ${ }^{23}$ In this study, we developed TrialMind that decomposes the clinical evidence synthesis process into four main tasks (Fig. 1 and Methods). Initially, using the provided research question enriched with population, intervention, comparison, and outcome (PICO) elements, TrialMind conducts a comprehensive search from the literature. It also works with users to build the eligibility criteria for target studies and then automate screening and ranking identified citations. Next, TrialMind browses the study details to extract the study characteristics and pertinent findings. To ensure the accuracy and integrity of the data, each output is linked to the sources for manual inspection. In the final step, TrialMind standardizes the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Literature search experiment results. a, The total number of involved studies and the number of review papers across different topics. b, The TrialMind's interface for users to retrieve studies. c, the Recall of the search results for reviews across four topics. The bar heights indicate the Recall, and the star indicates the number of studies found. d, Scatter plots of the Recall against the number of ground-truth studies. Each scatter indicates the results of one review. Regression estimates are displayed with the 95% CIs in blue or purple. e, Example cases comparing the outputs of three methods.</p>
<p>clinical outcomes for meta-analysis.</p>
<h1>TrialMind can make a comprehensive retrieval of studies from the literature</h1>
<p>Finding relevant studies from medical literature like PubMed, which contains over 35 million entries, can be challenging. Typically, this requires the research expertise to craft complex queries that comprehensively cover pertinent studies. The challenge lies in balancing the specificity of queries: too stringent, and the search may miss relevant studies; too broad, and it becomes impractical to manually screen the overwhelming number of results. Previous approaches propose to prompt LLMs to generate the searching query directly, ${ }^{9}$ which can induce incomplete searching results due to the limited knowledge of LLMs. In contrast, TrialMind is designed to produce comprehensive queries through a pipeline that includes query generation, augmentation, and refinement. It also provides users with the ability to make further adjustments (Fig. 2b).</p>
<p>The dataset involving clinical studies spanning ten cancer treatment areas was used for evaluation (Fig. 2a). For each review, we collected the involved studies' PubMed IDs as the ground-truth and measured the Recall, i.e., how many ground-truth studies are found in the search results. We created two baselines as the comparison: GPT-4 and Human. The GPT-4 baseline makes a guided prompt for LLMs to generate the boolean queries. ${ }^{9}$ It represents the common way of prompting LLMs for literature search query generation. The Human baseline represents a way where the key terms from PICO elements are extracted manually and expanded, referring to UMLS, ${ }^{24}$ to construct the search queries.</p>
<p>Overall, TrialMind achieved a Recall of 0.782 on average for all reviews in TrialReviewBench, meaning it can capture most of the target studies. By contrast, the GPT-4 baseline yielded Recall $=0.073$, and the Human baseline yielded Recall $=0.187$. We divided the search results across four topics determined by the treatments studied in each review (Fig. 2c). Our analysis showed that TrialMind can identify many more studies than the baselines. For instance, TrialMind achieved Recall $=0.797$ with identified studies $N=22,084$ for Immunotherapy-related reviews, while the GPT-4 baseline got Recall $=0.094$ ( $N$ studies $=27$ ), and the Human baseline got Recall $=0.154(N$ studies $=958)$, respectively. In Radiation/Chemotherapy, TrialMind achieved Recall $=0.780$, the GPT-4 baseline got Recall $=0.020$, and the Human baseline got Recall $=0.138$. In Hormone Therapy, TrialMind achieved Recall $=0.711$, the GPT-4 baseline got Recall $=0.067$, and the Human baseline got Recall $=0.232$. In Hyperthermia, TrialMind achieved Recall $=0.834$, the GPT-4 baseline got Recall $=0.106$, and the Human baseline got Recall $=0.202$. These results demonstrate that regardless of the search task's complexity, as indicated by the variability in the Human baseline, TrialMind consistently retrieves nearly all target studies from the PubMed database. This robust performance provides a solid foundation for accurately identifying target studies in the screening phase.</p>
<p>Furthermore, we made scatter plots of Recall versus the number of target studies for each review (Fig. 2d). The hypothesis was that an increase in target studies correlates with the difficulty of achieving complete coverage. Our findings reveal that TrialMind consistently maintained a high Recall, significantly outperforming the best baselines across all 100 reviews. A trend of declining Recall with an increasing number of target studies was confirmed through regression analysis. It was found that the GPT-4 baseline struggled, showing Recall close to 0 , and the Human baseline results varied, with most reviews below 0.5. As the number of target studies increased, the Human and GPT-4 baselines' Recall decreased to nearly zero. In contrast, TrialMind demonstrated remarkable resilience, showing minimal variation in performance despite the increasing number of target studies. For instance, in a review involving 141 studies, TrialMind achieved a Recall of 0.99 , while the GPT-4 and Human baselines obtained a Recall of 0.02 and 0 , respectively.</p>
<h2>TrialMind enhances literature screening and ranking</h2>
<p>Typically, human experts manually sift through thousands of retrieved studies to select relevant ones for inclusion in a systematic review. This process adheres to the PRISMA</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Literature screen experiment results. <strong>a</strong>, Streamline study screening using TrialMind with human in the loop. <strong>b</strong>, Ranking performances for Recall@20/50 within across therapeutic areas. <strong>c</strong>, Recall@20 and Recall@50 for TrialMind and selected baselines. <strong>d</strong>, Effect of individual criterion on the ranking results. <strong>e</strong>, Ranking performance for Recall@<em>K</em> with varying <em>K</em> in four topics. Shaded areas are 95% confidence interval.</p>
<p>statement, ${ }^{21}$ which involves creating a list of eligibility criteria and assessing each study's eligibility. TrialMind streamlines this task through a three-step approach: (1) it generates a set of inclusion criteria, which are subject to user's adjustments; (2) it applies these criteria to evaluate the study's eligibility, denoted by ${-1,0,1}$ where -1 and 1 represent eligible and non-eligible, and 0 represents unknown/uncertain, respectively; and (3) it ranks the studies by aggregating the eligibility predictions, where the aggregation strategy can be specified by users (Fig. 3a). We took a summation of the criteria-level eligibility predictions as the study-level relevance prediction scores for ranking. As such, TrialMind provides a rationale for the relevance scores by detailing the eligibility predictions for each criterion.</p>
<p>We chose MPNet ${ }^{25}$ and MedCPT ${ }^{26}$ as the general domain and medical domain ranking baselines, respectively. These methods compute study relevance by the cosine similarity between the encoded PICO elements as the query and the encoded study's abstracts. We also set a Random baseline that randomly samples from candidates. We created the evaluation data based on the search results in the first stage. For each review, we mixed the target studies with the other found studies to build a candidate set of 2,000 studies for ranking. Discriminating the target studies from the other candidates is challenging since all candidates meet the search queries, meaning they most probably investigate the relevant therapies or conditions. We evaluated the ranking performance using the Recall@20 and Recall@50 metrics. The concatenation of the title and abstract of each study is used for all methods as inputs.</p>
<p>We found that TrialMind greatly improved ranking performances, with the fold changes over the best baselines ranging from 1.3 to 2.6 across four topics (Table 3c). For instance, for the Hormone Therapy topic, TrialMind obtained Recall@20 $=0.431$ and Recall@50 $=0.674$. In the Hyperthermia topic, TrialMind obtained Recall@20 $=0.518$ and Recall@50 $=0.710$. In the Immunotherapy topic, TrialMind obtained Recall@20 $=$ 0.567 and Recall@50 $=0.713$. In the Radiation/Chemotherapy topic, TrialMind obtained Recall@20 $=0.416$ and Recall@50 $=0.654$. In contrast, other baselines exhibit significant variability across different topics. The general domain baseline MPNet was the worst as it performed similarly to the Random baseline in Recall@20. MedCPT showed marginal improvement over MPNet in the last three topics, while both failed to capture enough target studies in all topics.</p>
<p>Furthermore, TrialMind demonstrated significant improvements over the baselines across various therapeutic areas (Fig. 3b). For example, in "Cancer Vaccines" and "Hormone Therapy," TrialMind substantially increased Recall@50, achieving 33.33-fold and 10.53 -fold improvements, respectively, compared to the best-performing baseline. TrialMind generally attained a fold change greater than 2 (ranging from 1.57 to 33.33 ). Despite the challenge of selecting from a large pool of candidates $(n=2,000)$ where candidates were very similar, TrialMind identified an average of $43 \%$ of target studies within the top 50. We compared TrialMind to MedCPT and MPNet for Recall@ $K(K$ in 10 to 200) to gain insight into how $K$ influences the performances (Fig. 3e). We found TrialMind can capture most of the target studies (over $80 \%$ ) when $K=100$.</p>
<p>To thoroughly assess the quality of these criteria and their impact on ranking performance, we conducted a leave-one-out analysis to calculate $\Delta$ Recall@200 for each criterion (Fig. 3d). The $\Delta$ Recall@200 metric measures the difference in ranking performance with and without a specific criterion, with a larger value indicating superior criterion quality. Our findings revealed that most criteria positively influenced ranking performances, as the negative influence criteria are $n=1$ in Hormone Therapy, $n=1$ in Hyperthermia, $n=5$ in Radiation/Chemotherapy, and $n=7$ in Immunotherapy. Additionally, we identified redundancies among the generated criteria, as those with $\Delta$ Recall@200 $=0$ were the most frequently observed. This redundancy likely stems from some criteria covering similar eligibility aspects, thus not impacting performance when one is omitted.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Data and result extraction experiment results. a, Streamline study information extraction using TrialMind. b, Data extraction accuracy within each field type across four topics. c, Confusion matrix showing the hallucination and missing rates in the data extraction results. d, Result extraction accuracy across topics. e, Result extraction accuracy across clinical endpoints. f, Error analysis of the result extraction. g, Streamline result extraction using TrialMind.</p>
<h1>TrialMind scales data and result extraction from unstructured documents</h1>
<p>TrialMind leverages LLMs to streamline extracting study characteristics such as target therapies, study arm design, and participants' baseline information from involved studies. Specifically, TrialMind refers to the field names and the descriptions from users and use the full content of the study documents in PDF or XML formats as inputs (Fig. 4a). When the free full content is unavailable, TrialMind accepts the user-uploaded content as the input. We developed an evaluation dataset by converting the study characteristic tables from each review paper into data points. Our dataset comprises 1,334 target data points, including 696 on study design, 353 on population features, and 285 on results. We assessed the data extraction performance using the Accuracy metric.</p>
<p>TrialMind demonstrated strong extraction performance across various topics (Fig. 4b): it achieved an accuracy of $\mathrm{ACC}=0.78(95 \%$ confidence interval $(\mathrm{CI})=0.75-0.81)$ in the Immunotherapy topic, $\mathrm{ACC}=0.77(95 \% \mathrm{CI}=0.72-0.82)$ in the Radiation/Chemotherapy topic, $\mathrm{ACC}=0.72(95 \% \mathrm{CI}=0.63-0.80)$ in the Hormone Therapy topic, and $\mathrm{ACC}=0.83$ $(95 \% \mathrm{CI}=0.74-0.90)$ in the Hyperthermia topic. These results indicate that TrialMind can provide a solid initial data extraction, which human experts can refine. Importantly, each output can be cross-checked by the linked original sources, facilitating verification and further investigation.</p>
<p>Diving deeper into the accuracy across different types of fields, we observed varying performance levels. It performed best in extracting study design information, followed by population details, and showed the lowest accuracy in extracting results (Fig. 4b). For example, in the Immunotherapy topic, TrialMind achieved an accuracy of $\mathrm{ACC}=0.95$ $(95 \% \mathrm{CI}=0.92-0.96)$ for study design, $\mathrm{ACC}=0.74(95 \% \mathrm{CI}=0.67-0.80)$ for population data, and $\mathrm{ACC}=0.42(95 \% \mathrm{CI}=0.36-0.49)$ for results. This variance can be attributed to the prevalence of numerical data in the fields: fields with more numerical data are typically harder to extract accurately. Study design is mostly described in textual format and is directly presented in the documents, whereas population and results often include numerical data such as the number of patients or gender ratios. Results extraction is particularly challenging, often requiring reasoning and transformation to capture values accurately. Given these complexities, it is advisable to scrutinize the extracted numerical data more carefully.</p>
<p>We also evaluated the robustness of TrialMind against hallucinations and missing information (Fig. 4c). We constructed a confusion matrix detailing instances of hallucinations: false positives (FP) where TrialMind generated data not present in the input document and false negatives (FN) where it failed to extract available target field information. We observed that TrialMind achieved a precision of Precision $=0.994$ for study design, Precision $=0.966$ for population, and Precision $=0.862$ for study results. Missing information was slightly more common than hallucinations, with TrialMind achieving recall rates of Recall $=0.946$ for study design, Recall $=0.889$ for population, and Recall $=0.930$ for study results. The incidence of both hallucinations and missing information was generally low. However, hallucinations were notably more frequent in study results; this often occurred because LLMs could confuse definitions of clinical outcomes, for example, mistaking 'overall response' for 'complete response.' Nevertheless, such hallucinations are typically manageable, as human experts can easily identify and correct them while reviewing the referenced material.</p>
<p>The challenges in extracting study results primarily stem from (1) identifying the locations that describe the desired outcomes from lengthy papers, (2) accurately extracting relevant numerical values such as patient numbers, event counts, durations, and ratios from the appropriate patient groups, and (3) performing the correct calculations to standardize these values for meta-analysis. In response to these complexities, we developed a specialized pipeline for result extraction (Fig. 4g), where users provide the interested outcome and the cohort definition. TrialMind offers a transparent extraction workflow, documenting the sources of results along with the intermediate reasoning and calculations.</p>
<p>We compared TrialMind against two generalist LLM baselines, GPT-4 and Sonnet, which were prompted to extract the target outcomes from the full content of the study</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Results of human evaluation and user study. a, TrialMind's result extraction process. b, Winning rate of TrialMind against the GPT-4+Human baseline across studies. c, Violin plots of the ratings across studies. Each plot is tagged with the mean ratings (95% CI) from all the annotators. d,Violin plots of the ratings across annotators with different expertise levels. Each plot is tagged with the mean ratings (95% CI) from all the studies. e, Overall performance and time cost of study screening and data extraction tasks, respectively. f, Screening time cost and performance across reviews and participants. g, Data extraction accuracy across participants and different types of data.</p>
<p>documents. Since the baselines can only make text extractions, we manually convert them into numbers suitable for meta-analysis. ${ }^{27}$ This made very strong baselines since they combined LLM extraction with human post-processing. We assessed the performance using the Accuracy metric.</p>
<p>The evaluation conducted across four topics demonstrated the superiority of TrialMind (Fig. 4d). Specifically, in Immunotherapy, TrialMind achieved an accuracy of $\mathrm{ACC}=0.70$ ( $95 \%$ CI $0.62-0.77$ ), while GPT-4 scored $\mathrm{ACC}=0.54$ ( $95 \%$ CI $0.45-0.62$ ). In Radiation/Chemotherapy, TrialMind reached $\mathrm{ACC}=0.65$ ( $95 \%$ CI $0.51-0.76$ ), compared to GPT-4's ACC $=0.52$ ( $95 \%$ CI $0.39-0.65$ ). For Hormone Therapy, TrialMind achieved $\mathrm{ACC}=0.80$ ( $95 \%$ CI $0.58-0.92$ ), outperforming GPT-4, which scored $\mathrm{ACC}=0.50$ ( $95 \%$ CI $0.30-0.70$ ). In Hyperthermia, TrialMind obtained an accuracy of $\mathrm{ACC}=0.84$ ( $95 \%$ CI $0.71-0.92$ ), significantly higher than GPT-4's ACC $=0.52$ ( $95 \%$ CI $0.39-0.65$ ). The breakdowns of evaluation results by the most frequent types of clinical outcomes (Fig. 4e) showed TrialMind got fold changes in accuracy ranging from 1.05 to 2.83 and a median of 1.50 over the best baselines. This enhanced effectiveness is largely attributable to TrialMind's ability to accurately identify the correct data locations and apply logical reasoning, while the baselines often produced erroneous initial extractions.</p>
<p>We analyzed the error cases in our result extraction experiments and identified four primary error types (Fig. 4f). The most common error was 'Inaccurate' extraction ( $\mathrm{n}=36$ ), followed by 'Extraction failure' ( $\mathrm{n}=27$ ), 'Unavailable data' ( $\mathrm{n}=10$ ), and 'Hallucinations' $(\mathrm{n}=3)$. 'Inaccurate' extractions often occurred due to multiple sections ambiguously describing the same field. For example, a clinical study might report the total number of participants receiving CAR-T therapy early in the document and later provide outcomes for a subset with non-small cell lung cancer (NSCLC). The specific results for NSCLC patients are crucial for reviews focused on this subgroup, yet the presence of general data can lead to confusion and inaccuracies in extraction. 'Extraction failure' and 'Unavailable data' both illustrate scenarios where TrialMind could not retrieve the information. The latter case particularly showcases TrialMind's robustness against hallucinations, as it failed to extract data outside the study's main content, such as in appendices, which were not included in the inputs. Furthermore, errors caused by hallucinations were minor. The outputs were easy to identify and correct through manual inspection since no references were provided.</p>
<h1>TrialMind facilitates clinical evidence synthesis via human-AI collaboration</h1>
<p>We selected five systematic review studies as benchmarks and referenced the clinical evidence reported in the target studies. The baseline used GPT-4 with a simple prompting to extract the relevant text pieces that report the target outcome of interest (Methods). Manual calculations were necessary to standardize the data for meta-analysis. In contrast, TrialMind automated the extraction and standardization (Fig. 5a by (1) extracting the raw result description from the input document and (2) standardizing the results by generating a Python program to assist the calculation. The standardized results from all involved studies are then fed into the R program by human experts to make the aggregated evidence in a forest plot.</p>
<p>We engaged with human annotators to assess the quality of synthesized clinical evidence presented in forest plots. Each annotator was asked to evaluate the evidence quality by comparing it against the evidence reported in the target review and deciding which method, TrialMind or the baseline, produced superior results (Extended Fig. 1). Additionally, they rated the quality of the synthesized clinical evidence on a scale of 1 to 5. The assignment of our method and the baseline was randomized to ensure objectivity. The results highlighted TrialMind's superior performance compared to the direct use of GPT-4 for clinical evidence synthesis (Fig. 5b). We calculated the winning rate of TrialMind versus the baseline across the five studies. The results indicate a consistent preference by annotators for the evidence synthesized by TrialMind over that of the baseline. Specifically, TrialMind achieved winning rates of $87.5 \%, 100 \%, 62.5 \%, 62.5 \%$, and</p>
<p>$81.2 \%$, respectively. The baseline's primary shortcoming stemmed from the initial extraction step, where GPT-4 often failed to identify the relevant sources without well-crafted prompting. Therefore, the subsequent manual post-processing was unable to rectify these initial errors.</p>
<p>In addition, we illustrated the ratings of TrialMind and the baseline across studies (Fig. 5c). We found TrialMind was competent as the GPT-4+Human baseline and outperformed the baseline in many scenarios. For example, TrialMind obtained the mean rating of 4.25 ( $95 \%$ CI 3.93-4.57) in Study #1 while the baseline obtained 3.50 ( $95 \%$ CI 3.13-3.87). In Study #2, TrialMind yielded 3.50 ( $95 \%$ CI 3.13-3.87) while the baseline yielded 1.25 ( $95 \%$ CI 0.93-1.57). The performance of the two methods was comparable in the remaining three studies. These results highlight TrialMind as a highly effective alternative to conventional LLM usage in evidence synthesis, streamlining data extraction and processing while maintaining the critical benefit of human oversight.</p>
<p>We requested that annotators self-assess their expertise level in clinical studies, classifying themselves into three categories: 'Basic', 'Familiar', and 'Advanced'. The typical profile ranges from computer scientists at the basic level to medical doctors at the advanced level. We then analyzed the ratings given to both methods across these varying expertise levels (Fig. 5d). We consistently observed higher ratings for TrialMind than the baseline across all groups. Annotators with basic knowledge tended to provide more conservative ratings, while those with more advanced expertise offered a wider range of evaluations. For instance, the 'Basic' group provided average ratings of 3.67 ( $95 \%$ CI 3.34-3.39) for TrialMind compared to 3.22 ( $95 \%$ CI 2.79-3.66) for the baseline. The 'Advanced' group rated TrialMind at an average of 3.40 ( $95 \%$ CI 3.16-3.64) and the baseline at $3.07(95 \%$ CI $2.75-3.39)$.</p>
<p>We conducted user studies to compare the quality and time efficiency between purely manual efforts and human-AI collaboration using TrialMind. Two participants were involved in both study screening and data extraction tasks. For the screening task, each participant was assigned 4 systematic review papers, with 100 candidate citations identified for each review. The participants were asked to select the 10 most likely relevant citations from the candidate pool. Each participant was provided with 2 candidate sets pre-ranked by TrialMind and 2 unranked sets. The participants also recorded the time taken to complete the screening process for each set. For the data extraction task, each participant was given 10 clinical studies. They manually extracted the target information for 5 of these studies. For the other 5, TrialMind was first used to perform an initial extraction, and the participants were required to verify and correct the extracted results. The time taken for the extraction process was reported for each study.</p>
<p>In Fig. 5e, we present the average performance and time cost for the AI+Human and Human-only approaches across both the study screening and data extraction tasks. The results demonstrate that the AI+Human approach consistently outperforms the Humanonly approach. For the screening tasks, AI+Human achieved a $71.4 \%$ relative improvement in Recall, while reducing time by $44.2 \%$ compared to the Human-only arm. This underscores the significant advantage of TrialMind in accelerating the study screening process while also improving its quality. Similarly, for the data extraction tasks, the AI+Human approach improved extraction accuracy by $23.5 \%$ on average, with a $63.4 \%$ reduction in time required.</p>
<p>Detailed results of screening time and performance are shown in Fig. 5f, where two reviews showed the AI+Human approach achieving the same Recall as the Human-only arm with notable time savings, and in two other reviews, AI+Human achieved higher Recall with less time. From Fig. 5g, we see that the AI+Human approach delivered better or comparable accuracy across all three types of data, with the smallest gap in "Study design". This is likely because study design information is often readily available in the study abstract, making it relatively easier for humans to extract. In contrast, the other two data types are embedded deeper within the main content, which can sometimes make it challenging for human readers to locate the correct information.</p>
<h1>Discussion</h1>
<p>Clinical evidence forms the bedrock of evidence-based medicine, crucial for enhancing healthcare decisions and guiding the discovery and development of new therapies. It often comes from a systematic review of diverse studies found in the literature, encompassing clinical trials and retrospective analyses of real-world data. Yet, the burgeoning expansion of literature databases presents formidable challenges in efficiently identifying, summarizing, and maintaining the currency of this evidence. For instance, a study by the US Agency for Healthcare Research and Quality (AHRQ) found that half of 17 clinical guidelines became outdated within a couple of years. ${ }^{28}$</p>
<p>The rapid development of large language models (LLMs) and AI technologies has generated considerable interest in their potential applications in clinical research. ${ }^{29,30}$ However, most of them focused on an individual aspect of the clinical evidence synthesis process, such as literature search, ${ }^{31,32}$ citation screening, ${ }^{33-35}$ quality assessment, ${ }^{36}$ or data extraction. ${ }^{37,38}$ In addition, implementing these models in a manner that is collaborative, transparent, and trustworthy poses significant challenges, especially in critical areas such as medicine. ${ }^{39}$ For instance, when utilizing LLMs to summarize evidence from multiple studies, the descriptive summaries often usually merely echo the findings verbatim, omit crucial details, and fail to adhere to established best practices. ${ }^{14}$ Besides, when given a set of studies that are irrelevant to the research question, LLMs are prone to produce hallucinations and hence cause misleading evidence. ${ }^{40}$ This challenge highlights the need for an integrated pipeline, involving the study search and screening stages, to strategically pick the target studies for analysis, ${ }^{41,42}$ or enhanced with human-AI collaboration. ${ }^{43}$</p>
<p>This study introduces a clinical evidence synthesis pipeline enhanced by LLMs, named TrialMind. This pipeline is structured in accordance with established medical systematic review protocols, involving steps such as study searching, screening, data/result extraction, and evidence synthesis. At each stage, human experts have the capability to access, monitor, and modify intermediate outputs. This human oversight helps to eliminate errors and prevents their propagation through subsequent stages. Unlike approaches that solely depend on the knowledge of LLMs, TrialMind integrates human expertise through in-context learning and chain-of-thought prompting. Additionally, TrialMind extends external knowledge sources to its outputs through retrieval-augmented generation and leveraging external computational tools to enhance the LLM's reasoning and analytical capabilities. Comparative evaluations of TrialMind and traditional LLM approaches have demonstrated the advantages of this system design in LLM-driven applications within the medical field.</p>
<p>This study also has several limitations. First, despite incorporating multiple techniques, LLMs may still make errors at any stage. Therefore, human oversight and verification remain crucial when implementing TrialMind in practical settings. Second, the prompts used in TrialMind were developed based on prompt engineering experience, suggesting potential for performance enhancement through advanced prompt optimization or by fine-tuning the underlying LLMs to suit specific tasks better. Third, while TrialMind demonstrated effectiveness in study search, screening, and data extraction, the dataset used was limited in size due to the high costs associated with human labeling. Future research could expand on these findings with larger datasets to further validate the method's effectiveness. Fourth, the study coverage was restricted to publicly available sources from PubMed Central, which provides structured PDFs and XMLs. Many relevant studies are either not available on PubMed or are in formats that entail OCR algorithms as preprocessing, indicating a need for further engineering to incorporate broader data sources. Fifth, although TrialMind illustrated the potential of using advanced LLMs like GPT4 to streamline clinical evidence synthesis, developing techniques to adapt the pipeline for use with other LLMs could increase its applicability. Finally, while the use of LLMs like GPT-4 can accelerate study screening and data extraction, the associated costs and processing times may present bottlenecks in some scenarios. Future enhancements that improve efficiency or utilize localized, specialized smaller models could increase practical utility.</p>
<p>LLMs have made significant strides in AI applications. TrialMind exemplifies a crucial</p>
<p>aspect of system engineering in LLM-driven pipelines, facilitating the practical, robust, and transparent use of LLMs. We anticipate that TrialMind will benefit the medical AI community by fostering the development of LLM-driven medical applications and emphasizing the importance of human-AI collaboration.</p>
<h1>Methods</h1>
<h2>Description of the TrialReviewBench Dataset</h2>
<p>The overall flowchart for the study identification and screening process in building TrialReviewBench is illustrated in Extended Fig. 2.</p>
<p>Database search and initial filtering We undertook a comprehensive search on the PubMed database for meta-analysis papers related to cancer. The Boolean search terms were specifically chosen to encompass a broad spectrum of cancer-related topics. These terms included "cancer", "oncology", "neoplasm", "carcinoma", "melanoma", "leukemia", "lymphoma", and "sarcoma". Additionally, we incorporated terms related to various treatment modalities such as "therapy", "treatment", "chemotherapy", "radiation therapy", "immunotherapy", "targeted therapy", "surgical treatment", and "hormone therapy". To ensure that our search was exhaustive yet precise, we also included terms like "meta-analysis" and "systematic review" in our search criteria.</p>
<p>This initial search yielded an extensive pool of 46,192 results, reflecting the vast research conducted in these areas. We applied specific filters to refine these results and ensure relevance and quality. We focused on articles where PMC Full text was available and specifically categorized under "Meta-Analysis". Further refinement was done by restricting the time frame of publications to those between January 1, 2020, and January 1, 2023. We also narrowed our focus to studies conducted on humans and those available in English. This filtration process was critical in distilling the initial results into a more manageable and focused collection of 2,691 papers.</p>
<p>Refinement Building upon our initial search, we employed further refinement techniques using both MeSH terms and specific keywords. The MeSH terms were carefully selected to target papers precisely relevant to various forms of cancer. These terms included "cancer", "tumor", "neoplasms", "carcinoma", "myeloma", and "leukemia". This focused approach using MeSH terms effectively reduced our selection to 1,967 papers.</p>
<p>To further dive in on papers investigating cancer therapies, we utilized many keywords derived from the National Cancer Institute's "Types of Cancer Treatment" list. This approach was multi-faceted, with each set of keywords targeting a specific category of cancer therapy. For chemotherapy, we included terms like "chemotherapy", "chemo", and related variations. In the realm of hormone therapy, we searched for phrases such as "hormone therapy", "hormonal therapy", and similar terms. The keyword group for hyperthermia encompassed terms like "hyperthermia", "microwave", "radiofrequency", and related technologies. For cancer vaccines, we included keywords such as "cancer vaccines", "cancer vaccine", and other related terms. The search for immune checkpoint inhibitors and immune system modulators was comprehensive, including terms like "immune checkpoint inhibitors", "immunomodulators", and various cytokines and growth factors. Lastly, our search for monoclonal antibodies and T-cell transfer therapy included relevant terms like "monoclonal antibodies", "t-cell therapy", "car-t", and other related phrases.</p>
<p>The careful application of keyword filtering played a crucial role in narrowing down our pool of research papers to a more focused and relevant set of 352 . It represents a diverse and meaningful collection of studies in cancer therapy, highlighting a range of innovative and impactful research within this field.</p>
<p>Manual screening of titles and abstracts Then, we manually screened titles and abstracts, applying a rigorous classification and sorting methodology. The remaining papers were first categorized based on the type of cancer treatment they explored. We</p>
<p>then organized these papers by their citation count to gauge their impact and relevance in the field. Our selection criteria aimed to enhance the quality and relevance of our final dataset. We prioritized papers that focused on the study of treatment effects, such as safety and efficacy, of various cancer interventions. We preferred studies that compared individual treatments against a control group, as opposed to those examining the effects of combined therapies (e.g., Therapy A+B vs. A only). To build a list of representative meta-analyses, we needed to ensure diversity in the target conditions under each treatment category.</p>
<p>Further, we favored studies that involved a larger number of individual studies, providing a broader base of evidence. However, we excluded network analysis studies and meta-analyses that focused solely on prognostic and predictive effects, as they did not align with our primary research focus. To maintain a balanced representation, we limited our selection to a maximum of three papers per treatment category. This process culminated in a final dataset comprising 100 systematic review papers. This curated collection forms the backbone of our analysis, ensuring a concentrated and pertinent selection of high-quality studies directly relevant to our research objectives.</p>
<h1>LLM Prompting</h1>
<p>Prompting steers LLMs to conduct the target task without training the underlying LLMs. TrialMind proceeds clinical evidence synthesis in multiple steps associated with a series of prompting techniques.</p>
<p>In-context learning LLMs exhibit a profound ability to comprehend input requests and adhere to provided instructions during generation. The fundamental concept of incontext learning (ICL) is to enable LLMs to learn from examples and task instructions within a given context at inference time. ${ }^{8}$ Formally, for a specific task, we define $T$ as the task prompt, which includes the task definition, input format, and desired output format. During a single inference session with input $X$, the LLM is prompted with $P(T, X)$, where $P(\cdot)$ is a transformation function that restructures the task definition $T$ and input $X$ into the prompt format. The output $\hat{X}$ is then generated as $\hat{X}=\operatorname{LLM}(P(T, X))$.</p>
<p>Retrieval-augmented generation LLMs that rely solely on their internal knowledge often produce erroneous outputs, primarily due to outdated information and hallucinations. This issue can be mitigated through retrieval-augmented generation (RAG), which enhances LLMs by dynamically incorporating external knowledge into their prompts during generation. ${ }^{44}$ We denote $R_{K}(\cdot)$ as the retriever that utilizes the input $X$ to source relevant contextual information through semantic search. $R_{K}(\cdot)$ enables the dynamic infusion of tailored knowledge into LLMs at inference time.</p>
<p>Chain-of-thought Chain-of-though (CoT) guides LLMs in solving a target task in a step-by-step manner in one inference, hence handling complex or ambiguous tasks better and inducing more accurate outputs. ${ }^{45}$ CoT employs the function $P_{\mathrm{CoT}}(\cdot)$ to structure the task $T$ into a series of chain-of-thought steps $\left{S_{1}, S_{2}, \ldots, S_{T}\right}$. As a result, we obtain $\left{X_{S}^{1}, \ldots, X_{S}^{T}\right}=\operatorname{LLM}\left(P_{\mathrm{CoT}}(T, X)\right)$, all produced in a single inference session. This is rather critical when we aim to elicit the thinking process of LLM and urge it in selfreflection to improve its response. For instance, we may ask LLM to draft the initial response in the first step and refine it in the second.</p>
<p>LLM-driven pipeline Clinical evidence synthesis involves a multi-step workflow as outlined in the PRISMA statement. ${ }^{21}$ It can be generally outlined as identifying and screening studies from databases, extracting characteristics and results from individual studies, and synthesizing the evidence. To enhance each step's performance, task-specific prompts can be designed for an LLM to create an LLM-based module. This results in a chain of prompts that effectively addresses a complex problem, which we call LLM-driven workflow. Specifically, this approach breaks down the entire meta-analysis process into</p>
<p>a sequence of $N$ tasks, denoted as $\mathcal{T}=\left{T_{1}, \ldots, T_{N}\right}$. In the workflow, the output from one task, $\hat{X}<em n_1="n+1">{n}$, serves as the input for the next, $\hat{X}</em>\right)\right)$. This modular decomposition improves LLM performance by dividing the workflow into more manageable segments, increases transparency, and facilitates user interaction at various stages.}=\operatorname{LLM}\left(P\left(T_{n}, \hat{X}_{n</p>
<p>Incorporating these techniques, the formulation of TrialMind for any subtask can be represented as:</p>
<p>$$
\hat{X}<em n="n">{n+1}=\operatorname{LLM}\left(P\left(T</em>\right)\right), \forall n=1, \ldots, N
$$}, X_{n}\right), R_{K}\left(X_{n</p>
<p>where $R_{K}(\cdot)$ are optional.</p>
<h1>Implementation of TrialMind</h1>
<p>All experiments were run in Python v.3.9. Detailed software versions are: pandas v2.2.2; numpy v1.26.4; scipy v1.13.0; scikit-learn v1.4.1.post1; openai v1.23.6; langchain v0.1.16; boto3 v1.34.94; pypdf v4.2.0; lxml v5.2.1 and chromadb v0.5.0 with Python v.3.9.</p>
<p>LLMs We included GPT-4 and Sonnet in our experiments. GPT-4 ${ }^{19}$ is regarded as a state-of-the-art LLM and has demonstrated strong performances in many natural language processing tasks (version: gpt-4-0125-preview). Sonnet ${ }^{46}$ is an LLM developed by Anthropic, representing a more lightweight but also very capable LLM (version: anthropic.claude-3-sonnet-20240229-v1:0 on AWS Bedrock). Both models support long context lengths ( 128 K and 200 K ), enabling them to process the full content of a typical PubMed paper in a single inference session.</p>
<p>Research question inputs TrialMind processes research question inputs using the PICO (Population, Intervention, Comparison, Outcome) framework to define the study's research question. In our experiments, the title of the target review paper served as the general description. Subsequently, we extracted the PICO elements from the paper's abstract to detail the specific aspects of the research question.</p>
<p>Literature search TrialMind is tailored to adhere to the established guidelines ${ }^{21}$ in conducting literature search and screening for clinical evidence synthesis. In the literature search stage, the key is formulating Boolean queries to retrieve a comprehensive set of candidate studies from databases. These queries, in general, are a combination of treatment, medication, and outcome terms, which can be generated by LLM using in-context learning. However, direct prompting can yield low recall queries due to the narrow range of user inputs and the LLMs' tendency to produce incorrect queries, such as generating erroneous MeSH (Medical Subject Headings) terms. ${ }^{9}$ To address these limitations, TrialMind incorporates RAG to enrich the context with knowledge sourced from PubMed, and employs CoT processing to facilitate a more exhaustive generation of relevant terms.</p>
<p>Specifically, the literature search component has two main steps: initial query generation and then query refinement. In the first step, TrialMind prompts LLM to create the initial boolean queries derived from the input PICO to retrieve a group of studies (Prompt in Extended Fig. 4). The abstracts of these studies then enrich the context for refining the initial queries, working as RAG. In addition, we used CoT to enhance the refinement by urging LLMs to conduct multi-step reasoning for self-reflection enhancement (Prompt in Extended Fig. 5). This process can be described as</p>
<p>$$
\left{\hat{X}<em S="S">{S}^{1}, \hat{X}</em>}^{2}, \hat{X<em _mathrm_CoT="\mathrm{CoT">{S}^{3}\right}=\operatorname{LLM}\left(P</em>(X)\right)\right)
$$}}\left(T_{\mathrm{LS}}, X, R_{K</p>
<p>where $X$ denotes the input PICO; $R_{K}(X)$ is the set of abstracts of the found studies; $T_{\mathrm{LS}}$ is the definition of the query generation task for literature search. For the output, the first sub-step $\hat{X}<em S="S">{S}^{1}$ indicates a complete set of terms identified in the found studies; the second $\hat{X}</em>}^{2}$ indicates the subset of $\hat{X<em S="S">{S}^{1}$ by filtering out the irrelevant; and the third $\hat{X}</em>}^{3}$ indicates the extension of $\hat{X<em S="S">{S}^{2}$ by self-reflection and adding more augmentations. In this process, LLM will produce the outputs for all three substeps in one pass, and TrialMind takes $\hat{X}</em>$ as the final queries to fetch the candidate studies.}^{3</p>
<p>Study screening TrialMind follows PRISMA to take a transparent approach for study screening. It creates a set of eligibility criteria based on the input PICO as the basis for study selection (Prompt in Extended Fig. 6), produced by</p>
<p>$$
\hat{X}<em _mathrm_EC="\mathrm{EC">{\mathrm{EC}}=\operatorname{LLM}\left(P\left(T</em>, X\right)\right)
$$}</p>
<p>where $\hat{X}<em 1="1">{\mathrm{EC}}=\left{E</em>$ is the task definition of criteria generation. Users are given the opportunity to modify these generated criteria, further adjusting to their needs.}, E_{2}, \ldots, E_{M}\right}$ is the $M$ generated eligibility criteria; $X$ is the input PICO; and $T_{\mathrm{EC}</p>
<p>Based on $\hat{X}<em i="i">{\mathrm{EC}}$, TrialMind embarks the parallel processing for the candidate studies. For $i$-th study $F</em>$, the eligibility prediction is made by LLM as (Prompt in Extended Fig. 7)</p>
<p>$$
\left{I_{i}^{1}, \ldots, I_{i}^{M}\right}=\operatorname{LLM}\left(P\left(F_{i}, X, T_{\mathrm{SC}}, \hat{X}_{\mathrm{EC}}\right)\right)
$$</p>
<p>where $T_{\mathrm{SC}}$ is the task definition of study screening; $F_{i}$ is the study $i$ 's content; $I_{i}^{m} \in$ ${-1,0,1}, \forall m=1, \ldots, M$ is the prediction of study $i$ 's eligibility to the $m$-th criterion. Here, -1 and 1 mean ineligible and eligible, 0 means uncertain, respectively. These predictions offer a convenient way for users to inspect the eligibility and select the target studies by altering the aggregation strategies. $I_{i}^{m}$ can be aggregated to offer an overall relevance of each study, such as $\hat{I}<em m="m">{i}=\sum</em>$. Users are also encouraged to extend the criteria set or block the predictions of some criteria to make customized rankings during the screening phase.} I_{i}^{m</p>
<p>Data extraction Study data extraction is an open information extraction task that requires the model to extract specific information based on user inputs and handle long inputs, such as the full content of a paper. LLMs are particularly well-suited for this task because (1) they can perform zero-shot learning via in-context learning, eliminating the need for labeled training data, and (2) the most advanced LLMs can process extremely long inputs. As such, the TrialMind framework is engineered to streamline data extraction from structured or unstructured study documents using LLMs.</p>
<p>For the specified data fields to be extracted, TrialMind prompts LLMs to locate and extract the relevant information (Prompt in Extended Fig. 8). These data fields include (1) study characteristics such as study design, sample size, study type, and treatment arms; (2) population baselines; and (3) study findings. In general, the extraction process can be described as</p>
<p>$$
\left{\hat{X}<em _mathrm_EX="\mathrm{EX">{\mathrm{EX}}^{1}, \ldots, \hat{X}</em>\right)\right)
$$}}^{K}\right}=\operatorname{LLM}\left(P\left(F, C, T_{\mathrm{EX}</p>
<p>where $F$ represents the full content of a study; $T_{\mathrm{EX}}$ defines the task of data extraction; and $C=\left{C_{1}, C_{2}, \ldots, C_{K}\right}$ comprises the series of data fields targeted for extraction. $C_{k}$ is the user input natural language description of the target field, e.g., "the number of participants in the study". The input content $F$ is segmented into distinct chunks, each marked by a unique identifier. The outputs, denoted as $\hat{X}_{\mathrm{EX}}^{k}=\left{V^{k}, B^{k}\right}$, include the extracted values $V$ and the indices $B$ that link back to their respective locations in the source content. Hence, it is convenient to check and correct mistakes made in the extraction by sourcing the origin. The extraction can also be easily scaled by making paralleled calls of LLMs.</p>
<p>Result extraction Our analysis indicates that data extraction generally performs well for study design and population-related fields; however, extracting study results presents challenges. Errors frequently arise due to the diverse presentation of results within studies and subtle discrepancies between the target population and outcomes versus those reported. For instance, the target outcome is the risk ratios (treatment versus control) regarding the incidence of adverse events (AEs), while the study reports AEs among many groups separately. Or, the target outcome is the incidence of severe AEs, which implicitly correspond to those with grade III and more, while the study reports all grade AEs. To overcome these challenges, we have refined our data extraction process to create a specialized result extraction pipeline that improves clinical evidence synthesis. This enhanced pipeline consists of three crucial steps: (1) identifying the relevant content within the</p>
<p>study (Prompt in Extended Fig. 9), (2) extracting and logically processing this content to obtain numerical values (Prompt in Extended Fig. 10), and (3) converting these values into a standardized tabular format (Prompt in Extended Fig. 11).</p>
<p>Steps (1) and (2) are conducted in one pass using CoT reasoning as</p>
<p>$$
\left{\hat{X}<em _mathrm_RE="\mathrm{RE">{\mathrm{RE}, S}^{1}, \hat{X}</em>\right)\right)
$$}, S}^{2}\right}=\operatorname{LLM}\left(P_{\mathrm{CoT}}\left(X, O, F, T_{\mathrm{RE}</p>
<p>where $O$ is the natural language description of the clinical endpoint of interest and $T_{\mathrm{RE}}$ is the task definition of result extraction. In the outputs, $\hat{X}<em _mathrm_RE="\mathrm{RE">{\mathrm{RE}, S}^{1}$ represents the raw content captured from the input content $F$ regarding the clinical outcomes; $\hat{X}</em>$ to the standard tabular format.}, S}^{2}$ represents the elicited numerical values from the raw content, such as the number of patients in the group, the ratio of patients encountering overall response, etc. In step (3), TrialMind writes Python code to make the final calculation to convert $\hat{X}_{\mathrm{RE}, S}^{2</p>
<p>$$
\hat{X}<em _mathrm_PY="\mathrm{PY">{\mathrm{RE}}=\operatorname{exec}\left(\operatorname{LLM}\left(P\left(X, O, T</em>}}, \hat{X<em _mathrm_RE="\mathrm{RE">{\mathrm{RE}, S}^{2}\right)\right), \hat{X}</em>\right)
$$}, S}^{2</p>
<p>In this process, TrialMind adheres to the instructions in $T_{\mathrm{PY}}$ to generate code for data processing. This code is then executed, using $\hat{X}<em _mathrm_RE="\mathrm{RE">{\mathrm{RE}, S}^{2}$ as input, to produce the standardized result $\hat{X}</em>$. Additionally, it ensures that the calculation process remains transparent, enhancing the reliability and reproducibility of the synthesized evidence.}}$. An example code snippet made to do this transformation is shown in Extended Fig. 3. This approach facilitates verification of the extracted results by allowing for easy backtracking to $\hat{X}_{\mathrm{RE}, S}^{1</p>
<h1>Experimental setup</h1>
<p>Literature search and screening In our literature search experiments, we assessed performance using the overall Recall, aiming to evaluate the effectiveness of different methods in identifying all relevant studies from the PubMed database using APIs. ${ }^{47}$ For literature screening, we measured efficacy using Recall@20 and Recall@50, which gauge how well the methods can prioritize target studies at the top of the list, thereby facilitating quicker decisions about which studies to include in evidence synthesis. We constructed the ranking candidate set for each review paper by initially retrieving studies through TrialMind, then refining this list by ranking the relevance of these studies to the target review's PICO elements using OpenAI embeddings. The top 2,000 relevant studies were kept. We then ensured all target papers were included in the candidate set to maintain the integrity of our ground-truth data. The final candidate set was then deduplicated to be ranked by the selected methods.</p>
<p>In the criteria analysis experiment, we utilized Recall@200 to assess the impact of each criterion. This was done by first computing the relevance prediction using all eligibility predictions and then recalculating it without the eligibility prediction for the specific criterion in question. The difference in Recall@200 between these two relevance predictions, denoted as $\Delta$ Recall, indicates the criterion's effect. A larger $\Delta$ Recall suggests that the criterion plays a more significant role in influencing the ranking results.</p>
<p>Data extraction and result extraction To evaluate performance, we measured the accuracy of the values extracted by TrialMind against the groundtruth. We used the study characteristic tables from the review papers as our test set. Each table's column names served as input field descriptions for TrialMind. We manually downloaded the full content for the studies listed in the characteristic table. To verify the accuracy of the extracted values, we enlisted three annotators who manually compared them against the data reported in the original tables.</p>
<p>We also measured the performance of result extraction using accuracy. The annotators were asked to carefully read the extracted results and compare them to the results reported in the original review paper. For the error analysis of TrialMind, the annotators were asked to check the sources to categorize the errors for one of the reasons: inaccurate, extraction failure, unavailable data, or hallucination. We designed a vanilla</p>
<p>prompting strategy for GPT-4 and Sonnet models to set the baselines for the result extraction. Specifically, the prompt was kept minimal, as "Based on the {paper}, tell me the {outcome} from the input study for the population ${$ cohort $}$ ", where ${$ paper $}$ is the placeholder for the paper's content; {outcome} is the for the target endpoint; {cohort} is the for the target population's descriptions, including conditions and characteristics. The responses from these prompts were typically in free text, from which annotators manually extracted result values to evaluate the baselines' performance.</p>
<p>Evidence synthesis In evidence synthesis, we processed the input data using R and the 'meta' package to make the forest plots and the pooled results based on the standardized result values. This is for both TrialMind and the baselines. Nonetheless, for the baseline, the annotators also need to manually extract the result values and standardize the values to make them ready for meta-analysis, which forms the GPT-4+Human baseline in the experiments.</p>
<p>We engaged two groups of annotators for our evaluation: (1) three computer scientists with expertise in AI applications for medicine, and (2) five medical doctors to assess the generated forest plots. Each annotator was asked to evaluate five review studies. For each review, we randomly presented forest plots generated by both the baseline and TrialMind. The annotators were required to determine how closely each generated plot aligned with a reference forest plot taken from the target review paper. Additionally, they were asked to judge which method, the baseline or TrialMind, produced better results in a win/lose assessment. Extended Fig. 1 demonstrates the user interface for this study, which was created with Google Forms.</p>
<p>Research question: Microwave ablation (MWA) compared with radiofrequency ablation (RFA) for the treatment of liver cancer: a systematic review and meta-analysis
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Extended Fig. 1: The study design compares the synthesized clinical evidence from the baseline and TrialMind via human evaluation.</p>            </div>
        </div>

    </div>
</body>
</html>
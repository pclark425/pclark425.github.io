<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8981 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8981</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8981</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-7771402</p>
                <p><strong>Paper Title:</strong> Abstract Meaning Representation for Sembanking</p>
                <p><strong>Paper Abstract:</strong> We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8981.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8981.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstract Meaning Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sentence-level semantic representation consisting of rooted, directed, edge-labeled, leaf-labeled graphs that abstract away from syntactic idiosyncrasies and use PropBank framesets and a fixed relation inventory to capture sentence meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>AMR graph (rooted directed edge-labeled graph; PENMAN serializations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR represents whole-sentence meaning as a rooted, directed, edge-labeled, leaf-labeled graph. Nodes denote variables bound to concepts (English words, PropBank framesets, or special keywords); edges are semantic roles and relations (≈100 relations, plus inverses). Re-entrancy/variable reuse encodes co-reference; every relation has a corresponding reification concept. For human readability AMR is serialized in PENMAN notation and for evaluation it can be viewed as a conjunction of logical triples.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Sentence-level semantic graphs (rooted directed labeled graphs / AMRs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No single prescribed conversion from string to AMR (annotation is intentionally derivation-agnostic). For representation-to-text/serialization the paper uses PENMAN notation (nested parenthesized variable/concept/role format) and for machine processing AMRs are traversed as graphs or converted into logical triples (conjunctions) for evaluation (smatch). Re-entrancy is represented by variable reuse or multiple parents in graph form.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Statistical natural language understanding (semantic parsing), natural language generation, semantics-based machine translation (Chinese→AMR→English), paraphrase canonicalization (disjunctive AMR), and as general training data for semantic processors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Annotation inter-annotator agreement (smatch): average annotator vs consensus IAA = 0.83 (newswire) and 0.79 (web); annotator vs annotator IAA = 0.71 (382 web sentences). Annotation speed: full-sentence AMR construction 7–10 minutes; post-editing 1–3 minutes. (No downstream ML metrics for MT/NLG reported in this paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to other formalisms: uses PropBank framesets vs GMB/ST use DRT or NomBank etc.; AMR is agnostic about derivations unlike ST/GMB which annotate compositional derivations; AMR is sentence-local (not cross-sentence like some GMB annotations); AMR has ~80 entity types vs GMB's 7; AMR and UNL are manually annotated while GMB/ST may be produced automatically and corrected. The paper positions AMR as more syntax-abstract and human-readable than some alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Readable PENMAN serializations for humans; graph structure is convenient for programmatic traversal; abstracts away many syntactic variants (canonicalizes different surface forms to same AMR); heavy reuse of PropBank frames gives consistent role slots; supports re-entrancy to represent co-reference; enables a single whole-sentence semantic task to drive multiple downstream tasks; smatch provides a triples-based evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Deliberately omits some phenomena: no inflectional morphology (tense/number) and articles, no universal quantifiers ("all" is modeled as a modifier), and does not distinguish real vs hypothetical/future/imagined events. AMR is English-biased (not an Interlingua). AMR does not perform entity normalization (e.g., different surface forms of same named entity). Some semantic distinctions and specific frames are not covered, leading to inconsistent patterns for certain nouns/phrases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Cases where appropriate PropBank frames or relations are lacking (authors fallback to :prep-X or invent new frames); nominalization mismatches (e.g., 'history professor' vs 'history teacher' handled differently because appropriate frames differ); limits in representing quantification, temporal nuance, and morphology; complexity when relation modification requires reification. These limitations can cause representational inconsistency or loss of targeted information.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8981.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8981.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PENMAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PENMAN notation / PENMAN serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-readable parenthesized serialization format for AMR graphs that uses variable names, slash-separated concepts, and colon-prefixed role labels to express graph structure linearly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PENMAN linearization / serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Linearizes AMR graphs as nested parenthesized expressions: (var / concept :role child ...). Variable reuse in the linearization expresses re-entrancy (multiple parents). The format is equivalent to other views (graph, logical triples) but optimized for human reading and editing.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (rooted directed labeled graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Convert graph nodes and labeled edges into nested parenthesis with variables and concept labels; re-entrancy expressed by re-using variable identifiers instead of duplicating subtrees.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Manual annotation and editing; source/target format for creating training data for semantic parsers and text generation systems; potentially used as textual input/target for language models after further processing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No quantitative ML performance metrics reported specifically for PENMAN; annotation throughput reported (7–10 min to create an AMR, 1–3 min to postedit) when annotators used the AMR Editor which displays PENMAN.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>PENMAN is explicitly preferred for human readability over raw graph adjacency lists or logical triple listings; the paper uses PENMAN for human-facing tasks and pure graph/triple forms for evaluation/computation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Human-readable and writable; supports quick annotation and post-editing; variable reuse makes re-entrancy explicit in a readable way; supported by the AMR Editor and documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>As a linear sequence it can be long and not ideal as-is for training sequence models without additional preprocessing or linearization choices; the paper notes AMR is agnostic about derivation which means PENMAN is not a canonical derivation sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly enumerated in the paper; implicit concerns are verbosity for long graphs and potential variability in linearization choices if used naively as model input.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8981.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8981.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOGIC / triple view</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOGIC format (conjunction of logical triples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation of an AMR as an existentially-scoped conjunction of logical triples (instance and relation predicates) that makes the graph amenable to logic-style comparison and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Conjunctive logical triples (LOGIC format)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Converts graph nodes and edges to atomic predicates like instance(var, concept) and relation(var1, var2). The full AMR becomes a conjunction of such triples under existential quantification, providing a semantics-centric, alignment-free representation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Traverse AMR graph to emit triples for each node (instance) and each labeled edge (relation); optionally wrap in existential quantifiers for variable bindings. This triple set is the input to the smatch comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Evaluation of AMR annotation and parser output (via smatch); could be used as an intermediate representation for semantic comparisons or logic-based downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as the basis for smatch precision, recall, and F-score. The paper reports IAA smatch scores (0.83, 0.79, 0.71) computed on triple overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Contrasted with string-based evaluation metrics (e.g., BLEU); the triple view enables meaning-level comparison rather than surface string comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Removes reliance on English string indices and alignments; directly captures semantic overlap; suitable for graph-graph comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Variable names differ between AMRs, so an NP-hard search (heuristic search) is required to find the best variable mapping for comparison; conversion to triples loses any explicit alignment information to surface tokens which might be useful to some models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Mapping/variable alignment search can be computationally expensive or produce suboptimal matches; triples-only view may under-specify alignment details needed for some training/regression tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8981.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8981.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Smatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Smatch (AMR evaluation metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An F-score-based metric that measures semantic overlap between two AMRs by converting them to triples and searching for the variable mapping that maximizes triple overlap, yielding precision, recall, and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smatch: An accuracy metric for abstract meaning representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triples-based matching metric (Smatch)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Converts AMRs to sets of logical triples and performs a brief search over variable mappings to maximize F1 overlap between the two triple sets; reports precision, recall, and F-score for the semantic overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph→triples conversion followed by an optimization/search over variable renamings to compute maximal triple overlap; outputs P/R/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Evaluation of gold vs predicted AMRs (inter-annotator agreement, parser accuracy), used to report annotation quality and to measure parser performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Smatch scores cited for IAA: 0.83 (mean annotator vs consensus on newswire), 0.79 (web), and 0.71 (annotator vs annotator on 382 web sentences). Smatch outputs precision, recall, and F-score (unitless proportions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Presented as analogous to BLEU for strings but operating on meaning representations; unlike BLEU it is graph/triple based and does not reference source strings or token indices.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Direct semantic comparison unaffected by surface string variation or alignment choices; enables consistent reporting of parser/annotation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires search for variable mapping (additional computation), and collapsing to triples may miss structural nuances or partial credit beyond triple overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly enumerated, but variable-mapping ambiguity and the potential for degenerate mappings that inflate scores if heuristics are poor are implied risks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8981.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8981.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Disjunctive AMR (DAMR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disjunctive Abstract Meaning Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AMR extension/formulation to pack multiple paraphrase-equivalent graph variants into a single disjunctive AMR using *OR* nodes to represent alternative subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Disjunctive AMR guideline</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Disjunctive AMR (packed/paraphrase AMR with *OR* nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represents paraphrase alternatives within a single AMR by introducing *OR* concepts that have :op1, :op2 ... operands and nested :OR nodes, enabling a compact, highly-packed encoding of multiple semantically-equivalent sentence variants.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs extended with disjunctive nodes (packed AMRs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Construct an AMR where alternatives are expressed as special *OR* concepts with operand edges (:opN) and nested OR constructs to capture multiple paraphrase paths from paraphrase networks.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Canonicalization across paraphrase networks, potential training data compacting for generation/paraphrase-aware NLG, and experiments tying AMR to paraphrase resources.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No quantitative metrics reported in this paper for DAMR; guideline referenced and planned experiments discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Described as a planned extension to avoid creating separate AMRs for paraphrase variants; contrasts with the usual one-AMR-per-sentence approach.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>More compact representation of multiple equivalent meanings; could reduce annotation redundancy and improve generalization across paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increased complexity for annotation, storage, and downstream processing; standard tooling and evaluation metrics may not directly support disjunctive structures.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not evaluated here; potential practical issues include difficulty in training models on disjunctive structures and ambiguity in downstream interpretation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8981.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8981.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HRG (Hyperedge Replacement Grammars)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parsing graphs with hyperedge replacement grammars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph grammar formalism (hyperedge replacement grammars) used to parse and generate graph structures (such as AMRs) from strings and vice versa, applied in semantics-based MT and graph parsing research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Parsing graphs with hyperedge replacement grammars.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Hyperedge Replacement Grammar-based graph-string translation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses probabilistic hyperedge replacement grammars to derive/parse graph structures: hyperedges replace nonterminal graph fragments allowing compositional generation and parsing of graphs from strings and vice versa.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs / AMR-like semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Learn HRG productions that map string fragments (or syntactic derivations) to graph hyperedges and apply HRG parsing/generation algorithms to convert between graphs and surface strings (used in MT pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Semantics-based machine translation (Chinese↔AMR↔English), graph parsing/generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No HRG quantitative performance numbers reported in this paper; referenced papers (Jones et al., Chiang et al.) contain experimental results not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Mentioned as one probabilistic approach to map between strings and graphs; alternative approaches (e.g., tree/CCG-based compositional semantics or automata-on-DAGs) also discussed in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Expressive grammar formalism capable of modeling complex graph structures and enabling probabilistic parsing/generation.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Nontrivial learning and inference complexity; details and empirical tradeoffs are left to cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specified in this paper; complexity and scalability are potential practical issues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8981.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8981.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAGGER / Probabilistic transducers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAGGER: A toolkit for automata on directed acyclic graphs / Towards probabilistic acceptors and transducers for feature structures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that treat graphs (DAGs / feature-structure graphs) with automata or probabilistic transducers to accept/generate graph languages or map graphs to strings, facilitating string↔graph conversion operations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DAGGER: A toolkit for automata on directed acyclic graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Automata / probabilistic transducer-based graph↔string methods (DAGGER; probabilistic acceptors/transducers)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use automata operating over DAGs (DAGGER toolkit) and probabilistic acceptors/transducers for feature structures to model and probabilistically transform graph-structured data, enabling parsing/generation workflows for graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Directed acyclic graphs and feature-structure graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Model graph languages with automata/acceptors and define transducers that probabilistically map graph structures to strings or other graphs; toolkit provides implementations and algorithms for these operations.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph parsing/generation, semantic parsing, machine translation, and other string-graph mapping tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No concrete numeric results provided in this paper; referenced works are recommended for empirical detail.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Positioned as alternative formal approaches (automata/transducer-based) to grammar-based approaches (HRG) or purely statistical mapping methods.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Offers a formal, potentially probabilistic framework for graph operations; toolkits support experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Paper gives no empirical evaluation here; integrating with AMR annotation pipelines likely requires adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Smatch: An accuracy metric for abstract meaning representations. <em>(Rating: 2)</em></li>
                <li>Parsing graphs with hyperedge replacement grammars. <em>(Rating: 2)</em></li>
                <li>Semantics-based machine translation with hyperedge replacement grammars. <em>(Rating: 2)</em></li>
                <li>DAGGER: A toolkit for automata on directed acyclic graphs. <em>(Rating: 2)</em></li>
                <li>Towards probabilistic acceptors and transducers for feature structures. <em>(Rating: 2)</em></li>
                <li>Disjunctive AMR guideline <em>(Rating: 2)</em></li>
                <li>Hyter: Meaning-equivalent semantics for translation evaluation. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8981",
    "paper_id": "paper-7771402",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "AMR",
            "name_full": "Abstract Meaning Representation",
            "brief_description": "A sentence-level semantic representation consisting of rooted, directed, edge-labeled, leaf-labeled graphs that abstract away from syntactic idiosyncrasies and use PropBank framesets and a fixed relation inventory to capture sentence meaning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "AMR graph (rooted directed edge-labeled graph; PENMAN serializations)",
            "representation_description": "AMR represents whole-sentence meaning as a rooted, directed, edge-labeled, leaf-labeled graph. Nodes denote variables bound to concepts (English words, PropBank framesets, or special keywords); edges are semantic roles and relations (≈100 relations, plus inverses). Re-entrancy/variable reuse encodes co-reference; every relation has a corresponding reification concept. For human readability AMR is serialized in PENMAN notation and for evaluation it can be viewed as a conjunction of logical triples.",
            "graph_type": "Sentence-level semantic graphs (rooted directed labeled graphs / AMRs)",
            "conversion_method": "No single prescribed conversion from string to AMR (annotation is intentionally derivation-agnostic). For representation-to-text/serialization the paper uses PENMAN notation (nested parenthesized variable/concept/role format) and for machine processing AMRs are traversed as graphs or converted into logical triples (conjunctions) for evaluation (smatch). Re-entrancy is represented by variable reuse or multiple parents in graph form.",
            "downstream_task": "Statistical natural language understanding (semantic parsing), natural language generation, semantics-based machine translation (Chinese→AMR→English), paraphrase canonicalization (disjunctive AMR), and as general training data for semantic processors.",
            "performance_metrics": "Annotation inter-annotator agreement (smatch): average annotator vs consensus IAA = 0.83 (newswire) and 0.79 (web); annotator vs annotator IAA = 0.71 (382 web sentences). Annotation speed: full-sentence AMR construction 7–10 minutes; post-editing 1–3 minutes. (No downstream ML metrics for MT/NLG reported in this paper.)",
            "comparison_to_others": "Compared to other formalisms: uses PropBank framesets vs GMB/ST use DRT or NomBank etc.; AMR is agnostic about derivations unlike ST/GMB which annotate compositional derivations; AMR is sentence-local (not cross-sentence like some GMB annotations); AMR has ~80 entity types vs GMB's 7; AMR and UNL are manually annotated while GMB/ST may be produced automatically and corrected. The paper positions AMR as more syntax-abstract and human-readable than some alternatives.",
            "advantages": "Readable PENMAN serializations for humans; graph structure is convenient for programmatic traversal; abstracts away many syntactic variants (canonicalizes different surface forms to same AMR); heavy reuse of PropBank frames gives consistent role slots; supports re-entrancy to represent co-reference; enables a single whole-sentence semantic task to drive multiple downstream tasks; smatch provides a triples-based evaluation metric.",
            "disadvantages": "Deliberately omits some phenomena: no inflectional morphology (tense/number) and articles, no universal quantifiers (\"all\" is modeled as a modifier), and does not distinguish real vs hypothetical/future/imagined events. AMR is English-biased (not an Interlingua). AMR does not perform entity normalization (e.g., different surface forms of same named entity). Some semantic distinctions and specific frames are not covered, leading to inconsistent patterns for certain nouns/phrases.",
            "failure_cases": "Cases where appropriate PropBank frames or relations are lacking (authors fallback to :prep-X or invent new frames); nominalization mismatches (e.g., 'history professor' vs 'history teacher' handled differently because appropriate frames differ); limits in representing quantification, temporal nuance, and morphology; complexity when relation modification requires reification. These limitations can cause representational inconsistency or loss of targeted information.",
            "uuid": "e8981.0"
        },
        {
            "name_short": "PENMAN",
            "name_full": "PENMAN notation / PENMAN serialization",
            "brief_description": "A human-readable parenthesized serialization format for AMR graphs that uses variable names, slash-separated concepts, and colon-prefixed role labels to express graph structure linearly.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "PENMAN linearization / serialization",
            "representation_description": "Linearizes AMR graphs as nested parenthesized expressions: (var / concept :role child ...). Variable reuse in the linearization expresses re-entrancy (multiple parents). The format is equivalent to other views (graph, logical triples) but optimized for human reading and editing.",
            "graph_type": "AMR graphs (rooted directed labeled graphs)",
            "conversion_method": "Convert graph nodes and labeled edges into nested parenthesis with variables and concept labels; re-entrancy expressed by re-using variable identifiers instead of duplicating subtrees.",
            "downstream_task": "Manual annotation and editing; source/target format for creating training data for semantic parsers and text generation systems; potentially used as textual input/target for language models after further processing.",
            "performance_metrics": "No quantitative ML performance metrics reported specifically for PENMAN; annotation throughput reported (7–10 min to create an AMR, 1–3 min to postedit) when annotators used the AMR Editor which displays PENMAN.",
            "comparison_to_others": "PENMAN is explicitly preferred for human readability over raw graph adjacency lists or logical triple listings; the paper uses PENMAN for human-facing tasks and pure graph/triple forms for evaluation/computation.",
            "advantages": "Human-readable and writable; supports quick annotation and post-editing; variable reuse makes re-entrancy explicit in a readable way; supported by the AMR Editor and documentation.",
            "disadvantages": "As a linear sequence it can be long and not ideal as-is for training sequence models without additional preprocessing or linearization choices; the paper notes AMR is agnostic about derivation which means PENMAN is not a canonical derivation sequence.",
            "failure_cases": "Not explicitly enumerated in the paper; implicit concerns are verbosity for long graphs and potential variability in linearization choices if used naively as model input.",
            "uuid": "e8981.1"
        },
        {
            "name_short": "LOGIC / triple view",
            "name_full": "LOGIC format (conjunction of logical triples)",
            "brief_description": "A representation of an AMR as an existentially-scoped conjunction of logical triples (instance and relation predicates) that makes the graph amenable to logic-style comparison and evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Conjunctive logical triples (LOGIC format)",
            "representation_description": "Converts graph nodes and edges to atomic predicates like instance(var, concept) and relation(var1, var2). The full AMR becomes a conjunction of such triples under existential quantification, providing a semantics-centric, alignment-free representation.",
            "graph_type": "AMR graphs",
            "conversion_method": "Traverse AMR graph to emit triples for each node (instance) and each labeled edge (relation); optionally wrap in existential quantifiers for variable bindings. This triple set is the input to the smatch comparison.",
            "downstream_task": "Evaluation of AMR annotation and parser output (via smatch); could be used as an intermediate representation for semantic comparisons or logic-based downstream tasks.",
            "performance_metrics": "Used as the basis for smatch precision, recall, and F-score. The paper reports IAA smatch scores (0.83, 0.79, 0.71) computed on triple overlap.",
            "comparison_to_others": "Contrasted with string-based evaluation metrics (e.g., BLEU); the triple view enables meaning-level comparison rather than surface string comparison.",
            "advantages": "Removes reliance on English string indices and alignments; directly captures semantic overlap; suitable for graph-graph comparison.",
            "disadvantages": "Variable names differ between AMRs, so an NP-hard search (heuristic search) is required to find the best variable mapping for comparison; conversion to triples loses any explicit alignment information to surface tokens which might be useful to some models.",
            "failure_cases": "Mapping/variable alignment search can be computationally expensive or produce suboptimal matches; triples-only view may under-specify alignment details needed for some training/regression tasks.",
            "uuid": "e8981.2"
        },
        {
            "name_short": "Smatch",
            "name_full": "Smatch (AMR evaluation metric)",
            "brief_description": "An F-score-based metric that measures semantic overlap between two AMRs by converting them to triples and searching for the variable mapping that maximizes triple overlap, yielding precision, recall, and F1.",
            "citation_title": "Smatch: An accuracy metric for abstract meaning representations.",
            "mention_or_use": "use",
            "representation_name": "Triples-based matching metric (Smatch)",
            "representation_description": "Converts AMRs to sets of logical triples and performs a brief search over variable mappings to maximize F1 overlap between the two triple sets; reports precision, recall, and F-score for the semantic overlap.",
            "graph_type": "AMR graphs",
            "conversion_method": "Graph→triples conversion followed by an optimization/search over variable renamings to compute maximal triple overlap; outputs P/R/F1.",
            "downstream_task": "Evaluation of gold vs predicted AMRs (inter-annotator agreement, parser accuracy), used to report annotation quality and to measure parser performance.",
            "performance_metrics": "Smatch scores cited for IAA: 0.83 (mean annotator vs consensus on newswire), 0.79 (web), and 0.71 (annotator vs annotator on 382 web sentences). Smatch outputs precision, recall, and F-score (unitless proportions).",
            "comparison_to_others": "Presented as analogous to BLEU for strings but operating on meaning representations; unlike BLEU it is graph/triple based and does not reference source strings or token indices.",
            "advantages": "Direct semantic comparison unaffected by surface string variation or alignment choices; enables consistent reporting of parser/annotation quality.",
            "disadvantages": "Requires search for variable mapping (additional computation), and collapsing to triples may miss structural nuances or partial credit beyond triple overlap.",
            "failure_cases": "Not explicitly enumerated, but variable-mapping ambiguity and the potential for degenerate mappings that inflate scores if heuristics are poor are implied risks.",
            "uuid": "e8981.3"
        },
        {
            "name_short": "Disjunctive AMR (DAMR)",
            "name_full": "Disjunctive Abstract Meaning Representation",
            "brief_description": "An AMR extension/formulation to pack multiple paraphrase-equivalent graph variants into a single disjunctive AMR using *OR* nodes to represent alternative subgraphs.",
            "citation_title": "Disjunctive AMR guideline",
            "mention_or_use": "mention",
            "representation_name": "Disjunctive AMR (packed/paraphrase AMR with *OR* nodes)",
            "representation_description": "Represents paraphrase alternatives within a single AMR by introducing *OR* concepts that have :op1, :op2 ... operands and nested :OR nodes, enabling a compact, highly-packed encoding of multiple semantically-equivalent sentence variants.",
            "graph_type": "AMR graphs extended with disjunctive nodes (packed AMRs)",
            "conversion_method": "Construct an AMR where alternatives are expressed as special *OR* concepts with operand edges (:opN) and nested OR constructs to capture multiple paraphrase paths from paraphrase networks.",
            "downstream_task": "Canonicalization across paraphrase networks, potential training data compacting for generation/paraphrase-aware NLG, and experiments tying AMR to paraphrase resources.",
            "performance_metrics": "No quantitative metrics reported in this paper for DAMR; guideline referenced and planned experiments discussed.",
            "comparison_to_others": "Described as a planned extension to avoid creating separate AMRs for paraphrase variants; contrasts with the usual one-AMR-per-sentence approach.",
            "advantages": "More compact representation of multiple equivalent meanings; could reduce annotation redundancy and improve generalization across paraphrases.",
            "disadvantages": "Increased complexity for annotation, storage, and downstream processing; standard tooling and evaluation metrics may not directly support disjunctive structures.",
            "failure_cases": "Not evaluated here; potential practical issues include difficulty in training models on disjunctive structures and ambiguity in downstream interpretation.",
            "uuid": "e8981.4"
        },
        {
            "name_short": "HRG (Hyperedge Replacement Grammars)",
            "name_full": "Parsing graphs with hyperedge replacement grammars",
            "brief_description": "A graph grammar formalism (hyperedge replacement grammars) used to parse and generate graph structures (such as AMRs) from strings and vice versa, applied in semantics-based MT and graph parsing research.",
            "citation_title": "Parsing graphs with hyperedge replacement grammars.",
            "mention_or_use": "mention",
            "representation_name": "Hyperedge Replacement Grammar-based graph-string translation",
            "representation_description": "Uses probabilistic hyperedge replacement grammars to derive/parse graph structures: hyperedges replace nonterminal graph fragments allowing compositional generation and parsing of graphs from strings and vice versa.",
            "graph_type": "General graphs / AMR-like semantic graphs",
            "conversion_method": "Learn HRG productions that map string fragments (or syntactic derivations) to graph hyperedges and apply HRG parsing/generation algorithms to convert between graphs and surface strings (used in MT pipelines).",
            "downstream_task": "Semantics-based machine translation (Chinese↔AMR↔English), graph parsing/generation.",
            "performance_metrics": "No HRG quantitative performance numbers reported in this paper; referenced papers (Jones et al., Chiang et al.) contain experimental results not reproduced here.",
            "comparison_to_others": "Mentioned as one probabilistic approach to map between strings and graphs; alternative approaches (e.g., tree/CCG-based compositional semantics or automata-on-DAGs) also discussed in related work.",
            "advantages": "Expressive grammar formalism capable of modeling complex graph structures and enabling probabilistic parsing/generation.",
            "disadvantages": "Nontrivial learning and inference complexity; details and empirical tradeoffs are left to cited work.",
            "failure_cases": "Not specified in this paper; complexity and scalability are potential practical issues.",
            "uuid": "e8981.5"
        },
        {
            "name_short": "DAGGER / Probabilistic transducers",
            "name_full": "DAGGER: A toolkit for automata on directed acyclic graphs / Towards probabilistic acceptors and transducers for feature structures",
            "brief_description": "Approaches that treat graphs (DAGs / feature-structure graphs) with automata or probabilistic transducers to accept/generate graph languages or map graphs to strings, facilitating string↔graph conversion operations.",
            "citation_title": "DAGGER: A toolkit for automata on directed acyclic graphs.",
            "mention_or_use": "mention",
            "representation_name": "Automata / probabilistic transducer-based graph↔string methods (DAGGER; probabilistic acceptors/transducers)",
            "representation_description": "Use automata operating over DAGs (DAGGER toolkit) and probabilistic acceptors/transducers for feature structures to model and probabilistically transform graph-structured data, enabling parsing/generation workflows for graphs.",
            "graph_type": "Directed acyclic graphs and feature-structure graphs",
            "conversion_method": "Model graph languages with automata/acceptors and define transducers that probabilistically map graph structures to strings or other graphs; toolkit provides implementations and algorithms for these operations.",
            "downstream_task": "Graph parsing/generation, semantic parsing, machine translation, and other string-graph mapping tasks.",
            "performance_metrics": "No concrete numeric results provided in this paper; referenced works are recommended for empirical detail.",
            "comparison_to_others": "Positioned as alternative formal approaches (automata/transducer-based) to grammar-based approaches (HRG) or purely statistical mapping methods.",
            "advantages": "Offers a formal, potentially probabilistic framework for graph operations; toolkits support experimentation.",
            "disadvantages": "Paper gives no empirical evaluation here; integrating with AMR annotation pipelines likely requires adaptation.",
            "failure_cases": "Not described in this paper.",
            "uuid": "e8981.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Smatch: An accuracy metric for abstract meaning representations.",
            "rating": 2
        },
        {
            "paper_title": "Parsing graphs with hyperedge replacement grammars.",
            "rating": 2
        },
        {
            "paper_title": "Semantics-based machine translation with hyperedge replacement grammars.",
            "rating": 2
        },
        {
            "paper_title": "DAGGER: A toolkit for automata on directed acyclic graphs.",
            "rating": 2
        },
        {
            "paper_title": "Towards probabilistic acceptors and transducers for feature structures.",
            "rating": 2
        },
        {
            "paper_title": "Disjunctive AMR guideline",
            "rating": 2
        },
        {
            "paper_title": "Hyter: Meaning-equivalent semantics for translation evaluation.",
            "rating": 1
        }
    ],
    "cost": 0.0170635,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Abstract Meaning Representation for Sembanking
Association for Computational LinguisticsCopyright Association for Computational LinguisticsAugust 8-9. 2013. 2013</p>
<p>Laura Banarescu lbanarescu@sdl.com 
Linguistics Dept
School of Informatics Univ. Edinburgh
Linguistics Dept
SDL
Univ. Colorado
ISI USC
LDC
ISI USC
ISI USC
Univ. Colorado
LTI CMU</p>
<p>Claire Bonial claire.bonial@colorado.edu 
Linguistics Dept
School of Informatics Univ. Edinburgh
Linguistics Dept
SDL
Univ. Colorado
ISI USC
LDC
ISI USC
ISI USC
Univ. Colorado
LTI CMU</p>
<p>Shu Cai shucai@isi.edu 
Linguistics Dept
School of Informatics Univ. Edinburgh
Linguistics Dept
SDL
Univ. Colorado
ISI USC
LDC
ISI USC
ISI USC
Univ. Colorado
LTI CMU</p>
<p>Madalina Georgescu mgeorgescu@sdl.com 
Linguistics Dept
School of Informatics Univ. Edinburgh
Linguistics Dept
SDL
Univ. Colorado
ISI USC
LDC
ISI USC
ISI USC
Univ. Colorado
LTI CMU</p>
<p>Sdl 
Linguistics Dept
School of Informatics Univ. Edinburgh
Linguistics Dept
SDL
Univ. Colorado
ISI USC
LDC
ISI USC
ISI USC
Univ. Colorado
LTI CMU</p>
<p>Kira Griffitt 
Linguistics Dept
School of Informatics Univ. Edinburgh
Linguistics Dept
SDL
Univ. Colorado
ISI USC
LDC
ISI USC
ISI USC
Univ. Colorado
LTI CMU</p>
<p>Ulf Hermjakob 
Linguistics Dept
School of Informatics Univ. Edinburgh
Linguistics Dept
SDL
Univ. Colorado
ISI USC
LDC
ISI USC
ISI USC
Univ. Colorado
LTI CMU</p>
<p>Kevin Knight knight@isi.edu 
Linguistics Dept
School of Informatics Univ. Edinburgh
Linguistics Dept
SDL
Univ. Colorado
ISI USC
LDC
ISI USC
ISI USC
Univ. Colorado
LTI CMU</p>
<p>Philipp Koehn pkoehn@inf.ed.ac.uk 
Linguistics Dept
School of Informatics Univ. Edinburgh
Linguistics Dept
SDL
Univ. Colorado
ISI USC
LDC
ISI USC
ISI USC
Univ. Colorado
LTI CMU</p>
<p>Martha Palmer martha.palmer@colorado.edu 
Linguistics Dept
School of Informatics Univ. Edinburgh
Linguistics Dept
SDL
Univ. Colorado
ISI USC
LDC
ISI USC
ISI USC
Univ. Colorado
LTI CMU</p>
<p>Nathan Schneider nschneid@cs.cmu.edu 
Linguistics Dept
School of Informatics Univ. Edinburgh
Linguistics Dept
SDL
Univ. Colorado
ISI USC
LDC
ISI USC
ISI USC
Univ. Colorado
LTI CMU</p>
<p>Abstract Meaning Representation for Sembanking</p>
<p>Proceedings of the 7th Linguistic Annotation Workshop &amp; Interoperability with Discourse
the 7th Linguistic Annotation Workshop &amp; Interoperability with DiscourseSofia, BulgariaAssociation for Computational LinguisticsAugust 8-9. 2013. 2013
We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.</p>
<p>Introduction</p>
<p>Syntactic treebanks have had tremendous impact on natural language processing. The Penn Treebank is a classic example-a simple, readable file of natural-language sentences paired with rooted, labeled syntactic trees. Researchers have exploited manually-built treebanks to build statistical parsers that improve in accuracy every year. This success is due in part to the fact that we have a single, whole-sentence parsing task, rather than separate tasks and evaluations for base noun identification, prepositional phrase attachment, trace recovery, verb-argument dependencies, etc. Those smaller tasks are naturally solved as a by-product of whole-sentence parsing, and in fact, solved better than when approached in isolation.</p>
<p>By contrast, semantic annotation today is balkanized. We have separate annotations for named entities, co-reference, semantic relations, discourse connectives, temporal entities, etc. Each annotation has its own associated evaluation, and training data is split across many resources. We lack a simple readable sembank of English sentences paired with their whole-sentence, logical meanings. We believe a sizable sembank will lead to new work in statistical natural language understanding (NLU), resulting in semantic parsers that are as ubiquitous as syntactic ones, and support natural language generation (NLG) by providing a logical semantic input.</p>
<p>Of course, when it comes to whole-sentence semantic representations, linguistic and philosophical work is extensive. We draw on this work to design an Abstract Meaning Representation (AMR) appropriate for sembanking. Our basic principles are:</p>
<p>• AMRs are rooted, labeled graphs that are easy for people to read, and easy for programs to traverse. • AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences "he described her as a genius", "his description of her: genius", and "she was a genius, according to his description" are all assigned the same AMR. • AMR makes extensive use of PropBank framesets (Kingsbury and Palmer, 2002;Palmer et al., 2005). For example, we represent a phrase like "bond investor" using the frame "invest-01", even though no verbs appear in the phrase. • AMR is agnostic about how we might want to derive meanings from strings, or viceversa. In translating sentences to AMR, we do not dictate a particular sequence of rule applications or provide alignments that reflect such rule sequences. This makes sembanking very fast, and it allows researchers to explore their own ideas about how strings are related to meanings. • AMR is heavily biased towards English. It is not an Interlingua. AMR is described in a 50-page annotation guideline. 1 In this paper, we give a high-level description of AMR, with examples, and we also provide pointers to software tools for evaluation and sembanking.</p>
<p>AMR Format</p>
<p>We write down AMRs as rooted, directed, edgelabeled, leaf-labeled graphs. This is a completely traditional format, equivalent to the simplest forms of feature structures (Shieber et al., 1986), conjunctions of logical triples, directed graphs, and PENMAN inputs (Matthiessen and Bateman, 1991). Figure 1 shows some of these views for the sentence "The boy wants to go". We use the graph notation for computer processing, and we adapt the PENMAN notation for human reading and writing.</p>
<p>AMR Content</p>
<p>In neo-Davidsonian fashion (Davidson, 1969), we introduce variables (or graph nodes) for entities, events, properties, and states. Leaves are labeled with concepts, so that "(b / boy)" refers to an instance (called b) of the concept boy. Relations link entities, so that "(d / die-01 :location (p / park))" means there was a death (d) in the park (p). When an entity plays multiple roles in a sentence, we employ re-entrancy in graph notation (nodes with multiple parents) or variable re-use in PENMAN notation.</p>
<p>AMR concepts are either English words ("boy"), PropBank framesets ("want-01"), or special keywords. Keywords include special entity types ("date-entity", "world-region", etc.), quantities ("monetary-quantity", "distance-quantity", etc.), and logical conjunctions ("and", etc).</p>
<p>AMR uses approximately 100 relations:</p>
<p>• Frame arguments, following PropBank conventions. :arg0, :arg1, :arg2, :arg3, :arg4, :arg5.</p>
<p>• General semantic relations. :accompanier, :age, :beneficiary, :cause, :compared-to, :concession, :condition, :consist-of, :degree, :destination, :direction, :domain, :duration, 1 AMR guideline: amr.isi.edu/language.html LOGIC format:
∃ w, b, g: instance(w, want-01) ∧ instance(g, go-01) ∧ instance(b, boy) ∧ arg0(w, b) ∧ arg1(w, g) ∧ arg0(g, b)
AMR format (based on PENMAN):</p>
<p>(w / want-01 :arg0 (b / boy) :arg1 (g / go-01 :arg0 b)) GRAPH format: • Relations for quantities. :quant, :unit, :scale.</p>
<p>• Relations for date-entities. :day, :month, :year, :weekday, :time, :timezone, :quarter, :dayperiod, :season, :year2, :decade, :century, :calendar, :era.</p>
<p>• Relations for lists. :op1, :op2, :op3, :op4, :op5, :op6, :op7, :op8, :op9, :op10.</p>
<p>AMR also includes the inverses of all these relations, e.g., :arg0-of, :location-of, and :quant-of. In addition, every relation has an associated reification, which is what we use when we want to modify the relation itself. For example, the reification of :location is the concept "be-located-at-91".</p>
<p>Our set of concepts and relations is designed to allow us represent all sentences, taking all words into account, in a reasonably consistent manner. In the rest of this section, we give examples of how AMR represents various kinds of words, phrases, and sentences. For full documentation, the reader is referred to the AMR guidelines.</p>
<p>Frame arguments. We make heavy use of PropBank framesets to abstract away from English syntax. For example, the frameset "describe-01" has three pre-defined slots (:arg0 is the describer, :arg1 is the thing described, and :arg2 is what it is being described as).
(d / describe-01 :arg0 (m / man) :arg1 (m2 / mission) :arg2 (d / disaster))
The man described the mission as a disaster. The man's description of the mission: disaster. As the man described it, the mission was a disaster.</p>
<p>Here, we do not annotate words like "as" or "it", considering them to be syntactic sugar. General semantic relations. AMR also includes many non-core relations, such as :beneficiary, :time, and :destination.</p>
<p>(s / hum-02 :arg0 (s2 / soldier) :beneficiary (g / girl) :time (w / walk-01 :arg0 g :destination (t / town)))</p>
<p>The soldier hummed to the girl as she walked to town.</p>
<p>Co-reference. AMR abstracts away from coreference gadgets like pronouns, zero-pronouns, reflexives, control structures, etc. Instead we reuse AMR variables, as with "g" above. AMR annotates sentences independent of context, so if a pronoun has no antecedent in the sentence, its nominative form is used, e.g., "(h / he)".</p>
<p>Inverse relations. We obtain rooted structures by using inverse relations like :arg0-of and :quantof.</p>
<p>(s / sing-01 :arg0 (b / boy :source (c / college)))</p>
<p>The boy from the college sang.</p>
<p>(b / boy :arg0-of (s / sing-01) :source (c / college)) the college boy who sang ...</p>
<p>(i / increase-01 :arg1 (n / number :quant-of (p / panda)))</p>
<p>The number of pandas increased.</p>
<p>The top-level root of an AMR represents the focus of the sentence or phrase. Once we have selected the root concept for an entire AMR, there are no more focus considerations-everything else is driven strictly by semantic relations. Modals and negation. AMR represents negation logically with :polarity, and it expresses modals with concepts.</p>
<p>(g / go-01 :arg0 (b / boy) :polarity -)</p>
<p>The boy did not go.</p>
<p>(p / possible :domain (g / go-01 :arg0 (b / boy)) :polarity -))</p>
<p>The boy cannot go. It's not possible for the boy to go. It's possible for the boy not to go.</p>
<p>(p / obligate-01 :arg2 (g / go-01 :arg0 (b / boy)) :polarity -)</p>
<p>The boy doesn't have to go. The boy isn't obligated to go. The boy need not go.</p>
<p>(p / obligate-01 :arg2 (g / go-01 :arg0 (b / boy) :polarity -))</p>
<p>The boy must not go. It's obligatory that the boy not go.</p>
<p>(t / think-01 :arg0 (b / boy) :arg1 (w / win-01 :arg0 (t / team) :polarity -))</p>
<p>The boy doesn't think the team will win. The boy thinks the team won't win.</p>
<p>Questions. AMR uses the concept "amrunknown", in place, to indicate wh-questions.</p>
<p>(f / find-01 :arg0 (g / girl) :arg1 (a / amr-unknown))</p>
<p>What did the girl find?</p>
<p>(f / find-01 :arg0 (g / girl) :arg1 (b / boy) :location (a / amr-unknown))</p>
<p>Where did the girl find the boy?</p>
<p>(f / find-01 :arg0 (g / girl) :arg1 (t / toy :poss (a / amr-unknown))) Whose toy did the girl find?</p>
<p>Yes-no questions, imperatives, and embedded whclauses are treated separately with the AMR relation :mode.</p>
<p>Verbs. Nearly every English verb and verbparticle construction we have encountered has a corresponding PropBank frameset.</p>
<p>(l / look-05 :arg0 (b / boy) :arg1 (a / answer))</p>
<p>The boy looked up the answer. The boy looked the answer up.</p>
<p>AMR abstracts away from light-verb constructions.</p>
<p>(a / adjust-01 :arg0 (g / girl) :arg1 (m / machine))</p>
<p>The girl adjusted the machine. The girl made adjustments to the machine.</p>
<p>Nouns. We use PropBank verb framesets to represent many nouns as well. We never say "destruction-01" in AMR. Some nominalizations refer to a whole event, while others refer to a role player in an event.</p>
<p>(s / see-01 :arg0 (j / judge) :arg1 (e / explode-01))</p>
<p>The judge saw the explosion.</p>
<p>(r / read-01 :arg0 (j / judge) :arg1 (t / thing :arg1-of (p / propose-01))</p>
<p>The judge read the proposal.</p>
<p>(t / thing :arg1-of (o / opine-01 :arg0 (g / girl))) the girl's opinion the opinion of the girl what the girl opined</p>
<p>Many "-er" nouns invoke PropBank framesets. This enables us to make use of slots defined for those framesets.</p>
<p>(p / person :arg0-of (i / invest-01)) investor (p / person :arg0-of (i / invest-01 :arg1 (b / bond))) bond investor (p / person :arg0-of (i / invest-01 :manner (s / small))) small investor (w / work-01 :arg0 (b / boy) :manner (h / hard)) the boy is a hard worker the boy works hard However, a treasurer is not someone who treasures, and a president is not (just) someone who presides.</p>
<p>Adjectives. Various adjectives invoke Prop-Bank framesets.</p>
<p>(s / spy :arg0-of (a / attract-01)) the attractive spy (s / spy :arg0-of (a / attract-01 :arg1 (w / woman))) the spy who is attractive to women "-ed" adjectives frequently invoke verb framesets. For example, "acquainted with magic" maps to "acquaint-01". However, we are not restricted to framesets that can be reached through morphological simplification. For other adjectives, we have defined new framesets.</p>
<p>(r / responsible-41 :arg1 (b / boy) :arg2 (w / work))</p>
<p>The boy is responsible for the work. The boy has responsibility for the work.</p>
<p>While "the boy responsibles the work" is not good English, it is perfectly good Chinese. Similarly, we handle tough-constructions logically.</p>
<p>(t / tough :domain (p / please-01 :arg1 (g / girl)))</p>
<p>Girls are tough to please. It is tough to please girls. Pleasing girls is tough.</p>
<p>"please-01" and "girl" are adjacent in the AMR, even if they are not adjacent in English. "-able" adjectives often invoke the AMR concept "possible", but not always (e.g., a "taxable fund" is actually a "taxed fund").</p>
<p>(s / sandwich :arg1-of (e / eat-01 :domain-of (p / possible)))</p>
<p>an edible sandwich (f / fund :arg1-of (t / tax-01)) a taxable fund Pertainym adjectives are normalized to root form.</p>
<p>(b / bomb :mod (a / atom)) atom bomb atomic bomb</p>
<p>Prepositions. Most prepositions simply signal semantic frame elements, and are themselves dropped from AMR. Time and location prepositions are kept if they carry additional information.</p>
<p>(d / default-01 :arg1 (n / nation) :time (a / after :op1 (w / war-01))</p>
<p>The nation defaulted after the war.</p>
<p>Occasionally, neither PropBank nor AMR has an appropriate relation, in which case we hold our nose and use a :prep-X relation. The man was sued in the case.</p>
<p>Named entities. Any concept in AMR can be modified with a :name relation. However, AMR includes standardized forms for approximately 80 named-entity types, including person, country, sports-facility, etc.</p>
<p>(p / person :name (n / name :op1 "Mollie" :op2 "Brown")) Mollie Brown (p / person :name (n / name :op1 "Mollie" :op2 "Brown") :arg0-of (s / slay-01 :arg1 (o / orc))) the orc-slaying Mollie Brown Mollie Brown, who slew orcs AMR does not normalize multiple ways of referring to the same concept (e.g., "US" versus "United States"). It also avoids analyzing semantic relations inside a named entity-e.g., an organization named "Stop Malaria Now" does not invoke the "stop-01" frameset. AMR gives a clean, uniform treatment to titles, appositives, and other constructions.</p>
<p>(c / city :name (n / name :op1 "Zintan")) Zintan the city of Zintan (p / president :name (n / name :op1 "Obama")) President Obama Obama, the president ...</p>
<p>(g / group :name (n / name :op1 "Elsevier" :op2 "N.V.") :mod (c / country :name (n2 / name :op1 "Netherlands")) :arg0-of (p / publish-01)) Elsevier N.V., the Dutch publishing group... Dutch publishing group Elsevier N.V. ...</p>
<p>Copula. Copulas use the :domain relation.</p>
<p>(w / white :domain (m / marble))</p>
<p>The marble is white.</p>
<p>(l / lawyer :domain (w / woman))</p>
<p>The woman is a lawyer.</p>
<p>(a / appropriate :domain (c / comment) :polarity -))</p>
<p>The comment is not appropriate.</p>
<p>The comment is inappropriate.</p>
<p>Reification. Sometimes we want to use an AMR relation as a first-class concept-to be able to modify it, for example. Every AMR relation has a corresponding reification for this purpose. The marble was not in the jar yesterday.</p>
<p>If we do not use the reification, we run into trouble.</p>
<p>(m / marble :location (j / jar :polarity -) :time (y / yesterday)) yesterday's marble in the non-jar ... Some reifications are standard PropBank framesets (e.g., "cause-01" for :cause, or "age-01" for :age).</p>
<p>This ends the summary of AMR content. For lack of space, we omit descriptions of comparatives, superlatives, conjunction, possession, determiners, date entities, numbers, approximate numbers, discourse connectives, and other phenomena covered in the full AMR guidelines.</p>
<p>Limitations of AMR</p>
<p>AMR does not represent inflectional morphology for tense and number, and it omits articles. This speeds up the annotation process, and we do not have a nice semantic target representation for these phenomena. A lightweight syntactic-style representation could be layered in, via an automatic post-process.</p>
<p>AMR has no universal quantifier. Words like "all" modify their head concepts. AMR does not distinguish between real events and hypothetical, future, or imagined ones. For example, in "the boy wants to go", the instances of "want-01" and "go-01" have the same status, even though the "go-01" may or may not happen.</p>
<p>We represent "history teacher" nicely as "(p / person :arg0-of (t / teach-01 :arg1 (h / history)))". However, "history professor" becomes "(p / professor :mod (h / history))", because "profess-01" is not an appropriate frame. It would be reasonable in such cases to use a NomBank (Meyers et al., 2004) noun frame with appropriate slots.</p>
<p>Creating AMRs</p>
<p>We have developed a power editor for AMR, accessible by web interface. 2 The AMR Editor allows rapid, incremental AMR construction via text commands and graphical buttons. It includes online documentation of relations, quantities, reifications, etc., with full examples. Users log in, and the editor records AMR activity. The editor also provides significant guidance aimed at increasing annotator consistency. For example, users are warned about incorrect relations, disconnected AMRs, words that have PropBank frames, etc. Users can also search existing sembanks for phrases to see how they were handled in the past. The editor also allows side-by-side comparison of AMRs from different users, for training purposes.</p>
<p>In order to assess inter-annotator agreement (IAA), as well as automatic AMR parsing accuracy, we developed the smatch metric (Cai and Knight, 2013) and associated script. 3 Smatch reports the semantic overlap between two AMRs by viewing each AMR as a conjunction of logical triples (see Figure 1). Smatch computes precision, recall, and F-score of one AMR's triples against the other's. To match up variables from two input AMRs, smatch needs to execute a brief search, looking for the variable mapping that yields the highest F-score.</p>
<p>Smatch makes no reference to English strings or word indices, as we do not enforce any particular string-to-meaning derivation. Instead, we compare semantic representations directly, in the same way that the MT metric Bleu (Papineni et al., 2002) compares target strings without making reference to the source.</p>
<p>For an initial IAA study, and prior to adjusting the AMR Editor to encourage consistency, 4 expert AMR annotators annotated 100 newswire sentences and 80 web text sentences. They then created consensus AMRs through discussion. The average annotator vs. consensus IAA (smatch) was 0.83 for newswire and 0.79 for web text. When newly trained annotators doubly annotated 382 web text sentences, their annotator vs. annotator IAA was 0.71.</p>
<p>Current AMR Bank</p>
<p>We currently have a manually-constructed AMR bank of several thousand sentences, a subset of which can be freely downloaded, 4 the rest being distributed via the LDC catalog.</p>
<p>In initially developing AMR, the authors built consensus AMRs for:</p>
<p>• 225 short sentences for tutorial purposes • 142 sentences of newswire (*)</p>
<p>• 100 sentences of web data (*) Trained annotators at LDC then produced AMRs for:</p>
<p>• 1546 sentences from the novel "The Little Prince" • 1328 sentences of web data • 1110 sentences of web data (<em>) • 926 sentences from Xinhua news (</em>) • 214 sentences from CCTV broadcast conversation (<em>) Collections marked with a star (</em>) are also in the OntoNotes corpus (Pradhan et al., 2007;Weischedel et al., 2011).</p>
<p>Using the AMR Editor, annotators are able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes.</p>
<p>Related Work</p>
<p>Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (Böhmová et al., 2003), and UNL (Uchida et al., 1999;Uchida et al., 1996;Martins, 2012).</p>
<p>Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., "describe-01"), and UNL uses English WordNet synsets (e.g., "200752493").</p>
<p>Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations.</p>
<p>Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first-4 amr.isi.edu/download.html order logic. GMB and ST both include universal quantification.</p>
<p>Granularity. GMB and UCCA annotate short texts, so that the same entity can participate in events described in different sentences; other systems annotate individual sentences.</p>
<p>Entities. AMR uses 80 entity types, while GMB uses 7.</p>
<p>Manual versus automatic. AMR, UNL, and UCCA annotation is fully manual. GMB and ST produce meaning representations automatically, and these can be corrected by experts or crowds (Venhuizen et al., 2013).</p>
<p>Derivations. AMR and UNL remain agnostic about the relation between strings and their meanings, considering this a topic of open research. ST and GMB annotate words and phrases directly, recording derivations as (for example) Montaguestyle compositional semantic rules operating on CCG parses.</p>
<p>Top-down verus bottom-up. AMR annotators find it fast to construct meanings from the top down, starting with the main idea of the sentence (though the AMR Editor allows bottom-up construction). GMB and UCCA annotators work bottom-up.</p>
<p>Editors, guidelines, genres. These projects have graphical sembanking tools (e.g., Basile et al. (2012b)), annotation guidelines, 5 and sembanks that cover a wide range of genres, from news to fiction. UNL and AMR have both annotated many of the same sentences, providing the potential for direct comparison.</p>
<p>Future Work</p>
<p>Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b;Quernheim and Knight, 2012a;Chiang et al., 2013).</p>
<p>Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. (2012).</p>
<p>Disjunctive AMR. AMR aims to canonicalize multiple ways of saying the same thing. We plan to test how well we are doing by building AMRs on top of large, manually-constructed paraphrase networks from the HyTER project (Dreyer and Marcu, 2012). Rather than build individual AMRs for different paths through a network, we will construct highly-packed disjunctive AMRs. With this application in mind, we have developed a guideline 6 for disjunctive AMR. Here is an example:</p>
<p>(o / <em>OR</em> :op1 (t / talk-01) :op2 (m / meet-03) :OR (o2 / <em>OR</em> :mod (o3 / official) :arg1-of (s / sanction-01 :arg0 (s2 / state)))) official talks state-sanctioned talks meetings sanctioned by the state AMR extensions. Finally, we would like to deepen the AMR language to include more relations (to replace :mod and :prep-X, for example), entity normalization (perhaps wikification), quantification, and temporal relations. Ultimately, we would like to also include a comprehensive set of more abstract frames like "Earthquake-01" (:magnitude, :epicenter, :casualties), "CriminalLawsuit-01" (:defendant, :crime, :jurisdiction), and "Pregnancy-01" (:father, :mother, :due-date). Projects like FrameNet (Baker et al., 1998) and CYC (Lenat, 1995) have long pursued such a set.</p>
<p>Figure 1 :
1Equivalent formats for representating the meaning of "The boy wants to go". :employed-by, :example, :extent, :frequency, :instrument, :li, :location, :manner, :medium, :mod, :mode, :name, :part, :path, :polarity, :poss, :purpose, :source, :subevent, :subset, :time, :topic, :value.</p>
<p>of the room by the boy ... the boy's destruction of the room ... The boy destroyed the room.</p>
<p>was afraid of battle. The soldier feared battle. The soldier had a fear of battle.
AMR Editor: amr.isi.edu/editor.html 3 Smatch: amr.isi.edu/evaluation.html
UNL guidelines: www.undl.org/unlsys/unl/unl2005</p>
<p>UCCA: A semantics-based grammatical annotation scheme. O Abend, A Rappoport, Proc. IWCS. IWCSO. Abend and A. Rappoport. 2013. UCCA: A semantics-based grammatical annotation scheme. In Proc. IWCS.</p>
<p>The Berkeley FrameNet project. C Baker, C Fillmore, J Lowe, Proc. COLING. COLINGC. Baker, C. Fillmore, and J. Lowe. 1998. The Berke- ley FrameNet project. In Proc. COLING.</p>
<p>Developing a large semantically annotated corpus. V Basile, J Bos, K Evang, N Venhuizen, Proc. LREC. LRECV. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012a. Developing a large semantically annotated corpus. In Proc. LREC.</p>
<p>A platform for collaborative semantic annotation. V Basile, J Bos, K Evang, N Venhuizen, Disjunctive AMR guideline: amr.isi.edu/damr.1.0.pdf. Proc. EACL demonstrationsDisjunctive AMR guideline: amr.isi.edu/damr.1.0.pdf V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012b. A platform for collaborative semantic annotation. In Proc. EACL demonstrations.</p>
<p>The Prague dependency treebank. A Böhmová, J Hajič, E Hajičová, B Hladká, Treebanks. SpringerA. Böhmová, J. Hajič, E. Hajičová, and B. Hladká. 2003. The Prague dependency treebank. In Tree- banks. Springer.</p>
<p>Banking meaning representations from treebanks. Linguistic Issues in Language Technology. A Butler, K Yoshimoto, 7A. Butler and K. Yoshimoto. 2012. Banking meaning representations from treebanks. Linguistic Issues in Language Technology, 7.</p>
<p>Smatch: An accuracy metric for abstract meaning representations. S Cai, K Knight, Proc. ACL. ACLS. Cai and K. Knight. 2013. Smatch: An accu- racy metric for abstract meaning representations. In Proc. ACL.</p>
<p>Parsing graphs with hyperedge replacement grammars. D Chiang, J Andreas, D Bauer, K M Hermann, B Jones, K Knight, Proc. ACL. ACLD. Chiang, J. Andreas, D. Bauer, K. M. Hermann, B. Jones, and K. Knight. 2013. Parsing graphs with hyperedge replacement grammars. In Proc. ACL.</p>
<p>The individuation of events. D Davidson, Honor of Carl G. Hempel. D. ReidelDordrechtD. Davidson. 1969. The individuation of events. In N. Rescher, editor, Essays in Honor of Carl G. Hempel. D. Reidel, Dordrecht.</p>
<p>Hyter: Meaningequivalent semantics for translation evaluation. M Dreyer, D Marcu, Proc. NAACL. NAACLM. Dreyer and D. Marcu. 2012. Hyter: Meaning- equivalent semantics for translation evaluation. In Proc. NAACL.</p>
<p>Semantics-based machine translation with hyperedge replacement grammars. B Jones, J Andreas, D Bauer, K M Hermann, K Knight, Proc. COLING. COLINGB. Jones, J. Andreas, D. Bauer, K. M. Hermann, and K. Knight. 2012. Semantics-based machine trans- lation with hyperedge replacement grammars. In Proc. COLING.</p>
<p>Discourse representation theory. H Kamp, J Van Genabith, U Reyle, Handbook of philosophical logic. SpringerH. Kamp, J. Van Genabith, and U. Reyle. 2011. Dis- course representation theory. In Handbook of philo- sophical logic, pages 125-394. Springer.</p>
<p>From TreeBank to PropBank. P Kingsbury, M Palmer, Proc. LREC. LRECP. Kingsbury and M. Palmer. 2002. From TreeBank to PropBank. In Proc. LREC.</p>
<p>Cyc: A large-scale investment in knowledge infrastructure. D B Lenat, Communications of the ACM. 3811D. B. Lenat. 1995. Cyc: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11).</p>
<p>Le Petit Prince in UNL. R Martins, Proc. LREC. LRECR. Martins. 2012. Le Petit Prince in UNL. In Proc. LREC.</p>
<p>Text Generation and Systemic-Functional Linguistics. C M I M Matthiessen, J A Bateman, PinterLondonC. M. I. M. Matthiessen and J. A. Bateman. 1991. Text Generation and Systemic-Functional Linguis- tics. Pinter, London.</p>
<p>The NomBank project: An interim report. A Meyers, R Reeves, C Macleod, R Szekely, V Zielinska, B Young, R Grishman, HLT-NAACL 2004 workshop: Frontiers in corpus annotation. A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The NomBank project: An interim report. In HLT- NAACL 2004 workshop: Frontiers in corpus anno- tation.</p>
<p>The Proposition Bank: An annotated corpus of semantic roles. M Palmer, D Gildea, P Kingsbury, Computational Linguistics. 131M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1).</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, ACL. Philadelphia, PAK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL, Philadelphia, PA.</p>
<p>Ontonotes: A unified relational semantic representation. S Pradhan, E Hovy, M Marcus, M Palmer, L Ramshaw, R Weischedel, International Journal of Semantic Computing (IJSC). 41S. Pradhan, E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2007. Ontonotes: A unified relational semantic representation. In- ternational Journal of Semantic Computing (IJSC), 1(4).</p>
<p>DAGGER: A toolkit for automata on directed acyclic graphs. D Quernheim, K Knight, Proc. FSMNLP. FSMNLPD. Quernheim and K. Knight. 2012a. DAGGER: A toolkit for automata on directed acyclic graphs. In Proc. FSMNLP.</p>
<p>Towards probabilistic acceptors and transducers for feature structures. D Quernheim, K Knight, Proc. SSST Workshop. SSST WorkshopD. Quernheim and K. Knight. 2012b. Towards prob- abilistic acceptors and transducers for feature struc- tures. In Proc. SSST Workshop.</p>
<p>VerbNet: A broad-coverage, comprehensive verb lexicon. K Schuler, University of PennsylvaniaPh.D. thesisK. Schuler. 2005. VerbNet: A broad-coverage, com- prehensive verb lexicon. Ph.D. thesis, University of Pennsylvania.</p>
<p>Compilation of papers on unification-based grammar formalisms. S Shieber, F C N Pereira, L Karttunen, M Kay, CSLI-86- 48Center for the Study of Language and Information. Stanford, CaliforniaTechnical ReportS. Shieber, F. C. N. Pereira, L. Karttunen, and M. Kay. 1986. Compilation of papers on unification-based grammar formalisms. Technical Report CSLI-86- 48, Center for the Study of Language and Informa- tion, Stanford, California.</p>
<p>UNL: Universal Networking Language-an electronic language for communication, understanding and collaboration. H Uchida, M Zhu, T Della Senta, ; Ias/Unu, Tokyo, Technical reportH. Uchida, M. Zhu, and T. Della Senta. 1996. UNL: Universal Networking Language-an electronic lan- guage for communication, understanding and col- laboration. Technical report, IAS/UNU Tokyo.</p>
<p>A gift for a millennium. H Uchida, M Zhu, T Della Senta, ; Ias/Unu, Tokyo, Technical reportH. Uchida, M. Zhu, and T. Della Senta. 1999. A gift for a millennium. Technical report, IAS/UNU Tokyo.</p>
<p>Gamification for word sense labeling. N Venhuizen, V Basile, K Evang, J Bos, Proc. IWCS. IWCSN. Venhuizen, V. Basile, K. Evang, and J. Bos. 2013. Gamification for word sense labeling. In Proc. IWCS.</p>
<p>OntoNotes: A large training corpus for enhanced processing. R Weischedel, E Hovy, M Marcus, M Palmer, R Belvin, S Pradhan, L Ramshaw, N Xue, Handbook of Natural Language Processing and Machine Translation. J. Olive, C. Christianson, and J. McCarySpringerR. Weischedel, E. Hovy, M. Marcus, M. Palmer, R. Belvin, S. Pradhan, L. Ramshaw, and N. Xue. 2011. OntoNotes: A large training corpus for en- hanced processing. In J. Olive, C. Christianson, and J. McCary, editors, Handbook of Natural Language Processing and Machine Translation. Springer.</p>
<p>Learning for semantic parsing with statistical machine translation. Y W Wong, R J Mooney, Proc. HLT-NAACL. HLT-NAACLY. W. Wong and R. J. Mooney. 2006. Learning for se- mantic parsing with statistical machine translation. In Proc. HLT-NAACL.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7388 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7388</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7388</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-274777563</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.11142v4.pdf" target="_blank">AD-LLM: Benchmarking Large Language Models for Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity. Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough. This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection. We examine three key tasks: (i) zero-shot detection, using LLMs'pre-trained knowledge to perform AD without tasks-specific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models. Through experiments with different datasets, we find that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging. Based on these results, we outline six future research directions on LLMs for AD.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7388.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7388.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 3.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 8B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned, open-source decoder-only large language model (8B parameters) used in this work as a zero-shot anomaly detector and as a category-description generator; evaluated by prompt-based verbal anomaly scoring with explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned, decoder-only LLM (instruct variant).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Zero-shot prompt-based detection using verbal anomaly scores and required explanation (implicit chain-of-thought), and used to generate category descriptions (prompt augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>JSON-output prompt requiring keys "reason" and "anomaly_score"; asks the LLM to explain then give anomaly confidence between 0 and 1. Two settings: (1) provide normal category names only; (2) provide both normal and anomaly category names; extended prompts include generated category descriptions. Template enforces explanation before score and escapes quotes for JSON.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text (multi-class categorical text samples: news, reviews, SMS)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AG News, BBC News, IMDB Reviews, N24 News, SMS Spam</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUROC, AUPRC</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 1 (zero-shot): Normal Only setting AUROC/AUPRC per dataset: AG News 0.8226/0.4036, BBC News 0.7910/0.3602, IMDB Reviews 0.7373/0.3474, N24 News 0.6267/0.1130, SMS Spam 0.7558/0.2884. Normal + Anomaly setting: AG News 0.8754/0.3998, BBC News 0.8612/0.3960, IMDB Reviews 0.8625/0.4606, N24 News 0.8784/0.3802, SMS Spam 0.9487/0.6361 (improvements when anomaly names provided).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Often outperforms or is competitive with many traditional unsupervised baselines reported (18 methods); best conventional baselines include OpenAI+LUNAR, OpenAI+ECOD, OpenAI+LOF depending on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Observed failure modes include infinite-loop repetitive outputs causing token-limit hits; Llama 3.1 required explanation (implicit CoT) before score or it produced degenerate outputs (s=0). Sensitive to prompt ordering and long-context generation; excluded from multi-round synthetic generation tasks due to long-context termination issues.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Run locally on NVIDIA RTX 6000 Ada GPU with 48 GB RAM in experiments; inference cost/time noted as non-trivial but no numeric GPU-hour/tokens provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AD-LLM: Benchmarking Large Language Models for Anomaly Detection', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7388.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7388.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source, high-capability instruction-following decoder-only LLM accessed via API and used for zero-shot anomaly detection, category-description generation, and multi-round synthetic data generation; consistently strong performance in zero-shot detection across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source instruction-tuned decoder-only LLLM (API access); advanced reasoning and generation capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Zero-shot prompt-based detection (verbal anomaly score + explanation) and LLM-driven synthetic-data generation (multi-round, keyword-to-sample pipeline) and category-description augmentation for prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Same JSON-output template as Llama 3.1 (Table A9/A10); separate keyword-generation and sample-generation prompts for synthetic augmentation (Table A11/A12); category-description generation prompt (Table A13). Prompts include task instructions, CoT exemplars, and require valid JSON output.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text (multi-class categorical text samples: news, reviews, SMS)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AG News, BBC News, IMDB Reviews, N24 News, SMS Spam</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUROC, AUPRC</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 1 (zero-shot): Normal Only setting AUROC/AUPRC: AG News 0.9332/0.7207, BBC News 0.9574/0.8432, IMDB Reviews 0.9349/0.7823, N24 News 0.7674/0.3252, SMS Spam 0.7940/0.5568. Normal + Anomaly setting: AG News 0.9293/0.6310, BBC News 0.9919/0.9088, IMDB Reviews 0.9668/0.8465, N24 News 0.9902/0.9009, SMS Spam 0.9862/0.8953. GPT-4o and DeepSeek-V3 consistently outperform baselines in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms average baseline and often approaches or exceeds the best-performing baselines (e.g., OpenAI+LUNAR, OpenAI+ECOD) across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Occasional outputs blocked by OpenAI safety filters on sensitive content; generation may be costly in compute and latency; synthetic generation was successful but needs careful prompt design to avoid repetition and maintain domain alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Accessed via API (costly at inference); authors note LLM zero-shot AD is time-consuming and expensive at inference but no numeric cost reported; multi-round synthetic generation with temperature 0.7–1.0.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AD-LLM: Benchmarking Large Language Models for Anomaly Detection', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7388.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7388.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Mixture-of-Experts (MoE) architecture LLM used via API (technical report cited) as a zero-shot detector and category-description generator; shows strong zero-shot detection performance though results vary by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-Experts (MoE) LLM (commercial/experimental model); accessed via API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Zero-shot prompt-based detection with verbal score + explanation; used for category descriptions as well.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Same JSON-output prompt templates (Table A9/A10) and description-generation prompt (Table A13).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text (multi-class categorical text samples)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AG News, BBC News, IMDB Reviews, N24 News, SMS Spam</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUROC, AUPRC</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 1 (zero-shot): Normal Only AUROC/AUPRC: AG News 0.9104/0.6442, BBC News 0.8206/0.5604, IMDB Reviews 0.8544/0.6808, N24 News 0.8207/0.4495, SMS Spam 0.8797/0.5963. Normal + Anomaly setting: AG News 0.9273/0.7817, BBC News 0.9581/0.8972, IMDB Reviews 0.9626/0.8569, N24 News 0.9514/0.7730, SMS Spam 0.9535/0.7914.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Generally competitive with GPT-4o and stronger than many traditional baselines; performance varies more across datasets than GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Occasional instruction-following failures (incorrect JSON formatting, no output) and internal server errors were observed; excluded from synthetic generation experiments due to unsatisfactory generation results for those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Accessed via API; specific latency or cost not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AD-LLM: Benchmarking Large Language Models for Anomaly Detection', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7388.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7388.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o used for synthetic data generation and category-description augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o was used to generate keyword groups (three-granularity), multi-round synthetic text samples per normal class, and textual category descriptions which were then used to augment training data and prompts for downstream unsupervised AD methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-accessible high-capability instruction-tuned decoder-only LLM used to generate synthetic training examples and category descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>LLM-driven data augmentation: (1) keyword generation (groups of 3 granularity levels) produced t groups per class, (2) sample generation per keyword group to create D_synth, (3) combine with small real training set D_small_train to train standard unsupervised AD models (two-step or end-to-end). Also used to generate category descriptions that augment LLM prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Two-step pipeline: (i) keyword-generation prompt that requests exactly t unique keyword-groups per category (broad, intermediate, fine-grained) (Table A11); (ii) sample-generation prompt that produces one synthetic sample per keyword group incorporating all three keywords (Table A12). Category-description prompt in Table A13.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Augmentation experiments used D_small_train (v = 10 normal samples per class for AG News, BBC News, IMDB, SMS; v = 3 for N24 News) and generated synthetic samples D_synth (t = 50 per normal class for AG News, BBC, IMDB, SMS; t = 30 for N24 News).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text (training examples for unsupervised AD models)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AG News, BBC News, IMDB Reviews, N24 News, SMS Spam</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUROC, AUPRC (reported per model and averaged ± std over datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Data augmentation substantially improved many traditional detectors. Example (OpenAI+AE): without D_synth average AUROC/AUPRC 0.5641 ± 0.0834 / 0.1373 ± 0.0262; with D_synth average AUROC/AUPRC 0.7413 ± 0.1542 / 0.2932 ± 0.1069. Detailed Table (per-detector): AE often closed gap between limited-data and full-data performance; models with rigid geometric assumptions (e.g., DeepSVDD) sometimes degraded or showed mixed effects.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Augmented-training results compared to the same detectors trained only on D_small_train; baseline methods include AE, DeepSVDD, ECOD, IForest, LOF, LUNAR, VAE, SO-GAAL etc. Some detectors (AE, ECOD, LUNAR, VAE) saw substantial AUROC/AUPRC gains; others (DeepSVDD, SO-GAAL) saw limited/no improvement or declines.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>augmentation used to improve supervised/unsupervised training of downstream models (not zero-shot); original LLM generation is zero-shot generation but used to produce data for training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Synthetic generation can be repetitive; LLMs sometimes terminate keyword generation early on long prompts; synthetic data can harm models with rigid geometric assumptions; quality and domain alignment of synthetic data vary by dataset and detector; careful prompt design and filtering needed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Multi-round generation with temperature sweep [1.0,0.9,0.8,0.7] and multiple seeds; GPT-4o used via API; no numeric compute/time/cost reported but authors note scaling and cost concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AD-LLM: Benchmarking Large Language Models for Anomaly Detection', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7388.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7388.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI-o1-preview (UMS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI-o1-preview (reasoning LLM used for unsupervised model selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-enhanced LLM variant used to perform zero-shot unsupervised model selection (UMS) by ingesting structured dataset statistics and model abstracts, recommending AD models and justifications without historical performance data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI-o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Enhanced-reasoning LLM variant (system card cited) accessed via API; tuned/selected for chain-of-thought and structured reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Zero-shot unsupervised model selection: provide dataset metadata (name, size, avg/max/min/std lengths, sample examples, normal/anomaly labels) and model abstracts; LLM recommends an AD model from a constrained set and justifies the choice.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>UMS prompt (Table A14) that supplies dataset description, representative normal/anomalous samples, textual statistics, and abstracts of candidate models; asks for a JSON with keys 'reason' and 'choice' selecting from a pre-specified list of models (e.g., BERT+AE, OpenAI+LUNAR, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text dataset metadata (statistics and sample examples)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AG News, BBC News, IMDB Reviews, N24 News, SMS Spam (evaluated across these)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>AUROC, AUPRC of the recommended model when applied to dataset (average reported across 5 queries per dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table A7 aggregated results: OpenAI-o1-preview recommended models produced average AUROC/AUPRC across datasets (examples): AG News 0.8908/0.6193, BBC News 0.6992/0.2214, IMDB Reviews 0.6652/0.2787, N24 News 0.7706/0.3422, SMS Spam 0.5774/0.1220. In most cases, LLM recommendations beat the average baseline and sometimes approached the best baseline model.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against two references: (i) best baseline performance (upper bound) and (ii) average baseline (random selection). LLM recommendations frequently surpassed average baseline and sometimes approached best baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>zero-shot model selection</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Justifications from LLM often generic and not tightly linked to dataset-specific characteristics; some popular LLMs (GPT-4o, Llama 3.1) performed poorly on UMS (recommending same model regardless of context), indicating selection biases; context quality strongly affects recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>API-based reasoning calls; numeric cost/latency not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AD-LLM: Benchmarking Large Language Models for Anomaly Detection', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>How good are LLMs at out-of-distribution detection? <em>(Rating: 2)</em></li>
                <li>Language models meet anomaly detection for better interpretability and generalizability <em>(Rating: 2)</em></li>
                <li>Large language models for anomaly and out-of-distribution detection: A survey <em>(Rating: 2)</em></li>
                <li>MMAD: The first-ever comprehensive benchmark for multimodal large language models in industrial anomaly detection <em>(Rating: 1)</em></li>
                <li>Detecting anomalies in text via self-supervision of transformers (DATE) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7388",
    "paper_id": "paper-274777563",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "Llama 3.1",
            "name_full": "Llama 3.1 8B Instruct",
            "brief_description": "An instruction-tuned, open-source decoder-only large language model (8B parameters) used in this work as a zero-shot anomaly detector and as a category-description generator; evaluated by prompt-based verbal anomaly scoring with explanations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 3.1",
            "model_description": "Instruction-tuned, decoder-only LLM (instruct variant).",
            "model_size": "8B",
            "anomaly_detection_approach": "Zero-shot prompt-based detection using verbal anomaly scores and required explanation (implicit chain-of-thought), and used to generate category descriptions (prompt augmentation).",
            "prompt_template": "JSON-output prompt requiring keys \"reason\" and \"anomaly_score\"; asks the LLM to explain then give anomaly confidence between 0 and 1. Two settings: (1) provide normal category names only; (2) provide both normal and anomaly category names; extended prompts include generated category descriptions. Template enforces explanation before score and escapes quotes for JSON.",
            "training_data": null,
            "data_type": "text (multi-class categorical text samples: news, reviews, SMS)",
            "dataset_name": "AG News, BBC News, IMDB Reviews, N24 News, SMS Spam",
            "evaluation_metric": "AUROC, AUPRC",
            "performance": "Table 1 (zero-shot): Normal Only setting AUROC/AUPRC per dataset: AG News 0.8226/0.4036, BBC News 0.7910/0.3602, IMDB Reviews 0.7373/0.3474, N24 News 0.6267/0.1130, SMS Spam 0.7558/0.2884. Normal + Anomaly setting: AG News 0.8754/0.3998, BBC News 0.8612/0.3960, IMDB Reviews 0.8625/0.4606, N24 News 0.8784/0.3802, SMS Spam 0.9487/0.6361 (improvements when anomaly names provided).",
            "baseline_comparison": "Often outperforms or is competitive with many traditional unsupervised baselines reported (18 methods); best conventional baselines include OpenAI+LUNAR, OpenAI+ECOD, OpenAI+LOF depending on dataset.",
            "zero_shot_or_few_shot": "zero-shot",
            "limitations_or_failure_cases": "Observed failure modes include infinite-loop repetitive outputs causing token-limit hits; Llama 3.1 required explanation (implicit CoT) before score or it produced degenerate outputs (s=0). Sensitive to prompt ordering and long-context generation; excluded from multi-round synthetic generation tasks due to long-context termination issues.",
            "computational_cost": "Run locally on NVIDIA RTX 6000 Ada GPU with 48 GB RAM in experiments; inference cost/time noted as non-trivial but no numeric GPU-hour/tokens provided.",
            "uuid": "e7388.0",
            "source_info": {
                "paper_title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "Closed-source, high-capability instruction-following decoder-only LLM accessed via API and used for zero-shot anomaly detection, category-description generation, and multi-round synthetic data generation; consistently strong performance in zero-shot detection across datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Closed-source instruction-tuned decoder-only LLLM (API access); advanced reasoning and generation capabilities.",
            "model_size": "not specified in paper",
            "anomaly_detection_approach": "Zero-shot prompt-based detection (verbal anomaly score + explanation) and LLM-driven synthetic-data generation (multi-round, keyword-to-sample pipeline) and category-description augmentation for prompts.",
            "prompt_template": "Same JSON-output template as Llama 3.1 (Table A9/A10); separate keyword-generation and sample-generation prompts for synthetic augmentation (Table A11/A12); category-description generation prompt (Table A13). Prompts include task instructions, CoT exemplars, and require valid JSON output.",
            "training_data": null,
            "data_type": "text (multi-class categorical text samples: news, reviews, SMS)",
            "dataset_name": "AG News, BBC News, IMDB Reviews, N24 News, SMS Spam",
            "evaluation_metric": "AUROC, AUPRC",
            "performance": "Table 1 (zero-shot): Normal Only setting AUROC/AUPRC: AG News 0.9332/0.7207, BBC News 0.9574/0.8432, IMDB Reviews 0.9349/0.7823, N24 News 0.7674/0.3252, SMS Spam 0.7940/0.5568. Normal + Anomaly setting: AG News 0.9293/0.6310, BBC News 0.9919/0.9088, IMDB Reviews 0.9668/0.8465, N24 News 0.9902/0.9009, SMS Spam 0.9862/0.8953. GPT-4o and DeepSeek-V3 consistently outperform baselines in many settings.",
            "baseline_comparison": "Outperforms average baseline and often approaches or exceeds the best-performing baselines (e.g., OpenAI+LUNAR, OpenAI+ECOD) across datasets.",
            "zero_shot_or_few_shot": "zero-shot",
            "limitations_or_failure_cases": "Occasional outputs blocked by OpenAI safety filters on sensitive content; generation may be costly in compute and latency; synthetic generation was successful but needs careful prompt design to avoid repetition and maintain domain alignment.",
            "computational_cost": "Accessed via API (costly at inference); authors note LLM zero-shot AD is time-consuming and expensive at inference but no numeric cost reported; multi-round synthetic generation with temperature 0.7–1.0.",
            "uuid": "e7388.1",
            "source_info": {
                "paper_title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DeepSeek-V3",
            "name_full": "DeepSeek-V3",
            "brief_description": "A Mixture-of-Experts (MoE) architecture LLM used via API (technical report cited) as a zero-shot detector and category-description generator; shows strong zero-shot detection performance though results vary by dataset.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-V3",
            "model_description": "Mixture-of-Experts (MoE) LLM (commercial/experimental model); accessed via API.",
            "model_size": "not specified in paper",
            "anomaly_detection_approach": "Zero-shot prompt-based detection with verbal score + explanation; used for category descriptions as well.",
            "prompt_template": "Same JSON-output prompt templates (Table A9/A10) and description-generation prompt (Table A13).",
            "training_data": null,
            "data_type": "text (multi-class categorical text samples)",
            "dataset_name": "AG News, BBC News, IMDB Reviews, N24 News, SMS Spam",
            "evaluation_metric": "AUROC, AUPRC",
            "performance": "Table 1 (zero-shot): Normal Only AUROC/AUPRC: AG News 0.9104/0.6442, BBC News 0.8206/0.5604, IMDB Reviews 0.8544/0.6808, N24 News 0.8207/0.4495, SMS Spam 0.8797/0.5963. Normal + Anomaly setting: AG News 0.9273/0.7817, BBC News 0.9581/0.8972, IMDB Reviews 0.9626/0.8569, N24 News 0.9514/0.7730, SMS Spam 0.9535/0.7914.",
            "baseline_comparison": "Generally competitive with GPT-4o and stronger than many traditional baselines; performance varies more across datasets than GPT-4o.",
            "zero_shot_or_few_shot": "zero-shot",
            "limitations_or_failure_cases": "Occasional instruction-following failures (incorrect JSON formatting, no output) and internal server errors were observed; excluded from synthetic generation experiments due to unsatisfactory generation results for those tasks.",
            "computational_cost": "Accessed via API; specific latency or cost not provided.",
            "uuid": "e7388.2",
            "source_info": {
                "paper_title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4o (augmentation)",
            "name_full": "GPT-4o used for synthetic data generation and category-description augmentation",
            "brief_description": "GPT-4o was used to generate keyword groups (three-granularity), multi-round synthetic text samples per normal class, and textual category descriptions which were then used to augment training data and prompts for downstream unsupervised AD methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "API-accessible high-capability instruction-tuned decoder-only LLM used to generate synthetic training examples and category descriptions.",
            "model_size": "not specified",
            "anomaly_detection_approach": "LLM-driven data augmentation: (1) keyword generation (groups of 3 granularity levels) produced t groups per class, (2) sample generation per keyword group to create D_synth, (3) combine with small real training set D_small_train to train standard unsupervised AD models (two-step or end-to-end). Also used to generate category descriptions that augment LLM prompt context.",
            "prompt_template": "Two-step pipeline: (i) keyword-generation prompt that requests exactly t unique keyword-groups per category (broad, intermediate, fine-grained) (Table A11); (ii) sample-generation prompt that produces one synthetic sample per keyword group incorporating all three keywords (Table A12). Category-description prompt in Table A13.",
            "training_data": "Augmentation experiments used D_small_train (v = 10 normal samples per class for AG News, BBC News, IMDB, SMS; v = 3 for N24 News) and generated synthetic samples D_synth (t = 50 per normal class for AG News, BBC, IMDB, SMS; t = 30 for N24 News).",
            "data_type": "text (training examples for unsupervised AD models)",
            "dataset_name": "AG News, BBC News, IMDB Reviews, N24 News, SMS Spam",
            "evaluation_metric": "AUROC, AUPRC (reported per model and averaged ± std over datasets)",
            "performance": "Data augmentation substantially improved many traditional detectors. Example (OpenAI+AE): without D_synth average AUROC/AUPRC 0.5641 ± 0.0834 / 0.1373 ± 0.0262; with D_synth average AUROC/AUPRC 0.7413 ± 0.1542 / 0.2932 ± 0.1069. Detailed Table (per-detector): AE often closed gap between limited-data and full-data performance; models with rigid geometric assumptions (e.g., DeepSVDD) sometimes degraded or showed mixed effects.",
            "baseline_comparison": "Augmented-training results compared to the same detectors trained only on D_small_train; baseline methods include AE, DeepSVDD, ECOD, IForest, LOF, LUNAR, VAE, SO-GAAL etc. Some detectors (AE, ECOD, LUNAR, VAE) saw substantial AUROC/AUPRC gains; others (DeepSVDD, SO-GAAL) saw limited/no improvement or declines.",
            "zero_shot_or_few_shot": "augmentation used to improve supervised/unsupervised training of downstream models (not zero-shot); original LLM generation is zero-shot generation but used to produce data for training.",
            "limitations_or_failure_cases": "Synthetic generation can be repetitive; LLMs sometimes terminate keyword generation early on long prompts; synthetic data can harm models with rigid geometric assumptions; quality and domain alignment of synthetic data vary by dataset and detector; careful prompt design and filtering needed.",
            "computational_cost": "Multi-round generation with temperature sweep [1.0,0.9,0.8,0.7] and multiple seeds; GPT-4o used via API; no numeric compute/time/cost reported but authors note scaling and cost concerns.",
            "uuid": "e7388.3",
            "source_info": {
                "paper_title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "OpenAI-o1-preview (UMS)",
            "name_full": "OpenAI-o1-preview (reasoning LLM used for unsupervised model selection)",
            "brief_description": "A reasoning-enhanced LLM variant used to perform zero-shot unsupervised model selection (UMS) by ingesting structured dataset statistics and model abstracts, recommending AD models and justifications without historical performance data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI-o1-preview",
            "model_description": "Enhanced-reasoning LLM variant (system card cited) accessed via API; tuned/selected for chain-of-thought and structured reasoning.",
            "model_size": "not specified",
            "anomaly_detection_approach": "Zero-shot unsupervised model selection: provide dataset metadata (name, size, avg/max/min/std lengths, sample examples, normal/anomaly labels) and model abstracts; LLM recommends an AD model from a constrained set and justifies the choice.",
            "prompt_template": "UMS prompt (Table A14) that supplies dataset description, representative normal/anomalous samples, textual statistics, and abstracts of candidate models; asks for a JSON with keys 'reason' and 'choice' selecting from a pre-specified list of models (e.g., BERT+AE, OpenAI+LUNAR, etc.).",
            "training_data": null,
            "data_type": "text dataset metadata (statistics and sample examples)",
            "dataset_name": "AG News, BBC News, IMDB Reviews, N24 News, SMS Spam (evaluated across these)",
            "evaluation_metric": "AUROC, AUPRC of the recommended model when applied to dataset (average reported across 5 queries per dataset)",
            "performance": "Table A7 aggregated results: OpenAI-o1-preview recommended models produced average AUROC/AUPRC across datasets (examples): AG News 0.8908/0.6193, BBC News 0.6992/0.2214, IMDB Reviews 0.6652/0.2787, N24 News 0.7706/0.3422, SMS Spam 0.5774/0.1220. In most cases, LLM recommendations beat the average baseline and sometimes approached the best baseline model.",
            "baseline_comparison": "Compared against two references: (i) best baseline performance (upper bound) and (ii) average baseline (random selection). LLM recommendations frequently surpassed average baseline and sometimes approached best baseline.",
            "zero_shot_or_few_shot": "zero-shot model selection",
            "limitations_or_failure_cases": "Justifications from LLM often generic and not tightly linked to dataset-specific characteristics; some popular LLMs (GPT-4o, Llama 3.1) performed poorly on UMS (recommending same model regardless of context), indicating selection biases; context quality strongly affects recommendations.",
            "computational_cost": "API-based reasoning calls; numeric cost/latency not reported.",
            "uuid": "e7388.4",
            "source_info": {
                "paper_title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "How good are LLMs at out-of-distribution detection?",
            "rating": 2,
            "sanitized_title": "how_good_are_llms_at_outofdistribution_detection"
        },
        {
            "paper_title": "Language models meet anomaly detection for better interpretability and generalizability",
            "rating": 2,
            "sanitized_title": "language_models_meet_anomaly_detection_for_better_interpretability_and_generalizability"
        },
        {
            "paper_title": "Large language models for anomaly and out-of-distribution detection: A survey",
            "rating": 2,
            "sanitized_title": "large_language_models_for_anomaly_and_outofdistribution_detection_a_survey"
        },
        {
            "paper_title": "MMAD: The first-ever comprehensive benchmark for multimodal large language models in industrial anomaly detection",
            "rating": 1,
            "sanitized_title": "mmad_the_firstever_comprehensive_benchmark_for_multimodal_large_language_models_in_industrial_anomaly_detection"
        },
        {
            "paper_title": "Detecting anomalies in text via self-supervision of transformers (DATE)",
            "rating": 1,
            "sanitized_title": "detecting_anomalies_in_text_via_selfsupervision_of_transformers_date"
        }
    ],
    "cost": 0.016272,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AD-LLM: Benchmarking Large Language Models for Anomaly Detection</p>
<p>Tiankai Yang tiankaiy@usc.edu 
University of Southern California</p>
<p>Yi Nian 
University of Southern California</p>
<p>Shawn Li 
University of Southern California</p>
<p>Ruiyao Xu ruiyaoxu2028@u.northwestern.edu 
Northwestern University</p>
<p>Yuangang Li yuangang@usc.edu 
University of Southern California</p>
<p>Jiaqi Li 
University of Southern California</p>
<p>Zhuo Xiao zhuoxiao@usc.edu 
University of Southern California</p>
<p>Xiyang Hu xiyanghu@asu.edu 
Arizona State University
4 Adobe Research</p>
<p>Ryan Rossi ryrossi@adobe.com 
Kaize Ding kaize.ding@northwestern.edu 
Northwestern University</p>
<p>Xia Hu xia.hu@rice.edu 
Rice University</p>
<p>Yue Zhao 
University of Southern California</p>
<p>AD-LLM: Benchmarking Large Language Models for Anomaly Detection
A843A43B9EF9594B6C28F87AFAE4B894
Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring.Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity.Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough.This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection.We examine three key tasks: (i) zero-shot detection, using LLMs' pretrained knowledge to perform AD without taskspecific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models.Through experiments with different datasets, we find that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging.Based on these results, we outline six future research directions on LLMs for AD.</p>
<p>Introduction</p>
<p>Anomaly detection (AD) is an important topic in machine learning (ML) that identifies samples differing from the general distribution (Zhao et al., 2019;Liu et al., 2024c).This ability is critical for many practical applications, such as fraud detection (Abdallah et al., 2016), medical diagnosis (Fernando et al., 2021), software engineering (Sun et al., 2022), and industrial system monitoring (Sun et al., 2023).Within natural language processing (NLP), AD is also important for finding unusual text instances, which is needed for detecting spam * Equal contribution.(Rao et al., 2021), misinformation (Islam et al., 2020), or unusual user behavior (Xue et al., 2023).</p>
<p>In the current era of large language models (LLMs), we ask how AD can make use of their capabilities and what the current level of integration looks like.While LLMs have brought large improvements to areas such as text generation, summarization, and translation, their possible benefits for AD, especially in NLP, have received some attention (Li et al., 2024a;Xu and Ding, 2024) but have not been studied in detail.</p>
<p>This work presents the first comprehensive benchmark, called AD-LLM, to study the roles and potential of LLMs in NLP anomaly detection.Our analysis focuses on three key tasks that are central in AD research and in practice (Figure 1): • (i) LLM for Anomaly Detection ( §3): Many AD tasks lack enough labeled data, making it hard to train models from scratch (Han et al., 2022).LLMs, with their pre-trained knowledge, can perform zero-shot AD (Xu and Ding, 2024).</p>
<p>• (ii) LLM for Data Augmentation ( §4): AD tasks often suffer from unbalanced or limited data (Yoo et al., 2024;Li et al., 2023).For example, only a few insurance fraud samples may be available (Bauder and Khoshgoftaar, 2018).</p>
<p>Generative LLMs may produce synthetic data to strengthen AD cost-effectively.</p>
<p>• (iii) LLM for Model Selection ( §5): Picking a good AD model usually needs many trials and domain insights (Jiang et al., 2024a), and current choices in practice are often random (Zhao et al., 2021).LLMs, with the prior knowledge and ability to reason, may be able to suggest suitable AD models and save human effort.</p>
<p>Collectively, these three tasks tackle fundamental AD challenges from multiple angles: rapidly detecting anomalies with minimal supervision, enriching limited datasets for more robust learning, and guiding model selection without extensive domain expertise.As a result, AD-LLM not only improves individual AD components but also demonstrates how LLMs can streamline the entire process-from raw data to reliable, actionable insights.Key Takeaways.Our results reveal several noteworthy insights: (i) LLMs can achieve superior zero-shot AD performance, often outperforming conventional methods without relying on taskspecific data.(ii) Enriching LLM inputs with additional context, such as anomaly category names or descriptive prompts, further boosts detection quality.(iii) Employing LLM-driven data augmentation enhances AD performance, though the effectiveness varies with model features and dataset properties.(iv) LLM-based model selection can approach top-performing baselines, but improving interpretability and providing dataset-specific rationales remains an open area.These suggest future work that systematically integrates external knowledge, refines prompt engineering, and develops strategies to ensure more transparent, contextaware LLM recommendations in AD tasks.</p>
<p>Contributions.This paper makes the following key contributions:</p>
<p>• The First Comprehensive LLM-based AD benchmark.We introduce AD-LLM, a unified evaluation framework that examines how LLMs address three core AD tasks-detection, data augmentation, and model selection.</p>
<p>• Systematic and In-depth Experimental Analysis.Through extensive experiments across multiple datasets, we show that LLMs can achieve strong zero-shot AD performance, boost AD methods by generating synthetic data or descriptive prompts, and recommend effective AD models w/o relying on historical performance data.</p>
<p>• Reproducibility and Accessibility.We release AD-LLM under the MIT License at https:// github.com/USC-FORTIS/AD-LLM,providing a platform for the community to explore advanced applications of LLMs in AD.</p>
<p>2 Preliminaries on AD-LLM</p>
<p>Related Work</p>
<p>Recent studies have explored the role of LLMs in AD, highlighting both opportunities and challenges.Xu and Ding (2024) proposes a taxonomy categorizing LLMs as either detection or generative tools, but their work lacks experimental benchmarks.Similarly, Jiang et al. (2024b) presents MMAD, a benchmark designed for industrial AD, focusing on image datasets yet limiting its applicability to other modalities.Liu et al. (2024b) evaluates LLMs like Llama for out-of-distribution (OOD) detection, demonstrating the effectiveness of cosine distance detectors with isotropic embeddings achieved from LLMs.However, their study does not explore advanced LLM capabilities like data augmentation and zero-shot detection.</p>
<p>Our work, AD-LLM, bridges these gaps by introducing a comprehensive benchmark for evaluating LLMs in anomaly detection across diverse tasks.This makes AD-LLM a significant step toward advancing LLM-driven anomaly detection.</p>
<p>Datasets and Traditional Baselines</p>
<p>Our experiments encompass five NLP AD datasets sourced from Li et al. (2024c), derived from classification datasets.Each dataset contains text samples from multiple categories, with one designated as the anomaly category.The training data includes only normal samples.See the detailed information on datasets in Appx.A.1</p>
<p>We compare LLM-based AD with 18 traditional training-based unsupervised methods evaluated in Li et al. (2024c) and leverage LLMs to enhance them.These baselines can be categorized into two groups: (1) end-to-end algorithms that directly process raw text data to produce AD results and (2) two-step methods that first create text embeddings using language models and then apply traditional AD techniques to those embeddings.See a complete list of methods in Appx.A.2.</p>
<p>Common Experimental Settings</p>
<p>Evaluation Metrics.We evaluate the AD performance using two commonly used metrics (Han et al., 2022): (1) the Area Under the Receiver Operating Characteristic Curve (i.e., AUROC) and ( 2) the Area Under the Precision-Recall Curve (i.e., AUPRC).Both are the higher, the better.LLMs and Hardware.We select three LLMs as main backbones: (1) Llama 3.1 8B Instruct (referred to as Llama 3.1) (Dubey et al., 2024), (2) GPT-4o (OpenAI, 2024a), and (3) DeepSeek-V3 (Liu et al., 2024a).Llama 3.1 represents an open-source model with accessible size and cost, GPT-4o serves as a closed-source model with advanced capabilities, and DeepSeek-v3 represents the Mixture-of-Experts (MoE) architecture.</p>
<p>Llama 3.1 runs on one NVIDIA RTX 6000 Ada GPU with 48 GB RAM.GPT-4o and DeepSeek-V3 are accessed through official APIs.Seed is set = 42 for reproducibility.Specific experimental settings are highlighted separately in each subsequent task.</p>
<p>3 Task 1: LLM for Zero-shot Detection</p>
<p>Motivation</p>
<p>Classical AD methods often require extensive training data-either labeled for supervised methods or unlabeled for unsupervised ones-which is timeconsuming and costly (Han et al., 2022).In addition, setting up and tuning these models for realworld scenarios can be challenging and slow.</p>
<p>LLMs offer a practical alternative (Xu and Ding, 2024).With their broad pre-trained knowledge, they can perform zero-shot detection without additional training data.Their ability to understand language context and semantics makes them suitable for recognizing anomalies by logical reasoning.They can also explain their predictions, improving interpretability and trustworthiness (Huang et al., 2024b), which is important in sensitive domains such as healthcare, finance, and cybersecurity.</p>
<p>Problem Statement and Designs</p>
<p>Problem 1 (Zero-shot AD via LLMs) Given a test set D test = {x 1 , x 2 , . . ., x n } of text samples, where each sample x i belongs to either a normal category or an anomaly category, the objective is to identify the anomalous samples using a pre-trained LLM f LLM in a zero-shot setting without any task-specific training data.</p>
<p>Evaluation Protocol.We consider two settings, each reflecting different levels of prior knowledge: • Normal Only: We provide only the normal category name(s) C normal .This matches scenarios where normal behavior is known but anomalies are uncertain or emerging.</p>
<p>• Normal + Anomaly: We provide both normal and anomaly category names, C normal and C anomaly .This setting reflects situations where some information on anomalies is available, helping the LLM reason about what is anomalous.The detection process is defined as:
P = T x i , C normal , C * anomaly (r, s) = f LLM (P)(1)
Here, T (•) constructs the prompt P for a test sample x i , including known category information.The anomaly category is included only in the "Normal + Anomaly" setting, denoted as C * anomaly .The LLM f LLM processes the prompt to produce a verbal anomaly score s and an explanation r that describes the reasoning.This setup allows a systematic evaluation of LLMs in zero-shot AD, using promptbased inference to handle different levels of prior knowledge.See details in Appx.B.</p>
<p>Results, Insights, and Future Directions</p>
<p>We select Llama 3.1, GPT-4o, and DeepSeek-V3 as zero-shot detectors.Temperature is set as = 0 for stable outputs.LLMs are effective in zero-shot AD, surpassing existing training-based AD algorithms.We compare LLM-based zero-shot detectors with top baselines across five datasets in Table 1.GPT-4o and DeepSeek-V3 consistently outperform baselines;  (Gao et al., 2023).Future Direction 2: Optimize for Real-world Deployment.Despite their effectiveness, LLM-based zero-shot AD is inherently time-consuming and costly during the inference (Sinha et al., 2024).Reducing computational overhead is important for deploying LLMs in real settings, especially for AD applications, which are often time-critical.Methods like quantization (Dettmers et al., 2023;Xiao et al., 2023), pruning (Sun et al., 2024;Fu et al., 2024), and knowledge distillation (Wang et al., 2024b;Fu et al., 2023) can help reduce the model size and inference time while maintaining good performance.</p>
<p>4 Task 2: LLM for Data Augmentation</p>
<p>Motivation</p>
<p>Data augmentation (DA) in AD aims to produce additional samples to improve model training under data scarcity (Yoo et al., 2023).However, traditional methods often struggle to capture the complexity of natural language, potentially causing a shift in domain characteristics (Feng et al., 2021).LLMs offer a solution, using their broad pre-trained knowledge and autoregressive learning objectives to generate contextually relevant data with better semantic understanding (Xu and Ding, 2024).</p>
<p>In addition, LLMs can generate textual descriptions (Xu and Ding, 2024) that assist the LLMbased detectors in §3.For example, by producing descriptions of known categories, LLMs help detectors establish distant associations between normal and anomalous samples (Menon and Vondrick, 2022;Zhu et al., 2024) (Long et al., 2024).Additionally, LLMs face constraints such as token limits and challenges in processing long contexts (Gao et al., 2024).To address these issues, we adopt a multi-step strategy:</p>
<p>• Step1: Keyword Generation: Generate groups of keywords in one inquiry.Each group contains three keywords with a different level of granularity: broad/general, intermediate, or fine-grained.</p>
<p>• Step2: Sample Generation: For each keyword group, generate one synthetic sample xi .</p>
<p>Separating keyword generation from sample creation and enforcing different granularity levels ensures controlled variability and prevents overly long or repetitive outputs.This results in more contextually rich and diverse synthetic samples.</p>
<p>To scale up further, we generate synthetic data in multiple rounds.In each round, we adjust the random seed, decoding temperature, and prompt template to ensure diversity.Further details are provided in Appx.C.1 Results, Insights, and Future Directions.We use GPT-4o with temperature varying from 0.7 to 1.0 in multi-round synthetic generation.Table A6 presents the complete results.LLM-generated synthetic data effectively improves AD performance.Our results show that LLM-generated synthetic data significantly enhances AD performance for several detectors.As illustrated in Figure 2(a), models like AE, ECOD, LUNAR, and VAE achieve substantial AUROC and AUPRC improvements when synthetic samples are included alongside limited real data.Notably, these models often close the gap between limited-data performance and full-data performance, demonstrating that synthetic generation can effectively compensate for data scarcity.Performance impact varies across models.The effectiveness of synthetic generation is not consistent across all models.Methods relying on fixed geometric assumptions-such as DeepSVDD, iFor-  filters to steer generation (O'Neill et al., 2023), and incorporating human-in-the-loop interventions (Chung et al., 2023) to refine synthetic data quality and improve downstream AD performance.</p>
<p>Generating Category Descriptions for LLM-based Detectors</p>
<p>Problem 3 (Description DA via LLMs) Given category names C normal and, optionally, C anomaly , the objective is to generate comprehensive textual descriptions d normal and d anomaly using a pre-trained LLM f LLM .These descriptions are then incorporated into the prompts of LLM-based detectors, aiming to improve their performance compared to using category names alone.</p>
<p>Evaluation Protocol.Extending the zero-shot detection from §3, we employ LLMs to produce category descriptions that offer richer semantic signals beyond simple category names.Specifically, for each normal and anomaly category, we generate d normal and d anomaly based on the category names and the dataset's context.These descriptions can highlight distinctive features, typical lexical patterns, or behavioral characteristics that define normal or anomalous classes.By incorporating these descriptions into the prompt, we update Eq. ( 1) as:
P = T x i , C normal , d normal ,
C anomaly , d anomaly *</p>
<p>(2)</p>
<p>where (C anomaly , d anomaly ) * applies only in the "Normal + Anomaly" setting (see §3.2).By enriching category names with descriptions (highlighted with blue boxes), we enhance the LLM's ability to reason about subtle category distinctions.More details are provided in Appx.C.2. Results, Insights, and Future Directions.We utilize Llama 3.1, GPT-4o, and DeepSeek-V3 to generate category descriptions.We set the temperature = 0.5 to balance the diversity and precision.</p>
<p>Augmented descriptions improve LLM-based AD.</p>
<p>As shown in Table 2, incorporating category descriptions increases performance in most datasets.This suggests that the added semantic information helps LLM-based detectors discriminate anomalous samples more effectively.For example, in the "IMDB Reviews" dataset, providing richer textual representations of classes translates to noticeable gains in both metrics across LLMs.Future Direction 4: Select Representative Samples.An effective way to refine enhanced information is to ground it in representative samples from the dataset.Sampling strategies based on clustering (Axiotis et al., 2024) or diversity maximization (Moumoulidou et al., 2020)  5 Task 3: LLM for AD Model Selection</p>
<p>Motivation</p>
<p>Unsupervised model selection (UMS) is critical for identifying the most suitable AD model by aligning its features with the attributes of a given dataset and the task's requirements.Given the diverse range of AD models available and the absence of a universal solution, effective UMS is essential to ensure optimal performance.Traditional UMS methods often rely on historical performance data or domain-specific expertise; however, such data may be unavailable or irrelevant for novel or evolving datasets (Zhao et al., 2021;Zhao, 2024).</p>
<p>Inspired by recent research (Qin et al., 2024;Chen et al., 2024;Wei et al., 2025), LLMs offer a promising zero-shot alternative by utilizing their extensive pre-trained knowledge to analyze datasets and recommend suitable models without relying on past performance metrics.They can streamline the model selection process, reducing manual overhead and domain knowledge requirements while also improving adaptability to novel data scenarios.</p>
<p>Problem Statement and Designs</p>
<p>Problem 4 (Zero-shot UMS via LLMs) Given a dataset D = {x 1 , x 2 , . . ., x n } and a set of AD models M = {M 1 , M 2 , . . ., M m }, the task is to identify a suitable model M * ∈ M using a pretrained LLM f LLM , based solely on provided information about the dataset and the candidate models.</p>
<p>Evaluation Protocol.To enable LLM-based zeroshot UMS, we provide structured, detailed information of both the dataset and the candidate models: We then construct prompts that combine these datasets and model descriptions, asking the LLM to select and justify a recommended model.Further details about the prompt format and implementation can be found in Appx.D.</p>
<p>Results, Insights, and Future Directions</p>
<p>The UMS scenario requires sophisticated reasoning.We select recent enhanced reasoning models, including OpenAI-o1-preview and OpenAI-o1 (Ope-nAI, 2024c), OpenAI-o3-mini (OpenAI, 2025), and DeepSeek-R1 (Guo et al., 2025).LLM recommendations demonstrate strong potential.Figure 3 presents the model selection performance of four reasoning LLMs across five datasets, compared against two reference baselines: (i) the best result achieved by any baseline model, representing the performance upper bound; and (ii) the average performance of all baseline models, reflecting random model selection.In most cases, the AD performance of LLM-recommended models surpasses the average baseline and even approaches the best-performing model.These results highlight the strong potential of LLM-based reasoning to identify effective AD models using only public information, without reliance on historical performance or domain specialists.</p>
<p>Conclusion</p>
<p>In this work, we presented AD-LLM, the first comprehensive benchmark that integrates LLMs into three core aspects of anomaly detection in NLP: detection, data augmentation, and model selection.</p>
<p>Our results show that LLMs exhibit promising capabilities in zero-shot AD without task-specific training.LLM-generated synthetic data significantly boosted performance for models that learn flexible representations, while it may negatively impact models that rely on rigid geometric assumptions.Additionally, LLM-driven model selection frequently exceeded baseline performance, though explanations for these selections often lacked dataset-specific detail.</p>
<p>Future Directions</p>
<p>Future research should focus on improving contextual prompts to enhance zero-shot AD capabilities while considering the cost, developing methods to balance diversity and domain alignment in synthetic data generation, and increasing the specificity and interpretability of LLM-generated model selection justifications.Expanding the AD-LLM benchmark to include additional tasks and applications in different fields (Huang et al., 2024b;Li et al., 2024b) also represents a valuable direction for broadening its impact.</p>
<p>Broader Impact Statement</p>
<p>AD-LLM explores the use of LLMs in enhancing AD through zero-shot detection, data augmentation, and model selection.These contributions have the potential to significantly improve real-world AD systems in critical areas such as healthcare, finance, and cybersecurity.By enabling robust, adaptable, and efficient solutions for AD tasks, this research empowers practitioners to deploy systems responsive to novel challenges while reducing reliance on labeled data and extensive domain expertise.</p>
<p>Ethics Statement</p>
<p>This study adheres to ethical guidelines, emphasizing considerations around fairness, transparency, and privacy in developing and applying LLM-based AD systems.We emphasize the importance of evaluating and mitigating biases in LLM recommendations, ensuring that outputs are equitable and unbiased.Moreover, privacy is preserved by relying on public data and avoiding the collection of sensitive information.Also, note that we used ChatGPT exclusively to improve minor grammar in the final manuscript text.</p>
<p>Limitations</p>
<p>Despite promising results, several limitations remain.First, our evaluation is constrained to a narrow set of datasets with clear normal-anomaly distinctions, and our settings in AD and category descriptions in DA follow the structure of these datasets, limiting applicability to various domains with ambiguous anomaly definitions.Second, UMS depends on simplistic input data and matching mechanisms.Furthermore, biases in LLM recommendations, such as favoring well-documented or familiar models, need further investigation.Additionally, we do not explore few-shot learning or fine-tuning, which are widely adopted techniques for enhancing LLM performance and could offer valuable complementary insights for AD tasks.</p>
<p>A.2 Traditional Baselines Details</p>
<p>This study utilizes 18 traditional methods as baselines.We compare the performance of LLM-based anomaly detection methods with these baselines in §3 and further enhance the baselines with LLMgenerated synthetic data, demonstrating the effectiveness of augmentation in §4.2.These methods are categorized into two groups: • End-to-end Methods.These methods directly process raw text data to generate AD results:</p>
<p>-CVDD: Context Vector Data Description (Ruff et al., 2019).CVDD uses embeddings and selfattention to learn context vectors, detecting anomalies via deviations.-DATE: Detecting Anomalies in Text via Self-Supervision of Transformers (Manolache et al., 2021).DATE trains self-supervised transformers to identify anomalies in text.</p>
<p>• Two-Step Methods.These approaches first generate text embeddings using BERT (Kenton and Toutanova, 2019) or OpenAI's text-embedding-3large (OpenAI, 2024b) and then apply traditional AD techniques to the embeddings.</p>
<p>-AE: AutoEncoder (Aggarwal, 2015).AE uses high reconstruction errors to detect anomalies.</p>
<p>-DeepSVDD: Deep Support Vector Data Description (Ruff et al., 2018).DeepSVDD identifies anomalies outside a hypersphere that encloses normal data representations.-ECOD: Empirical-Cumulative-distributionbased Outlier Detection (Li et al., 2022).ECOD flags point in distribution tails using empirical cumulative distributions.-IForest: Isolation Forest (Liu et al., 2008)</p>
<p>B Additional Details for Task 1 B.1 Prompt Details</p>
<p>Prompt design is crucial for zero-shot LLM-based detection, as the performance heavily relies on its instructiveness and clarity.As discussed in §3.2 about LLM-based zero-shot AD, we evaluate two settings based on varying levels of prior knowledge in the real world: "Normal Only" and "Normal + Anomaly."The LLM prompt template for setting "Normal Only" is provided in Table A9, and the prompt template for setting "Normal + Anomaly" is presented in Table A10.The prompt templates of the two settings are different in the definition of anomaly, marked in red in Table A10.</p>
<p>We utilize a series of prompt engineering techniques, including: • Task Information (Cao et al., 2023).It is essential to provide clear task information.We carefully define the detection scenario, the anomaly definition, and the rules to reduce hallucinations.</p>
<p>• Chain-of-Thought (CoT) (Wei et al., 2022).CoT prompting encourages LLMs to decompose their reasoning into sequential intermediate steps and organize information logically.We explicitly provide a completed chain of thoughts in the prompt.• Explanation and Implicit CoT.We require an explanation r generated before the anomaly score s for each inquiry as shown in Eq. ( 1).When generating the explanation, LLMs implicitly create the CoT in the background (Liu et al., 2024d).This approach aligns with the auto-regressive nature of decoder-only LLMs, encouraging them to think carefully and logically before determining the anomaly score, thereby enhancing reliability.</p>
<p>In our experiments, we discovered that Llama 3.1 requires implicit CoT.Presenting the anomaly score s before the explanation r causes the Llama 3.1-based detector to crash and consistently outputs s = 0.This issue does not impact GPT-4o and DeepSeek-V3.We attribute this to their significantly larger parameter count, which grants it a stronger resilience to prompt changes.</p>
<p>B.2 Complete Baseline Results</p>
<p>In addition to the top two baseline results in §3.3, we provide the complete results for all 18 baseline methods in Table A8.We observe that Llama 3.1 outperforms most of these baselines, further supporting the efficacy of zero-shot AD via LLMs shown in Table 1.The baseline results presented in this study are derived from Li et al. (2024c).</p>
<p>B.3 Error Analysis</p>
<p>In our experiments, LLMs occasionally encountered errors.Llama 3.1 sometimes entered infinite loops, repeatedly generating the same sentence until reaching the token limit, as illustrated in the AG News example shown in Text Box B.3, where the repetitive nonsensical response is highlighted in italics.However, the underlying causes of such behavior remain underexplored.DeepSeek-V3 sometimes struggles to follow instructions, resulting in incorrect JSON formatting or failing to return any output.Additionally, it occasionally experiences internal server errors.For GPT-4o, outputs were sometimes restricted by OpenAI's safety filters when addressing potentially harmful content, such as politically sensitive topics.</p>
<p>An infinite loop example in Llama 3.1</p>
<p>Text sample: "This week's TravelWatch column profiles Anangu Tours, an Aborigine owned tour company in Australia's Red Center."Response: {"reason": "The text sample is about a tour company in Australia, which relates to travel and geography, making it most closely align with the Sports category is not the best fit, but it does not fit into the Business or Sci/Tech categories, but it does fit into the Sports category is not the best fit, but it does fit into the category of travel which is related to Sports, but more closely related to the category of travel which is not listed, but the closest is Sports, but it is more closely related to the category of travel which is not listed, but the closest is Sports, but it is more closely related to the category of travel which is not listed, but the closest is Sports, but it is more closely related to the category of travel which is not listed, but the closest is Sports... We exclude these errors from our results but provide the error counts in both "Normal Only" and "Normal + Anomaly" settings for reference in Table A3 for Llama 3.1, Table A4 for GPT-4o, and Table A5 for DeepSeek-V3.Notably, the error counts vary between the two settings, suggesting that the triggers for errors, such as Llama's infinite loop or GPT-4o's safety filters, are sensitive to prompt variations.This occurs even though the prompts in both settings have similar semantic meanings.</p>
<p>B.4 Verbal Score</p>
<p>In the zero-shot AD task, we utilize LLMgenerated verbal anomaly scores as a signal for detection.Verbalization methods are widely used because they offer an intuitive and straightforward estimation (Xia et al., 2025).However, LLMs can often be overconfident in their responses due to the influence of reinforcement learning from human feedback (RLHF) (Kadavath et al., 2022).</p>
<p>C Additional Details for Task 2
∈ C normal = C 1 normal , . . . , C k normal ,
where k is the number of normal categories.</p>
<p>We employ a multi-step strategy with multiple rounds to mitigate repetitive outputs, token limit constraints, and difficulties in handling long contexts.The detailed pipeline is outlined below: 1. Keywords Generation.To ensure a consistent synthetic data distribution compared with the original training data, t groups of keywords are generated for each normal category C j normal .We construct the prompt P keywords using a template T keywords (•) as shown in Table A11
D synth = {x 1 , x2 , . . . , xt×k }.
The pipeline is formally summarized as follows:
P keywords = T keywords ({name}, {original_task}) K = f LLM P keywords = {K 1 , . . . , K t×k } P synth = T synth (K 1 ) , . . . , T synth (K t×k ) D synth = f LLM P 1 synth , . . . , f LLM P t×k synth
The prompt templates T keywords and T synth leverage the prompt techniques, including task information and CoT, as discussed in §B.1.</p>
<p>C.1.2 Experiments Details and Challenges</p>
<p>We set the number of samples from each normal category C j normal in the limited training set D small_train to v = 10.Similarly, the number of synthetic samples generated for each normal category C j normal in the synthetic set D synth is t = 50 for the "AG New", "BBC News", "IMDB Reviews", and "SMS Spam" datasets.For the "N24 News" dataset, we set v = 3 and t = 30 due to its numerous normal categories.</p>
<p>We use GPT-4o for synthetic data generation.We observed that increasing t occasionally causes them to terminate the keyword generation process before reaching the token limit.A similar issue occurs with Llama 3.1, even for smaller values of t.As a result, Llama 3.1 is excluded from this task.We presume these issues stem from the inherent challenges LLMs face in processing long contexts.We also exclude DeepSeek-V3 due to its unsatisfactory results.</p>
<p>Table A6: Performance comparison of AD baselines with and without LLM-generated synthetic data across five datasets.We also show the average performance ± its standard deviation over five datasets.The better results for each detector are highlighted in bold.The performance may vary due to the embedding changes.We repeat the generation of keywords four times, with different temperatures [1.0, 0.9, 0.8, 0.7] and different seeds [42,43,44,45].To further avoid repetition, we add additional sentences to the end of the prompts, including:</p>
<p>•"This is the first time you do this task, good luck!" •"You've completed this task before, and you're improving at it."•"After doing this task twice, you have a better understanding of it."•"You have done this task three times, you are now an expert at it."We carefully examine and remove duplicate keyword groups.Out of 200 generations, there are typically fewer than 5 repeated groups, with a maximum of 15.It shows that our method is effective.</p>
<p>C.1.3 Complete Results</p>
<p>The detailed results are provided in Table A6.These additional experiments on baselines follow the settings used in Li et al. (2024c), except that the "batch_size" is set = 4 due to the amount of D small_train in AE, VAE, and DeepSVVD.</p>
<p>C.1.4 Edges over LLM-based Zero-shot AD</p>
<p>At first glance, LLM-based zero-shot AD could eliminate the need to generate synthetic datasets for traditional models.However, they address different needs and offer complementary advantages.LLMbased zero-shot detection requires no task-specific training, offering easy deployment, adaptability across scenarios, and real-time inference-ideal for dynamic environments.However, its high computational cost can limit scalability for long-term or large-scale use.</p>
<p>In contrast, LLM-generated synthetic data enables the training of traditional models, significantly reducing inference costs for long-term or high-frequency detection tasks.Moreover, synthetic data can be a valuable resource for finetuning LLMs (Xu et al., 2024;Mitra et al., 2023).This dual utility highlights the importance of synthetic data generation as both a complementary and cost-efficient solution in the AD ecosystem.</p>
<p>C.2 Genarating Category Description Details</p>
<p>C.2.1 Prompt Details</p>
<p>As discussed in §4.3, we generate category descriptions to enhance LLM-based zero-shot AD.The prompt template used for generating category descriptions is shown in Table A13.It leverages the prompt techniques, including task information and CoT, as discussed in §B.1.</p>
<p>D Additional Details for Case Study 3 D.1 Evaluation Protocol and Prompt Details</p>
<p>As discussed in §4.3, we utilize the information of both dataset and candidate models to achieve UMS.The prompt template used for generating category descriptions is shown in Table A13.Importantly, we restrict our selection to two-step methods mentioned in §A.2, as the structural differences between end-to-end and two-step methods introduce additional complexities to an already challenging task.</p>
<p>D.2 Failures on Popular LLMs</p>
<p>Despite the promising results achieved with GPT-o1-preview, widely used LLMs like GPT-4o and Llama 3.1 struggle with zero-shot UMS, frequently recommending the same model regardless of dataset context.This limitation highlights the need for enhanced reasoning abilities to better analyze dataset-specific requirements, model strengths and weaknesses, and their overall compatibility.</p>
<p>D.3 Complete Results</p>
<p>The detailed results with precise numerical values are provided in Table A7 for reference.</p>
<p>Table A8: Performance comparison of LLM-based detectors and baseline methods across five datasets.LLM-based detectors are evaluated under two settings as described in §3.2 with AUROC and AUPRC as the metrics (higher (↑), the better).The best results are highlighted in bold, the second-best results are double-underlined, and the third-best results are single-underlined.</p>
<p>Table A9: LLM prompt template used for zero-shot AD in "Normal Only" setting discussed in §3.2.{normal_category_x} refers to the name of x th normal category.{text} represents the test sample to be detected.</p>
<p>You are an intelligent and professional assistant that detects anomalies in text data.## Task: -Following the rules below, determine whether the given text sample is an anomaly.Provide a brief explanation of your reasoning and assign an anomaly confidence score between 0 and 1. -Provide responses in a strict <strong>JSON</strong> format with the keys "reason" and "anomaly_score."-"reason": Your brief explanation of the reasoning in one to three sentences logically.</p>
<p>-"anomaly_score": Your anomaly confidence score between 0 and 1. -Ensure the JSON output is correctly formatted, including correct placement of commas between key-value pairs.</p>
<p>-Add a backslash () before any double quotation marks (") within the values of JSON output for proper parsing (i.e., from " to \"), and ensure that single quotation marks (') are preserved without escaping.Text sample: "{text}" Response in JSON format: Table A10: LLM prompt template used for zero-shot AD in "Normal + Anomaly" setting discussed in §3.2.{normal_category_x} refers to the name of x th normal category and {anomaly_category} refers to the name of anomaly category.{text} represents the test sample to be detected.The different part compared with the prompt in the "Normal Only" setting is marked in red.</p>
<p>You are an intelligent and professional assistant that detects anomalies in text data.## Task: -Following the rules below, determine whether the given text sample is an anomaly.Provide a brief explanation of your reasoning and assign an anomaly confidence score between 0 and 1. -Provide responses in a strict <strong>JSON</strong> format with the keys "reason" and "anomaly_score."-"reason": Your brief explanation of the reasoning in one to three sentences logically.</p>
<p>-"anomaly_score": Your anomaly confidence score between 0 and 1. -Ensure the JSON output is correctly formatted, including correct placement of commas between key-value pairs.</p>
<p>-Add a backslash () before any double quotation marks (") within the values of JSON output for proper parsing (i.e., from " to \"), and ensure that single quotation marks (') are preserved without escaping.Text sample: "{text}" Response in JSON format: Table A11: LLM prompt template used for keyword generation, which is the first step of generating synthetic samples as discussed in §4.2.{normal_category_x} refers to the name of x th normal category.{name} and {original_task} can be found in Tab.A.1.{num_keyword_groups} set the number of keyword groups that LLM needs to generate for each category.</p>
<p>You are an intelligent and professional assistant that generates groups of keywords for given categories in a dataset.## Task: -Following the rules below, generate <strong>exactly</strong> {num_keyword_groups} unique keyword groups for <strong>each given category</strong> according to your understanding of the category (and its description).</p>
<p>-Each keyword group will be used to generate synthetic data for the corresponding category.## Rules: 1. <strong>Keyword Group Generation</strong>:</p>
<p>-For <strong>each given category</strong>, generate <strong>exactly</strong> {num_keyword_groups} keyword groups.Each group should contain exactly three keywords, with different levels of granularity: one broad/general, one intermediate, and one fine-grained.</p>
<p>-Ensure that the three keywords in each group are thematically related to each other and align with the category's description.</p>
<p>-Avoid redundancy or overly similar keywords across different groups.</p>
<p>-Ensure that each group is unique and relevant to the key topics described in the category.2. <strong>Granularity</strong>:</p>
<p>-The first keyword should be broad/general, representing a high-level or overarching topic.</p>
<p>-The second keyword should be intermediate, more specific than the first, but not overly narrow.</p>
<p>-The third keyword should be fine-grained and specific, related to detailed subtopics or precise aspects of the category.3. <strong>Response Format</strong>:</p>
<p>-For each given category, provide the keyword groups as a list, where each entry is a group of three keywords (broad, intermediate, fine-grained).</p>
<p>-Structure the response so that the key is the category name, and the value is a list of generated keyword groups.</p>
<p>-Ensure the JSON output is properly formatted, including correct placement of commas between key-value pairs and no missing brackets.</p>
<p>-Add a backslash () before any double quotation marks (") within the values of JSON output for proper parsing (i.e., from " to \"), and ensure that single quotation marks (') are preserved without escaping.</p>
<p>The "{name}" dataset's original task is {original_task}.-Generate a synthetic text sample that naturally incorporates the three provided keywords (broad, intermediate, and fine-grained).</p>
<p>-Ensure that the text sample is coherent and contextually relevant to the themes suggested by the keywords.2. <strong>Keyword Usage</strong>:</p>
<p>-The three keywords must appear naturally within the content.</p>
<p>-Ensure that the broad keyword sets the overall context, the intermediate keyword refines the discussion, and the fine-grained keyword offers more detailed insight into a specific subtopic.3. <strong>Response Format</strong>:</p>
<p>-Provide the generated sample as a single string response representing the text sample.</p>
<p>-Ensure the output is in a readable format.</p>
<p>-Do not include any additional messages or commentary.</p>
<p>-Add a backslash () before any double quotation marks (") within the values of JSON output for proper parsing (i.e., from " to \"), and ensure that single quotation marks (') are preserved without escaping.</p>
<p>The "{name}" dataset's original task is {original_task}.The category is "{category}", and the group of keywords to use is: -Broad: {keyword_group[0]} -Intermediate: {keyword_group[1]} -Fine-grained: {keyword_group[2]} Response in JSON format: Table A13: LLM prompt template for generating category descriptions discussed in §4.3.{normal_category_x} refers to the name of x th normal category and {anomaly_category} refers to the name of anomaly category.{name} and {original_task} can be found in Tab.A.1.</p>
<p>You are an intelligent and professional assistant that generates descriptions for given categories in a text dataset.## Task: -Following the rules below, generate detailed textual descriptions that explain the main characteristics, typical topics, and common examples for each given category.## Rules: 1.For each category, provide a continuous, coherent description in a single paragraph that includes:</p>
<p>-<strong>Definition or overview</strong>: Start by briefly defining or describing the category in one to two sentences.If you list multiple aspects or features in the definition (such as related fields or industries), ensure you append expressions like "etc." or "and so on" to indicate that the list is not exhaustive.</p>
<p>-<strong>Main topics or subjects</strong>: Highlight the typical topics or subjects covered by this category.Ensure that you use phrases like "etc." or "and so on" at the end of each list to indicate that the list is not exhaustive.</p>
<p>-<strong>Relevant examples</strong>: Mention examples of content that belong to this category.Also, use expressions like "etc." or "and so on" at the end of the list to show that these are illustrative, not exhaustive.2. Use <strong>step-by-step reasoning</strong> to ensure the descriptions are logical and clear.3.Each description should be clear, coherent, and helpful for someone unfamiliar with the dataset and the task.4. Always append phrases like "etc." or "and so on" to lists or enumerations of examples, topics, or aspects, <strong>including the definition part</strong>.</p>
<p>Response Format:</p>
<p>-Provide a response where each key is the category name, and the value is the corresponding description as a continuous paragraph.</p>
<p>-Ensure the JSON output is correctly formatted, including correct placement of commas between key-value pairs.</p>
<p>-Add a backslash () before any double quotation marks (") within the values of JSON output for proper parsing (i.e., from " to \"), and ensure that single quotation marks (') are preserved without escaping.</p>
<p>The "{name}" dataset's original task is {origianl_task}.It contains the following categories: {normal_category_1} {normal_category_2} ... {anomaly_category}</p>
<p>Response in JSON format:</p>
<p>Table A14: LLM prompt template used for UMS discussed in §5.{normal_category_x} refers to the name of x th normal category and {anomaly_category} refers to anomaly one.We randomly select examples from the training set for both normal and anomaly data, denoted as {normal_text} and {anomaly_text}.{name}, {size} (i.e., # of test set), and {original_task} can be found in Tab.A.1.{avg_len}, {max_len}, {min_len}, and {std_len} are statistics of datasets as shown in Tab.A1. {abstract} is the abstract in the published paper of each model.</p>
<p>You are an expert in model selection for anomaly detection on text datasets.## Task: -Given the information of a dataset and a set of models, select the model you believe will achieve the best performance for detecting anomalies in this dataset.Provide a brief explanation of your choice.</p>
<h2>Dataset Information: -Dataset Name: {name} -Dataset Size: {size} -Background: This dataset is originally for {original_task}.</h2>
<p>-Data Structure: Textual data with multiple categories.One category is considered anomalous, while the others are normal.</p>
<p>-Normal Category(ies): {normal_category_1}, {normal_category_2} -Models utilize language models to generate embeddings and feed the embeddings into the models.</p>
<p>-We provide the abstracts of the papers that introduce the models for your reference.-"text-embedding-3-large" from OpenAI (referred to as OpenAI): {abstract} (OpenAI, 2024b) ## Rules: 1. Availabel options include "BERT+AE", "BERT+DeepSVDD", "BERT+ECOD", "BERT+iForest", "BERT+LOF", "BERT+LUNAR", "BERT+SO-GAAL", "BERT+VAE", "OpenAI+AE", "Ope-nAI+DeepSVDD", "OpenAI+ECOD", "OpenAI+iForest", "OpenAI+LOF", "OpenAI+LUNAR", "OpenAI+SO-GAAL", "OpenAI+VAE."2. Treat all models equally and evaluate them based on their compatibility with the dataset characteristics and the anomaly detection task.</p>
<p>Response Format:</p>
<p>-Provide responses in a strict <strong>JSON</strong> format with the keys "reason" and "choice."-"reason": Your explanation of the reasoning.</p>
<p>-"choice": The model you have selected for anomaly detection in this dataset.</p>
<p>Response in JSON format:</p>
<p>Figure 1 :
1
Figure 1: AD-LLM examines how LLMs contribute to three key AD tasks: (Task 1, §3) Zero-shot detection (left), where LLMs directly identify anomalies and provide explanations without task-specific training data; (Task 2, §4) Data augmentation (center), where LLMs generate synthetic samples and produce category descriptions to alleviate data scarcity and improve semantic reasoning; and (Task 3, §5) Model selection (right), where LLMs analyze dataset attributes and model descriptions to recommend suitable AD models along with justifications.</p>
<h1></h1>
<p><em>Anomaly Definition</em><em>: -A text sample is considered an </em><em>anomaly</em><em> if it does </em><em>not</em><em> belong to </em><em>any of the categories</em><em> listed above.2. </em><em>Scoring</em><em>: -Assign an anomaly confidence score between 0 and 1. -Use higher scores when you are highly confident in your decision.-Use lower scores when you are uncertain or think the text sample is </em><em>not</em><em> an anomaly.3. </em><em>Step-by-step Reasoning</em><em> (Chain of Thought): -</em><em>Step 1</em><em>.Read the entire text sample carefully and understand it thoroughly.-</em><em>Step 2</em><em>.Analyze the text sample by comparing its content to each category listed in the "Categories" section above, considering factors such as main topics, meanings, background, sentiments, etc. -</em><em>Step 3</em><em>.Determine which category the text sample </em><em>most closely aligns with</em><em>.-If it aligns with any category, it is </em><em>not</em><em> an anomaly.-If it does </em><em>not</em><em> align with any category, it is an anomaly.-</em><em>Step 4</em><em>.Assign an anomaly confidence score based on how confident you are that the text sample is an anomaly.4. </em><em>Additional Notes</em><em>:-A text sample may relate to multiple categories, but it should be classified into the </em><em>most relevant</em><em> one in this task.-Ifyou are uncertain whether the text sample </em><em>significantly aligns</em><em> with </em><em>any of the anomaly category(ies)</em><em>, assume that it does </em><em>not</em><em>, which means it is </em><em>not</em><em> an anomaly. 5. </em><em>Response Format</em>*:</p>
<h1></h1>
<p><em>Anomaly Definition</em><em>: -A text sample is considered an </em><em>anomaly</em><em> if it belongs to the </em><em>anomaly category(ies)</em><em> rather than </em><em>any of the normal category(ies)</em><em> listed above.2. </em><em>Scoring</em><em>: -Assign an anomaly confidence score between 0 and 1. -Use higher scores when you are highly confident in your decision.-Use lower scores when you are uncertain or think the text sample is </em><em>not</em><em> an anomaly.3. </em><em>Step-by-step Reasoning</em><em> (Chain of Thought): -</em><em>Step 1</em><em>.Read the entire text sample carefully and understand it thoroughly.-</em><em>Step 2</em><em>.Analyze the text sample by comparing its content to each category listed in the "Categories" section above, considering factors such as main topics, meanings, background, sentiments, etc. -</em><em>Step 3</em><em>.Determine which category the text sample </em><em>most closely aligns with</em><em>.-If it </em><em>most closely aligns with</em><em> </em><em>any of the anomaly category(ies)</em><em>, it is an </em><em>anomaly</em><em>.-If it </em><em>most closely aligns with</em><em> </em><em>any of the normal category(ies)</em><em> instead, it is </em><em>not</em><em> an anomaly.-</em><em>Step 4</em><em>.Assign an anomaly confidence score based on how confident you are that the text sample is an anomaly.4. </em><em>Additional Notes</em><em>:-A text sample may relate to multiple categories, but it should be classified into the </em><em>most relevant</em><em> one in this task.-Ifyou are uncertain whether the text sample </em><em>significantly aligns</em><em> with </em><em>any of the anomaly category(ies)</em><em>, assume that it does </em><em>not</em><em>, which means it is </em><em>not</em><em> an anomaly. 5. </em><em>Response Format</em>*:</p>
<h3>Model Options: -AutoEncoder (AE): {abstract} (Aggarwal, 2015) -Deep Support Vector Data Description (DeepSVDD): {abstract} (Ruff et al., 2018) -Empirical-Cumulative-Distribution-Based Outlier Detection (ECOD):{abstract} (Li et al., 2022) -Isolation Forest (IForest): {abstract} (Liu et al., 2008) -Local Outlier Factor (LOF): {abstract} (Breunig et al., 2000) -Unifying Local Outlier Detection Methods via Graph Neural Networks (LUNAR): {abstract} (Goodge et al., 2022) -Single-Objective Generative Adversarial Active Learning (SO-GAAL): {abstract} (Liu et al., 2019) -Variational AutoEncoder (VAE): {abstract} (Kingma and Welling, 2014) ### Embedding Options: -Bidirectional Encoder Representations from Transformers (BERT): {abstract} (Kenton and Toutanova, 2019)</h3>
<p>Table 1 :
1
Performance comparison of LLM-based detectors and baseline methods across five datasets, evaluated under two settings as described in §3.2 with AUROC and AUPRC as the metrics (higher (↑), the better).Complete results are provided in Appx.A2.The best results are highlighted in bold, and the second-best results are underlined.
SettingsAG NewsBBC NewsIMDB ReviewsN24 NewsSMS SpamLlama 3.1 8B Instruct(1) with C normal0.82260.40360.79100.36020.73730.34740.62670.11300.75580.2884(2) with C normal , C anomaly 0.87540.39980.86120.39600.86250.46060.87840.38020.94870.6361GPT-4o(1) with C normal0.93320.72070.95740.84320.93490.78230.76740.32520.79400.5568(2) with C normal , C anomaly 0.92930.63100.99190.90880.96680.84650.99020.90090.98620.8953DeepSeek-V3(1) with C normal0.91040.64420.82060.56040.85440.68080.82070.44950.87970.5963(2) with C normal , C anomaly 0.92730.78170.95810.89720.96260.85690.95140.77300.95350.7914Best BaselinesOpenAI + LUNAR OpenAI + LUNAR 0.9226 0.6918 0.9732 0.8653OpenAI + ECOD 0.7366 0.5165OpenAI + LUNAR 0.8320 0.4425DATE 0.9398 0.6112Second-best BaselineOpenAI + LOF 0.8905 0.5443OpenAI + LOF 0.9558 0.7714OpenAI + DeepSVDD 0.6563 0.3278OpenAI + LOF 0.7806 0.2248OpenAI + LOF 0.7862 0.2450Llama 3.1 shows competitive performance whenanomaly information is available. Despite operat-ing with limited prior information, LLMs exhibitsignificant potential for anomaly detection tasks.These results highlight the strength of LLMs inzero-shot AD scenarios.Additional context helps. Table 1 shows that LLM-based detectors achieve improved AUROC andAUPRC when transitioning from setting "NormalOnly", which uses only C normal , to setting "Nor-mal + Anomaly", which includes both C normal andC anomaly . These results indicate that richer contex-tual information improves the LLMs' ability todistinguish anomalous samples and enhances de-tection performance.Future Direction 1: Improve Context Integration.Providing additional context improves detection, asseen in "with C normal , C anomaly ." Future work mayinvolve more systematic ways to integrate domain-specific details, such as prompt design or retrieval-augmented methods
AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑</p>
<p>Given a small training set D small_train = {x 1 , x 2 , . . ., x m } of normal samples, the goal is to produce a synthetic dataset D synth = {x 1 , x2 , . . ., xn } using a pre-trained LLM f LLM .The combined dataset D DA = D small_train ∪ D synth is used to train an unsupervised AD method M , improving performance compared to using D small_train alone.
. Thus, We examine two approaches that address data scarcity and improve semantic reasoning: 1. ( §4.2) generates synthetic samples to improve training-based AD models. 2. ( §4.3) produces category descriptions to refine prompts and enhance LLM-based detectors. 4.2 Generating Synthetic Samples for Training-based AD Models Problem 2 (Synthetic DA via LLMs) Evaluation Protocol. To evaluate the impact of LLM-generated synthetic data, we set unsupervised AD baselines listed in Appx. A.2 in a scenario with limited training data. LLMs are then utilized to gen-erate a synthetic training dataset. However, direct prompting often leads to highly repetitive outputs, even with high decoding temperatures</p>
<p>Table 2 :
2
Performance (and △ changes)of LLM-based detectors with augmented descriptions under two settings in §3.2.The description generators and LLM-based detectors adopt the same backbone.Values in brackets indicate changes compared to the results in Table1.Green denotes for improvements and red for declines.Changes below 0.03 are not colored for better visualization, also reflecting minor fluctuations.
SettingsAG NewsBBC NewsIMDB ReviewsN24 NewsSMS SpamLlama 3.1 8B Instruct(1) with C normal0.8081 (-0.0145) (-0.0448) (-0.0108) (-0.0596) (+0.1666) (+0.2798) (+0.0384) (+0.0253) (-0.0102) (-0.0659) 0.3588 0.7802 0.3006 0.9039 0.6272 0.6651 0.1383 0.7456 0.2225(2) with C normal , C anomaly0.9046 (+0.0292) (+0.1099) (+0.0477) (+0.2571) (+0.0726) (+0.1763) (-0.0884) (-0.1406) (-0.0074) (+0.0657) 0.5097 0.9089 0.6531 0.9351 0.6369 0.7900 0.2396 0.9413 0.7018GPT-4o(1) with C normal0.9255 (-0.0077) (-0.0222) (+0.0037) (-0.0270) (+0.0223) (+0.0484) (+0.1118) (+0.2147) (+0.0425) (-0.0803) 0.6985 0.9611 0.8162 0.9572 0.8307 0.8792 0.5399 0.8365 0.4765(2) with C normal , C anomaly0.9331 (+0.0038) (+0.0349) (-0.0070) (-0.0090) (+0.0187) (+0.0754) (-0.0007) (-0.0329) (-0.0062) (-0.0064) 0.6659 0.9849 0.8998 0.9855 0.9219 0.9895 0.8680 0.9800 0.8889DeepSeek-V3(1) with C normal0.8791 (-0.0482) (-0.1262) (+0.0594) (+0.0566) (+0.1068) (+0.1080) (+0.0054) (-0.0546) (+0.0465) (+0.0165) 0.5180 0.8800 0.6170 0.9612 0.7888 0.8261 0.3949 0.9262 0.6128(2) with C normal , C anomaly0.9231 (-0.0042) (-0.1325) (-0.0004) (+0.0134) (+0.0167) (+0.0672) (+0.0083) (+0.0342) (-0.0013) (+0.0783) 0.6492 0.9577 0.9106 0.9793 0.9241 0.9591 0.8072 0.9522 0.8697
AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑</p>
<p>Table 3 :
3
The top 2 frequent picks made by each LLM.Counts are aggregated over 25 queries (5 per dataset).
LLMsTop-2 Picks (counts)OpenAI-o1-preview OpenAI+LUNAR (13), OpenAI+ECOD (8)DeepSeek-R1OpenAI+ECOD (16), OpenAI+LUNAR (6)OpenAI-o1OpenAI+DeepSVDD (11), OpenAI+iForest (7)OpenAI-o3-miniBERT+DeepSVDD (10), OpenAI+ECOD (6)• Dataset Description: dataset name, size, back-ground, normal and anomaly categories, text-length statistics (average, maximum, minimum,and standard deviation), and representative sam-ples of both normal and anomalous data. Theseattributes help the LLM understand the dataset'sstructure, complexity, and potential challenges,and are generally easy to obtain for new datasets.
• Model Description: abstracts from published AD papers describing each candidate model.These abstracts highlight key model features, underlying assumptions, and targeted use cases.By examining these summaries, the LLM can align dataset attributes with model strengths, improving the relevance of its recommendations.</p>
<p>Table 4 :
4
Selections by each LLM without any dataset or model context.Each LLM was queried five times.
tant steps to improve the fairness and reliability ofLLM-based UMS (Dai et al., 2024).LLMsContext-Free Picks (counts)Future Direction 6: Enhancing Interpretability.Improving LLMs' capacity to produce transparent,OpenAI-o1-preview OpenAI+LUNAR (3), OpenAI+VAE (2) DeepSeek-R1 OpenAI+LUNAR (3), OpenAI+ECOD (2) OpenAI-o1 OpenAI+VAE (4), OpenAI+LUNAR (1)dataset-tailored justifications for model selection decisions is key (Huang et al., 2024a). TechniquesOpenAI-o3-miniOpenAI+VAE (4), OpenAI+DeepSVDD (1)such as fine-tuning with richly annotated explana-tions or using prompt engineering to explicitly re-LLMs exhibit inherent yet context-sensitive se-quest structured reasoning can encourage the LLMlection biases. The aggregated selection re-to articulate clear, context-sensitive arguments.sults in Table 3 highlight distinct model-selectionpreferences among the four LLMs. For exam-ple, OpenAI-o1-preview often recommends Ope-nAI+LUNAR, a consistently strong model in ourbenchmarks. In contrast, OpenAI-o3-mini prefersBert+DeepSVDD, a generally weaker option. Toinvestigate whether these biases arise from inter-nal priors or context-specific information, we con-ducted a selection experiment without any datasetor model details (Table 4). Although each LLMstill favors a distinct, limited set of models evenwithout context, their specific preferences notablyshift when introducing the context. This indicatesthat while LLMs possess intrinsic biases from theirpretraining or tuning phases, their selections arealso influenced by provided context information.Context information improves selections, but justi-fications remain generic. When comparing modelselection results with and without context (Table 3-Table 4), we notice a clear shift in model recom-mendations that generally better align with bench-mark results. Despite this improved selection accu-racy, its explanations often remain generic and donot clearly link model selection to specific datasetcharacteristics. For example, in the "AG News"dataset, the OpenAI-o1-preview alternated betweenrecommending "OpenAI + LUNAR" and "Ope-nAI + ECOD," justifying choices with broad state-ments like "effective for high-dimensional data"or "parameter-free scalability." Such non-specificrationales diminish interpretability and user trust,especially when understanding the rationale behindmodel choice is important.Future Direction 5: Refine Input Specificity andAlleviate Biases. Future work should explore howto provide more dataset-specific details and miti-gate potential LLM biases. Ambiguous or incom-plete input information may cause the LLM to fa-vor well-known models or those frequently encoun-tered during training. Ensuring detailed and bal-anced inputs, and exploring how inherent biasesin LLMs affect recommendations, will be impor-</p>
<p>Table A2 :
A2
Detailed information of five datasets used in AD-LLM, including the original task, normal category(ies), anomaly category, the size of the training set, the size, and the anomaly ratio of the test set.
DatasetOriginal TaskNormal Category(ies)Anomaly Category # Train # Test % AnomalyAG NewsAG news topics classificationSports, Business, Sci/TechWorld66,098 32,10911.77%BBC NewsBBC news topics classification Business, Politics, Sport, Tech Entertainment1,20657910.71%IMDB Reviews binary sentiment classification PositiveNegative17,417 8,95216.61%of IMDb movie reviewsN24 NewsNew York Times newsTelevision, Your Money,Food40,569 19,2279.51%classificationAutomobiles, Science,Economy, Dance, Travel,Technology, Sports, Movies,Music, Real Estate, Books,Education, Art &amp; Design,Theater, Media, Style,Global Business, Well,Health, Fashion &amp; Style,OpinionSMS Spammobile phone SMS spamNon-spam (Ham)Spam3,162 1,51010.20%messages detection</p>
<p>Table A3 :
A3
Error count in Llama 3.1
Dataset"Normal Only" "Normal + Anomaly"AG News55248BBC News03IMDB Reviews2129N24 News299898SMS Spam02Dataset"Normal Only" "Normal + Anomaly"AG News10BBC News00IMDB Reviews19N24 News00SMS Spam00</p>
<p>Table A4 :
A4
Error count in</p>
<p>Table A5 :
A5
. This template utilizes {name} and {original_task} Error count in DeepSeek-V3 information from TableA.1.The prompt P keywords is processed by the LLM f LLM (•) to produce t × k groups of keywords K = {K 1 , K 2 , ..., K t×k }.Each keyword group K i contains three keywords with increasing levels of granularity from coarse to fine.2.Synthetic Sample Generation.We iterate the groups of keywords, constructing P synth = synth (•) as displayed in TableA12.Each prompt P j synth is fed into the LLM f LLM (•) to generate a corresponding synthetic sample xj .Finally, we obtain a synthetic dataset
Dataset"Normal Only" "Normal + Anomaly"AG News1415BBC News31IMDB Reviews20616N24 News138252SMS Spam50P 1 synth , P 2 synth , . . . , P t×k synthusing a templateT</p>
<p>AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑
Training SetAG News AUROC ↑ OpenAI + AE BBC News IMDB Reviews N24 NewsSMS SpamAverage Performancewithout D synth 0.50540.11890.60160.13090.50140.16650.71190.16810.50000.1020 0.5641±0.0834 0.1373±0.0262with D synth 0.80970.32900.84340.39360.80970.32900.80970.32900.43410.0852 0.7413±0.1542 0.2932±0.1069OpenAI + DeepSVDDwithout D synth 0.51710.12370.61270.14150.56670.19690.62780.15110.63980.1479 0.5928±0.0453 0.1522±0.0243with D synth 0.55540.13650.58670.12670.55540.13650.55540.13650.30860.0681 0.5123±0.1026 0.1209±0.0267OpenAI + ECODwithout D synth 0.50140.11800.56230.12080.50000.16610.62020.13110.40780.0789 0.5183±0.0709 0.1230±0.0279with D synth 0.67090.19540.76600.32100.67090.19540.67090.19540.33510.0708 0.6228±0.1485 0.1956±0.0791OpenAI + IForestwithout D synth 0.61200.16200.71020.19030.57880.19470.53310.10100.63860.1467 0.6145±0.0594 0.1589±0.0340with D synth 0.67590.21590.66550.21070.67590.21590.67590.21590.27000.0649 0.5926±0.1614 0.1847±0.0599OpenAI + LOFwithout D synth 0.64040.16610.71280.25650.67590.24850.71790.20610.75820.2445 0.7010±0.0400 0.2243±0.0339with D synth 0.54690.14110.65130.20750.54690.14110.54690.14110.81500.2602 0.6214±0.1049 0.1782±0.0484OpenAI + SO_GAALwithout D synth 0.56570.13240.32400.07700.53880.16590.33510.06540.39530.0823 0.4318±0.1017 0.1046±0.0383with D synth 0.44610.09760.27870.07030.44610.09760.44610.09760.06980.0637 0.3374±0.1487 0.0854±0.0151OpenAI + LUNARwithout D synth 0.65270.20350.85540.46700.65460.23150.78790.24730.15060.0573 0.6202±0.2475 0.2413±0.1314with D synth 0.86510.42280.93300.73320.86510.42280.86510.42280.13750.0568 0.7332±0.2990 0.4117±0.2143OpenAI + VAEwithout D synth 0.68570.18420.71430.18160.50310.16700.69320.16980.50000.1020 0.6193±0.0966 0.1609±0.0302with D synth 0.79050.36540.76740.26540.79050.36540.79050.36540.06960.0545 0.6417±0.2862 0.2832±0.1207</p>
<p>Table A7 :
A7
Complete model selection results across five datasets.We display the average AUROC and AUPRC of models recommended by querying each reasoning LLM five times (duplicates allowed)."Best Performance" marks the highest performance achieved by any baseline model for each dataset, while "Average Performance" denotes the mean performance across all baseline models.AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑ AUROC ↑ AUPRC ↑
AG News AUROC ↑ OpenAI-o1 Settings 0.7132 0.3199BBC News 0.6798 0.2831IMDB Reviews 0.6563 0.3278N24 News 0.7091 0.2419SMS Spam 0.3647 0.0752OpenAI-o1-preview 0.89080.61930.69920.22140.66520.27870.77060.34220.57740.1220OpenAI-o3-mini0.64550.24010.73290.31320.53580.25210.48700.09520.57580.1169DeepSeek-R10.82730.47440.72240.24240.70090.39760.77330.31130.50900.1022Baseline Average0.69240.26850.71780.35740.52980.20380.60040.15850.55650.1277Best Performance0.92260.69180.97320.86530.73660.51650.83200.44250.78620.2450C.2.2 A Universal ComponentLLM-generated category descriptions serve as auniversal component that can be integrated intoprompts to enhance any LLM-based task requir-ing category-specific information. In our study, wedemonstrate its effectiveness in improving LLM-based zero-shot AD as shown in Table 2. Ad-ditionally, these descriptions can enhance LLM-based synthetic data generation similarly. This ap-proach aligns with the Native Chain-of-Thought(NCoT) process (Wang et al., 2024a) used in Ope-nAI o1 (OpenAI, 2024c). Extending this idea, otherdatasets with distinct structures could inspire thedevelopment of task-specific universal components,enabling tailored augmentation strategies for di-verse LLM-based applications.</p>
<p>Table A12 :
A12
It contains the following category(ies): LLM prompt template used for sample generation, which is the second step of generating synthetic samples as discussed in §4.2.We generate a single synthetic sample per keyword group.{keyword_group[i]}refers to (i + 1) th granularity level's keyword in this keyword group.{category}represents the name of the corresponding category for this keyword group.You are an intelligent and professional assistant that generates a synthetic text sample based on a group of 3 keywords with different levels of granularity.##Task: -Generate a synthetic text sample that incorporates the provided group of 3 keywords (broad, intermediate, and fine-grained) listed below.-Thegenerated sample should align with the meanings and themes suggested by the keywords provided.</p>
<h2>Rules:1. <strong>Sample Characteristics</strong>:{normal_category_1}{normal_category_2}...Response in JSON format:</h2>
<p>AcknowledgmentsThis work was partially supported by the National Science Foundation under Award No. 2428039 and No. 2346158.We also acknowledge the use of computational resources provided by the Advanced Cyberinfrastructure Coordination Ecosystem (Boerner et al., 2023): Services &amp; Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296.Specifically, this work used NCSA Delta GPU at the National Center for Supercomputing Applications (NCSA) through allocation CIS250073.Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.SettingsAG News BBC News IMDB Reviews N24 News SMS Spam
Fraud detection system: A survey. Aisha Abdallah, Mohd Aizaini Maarof, Anazida Zainal, Journal of Network and Computer Applications. 682016</p>
<p>Outlier analysis. C Charu, Aggarwal, Data mining. Springer2015</p>
<p>Data-efficient learning via clustering-based sensitivity sampling: Foundation models and beyond. Kyriakos Axiotis, Vincent Cohen-Addad, Monika Henzinger, Sammy Jerome, Vahab Mirrokni, David Saulpic, David Woodruff, Michael Wunder, Forty-first International Conference on Machine Learning. 2024</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Lunar: Unifying local outlier detection methods via graph neural networks. Adam Goodge, Bryan Hooi, See-Kiong Ng, Wee Siong Ng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Adbench: Anomaly detection benchmark. Xu Guo, Yiqiang Chen, ; Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, Yue Zhao, arXiv:2403.04190Generative ai for synthetic data generation: Methods, challenges and the future. 2024. 202235arXiv preprintAdvances in Neural Information Processing Systems</p>
<p>Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Hanchi Sun, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric P Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, ; Philip, S Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Yang Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, Yue Zhao, Forty-first International Conference on Machine Learning. Neil Zhenqiang Gong2024aPosition: TrustLLM: Trustworthiness in large language models</p>
<p>Position: Trustllm: Trustworthiness in large language models. Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, International Conference on Machine Learning. PMLR2024b</p>
<p>Deep learning for misinformation detection on online social networks: a survey and new perspectives. Rafiqul Md, Shaowu Islam, Xianzhi Liu, Guandong Wang, Xu, Social Network Analysis and Mining. 101822020</p>
<p>Adgym: Design choices for deep anomaly detection. Minqi Jiang, Chaochuan Hou, Ao Zheng, Songqiao Han, Hailiang Huang, Qingsong Wen, Xiyang Hu, Yue Zhao, Advances in Neural Information Processing Systems. 2024a36</p>
<p>Mmad: The first-ever comprehensive benchmark for multimodal large language models in industrial anomaly detection. Xi Jiang, Jian Li, Hanqiu Deng, Yong Liu, Bin-Bin Gao, Yifeng Zhou, Jialin Li, Chengjie Wang, Feng Zheng, arXiv:2410.094532024barXiv preprint</p>
<p>Language models (mostly) know what they know. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, arXiv:2207.052212022arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of naacL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , naacL-HLTMinnesota20191Minneapolis</p>
<p>Autoencoding variational bayes. P Diederik, Max Kingma, Welling, ICLR 20142nd International Conference on Learning Representations. Conference Track Proceedings. Banff, AB, Canada2014. April 14-16, 2014</p>
<p>Language models meet anomaly detection for better interpretability and generalizability. Jun Li, Cosmin I Bercea, Philip Müller, Lina Felsner, Suhwan Kim, Daniel Rueckert, Benedikt Wiestler, Julia A Schnabel, arXiv:2404.076222024aarXiv preprint</p>
<p>Biased-predicate annotation identification via unbiased visual predicate representation. Li Li, Chenwei Wang, You Qin, Wei Ji, Renjie Liang, Proceedings of the 31st ACM International Conference on Multimedia. the 31st ACM International Conference on Multimedia2023</p>
<p>Political-llm: Large language models in political science. Lincan Li, Jiaqi Li, Catherine Chen, Fred Gui, Hongjia Yang, Chenxiao Yu, Zhengguang Wang, Jianing Cai, Junlong , Aaron Zhou, Bolin Shen, arXiv:2412.068642024barXiv preprint</p>
<p>Yuangang Li, Jiaqi Li, Zhuo Xiao, Tiankai Yang, Yi Nian, Xiyang Hu, Yue Zhao, arXiv:2412.04784Nlpadbench: Nlp anomaly detection benchmark. 2024carXiv preprint</p>
<p>Ecod: Unsupervised outlier detection using empirical cumulative distribution functions. Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, George H Chen, IEEE Transactions on Knowledge and Data Engineering. 35122022</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024aarXiv preprint</p>
<p>How good are LLMs at out-of-distribution detection?. Bo Liu, Li-Ming Zhan, Zexin Lu, Yujie Feng, Lei Xue, Xiao-Ming Wu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024b</p>
<p>Isolation forest. Tony Fei, Kai Ming Liu, Zhi-Hua Ting, Zhou, 2008 eighth ieee international conference on data mining. IEEE2008</p>
<p>Pygod: A python library for graph outlier detection. Kay Liu, Yingtong Dou, Xueying Ding, Xiyang Hu, Ruitong Zhang, Hao Peng, Lichao Sun, Philip Yu, Journal of Machine Learning Research. 251412024c</p>
<p>Generative adversarial active learning for unsupervised outlier detection. Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, Xiangnan He, IEEE Transactions on Knowledge and Data Engineering. 3282019</p>
<p>Interpretable online log analysis using large language models with prompt strategies. Yilun Liu, Shimin Tao, Weibin Meng, Jingyu Wang, Wenbing Ma, Yuhang Chen, Yanqing Zhao, Hao Yang, Yanfei Jiang, Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension. the 32nd IEEE/ACM International Conference on Program Comprehension2024d</p>
<p>On llmsdriven synthetic data generation, curation, and evaluation: A survey. Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, Haobo Wang, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>Date: Detecting anomalies in text via selfsupervision of transformers. Andrei Manolache, Florin Brad, Elena Burceanu, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>Visual classification via description from large language models. Sachit Menon, Carl Vondrick, arXiv:2210.071832022arXiv preprint</p>
<p>Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, arXiv:2311.11045Teaching small language models how to reason. 20232arXiv preprint</p>
<p>Diverse data selection under fairness constraints. Zafeiria Moumoulidou, Andrew Mcgregor, Alexandra Meliou, arXiv:2010.091412020arXiv preprint</p>
<p>Steering language generation: Harnessing contrastive expert guidance and negative prompting for coherent and diverse synthetic data generation. O' Charles, Yuan-Sen Neill, Ioana Ting, Jack W Ciuca, Thang Miller, Bui, 2023CoRR</p>
<p>arXiv:2410.21276Gpt-4o system card. 2024aOpenAIarXiv preprint</p>
<p>New embedding models and api updates. 2024bOpenAI</p>
<p>Openai o1 system card. OpenAI. 2025. Openai o3-mini. 2024cOpenAI</p>
<p>Metaood: Automatic selection of ood detection models. Yuehan Qin, Yichi Zhang, Yi Nian, Xueying Ding, Yue Zhao, arXiv:2410.030742024arXiv preprint</p>
<p>A review on social spam detection: Challenges, open issues, and future directions. Sanjeev Rao, Anil Kumar Verma, Tarunpreet Bhatia, Expert Systems with Applications. 1861157422021</p>
<p>. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Ahmed Shoaib, Alexander Siddiqui, Emmanuel Binder, Marius Müller, Kloft, </p>
<p>Selfattentive, multi-context one-class classification for unsupervised anomaly detection on text. Pmlr Lukas Ruff, Yury Zemlyanskiy, Robert Vandermeulen, Thomas Schnake, Marius Kloft, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019International conference on machine learning</p>
<p>Real-time anomaly detection and reactive planning with large language models. Rohan Sinha, Amine Elhafsi, Christopher Agia, Matthew Foutter, Ed Schmerling, Marco Pavone, Robotics: Science and Systems. 2024</p>
<p>A simple and effective pruning approach for large language models. Mingjie Sun, Zhuang Liu, Anna Bair, J Zico Kolter, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Efficient and robust kpi outlier detection for large-scale datacenters. Yongqian Sun, Daguo Cheng, Tiankai Yang, Yuhe Ji, Shenglin Zhang, Man Zhu, Xiao Xiong, Qiliang Fan, Minghan Liang, Dan Pei, IEEE Transactions on Computers. 72102023</p>
<p>On the importance of building high-quality training datasets for neural code search. Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, Li Li, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software Engineering2022</p>
<p>Openr: An open source framework for advanced reasoning with large language models. Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel M Ni, arXiv:2410.096712024aarXiv preprint</p>
<p>RDRec: Rationale distillation for LLM-based recommendation. Xinfeng Wang, Jin Cui, Yoshimi Suzuki, Fumiyo Fukumoto, 10.18653/v1/2024.acl-short.6Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024b2Short Papers)</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Efficient model selection for time series forecasting via llms. Wang Wei, Tiankai Yang, Hongjie Chen, Ryan A Rossi, Yue Zhao, Franck Dernoncourt, Hoda Eldardiry, arXiv:2504.021192025arXiv preprint</p>
<p>Zhiqiu Xia, Jinxuan Xu, Yuqian Zhang, Hang Liu, arXiv:2503.00172A survey of uncertainty estimation methods on large language models. 2025arXiv preprint</p>
<p>Smoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han, International Conference on Machine Learning. PMLR2023</p>
<p>WizardLM: Empowering large pre-trained language models to follow complex instructions. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, Daxin Jiang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Large language models for anomaly and out-of-distribution detection: A survey. Ruiyao Xu, Kaize Ding, arXiv:2409.019802024arXiv preprint</p>
<p>Social media user behavior analysis applied to the fashion and apparel industry in the big data era. Zhebin Xue, Qing Li, Xianyi Zeng, Journal of Retailing and Consumer Services. 721032992023</p>
<p>Data augmentation is a hyperparameter: Cherrypicked self-supervision for unsupervised anomaly detection is creating the illusion of success. Jaemin Yoo, Tiancheng Zhao, Leman Akoglu, Transactions on Machine Learning Research. 2024</p>
<p>Dsv: An alignment validation loss for self-supervised outlier model selection. Jaemin Yoo, Yue Zhao, Lingxiao Zhao, Leman Akoglu, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer2023</p>
<p>Towards reproducible, automated, and scalable anomaly detection. Yue Zhao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Pyod: A python toolbox for scalable outlier detection. Yue Zhao, Zain Nasrullah, Zheng Li, Journal of machine learning research. 20962019</p>
<p>Automatic unsupervised outlier model selection. Yue Zhao, Ryan Rossi, Leman Akoglu, Advances in Neural Information Processing Systems. 202134</p>
<p>Do llms understand visual anomalies? uncovering llm's capabilities in zero-shot anomaly detection. Jiaqi Zhu, Shaofeng Cai, Fang Deng, Beng , Chin Ooi, Junran Wu, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>            </div>
        </div>

    </div>
</body>
</html>
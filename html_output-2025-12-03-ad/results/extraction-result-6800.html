<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6800 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6800</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6800</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-270219176</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.00284v2.pdf" target="_blank">A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of Large Language Models (LLMs) has demonstrated promising progress in solving logical reasoning tasks effectively. Several recent approaches have proposed to change the role of the LLM from the reasoner into a translator between natural language statements and symbolic representations which are then sent to external symbolic solvers to resolve. This paradigm has established the current state-of-the-art result in logical reasoning (i.e., deductive reasoning). However, it remains unclear whether the variance in performance of these approaches stems from the methodologies employed or the specific symbolic solvers utilized. There is a lack of consistent comparison between symbolic solvers and how they influence the overall reported performance. This is important, as each symbolic solver also has its own input symbolic language, presenting varying degrees of challenge in the translation process. To address this gap, we perform experiments on 3 deductive reasoning benchmarks with LLMs augmented with widely used symbolic solvers: Z3, Pyke, and Prover9. The tool-executable rates of symbolic translation generated by different LLMs exhibit a near 50% performance variation. This highlights a significant difference in performance rooted in very basic choices of tools. The almost linear correlation between the executable rate of translations and the accuracy of the outcomes from Prover9 highlight a strong alignment between LLMs ability to translate into Prover9 symbolic language, and the correctness of those translations.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6800.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6800.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capability family member of OpenAI's GPT models used in the study as a translator from natural language to solver-specific symbolic input; evaluated in one-shot (and few-shot) tool-based logical reasoning setups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer family model from OpenAI, used as a black-box generator to translate natural language premises/questions into solver-specific symbolic formats (Z3, Prover9, Pyke).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; evaluated on ProofWriter, PrOntoQA, FOLIO datasets for downstream evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Tool-based translation: LLM acts as translator from NL to solver input (FOL or solver-specific syntax); solver (Z3/Prover9/Pyke) performs deterministic proof/satisfiability check.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Integrated with three symbolic solvers: Z3 (SMT solver; Python-like API), Prover9 (FOL theorem prover via NLTK), and Pyke (rule-based Python expert system). The LLM outputs solver-specific code / formulas which are executed by the external solver; performance depends on correct translation (syntax & semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>ProofWriter: synthetic deductive reasoning (depth 2/3/5) under OWA/CWA; PrOntoQA: synthetic multi-hop deductive tasks (closed-world True/False); FOLIO: expert-written first-order logic natural-language problems demanding complex FOL reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive logical entailment / first-order logic theorem proving (translation + solver execution).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percentage of correct final answers) and ExecR. (percentage of translations that were syntactically executable by the solver).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative numbers (1-shot runs reported): ProofWriter (Avg OWA) — Z3: ExecR 75.00%, Acc 74.17%; Prover9: ExecR 97.33%, Acc 95.67%; Pyke: ExecR 99.83%, Acc 79.17%. PrOntoQA — Z3: Acc 96.00%, Prover9: Acc 100.00%, Pyke: Acc 100.00%. FOLIO (1-shot) — Z3: ExecR 40.00%, Acc 36.00%; Prover9: ExecR 84.00%, Acc 66.50%; Pyke: unable to solve (✗). Best reported FOLIO result: 66.5% accuracy with GPT-4o + Prover9 (1-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Tool choice drastically changes performance: Prover9 translations/executions generally gave much higher accuracies than Z3 or Pyke for many tasks (e.g., Prover9 ~95.7% vs Z3 ~74.2% on ProofWriter OWA for GPT-4o); the paper reports up to ~50% performance variation for the same LLM under different solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4o translates most successfully to Prover9's FOL style (high ExecR and high accuracy); Prover9 often yields the best final accuracy when the translation is executable. Z3 shows useful error messages and flexibility (can add custom rules), but translations are harder to get correct consistently. Pyke is syntactically easy for LLMs in some datasets but limited (poor on FOLIO).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires flawless translation — minor syntax mistakes cause parse/runtime errors; solver-specific features (e.g., XOR, quantifier handling) lead to failures; tool-based approach fails when the natural-language premise chain is incomplete; black-box LLM instabilities (bracket omissions across runs) affected reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6800.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6800.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used OpenAI conversational transformer model evaluated as a translator into symbolic solver inputs (Z3, Prover9, Pyke) under one-shot prompting for deductive reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based chat model used to translate natural language reasoning problems into solver-specific formal representations (FOL / solver APIs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (chat-optimized)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; evaluated on ProofWriter, PrOntoQA, FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Tool-based translation to external symbolic solvers (Z3 / Prover9 / Pyke); one-shot prompting; no self-refinement used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Same integration pattern as above: LLM outputs symbolic formulas or solver code which the external solver executes; parser/execution errors recorded when translation is invalid or facts inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See GPT-4o entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive entailment / first-order reasoning (translation + solver execution).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy and ExecR. reported per-dataset and per-solver.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative numbers (1-shot): ProofWriter (Avg OWA) — Z3: ExecR 84.83%, Acc 82.88%; Prover9: ExecR 90.67%, Acc 87.00%; Pyke: ExecR 62.83%, Acc 53.33%. PrOntoQA — Z3: ~95.50% ExecR/Acc region (high); Prover9 and Pyke also high. FOLIO (1-shot) — Z3: ExecR 29.00%/Acc 31.00% (low); Prover9: ExecR ~24.49%/Acc ~25.50% (low); Pyke: unable to solve. Few-shot increases improved some ExecR/Acc but also sometimes increased parse errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to GPT-4o, GPT-3.5-Turbo had similar tool-preference shifts but generally lower accuracy in many configurations; tool choice still yields large performance swings (tens of percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5-Turbo benefits from Prover9 translations but is less robust than GPT-4o; shows varied sensitivity to dataset wording (fictional vs natural) and to number of demonstration shots.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Same as for GPT-4o: fragile to small syntax mistakes, inconsistent bracket insertion errors occurred recurrently (e.g., Forall()), leading to dramatic drops in execution rate; poorer predicate extraction on naturalistic phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6800.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6800.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.0-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.0-Pro (Google/Alphabet 'Gemini' family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capability multimodal/LLM model evaluated in the paper as a translator for tool-based logical reasoning against Z3, Prover9, and Pyke; exhibited different tool preferences and lower final accuracy than GPT models in several settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.0-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A powerful model from the Gemini family (transformer-based); used here as a black-box translator from NL to solver-specific input languages.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (multimodal family)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; evaluated on ProofWriter, PrOntoQA, FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Tool-based translation into external symbolic solvers (Z3, Prover9, Pyke); one-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Translations produced by Gemini are executed by Z3/Prover9/Pyke; the paper tracks executable-rate and correctness of solver outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See GPT-4o entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive logical entailment / FOL reasoning via translation + solver execution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ExecR. and accuracy per dataset/solver.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative numbers: ProofWriter (Avg OWA) — Z3: ExecR 93.00%, Acc 91.00%; Prover9: ExecR 86.83%, Acc 62.50%; Pyke: ExecR 49.33%, Acc 36.67%. Combined behavior differs from GPTs: Gemini often had higher executable rates for Z3 but much lower accuracy with Prover9 in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Gemini's relative performance depends on solver; it was more executable with Z3 but produced lower correctness with Prover9 compared to GPT models, showing model-specific tool preference.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Different LLM architectures show distinct affinities to solver syntaxes; executable rate does not always imply high accuracy — e.g., high ExecR with Z3 but lower final accuracy for some model+solver pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Struggles to reach GPT-level accuracy on some solver-dataset pairs; executable formulations sometimes yielded incorrect solver outputs (no solver-internal feedback for correctness when translations are syntactically valid).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6800.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6800.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Command R+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cohere Command R Plus (command-r-plus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Cohere model variant evaluated as a translator in tool-based logical reasoning experiments; exhibited different tool preferences (shifted towards Pyke in some settings) and variable robustness to reasoning depth and shots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Cohere Command R Plus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A high-quality instruction-tuned transformer model from Cohere; used to translate NL to solver-specific formats under one-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; evaluated on ProofWriter, PrOntoQA, FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Tool-based translation to Z3/Prover9/Pyke with one-shot demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Outputs translated formulas/code executed by Z3/Prover9/Pyke; the study logs parser/run-time errors and execution success.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See GPT-4o entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive entailment / FOL reasoning (translation + solver execution).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ExecR. and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative numbers: ProofWriter (Avg OWA) — Z3: ExecR 88.67%, Acc 87.00%; Prover9: ExecR 61.33%, Acc 56.66%; Pyke: ExecR 61.83%, Acc 51.50%. PrOntoQA: Z3 ~93.00%/87.00% region; Prover9 and Pyke mixed. FOLIO (1-shot): low performance and Pyke unable to solve some FOLIO problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Command R+ shifted solver preference across datasets (e.g., improved relative Pyke performance on some fictional settings), but overall performance trailed GPT-4o in many Prover9/Z3 settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Different LLMs change relative solver effectiveness; Command R+ sometimes benefits from fictional wording of problems and shows sensitivity to dataset phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Suffers from same brittle translation issues (bracket omissions, wrong function names); increased shots sometimes increased parse errors rather than decreasing them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6800.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6800.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool-based LLM→Solver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool-based logical reasoning (LLM as translator + external symbolic solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach where an LLM translates natural-language premises/questions into a symbolic input language for an automated theorem prover or SMT/logic solver, which then deterministically executes the proof/search and returns a verdict; main paradigm studied in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM + Symbolic Solver (general paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Paradigm combining large language models (as translators or program generators) with symbolic solvers (ATPs/SMT/logic engines) such as Z3, Prover9, and Pyke to obtain logically grounded, verifiable reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Neuro-symbolic pipeline: Transformer (LLM) + External symbolic solver (SMT/ATP/expert-system)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Benchmarks used for evaluation in this paper: ProofWriter, PrOntoQA, FOLIO. Related works use theorem corpora or solver traces (cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Translation-to-formalism + external deterministic proof search; contrasts with free-form Chain-of-Thought LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Tools analyzed: Z3 (SMT solver; supports theories and flexible API), Prover9 (first-order theorem prover with FOL-style input, accessible via NLTK), Pyke (Python Knowledge Engine — rule-based expert system with limited syntax). Integration: LLM outputs solver-specific text/code; solver executes and returns success/failure/counterexample; execution errors enable explicit self-refinement (not used in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See model entries.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive reasoning / first-order logic entailment and satisfiability checks performed by solvers after translation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (final answer correctness) and ExecR. (fraction of translations that are syntactically executable by solver); parse vs runtime error breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Key summary findings across experiments: Tool choice produces up to ~50% variation in LLM downstream accuracy; Prover9 generally had the highest ExecR. and highest downstream accuracy across many LLMs/datasets (e.g., Prover9 achieved 95.67% accuracy on ProofWriter OWA with GPT-4o vs Z3 74.17%); Pyke failed on FOLIO (no XOR/exclusive-disjunction support) and underperformed on complex FOL tasks; best reported FOLIO result in this work: 66.5% accuracy (GPT-4o + Prover9, 1-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to free-form chain-of-thought-style LLM reasoning, the tool-based approach provides deterministic logical grounding and verifiable proof traces; however, the final system performance is heavily dependent on solver choice and translation quality rather than solely on LLM internal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Choice of symbolic solver matters strongly — identical prompting and models yield markedly different results across Z3, Prover9, Pyke. 2) Executable rate of translations is strongly correlated with final accuracy (especially for Prover9). 3) Prover9 is easiest for LLMs to target and gives high accuracy when translations are executable; Z3 offers useful diagnostics and flexibility; Pyke's limited syntax constrains performance on complex FOL tasks. 4) Tool-based approach enforces logical coherence (no hallucinated steps from solver) but is unforgiving to translation errors and incomplete reasoning chains in premises.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Applicable only when a complete reasoning chain is present (symbolic solvers require explicit predicates and premises); brittle to small translation mistakes (syntax, predicate tokenization); LLM instability (e.g., bracket omission) can drastically reduce execution rate; solver-specific language mismatches and missing logical primitives (e.g., XOR in Pyke) cause failures; experiments here excluded self-refinement methods so some recoverable runtime errors were not addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LogicLM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>PAL: program-aided language models <em>(Rating: 2)</em></li>
                <li>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 2)</em></li>
                <li>FOLIO: natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>ProofWriter <em>(Rating: 2)</em></li>
                <li>PrOntoQA <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6800",
    "paper_id": "paper-270219176",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A high-capability family member of OpenAI's GPT models used in the study as a translator from natural language to solver-specific symbolic input; evaluated in one-shot (and few-shot) tool-based logical reasoning setups.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Large autoregressive transformer family model from OpenAI, used as a black-box generator to translate natural language premises/questions into solver-specific symbolic formats (Z3, Prover9, Pyke).",
            "model_size": null,
            "architecture_type": "Transformer (autoregressive)",
            "training_data": "Not specified in this paper; evaluated on ProofWriter, PrOntoQA, FOLIO datasets for downstream evaluation.",
            "reasoning_method": "Tool-based translation: LLM acts as translator from NL to solver input (FOL or solver-specific syntax); solver (Z3/Prover9/Pyke) performs deterministic proof/satisfiability check.",
            "external_tool_used": true,
            "external_tool_description": "Integrated with three symbolic solvers: Z3 (SMT solver; Python-like API), Prover9 (FOL theorem prover via NLTK), and Pyke (rule-based Python expert system). The LLM outputs solver-specific code / formulas which are executed by the external solver; performance depends on correct translation (syntax & semantics).",
            "benchmark_name": "ProofWriter, PrOntoQA, FOLIO",
            "benchmark_description": "ProofWriter: synthetic deductive reasoning (depth 2/3/5) under OWA/CWA; PrOntoQA: synthetic multi-hop deductive tasks (closed-world True/False); FOLIO: expert-written first-order logic natural-language problems demanding complex FOL reasoning.",
            "task_type": "Deductive logical entailment / first-order logic theorem proving (translation + solver execution).",
            "performance_metric": "Accuracy (percentage of correct final answers) and ExecR. (percentage of translations that were syntactically executable by the solver).",
            "performance_value": "Representative numbers (1-shot runs reported): ProofWriter (Avg OWA) — Z3: ExecR 75.00%, Acc 74.17%; Prover9: ExecR 97.33%, Acc 95.67%; Pyke: ExecR 99.83%, Acc 79.17%. PrOntoQA — Z3: Acc 96.00%, Prover9: Acc 100.00%, Pyke: Acc 100.00%. FOLIO (1-shot) — Z3: ExecR 40.00%, Acc 36.00%; Prover9: ExecR 84.00%, Acc 66.50%; Pyke: unable to solve (✗). Best reported FOLIO result: 66.5% accuracy with GPT-4o + Prover9 (1-shot).",
            "comparison_with_baseline": "Tool choice drastically changes performance: Prover9 translations/executions generally gave much higher accuracies than Z3 or Pyke for many tasks (e.g., Prover9 ~95.7% vs Z3 ~74.2% on ProofWriter OWA for GPT-4o); the paper reports up to ~50% performance variation for the same LLM under different solvers.",
            "key_findings": "GPT-4o translates most successfully to Prover9's FOL style (high ExecR and high accuracy); Prover9 often yields the best final accuracy when the translation is executable. Z3 shows useful error messages and flexibility (can add custom rules), but translations are harder to get correct consistently. Pyke is syntactically easy for LLMs in some datasets but limited (poor on FOLIO).",
            "limitations": "Requires flawless translation — minor syntax mistakes cause parse/runtime errors; solver-specific features (e.g., XOR, quantifier handling) lead to failures; tool-based approach fails when the natural-language premise chain is incomplete; black-box LLM instabilities (bracket omissions across runs) affected reproducibility.",
            "uuid": "e6800.0",
            "source_info": {
                "paper_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "GPT-3.5-Turbo (OpenAI)",
            "brief_description": "A widely used OpenAI conversational transformer model evaluated as a translator into symbolic solver inputs (Z3, Prover9, Pyke) under one-shot prompting for deductive reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Large transformer-based chat model used to translate natural language reasoning problems into solver-specific formal representations (FOL / solver APIs).",
            "model_size": null,
            "architecture_type": "Transformer (chat-optimized)",
            "training_data": "Not specified in this paper; evaluated on ProofWriter, PrOntoQA, FOLIO.",
            "reasoning_method": "Tool-based translation to external symbolic solvers (Z3 / Prover9 / Pyke); one-shot prompting; no self-refinement used in experiments.",
            "external_tool_used": true,
            "external_tool_description": "Same integration pattern as above: LLM outputs symbolic formulas or solver code which the external solver executes; parser/execution errors recorded when translation is invalid or facts inconsistent.",
            "benchmark_name": "ProofWriter, PrOntoQA, FOLIO",
            "benchmark_description": "See GPT-4o entry.",
            "task_type": "Deductive entailment / first-order reasoning (translation + solver execution).",
            "performance_metric": "Accuracy and ExecR. reported per-dataset and per-solver.",
            "performance_value": "Representative numbers (1-shot): ProofWriter (Avg OWA) — Z3: ExecR 84.83%, Acc 82.88%; Prover9: ExecR 90.67%, Acc 87.00%; Pyke: ExecR 62.83%, Acc 53.33%. PrOntoQA — Z3: ~95.50% ExecR/Acc region (high); Prover9 and Pyke also high. FOLIO (1-shot) — Z3: ExecR 29.00%/Acc 31.00% (low); Prover9: ExecR ~24.49%/Acc ~25.50% (low); Pyke: unable to solve. Few-shot increases improved some ExecR/Acc but also sometimes increased parse errors.",
            "comparison_with_baseline": "Compared to GPT-4o, GPT-3.5-Turbo had similar tool-preference shifts but generally lower accuracy in many configurations; tool choice still yields large performance swings (tens of percentage points).",
            "key_findings": "GPT-3.5-Turbo benefits from Prover9 translations but is less robust than GPT-4o; shows varied sensitivity to dataset wording (fictional vs natural) and to number of demonstration shots.",
            "limitations": "Same as for GPT-4o: fragile to small syntax mistakes, inconsistent bracket insertion errors occurred recurrently (e.g., Forall()), leading to dramatic drops in execution rate; poorer predicate extraction on naturalistic phrasing.",
            "uuid": "e6800.1",
            "source_info": {
                "paper_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Gemini-1.0-Pro",
            "name_full": "Gemini-1.0-Pro (Google/Alphabet 'Gemini' family)",
            "brief_description": "A high-capability multimodal/LLM model evaluated in the paper as a translator for tool-based logical reasoning against Z3, Prover9, and Pyke; exhibited different tool preferences and lower final accuracy than GPT models in several settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-1.0-Pro",
            "model_description": "A powerful model from the Gemini family (transformer-based); used here as a black-box translator from NL to solver-specific input languages.",
            "model_size": null,
            "architecture_type": "Transformer (multimodal family)",
            "training_data": "Not specified in this paper; evaluated on ProofWriter, PrOntoQA, FOLIO.",
            "reasoning_method": "Tool-based translation into external symbolic solvers (Z3, Prover9, Pyke); one-shot prompts.",
            "external_tool_used": true,
            "external_tool_description": "Translations produced by Gemini are executed by Z3/Prover9/Pyke; the paper tracks executable-rate and correctness of solver outputs.",
            "benchmark_name": "ProofWriter, PrOntoQA, FOLIO",
            "benchmark_description": "See GPT-4o entry.",
            "task_type": "Deductive logical entailment / FOL reasoning via translation + solver execution.",
            "performance_metric": "ExecR. and accuracy per dataset/solver.",
            "performance_value": "Representative numbers: ProofWriter (Avg OWA) — Z3: ExecR 93.00%, Acc 91.00%; Prover9: ExecR 86.83%, Acc 62.50%; Pyke: ExecR 49.33%, Acc 36.67%. Combined behavior differs from GPTs: Gemini often had higher executable rates for Z3 but much lower accuracy with Prover9 in some cases.",
            "comparison_with_baseline": "Gemini's relative performance depends on solver; it was more executable with Z3 but produced lower correctness with Prover9 compared to GPT models, showing model-specific tool preference.",
            "key_findings": "Different LLM architectures show distinct affinities to solver syntaxes; executable rate does not always imply high accuracy — e.g., high ExecR with Z3 but lower final accuracy for some model+solver pairs.",
            "limitations": "Struggles to reach GPT-level accuracy on some solver-dataset pairs; executable formulations sometimes yielded incorrect solver outputs (no solver-internal feedback for correctness when translations are syntactically valid).",
            "uuid": "e6800.2",
            "source_info": {
                "paper_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Command R+",
            "name_full": "Cohere Command R Plus (command-r-plus)",
            "brief_description": "A Cohere model variant evaluated as a translator in tool-based logical reasoning experiments; exhibited different tool preferences (shifted towards Pyke in some settings) and variable robustness to reasoning depth and shots.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Cohere Command R Plus",
            "model_description": "A high-quality instruction-tuned transformer model from Cohere; used to translate NL to solver-specific formats under one-shot prompting.",
            "model_size": null,
            "architecture_type": "Transformer (instruction-tuned)",
            "training_data": "Not specified in this paper; evaluated on ProofWriter, PrOntoQA, FOLIO.",
            "reasoning_method": "Tool-based translation to Z3/Prover9/Pyke with one-shot demonstrations.",
            "external_tool_used": true,
            "external_tool_description": "Outputs translated formulas/code executed by Z3/Prover9/Pyke; the study logs parser/run-time errors and execution success.",
            "benchmark_name": "ProofWriter, PrOntoQA, FOLIO",
            "benchmark_description": "See GPT-4o entry.",
            "task_type": "Deductive entailment / FOL reasoning (translation + solver execution).",
            "performance_metric": "ExecR. and accuracy.",
            "performance_value": "Representative numbers: ProofWriter (Avg OWA) — Z3: ExecR 88.67%, Acc 87.00%; Prover9: ExecR 61.33%, Acc 56.66%; Pyke: ExecR 61.83%, Acc 51.50%. PrOntoQA: Z3 ~93.00%/87.00% region; Prover9 and Pyke mixed. FOLIO (1-shot): low performance and Pyke unable to solve some FOLIO problems.",
            "comparison_with_baseline": "Command R+ shifted solver preference across datasets (e.g., improved relative Pyke performance on some fictional settings), but overall performance trailed GPT-4o in many Prover9/Z3 settings.",
            "key_findings": "Different LLMs change relative solver effectiveness; Command R+ sometimes benefits from fictional wording of problems and shows sensitivity to dataset phrasing.",
            "limitations": "Suffers from same brittle translation issues (bracket omissions, wrong function names); increased shots sometimes increased parse errors rather than decreasing them.",
            "uuid": "e6800.3",
            "source_info": {
                "paper_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Tool-based LLM→Solver",
            "name_full": "Tool-based logical reasoning (LLM as translator + external symbolic solver)",
            "brief_description": "An approach where an LLM translates natural-language premises/questions into a symbolic input language for an automated theorem prover or SMT/logic solver, which then deterministically executes the proof/search and returns a verdict; main paradigm studied in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM + Symbolic Solver (general paradigm)",
            "model_description": "Paradigm combining large language models (as translators or program generators) with symbolic solvers (ATPs/SMT/logic engines) such as Z3, Prover9, and Pyke to obtain logically grounded, verifiable reasoning.",
            "model_size": null,
            "architecture_type": "Neuro-symbolic pipeline: Transformer (LLM) + External symbolic solver (SMT/ATP/expert-system)",
            "training_data": "Benchmarks used for evaluation in this paper: ProofWriter, PrOntoQA, FOLIO. Related works use theorem corpora or solver traces (cited works).",
            "reasoning_method": "Translation-to-formalism + external deterministic proof search; contrasts with free-form Chain-of-Thought LLM reasoning.",
            "external_tool_used": true,
            "external_tool_description": "Tools analyzed: Z3 (SMT solver; supports theories and flexible API), Prover9 (first-order theorem prover with FOL-style input, accessible via NLTK), Pyke (Python Knowledge Engine — rule-based expert system with limited syntax). Integration: LLM outputs solver-specific text/code; solver executes and returns success/failure/counterexample; execution errors enable explicit self-refinement (not used in this paper).",
            "benchmark_name": "ProofWriter, PrOntoQA, FOLIO",
            "benchmark_description": "See model entries.",
            "task_type": "Deductive reasoning / first-order logic entailment and satisfiability checks performed by solvers after translation.",
            "performance_metric": "Accuracy (final answer correctness) and ExecR. (fraction of translations that are syntactically executable by solver); parse vs runtime error breakdown.",
            "performance_value": "Key summary findings across experiments: Tool choice produces up to ~50% variation in LLM downstream accuracy; Prover9 generally had the highest ExecR. and highest downstream accuracy across many LLMs/datasets (e.g., Prover9 achieved 95.67% accuracy on ProofWriter OWA with GPT-4o vs Z3 74.17%); Pyke failed on FOLIO (no XOR/exclusive-disjunction support) and underperformed on complex FOL tasks; best reported FOLIO result in this work: 66.5% accuracy (GPT-4o + Prover9, 1-shot).",
            "comparison_with_baseline": "Compared to free-form chain-of-thought-style LLM reasoning, the tool-based approach provides deterministic logical grounding and verifiable proof traces; however, the final system performance is heavily dependent on solver choice and translation quality rather than solely on LLM internal reasoning.",
            "key_findings": "1) Choice of symbolic solver matters strongly — identical prompting and models yield markedly different results across Z3, Prover9, Pyke. 2) Executable rate of translations is strongly correlated with final accuracy (especially for Prover9). 3) Prover9 is easiest for LLMs to target and gives high accuracy when translations are executable; Z3 offers useful diagnostics and flexibility; Pyke's limited syntax constrains performance on complex FOL tasks. 4) Tool-based approach enforces logical coherence (no hallucinated steps from solver) but is unforgiving to translation errors and incomplete reasoning chains in premises.",
            "limitations": "Applicable only when a complete reasoning chain is present (symbolic solvers require explicit predicates and premises); brittle to small translation mistakes (syntax, predicate tokenization); LLM instability (e.g., bracket omission) can drastically reduce execution rate; solver-specific language mismatches and missing logical primitives (e.g., XOR in Pyke) cause failures; experiments here excluded self-refinement methods so some recoverable runtime errors were not addressed.",
            "uuid": "e6800.4",
            "source_info": {
                "paper_title": "A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LogicLM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "PAL: program-aided language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "FOLIO: natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "ProofWriter",
            "rating": 2,
            "sanitized_title": "proofwriter"
        },
        {
            "paper_title": "PrOntoQA",
            "rating": 2
        }
    ],
    "cost": 0.016895749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters
11 Jul 2024</p>
<p>Long Hei 
DSAI
Monash University</p>
<p>Matthew Lam 
DSAI
Monash University</p>
<p>Ramya Keerthy 
DSAI
Monash University</p>
<p>Thatikonda Ehsan ramya.thatikonda1@monash.edu 
DSAI
Monash University</p>
<p>A Closer Look at Tool-based Logical Reasoning with LLMs: The Choice of Tool Matters
11 Jul 2024E5F7884B328C4600167E2852182EBE8DarXiv:2406.00284v2[cs.CL]
The emergence of Large Language Models (LLMs) has demonstrated promising progress in solving logical reasoning tasks effectively.Several recent approaches have proposed to change the role of the LLM from the reasoner into a translator between natural language statements and symbolic representations which are then sent to external symbolic solvers to resolve.This paradigm has established the current state-of-the-art result in logical reasoning (i.e., deductive reasoning).However, it remains unclear whether the variance in performance of these approaches stems from the methodologies employed or the specific symbolic solvers utilized.There is a lack of consistent comparison between symbolic solvers and how they influence the overall reported performance.This is important, as each symbolic solver also has its own input symbolic language, presenting varying degrees of challenge in the translation process.To address this gap, we perform experiments on 3 deductive reasoning benchmarks with LLMs augmented with widely used symbolic solvers: Z3, Pyke, and Prover9.The tool-executable rates of symbolic translation generated by different LLMs exhibit a near 50% performance variation.This highlights a significant difference in performance rooted in very basic choices of tools.The almost linear correlation between the executable rate of translations and the accuracy of the outcomes from Prover9 highlight a strong alignment between LLMs ability to translate into Prover9 symbolic language, and the correctness of those translations. 1</p>
<p>Introduction</p>
<p>The recent state-of-the-art approaches to logical reasoning have combined Large Language Models (LLMs) with external symbolic mechanisms (Nye et al., 2021;Pan et al., 2023;Ye et al., 2023;Gao et al., 2023;Lyu et al., 2023).This approach leverages LLMs' remarkable proficiency in translating natural language into symbolic representation such as First Order Logic (FOL) or symbolic solvers' specified language (e.g., Pyke, Z3) (Yang et al., 2023), and the symbolic solver's ability to execute these translations through a fully deterministic proof process (Metaxiotis et al., 2002).These existing published methods try a variety of tools and tool-specific formalism.Table 1 summarises various tools used in recent state-of-the-art studies.This variability of tools makes it impossible to have a fair understanding of each approach.There is currently a lack of consistent comparison that will allow others to understand better where this performance gain stems from.</p>
<p>In this paper, we take 3 widely used tools: Z3 (de Moura and Bjørner, 2008), Pyke (Frederiksen, 2008), and Prover9 (McCune, 2005) and analyse the difficulty LLMs face for translating natural language into their desired input format, and the internal capability of these tools at solving certain satisfiability tasks.We select GPT4o, GPT-3.5-Turbo (OpenAI, 2023), Gemini-1.0-Pro(Team et al., 2023) and Cohere Command R Plus, as representatives of the most capable family of LLMs, along with 3 widely used deductive reasoning benchmarks ProofWriter (Tafjord et al., 2021), FO-LIO (Han et al., 2022), and ProntoQA (Saparov and He, 2023).We conduct a fair side-by-side comparison of tools by trying various number of identical prompts, demonstration shots, and minimal adjustment for each solver.</p>
<p>Our findings indicate that LLMs find it easier to translate for Prover9, followed by Z3, and lastly Pyke.Although Prover9 can solve more questions accurately, Prover9 demonstrates a lower discrepancy between execution rate and overall accuracy.This means that Prover9 is more likely to solve a question given the right syntax and format produced by LLMs.Overall, Z3 and Prover9 are all</p>
<p>Solver</p>
<p>Dataset</p>
<p>Papers Problem Z3 AR-LSAT (Zhong et al., 2022), ProntoQA (Saparov and He, 2023), ProofWriter (Tafjord et al., 2021), BoardgameQA (Kazemi et al., 2023) LogicLM, SatLM Analytical, Deductive, FOL Pyke ProntoQA (Saparov and He, 2023), ProofWriter (Tafjord et al., 2021) LogicLM, Logical Solver</p>
<p>Deductive, FOL</p>
<p>Prover9 FOLIO (Han et al., 2022) LogicLM, LINC</p>
<p>Deductive, FOL</p>
<p>Table 1: A summary of the symbolic solvers and the datasets it has solved in different studies: LogicLM (Pan et al., 2023), LINC (Olausson et al., 2023), Logical Solver (Feng et al., 2023), andSatLM (Ye et al., 2023).</p>
<p>competitive options, Pyke's performance is significantly inferior and only comparable to the other tools in solving PrOntoQA.Our experiments across 3 benchmarks (based on the accuracy of outputs) highlight an up-to 50% of performance variation for each LLM under different tools, and well as the performance change for each tool under different LLMs.</p>
<p>Tools &amp; Logical Reasoning with LLMs</p>
<p>The tool-based approaches to logical reasoning combine LLMs with external symbolic solvers.This synergy harnesses the capability of LLMs to convert diverse natural language statements into logical symbolic formalism.While being less flexible compared with free-form reasoning methods, such as Chain-of-Thought (Wei et al., 2022), the tool-based approach, given a correct formal translation, has important advantages: logical coherence during the reasoning (i.e., unlike LLMs, theorem provers cannot make reasoning shortcuts or hallucinate) is guaranteed, while the internal proof trace of the theorem provers offers a transparent and verifiable reasoning chain.</p>
<p>Logical Solvers</p>
<p>Automated theorem provers (ATPs) and Satisfiability Modulo Theories (SMT) solvers are tools equipped with built-in functions designed to assist in logical reasoning tasks.These solvers can vary in syntax, proof search strategies, theorem automation, and complexity.ATPs efficiently resolve first order logic problems without external interaction.SMT solvers closely resemble ATPs in solving first-order formulae but add complexity by handling theories such as equality, arrays, and bitvectors.Logical solvers, specifically Z3, Prover9, and Pyke, are used for logical reasoning tasks with LLMs due to their ease of use in a Python environment (Pan et al., 2023;Ye et al., 2023).We study the logical solvers based on their ability to handle first-order logic and explore the crucial differences in external syntax and internal theories of these tools.In this context, we define the task as follows: given a set of premises P ∈ {P 1 , P 2 , . . ., P n }, the objective is to determine whether the conclusion C logically follows from these premises.The translation syntax for each tool is presented in Figure1.In Python, Prover9 is accessible through the NLTK logic library.</p>
<p>Pyke short for Python Knowledge Engine, is a solver used for building and executing rulebased expert systems (Frederiksen, 2008).Although pyke is used for optimizing software development, Pan et al. (2023) demonstrated its application in solving a first-order logic problem.Given a logical inference task, Pyke establishes a knowledge base and incorporates known facts (fact.kfb)and rules (rule.krb)from the input, i.e., P − → (P facts , P rules ).The conclusion is parsed as a rule that is propagated through the knowledge base until it reaches a resolution.The predicates in the first order logic are treated as facts and are connected to form rules.Given its limited syntax, Pyke supports simple connectives such as 'and', 'or', and 'implies'.The free variables (e.g., $x) are generally considered to be universal quantifiers, thus restricting the use of existential quantifiers.Due to these limitations, Pyke may not adequately handle complex tasks involving first-order logic, such as FOLIO.However, it remains well-suited for rule-based tasks like ProofWriter and ProntoQA.</p>
<p>Free-form Logical Reasoning with LLMs</p>
<p>The free-form approaches to reasoning rely on LLMs' internal capabilities via various mechanisms to help improve LLM's performance in logical reasoning.For example, prompts that encourage LLMs to solve tasks in a Chain-of-Thought approach is a general technique that enhances LLM's performance (Wei et al., 2022;Kojima et al., 2022).</p>
<p>Despite the promising outcomes, this approach falls short when dealing with complex logical reasoning tasks.This limitation stems from the lack of explicit logical grounding and the inherent ambiguous and nuanced nature of natural language.Recent studies have revisited Formal Logic to address this challenge.Han et al. (2022) shows that incorporating first-order logic (FOL) translations into the context can notably enhance LLM's performance.Feng et al. (2023) emulates the reasoning processes of an automated theorem solver (Pyke) through solving Logical tasks using the tool-based approach and training LLMs on Pyke's reasoning steps.The free-form approach capitalises on the inherent capabilities of LLM to learn complex logical rules.However, this approach solely relies upon LLM's logical reasoning prowess and is susceptible to issues such as hallucinations and taking shortcuts (Dasgupta et al., 2022;Ji et al., 2023).To address this issue, recent approaches aim to augment LLMs with external symbolic solvers (Ye et al., 2023;Gao et al., 2023).</p>
<p>Tool-based Logical Reasoning with LLMs</p>
<p>Ye et al. ( 2023) and Gao et al. (2023) integrated Z3 and Python interpreters with LLMs to tackle various reasoning datasets.Pan et al. (2023) expanded upon this by incorporating a broader range of symbolic solvers and employing error-solving self-refinement techniques.However, the rationale behind the adoption of symbolic solvers primarily relied on theoretical definitions rather than empirical performance evaluations.Consequently, there exists a gap in the literature regarding the exploration of the interplay between LLMs, symbolic solvers, and their respective performance characteristics.</p>
<p>The primary advantages of the tool-based ap-proach are: (1) The tasks are now processed with clear logical grounding and unambiguous language.This approach guarantees that the answer is not a product of hallucination or shortcuts, because the symbolic tools will exhaustively process all logical rules in the premise and only execute clear and correct commands.(2) As LLM's translation capability continues to improve, the tool-based approach will be able to solve more complex logical problems, provided they fall within the logical reasoning capacity of symbolic solvers.</p>
<p>(3) The tool errors are clearly labeled and displayed (i.e., run-time error messages).This allows the introduction of various error-solving mechanisms like selfrefinement (Pan et al., 2023).In contrast, it is difficult for the free-form approach to improve upon its current results in the absence of any reliable feedback, specially in the light of recent debates on LLMs self-correction capability (Huang et al., 2024;Li et al., 2024).In this study, errors are isolated into solver-specific errors (e.g., LLM's translation misses a bracket, which causes the solver to throw an error) and parse errors (i.e., Predicate extraction mistakes or LLMs interpreting the logical statements incorrectly, examples of these are shown in Appendix A.3).</p>
<p>The main disadvantages of the tool-based approach are: (1) This approach does not apply to tasks that do not have a complete reasoning chain.All symbolic solvers require a full chain of logic to reach the correct conclusion.For instance, consider the followint example: Premise: People like Mark love bbq.Question: Mark is not Human?Both humans and LLMs can answer this question correctly, but a tool-based approach will fail.This is due to the break in the chain of logic.The term "Mark is human" is missing from the premise.Although this term is obvious for humans and LLMs, symbolic solvers require the exact match in predicates to process the task.A detailed discussion of this issue is included in section 3.2.(2) Changes in LLMs can cause solver-specific errors.2(3) This approach is unforgiving to simple translation errors.While processing logical tasks, Human and LLMs can often bypass errors to some extent and still reach the correct conclusion.However, a tool-based approach requires the LLM to translate tasks flawlessly, even minor mistakes like misusing suffixes (e.g., "Jompuses(x)" instead of "Jompus(x)") will cause the symbolic solver to throw an error.One of the main focuses of this study is the analysis of how different symbolic tools handle errors caused by LLMs.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>In our experiments we assess the performance variations of LLM when paired with various symbolic solvers.We evaluate GPT-4o, GPT-3.5-Turbo,Gemini-Pro-1.0,and Command-r-plus integrated with Z3, Pyke, Prover9 on three common logical reasoning benchmarks (introduced shortly).Unlike Pan et al. (2023) and other studies, we exclude self-refinement methods and random guessing procedures.In cases where LLM's translation is infeasible, it will not yield an answer, and any specific errors encountered are documented.The only exception is the missing bracket issue for the translation of Z3, as this was not an issue in experiments done in Ye et al. ( 2023) and Pan et al. (2023).We use a one-shot demonstration for all experiments.If different solvers are employed to tackle the same dataset, the given prompt problem remains consistent, with the sole variance lying in the solver-specific translations of the prompts.Examples of the prompt are shown in Appendix A.2.We also expand the one-shot experiment for FOLIO to two-shot and four-shot to highlight the impact of additional shots.The primary metrics for evaluation consist of two key factors: the percentage of executable logical formulations (ExecR.),and the overall accuracy (Acc).</p>
<p>Data The 3 benchmarks are introduced shortly and examples are included in Appendix A.1.We limit the test set size to 200 for cost reason.PrOn-toQA (Saparov and He, 2023) is a synthetic dataset created to analyze the capacity of LLMs for deductive reasoning.We use the hardest fictional characters version and the hardest 5-hop subset for evaluation.PrOntoQA only has questions in the close world setting (i.e., True/False only).We include this dataset in the experiment to compare natural and fictional settings, as it has a similar level of logical difficulty to ProofWriter.ProofWriter (Tafjord et al., 2021) and 5).We present the percentage of executable logical formulations (ExecR.)together with the overall accuracy (Acc.).✗: the tool was unable to solve this dataset.The numbers highlighted in red color represent the highest accuracy between the 3 chosen tools.</p>
<p>(OWA) and close-world assumptions (CWA), including depth-2, depth-3, and depth-5 (i.e., each part requiring 2, 3, and 5 hops of reasoning).To ensure a fair evaluation, we control all datasets to have a uniform distribution of True, False, and Unknown (if applicable) answers.FOLIO (Han et al., 2022) is a difficult expert-written dataset for firstorder logical reasoning.The problems are mostly aligned with real-world knowledge and expressed in natural flowing language.Tackling its questions demands adeptness in complex first-order logic reasoning.Pyke is unable to solve FOLIO, this is due to the lack of a built-in function for the exclusive disjunction (i.e., either-or).In contrast, Prover9 and Z3 offer a built-in function to handle this logic seamlessly.</p>
<p>Main Results</p>
<p>We report the results of the tool-based reasoning approach experiments in   The executable rate on average decreases for all LLMs, and average accuracy drops by 1.38% in a fictional setting.Both Z3 and Pyke's overall accuracy increased by 6.62% and 30.87%.This shows that while using Z3 and Prover9, fictional wording helps LLMs in generating consistent and correct translations.Overall, in a fictional setting, Pyke's performance is significantly boosted.Meanwhile, GPT-3.5-Turboshifts its preference from Prover9 to Z3, and Command R+ changes its preference to Pyke.We speculate the nuance in results to be reflective of potential interference between commonsense knowledge and fictional statements.</p>
<p>Depth The relaiton between depth and executable rate is somewhat mixed, specially between depth 2 and 3.While for command-r-plus we observe a general decay in performance (i.e., between depth 2 and 5) across all tools, both GPT models and Gemini exhibit resilence to depth, with performance even improving across most tools (except for Prover9).This observation highlights the robustness of translation-based approaches (i.e., using LLMs for translation and tools for solving) in handling various complexities, while prior findings reported the reasoning ability of LLMs (alone) generally diminish as the number of reasoning hops increases (Han et al., 2022).</p>
<p>Demonstration Shots We present the statistics of the FOLIO dataset in varying number of shots in Table 3. Prover9 achieves the best performance, while Z3 struggles with execution rate.The best result for FOLIO was 66.5%, which is achieved with 1 shot prompting using GPT-4o and Prover9.The primary factors that limit the execution rate performance on FOLIO are: (1) some natural wordings in FOLIO make it difficult for predicate extraction.For example, GPT4o interpreted the term "Eastern wild turkey" as two separate terms "Eastern(x)" and "WildTurkey(x)", but "Eastern(x)" has no meaning and the predicate should be extracted as EasternWildTurkey(x). ( 2) FOLIO is annotated by humans and thus assumes a degree of commonsense, this presents incomplete reasoning chains and ambiguous sentences.As shown in A.3, GPT-3.5-Turboincorrectly translated the statement "Marvin cannot be from Earth and from Mars." into "Not(And(FromEarth(marvin), From-Mars(marvin)))", which entails Marvin is not from Earth and not from Mars.The simple fix is just to change Not() into Xor().This problem was caused by the inherently ambiguous nature of the natural language.</p>
<p>(3) there is a limitation to learning by increasing the number of shots.Specifically, GPT-4o and Prover9's parse errors increased with a higher number of shots, as shown in  The effect of varying number of shots (k = 1, 2, 4) on accuracy and executable rates under GPT-4o, GPT-3.5-turbo,Gemini-1.0-proand command-r-plus on FOLIO.We present the percentage of executable logical formulations (ExecR.)together with the overall accuracy (Acc.).</p>
<p>Analysis</p>
<p>As indicated by the executable rate in Table 2, LLMs generally find it easier to produce executable logical formulations for Prover9.This is attributed to its foundation in FOL-based programming language, which most large language models (LLMs) are familiar with as a form of logical formulation.</p>
<p>While GPT models are more successful at converting these logical formulations into accurate results, Gemini-1.0-proand Command R+ face challenges in achieving similar accuracy.This is an issue because an executable formulation cannot provide feedback when an incorrect result is given.This hinders further improvement and self-refinement.Z3 does not have this issue.Its executable rate is a reflection of its accuracy.Moreover, Z3's programming language closely aligns with Python, offering a unique advantage in error displaying and further improvement.Z3 is also a flexible tool that allows the inclusion of self-defined complex logical rules like "XorAnd()" (i.e., a combination of the rule "Either or" and "And".).This capability is par-ticularly useful for addressing complex reasoning datasets like FOLIO.We did not define such a rule during our experiment but this capability should be considered in further studies.Non-executable logical formulations can be categorized into parse errors and execution errors.Additionally, for Z3, there is a separate category known as execution exceptions.</p>
<p>• parse error refers to the mistakes identified by the parser.Through the prompt, we have predefined a set of instructions and logical rules that LLMs can use.However, when LLMs hallucinate and generate logical rules or code that do not exist in the solver, the parser will detect these discrepancies and throw an parse error.This error indicates the LLM's inability to adhere to the oneshot prompt, resulting in methods or code that the parser cannot process.For instance, using Exist() instead of Exists() for Z3 is an example of such an error.</p>
<p>• execution error occurs when the solver encounters given facts that are inconsistent, predicates that are defined wrong, or when there are solverspecific syntax errors.This type of error can be resolved through self-refinement, as the errors are explicitly displayed.We call this run-time error.</p>
<p>• execution exception is a special case for Z3, where the solver runs both the original conclusion and the negation of the same conclusion but receives true as the answer in both cases.This indicates that the facts are inconsistent.We combined these errors into run-time errors for Figure 3 Z3 visualisation.</p>
<p>As shown in Figure 3, for GPT4o, while Pyke produced 3 execution errors on easier logical reasoning datasets in total, its high execution rate did not translate to high accuracy.Predominately Prover9 and Z3's error is a parse error, with execution error controlled at around 8 questions.In addition, all non-executable questions are different, there are no common questions that all 3 solvers find difficult to solve.For FOLIO, the execution error increases, and the parse error drops significantly.</p>
<p>Challenging datasets, such as FOLIO, encompass a larger number of unseen, complex logical rules and more intricate predicates, which result in higher error rates during translation by LLMs.Additionally, there is an increasing number of questions that both solvers are unable to process.This suggests that both solvers find around 25-30% of questions hard to solve.</p>
<p>Conclusion</p>
<p>In this study, we investigated and compared the performance of LLMs combined with three widely used symbolic solvers to closely examine how each solver influences the performance of toolaugmented LLMs in logical reasoning.Our experiments demonstrated that the choice of tools (i.e., Z3, Pyke, Prover9) has a significant impact on the downstream performance across various benchmarks and LLMs.</p>
<p>Limitations</p>
<p>The tool-based approach to logical reasoning is limited to deductive reasoning datasets with a complete reasoning chain.This constraint arises from the inherent nature of symbolic solvers.A potential solution is for LLMs to generate the missing segments of the reasoning chain.Additionally, black-box LLMs can exhibit inconsistencies, producing results that change in the course of time.For instance, during our experiment, GPT-3.5-Turboconsistently failed to add a closing bracket to the method "Forall()", while Command R+ failed to include an opening bracket.This was not an issue for Pan et al. (2023) and Ye et al. ( 2023) (or at least was not reported in their papers).We limited our use of solvers to their built-in functions.To enhance the performance of each tool, more unique logical combinations can be integrated and implemented.For example, Z3 is a flexible tool that allows the inclusion of rules such as "Male(x) == Not(Female(x))".There is further potential to include more defined complex logical rules that can make LLM translation easier.both attends and is very engaged with school events and is a student who attends the school, or she neither attends and is very engaged with school events nor is a student who attends the school.</p>
<p>Question: Based on the above information, is the following statement true, false, or uncertain?If Bonnie is either both a young child or teenager who wishes to further her academic career and educational opportunities and chaperones high school dances or neither is a young child nor teenager who wishes to further her academic career and educational opportunities, then Bonnie is either a student who attends the school or is an inactive and disinterested member of the community.</p>
<p>Answer  ForAll([x], Implies(And(young(x), green(x)), rough(x))) # If someone is green then they are white.</p>
<p>ForAll ([x], Implies(green(x), white(x))) # If someone is furry and quiet then they are white.</p>
<p>ForAll ([x], Implies(And(furry(x), quiet(x)), white(x))) # If someone is young and white then they are rough.</p>
<p>ForAll ([x], Implies(And(young(x), white(x)), rough(x))) # All red people are young.</p>
<p>ForAll ([x], Implies(red(x), young(x))) # Question: the following statement true, false, or unknown?Anne is white.return white(Anne)</p>
<p>ProofWriter Prompts for Prover9 One shot demonstration for LLM Given a problem description and a question, the task is to parse the problem and the question into first-order logic formulas.The grammar of the first-order logic formula is defined as follows:</p>
<ol>
<li>Logical conjunction of expr1 and expr2: expr1 ∧ expr2 2. Logical disjunction of expr1 and expr2: expr1 ∨ expr2 A.5 GPT4o and Cohere command-r-plus Prompts</li>
</ol>
<p>The prompts require some adjustments for GPT-4O and Cohere, as both models tend to produce complete executable code rather than adhering to the provided example.For instance, GPT-4O will define "s.solver()" and create the decision rule for Z3, instead of generating translations as specified in the prompt.Here we provide an overview of what is changed in the prompt.</p>
<p>ProofWriter GPT4O Prompts for Z3 Solver One-shot demonstration</p>
<p>The grammar of the first-order logic formula is defined as follows: 1) logical conjunction of expr1 and expr2: And(expr1, expr2) 2) logical disjunction of expr1 and expr2: Or(expr1, expr2) 3) logical exclusive disjunction of expr1 and expr2: Xor(expr1, expr2) 4) logical negation of expr1: Not(expr1) 5) expr1 implies expr2 ProofWriter Cohere Prompts for Z3 Solver One-shot demonstration For the Z3 solver, the Cohere prompt was slightly adjusted because produces translation not aligned with the given example.</p>
<p>The grammar of the first-order logic formula is defined as follows: 1) logical conjunction of expr1 and expr2: And(expr1, expr2) 2) logical disjunction of expr1 and expr2: Or(expr1, expr2) 3) logical exclusive disjunction of expr1 and expr2: Xor(expr1, expr2) 4) logical negation of expr1: Not(expr1) 5) expr1 implies expr2: Implies(expr1, expr2) 6) expr1 if and only if expr2: expr1 == expr2 7) logical universal quantification: ForAll() 8) logical existential quantification: Exists() Given a problem description and a question.The task is to parse the [Problem] and the [Question] into Python Z3 solver.You are meant to follow the example format and do not provide any further explanations.Follow the format given and do not define "s" and "s.solver" for the Z3 solver.Keep all the # signs as symbols and do not interpret them as markdown marker.The grammar of the first-order logic formula is defined as follows: 1) logical conjunction of expr1 and expr2: expr1 &amp;&amp; expr2 2) logical negation of expr1: expr1($x, False), as example if "Anne is not quiet", the term would be "Quiet(Anne, False)" 3) expr1 implies expr2: expr1 »&gt; expr2 Given a problem description and a question.The task is to parse the [Problem] and the [Question] into Pyke solver.You are meant to follow the example format and do not provide any further explanations.Keep all the ::: signs as symbols and do not interpret them as markdown marker.</p>
<p>[Problem]: Anne is quiet.Erin is furry.Erin is green.Fiona is furry.Fiona is quiet.Fiona is red.Fiona is rough.Fiona is white.4: Average accuracy of Experiment done with GPT-4o, GPT-3.5-turbo,Gemini-1.0-proand command-r-plus on all datasets.We present the percentage of the overall average accuracy of tools (Avg_Acc).The shots represent the number of shots used in the prompt.✗:the tool was unable to solve this dataset.The numbers highlighted in red color represent the highest accuracy between the 3 chosen tools.</p>
<p>Figure 1 :
1
Figure 1: Overview of syntax used for different Theorem Provers: Z3 and Prover9 adhere to the traditional first-order logic (FOL) format, while Pyke adopts a simplified formula approach, distinguishing premises into rules and facts</p>
<p>Figure 2 :
2
Figure 2: Executable Rate for different LLM-Tool combinations, for depth 2, 3, 5 of the ProofWriter Open World Assumption (OWA).Similar trend exists for the Close World Assumption (CWA).</p>
<p>white(Fiona) ::: Fiona is white.furry(Harry) ::: Harry is furry.quiet(Harry) ::: Harry is quiet.white(Harry) ::: Harry is white.∀x(young(x) → furry(x)) ::: Young people are furry.(quiet(Anne) → red(Anne)) ::: If Anne is quiet then Anne is red.∀x(young(x) ∧ green(x) → rough(x)) ::: Young, green people are rough.∀x(green(x) → white(x)) ::: If someone is green then they are white.∀x((furry(x) ∧ quiet(x)) → white(x)) ::: If someone is furry and quiet then they are white.∀x((young(x) ∧ white(x)) → rough(x)) ::: If someone is young and white then they are rough.∀x(red(x) → young(x)) ::: All red people are young.[Question Parse Output]: Conclusion: white(Anne) ProofWriter GPT4o and Cohere Prompts for Pyke Solver One-shot demonstration</p>
<p>is a commonly used dataset for deductive logical reasoning.Compared with PrOntoQA, the problems are expressed in a more naturalistic language form.We evaluate 6 different variations of ProofWriter.We use both open-world
Z3Prover9PykeDatasetLLMsExecR.Acc.ExecR.Acc.ExecR.Acc.gpt-4o75.00%74.17%97.33%95.67%99.83%79.17%ProofWritergpt-3.5-turbo84.83%82.88%90.67%87.00%62.83%53.33%(Avg. OWA) gemini-1.0-pro93.00%91.00%86.83%62.50%49.33%36.67%command-r-plus88.67%87.00%61.33%56.66%61.83%51.50%gpt-4o77.83%77.83%98.00%98.00%99.83%87.00%ProofWritergpt-3.5-turbo88.33%88.00%94.00%93.83%58.17%51.67%(Avg. CWA) gemini-1.0-pro96.83%96.83%84.83%58.50%42.83%34.17%command-r-plus92.50%92.50%58.67%58.33%45.33%41.33%gpt-4o96.00%96.00% 100.00% 100.00% 100.00% 100.00%PrOntoQAgpt-3.5-turbo gemini-1.0-pro95.50% 100.00% 100.00% 100.00% 97.50% 100.00% 100.00% 93.49% 85.50% 63.50% 99.50% 72.50%command-r-plus93.00%87.00%64.50%46.50%96.50%92.00%gpt-4o40.00%36.00%84.00%66.50%✗✗FOLIOgpt-3.5-turbo gemini-1.0-pro29.00% 31.00%24.49% 25.50%61.00% 67.50%39.99% 50.00%✗ ✗✗ ✗command-r-plus25.50%19.00%50.50%32.50%✗✗gpt-4o74.31%73.50%94.06%91.71%99.86%85.50%Combinedgpt-3.5-turbo gemini-1.0-pro80.50% 87.56%78.83% 86.12%87.56% 85.31%80.75% 63.81%66.07% 53.79%55.36% 44.64%command-r-plus82.75%80.56%59.38%53.00%60.64%52.93%</p>
<p>Table 2 :
2
Accuracy and execution rate of 1-shot experiments done with gpt-4o, gpt-3.5-turbo,gemini-pro-1.0andcommand-r-plus on 3 Datasets.Results for Proofwriter Open and Closed World Assumptions (OWA and CWA) are averaged over depths(Depth 2, 3,</p>
<p>Table 2
2. Different LLMsexhibit varying preferences for tools. For datasets</p>
<p>Table 3
3.</p>
<p>Table 3 :
3</p>
<p>Figure3: The proportion of various executable and non-executable instances per each tool for GPT4o.Note, Pyke does not include FOLIO (hence 1400 instances compared to Z3 and Prover 9).The Exec w/ CorrectO, and Exec w/ IncorrectO denote Executable translations that lead to correct, and incorrect outputs once executed by the tool.The Non-exec (Parse) or (Runtime) denote the non-executable translations which are either due to parsing error or other potential runtime issues.
Z3 (1600)Prover9 (1600)Pyke (1400)73.50%8.06%93.44%2.81% 3.00% 0.75%85.50%14.36% 0.00% 0.14%0.81% 17.62%Exec w/ CorrectOExec w/ IncorrectONon-Exec (Parse)Non-Exec (Runtime)</p>
<p>ProblemAnne is quiet.Erin is furry.Erin is green.Fiona is furry.Fiona is quiet.Fiona is red.Fiona is rough.Fiona is white.Harry is furry.Harry is quiet.Harry is white.Young people are furry.If Anne is quiet then Anne is red.Young, green people are rough.If someone is green then they are white.If someone is furry and quiet then they are white.If someone is young and white then they are rough.All red people are young.Erin is furry.Erin is green.Fiona is furry.Fiona is quiet.Fiona is red.Fiona is rough.Fiona is white.Harry is furry.Harry is quiet.Harry is white.Young people are furry.If Anne is quiet then Anne is red.Young, green people are rough.If someone is green then they are white.If someone is furry and quiet then they are white.If someone is young and white then they are rough.All red people are young.&gt;&gt;&gt;red(Anne,True)) ::: If Anne is quiet then Anne is red.young($x,True)&amp;&amp; green($x, True) &gt;&gt;&gt; rough($x, True) ::: Young, green people are rough.green($x,True)&gt;&gt;&gt; white($x, True) ::: If someone is green then they are white.furry($x,True)&amp;&amp; quiet($x, True) &gt;&gt;&gt; white($x, True) ::: If someone is furry and quiet then they are white.young($x,True)&amp;&amp; white($x, True) &gt;&gt;&gt; rough($x, True) ::: If someone is young and white then they are rough.red($x,True)&gt;&gt;&gt; young($x, True) ::: All red people are young.All aliens are extraterrestrial.If someone is from Mars, then they are aliens.No extraterrestrial is human.Everyone from Earth is a human.Marvin cannot be from Earth and from Mars.If Marvin is not from Earth, then Marvin is an extraterrestrial.A.4 How Z3 tackles BoardgameQAA few players are playing a boardgame.The current state of the game is as follows.The cat is named Meadow.The cat published a high-quality paper.The gecko winks at the eagle.The puffin is named Peddi.And the rules of the game are as follows.Rule1: Regarding the cat, if it has a high-quality paper, then we can conclude that it prepares armor for the canary.Rule2: If you are positive that you saw one of the animals rolls the dice for the eagle, you can be certain that it will also proceed to the spot right after the sun bear.Rule3: If the cat prepares armor for the canary and the cricket gives a magnifier to the canary, then the canary will not remove one of the pieces of the aardvark.Rule4: If the cat has a name whose first letter is the same as the first letter of the puffin's name, then the cat does not prepare armor for the canary.Rule5: The canary removes one of the pieces of the aardvark whenever at least one animal proceeds to the spot right after the sun bear.Rule6: Regarding the cat, if it has something to sit on, then we can conclude that it does not prepare armor for the canary.Rule3 is preferred over Rule5.Rule4 is preferred over Rule1.Rule6 is preferred over Rule1.Based on the game state and the rules and preferences, does the canary remove from the board one of the pieces of the aardvark?If you are positive that you saw one of the animals rolls the dice for the eagle, you can be certain that it will also proceed to the spot right after the sun bear.The current state of the game is as follows.The cat is named Meadow.The cat published a high-quality paper.The gecko winks at the eagle.The puffin is named Peddi.# The cat is named Meadow.The puffin is named Peddi.The first letter of Meadow is M. The first letter of the Peddi is P.So the cat does not have the same first letter name as the puffin.
green(x) ::: x is green. young($x, bool) ::: x is young over Rule1.red(x) ::: x is red. Facts: soft_rules = [Rule5, Rule1, Rule1]rough(x) ::: x is rough. quiet(Anne, True) ::: Anne is quiet. # Rule3 is preferred over Rule5. So Rule5 is suppressed by the precondition of Rule3.white(x) ::: x is white. furry(Erin, True) ::: Erin is furry. Rule5 = Or(And(prepare_armor(cat, canary), give_magnifier(cricket, canary)), Rule5)young(x) ::: x is young green(Erin, True) ::: Erin is green. # Rule4 is preferred over Rule1. So Rule1 is suppressed by the precondition of Rule4.Premises furry(Fiona, True) ::: Fiona is furry. Rule1 = Or(has_same_first_letter_name(cat, puffin), Rule1)quiet(Anne) ::: Anne is quiet. quiet(Fiona, True) ::: Fiona is quiet. # Rule6 is preferred over Rule1. So Rule1 is suppressed by the precondition of Rule6.furry(Erin) ::: Erin is furry. red(Fiona, True) ::: Fiona is red. Rule1 = Or(has_something_to_sit_on(cat), Rule1)green(Erin) ::: Erin is green. rough(Fiona, True) ::: Fiona is rough. # question: does the canary remove from the board one of the pieces of the aardvark?furry(Fiona) ::: Fiona is furry. white(Fiona, True) ::: Fiona is white. return remove_piece(canary, aardvark)quiet(Fiona) ::: Fiona is quiet. furry(Harry, True) ::: Harry is furry.red(Fiona) ::: Fiona is red. quiet(Harry, True) ::: Harry is quiet.rough(Fiona) ::: Fiona is rough. white(Harry, True) ::: Harry is white.white(Fiona) ::: Fiona is white. young($x, True) &gt;&gt;&gt; furry($x, True)) ::: Young people are furry.3. Logical exclusive disjunction of expr1 and expr2: expr1 ⊕ expr2 4. Logical negation of expr1: ¬expr1 5. expr1 implies expr2: expr1 → expr2 6. expr1 if and only if expr2: expr1 ↔ expr2 7. Logical universal quantification: ∀x 8. Logical existential quantification: ∃x Question: Based on the above information, is the following statement true, false, or unknown? Anne is white. ### Predicates furry(Harry) ::: Harry is furry. quiet(Harry) ::: Harry is quiet. Conclusion: white(Anne) ProofWriter Prompts for Pyke Solver One-shot demonstration Task Description: You are given a problem description and a question. The task is to: 1) define all the predicates in the problem 2) parse the problem into logic rules based on the defined predicates 3) write all the facts mentioned in the problem 4) parse the question into the logic form Problem: Anne is quiet. Question: Based on the above information, is the following statement true, false, or unknown? Anne is white. ### Predicates: quiet($x, bool) ::: x is quiet. furry($x, bool) ::: x is furry. quiet(Anne, True) Query: white(Anne) A.3 Incorrect Example Generation The following section includes classic Incorrect translations, more incorrect translations can be found in Processed_Datasets in https://github.com/Mattylam/Logic_Symbolic_Solvers_Experiment Example 1: Prover9 PrOntoQA_2 Parse error .......... ∀x(Dumpus(x) → Happy(x)) ::: Every dumpus is happy. ∀x(Dumpus(x) → Vumpus(x)) ::: Each dumpus is a vumpus. ∀x(Vumpus(x) → Bright(x)) ::: Vumpuses are bright. ∀x(Vumpus(x) → Jompus(x)) ::: Every vumpus is a jompus. ∀x(Jompus(x) → Large(x)) ::: Jompuses are large. Stella is a yumpus. Conclusion: Bright(Stella) ::: Stella is bright. Analysis: LLM failed to perform all translations, "Stella is yumpus" was not translated, thus system throws a parse error Example 2: Z3 FOLIO_dev_27 Incorrect Interpretation def solution(): # rules # Rule1: Regarding the cat, if it has a high-quality paper, then we can conclude that it prepares armor for the canary. Rule1 = Implies(has_high_quality_paper(cat), prepare_armor(cat, canary)) # Rule2: Rule2 = ForAll([x], Implies(roll_dice(x, eagle), proceed_to_spot_right_after(x, sun_bear))) # Rule3: If the cat prepares armor for the canary and the cricket gives a magnifier to the canary, then the canary will not remove one of the pieces of the aardvark. Rule3 = Implies(And(prepare_armor(cat, canary), give_magnifier(cricket, canary)), Not(remove_piece(canary, aardvark))) # Rule4: If the cat has a name whose first letter is the same as the first letter of the puffin's name, then the cat does not prepare armor for the canary. Rule4 = Implies(has_same_first_letter_name(cat, puffin), Not(prepare_armor(cat, canary))) # Rule5: The canary removes one of the pieces of the aardvark whenever at least one animal proceeds to the spot right after the sun bear. Rule5 = Implies(Exists([x], proceed_to_spot_right_after(x, sun_bear)), remove_piece(canary, aardvark)) # Rule6: Regarding the cat, if it has something to sit on, then we can conclude that it does not prepare armor for the canary. Rule6 = Implies(has_something_to_sit_on(cat), Not(prepare_armor(cat, canary))) # has_same_first_letter_name(cat, puffin) == False # The cat published a high-quality paper. has_high_quality_paper(cat) == True Problem: # solution in Python: # The gecko winks at the eagle.quiet(x) ::: x is quiet. winks_at(gecko, eagle) == Truefurry(x) ::: x is furry. # preferences. Rule3 is preferred over Rule5. Rule4 is preferred over Rule1. Rule6 is preferred
white(Harry) ::: Harry is white.∀x(young(x) → furry(x)) ::: Young people are furry.(quiet(Anne) → red(Anne)) ::: If Anne is quiet then Anne is red.∀x(young(x) ∧ green(x) → rough(x)) ::: Young, green people are rough.∀x(green(x) → white(x)) ::: If someone is green then they are white.∀x((furry(x) ∧ quiet(x)) → white(x)) ::: If someone is furry and quiet then they are white.∀x((young(x) ∧ white(x)) → rough(x)) ::: If someone is young and white then they are rough.∀x(red(x) → young(x)) ::: All red people are young.green($x, bool) ::: x is green.red($x, bool) ::: x is red.rough($x, bool) ::: x is rough.white($x, bool) ::: x is white.</p>
<p>Erin is furry.Erin is green.Fiona is furry.Fiona is quiet.Fiona is red.Fiona is rough.Fiona is white.Harry is furry.Harry is quiet.Harry is white.Young people are furry.If Anne is quiet then Anne is red.Young, green people are rough.If someone is green then they are white.If someone is furry and quiet then they are white.If someone is young and white then they are rough.All red people are young.
furry(Fiona)# Fiona is quiet.quiet(Fiona)# Fiona is red.red(Fiona)# Fiona is rough.rough(Fiona)# Fiona is white.white(Fiona)# Harry is furry.furry(Harry)# Harry is quiet.quiet(Harry)# Harry is white.white(Harry)# Young people are furry.ForAll([x], Implies(young(x), furry(x)))# If Anne is quiet then Anne is red.Implies(quiet(Anne), red(Anne))# Young, green people are rough.ForAll([x], Implies(And(young(x), green(x)), rough(x)))# If someone is green then they are white.ForAll([x], Implies(green(x), white(x)))# If someone is furry and quiet then they are white.ForAll([x], Implies(And(furry(x), quiet(x)), white(x)))# If someone is young and white then they are rough. : Implies(expr1, expr2) ForAll([x], Implies(And(young(x), white(x)), rough(x))) 6) expr1 if and only if expr2: expr1 == expr2 # All red people are young. 7) logical universal quantification: ForAll() ForAll([x], Implies(red(x), young(x))) 8) logical existential quantification: Exists() [Question Parse Output]: Given a problem description and a question. The task is to parse the [Problem] and the [Question] # Question: the following statement true, false, or unknown? Anne is white. into Python Z3 solver. You are meant to follow the example format and do not provide any further explanations. Keep all the # signs as symbols and do not interpret them as markdown marker. return white(Anne)--[Problem]:Anne is quiet. [Question]:Based on the above information, is the following statement true, false, or unknown? Anne is white.####[Problem Parse Output]:# Anne is quiet.quiet(Anne)# Erin is furry.furry(Erin)# Erin is green.green(Erin)# Fiona is furry.</p>
<p>Table</p>
<p>Harry is furry.Harry is quiet.Harry is white.Young people are furry.If Anne is quiet then Anne is red.Young, green people are rough.If someone is green then they are white.If someone is furry and quiet then they are white.If someone is young and white then they are rough.All red people are young.[Question]:Based on the above information, is the following statement true, false, or unknown?Anne is white.
DatasetZ3Prover9 PykeAvg_Acc Avg_Acc Avg_AccProofWriter D5 OWA 85.75%75.00%56.63%ProofWriter D3 OWA 83.04%75.37%52.37%ProofWriter D2 OWA 82.50%76.00%56.50%ProofWriter D5 CWA 87.50%78.25%60.25%ProofWriter D3 CWA 89.25%76.13%45.63%ProofWriter D2 CWA 89.63%77.12%54.75%PrOntoQA94.12%76.87%91.12%FOLIO (1 Shot)26.25%43.78%✗FOLIO (2 Shot)34.36%41.60%✗FOLIO (4 Shot)36.87%49.13%✗####[Problem Parse Output]:Predicates:quiet($x, bool) ::: x is quiet.furry($x, bool) ::: x is furry.green($x, bool) ::: x is green.red($x, bool) ::: x is red.rough($x, bool) ::: x is rough.white($x, bool) ::: x is white.young($x, bool) ::: x is youngFacts:quiet(Anne, True) ::: Anne is quiet.furry(Erin, True) ::: Erin is furry.
green(Erin, True) ::: Erin is green.furry(Fiona, True) ::: Fiona is furry.quiet(Fiona, True) ::: Fiona is quiet.red(Fiona, True) ::: Fiona is red.</p>
<p>Code and data are publicly available at https://github. com/Mattylam/Logic_Symbolic_Solvers_Experiment.
For instance, during the experiment stage, we tried to rerun the SatLM experiment on ProofWriter, but the execution rate dropped from 99% to 20%. This is caused by GPT3.5 not being able to add a complete bracket to the method Forall()). It is a surprising mistake that continues to happen.
p-values: 1.02 × 10 −18 , 5.37 × 10 −7 , 6.1 × 10 −12
A AppendixThe bald eagle is blue.The bald eagle is kind.The bald eagle likes the cat.The bald eagle does not visit the tiger.The cat chases the mouse.The cat is green.The cat likes the bald eagle.The cat likes the mouse.The cat does not like the tiger.The mouse likes the cat.The tiger chases the cat.The tiger chases the mouse.The tiger is red.The tiger likes the cat.The tiger visits the cat.The tiger visits the mouse.If something likes the bald eagle then it is blue.If something visits the bald eagle and it visits the cat then the bald eagle is red.If something chases the mouse then it visits the cat.If something is blue then it chases the tiger.If something visits the cat and the cat chases the tiger then the tiger likes the bald eagle.If something likes the tiger then the tiger likes the bald eagle.If something chases the mouse then it visits the mouse.Question:Based on the above information, is the following statement true, false, or unknown?The cat does not like the mouse.Answer: FalsePrOntoQA Example: ProntoQA Q3 Problem: Vumpuses are floral.Vumpuses are tumpuses.Tumpuses are brown.Each tumpus is a wumpus.Wumpuses are small.Each wumpus is a rompus.Each zumpus is metallic.Every rompus is happy.Rompuses are impuses.Each impus is amenable.Each impus is a dumpus.Every dumpus is not metallic.Dumpuses are numpuses.Each numpus is bitter.Each numpus is a jompus.Every jompus is cold.Each jompus is a yumpus.Wren is a tumpus.Question: Is the following statement true or false?Wren is not metallic.Answer: TrueFOLIOExample: FOLIO dev Q1Problem: If people perform in school talent shows often, then they attend and are very engaged with school events.People either perform in school talent shows often or are inactive and disinterested members of their community.If people chaperone high school dances, then they are not students who attend the school.All people who are inactive and disinterested members of their community chaperone high school dances.All young children and teenagers who wish to further their academic careers and educational opportunities are students who attend the school.Bonnie either Question: Based on the above information, is the following statement true, false, or uncertain?Marvin is an alien.Answer: False Predicted Answer: Unknown Translation from LLM: ........ # All aliens are extraterrestrial.ForAll([x], Implies(Alien(x), Extraterrestrial(x))) # If someone is from Mars, then they are aliens.ForAll([x], Implies(FromMars(x), Alien(x))) # No extraterrestrial is human.ForAll([x], Implies(Extraterrestrial(x), Not(Human(x)))) # Everyone from Earth is a human.ForAll([x], Implies(FromEarth(x), Human(x))) # Marvin cannot be from Earth and from Mars.Not(And(FromEarth(marvin), FromMars(marvin))) # If Marvin is not from Earth, then Marvin is an extraterrestrial.Implies(Not(FromEarth(marvin)), Extraterrestrial(marvin)) # Question: Marvin is an alien.return Alien(marvin) Analysis: LLM incorrectly interpreted the logic of the statement.The correct translation should be Xor(And(FromEarth(marvin), FromMars(marvin))), the wording of the statement makes it ambiguous for LLMs to interpret.If the statement was, Marvin is either from Earth or from Mars, LLMs would have been able to translate it.Example 3: Z3 FOLIO_dev_16 Difficult predicate extractionProblem: All rabbits are cute.Some turtles exist.An animal is either a rabbit or a squirrel.If something is skittish, then it is not still.All squirrels are skittish.Rock is still.Question: Based on the above information, is the following statement true, false, or uncertain?Rock is a turtle.Answer: Unknown Error: ExecutionError NameError: name 'Exist' is not defined.Did you mean: 'exit'?"Translation from LLM: ........All rabbits are cute.ForAll([x], Implies(Rabbit(x), Cute(x))) # Some turtles exist.Exist([x], Turtle(x)) # An animal is either a rabbit or a squirrel.ForAll([x], Or(Rabbit(x), Squirrel(x))) # If something is skittish, then it is not still.ForAll([x], Implies(Skittish(x), Not(Still(x)))) # All squirrels are skittish.[Problem]: Anne is quiet.Erin is furry.Erin is green.Fiona is furry.Fiona is quiet.Fiona is red.Fiona is rough.Fiona is white.Harry is furry.Harry is quiet.Harry is white.Young people are furry.If Anne is quiet then Anne is red.Young, green people are rough.If someone is green then they are white.If someone is furry and quiet then they are white.If someone is young and white then they are rough.All red people are young.[Question]: Based on the above information, is the following statement true, false, or unknown?Anne is white.#### [Problem Parse Output]: Predicates quiet(x) ::: x is quiet.furry(x) ::: x is furry.green(x) ::: x is green.red(x) ::: x is red.rough(x) ::: x is rough.white(x) ::: x is white.young(x) ::: x is young Premises quiet(Anne) ::: Anne is quiet.furry(Erin) ::: Erin is furry.green(Erin) ::: Erin is green.furry(Fiona) ::: Fiona is furry.quiet(Fiona) ::: Fiona is quiet.red(Fiona) ::: Fiona is red.rough(Fiona) ::: Fiona is rough.
Language models show human-like content effects on reasoning. Ishita Dasgupta, Andrew K Lampinen, C Y Stephanie, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, 10.48550/ARXIV.2207.07051CoRR, abs/2207.070512022</p>
<p>Z3: an efficient SMT solver. Leonardo Mendonça, De Moura, Nikolaj S Bjørner, 10.1007/978-3-540-78800-3_24Tools and Algorithms for the Construction and Analysis of Systems, 14th International Conference, TACAS 2008, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2008. Lecture Notes in Computer Science. HungarySpringer2008. March 29-April 6, 20084963Proceedings</p>
<p>Jiazhan Feng, Ruochen Xu, Junheng Hao, Hiteshi Sharma, Yelong Shen, Dongyan Zhao, Weizhu Chen, arXiv:2311.06158Language models can be logical solvers. 2023arXiv preprint</p>
<p>Applying expert system technology to code reuse with pyke. Bruce Frederiksen, 2008PyCon: Chicago</p>
<p>PAL: program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. July 2023202of Proceedings of Machine Learning Research</p>
<p>FOLIO: natural language reasoning with first-order logic. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R Joty, Alexander R Fabbri, Wojciech Kryscinski, 10.48550/ARXIV.2209.00840CoRR, abs/2209.008402022Xi Victoria Lin, Caiming Xiong, and Dragomir Radev</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Comput. Surv. 5512382023</p>
<p>Boardgameqa: A dataset for natural language reasoning with contradictory information. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. 2023. December 10 -16, 2023Xin Xu, Vaiva Imbrasaite, and Deepak Ramachandran</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Confidence matters: Revisiting intrinsic selfcorrection capabilities of large language models. Loka Li, Guangyi Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric P Xing, Kun Zhang, 10.48550/ARXIV.2402.12563CoRR, abs/2402.125632024</p>
<p>Faithful chain-ofthought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, 10.48550/ARXIV.2301.13379CoRR, abs/2301.133792023</p>
<p>William Mccune, arXiv preprint cs/0310056Otter 3.3 reference manual. 2003</p>
<p>Release of prover9. William Mccune, Mile high conference on quasigroups, loops and nonassociative systems. Denver, Colorado2005</p>
<p>Expert systems in production planning and scheduling: A state-of-the-art survey. Kostas S Metaxiotis, Dimitris Askounis, John E Psarras, 10.1023/A%3A1016064126976J. Intell. Manuf. 1342002</p>
<p>Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. I Maxwell, Michael Henry Nye, Joshua B Tessler, Brenden M Tenenbaum, Lake, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems. NeurIPS2021. 2021. 2021. December 6-14, 2021</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, Roger Levy, 10.18653/v1/2023.emnlp-main.313Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Testing the general deductive reasoning capacity of large language models using OOD examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Mehran Kazemi, Najoung Kim, He He, 10.18653/V1/2021.FINDINGS-ACL.317Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, New Orleans, LA, USAAssociation for Computational Linguistics2023. 2023. December 10 -16, 2023. 2021. August 1-6, 2021ACL/IJCNLP 2021 of Findings of ACL</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>furry(Harry, True) ::: Harry is furry. quiet(Harry, True) ::: Harry is quiet. white(Harry, True) ::: Harry is white. young($x, True) &gt;&gt;&gt; furry($x, True)) ::: Young people are furry. quiet(Anne, True) &gt;&gt;&gt; red(Anne, True)) ::: If Anne is quiet then Anne is red. young($x, True) &amp;&amp; green($x, True) &gt;&gt;&gt; rough($x, True) ::: Young, green people are rough. green($x, True) &gt;&gt;&gt; white($x, True) ::: If someone is green then they are white. furry($x, True) &amp;&amp; quiet($x, True) &gt;&gt;&gt; white($x, True) ::: If someone is furry and quiet then they are white. young($x, True) &amp;&amp; white($x, True) &gt;&gt;&gt; rough($x, True) ::: If someone is young and white then they are rough. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: rough(Fiona, True) ::: Fiona is rough. white(Fiona, True) ::: Fiona is white. 2022Chain-of-thought prompting elicits reasoning in large language models. red($x, True) &gt;&gt;&gt; young($x, True) ::: All red people are young. Question Parse Output]: Query: white(Anne</p>            </div>
        </div>

    </div>
</body>
</html>
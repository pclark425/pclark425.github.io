<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9220 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9220</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9220</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-273377037</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.02174v1.pdf" target="_blank">Behavioral Sequence Modeling with Ensemble Learning</a></p>
                <p><strong>Paper Abstract:</strong> We investigate the use of sequence analysis for behavior modeling, emphasizing that sequential context often outweighs the value of aggregate features in understanding human behavior. We discuss framing common problems in fields like healthcare, finance, and e-commerce as sequence modeling tasks, and address challenges related to constructing coherent sequences from fragmented data and disentangling complex behavior patterns. We present a framework for sequence modeling using Ensembles of Hidden Markov Models, which are lightweight, interpretable, and efficient. Our ensemble-based scoring method enables robust comparison across sequences of different lengths and enhances performance in scenarios with imbalanced or scarce data. The framework scales in real-world scenarios, is compatible with downstream feature-based modeling, and is applicable in both supervised and unsupervised learning settings. We demonstrate the effectiveness of our method with results on a longitudinal human behavior dataset.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9220.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9220.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformers (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer neural network architectures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based neural architectures are mentioned as a class of deep-learning models that have shown success on sequence and intrusion-detection tasks but are noted to have drawbacks like high computational cost and reduced interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>sequential/time-series (e.g., network traffic, temporal data)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>network intrusion detection, other sequence-based domains (general mention in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>rare events / intrusion / anomaly detection (general)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned as neural-network-based approaches that have been applied to sequence and intrusion-detection problems (no specific method described or evaluated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Not applicable in-paper (general literature mention).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper for transformers (general mention).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Discussed qualitatively: transformers (and other deep models) have been successful on some tasks but face drawbacks relative to HMM ensembles (higher compute, risk of overfitting, lower interpretability).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High computational cost, overfitting, reduced interpretability (explicitly noted in Background & Related Work).</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Paper emphasizes that despite advances in deep models including transformers, lightweight interpretable sequence models (HMM ensembles) can match or approach performance while being faster, smaller, and more interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Behavioral Sequence Modeling with Ensemble Learning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9220.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9220.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reorder (CNN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reorder (CNN-based deep learning algorithm used in GLOBEM benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CNN-based deep-learning method (referred to as 'Reorder' in the GLOBEM benchmark) used as the top-performing comparator for the GLOBEM depression-detection task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reorder</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>convolutional neural network (CNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>longitudinal human behavior time-series (multi-feature daily sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>longitudinal human behavior / depression detection (GLOBEM dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>classification of depressed vs not-depressed (framed as imbalanced detection task)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>CNN-based sequence model used in the benchmark; in this paper it is used as a baseline comparator (no internal implementation details provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against HMM and HMM-e (this paper), SVM, Random Forest (benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AUC-ROC and balanced accuracy (metrics used in paper comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>HMM-e falls short of Reorder by 2.7 percentage points in AUC-ROC and 2.2 percentage points in balanced accuracy (reported as comparison in Results & Discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Reorder (CNN) is the best-performing deep-learning approach in the GLOBEM benchmark; HMM-e achieves similar performance but is slightly worse on the listed metrics while using fewer features and far fewer parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Implied limitations compared to HMM-e: larger model parameter count and longer training time (Reorder reported 18 minutes vs HMM-e 5 minutes on same hardware), greater model complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Despite Reorder's stronger absolute metrics, HMM-e achieves competitive performance with far fewer features and simpler models, showing a favorable trade-off between complexity, interpretability, and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Behavioral Sequence Modeling with Ensemble Learning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9220.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9220.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Credit-card fraud Transformer (citation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Credit Card Fraud Detection Using Advanced Transformer Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced paper applying transformer architectures to credit-card fraud detection (cited in the references as a recent transformer application to tabular/transactional anomaly detection).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Credit Card Fraud Detection Using Advanced Transformer Model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (as used in cited paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>transactional/tabular time-sequenced financial data (credit card transactions)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>financial transactions / credit card fraud detection</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>fraud / rare-event detection</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as an example of applying an advanced transformer model to credit-card fraud detection (no details or experiments from that paper are reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Not specified within this paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>No direct comparison in this paper; transformers are discussed in related work as successful but costly and less interpretable relative to HMM ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes general limitations of neural models (compute, overfitting, interpretability); no fraud-paper-specific failure cases are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Cited to show that transformer architectures are being explored for anomaly/fraud detection on transactional/tabular/sequential financial data, motivating alternative lightweight approaches like HMM-e.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Behavioral Sequence Modeling with Ensemble Learning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9220.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9220.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IDS-INT (transformer intrusion detection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IDS-INT: Intrusion detection system using transformer-based transfer learning for imbalanced network traffic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work applying transformer-based transfer learning to imbalanced network-traffic intrusion detection problems (used as an example of applying transformers to anomaly detection in sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>IDS-INT: Intrusion detection system using transformer-based transfer learning for imbalanced network traffic.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (IDS-INT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (transfer learning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>network traffic sequences / time-series (tabular flow records)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>network intrusion detection (cybersecurity)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>intrusion / anomalous network events (rare classes)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as an example of transformer-based transfer-learning approaches for imbalanced network traffic; no implementation or experiments from IDS-INT are performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Not specified in this paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Discussed only qualitatively: modern neural methods (including transformers) can be effective but carry costs in compute and interpretability compared to HMM-e.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>General neural-model limitations noted (compute, overfitting, interpretability); no IDS-INT-specific failures reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Serves as a literature pointer demonstrating that transformer-based methods are being explored for anomaly detection in sequence/tabular network data; motivates the HMM-e alternative for scalable, interpretable sequence modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Behavioral Sequence Modeling with Ensemble Learning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9220.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9220.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoencoder anomaly detection (depression)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classifying Depression in Imbalanced Datasets Using an Autoencoder-Based Anomaly Detection Approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced paper that applies an autoencoder-based anomaly detection method to classifying depression in imbalanced datasets, included in related work to illustrate one-class / anomaly approaches to rare-event detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Classifying Depression in Imbalanced Datasets Using an Autoencoder-Based Anomaly Detection Approach.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoencoder-based anomaly detector</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoencoder (neural network)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>feature vectors / time-aggregated behavioral features (longitudinal human behavior data)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>health / depression detection</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>rare-class detection (depressed vs not-depressed framed as anomaly)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Autoencoder-based one-class / anomaly-detection approach cited as related work for imbalanced classification tasks; this paper does not run the autoencoder approach itself.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Mentioned in related work context (one-class anomaly detection approaches); not compared directly in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper for the autoencoder (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>No direct comparison within this paper; HMM-e is proposed as an alternative that can operate in supervised and unsupervised (label-free clustering) settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>General discussion of class imbalance challenges and the need to robustly model the nominal class; no autoencoder-specific failures reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Paper positions HMM-e as a lightweight, interpretable ensemble alternative to neural anomaly-detection approaches like autoencoders, while highlighting that HMM-e can be used both for supervised classification and unsupervised clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Behavioral Sequence Modeling with Ensemble Learning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Credit Card Fraud Detection Using Advanced Transformer Model. <em>(Rating: 2)</em></li>
                <li>IDS-INT: Intrusion detection system using transformer-based transfer learning for imbalanced network traffic. <em>(Rating: 2)</em></li>
                <li>Classifying Depression in Imbalanced Datasets Using an Autoencoder-Based Anomaly Detection Approach. <em>(Rating: 2)</em></li>
                <li>Deep neural networks for bot detection. <em>(Rating: 1)</em></li>
                <li>Reorder <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9220",
    "paper_id": "paper-273377037",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "Transformers (general)",
            "name_full": "Transformer neural network architectures",
            "brief_description": "Transformer-based neural architectures are mentioned as a class of deep-learning models that have shown success on sequence and intrusion-detection tasks but are noted to have drawbacks like high computational cost and reduced interpretability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Transformer",
            "model_type": "transformer",
            "model_size": null,
            "data_type": "sequential/time-series (e.g., network traffic, temporal data)",
            "data_domain": "network intrusion detection, other sequence-based domains (general mention in related work)",
            "anomaly_type": "rare events / intrusion / anomaly detection (general)",
            "method_description": "Mentioned as neural-network-based approaches that have been applied to sequence and intrusion-detection problems (no specific method described or evaluated in this paper).",
            "baseline_methods": "Not applicable in-paper (general literature mention).",
            "performance_metrics": "Not reported in this paper for transformers (general mention).",
            "performance_results": "",
            "comparison_to_baseline": "Discussed qualitatively: transformers (and other deep models) have been successful on some tasks but face drawbacks relative to HMM ensembles (higher compute, risk of overfitting, lower interpretability).",
            "limitations_or_failure_cases": "High computational cost, overfitting, reduced interpretability (explicitly noted in Background & Related Work).",
            "unique_insights": "Paper emphasizes that despite advances in deep models including transformers, lightweight interpretable sequence models (HMM ensembles) can match or approach performance while being faster, smaller, and more interpretable.",
            "uuid": "e9220.0",
            "source_info": {
                "paper_title": "Behavioral Sequence Modeling with Ensemble Learning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Reorder (CNN)",
            "name_full": "Reorder (CNN-based deep learning algorithm used in GLOBEM benchmark)",
            "brief_description": "A CNN-based deep-learning method (referred to as 'Reorder' in the GLOBEM benchmark) used as the top-performing comparator for the GLOBEM depression-detection task.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Reorder",
            "model_type": "convolutional neural network (CNN)",
            "model_size": null,
            "data_type": "longitudinal human behavior time-series (multi-feature daily sequences)",
            "data_domain": "longitudinal human behavior / depression detection (GLOBEM dataset)",
            "anomaly_type": "classification of depressed vs not-depressed (framed as imbalanced detection task)",
            "method_description": "CNN-based sequence model used in the benchmark; in this paper it is used as a baseline comparator (no internal implementation details provided here).",
            "baseline_methods": "Compared against HMM and HMM-e (this paper), SVM, Random Forest (benchmarks).",
            "performance_metrics": "AUC-ROC and balanced accuracy (metrics used in paper comparisons).",
            "performance_results": "HMM-e falls short of Reorder by 2.7 percentage points in AUC-ROC and 2.2 percentage points in balanced accuracy (reported as comparison in Results & Discussion).",
            "comparison_to_baseline": "Reorder (CNN) is the best-performing deep-learning approach in the GLOBEM benchmark; HMM-e achieves similar performance but is slightly worse on the listed metrics while using fewer features and far fewer parameters.",
            "limitations_or_failure_cases": "Implied limitations compared to HMM-e: larger model parameter count and longer training time (Reorder reported 18 minutes vs HMM-e 5 minutes on same hardware), greater model complexity.",
            "unique_insights": "Despite Reorder's stronger absolute metrics, HMM-e achieves competitive performance with far fewer features and simpler models, showing a favorable trade-off between complexity, interpretability, and performance.",
            "uuid": "e9220.1",
            "source_info": {
                "paper_title": "Behavioral Sequence Modeling with Ensemble Learning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Credit-card fraud Transformer (citation)",
            "name_full": "Credit Card Fraud Detection Using Advanced Transformer Model",
            "brief_description": "A referenced paper applying transformer architectures to credit-card fraud detection (cited in the references as a recent transformer application to tabular/transactional anomaly detection).",
            "citation_title": "Credit Card Fraud Detection Using Advanced Transformer Model.",
            "mention_or_use": "mention",
            "model_name": "Transformer (as used in cited paper)",
            "model_type": "transformer",
            "model_size": null,
            "data_type": "transactional/tabular time-sequenced financial data (credit card transactions)",
            "data_domain": "financial transactions / credit card fraud detection",
            "anomaly_type": "fraud / rare-event detection",
            "method_description": "Cited as an example of applying an advanced transformer model to credit-card fraud detection (no details or experiments from that paper are reproduced here).",
            "baseline_methods": "Not specified within this paper (reference only).",
            "performance_metrics": "Not reported in this paper (reference only).",
            "performance_results": "",
            "comparison_to_baseline": "No direct comparison in this paper; transformers are discussed in related work as successful but costly and less interpretable relative to HMM ensembles.",
            "limitations_or_failure_cases": "Paper notes general limitations of neural models (compute, overfitting, interpretability); no fraud-paper-specific failure cases are reported here.",
            "unique_insights": "Cited to show that transformer architectures are being explored for anomaly/fraud detection on transactional/tabular/sequential financial data, motivating alternative lightweight approaches like HMM-e.",
            "uuid": "e9220.2",
            "source_info": {
                "paper_title": "Behavioral Sequence Modeling with Ensemble Learning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "IDS-INT (transformer intrusion detection)",
            "name_full": "IDS-INT: Intrusion detection system using transformer-based transfer learning for imbalanced network traffic",
            "brief_description": "A cited work applying transformer-based transfer learning to imbalanced network-traffic intrusion detection problems (used as an example of applying transformers to anomaly detection in sequences).",
            "citation_title": "IDS-INT: Intrusion detection system using transformer-based transfer learning for imbalanced network traffic.",
            "mention_or_use": "mention",
            "model_name": "Transformer (IDS-INT)",
            "model_type": "transformer (transfer learning)",
            "model_size": null,
            "data_type": "network traffic sequences / time-series (tabular flow records)",
            "data_domain": "network intrusion detection (cybersecurity)",
            "anomaly_type": "intrusion / anomalous network events (rare classes)",
            "method_description": "Referenced as an example of transformer-based transfer-learning approaches for imbalanced network traffic; no implementation or experiments from IDS-INT are performed in this paper.",
            "baseline_methods": "Not specified in this paper (reference only).",
            "performance_metrics": "Not reported in this paper.",
            "performance_results": "",
            "comparison_to_baseline": "Discussed only qualitatively: modern neural methods (including transformers) can be effective but carry costs in compute and interpretability compared to HMM-e.",
            "limitations_or_failure_cases": "General neural-model limitations noted (compute, overfitting, interpretability); no IDS-INT-specific failures reported here.",
            "unique_insights": "Serves as a literature pointer demonstrating that transformer-based methods are being explored for anomaly detection in sequence/tabular network data; motivates the HMM-e alternative for scalable, interpretable sequence modeling.",
            "uuid": "e9220.3",
            "source_info": {
                "paper_title": "Behavioral Sequence Modeling with Ensemble Learning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Autoencoder anomaly detection (depression)",
            "name_full": "Classifying Depression in Imbalanced Datasets Using an Autoencoder-Based Anomaly Detection Approach",
            "brief_description": "A referenced paper that applies an autoencoder-based anomaly detection method to classifying depression in imbalanced datasets, included in related work to illustrate one-class / anomaly approaches to rare-event detection.",
            "citation_title": "Classifying Depression in Imbalanced Datasets Using an Autoencoder-Based Anomaly Detection Approach.",
            "mention_or_use": "mention",
            "model_name": "Autoencoder-based anomaly detector",
            "model_type": "autoencoder (neural network)",
            "model_size": null,
            "data_type": "feature vectors / time-aggregated behavioral features (longitudinal human behavior data)",
            "data_domain": "health / depression detection",
            "anomaly_type": "rare-class detection (depressed vs not-depressed framed as anomaly)",
            "method_description": "Autoencoder-based one-class / anomaly-detection approach cited as related work for imbalanced classification tasks; this paper does not run the autoencoder approach itself.",
            "baseline_methods": "Mentioned in related work context (one-class anomaly detection approaches); not compared directly in experiments here.",
            "performance_metrics": "Not reported in this paper for the autoencoder (reference only).",
            "performance_results": "",
            "comparison_to_baseline": "No direct comparison within this paper; HMM-e is proposed as an alternative that can operate in supervised and unsupervised (label-free clustering) settings.",
            "limitations_or_failure_cases": "General discussion of class imbalance challenges and the need to robustly model the nominal class; no autoencoder-specific failures reported here.",
            "unique_insights": "Paper positions HMM-e as a lightweight, interpretable ensemble alternative to neural anomaly-detection approaches like autoencoders, while highlighting that HMM-e can be used both for supervised classification and unsupervised clustering.",
            "uuid": "e9220.4",
            "source_info": {
                "paper_title": "Behavioral Sequence Modeling with Ensemble Learning",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Credit Card Fraud Detection Using Advanced Transformer Model.",
            "rating": 2,
            "sanitized_title": "credit_card_fraud_detection_using_advanced_transformer_model"
        },
        {
            "paper_title": "IDS-INT: Intrusion detection system using transformer-based transfer learning for imbalanced network traffic.",
            "rating": 2,
            "sanitized_title": "idsint_intrusion_detection_system_using_transformerbased_transfer_learning_for_imbalanced_network_traffic"
        },
        {
            "paper_title": "Classifying Depression in Imbalanced Datasets Using an Autoencoder-Based Anomaly Detection Approach.",
            "rating": 2,
            "sanitized_title": "classifying_depression_in_imbalanced_datasets_using_an_autoencoderbased_anomaly_detection_approach"
        },
        {
            "paper_title": "Deep neural networks for bot detection.",
            "rating": 1,
            "sanitized_title": "deep_neural_networks_for_bot_detection"
        },
        {
            "paper_title": "Reorder",
            "rating": 2
        }
    ],
    "cost": 0.01051075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Behavioral Sequence Modeling with Ensemble Learning
4 Nov 2024</p>
<p>Maxime Kawawa-Beaudan maxime.kawawa-beaudan@jpmorgan.com 
AI Research
383 Madison AvenueNew York, NYUSA</p>
<p>J P Morgan 
AI Research
383 Madison AvenueNew York, NYUSA</p>
<p>AI Research
383 Madison AvenueNew York, NYUSA</p>
<p>AI Research
383 Madison AvenueNew York, NYUSA</p>
<p>AI Research
383 Madison AvenueNew York, NYUSA</p>
<p>AI Research
383 Madison AvenueNew York, NYUSA</p>
<p>Srijan Sood srijan.sood@jpmorgan.com 
AI Research
383 Madison AvenueNew York, NYUSA</p>
<p>Soham Palande soham.palande@jpmorgan.com 
AI Research
383 Madison AvenueNew York, NYUSA</p>
<p>Ganapathy Mani ganapathy.mani@jpmorgan.com 
AI Research
383 Madison AvenueNew York, NYUSA</p>
<p>Tucker Balch tucker.balch@emory.edu 
Emory University
1300 Clifton Road NEAtlantaGAUSA</p>
<p>Manuela Veloso manuela.veloso@jpmorgan.com 
AI Research
383 Madison AvenueNew York, NYUSA</p>
<p>Behavioral Sequence Modeling with Ensemble Learning
4 Nov 20248F7A1234CAAF349051C2366FDBCD48DCarXiv:2411.02174v1[cs.LG]
We investigate the use of sequence analysis for behavior modeling, emphasizing that sequential context often outweighs the value of aggregate features in understanding human behavior.We discuss framing common problems in fields like healthcare, finance, and e-commerce as sequence modeling tasks, and address challenges related to constructing coherent sequences from fragmented data and disentangling complex behavior patterns.We present a framework for sequence modeling using Ensembles of Hidden Markov Models, which are lightweight, interpretable, and efficient.Our ensemble-based scoring method enables robust comparison across sequences of different lengths and enhances performance in scenarios with imbalanced or scarce data.The framework scales in real-world scenarios, is compatible with downstream feature-based modeling, and is applicable in both supervised and unsupervised learning settings.We demonstrate the effectiveness of our method with results on a longitudinal human behavior dataset.Disclaimer: This paper was prepared for informational purposes by the Artificial Intelligence Research group of JPMorgan Chase &amp; Co. and its affiliates ("JP Morgan") and is not a product of the Research Department of JP Morgan.JP Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein.This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.</p>
<p>Introduction</p>
<p>Modeling human behavior is a complex task with applications spanning multiple domains such as user research, healthcare, payments, trading, and e-commerce.Applications range from classifying human activity (28), distinguishing humans from bots (16), detecting credit card fraud (25) etc.</p>
<p>Behavior is often captured as sequences of actions or events over time, and understanding patterns within these sequences is crucial for tasks like classification, anomaly detection, and user modeling.</p>
<p>A key challenge in utilizing the captured sequences is class imbalance, where underrepresented behavior profiles or a disproportionate number of anomalous examples hinder model generalization.</p>
<p>Many solutions leverage complex deep learning models (33), focusing on event-level classification with extensive feature engineering (6).However, such event-level or feature-aggregated methods frequently fall short in capturing the sequential dynamics essential for understanding and modeling behavior.These approaches are also susceptible to overfitting, with performance rapidly degrading in class-imbalanced scenarios.</p>
<p>Sequential context is crucial for effective behavior modeling.For example, in credit card fraud detection or anti-money laundering, a user's transaction history provides deeper insights into behavioral intent than isolated transactions or aggregated features.Yet, many datasets and approaches remain confined to the event level, overlooking the broader sequential context.In this study, we advocate for a sequence modeling approach to behavior modeling, particularly in scenarios where behaviors are represented as action sequences derived from unstructured data.Aggregating this data into coherent sequences that reflect an agent's decision-making process is a non-trivial challenge.Our method is tested in a real-world setting characterized by extreme class imbalance, with millions of diverse user behavior examples contrasted against only a few hundred instances of the anomalous class.</p>
<p>We propose a novel and lightweight ensemble-based framework for behavior modeling, and show efficacy on downstream (imbalanced) sequence classification tasks.While our approach is modelagnostic, we leverage Hidden Markov Models (HMMs) for their simplicity, interpratibility, and efficacy at capturing temporal dependencies and latent patterns.Our real-world deployment scales to millions of sequences, while being compatible with downstream machine learning methods.We demonstrate its effectiveness on a publicly available human behavior dataset (31).</p>
<p>Our contributions: (1) We introduce a behavior modeling framework based on sequences of events/actions, applicable to various domains; (2) We outline its application to supervised and unsupervised tasks; (3) We demonstrate its effectiveness on a longitudinal human behavior dataset.</p>
<p>Background &amp; Related Work</p>
<p>HMMs Hidden Markov Models (HMMs) are statistical models for sequential data, which have a long history of use in natural language processing, finance, and bioinformatics (24; 5; 32; 22).HMMs have been used extensively for behavior modeling, including sensor surveillance (21), humancomputer interfaces (11), and web user interactions (15), with recent applications in social media bot detection (19).While neural network-based approaches like CNNs, LSTMs, and Transformers have shown success in settings like sentiment analysis (13) and network intrusion detection (27), they face challenges such as high computational cost, overfitting, and reduced interpretability.</p>
<p>Resolution Event-level classification still dominates in areas like anti-money laundering and network security, where sequence-level labels are often missing (1; 8).This lends itself to aggregate feature based approaches, missing key historical context.While some work has tackled sequence modeling in network intrusion detection with favorable results (26), much remains to be done.</p>
<p>Data Imbalance and Anomaly Detection Many real-world problems, including intrusion detection (12), credit card fraud (25), and money laundering (2), involve detecting rare events and suffer from class imbalance.One-class anomaly detection focuses on robustly modeling the nominal class and identifying deviations (9), while more targeted approaches model both normal and anomalous sequences to detect specific behavioral anomalies (10).</p>
<p>Approach</p>
<p>Consider a sequence observation O = {a 1 , a 2 , ..., a T }, where each a i is drawn from a discrete set of actions A. Such sequences can represent various behaviors, such as user interactions in an app, trading actions in financial markets, or other human decision-making processes.Our goal is to model these behaviors, either discovering behavior clusters, or classifying behaviors when labels are available (e.g., online bot detection, credit card fraud detection, physical activity recognition (20; 33; 18).</p>
<p>Sequence Construction</p>
<p>Figure 1: Flow diagram of our sequence construction approach, as detailed in Section 3.1.We disentangle the monolithic dataset D into data streams, then process these further into sets of observation sequences.Our subsequent HMM-e ensemble training approach is detailed in Section 3.2.While we adopt this approach using HMMs, the framework itself is model agnostic.The training data is broken into random subsets, and a diverse ensemble of learners is trained on these subsets.</p>
<p>Sequence Construction</p>
<p>One of the primary challenges lies not in modelling but in organizing coherent data streams from raw, fragmented data D, which often contains interwoven behaviors from multiple agents/users.For instance, in trading, D may span billions of transactions across participants, assets, and exchanges, requiring grouping data streams by participant, and further by exchange or asset to capture specific behaviors.In network analysis, interactions between devices and servers can be grouped by source IP for individual user activity, or further by target IP to constitute specific behavior streams.</p>
<p>As illustrated in Fig. 1, we begin by disentangling D into separate data streams D 1 , ..., D H , each corresponding to one of H agents. Feature engineering refines these data streams through dimensionality reduction, tokenization, or discretization, enhancing model generalization, particularly in the presence of imbalanced or sparse datasets.Continuous features can also be normalized and estimated directly, through techniques like Gaussian HMMs (23).</p>
<p>Once data is organized into streams
D h , sequences of observations O (1) h , ..., O (n[h]) h
are then extracted from each stream, with domain knowledge or sessions guiding the sequence span (start and end points).For example, web user behavior may span minutes to hours, whereas medical trial observations could extend over days or weeks.Breaks in continuous data streams often demarcate sequences, with shorter pauses treated as wait events and longer breaks as sequence endpoints.The number of sequences can vary significantly across agents, reflecting differing activity levels (e.g.power users vs intermittent monthly users).</p>
<p>Ensembles of Hidden Markov Models</p>
<p>Singleton HMMs First, considering binary sequence classification, we separate training data by class and train two indivudal HMMs: one positive class HMM  + , and one negative class HMM   .Given an unseen sequence O, the predicted class c(O) is determined by comparing the likelihoods:
c(O) = 1{p(O |  + ) &gt; p(O |   )}(1)
Variable Sequence Length HMMs excel at sequence analysis ( 5), but struggle when comparing sequences of varying length, as length influences likelihood computation exponentially.We address this through model-driven normalization, computing likelihoods for a given sequence across multiple models, and deriving a rank-based composite score (rather than comparing sequence likelhihoods).</p>
<p>HMM Ensembles HMMs, while lightweight and efficient, can struggle to capture the complexity of behaviors in training data when using a singular model (per class).Ensemble methods train multiple models on subsets of the data (7), enabling each learner to specialize on distinct patterns or behaviors, while collectively capturing the full data distribution.This results in a more robust approach, particularly in scenarios with data imbalance, where monolithic models skew towards modeling the majority class (or underfitting for class specific models) (17).We propose HMM-e, an ensemble framework that computes composite scores from individual learners (14).While HMMs are effective for our case, this framework is model-agnostic and can incorporate other model classes such as neural networks, SVMs, or decision trees.</p>
<p>Formalization and Algorithmic Framework</p>
<p>First, we train N models { + 1 , ...,  + N } on the positive class and M models {  1 , ...,   M } on the negative class, taking care to ensure diversity among the models by training each on a randomly selected subset of samples from the training data.Each model sees s% of the training data in its relevant class.While we set N = M for all settings, these parameters (M, N, s) can be established using typical hyperparameter optimization approaches.For any given sequence in the training data, the probability of not being selected for any model's random subset is (1  s) N .The expected number of unsampled sequences is the same, so it is important to select s and N to keep this proportion of the training data small.</p>
<p>For an unseen observation sequence O, we compute its likelihood scores under all models:
{p(O |  + 1 ), ..., p(O |  + N )} and {p(O |   1 ), ..., p(O |   M )}.
We then compute a composite score:
s(O) = N i=1 M j=1 1{p(O |  + i ) &gt; p(O |   j )}(2)
The score s(O) represents the pairwise comparisons where positive-class models assign a higher likelihood than negative-class models, taking values in [0, N  M ].A low score indicates that the sequence is more likely under the negative-class models, and vice versa.As likelihoods across different sequence lengths are not directly compared, this composite score acts as an implicit normalization technique.N and M should be chosen such that the score range adequately distinguishes the classes.</p>
<p>For our HMM and HMM-e approaches, we use 3 states in each of our models, and converge on an ensemble size of 250 and a subset factor of 1%.We perform hyperparameter search for ensemble size, trying other values in [10, 50, 100, 500, 1000].We settle on 250 for its good performance at a relatively low complexity.</p>
<p>Downstream Modeling using HMM-e Scores Given a corpus of sequences and corresponding scores {O, s(O)}, we classify sequences using a threshold s thresh : c
(O i ) = 1{s(O i )  s thresh }.
Alternatively, base learner likelihoods can serve as features for downstream classifiers.For each sequence O i , we define a feature vector
f i = p(O i | + 1 ), . . . , p(O i | + N ), p(O i |  1 ), . . . , p(O i |  M )(3)
To account for sequence length sensitivity, f i is normalized by ||f i || 2 .This technique utilizes HMMs as feature extractors, where each feature p(O i | j ) represents the similarity between the sequence O i , and the random subset of training data underlying  j .</p>
<p>Clustering HMM-e Scores in Unsupervised Settings</p>
<p>In label-free settings, behavior clustering can be achieved using unsupervised learning approaches.We train N models { 1 , ...,  N } on random s% data subsets and generate feature vectors of base learner likelihoods f i (Section 3.2.1).Unsupervised clustering like K-Means can be applied to discover behavioral groups, dimensionality reduction (e.g., PCA) can be helpful when N is large.</p>
<p>Experiments Data</p>
<p>We evaluate our approach on the GLOBEM dataset (31), a longitudinal human behavior study featuring over 3,700 attributes from 497 participants across four years (2018-2021).The dataset includes survey, smartphone, and wearable data, with a focus on depression detection (the task we consider).Data includes mood assessments, step counts, location variability, and sleep metrics.We utilize the data standardization platform for reproducibility provided by the authors (30).</p>
<p>SVM (4)</p>
<p>Random Forest ( 29 While GLOBEM includes thousands of features, we select just four features to employ, each evaluated daily: smartphone moving/static time ratio, total screen time (minutes), total sleep time (minutes), and total steps.We train Gaussian HMMs on these continuous features, depicted for three anonymous participants in 4. Our small feature set allows us to learn meaningful correlations and avoid converging to degenerate or redundant models due to insufficient samples.Most deep-learning based models included in the benchmark leverage the 14-day history versions of the provided features, which sum over the prescribed time period.This helps reduce the frequency of missing values, but results in a lagging time series.We use the raw feature for the current day, rather than the 14-day history, which we find boosts performance by  3% for our HMM-e approach.Sequences are constructed from a 28-day history of normalized features, aggregated by participant and preprocessed using the provided data platform.We filter out days where more than half of these features are missing, and within each study participant, fill remaining missing values using median imputation.After filtering, we have 5,393 training samples from 2018, 3,228 from 2019, 2,036 from 2020, and 1,192 from 2020, with class ratios (not-depressed to depressed) of 1.13, 1.23, 1.29, and 1.14 respectively.</p>
<p>Evaluation We use the "all-but-one" validation scheme (30), training on three years and testing on the fourth.Performance metrics include AUC-ROC and balanced accuracy (average of specificity and sensitivity), which we adopt as in (31) for its robustness to class imbalance (3).We compare our approach to the top-performing model from the GLOBEM study (31), Reorder, a CNN-based deep learning algorithm, along with a traditional SVM-based method (Canzian et al. ( 4)) from the benchmark.We also compare against a Random Forest-based approach included in the benchmark, based on Wahle et al. (29).We train the Random Forest approach on our selected four features rather than the original paper's six, using 450 trees, with the number of leaf nodes selected via K-Folds cross validation on a small training subset.</p>
<p>Results &amp; Discussion Our approach using just four features outperforms machine learning approaches like (4) which uses up to 17 features, and performs comparably to or outperforms many other machine learning approaches included in the benchmark in (31).Our mean AUC-ROC and balanced accuracy using singleton HMMs beat out (4) by 1.9 and 2 percentage points, respectively.Using HMM-e , our AUC-ROC and balanced accuracy beat the same by 3.7 and 2.7 percentage points.In terms of balanced accuracy, HMM-e outperforms (29) 1.8 percentage points while achieving the same AUC-ROC.We also achieve similar performance as the complex deep-learning approach Reorder, falling 2.7 percentage points short in AUC-ROC and 2.2 in balanced accuracy.Notably, we achieve this performance with traditional machine learning techniques, simpler models, and fewer features.</p>
<p>For each of our N  M base learners with num_states states, on num_features features, we learn num_states + (num_states  num_states) + (num_states  num_features) parameters.In our case, with 4 features and 3 states, this results in 6,000 total parameters -versus Reorder's 10,099 parameters.On our hardware, detailed in C, HMM-e trains on 2019-2021 data in 5 minutes versus 18 minutes for Reorder, 3 minutes for Wahle, and 29 seconds for Canzian.</p>
<p>In an unsupervised setting, training HMM-e without class labels and using subsampled data, we perform clustering with K-Means and dimensionality reduction with UMAP; results shown in Fig. 2.</p>
<p>Conclusion</p>
<p>We explore the connection between human behavior modeling and sequence analysis, providing a general framework for extracting coherent sequences from fragmented data.We present HMM-e, an ensemble learning approach that effectively models behavior sequences with minimal feature engineering.Our experiments demonstrate that HMM-e outperforms traditional machine learning baselines and delivers results comparable to complex deep-learning models, despite using fewer features.This highlights the efficiency and potential of our approach for scalable and interpretable sequence modeling in behavior-driven applications.2: Balanced Accuracy and AUC-ROC on GLOBEM data.For each year indicated, we train on the other 3 years, and hold out that year for testing.</p>
<p>In addition to the aggregated results in Table 1, we present results for each of the four years in the GLOBEM dataset in Table 2.Each year indicated is the held-out test year, while the other three are used for training.We compare our approaches against the best-overall-performing approach in the GLOBEM study, Reorder (31).Reorder is a deep-learning based approach tailored for the GLOBEM problem, built on a CNN backbone.</p>
<p>We also present results from ( 4), an SVM-based traditional machine learning approach.We find that we are consistently able to outperform (4), and in terms of balanced accuracy, achieve comparable results to Reorder with far fewer features and much less complex models.</p>
<p>We illustrate the training and inference pipelines of the HMM-e approach detailed in Section 3.2.1, in Figure 3.</p>
<p>B Feature Selection</p>
<p>Figure 4 shows the four features we choose to model, across three anonymized participants from the 2018 study.While many baseline models included in the benchmark choose to model tens or even hundreds of features, we use a small feature set.This allows us to learn meaningful correlations and avoid converging to degenerate models due to insufficient samples.We include two smartphone features (screen time and location-based moving-to-static ratio) and two wearable features (sleep time and steps).All of these features are computed daily.While we adopt this approach using HMMs, the framework itself is model agnostic.</p>
<p>The training data is broken into random subsets, and a diverse ensemble of learners is trained on these subsets.At inference time, pairwise matchups of likelihoods given by the models are compared, giving the composite score s.</p>
<p>Figure 2 :
2
Figure 2: UMAP embeddings of features f i , as discussed in Section 3.2.1, from a 500-model ensemble.Colors correspond to clusters discovered via K-Means.</p>
<p>Figure 3 :
3
Figure3: Flow diagram of our HMM-e ensemble training and inference approach, as detailed in Section 3.2.While we adopt this approach using HMMs, the framework itself is model agnostic.The training data is broken into random subsets, and a diverse ensemble of learners is trained on these subsets.At inference time, pairwise matchups of likelihoods given by the models are compared, giving the composite score s.</p>
<p>Figure 4 :
4
Figure 4: Samples of the daily features we model, across 3 anonymized 2018 participants.For visualization purposes, features are lightly smoothed using an exponential weighted moving average with a half-life of 4 days.For modeling, features are normalized.C Hardware and Software Stack Our experiments are performed on an AWS r5.24xlarge EC2 instance featuring 96 virtual CPUs and 768 GB of memory.Due to the lightweight nature of the models trained, we do not have to leverage GPU acceleration.The environment is configured with Ubuntu 20.04 LTS as the operating system, and we use Python version 3.8.10.Aside from standard machine-learning libraries like Pandas, NumPy, Pytorch, Scikit-Learn, and Tensorflow, we also use HMMLearn to train our HMMs, and Ray to parallelize training and data processing.</p>
<p>Table 1 :
1
Balanced Accuracy and AUC-ROC on GLOBEM.Our approach outperforms baseline machine learning methods and achieves similar results to the best-performing deep learning approach.
) CNN (Reorder)HMMHMM-e</p>
<p>Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. E Altman, J Blanua, L Von Niederhusern, B Egressy, A Anghel, K Atasu, arXiv:2306.164242024</p>
<p>Simulating and classifying behavior in adversarial environments based on action-state traces: An application to money laundering. D Borrajo, M Veloso, S Shah, Proceedings of the First ACM International Conference on AI in Finance. the First ACM International Conference on AI in Finance2020</p>
<p>The Balanced Accuracy and Its Posterior Distribution. K H Brodersen, C S Ong, K E Stephan, J M Buhmann, 20th International Conference on Pattern Recognition. 2010. 2010</p>
<p>Trajectories of depression: unobtrusive monitoring of depressive states by means of smartphone mobility traces analysis. L Canzian, M Musolesi, Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, Ubi-Comp '15. the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, Ubi-Comp '15New York, NY, USAAssociation for Computing Machinery2015ISBN 9781450335744</p>
<p>. K H Choo, J C Tong, L Zhang, Recent Applications of Hidden Markov Models in Computational Biology. 222004Proteomics &amp; Bioinformatics</p>
<p>Feature engineering strategies for credit card fraud detection. A Correa Bahnsen, D Aouada, A Stojanovic, B Ottersten, Expert Systems with Applications. 512016</p>
<p>Ensemble methods in machine learning. T G Dietterich, International workshop on multiple classifier systems. Springer2000</p>
<p>Benchmarking datasets for Anomaly-based Network Intrusion Detection: KDD CUP 99 alternatives. A Divekar, M Parekh, V Savla, R Mishra, M Shirole, 2018 IEEE 3rd International Conference on Computing, Communication and Security (ICCCS). IEEE2018</p>
<p>Classifying Depression in Imbalanced Datasets Using an Autoencoder-Based Anomaly Detection Approach. W Gerych, E Agu, E Rundensteiner, 2019 IEEE 13th International Conference on Semantic Computing (ICSC). 2019</p>
<p>Model-Based Oversampling for Imbalanced Sequence Classification. Z Gong, H Chen, Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM '16. the 25th ACM International on Conference on Information and Knowledge Management, CIKM '16New York, NY, USAAssociation for Computing Machinery2016ISBN 9781450340731</p>
<p>Hidden Markov model finds behavioral patterns of users working with a headmouse driven writing tool. G Hevizi, IEEE CatM Biczo, IEEE CatB Poczos, IEEE CatZ Szabo, IEEE CatB Takics, IEEE CatA Lorincz, IEEE Cat2004 IEEE International Joint Conference on Neural Networks. 20041</p>
<p>A review of anomaly based intrusion detection systems. V Jyothsna, R Prasad, K M Prasad, International Journal of Computer Applications. 2872011</p>
<p>Comparison of Traditional Machine Learning and Deep Learning Approaches for Sentiment Analysis. D Kansara, V Sawant, Advanced Computing Technologies and Applications. H Vasudevan, A Michalas, N Shekokar, M Narvekar, SingaporeSpringer2020</p>
<p>M Kawawa-Beaudan, S Sood, S Palande, G Mani, T Balch, M Veloso, arXiv:2409.07619Ensemble Methods for Sequence Classification with Hidden Markov Models. 2024</p>
<p>Analytical method of web user behavior using Hidden Markov Model. H Kawazu, F Toriumi, M Takano, K Wada, I Fukuda, 2016 IEEE International Conference on Big Data (Big Data). 2016</p>
<p>Deep neural networks for bot detection. S Kudugunta, E Ferrara, Information Sciences. 4672018</p>
<p>Combining pattern classifiers: methods and algorithms. L I Kuncheva, 2014John Wiley &amp; Sons</p>
<p>Wearable-based Human Activity Recognition with Spatio-Temporal Spiking Neural Networks. Y Li, R Yin, H Park, Y Kim, P Panda, NeurIPS 2022 Workshop on Learning from Time Series for Health. 2022</p>
<p>Enhancing Botnet Detection in Network Security Using Profile Hidden Markov Models. R Mannikar, F Di Troia, Applied Sciences. 10142024</p>
<p>M Mazza, S Cresci, M Avvenuti, W Quattrociocchi, M Tesconi, arXiv:1902.04506RTbust: Exploiting Temporal Patterns for Botnet Detection on Twitter. 2019</p>
<p>Behavior Recognition and Prediction With Hidden Markov Models for Surveillance Systems. J Mitterbauer, D Bruckner, R Velik, 8th IFAC Conference on Fieldbuses and Networks in Industrial and Embedded Systems. 200942IFAC Proceedings Volumes</p>
<p>Global Stock Selection with. N Nguyen, D Nguyen, Hidden Markov Model. Risks. 192021</p>
<p>Gaussian mixture based HMM for human daily activity recognition using 3D skeleton features. L Piyathilaka, S Kodagoda, 2013 IEEE 8th Conference on Industrial Electronics and Applications (ICIEA). 2013</p>
<p>An introduction to hidden Markov models. L Rabiner, B Juang, IEEE ASSP Magazine. 311986</p>
<p>Sequential fraud detection for prepaid cards using hidden Markov model divergence. W N Robinson, A Aria, Expert Systems with Applications. 912018</p>
<p>T Su, H Sun, J Zhu, S Wang, Y Li, BAT: Deep Learning Methods on Network Intrusion Detection Using NSL-KDD Dataset. 20208</p>
<p>IDS-INT: Intrusion detection system using transformer-based transfer learning for imbalanced network traffic. F Ullah, S Ullah, G Srivastava, J C Lin, -W , Digital Communications and Networks. 1012024</p>
<p>M Vrigkas, C Nikou, I A Kakadiaris, A Review of Human Activity Recognition Methods. Frontiers in Robotics and AI. 2015</p>
<p>Mobile Sensing and Support for People With Depression: A Pilot Trial in the Wild. F Wahle, T Kowatsch, E Fleisch, M Rufer, S Weidt, JMIR Mhealth Uhealth. 43e1112016</p>
<p>GLOBEM: Cross-Dataset Generalization of Longitudinal Human Behavior Modeling. X Xu, X Liu, H Zhang, W Wang, S Nepal, Y Sefidgar, W Seo, K S Kuehn, J F Huckins, M E Morris, P S Nurius, E A Riskin, S Patel, T Althoff, A Campbell, A K Dey, J Mankoff, Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 642023</p>
<p>X Xu, H Zhang, Y Sefidgar, Y Ren, X Liu, W Seo, J Brown, K Kuehn, M Merrill, P Nurius, S Patel, T Althoff, M E Morris, E Riskin, J Mankoff, A K Dey, arXiv:2211.02733GLOBEM Dataset: Multi-Year Datasets for Longitudinal Human Behavior Modeling Generalization. 2023</p>
<p>Hidden Markov models and their applications in biological sequence analysis. B.-J Yoon, Curr. Genomics. 1062009</p>
<p>C Yu, Y Xu, J Cao, Y Zhang, Y Jin, M Zhu, arXiv:2406.03733Credit Card Fraud Detection Using Advanced Transformer Model. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7791 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7791</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7791</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-273695178</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.22394v2.pdf" target="_blank">AAAR-1.0: Assessing AI’s Potential to Assist Research</a></p>
                <p><strong>Paper Abstract:</strong> Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brain-storming research ideas, designing experiments, and writing or reviewing papers. In this study, we introduce AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) E QUATION I NFERENCE , assessing the correctness of equations based on the contextual information in paper submissions; (ii) E XPERIMENT - D ESIGN , designing experiments to validate research ideas and solutions; (iii) P APER W EAK - NESS , identifying weaknesses in paper submissions; and (iv) R EVIEW C RITIQUE , identifying each segment in human reviews is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis. An evaluation of both open-source and closed-source LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new versions. Project Webpage: https:</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7791.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7791.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AAAR-1.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Assessing AI's Potential to Assist Research (AAAR-1.0)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark dataset and evaluation suite introduced in this paper to measure LLM capability on four expertise-intensive research tasks (EQINFER, EXPDESIGN, PAPERWEAKNESS, REVIEWCRITIQUE) with curated data, task-specific metrics, and human-expert annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs evaluated (GPT-4o, GPT-4, o1-preview, Gemini 1.5 Pro, Claude 3.5/Opus/Sonnet, Llama3, Qwen2.5-72B, Mixtral, Mistral, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (e.g., GPT-4o, GPT-4, Gemini 1.5 Pro, Claude 3.5 sonnet, Llama 3.1-70B, Qwen 2.5-72B, Mixtral-8x22B-MoE, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science / AI research (peer-review, experiment design, algorithm validation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Benchmarking and evaluation of hypotheses/explanations/experiment designs and equation correctness produced or assessed by LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>AAAR-1.0 task-specific evaluation suite</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Four tasks (EQINFER, EXPDESIGN, PAPERWEAKNESS, REVIEWCRITIQUE) each with curated inputs/ground truth; evaluation uses a mix of automatic metrics (F1, En-Precision/Recall, S-Match, S-Precision/Recall, ITF-IDF, ROUGE, BERTScore), LLM-based entailment checks, and multi-round human expert annotation and human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1; En-Precision/En-Recall; S-Match (SentenceBERT similarity); S-Precision/S-Recall; ITF-IDF (novel diversity); ROUGE; BERTScore; human acceptance ratios</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See task-specific entries; overall metrics include classification F1 (percent), semantic similarity averages (cosine similarity, 0–1), S-Precision/Recall aggregated over reviewers, ITF-IDF combining intra-paper occurrence and cross-paper specificity (formula in paper), ROUGE/BERTScore as standard text-overlap/embedding metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AAAR-1.0 (composed of EQINFER, EXPDESIGN, PAPERWEAKNESS, REVIEWCRITIQUE subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Data collected/validated by senior AI researchers: EQINFER expert examination by 5 senior PhD students (pairs checked by ≥2 experts); EXPDESIGN annotation by 10 senior PhD annotators with multi-round peer review; REVIEWCRITIQUE labels from >40 AI research experts (100 papers, 380 reviews, 11,376 segments); additional human evaluations: 5 annotators judged explanation acceptability on subset.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Top closed-source LLM F1 on EQINFER ≈ 46.74% (baseline All-Positive F1=40%); EXPDESIGN shows low En-Precision (models produce many novel but often unnecessary experiments); WEAKNESS: closed-source models better S-Recall but low ITF-IDF compared to humans; REVIEWCRITIQUE: closed-source models outperform open-source but F1 remains low.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM outputs often fall short of human experts: equation inference near chance, many model experiments are optional or unfeasible, weaknesses are vague and lack domain specificity, deficient-review explanations scored low against human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Tasks remain challenging; automatic metrics (esp. LLM-as-judge) have biases; some LLM evaluation relies on other LLMs (e.g., GPT-4o) as evaluators; multi-modal inputs and longer contexts do not always improve performance; human-level expertise still required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAAR-1.0: Assessing AI’s Potential to Assist Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7791.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7791.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EQINFER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EQUATIONINFERENCE (EQINFER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A binary classification task (introduced in this paper) that asks whether a given equation appearing in a paper context is correct (positive) or a synthesized incorrect counterpart (negative).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs evaluated (Mixtral, Llama3, Qwen2.5-72B, Gemini 1.5 Pro, Claude, GPT-4o, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (see AAAR-1.0 for exact model list)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science / AI (mathematical/algorithmic equation validation in papers)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Validation of equations (correctness inference) — a falsifiability-style check of claimed mathematical expressions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Binary classification with F1 score</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given LaTeX source with surrounding context, models predict whether an equation is correct (positive) or incorrect (negative); evaluation uses standard classification metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1, Precision, Recall</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>F1: harmonic mean of precision and recall expressed as percent; Precision and Recall as usual for binary classification.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AAAR-1.0 / EQINFER subset (1,049 positive equations, 3,147 negatives after synthesis/filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Data curated using GPT-4 to synthesize negatives and then filtered by 5 senior PhD students; each pair examined by ≥2 experts; LaTeX compilation (TeXlive) used to weed out false negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>All-Positive baseline F1=40%; many open-source LLMs below baseline; best closed-source model ≈46.74% F1 (Gemini 1.5 Pro); models show high recall but low precision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Humans used to construct and validate data; LLMs perform only slightly above chance on this task, substantially worse than expert-level expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Task depends on precise LaTeX context and notation definitions; synthetic negatives can be context-unaligned and require human filtering; LLMs tend to overpredict positives (high recall, low precision).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAAR-1.0: Assessing AI’s Potential to Assist Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7791.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7791.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>En-Precision / En-Recall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entailment-based Precision and Recall (En-Precision / En-Recall)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>List-level precision and recall metrics for experiment lists in EXPDESIGN using an LLM (GPT-4o) as an entailment judge to decide whether predicted experiment items are entailed by ground-truth and vice versa.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated LLMs (for generation) and GPT-4o (as the entailment evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>generation: various; evaluator: GPT-4o (closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science / experimental design evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation of generated experimental designs (list alignment/entailment)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>En-Precision and En-Recall (LLM-based entailment checking)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For predicted experiment list p and ground-truth list g: En-Precision = average over predicted items of f(pi,g) where f(.) is a binary entailment decision by an LLM; En-Recall = average over ground-truth items of f(gj,p). GPT-4o is prompted to decide entailment (binary).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>En-Precision, En-Recall (both reported as averages over list items)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>En-Precision = (1/m) * sum_{i=1..m} f(pi, g) with f(.) ∈ {0,1}; En-Recall = (1/n) * sum_{j=1..n} f(gj, p). Reported as proportions (0–1 or percentage).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AAAR-1.0 / EXPDESIGN subset (100 instances)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human expert annotation of ground-truth experiments; additional manual checks of model-novel experiments by 3 experts on sampled papers to judge necessity (A/B/C).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLMs show low En-Precision indicating high creativity/novelty; closed-source models vary; specific numeric En-Precision/Recall values reported per model in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLMs generate many novel experiments (low En-Precision), some helpful but many optional or unrelated compared to human experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Using an LLM as entailment judge can introduce evaluator bias; entailment decisions depend on prompt and evaluator LLM; En-Precision favors overlap with annotated ground truth and penalizes useful novel experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAAR-1.0: Assessing AI’s Potential to Assist Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7791.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7791.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S-Match</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Match (S-Match) using SentenceBERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic similarity metric that measures alignment between model-generated explanations and ground-truth explanations using dense sentence embeddings (SentenceBERT) and cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SentenceBERT embedding model (all-mpnet-base-v2) used for similarity; generation by multiple LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>SentenceBERT: all-mpnet-base-v2; generation models: various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science / natural language evaluation of experiment explanations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation metric for explanatory text (semantic similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>S-Match (sentence embedding cosine similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute embedding similarity sim(pi, gi) between each predicted explanation pi and corresponding ground-truth gi using SentenceBERT; S-Match is the average similarity across the aligned pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>S-Match (mean cosine similarity, range -1 to 1 but practically 0–1 for normalized embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>S-Match = (1/m) * sum_{i=1..m} sim(pi, gi), where sim(.) is cosine similarity of SentenceBERT embeddings; reported as decimal or percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AAAR-1.0 / EXPDESIGN subset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluation conducted separately (5 annotators on 20 papers) to validate acceptability; S-Match correlates with human ranking (Spearman's rho reported = 1 for a cited comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>S-Match scores reported per model (e.g., ranges in paper tables); used to avoid surface-overlap bias seen with ROUGE.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>S-Match provides semantic rather than lexical alignment; correlates better with human judgments than ROUGE in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Semantic similarity can conflate plausible-but-incorrect explanations; embedding-based metrics depend on embedding model choice and thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAAR-1.0: Assessing AI’s Potential to Assist Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7791.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7791.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S-Precision / S-Recall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Precision and Recall for WEAKNESS (S-Precision / S-Recall)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metrics that evaluate predicted paper weaknesses against multiple reviewers' weakness lists using semantic similarity (SentenceBERT) and max-over mappings to preserve reviewer structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs under evaluation; SentenceBERT used for similarity computations</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science / peer-review weakness identification</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation of generated critique/weakness lists (semantic matching across multiple reviewers)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>S-Precision and S-Recall (reviewer-aware semantic metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>S-Precision averages, for each predicted weakness pi, the maximum similarity to any ground-truth weakness across all reviewers for that paper (averaged across predicted items). S-Recall averages, for each reviewer k, the average over that reviewer's items of the maximum similarity to any predicted weakness (averaged across reviewers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>S-Precision, S-Recall (semantic matching aggregates)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>S-Precision = (1/m) * sum_{i=1..m} (1/r) * sum_{k=1..r} max_j sim(pi, g_kj); S-Recall = (1/r) * sum_{k=1..r} (1/n_k) * sum_{j=1..n_k} max_i sim(g_kj, pi); sim() is SentenceBERT cosine similarity. Reported as proportion (0–1).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AAAR-1.0 / PAPERWEAKNESS subset (993 instances from ICLR 2023 OpenReview)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Ground truth weakness lists are direct reviewer sentences extracted from OpenReview; average reviewers per paper = 3.8; GPT-4 extraction validated on 200 cases (≤1% misses).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Closed-source LLMs typically have higher S-Recall (generate many weaknesses) but lower diversity (ITF-IDF) than humans; numeric S-Precision/S-Recall per model reported in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLMs generate many broadly-applicable weaknesses reflected in high recall but low specificity/informativeness compared to human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Semantic matching can reward generic weaknesses; max-over-reviewer structure helps preserve reviewer perspectives but still cannot fully capture nuance; similarity thresholds matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAAR-1.0: Assessing AI’s Potential to Assist Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7791.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7791.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ITF-IDF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ITF-IDF (Intra-paper TF-like / Inverse-Reviewer Frequency) novel diversity metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel metric proposed in this paper to capture both informativeness (intra-paper uniqueness) and specificity (how widely a weakness appears across papers) of predicted weaknesses, inspired by TF-IDF.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs under evaluation (predictions scored by ITF-IDF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science / peer-review critique evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Metric for diversity, informativeness, and specificity of generated weaknesses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ITF-IDF (novel review diversity metric)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Combines an intra-paper occurrence measure O_j_i (soft similarity-based intra-paper frequency) and a cross-paper specificity measure R_j_i (soft count of papers containing similar weaknesses) into a score analogous to TF-IDF; averages over dataset to reflect overall weakness diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ITF-IDF (scalar per dataset/model)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>ITF-IDF = (1/w) * sum_{j=1..w} (1/m_j) * sum_{i=1..m_j} log(m_j / O_j_i) * log(w / R_j_i), where O_j_i = sum_{k=1..m_j} sim(p_ji, p_jk) (intra-paper 'occurrence') and R_j_i = sum_{l=1..w} max_s sim(p_ji, p_ls) (cross-paper 'soft document frequency'), sim() is SentenceBERT cosine similarity. Higher score => more informative and specific weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AAAR-1.0 / PAPERWEAKNESS subset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human reviewers' raw weaknesses retained (including repeats) when computing human ITF-IDF baseline; paper notes human ITF-IDF may be slightly underestimated due to repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLMs show substantially lower ITF-IDF than humans, indicating less informative and less specific weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Humans have higher ITF-IDF (more specific/informative weaknesses); many LLM weaknesses are generic and widely applicable, reducing ITF-IDF.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Metric depends on similarity model choice and thresholds; repeats in human reviews affect intra-paper informativeness; soft similarity sums can be sensitive to similarity calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAAR-1.0: Assessing AI’s Potential to Assist Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7791.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7791.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REVIEWCRITIQUE metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REVIEWCRITIQUE evaluation (F1 classification; ROUGE & BERTScore for explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation approach for identifying deficient review segments: classification evaluated with F1 and generated deficiency explanations evaluated with ROUGE and BERTScore against human expert explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (GPT-4, GPT-4o, Claude Opus, Gemini 1.5, Llama3, Qwen2-72B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science / meta-review and review quality assessment</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Classification of review segments (deficient/not) and generation of explanatory text</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>F1 for classification; ROUGE and BERTScore for explanatory alignment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Sentence-level review segments labeled deficient or not by human experts; models either label all segments or select deficient ones; F1 computes detection performance. For segments correctly identified as deficient, generated explanations are compared to human explanations using ROUGE and BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1 (classification); ROUGE-1/2/L; BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>F1: harmonic mean of precision/recall for deficient-label detection; ROUGE: lexical overlap recall-oriented metrics (1/2/L); BERTScore: embedding-based similarity metric (reported as percent).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>REVIEWCRITIQUE subset (reused from Du et al., 2024) — 100 papers, 380 reviews, 11,376 review segments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Labeling performed by >40 AI research experts with detailed explanations; paper reports evaluation of LLM generated explanations against these expert annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Closed-source models outperform open-source on F1 but overall F1 is low; explanation ROUGE/BERTScore values low (examples: GPT-4 ROUGE-1=17.13 ROUGE-2=2.71 ROUGE-L=14.64 BERTScore≈55.63).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLMs can detect some deficient segments (higher recall than precision) but struggle to produce explanations that align well with expert rationales (low ROUGE/BERTScore).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLMs tend to over-predict deficiency (high recall, low precision); explanation metrics reward surface overlap or embedding similarity but might not capture pragmatic correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAAR-1.0: Assessing AI’s Potential to Assist Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7791.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7791.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o as evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o used as an LLM-based evaluator for entailment (in En-Precision/En-Recall)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source LLM (GPT-4o) is employed not only as a generation model but as an entailment judge to decide whether a predicted experiment item is entailed by ground-truth or vice versa.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (used as automatic evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>closed-source GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / automated evaluation of generated experiment lists</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>LLM-as-judge entailment evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based entailment decision (prompted GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt GPT-4o to output a binary decision whether an item pi is entailed by ground-truth list g (and reciprocally). Use outputs to compute En-Precision and En-Recall.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Binary entailment decisions aggregated into En-Precision/En-Recall</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>f(pi,g) ∈ {0,1} decided by GPT-4o; aggregated per formulas for En-Precision/En-Recall.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AAAR-1.0 / EXPDESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human annotations used for ground truth; GPT-4o used to automate evaluation of model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4o used successfully as an evaluator in the study; the authors note potential evaluator bias when using LLMs to judge other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Not directly compared to human entailment judgments in numbers in the main text; authors use human evaluation to validate quality separately.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Using an LLM as both generator and evaluator can introduce circularity and evaluator bias; entailment judgments depend on prompt design and evaluator idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAAR-1.0: Assessing AI’s Potential to Assist Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7791.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7791.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SentenceBERT (all-mpnet-base-v2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence-BERT embeddings (all-mpnet-base-v2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained sentence embedding model used to encode sentences/weaknesses/experiment explanations to compute cosine similarity for multiple semantic metrics (S-Match, S-Precision, S-Recall, ITF-IDF).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SentenceBERT (all-mpnet-base-v2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>all-mpnet-base-v2 (pretrained SentenceTransformers variant)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / semantic similarity for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Embedding-based semantic similarity evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Embedding cosine similarity using SentenceBERT</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Encode text items into dense vectors with SentenceBERT and compute cosine similarity for semantic matching in S-Match, S-Precision/S-Recall and components of ITF-IDF.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Cosine similarity between SentenceBERT embeddings averaged per metric</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>sim(a,b) = cosine(emb(a), emb(b)); used in means and max operations as defined in each metric; similarity range [-1,1] but practically [0,1] for normalized embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AAAR-1.0 (used across EXPDESIGN and PAPERWEAKNESS metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Authors note correlation checks between S-Match and human judgments; computing embeddings requires ~1GB memory on a single A100.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used throughout semantic metrics; reported as intermediate computations (S-Match, S-Precision, S-Recall, ITF-IDF) in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Used as a tool for automatic comparison; human evaluation used to validate results separately.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Embedding-based similarity can miss fine-grained factual errors and is sensitive to embedding model choice and similarity thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAAR-1.0: Assessing AI’s Potential to Assist Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7791.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7791.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Split-Combine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Split-Combine input processing for long documents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An input-processing method that splits long scientific documents into smaller windows, obtains model predictions on each piece, and then merges piece-wise outputs to form the final prediction; proposed/used here for PAPERWEAKNESS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs under evaluation (including long-context LMMs like GPT-4o and GPT-4-Turbo in ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various; ablations include models with up to 128k token context</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / document-level evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Preprocessing framework to enable evaluation over long documents</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Split-Combine</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Split full paper into smaller pieces (window sizes: 2k words for open-source, 3k for closed-source), let LLM predict weaknesses per piece, then merge all predicted weaknesses into final set; compared against 'no-split' full-document input when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Improvement measured via S-Precision/S-Recall and ITF-IDF on PAPERWEAKNESS task</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Window size parameter and ensemble/merge strategy; performance differences reported in standard evaluation metrics (S-Precision/S-Recall/ITF-IDF).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AAAR-1.0 / PAPERWEAKNESS subset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual checking shows split-combine helps ensure coverage of sections; compared with human annotation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Split-combine generally improves WEAKNESS performance versus providing full paper; in ablations split-combine often outperforms 'no-split'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Split-combine yields outputs more aligned with human review coverage; full-document inputs sometimes lead models to miss important sections.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Merging piece-wise predictions can introduce redundancy; method relies on careful window selection and merging heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAAR-1.0: Assessing AI’s Potential to Assist Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7791.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7791.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting strategies (Labeling-All / Select-Deficient / Ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Labeling-All and Select-Deficient prompting strategies with ensemble rules (Both 'No' / Either 'No')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two prompted output formats for REVIEWCRITIQUE and an ensemble rule to combine their outputs to improve robustness of deficient-segment detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs evaluated for REVIEWCRITIQUE (GPT-4, Claude, Gemini, Llama3, Qwen2-72B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / prompt engineering for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Prompting framework and ensemble criteria for classification and selection tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Labeling-All; Select-Deficient; Ensemble (Both 'No' / Either 'No')</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Labeling-All: prompt model to output {id, reliable or not, explanation} for all segments. Select-Deficient: prompt model to output {id, explanation} only for segments it deems deficient. Ensemble rules: 'Both No' marks a segment as deficient only if both prompts label it so; 'Either No' marks it if either prompt labels it deficient.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1 (detection) and explanation alignment metrics (ROUGE/BERTScore) for identified segments</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>As above for classification and explanation metrics; ensemble changes binary labeling prior to metric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AAAR-1.0 / REVIEWCRITIQUE subset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Evaluation uses human-labeled segments as gold; ensemble used to increase robustness; >40 experts created labels.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Different prompting strategies and ensemble choices change precision/recall trade-offs; closed-source models still achieve modest F1s under these strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Prompting and ensemble improve robustness somewhat but do not close gap to human meta-reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Prompt design strongly affects outcomes; ensemble rules trade precision/recall; strategies do not solve low explanatory alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAAR-1.0: Assessing AI’s Potential to Assist Research', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models unlock novel scientific research ideas? <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards fully automated open-ended scientific discovery. <em>(Rating: 2)</em></li>
                <li>NLP researchers: Critique paper (meta-)reviewing. <em>(Rating: 2)</em></li>
                <li>MLAgentBench: Evaluating language agents on machine learning experimentation. <em>(Rating: 1)</em></li>
                <li>Mlagentbench: Evaluating language agents on machine learning experimentation. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7791",
    "paper_id": "paper-273695178",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "AAAR-1.0",
            "name_full": "Assessing AI's Potential to Assist Research (AAAR-1.0)",
            "brief_description": "A benchmark dataset and evaluation suite introduced in this paper to measure LLM capability on four expertise-intensive research tasks (EQINFER, EXPDESIGN, PAPERWEAKNESS, REVIEWCRITIQUE) with curated data, task-specific metrics, and human-expert annotations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs evaluated (GPT-4o, GPT-4, o1-preview, Gemini 1.5 Pro, Claude 3.5/Opus/Sonnet, Llama3, Qwen2.5-72B, Mixtral, Mistral, etc.)",
            "model_size": "various (e.g., GPT-4o, GPT-4, Gemini 1.5 Pro, Claude 3.5 sonnet, Llama 3.1-70B, Qwen 2.5-72B, Mixtral-8x22B-MoE, etc.)",
            "scientific_domain": "Computer science / AI research (peer-review, experiment design, algorithm validation)",
            "theory_type": "Benchmarking and evaluation of hypotheses/explanations/experiment designs and equation correctness produced or assessed by LLMs",
            "evaluation_method_name": "AAAR-1.0 task-specific evaluation suite",
            "evaluation_method_description": "Four tasks (EQINFER, EXPDESIGN, PAPERWEAKNESS, REVIEWCRITIQUE) each with curated inputs/ground truth; evaluation uses a mix of automatic metrics (F1, En-Precision/Recall, S-Match, S-Precision/Recall, ITF-IDF, ROUGE, BERTScore), LLM-based entailment checks, and multi-round human expert annotation and human evaluation.",
            "evaluation_metric": "F1; En-Precision/En-Recall; S-Match (SentenceBERT similarity); S-Precision/S-Recall; ITF-IDF (novel diversity); ROUGE; BERTScore; human acceptance ratios",
            "metric_definition": "See task-specific entries; overall metrics include classification F1 (percent), semantic similarity averages (cosine similarity, 0–1), S-Precision/Recall aggregated over reviewers, ITF-IDF combining intra-paper occurrence and cross-paper specificity (formula in paper), ROUGE/BERTScore as standard text-overlap/embedding metrics.",
            "dataset_or_benchmark": "AAAR-1.0 (composed of EQINFER, EXPDESIGN, PAPERWEAKNESS, REVIEWCRITIQUE subsets)",
            "human_evaluation_details": "Data collected/validated by senior AI researchers: EQINFER expert examination by 5 senior PhD students (pairs checked by ≥2 experts); EXPDESIGN annotation by 10 senior PhD annotators with multi-round peer review; REVIEWCRITIQUE labels from &gt;40 AI research experts (100 papers, 380 reviews, 11,376 segments); additional human evaluations: 5 annotators judged explanation acceptability on subset.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Top closed-source LLM F1 on EQINFER ≈ 46.74% (baseline All-Positive F1=40%); EXPDESIGN shows low En-Precision (models produce many novel but often unnecessary experiments); WEAKNESS: closed-source models better S-Recall but low ITF-IDF compared to humans; REVIEWCRITIQUE: closed-source models outperform open-source but F1 remains low.",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM outputs often fall short of human experts: equation inference near chance, many model experiments are optional or unfeasible, weaknesses are vague and lack domain specificity, deficient-review explanations scored low against human annotations.",
            "limitations_noted": "Tasks remain challenging; automatic metrics (esp. LLM-as-judge) have biases; some LLM evaluation relies on other LLMs (e.g., GPT-4o) as evaluators; multi-modal inputs and longer contexts do not always improve performance; human-level expertise still required.",
            "uuid": "e7791.0",
            "source_info": {
                "paper_title": "AAAR-1.0: Assessing AI’s Potential to Assist Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "EQINFER",
            "name_full": "EQUATIONINFERENCE (EQINFER)",
            "brief_description": "A binary classification task (introduced in this paper) that asks whether a given equation appearing in a paper context is correct (positive) or a synthesized incorrect counterpart (negative).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs evaluated (Mixtral, Llama3, Qwen2.5-72B, Gemini 1.5 Pro, Claude, GPT-4o, etc.)",
            "model_size": "various (see AAAR-1.0 for exact model list)",
            "scientific_domain": "Computer science / AI (mathematical/algorithmic equation validation in papers)",
            "theory_type": "Validation of equations (correctness inference) — a falsifiability-style check of claimed mathematical expressions",
            "evaluation_method_name": "Binary classification with F1 score",
            "evaluation_method_description": "Given LaTeX source with surrounding context, models predict whether an equation is correct (positive) or incorrect (negative); evaluation uses standard classification metrics.",
            "evaluation_metric": "F1, Precision, Recall",
            "metric_definition": "F1: harmonic mean of precision and recall expressed as percent; Precision and Recall as usual for binary classification.",
            "dataset_or_benchmark": "AAAR-1.0 / EQINFER subset (1,049 positive equations, 3,147 negatives after synthesis/filtering)",
            "human_evaluation_details": "Data curated using GPT-4 to synthesize negatives and then filtered by 5 senior PhD students; each pair examined by ≥2 experts; LaTeX compilation (TeXlive) used to weed out false negatives.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "All-Positive baseline F1=40%; many open-source LLMs below baseline; best closed-source model ≈46.74% F1 (Gemini 1.5 Pro); models show high recall but low precision.",
            "comparison_to_human_generated": true,
            "comparison_results": "Humans used to construct and validate data; LLMs perform only slightly above chance on this task, substantially worse than expert-level expectations.",
            "limitations_noted": "Task depends on precise LaTeX context and notation definitions; synthetic negatives can be context-unaligned and require human filtering; LLMs tend to overpredict positives (high recall, low precision).",
            "uuid": "e7791.1",
            "source_info": {
                "paper_title": "AAAR-1.0: Assessing AI’s Potential to Assist Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "En-Precision / En-Recall",
            "name_full": "Entailment-based Precision and Recall (En-Precision / En-Recall)",
            "brief_description": "List-level precision and recall metrics for experiment lists in EXPDESIGN using an LLM (GPT-4o) as an entailment judge to decide whether predicted experiment items are entailed by ground-truth and vice versa.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluated LLMs (for generation) and GPT-4o (as the entailment evaluator)",
            "model_size": "generation: various; evaluator: GPT-4o (closed-source)",
            "scientific_domain": "Computer science / experimental design evaluation",
            "theory_type": "Evaluation of generated experimental designs (list alignment/entailment)",
            "evaluation_method_name": "En-Precision and En-Recall (LLM-based entailment checking)",
            "evaluation_method_description": "For predicted experiment list p and ground-truth list g: En-Precision = average over predicted items of f(pi,g) where f(.) is a binary entailment decision by an LLM; En-Recall = average over ground-truth items of f(gj,p). GPT-4o is prompted to decide entailment (binary).",
            "evaluation_metric": "En-Precision, En-Recall (both reported as averages over list items)",
            "metric_definition": "En-Precision = (1/m) * sum_{i=1..m} f(pi, g) with f(.) ∈ {0,1}; En-Recall = (1/n) * sum_{j=1..n} f(gj, p). Reported as proportions (0–1 or percentage).",
            "dataset_or_benchmark": "AAAR-1.0 / EXPDESIGN subset (100 instances)",
            "human_evaluation_details": "Human expert annotation of ground-truth experiments; additional manual checks of model-novel experiments by 3 experts on sampled papers to judge necessity (A/B/C).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "LLMs show low En-Precision indicating high creativity/novelty; closed-source models vary; specific numeric En-Precision/Recall values reported per model in paper tables.",
            "comparison_to_human_generated": true,
            "comparison_results": "LLMs generate many novel experiments (low En-Precision), some helpful but many optional or unrelated compared to human experiments.",
            "limitations_noted": "Using an LLM as entailment judge can introduce evaluator bias; entailment decisions depend on prompt and evaluator LLM; En-Precision favors overlap with annotated ground truth and penalizes useful novel experiments.",
            "uuid": "e7791.2",
            "source_info": {
                "paper_title": "AAAR-1.0: Assessing AI’s Potential to Assist Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "S-Match",
            "name_full": "Semantic Match (S-Match) using SentenceBERT",
            "brief_description": "A semantic similarity metric that measures alignment between model-generated explanations and ground-truth explanations using dense sentence embeddings (SentenceBERT) and cosine similarity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SentenceBERT embedding model (all-mpnet-base-v2) used for similarity; generation by multiple LLMs",
            "model_size": "SentenceBERT: all-mpnet-base-v2; generation models: various",
            "scientific_domain": "Computer science / natural language evaluation of experiment explanations",
            "theory_type": "Evaluation metric for explanatory text (semantic similarity)",
            "evaluation_method_name": "S-Match (sentence embedding cosine similarity)",
            "evaluation_method_description": "Compute embedding similarity sim(pi, gi) between each predicted explanation pi and corresponding ground-truth gi using SentenceBERT; S-Match is the average similarity across the aligned pairs.",
            "evaluation_metric": "S-Match (mean cosine similarity, range -1 to 1 but practically 0–1 for normalized embeddings)",
            "metric_definition": "S-Match = (1/m) * sum_{i=1..m} sim(pi, gi), where sim(.) is cosine similarity of SentenceBERT embeddings; reported as decimal or percentage.",
            "dataset_or_benchmark": "AAAR-1.0 / EXPDESIGN subset",
            "human_evaluation_details": "Human evaluation conducted separately (5 annotators on 20 papers) to validate acceptability; S-Match correlates with human ranking (Spearman's rho reported = 1 for a cited comparison).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "S-Match scores reported per model (e.g., ranges in paper tables); used to avoid surface-overlap bias seen with ROUGE.",
            "comparison_to_human_generated": true,
            "comparison_results": "S-Match provides semantic rather than lexical alignment; correlates better with human judgments than ROUGE in this task.",
            "limitations_noted": "Semantic similarity can conflate plausible-but-incorrect explanations; embedding-based metrics depend on embedding model choice and thresholds.",
            "uuid": "e7791.3",
            "source_info": {
                "paper_title": "AAAR-1.0: Assessing AI’s Potential to Assist Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "S-Precision / S-Recall",
            "name_full": "Semantic Precision and Recall for WEAKNESS (S-Precision / S-Recall)",
            "brief_description": "Metrics that evaluate predicted paper weaknesses against multiple reviewers' weakness lists using semantic similarity (SentenceBERT) and max-over mappings to preserve reviewer structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs under evaluation; SentenceBERT used for similarity computations",
            "model_size": "various",
            "scientific_domain": "Computer science / peer-review weakness identification",
            "theory_type": "Evaluation of generated critique/weakness lists (semantic matching across multiple reviewers)",
            "evaluation_method_name": "S-Precision and S-Recall (reviewer-aware semantic metrics)",
            "evaluation_method_description": "S-Precision averages, for each predicted weakness pi, the maximum similarity to any ground-truth weakness across all reviewers for that paper (averaged across predicted items). S-Recall averages, for each reviewer k, the average over that reviewer's items of the maximum similarity to any predicted weakness (averaged across reviewers).",
            "evaluation_metric": "S-Precision, S-Recall (semantic matching aggregates)",
            "metric_definition": "S-Precision = (1/m) * sum_{i=1..m} (1/r) * sum_{k=1..r} max_j sim(pi, g_kj); S-Recall = (1/r) * sum_{k=1..r} (1/n_k) * sum_{j=1..n_k} max_i sim(g_kj, pi); sim() is SentenceBERT cosine similarity. Reported as proportion (0–1).",
            "dataset_or_benchmark": "AAAR-1.0 / PAPERWEAKNESS subset (993 instances from ICLR 2023 OpenReview)",
            "human_evaluation_details": "Ground truth weakness lists are direct reviewer sentences extracted from OpenReview; average reviewers per paper = 3.8; GPT-4 extraction validated on 200 cases (≤1% misses).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Closed-source LLMs typically have higher S-Recall (generate many weaknesses) but lower diversity (ITF-IDF) than humans; numeric S-Precision/S-Recall per model reported in paper tables.",
            "comparison_to_human_generated": true,
            "comparison_results": "LLMs generate many broadly-applicable weaknesses reflected in high recall but low specificity/informativeness compared to human reviewers.",
            "limitations_noted": "Semantic matching can reward generic weaknesses; max-over-reviewer structure helps preserve reviewer perspectives but still cannot fully capture nuance; similarity thresholds matter.",
            "uuid": "e7791.4",
            "source_info": {
                "paper_title": "AAAR-1.0: Assessing AI’s Potential to Assist Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ITF-IDF",
            "name_full": "ITF-IDF (Intra-paper TF-like / Inverse-Reviewer Frequency) novel diversity metric",
            "brief_description": "A novel metric proposed in this paper to capture both informativeness (intra-paper uniqueness) and specificity (how widely a weakness appears across papers) of predicted weaknesses, inspired by TF-IDF.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs under evaluation (predictions scored by ITF-IDF)",
            "model_size": "various",
            "scientific_domain": "Computer science / peer-review critique evaluation",
            "theory_type": "Metric for diversity, informativeness, and specificity of generated weaknesses",
            "evaluation_method_name": "ITF-IDF (novel review diversity metric)",
            "evaluation_method_description": "Combines an intra-paper occurrence measure O_j_i (soft similarity-based intra-paper frequency) and a cross-paper specificity measure R_j_i (soft count of papers containing similar weaknesses) into a score analogous to TF-IDF; averages over dataset to reflect overall weakness diversity.",
            "evaluation_metric": "ITF-IDF (scalar per dataset/model)",
            "metric_definition": "ITF-IDF = (1/w) * sum_{j=1..w} (1/m_j) * sum_{i=1..m_j} log(m_j / O_j_i) * log(w / R_j_i), where O_j_i = sum_{k=1..m_j} sim(p_ji, p_jk) (intra-paper 'occurrence') and R_j_i = sum_{l=1..w} max_s sim(p_ji, p_ls) (cross-paper 'soft document frequency'), sim() is SentenceBERT cosine similarity. Higher score =&gt; more informative and specific weaknesses.",
            "dataset_or_benchmark": "AAAR-1.0 / PAPERWEAKNESS subset",
            "human_evaluation_details": "Human reviewers' raw weaknesses retained (including repeats) when computing human ITF-IDF baseline; paper notes human ITF-IDF may be slightly underestimated due to repeats.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "LLMs show substantially lower ITF-IDF than humans, indicating less informative and less specific weaknesses.",
            "comparison_to_human_generated": true,
            "comparison_results": "Humans have higher ITF-IDF (more specific/informative weaknesses); many LLM weaknesses are generic and widely applicable, reducing ITF-IDF.",
            "limitations_noted": "Metric depends on similarity model choice and thresholds; repeats in human reviews affect intra-paper informativeness; soft similarity sums can be sensitive to similarity calibration.",
            "uuid": "e7791.5",
            "source_info": {
                "paper_title": "AAAR-1.0: Assessing AI’s Potential to Assist Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "REVIEWCRITIQUE metrics",
            "name_full": "REVIEWCRITIQUE evaluation (F1 classification; ROUGE & BERTScore for explanations)",
            "brief_description": "Evaluation approach for identifying deficient review segments: classification evaluated with F1 and generated deficiency explanations evaluated with ROUGE and BERTScore against human expert explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various LLMs (GPT-4, GPT-4o, Claude Opus, Gemini 1.5, Llama3, Qwen2-72B, etc.)",
            "model_size": "various",
            "scientific_domain": "Computer science / meta-review and review quality assessment",
            "theory_type": "Classification of review segments (deficient/not) and generation of explanatory text",
            "evaluation_method_name": "F1 for classification; ROUGE and BERTScore for explanatory alignment",
            "evaluation_method_description": "Sentence-level review segments labeled deficient or not by human experts; models either label all segments or select deficient ones; F1 computes detection performance. For segments correctly identified as deficient, generated explanations are compared to human explanations using ROUGE and BERTScore.",
            "evaluation_metric": "F1 (classification); ROUGE-1/2/L; BERTScore",
            "metric_definition": "F1: harmonic mean of precision/recall for deficient-label detection; ROUGE: lexical overlap recall-oriented metrics (1/2/L); BERTScore: embedding-based similarity metric (reported as percent).",
            "dataset_or_benchmark": "REVIEWCRITIQUE subset (reused from Du et al., 2024) — 100 papers, 380 reviews, 11,376 review segments",
            "human_evaluation_details": "Labeling performed by &gt;40 AI research experts with detailed explanations; paper reports evaluation of LLM generated explanations against these expert annotations.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Closed-source models outperform open-source on F1 but overall F1 is low; explanation ROUGE/BERTScore values low (examples: GPT-4 ROUGE-1=17.13 ROUGE-2=2.71 ROUGE-L=14.64 BERTScore≈55.63).",
            "comparison_to_human_generated": true,
            "comparison_results": "LLMs can detect some deficient segments (higher recall than precision) but struggle to produce explanations that align well with expert rationales (low ROUGE/BERTScore).",
            "limitations_noted": "LLMs tend to over-predict deficiency (high recall, low precision); explanation metrics reward surface overlap or embedding similarity but might not capture pragmatic correctness.",
            "uuid": "e7791.6",
            "source_info": {
                "paper_title": "AAAR-1.0: Assessing AI’s Potential to Assist Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o as evaluator",
            "name_full": "GPT-4o used as an LLM-based evaluator for entailment (in En-Precision/En-Recall)",
            "brief_description": "A closed-source LLM (GPT-4o) is employed not only as a generation model but as an entailment judge to decide whether a predicted experiment item is entailed by ground-truth or vice versa.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (used as automatic evaluator)",
            "model_size": "closed-source GPT-4o",
            "scientific_domain": "NLP / automated evaluation of generated experiment lists",
            "theory_type": "LLM-as-judge entailment evaluation",
            "evaluation_method_name": "LLM-based entailment decision (prompted GPT-4o)",
            "evaluation_method_description": "Prompt GPT-4o to output a binary decision whether an item pi is entailed by ground-truth list g (and reciprocally). Use outputs to compute En-Precision and En-Recall.",
            "evaluation_metric": "Binary entailment decisions aggregated into En-Precision/En-Recall",
            "metric_definition": "f(pi,g) ∈ {0,1} decided by GPT-4o; aggregated per formulas for En-Precision/En-Recall.",
            "dataset_or_benchmark": "AAAR-1.0 / EXPDESIGN",
            "human_evaluation_details": "Human annotations used for ground truth; GPT-4o used to automate evaluation of model outputs.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "GPT-4o used successfully as an evaluator in the study; the authors note potential evaluator bias when using LLMs to judge other LLMs.",
            "comparison_to_human_generated": false,
            "comparison_results": "Not directly compared to human entailment judgments in numbers in the main text; authors use human evaluation to validate quality separately.",
            "limitations_noted": "Using an LLM as both generator and evaluator can introduce circularity and evaluator bias; entailment judgments depend on prompt design and evaluator idiosyncrasies.",
            "uuid": "e7791.7",
            "source_info": {
                "paper_title": "AAAR-1.0: Assessing AI’s Potential to Assist Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SentenceBERT (all-mpnet-base-v2)",
            "name_full": "Sentence-BERT embeddings (all-mpnet-base-v2)",
            "brief_description": "A pre-trained sentence embedding model used to encode sentences/weaknesses/experiment explanations to compute cosine similarity for multiple semantic metrics (S-Match, S-Precision, S-Recall, ITF-IDF).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SentenceBERT (all-mpnet-base-v2)",
            "model_size": "all-mpnet-base-v2 (pretrained SentenceTransformers variant)",
            "scientific_domain": "NLP / semantic similarity for evaluation",
            "theory_type": "Embedding-based semantic similarity evaluator",
            "evaluation_method_name": "Embedding cosine similarity using SentenceBERT",
            "evaluation_method_description": "Encode text items into dense vectors with SentenceBERT and compute cosine similarity for semantic matching in S-Match, S-Precision/S-Recall and components of ITF-IDF.",
            "evaluation_metric": "Cosine similarity between SentenceBERT embeddings averaged per metric",
            "metric_definition": "sim(a,b) = cosine(emb(a), emb(b)); used in means and max operations as defined in each metric; similarity range [-1,1] but practically [0,1] for normalized embeddings.",
            "dataset_or_benchmark": "AAAR-1.0 (used across EXPDESIGN and PAPERWEAKNESS metrics)",
            "human_evaluation_details": "Authors note correlation checks between S-Match and human judgments; computing embeddings requires ~1GB memory on a single A100.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Used throughout semantic metrics; reported as intermediate computations (S-Match, S-Precision, S-Recall, ITF-IDF) in tables.",
            "comparison_to_human_generated": false,
            "comparison_results": "Used as a tool for automatic comparison; human evaluation used to validate results separately.",
            "limitations_noted": "Embedding-based similarity can miss fine-grained factual errors and is sensitive to embedding model choice and similarity thresholds.",
            "uuid": "e7791.8",
            "source_info": {
                "paper_title": "AAAR-1.0: Assessing AI’s Potential to Assist Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Split-Combine",
            "name_full": "Split-Combine input processing for long documents",
            "brief_description": "An input-processing method that splits long scientific documents into smaller windows, obtains model predictions on each piece, and then merges piece-wise outputs to form the final prediction; proposed/used here for PAPERWEAKNESS.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs under evaluation (including long-context LMMs like GPT-4o and GPT-4-Turbo in ablations)",
            "model_size": "various; ablations include models with up to 128k token context",
            "scientific_domain": "NLP / document-level evaluation",
            "theory_type": "Preprocessing framework to enable evaluation over long documents",
            "evaluation_method_name": "Split-Combine",
            "evaluation_method_description": "Split full paper into smaller pieces (window sizes: 2k words for open-source, 3k for closed-source), let LLM predict weaknesses per piece, then merge all predicted weaknesses into final set; compared against 'no-split' full-document input when possible.",
            "evaluation_metric": "Improvement measured via S-Precision/S-Recall and ITF-IDF on PAPERWEAKNESS task",
            "metric_definition": "Window size parameter and ensemble/merge strategy; performance differences reported in standard evaluation metrics (S-Precision/S-Recall/ITF-IDF).",
            "dataset_or_benchmark": "AAAR-1.0 / PAPERWEAKNESS subset",
            "human_evaluation_details": "Manual checking shows split-combine helps ensure coverage of sections; compared with human annotation behavior.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Split-combine generally improves WEAKNESS performance versus providing full paper; in ablations split-combine often outperforms 'no-split'.",
            "comparison_to_human_generated": false,
            "comparison_results": "Split-combine yields outputs more aligned with human review coverage; full-document inputs sometimes lead models to miss important sections.",
            "limitations_noted": "Merging piece-wise predictions can introduce redundancy; method relies on careful window selection and merging heuristics.",
            "uuid": "e7791.9",
            "source_info": {
                "paper_title": "AAAR-1.0: Assessing AI’s Potential to Assist Research",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Prompting strategies (Labeling-All / Select-Deficient / Ensemble)",
            "name_full": "Labeling-All and Select-Deficient prompting strategies with ensemble rules (Both 'No' / Either 'No')",
            "brief_description": "Two prompted output formats for REVIEWCRITIQUE and an ensemble rule to combine their outputs to improve robustness of deficient-segment detection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs evaluated for REVIEWCRITIQUE (GPT-4, Claude, Gemini, Llama3, Qwen2-72B, etc.)",
            "model_size": "various",
            "scientific_domain": "NLP / prompt engineering for evaluation",
            "theory_type": "Prompting framework and ensemble criteria for classification and selection tasks",
            "evaluation_method_name": "Labeling-All; Select-Deficient; Ensemble (Both 'No' / Either 'No')",
            "evaluation_method_description": "Labeling-All: prompt model to output {id, reliable or not, explanation} for all segments. Select-Deficient: prompt model to output {id, explanation} only for segments it deems deficient. Ensemble rules: 'Both No' marks a segment as deficient only if both prompts label it so; 'Either No' marks it if either prompt labels it deficient.",
            "evaluation_metric": "F1 (detection) and explanation alignment metrics (ROUGE/BERTScore) for identified segments",
            "metric_definition": "As above for classification and explanation metrics; ensemble changes binary labeling prior to metric computation.",
            "dataset_or_benchmark": "AAAR-1.0 / REVIEWCRITIQUE subset",
            "human_evaluation_details": "Evaluation uses human-labeled segments as gold; ensemble used to increase robustness; &gt;40 experts created labels.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Different prompting strategies and ensemble choices change precision/recall trade-offs; closed-source models still achieve modest F1s under these strategies.",
            "comparison_to_human_generated": false,
            "comparison_results": "Prompting and ensemble improve robustness somewhat but do not close gap to human meta-reviewers.",
            "limitations_noted": "Prompt design strongly affects outcomes; ensemble rules trade precision/recall; strategies do not solve low explanatory alignment.",
            "uuid": "e7791.10",
            "source_info": {
                "paper_title": "AAAR-1.0: Assessing AI’s Potential to Assist Research",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models unlock novel scientific research ideas?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_unlock_novel_scientific_research_ideas"
        },
        {
            "paper_title": "The AI Scientist: Towards fully automated open-ended scientific discovery.",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "NLP researchers: Critique paper (meta-)reviewing.",
            "rating": 2,
            "sanitized_title": "nlp_researchers_critique_paper_metareviewing"
        },
        {
            "paper_title": "MLAgentBench: Evaluating language agents on machine learning experimentation.",
            "rating": 1,
            "sanitized_title": "mlagentbench_evaluating_language_agents_on_machine_learning_experimentation"
        },
        {
            "paper_title": "Mlagentbench: Evaluating language agents on machine learning experimentation.",
            "rating": 1,
            "sanitized_title": "mlagentbench_evaluating_language_agents_on_machine_learning_experimentation"
        }
    ],
    "cost": 0.022241499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AAAR-1.0: Assessing AI's Potential to Assist Research
24 Feb 2025</p>
<p>Renze Lou 
Hanzi Xu 
Sijia Wang 
Jiangshu Du 
Ryo Kamoi 
Xiaoxin Lu 
Jian Xie 
Yuxuan Sun 
Yusen Zhang 
Jihyun Janice Ahn 
Hongchao Fang 
Zhuoyang Zou 
Wenchao Ma 
Xi Li 
Kai Zhang 
Congying Xia 
Lifu Huang 
Wenpeng Yin 
AAAR-1.0: Assessing AI's Potential to Assist Research
24 Feb 2025C25B2B3BAF44DCB9C7135879553C192CarXiv:2410.22394v2[cs.CL]
Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation.However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers.In this study, we introduce AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EQUATIONINFERENCE, assessing the correctness of equations based on the contextual information in paper submissions; (ii) EXPERIMENT-DESIGN, designing experiments to validate research ideas and solutions; (iii) PAPERWEAK-NESS, identifying weaknesses in paper submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews is deficient or not.AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly researchoriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis.An evaluation of both opensource and closed-source LLMs reveals their potential as well as limitations in conducting sophisticated research tasks.We will keep iterating AAAR-1.0 to new versions.</p>
<p>Introduction</p>
<p>Although AI has brought transformative changes to various aspects of life, its impact on researchers unfolds in a nuanced manner.On the one hand, AI assists in various research disciplines, such as Social Science (Neuman et al., 2023), Finance (Gu et al., 2024), Medicine (Rakhimov et al., 2022), GeoScience (Praskievicz, 2018), etc., significantly expediting academic processes.However, many of these applications are superficial, often limited to data-driven clustering or classification.On the flip side, the AI era poses challenges for researchers.Despite its ability to streamline some activities, researchers still face demanding, cognitively intensive tasks such as staying current through extensive paper reading, rapidly generating ideas in response to fastpaced advancements, conducting rigorous experiments to substantiate claims, and managing an increasing volume of peer reviews.Then a question looms: How effectively can AI assist researchers in tasks that are domain-specific, expertise-demanding, and reasoning-intensive?</p>
<p>Existing works proved the promising potential for using LLMs in assisting AI research.Si et al. (2024) conducted a large-scale human study and found that LLMs can generate creative research ideas.Lu et al. (2024) proposed an autonomous agent to handle complicated research workflow and write a whole research paper.However, most of these works focus on addressing highly subjective problems that require a high degree of expertise, making evaluation laborious and hard to reproduce.This underscores the need for a comprehensive benchmark that rigorously assesses LLMs' capabilities in expertise-intensive research activities.</p>
<p>To this end, in this work, we introduce AAAR-1.0, a novel benchmark that aims to comprehensively assess the LLMs' capacity on expert-level research tasks.As illustrated in Figure 1, AAAR-1.0 decomposes three distinct expert-level AI research tasks from the researcher's daily activities, including i) EQUATIONINFERENCE, investigating whether the LLMs can infer the equation correctness based on the paper context; ii) EXPERIMENTDESIGN, validating LLMs' ability on designing reliable experiments for a research idea; iii) PAPERWEAKNESS, testing the quality of weaknesses discovered by LLMs from paper drafts; and iv) REVIEW- This paper proposes an algorithm for the robustness of […] In the below sections, we conduct the experiments 1. Missed references […] 2. Insufficient experiments […] 3. Missed running details […] Figure 1: The input-output illustration of four tasks in the proposed AAAR-1.0benchmark.CRITIQUE, investigating whether LLMs can identify and explain the deficient/unreliable human-written paper reviews.To ensure data quality, senior AI researchers with extensive domain expertise perform data annotation for AAAR-1.0,followed by rigorous multi-round data examination and filtering.All three tasks require models to possess strong domain knowledge covering various cutting-edge research findings, as well as expert-level research experience, to the extent that even humans need substantial research accumulation to tackle the tasks we designed.Crucially, tasks here are singular, stand-alone challenges (with clear input and output expectations) rather than a complicated task chain (Li et al., 2024;Lu et al., 2024), providing a more transparent assessment of the model's intermediate output.</p>
<p>Benefiting from the proposed automatic metrics, we conduct extensive experiments across numerous mainstream LLMs, where we find that:</p>
<p>• With a random guess baseline of 40% F 1 , the performance of most LLMs on EQINFER hovers just slightly above chance, with the top models reaching around 46%.This highlights the difficulty of the task, despite its reliance primarily on local context reasoning.</p>
<p>• In EXPDESIGN, LLM-designed experiments are innovative and more diverse than those by humans; however, many are trivial, lack feasibility, and stray from the original research objectives.</p>
<p>• In PAPERWEAKNESS, LLM-identified weaknesses often lack depth and specificity, making them broadly applicable and less useful for providing feedback on paper drafts.</p>
<p>• In REVIEWCRITIQUE, LLMs struggle to effectively identify deficient human reviews, indicating limited usefulness in assisting meta-reviewers in evaluating the quality of individual paper reviews.LLMs generated weakness lacks sufficient domain-specific knowledge, model always tends to generate some vague weaknesses that can be applied to any other papers.</p>
<p>Related Work</p>
<p>LLMs for AI Research.With the rapid evolution of pertaining techniques, LLMs are found to be useful in assisting various research disciplines (Yu et al., 2024a;Labrak et al., 2024), particularly in AI research, such as generating novel research ideas (Kumar et al., 2024;Yu et al., 2024b), reviewing research draft (Gao et al., 2024;Du et al., 2024;Liang et al., 2024), and writing scientific papers (Chamoun et al., 2024;Lu et al., 2024).For example, Si et al. (2024) conducted a large-scale human investigation on LLM-generated research ideas and found that LLMs can generate novel ideas compared with humans while lacking feasibility.Du et al. (2024) found that while LLMs are effective at summarizing papers, they tend to overly trust the authors' claimed strengths and struggle to identify weaknesses specific to the paper.Furthermore, some works try to employ LLMs to solve more complicated research tasks that are composed of multiple steps (Li et al., 2024;2023;Tang et al., 2023).Notably, Lu et al. (2024) proposed AI-SCIENTIST, an autonomous agent framework that can handle a series of challenging research tasks consecutively, including generating research ideas, coming up with the corresponding experiments along with the implementations, and then writing the final research paper -exactly how human conduct a whole research pipeline.However, there is still a lack of systematic evaluations and quantitative analyses on the LLMs' (intermediate) output of each single-step research task.Our work focuses on building a benchmark that has individual research steps with clear input-output expectations, thus making it suitable for comprehensive LLMs evaluation.</p>
<p>Benchmarks for AI Research Tasks.Existing "LLM assists research" benchmarks mainly focus on the implementation and execution part of the research pipeline (Lu et al., 2024;Chen et al., 2024a;Li et al., 2024;Chan et al., 2024).</p>
<p>For instance, Huang et al. (2024) proposed MLAgentBench to test the LLMs' capacity for writing project code and training the ML models, where the evaluation metric is the test performance of the models trained by LLMs.However, realworld AI research activities are diverse and some of them are hard to assess for quality, such as generating research ideas, which requires intensive manual assessment (Si et al., 2024;Liang et al., 2024).Our work centers on tasks that emphasize a comprehensive mastery of the scientific research field and core elements of a researcher's daily workload, and we try to build curated task-specific metrics for every single task for a more efficient and accurate LLMs appraisal.</p>
<p>AAAR-1.0</p>
<p>Figure 2 provides a data construction overview.In the following sections, we elaborate on the data collection details, including § 3.1 EQUATIONINFERENCE ( EQINFER ), § 3.2 EXPERIMENTDESIGN ( EXPDESIGN ), and § 3.3 PAPERWEAKNESS ( WEAKNESS ), and § 3.4 REVIEWCRITIQUE .</p>
<p>EQUATIONINFERENCE</p>
<p>Crafting a correct scientific equation in paper writing or validating an equation in paper reviewing is challenging, as it requires a thorough understanding of an algorithm or the intricate relationships among numerous variables.Directly prompting LLMs to generate equations proves overly demanding.Therefore, this work formulates EQINFER (Figure 1) as a binary inference task.1 ① Data crawling and cleaning.For the data source, we adopt the pre-compilation LaTeX code for two reasons: i) existing PDF parsing tools, such as PyMuPDF and Paper-Mage (Lo et al., 2023), can introduce considerable noise to the parsed equation text; ii) considering most of exiting LLMs are capable with processing LaTeX code, using LaTeX source instead of parsed text can be more accurate and provide LLMs with richer information.Meanwhile, we only crawl those peer-reviewed papers accepted by top-tier conferences to avoid using low-quality human-written equations.Accordingly, we first obtain the accepted paper list from ACL Anthology, from year 2019 to 2023.Next, we search each paper on arXiv to crawl its LaTeX source (if it exists).Finally, we get a total of 1,762 papers' source LaTeX packages.We then clean the LaTeX sources by deleting all the comments and combining multiple cross-referred .texfiles into a main file.Afterward, we use regex to randomly extract (at most) 3 equations' code snippets per paper, resulting in 3,877 human-written equations.</p>
<p>② LLM-based equation synthesis.As EQINFER assessing whether the LLMs can infer the correctness of equation (i.e., binary classification), for each human-written positive equation, we have to craft counterpart negative equations.To this end, for each positive equation, we prompt GPT-4 to synthesize a negative equation based on the paper context.We repeat this prompt (with a high decoding temperature) until three different negative equations are synthesized.2③ LLM-based filtering.However, the LLM-synthetic equations can be context-unaligned, i.e., some synthesized equations contain notation that is never defined in the paper context, which becomes a superficial shortcut and too effortless for LLMs to identify.To improve data quality, we prompt GPT-4 to identify context-unaligned negative equations.We then eliminate the positive equation and its negative counterparts, where all three negative counterparts are unaligned.This filtering leads to a final of 1,449 positive equations and 4,347 negative equations (each positive equation has three negative counterparts, and at least one negative counterpart is "challenging").</p>
<p>④ Expert-based examination.Furthermore, it's also possible that synthesized negative equations are actually correct (i.e., false negative) -even if the negative and positive  equations are written differently, the final compiled results might be the same.We then employ human experts to review the data further and filter out false negative equations, checking the classification instances for accuracy.</p>
<p>We asked 5 senior PhD students who are experienced in AI research to check all instances.We ask human experts to consider the following criteria for each positive equation and its negative counterparts (each pair): i) Are all equations grammatically correct?ii) After compilation, are all negative equations different from the positive ones?We ask every human expert to use external LaTeX compilation tools (e.g., TeXlive), and identify the pairs that cannot meet the criteria.Each pair is examined by at least two experts, and we only keep pairs that all experts decide to keep.After this strict examination, a total of 1,049 pairs are eventually kept (27.6% pairs are filtered)</p>
<p>Final data.We finally obtain 1,049 positive equations (each has three negative counterparts).We show data statistics of EQINFER in Table 9 and data examples in Figure 8.</p>
<p>EXPERIMENTDESIGN</p>
<p>Given a research topic, such as a novel ML algorithm, a qualified researcher can design a solid experiment plan for it, and clarify underlying motivation to ensure the reliability of the designed experiment.Unlike the concurrent works that focus on the experiment implementation (Lu et al., 2024;Huang et al., 2024), we emphasize the importance of assessing the high-level experiment design of LLMs before the subsequent implementation to avoid any expensive execution iteration.Therefore, as shown in Figure 1, we formulate EXPDESIGN as a text-generation task that takes pre-experiment paper context as input, and then generates the experiment and explanation list.</p>
<p>① Data crawling.As for the data source, we first collect ≥ 10k papers' data from arXiv, including LaTeX sources and PDFs, which cover broad AI categories, including cs.AI, cs.CL, and cs.CV, from year 2018 to 2023.Similarly, to ensure the source data quality, we only use papers that have appeared at well-known conferences.</p>
<p>② Domain-expert annotation.Making a reliable and executable experiment plan requires solid foundation knowledge of a specific research area.Consequently, we set a high standard for choosing annotators: i) be a senior Ph.D. student with at least one peer-reviewed publication in leading AI venues; ii) have more than 4 years of AI research experience; iii) frequently serve as conference reviewers.Finally, we invite a total of 10 qualified experts to participate in our data collection procedure.Given the 10k crawled papers, we first ask every annotator to bid on the papers that they are interested in.After bidding, each of them is assigned 10 papers, i.e., a total of 100 papers to be annotated.During annotation, we post each paper PDF on online Google Drive and ask the annotator to first carefully read the whole paper.Then, we ask them to identify and locate the key experiments in each paper (i.e., highlighting the relevant paragraphs of each experiment).We don't consider some trivial experiments, such as those supplemental analyses in the appendix section.For each identified experiment, the annotator has to concisely answer two questions: i) What did this experiment do? ii) Why did the paper authors conduct this experiment?In other words, we ask the annotator to summarize all the key experiments in this paper and explain the underlying motivations based on their rich domain experience.</p>
<p>③ Multi-round peer discussion.Intuitively, different experts might have different opinions on the same research topic.Particularly, when explaining the underlying motivation of an experiment, adopting only a single expert's opinion might introduce bias to our annotation.Hence, we conduct a further multi-round peer discussion.For each paper, where all the key experiments are identified, summarized, and explained, we ask a different expert (reviewer) to review the annotation by considering the following three criteria: i) Are the identified experiments all the key experiments?ii) Does each experiment summarization covers all key information?iii) Does each explanation sound reasonable and reliable?Each reviewer must leave comments on the online PDF regarding the above criteria, and then the annotator must respond to each commenteither accept the suggestion and revise the previous annotation or provide a "rebuttal" to the reviewer to uphold the annotation.This discussion is iterative until both opinions align.Eventually, for each paper, we collect two lists: i) the experiment list, summarizing each experiment step of the paper; ii) the explanation list, the underlying motivations that are one-one corresponding to the experiment.</p>
<p>Final data.After annotation, we use the pre-experiment context of each paper (according to the first-experiment location identified by the annotator) as the input.Furthermore, we use GPT-4 to delete any sentence that potentially leaks the experiment from the input.3Similar to the EQINFER, we utilize the source LaTeX as the input text to avoid PDF paring noise.As for the image input, we collect those figures within each paper's source LaTeX package and only keep figures that are used in the pre-experiment context.Overall, a total of 100 instances are collected.As shown in Figure 1, the input of each instance is the pre-experiment context (including the figures), and the ground-truth output is the expert-annotated experiment plan and the explanations.Table 10 shows data statistics and Figure 9 illustrates the sample case in EXPDESIGN.</p>
<p>PAPERWEAKNESS</p>
<p>Another critical research task is paper review.Previous works have demonstrated the usefulness of the LLM-based review feedback (Gao et al., 2024;Jin et al., 2024;Lu et al., 2024).However, as indicated by Du et al. (2024); Liang et al. (2024), LLMs only excel at summarizing the research strengths while falling significantly short on weakness criticism.Hence, we build WEAKNESS for particularly investigating the LLM-generated weaknesses.</p>
<p>① Data crawling.We first crawl a total of 3,779 anonymous submissions of ICLR 2023 from OpenReview,4 including PDF and other meta information (e.g., scores, decisions, and tracks).As the ICLR 2023 has 13 distinct tracks while the paper distribution across different tracks is highly biased, we then uniformly sample papers from different research tracks to improve the domain diversity.Meanwhile, during sampling, we also keep the accept/reject papers distributed equally to avoid data bias.In a word, we finally collect a total of 1,000 papers (500 accepted; 500 rejected), uniformly covering all 13 tracks.Please refer to Figure 3 for the track and score distribution of the 1,000 papers.</p>
<p>② Extraction of human-written weaknesses.Since the raw comments crawled from ICLR 2023 are mixed with both strengths and weaknesses, we further employ GPT-4 to extract all the weaknesses from each reviewer's comments and compose multiple weaknesses into a list.Notably, we force GPT-4 to keep the original text of the reviewer, i.e., all weaknesses in our dataset are those original sentences written by the reviewer without any modifications.5What's more, sometimes one reviewer might repeatedly mention the same weakness throughout the comment.In this case, we simply keep all the repeated weaknesses because, if one weakness is repeatedly mentioned by the reviewer, it's intuitively an important weakness that the reviewer wants to emphasise; accordingly, keeping the repeat items can penalize LLMs more on missing this weakness.</p>
<p>For each paper, we can finally get multiple weakness lists (one weakness list per reviewer, one paper can have multiple reviewers).We further delete a few papers without any weaknesses found in the raw comments, resulting in a total of 993 instances, i.e., 993 {paper, weakness lists} pairs.</p>
<p>③ Input data processing.As we mentioned before, we crawl papers from OpenReview instead of arXiv because the under-review paper draft is required for this task.However, not every paper from OpenReview can be found on arXiv, i.e., the source LaTeX code and figures of most under-review papers are unavailable.Therefore, we utilize VILA (Lin et al., 2023) to parse text data out from the PDF; we also employ PDFFigures-2.0(Clark &amp; Divvala, 2016) to extract all the figures and tables (in image) from the paper, as Vila is not good at processing the table data.</p>
<p>Final data.Our final data is composed of 993 instances, each input is paper text along with figure/table images, and each output is peer reviewers' weakness lists.Table 11 shows data statistics; Figure 10 presents an example of the data instances.We show the data diversity (score and track distribution) in Figure 3.</p>
<p>REVIEWCRITIQUE</p>
<p>In addition to identifying weaknesses in paper drafts, a more challenging research task that requires more senior research experience is conducting meta-reviewing.Given a paper submission, along with individual reviews and author rebuttals, meta-reviewing is not to summarize individual reviews.Instead, a meta-reviewer must go through all the information and make a final recommendation.This requires the metareviewer to identify deficient/unreliable review segments (e.g., if a viewpoint is too subjective or contains factual errors) in each individual review and make a decision based on the non-deficient ones.This task demands years of experience in the relevant domain; even for human experts, only senior researchers are typically qualified for meta-reviewing.Therefore, as illustrated in Figure 1, we also investigate how LLMs assist meta-reviewers, specifically in identifying deficient review points.</p>
<p>We reuse the REVIEWCRITIQUE dataset from our recent work (Du et al., 2024), where we crawled papers' initial submissions along with their reviews from OpenReview and employed more than 40 AI research experts to label each review segment (i.e., deficient or not), with detailed human explanations.In total, there were 100 papers with 380 human reviews.Each review was divided into sentence-level segments, resulting in 11,376 review segments (viewpoints).</p>
<p>Evaluation Criteria</p>
<p>For EQINFER , we adopt F 1 as the classification criterion.</p>
<p>For EXPDESIGN and WEAKNESS, since both tasks have free-form outputs, we develop several novel task-specific metrics in addition to the conventional ROUGE (Lin, 2004).</p>
<p>We use LLMs to evaluate the experiment list of EXPDESIGN .Specifically, given a model-predicted experiment list p, and the ground-truth list g, we calculate:
En-Precision = 1 m m i=1 f (pi, g) (1) En-Recall = 1 n n j=1 f (gj, p)(2)
where the m and n are the list length of p and g; f (.) represents the LLM prompting, where we prompt LLM to decide whether each predicted experiment item (p i ) is entailed by the whole ground-truth list (g), proceeding with binary output, and vice versa.Intuitively, En-Precision reflects how many prediction experiments match ground-truth experiments.In this work, we used GPT-4o as an evaluator.</p>
<p>While for the explanation generation of EXPDESIGN, as the prediction experiments are one-on-one corresponding to the ground truth, we adopt a semantic-based metric:
S-Match = 1 m m i=1 sim(pi, gi)(3)
where we use SentenceBERT (Reimers, 2019) to measure the semantic similarity between p i and g j .</p>
<p>Unlike EXPDESIGN, the ground truth of WEAKNESS is multiple reviewers' weakness lists.Instead of merely merging the opinions of various reviewers into one flattened list and keeping LLM-as-judge as the metric (which is not only costly but also loses the structural information of diverse research perspectives), we employ the following semanticbased metric to efficiently evaluate predicted weaknesses:
S-Precision = 1 m m i=1 1 r r k=1 max j sim(pi, g k j ) (4) S-Recall = 1 r r k=1 1 n k n k j=1 max i sim(g k j , pi)(5)
where r is the number of reviewers of the given paper, n k means the length of k-th reviewer's weakness list, and g k j indicates the j-th item in k-th reviewer's weakness list.</p>
<p>Additionally, in the real world, we would think a review weakness is reliable if it is specific to a paper.Meanwhile, we also hope the review is informative, i.e., no excessive similar weaknesses in one review.Inspired by the classic TF-IDF, we propose a novel review diversity metric:
ITF-IDF = 1 w w j=1 1 mj m j i=1 log mj O j i × log w R j i (6) O j i = m j k=1 sim(p j i , p j k )(7)R j i = w l=1 max s sim(p j i , p l s )(8)
where the w is the total number of papers in the dataset, p j is j-th paper's prediction weakness list, p j i is the i-th weakness in p j .Moreover, O j i calculates the intra-paper occurrence frequency of p j i ; R j i is the "soft" number of papers that also contain the p j i , which is computed by summing the maximum similarity scores between p j i and other paper's weaknesses.In a word, O j i measures informativeness, and R j i measures specificity.The complete ITF-IDF consider both aspects and reflects the overall weakness diversity.</p>
<p>For REVIEWCRITIQUE , we use F 1 score as the classification metric; while for the deficiency explanation, we use ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020) to reflect how well the model-generated explanation aligns with the expert's annotation.</p>
<p>Table 1: Various LLMs' performances on EQINFER task (1,049 positive and 3,147 negative samples)."All-positive" indicates a baseline that predicts all equations as positive.</p>
<p>Methods</p>
<p>F 1 Prec.Rec.</p>
<p>All-Positive 40.00 25.00 100.00 Open-source LLMs OLMo-7B (Groeneveld et al., 2024) 13.64 11.93 15.91 Mistral-7B (Jiang et al., 2023) 28.45 19.28 54.24 Mixtral-8x22B-MoE (Jiang et al., 2024) 40.90 26.15 93.80 Qwen 2.5-72B (Qwen Team, 2024) 31.22 26.28 57.40 Llama 3.1-70B (MetaAI, 2024) 33.08 22.14 65.39 Closed-source LLMs Gemini 1.5 Pro (Anil et al., 2023) 46.74 32.05 86.27 Claude 3.5 sonnet (Anthropic, 2024a) 45</p>
<p>Experiments and Analyses</p>
<p>In this section, we conduct extensive experiments on AAAR-1.0,across various mainstream LLMs, to quantify the current LLMs' capacity to tackle high-level research tasks.Specifically, § 5.1 for EQINFER , § 5.2 for EXPDESIGN , § 5.3 for WEAKNESS ; and § 5.4 for REVIEWCRITIQUE .Please refer to Appendix B.2 for running details of the LLMs.</p>
<p>EQUATIONINFERENCE</p>
<p>Settings.As different LLMs have distinct context windows, to ensure a fair comparison, we fix the maximum input length for all models.According to Table 9, we empirically use 1,000 words for both contexts before and after equations, i.e., 2,000 surrounding words.</p>
<p>Main results.Table 1 shows the main results.Firstly, a simple baseline that predicts all equations as positive achieves 40% F 1 (due to the 1:3 of positive and negative equations), while nearly all open-source LLMs even cannot beat this naive baseline.Notably, though the performance of Mixtral is slightly superior to the baseline, the extremely biased precision and recall scores imply that Mixtral is also simply predicting almost all samples as positive instead of truly inferring.Meanwhile, compared to the All-Positive baseline, the performance superiority of the strong closesource LLMs is not significant, the best LLM on this task only obtains 46.74%, which demonstrates the challenge of EQINFER compared with other similar benchmarks (Song et al., 2023).The generally high recall with low precision of all LLMs also indicates real-world risks, e.g., relying on LLMs to check the validity of equations in paper review.</p>
<p>Q: Do more contexts boost performance?EQIN-FER places high demands on reasoning within the scientific context.To quantify the impact of input context length, we scale the input length (per side) from 100 to 1,500 words.As shown in Figure 4, for the open-source LLMs (Llama and Qwen), an appropriate context length can boost the performance; while for GPT-4o, scaling up the context length doesn't contribute much to the F 1 .However, during the scaling, we find that the precision of GPT-4o is gradually increased, and the recall is decreased accordingly; considering the label distribution of EQINFER, we believe precision can better reflect the model's true capacities on this task.Thus, we anticipate that scaling up context shall be beneficial to those strong close-source LLMs such as GPT-4o.</p>
<p>EXPERIMENTDESIGN</p>
<p>Settings.Similarly, we unify the input context length of different LLMs to ensure a fair comparison.According to Table 10, we set 2,000 and 3,000 input words for openand closed-source LLMs, respectively.Meanwhile, as experiment explanation is the subsequent task of experiment design, using model-generated experiments can propagate errors in explanation, leading to inferior results for most LLMs.To this end, we provide LLMs with the oracle experiments when generating explanations.</p>
<p>Main results.We find that the open-source LLMs often try to copy the terms or phrases from the given experiment, or even simply paraphrase the experiment instead of explaining, which results in a high superficial overlap with the ground-truth explanation.This observation highlights the importance of adopting the proposed S-Match to avoid evaluation bias of traditional generation metrics.</p>
<p>Q 1 : What is the quality of the model-generated novel experiments?The low En-Precision of LLMs in Table 2 indicates the creativity of LLMs in generating novel experiments.We then randomly sample 15 papers from the EX-PDESIGN and ask 3 experts to manually review the modelgenerated novel experiments.Specifically, we ask the experts to judge the necessity of the novel experiments, where we set three necessity levels: "A" indicates the experiment is necessary/mandatory to support the main claim, "B" rep-   3 shows the necessity scores of the three strongest LLMs.We find that LLMs consistently generate a lot of novel experiments, especially the Claude; though most of them are optional, even fancy/unrelated experiments, there are still a considerable amount of necessary experiments generated, e.g., the results of o1.We further find that some novel experiments can be regarded as useful supplementary analyses w.r.t. the human experiments.Table 13 shows examples of model-suggested experiments.</p>
<p>Q 2 : Can self-contained experiment design enhance the experiment explanation?When generating the explanation in Table 2, we provide LLMs with each individual experiment and let them explain one by one, because we find that, when providing the whole experiment list, those opensource models only explain partial experiments because of their poor instruction-following capacity.However, there are intuitively some semantic or logical relations between different experiments, e.g., some experiments are prerequisites to others.Therefore, this one-by-one prompting might break the self-containment of an experiment plan.Consequently, we test with the "whole-list" prompting, where the LLMs are given the complete experiment list and are asked to explain all experiment steps together.</p>
<p>As shown in Table 4, unlike the open-source LLMs, the explanation performances of those closed-source LLMs are generally improved after adopting whole-list prompting.According to further manual checking, after maintaining the self-containment of the experiments, the LLMs can refer to other experiments and better grasp the underlying motivation of the current experiment.</p>
<p>Q 3 : Do human evaluation results align with automatic metrics for explanation?As the explanation can be openended, in this paragraph, we provide the human evaluation results on different LLMs' experiment explanation outputs.</p>
<p>In detail, we randomly select 20 out of 100 papers and ask 5 annotators to read the experiments along with each model's explanations; we then let the annotator decide whether each model's explanation is acceptable (see Appendix C.3 for more details).Table 5 illustrates the results, where the score variance is higher than Table 2.However, the performance ranking of both tables is perfectly correlated with each other Q 4 : Do more contexts boost performance?We also investigate the impact of input context length for EXPDESIGN.As shown in Figure 5, we scale up the input pre-experiment context length from 0.1k to 10k tokens (10k is the length of the longest paper).For the experiment design, more input context does improve the performance of different LLMs, while this benefit stops after exceeding 8k tokens, which means that after the necessary information has been covered, scaling context becomes inefficient.Meanwhile, the explanation generation results reveal that LLMs primarily depend on given experiments rather than paper context to explain motivations.However, we do not expect this as we hope LLMs can explain the motivation based on a thorough understanding of the paper, just like how human experts do.Hence, there is still a considerable gap between the LLMs and humans in terms of grasping research motivations.</p>
<p>Q 5 : Does multi-modal input boost performance?Intuitively, besides the text, when designing experiments for a given research topic, the figures can provide rich supplementary information, such as an algorithm illustration that can help better understand this research topic and underlying motivations.Hence, we test the performance of different LMMs (Large Multimodal Models), including GPT4-o and InternVL2 (Chen et al., 2024b).Table 14 shows the ablation results on the figure data.To our surprise, the figure data doesn't improve the LMMs' results in this task, even harming the performances.This might be due to the low informativeness of the figures, as figures usually consume more input tokens but act only as supplementary information to the text, indicating future work on developing LMMs that can effectively leverage the scientific figures.</p>
<p>PAPERWEAKNESS</p>
<p>Settings.Intuitively, full paper content is necessary for paper reviewing.Therefore, instead of setting a maximum input length, in WEAKNESS, we try to utilize the whole paper.As the input length of WEAKNESS is extremely long (see Table 11), we adopt a "split-combine" method -we first split the whole paper into smaller pieces and let LLMs predict the weaknesses of each piece separately; after that, we merge all pieces' weaknesses as a final prediction.</p>
<p>For the length of each small piece, we set 2,000 and 3,000 words for open-and closed-source LLMs, respectively.Additionally, in this task, we also examine the performance of AI-SCI (Lu et al., 2024), which enhances LLMs' paper review ability by leveraging advanced prompting techniques, e.g., self-reflection (Shinn et al., 2024) and response ensembling (Wang et al., 2023). 6ain results.Table 6 shows the main results, where the closed-source LLMs' overall performances are generally superior to the results of open-source LLMs.Similarly, closedsource LLMs are particularly excellent in S-Recall because of more generated weaknesses.However, there is still a considerable gap in the weakness diversity between the LLMs and human experts. 7Compared with human review, most LLM-generated weaknesses are vague and lack the necessary knowledge about some frontier research works.Surprisingly, AI-SCI performs worse than backbone GPT-4o, especially on ITF-IDF, which suggests the challenge of WEAKNESS, i.e., simply adopting popular prompting techniques cannot well address this task.</p>
<p>Q 1 : Is the split-combine effective?Ideally, if the LLM has a sufficient context window size, splitting the input papers for separate processing is unnecessary.Consequently, in this paragraph, we utilize the LLMs accepting long context input to compare "split-combine" with "no-split", i.e., letting LLMs write weaknesses by giving the full paper.In practice, we set the maximum number of input words to 20k, which ensures ≥95% papers in the WEAKNESS can be fully processed.As shown in Table 12, compared with giving the full paper contexts, split-combine generally brings about superior performances.During manual checking, we find that, when full paper is available, LLMs frequently neglect some important sections and omit weaknesses accordingly, while split-combine ensures that the LLMs can carefully brainstorm weaknesses within each smaller piece.Surprisingly, the LLMs' performances with full paper context can be even worse than just remaining the first 3,000 words.This implies that even the current powerful long-context LLMs still fall short when processing long scientific documents.Q 2 : Does multi-modal input boost performance?Our dataset covers both tables and figure illustrations extracted from the paper PDF as inputs.Intuitively, when reviewing a paper, both figures and tables are critical, not only for a better understanding, but also because some weaknesses are related to tables/figures.8Therefore, in Table 15, we adopt two LMMs to investigate the effectiveness of image inputs.Overall, image information, including both figures and tables, doesn't bring significant performance improvement, i.e., only InternVL2 gains a performance boost after incorporating figures; while tables slightly drop both models' results.This is probably because the LMMs cannot reason well over the information-intensive images, especially the table images.</p>
<p>REVIEWCRITIQUE</p>
<p>Settings.As individual review comments are split into multiple smaller segments (sentences), in order to avoid the performance variance that comes from the prompting, we follow Du et al. (2024) to utilize two prompting strategies.i) Labeling-All: given everything necessary including a list of indexed review segments, require the LLM to output a list of triples, like {id, reliable or not, explanation}.ii) Select-Deficient: Given everything necessary including a list of indexed review segments, require the LLM to output a list of tuples, {id, explanation}, when it believes the "id" corresponds to a deficient segment.</p>
<p>To further enhance evaluation robustness, we ensemble the results obtained from the two prompting strategies using two methods.i) Both "No": if both prompts classify a segment as deficient, we consider it to be deficient.ii) Either "No": if either of the prompts labels a segment as Deficient, we consider it to be deficient.</p>
<p>Main results.We put the results of Du et al. (2024) in Table 7. Closed-source models (GPT-4, Claude Opus, and Gemini 1.5) generally outperform open-source models (Llama3-8B and 70B, Qwen2-72B) in F 1 score.Claude Opus achieves the highest F 1 scores, with GPT-4 and Gemini 1.5 performing slightly worse.Notably, recall scores are consistently higher than precision scores across all LLMs and prompting strategies, suggesting that LLMs tend to incorrectly identify segments as deficient.Despite the superior performance of the closed-source models, their F 1 scores remain relatively low even with different prompt strategies, highlighting the challenges LLMs face in such expertiseintensive tasks and emphasizing the importance of human expertise in the meta-reviewing process.</p>
<p>Q: How about the LLMs explanation quality regarding the deficient review?As we also prompt the LLMs to generate the corresponding explanation on why they think each review segment is deficient, we report how well the model-generated deficiency explanation aligns with the human explanation.</p>
<p>We put the results of Du et al. (2024) in Table 8.The results in Table 8 show that overall scores for all LLMs are relatively low, indicating they can identify some Deficient segments but struggle to articulate their reasoning.Among the LLMs, Claude Opus achieves the highest scores across all metrics, suggesting its explanations align best with human annotators.Claude Opus also excels in identifying Deficient</p>
<p>Conclusion</p>
<p>In this work, we propose AAAR-1.0, a novel benchmark targeting a comprehensive evaluation of the current LLMs' capacity in assisting AI research.AAAR-1.0 consists of distinct expertise-intensive tasks along with the curated evaluation metrics.We collect high-quality data by employing senior AI researchers and conducting strict data examinations.Extensive experiments highlight the challenges and values of AAAR-1.0.</p>
<p>Impact Statement</p>
<p>Our study explores whether LLMs can assist human researchers in AI research.We do not advocate for AI replacing human researchers.Instead, we stress that the primary responsibility for scientific research should remain with humans to prevent societal risks, with LLMs serving as tools to enhance research efficiency.Specifically, our work analyzes the strengths and weaknesses of LLMs to ensure researchers remain judicious in their use of these tools.Our goal is to mitigate risks while maximizing the benefits offered by LLMs.We are committed to the careful distribution of data collected in our research, ensuring it is used solely for research purposes.When calculating the metrics, specifically for the similarity-based scores, we utilize SentenceBERT (Reimers, 2019) to encode each segment (e.g., each experiment idea in the list) into a dense vector, and then calculate the cosine similarity,11 which takes about 1GB of memory when running on a single A100 GPU.• OLMo-7B (Groeneveld et al., 2024): https://huggingface.co/allenai/OLMo-7B</p>
<p>• Falcon-40B (Almazrouei et al., 2023): https://huggingface.co/tiiuae/falcon-40b</p>
<p>• Gemma 2-27B (Gemma Team, 2024): https://huggingface.co/google/gemma-2-27b</p>
<p>• Mistral-7B (Jiang et al., 2023): https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3</p>
<p>• Mixtral-8x22B-MoE (Jiang et al., 2024) : https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1</p>
<p>• Llama 3.1-70B (MetaAI, 2024): https://huggingface.co/meta-llama/Llama-3.1-70B</p>
<p>• Qwen 2.5-72B (Qwen Team, 2024): https://huggingface.co/Qwen/Qwen2.5-72B</p>
<p>We use VLLM to unify the inference endpoints of all the above models.12We use Pytorch 2.4.0 with CUDA 12.1, and use 8 NVIDIA A100 GPUs for the LLMs inference.</p>
<p>Meanwhile, we use the gpt-4o-2024-08-06, gpt-4-1106-preview, o1-preview-2024-09-12, gemini-1.5-pro-002,and claude-3-5-sonnet-20240620 for the closed-source LLMs.We use LiteLLM to unify the API calling for all these LLMs.13</p>
<p>Given the unstable performance of LLMs, particularly closed-source ones, we run each model thrice during our experiments, selecting the median result from these repeated runs.12 show the context scaling results of EQINFER, EXPDESIGN, and WEAKNESS.</p>
<p>Table 12: The performance comparison of different input processing methods for WEAKNESS .We use GPT-4o and GPT-4-Turbo because both accept a maximum of 128k tokens input.We also put the results of AI-SCI in the table for reference.Here, "split-combine" splits the input paper into several pieces, where each piece's length is denoted as "window size"; "no-split" means the conventional input cutting, for example, if the window size is 3,000, then only the first 3,000 words in the paper are used.According to the data statistics, 20,000 words can cover maximum lengths of more than 95% of the papers in our dataset.Figure 6 illustrates the evaluation guideline for novel experiments generated by LLMs.We ask 3 senior PhD students to evaluate each paper; that is, if the first two annotators disagree with each other, a third annotator will make a final decision.Table 13 presents several human evaluation cases.</p>
<p>Models</p>
<p>C.3. Human Evaluation on LLM-Generated Explanation</p>
<p>We ask 5 annotators to evaluate the LLM-generated explanations.Specifically, each of them is assigned 4 or 5 papers, along with the corresponding experiment lists.For each paper, the annotator is given 5 different models' outputs (model names are anonymized), and the annotator has to decide if each LLM-generated explanation is acceptable according to the experiment.We show the human evaluation results in Table 5.</p>
<p>C.4. Multi-Modal Input Ablation</p>
<p>We post the multi-modal ablation study of EXPDESIGN and WEAKNESS in Table 14 and Table 15.</p>
<p>D. Data cases and Annotation Platform Illustration</p>
<p>As shown in Figure 8, 9, and 10, we show the sample cases of the three tasks in AAAR-1.0.Meanwhile, we illustrate the screenshot of our annotation platform in Figure 7.</p>
<p>E. Prompt Templates</p>
<p>In this appendix, we attach all the prompts used in this work, including prompts in data collection and model prediction, as shown in Figure 11, 12, and 13.</p>
<p>1.4% 7.8% 47.8% 40.1% 2.9% Overall Score Distribution Score Range [1, 3) [3, 5) [5, 6) [6, 8) [8, 10) (a) The review score distribution of the papers used in WEAKNESS .</p>
<p>This paper proposes an algorithm […], the result z is defined as below: z = where W is the parameter, a and b are the […] (A).z = W<em>a+b (B).z = W</em>b+a (C).z = W<em>a</em>b (D).z = W*a/b</p>
<p>Figure 2 :
2
Figure 2: Data construction workflows of the three tasks in AAAR-1.0.</p>
<p>Figure 4 ,
4
Figure 4, Figure 5, and Table12show the context scaling results of EQINFER, EXPDESIGN, and WEAKNESS.</p>
<p>The track distribution of the papers used in WEAKNESS .</p>
<p>Figure 3 :
3
Figure 3: The data diversity illustration of WEAKNESS , including the score distribution and track distribution of the papers used in our dataset.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: The input context length scaling trend on the EQINFER task.</p>
<p>Figure 6 :
6
Figure 6: The human guideline for evaluating the LLM-generated novel experiments.</p>
<p>Figure 7 :
7
Figure7: The annotation platform for collecting the annotation of EXPDESIGN .We ask annotators to first make comments on the Google Drive PDF, then move all the annotations to the online Google Doc (for further verification and discussion).</p>
<p>Figure 8 :
8
Figure 8: Two sample cases of EQINFER .</p>
<p>Figure 10 :
10
Figure 10: A sample case of WEAKNESS .</p>
<p>Figure 11 :
11
Figure 11: The prompts used in EQINFER , including both data collection and model prediction.</p>
<p>Figure 12 :
12
Figure 12: The prompts used in EXPDESIGN , including both data collection and model prediction.</p>
<p>Figure 13 :
13
Figure 13: The prompts used in WEAKNESS .</p>
<p>Table 2 :
2
Various LLMs' performances on the 100 instances of EXPDESIGN .The explanation generation is based on the oracle experiments to prevent error propagation."Copy Input" directly copies each experiment idea as the explanation.
Experiment DesignExperiment Explanation</p>
<p>Table 3 :
3
The human evaluation results on the novel experiments suggested by LLMs."A", "B", and "C" represent the different quality level (i.e., necessity); "A" is the best level.
Models# of novel EXPNecessity (%)ABGemini 1.5 Pro5930.5945.76Claude 3.5 sonnet11221.7850.00o1-preview7135.8436.61resents optional/supplementary experiments, and "C" forthose unrelated experiments (see Appendix C.2 for evalua-tion details). Table</p>
<p>Table 4 :
4
The impact on S-Match scores of maintaining the experiment's self-containment for EXPDESIGN .
ModelsOne-by-OneWhole-ListLlama 3.1-70B50.0549.36 (↓ 0.7)Qwen 2.5-72B51.1248.56 (↓ 2.6)Gemini 1.5 Pro52.8757.48 (↑ 4.6)Claude 3.5 sonnet53.0359.11 (↑ 6.1)GPT-455.0356.95 (↑ 1.9)GPT-4o54.7958.54 (↑ 3.8)o1-preview58.5561.58 (↑ 3.0)</p>
<p>Table 5 :
5
The human evaluation results on LLMs' output explanations of EXPDESIGN ."Acc.ratio" means how many model outputs are accepted by the annotator.
ModelsAcc. ratioLlama 3.1-70B22.93Gemini 1.5 Pro55.07Claude 3.5 sonnet61.46GPT-4o69.72o1-preview76.14(Spearman's rank correlation coefficient = 1), demonstrat-ing the effectiveness of S-Match.</p>
<p>Table 6 :
6
Various LLMs' performances on the 993 instances of WEAKNESS .
Weakness Diversity</p>
<p>Table 7 :
7
(Du et al., 2024)024), various LLMs' performances on the 11,376 instances of REVIEWCRITIQUE .The best F1 score among different prompt methods for a single model is underlined.The best F1 score across all models is also bold.
Models</p>
<p>Table 8 :
8
Evaluation of LLMs' explanations for correctly identified deficient segments.
ModelROUGE-1/2/L/BERTScoreGPT-417.13 / 2.71 / 14.64 / 55.63Claude Opus 20.18 / 3.69 / 17.52 / 57.28Gemini 1.518.47 / 2.98 / 16.38 / 56.46Llama3-8B16.49 / 2.22 / 13.65 / 55.23Llama3-70B 15.94 / 1.95 / 13.78 / 57.09Qwen2-72B17.07 / 3.00 / 14.69 / 56.88segments, as shown previously. GPT-4 and Gemini 1.5 showsimilar performance to Claude Opus. The open-source mod-els, Llama3 (8B and 70B) and Qwen2-72B, generally scorelower than the closed-source models.</p>
<p>Table 9 :
9
The statistics of EQINFER .Here, the "left" and "right" input context indicates the paper contexts before and after the missed equation; "pos."means the ground-truth equations (written by the source paper authors), while "neg." is the GPT4-synthetic wrong equations.</p>
<h1>of positive equations1,049# of negative equations3,147# of source papers869ave. "left" input context length (in words)4,377ave. "right" input context length (in words)6,362max "left" input context length (in words)24,849max "right" input context length (in words)32,948min "left" input context length (in words)711min "right" input context length (in words)8ave. "pos." output equation length (in character)55ave. "neg." output equation length (in character)48max "pos." output equation length (in character)1,039max "neg." output equation length (in character)306min "pos." output equation length (in character)6min "neg." output equation length (in character)4B. Implementation DetailsB.1. Metric Details</h1>
<p>Table 10 :
10
The statistics of EXPDESIGN .</p>
<h1>of instances100# of source papers100ave. input context length (in words)4,288max input context length (in words)9,799min input context length (in words)698ave. # of input figures2.6max # of input figures16.0min # of input figures0.0ave. length of Experiment&amp;Explanation list5.7ave. length per experiment (in words)34.3ave. length per explanation (in words)27.1max length of Experiment&amp;Explanation list13max length per experiment (in words)135max length per explanation (in words)89min length of Experiment&amp;Explanation list2min length per experiment (in words)9min length per explanation (in words)9</h1>
<p>B.2. LLMs Running DetailsIn our experiments, we utilize various LLMs, including both closed and open-sourced.We list the model weight sources for the open-source LLMs:</p>
<p>Table 11 :
11
The statistics of WEAKNESS .</p>
<h1>of instances993# of source papers993ave. input context length (in words)9,811max input context length (in words)49,195min input context length (in words)24ave. # of input figures7.0max # of input figures37.0min # of input figures0.0ave. # of input tables4.3max # of input tables53.0min # of input tables0.0ave. # of reviewers per paper3.8max # of reviewers per paper9.0min # of reviewers per paper3.0ave. # of weaknesses per reviewer4.8max # of weaknesses per reviewer39.0min # of weaknesses per reviewer1.0ave. length of weakness (in words)39.1max length of weakness (in words)371.0min length of weakness (in words)2.0C. More Experiment Results and DetailsC.1. Input Context Scaling Investigation</h1>
<p>Pennsylvania State University;
Netflix;
University of California, Davis
; 4 University of Illinois
Chicago; 5 Fudan
 6<br />
Zhejiang University; 7 University of
Alabama at Birmingham;
 8  Ohio State University; 9 Salesforce Research. Correspondence to: Renze Lou <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#114;&#101;&#110;&#122;&#101;&#46;&#108;&#111;&#117;&#64;&#112;&#115;&#117;&#46;&#101;&#100;&#117;">&#114;&#101;&#110;&#122;&#101;&#46;&#108;&#111;&#117;&#64;&#112;&#115;&#117;&#46;&#101;&#100;&#117;</a>, Wenpeng Yin <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#119;&#101;&#110;&#112;&#101;&#110;&#103;&#64;&#112;&#115;&#117;&#46;&#101;&#100;&#117;">&#119;&#101;&#110;&#112;&#101;&#110;&#103;&#64;&#112;&#115;&#117;&#46;&#101;&#100;&#117;</a>.
EQINFER also facilitates a multiple-choice QA setting; while we find a binary inference is more challenging for LLMs.
The number of negative equations is empirically decided.
About 9.8% sentences are deleted.
We adopt ICLR because it releases full submissions, while some other conferences only release accepted papers.
We manually checked GPT-4's extraction results of 200 cases -GPT-4 only missed ≤1% of reviewer-written weaknesses and maintained almost all the original text.
We don't run AI-SCI on EXPDESIGN, because AI-SCI takes model-generated ideas as the inputs, which are incompatible with our task setting.
The human's ITF-IDF score in Table6can be slightly underestimated. This is because we keep the repeated weaknesses in the human review, which affects the human review's informativeness (lower ITF) but is useful when calculating the S-Recall for LLMs.
We find that there is approximately one human-written weakness related to figures or tables in each paper.
https://www.nltk.org/
https://github.com/phfaist/pylatexenc
https://huggingface.co/sentence-transformers/all-mpnet-base-v2
https://github.com/vllm-project/vllm
https://github.com/BerriAI/litellm
AppendicesWithin this supplementary material, we elaborate on the following aspects:A. Data Statistics and DiversityWe provide the detailed data statistics of three datasets in our benchmark, as shown in Table9, 10, and 11.We use the NLTK package 9 to tokenize words and count the length.When calculating the length of equations, we use the pylatexenc tool 10 to simplify the equations first.Meanwhile, for the WEAKNESS, we also plot the review scores distribution of the papers used in the dataset, as well as the track distribution.As can be found in Figure3, our dataset has a decent distribution, where the papers are uniformly distributed across 13 tracks, and most papers' scores ranged from 5 to 8 (i.e., most papers are weakly rejected or accepted).2. Off-the-shelf entailment classification performance: The authors should provide entailment classification performance of existing models on the proposed dataset without fine-tuning.3. Human Performance: The authors should show human performance on the proposed dataset.4.Performance of fine-tuned models: The authors should provide the performance of models fine-tuned on the proposed dataset.5. Performance on the evidence retrieval task: The authors should show the performance on the evidence retrieval task, which is a sub-task of the proposed dataset.Performance of LLMs:The authors should provide the performance of LLMs on the proposed dataset.7. Retrieval+Entailment: Authors should provide experiments on a framework of retrieving evidence sentences and evaluate entailment by using the retrieved sentences.
E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, E Goffinet, D Heslow, J Launay, Q Malartic, B Noune, B Pannier, G Penedo, Falcon-40B: an open large language model with stateof-the-art performance. 2023</p>
<p>R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, G Team, arXiv:2312.11805family of highly capable multimodal models. 2023arXiv preprint</p>
<p>. Anthropic, June 2024aIntroducing claude 3.5 sonnet</p>
<p>Introducing the next generation of claude \ anthropic. Anthropic, March 2024b</p>
<p>. J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, arXiv:2309.166092023arXiv preprint</p>
<p>Automated focused feedback generation for scientific writing assistance. E Chamoun, M Schlichktrull, A Vlachos, arXiv:2405.204772024arXiv preprint</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. J S Chan, N Chowdhury, O Jaffe, J Aung, D Sherburn, E Mays, G Starace, K Liu, L Maksin, T Patwardhan, arXiv:2410.070952024arXiv preprint</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Z Chen, S Chen, Y Ning, Q Zhang, B Wang, B Yu, Y Li, Z Liao, C Wei, Z Lu, arXiv:2410.050802024aarXiv preprint</p>
<p>How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Z Chen, W Wang, H Tian, S Ye, Z Gao, E Cui, W Tong, K Hu, J Luo, Z Ma, arXiv:2404.168212024barXiv preprint</p>
<p>Pdffigures 2.0: Mining figures from research papers. C Clark, S Divvala, Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries. the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries2016</p>
<p>NLP researchers: Critique paper (meta-)reviewing. J Du, Y Wang, W Zhao, Z Deng, S Liu, R Lou, H P Zou, P N Venkit, N Zhang, M Srinath, H R Zhang, V Gupta, Y Li, T Li, F Wang, Q Liu, T Liu, P Gao, C Xia, C Xing, J Cheng, Z Wang, Y Su, R S Shah, R Guo, J Gu, H Li, K Wei, Z Wang, L Cheng, S Ranathunga, M Fang, J Fu, F Liu, R Huang, E Blanco, Y Cao, R Zhang, P S Yu, W Yin, 10.48550/arXiv.2406.16253The 2024 Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>Z Gao, K Brantley, T Joachims, Reviewer2, arXiv:2402.10886Optimizing review generation through prompt generation. 2024arXiv preprint</p>
<p>D Groeneveld, I Beltagy, P Walsh, A Bhagia, R Kinney, O Tafjord, A H Jha, H Ivison, I Magnusson, Y Wang, S Arora, D Atkinson, R Authur, K Chandu, A Cohan, J Dumas, Y Elazar, Y Gu, J Hessel, T Khot, W Merrill, J Morrison, N Muennighoff, A Naik, C Nam, M E Peters, V Pyatkin, A Ravichander, D Schwenk, S Shah, W Smith, N Subramani, M Wortsman, P Dasigi, N Lambert, K Richardson, J Dodge, K Lo, L Soldaini, N A Smith, H Hajishirzi, Olmo, Accelerating the science of language models. 2024Preprint</p>
<p>Adaptive and explainable margin trading via large language models on portfolio management. J Gu, J Ye, W Yin, G Wang, Proceedings of the 5th ACM International Conference on AI in Finance (ICAIF'24). the 5th ACM International Conference on AI in Finance (ICAIF'24)2024</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Q Huang, J Vora, P Liang, J Leskovec, Forty-first International Conference on Machine Learning. 2024</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, C Bamford, D S Chaplot, D Casas, E B Hanna, F Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>Y Jin, Q Zhao, Y Wang, H Chen, K Zhu, Y Xiao, J Wang, Agentreview, arXiv:2406.12708Exploring peer review dynamics with llm agents. 2024arXiv preprint</p>
<p>Can large language models unlock novel scientific research ideas?. S Kumar, T Ghosal, V Goyal, A Ekbal, arXiv:2409.061852024arXiv preprint</p>
<p>Biomistral: A collection of open-source pretrained large language models for medical domains. Y Labrak, A Bazoge, E Morin, P.-A Gourraud, M Rouvier, R Dufour, arXiv:2402.103732024arXiv preprint</p>
<p>H Li, H Jiang, T Zhang, Z Yu, A Yin, H Cheng, S Fu, Y Zhang, W He, Traineragent, arXiv:2311.06622Customizable and efficient model training through llm-powered multiagent system. 2023arXiv preprint</p>
<p>R Li, T Patel, Q Wang, X Du, arXiv:2408.14033Mlr-copilot: Autonomous machine learning research based on large language models agents. 2024arXiv preprint</p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. W Liang, Y Zhang, H Cao, B Wang, D Y Ding, X Yang, K Vodrahalli, S He, D S Smith, Y Yin, NEJM AI. 18AIoa2400196, 2024</p>
<p>Rouge: A Package for Automatic Evaluation of Summaries. C.-Y Lin, Text summarization branches out. 2004</p>
<p>On pre-training for visual language models. J Lin, H Yin, W Ping, Y Lu, P Molchanov, A Tao, H Mao, J Kautz, M Shoeybi, S Han, Vila, 2023</p>
<p>Lost in the middle: How language models use long contexts. N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Papermage: A unified toolkit for processing, representing, and manipulating visually-rich scientific documents. K Lo, Z Shen, B Newman, J Z Chang, R Authur, E Bransom, S Candra, Y Chandrasekhar, R Huff, B Kuehl, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2023</p>
<p>C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.06292The AI Scientist: Towards fully automated open-ended scientific discovery. 2024arXiv preprint</p>
<p>Introducing llama 3.1: Our most capable models to date. Metaai, July 2024</p>
<p>Identifying social norm violation in movie plots: from borat to american pie. Y Neuman, Y Cohen, W Yin, 10.1093/llc/fqad052Digit. Scholarsh. Humanit. 3842023</p>
<p>Hello gpt-4o. Openai, May 2024a</p>
<p>Introducing openai o1. Openai, September 2024b</p>
<p>. Achiam Openai, J Adler, S Agarwal, S Ahmad, L Akkaya, I Aleman, F L Almeida, D Altenschmidt, J Altman, S Anadkat, S , arXiv:2303.087742023arXiv preprint</p>
<p>River classification as a geographic tool in the age of big data and global change. S Praskievicz, Geographical Review. 10812018</p>
<p>Artificial intelligence in medicine for chronic disease classification using machine learning. M Rakhimov, R Akhmadjonov, S Javliev, 2022 IEEE 16th International Conference on Application of Information and Communication Technologies (AICT). IEEE2022</p>
<p>N Reimers, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. C Si, D Yang, T Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Ml-bench: Evaluating large language models and agents for machine learning tasks on repository-level code. L Song, J Zhang, L Cheng, P Zhou, T Zhou, I Li, Nlpbench, X Tang, Y Liu, Z Cai, Y Shao, J Lu, Y Zhang, Z Deng, H Hu, K An, R Huang, arXiv:2309.15630Evaluating large language models on solving nlp problems. 2023. 20232311arXiv preprintarXiv e-prints</p>
<p>Google launches gemma 2, its next generation of open models. G Team, Jun 2024a</p>
<p>5: A party of foundation models. Q Team, Qwen2, September 2024b. </p>
<p>Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. M Wang, L Chen, C Fu, S Liao, X Zhang, B Wu, H Yu, N Xu, L Zhang, R Luo, arXiv:2406.174192024arXiv preprint</p>
<p>Selfconsistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. OpenReview.net, 2023</p>
<p>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. B Yu, F N Baker, Z Chen, X Ning, H Sun, arXiv:2402.093912024aarXiv preprint</p>
<p>H Yu, Z Hong, Z Cheng, K Zhu, K Xuan, J Yao, T Feng, J You, Researchtown, arXiv:2412.17767Simulator of human research community. 2024barXiv preprint</p>
<p>Evaluating text generation with BERT. T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, Bertscore, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. 20202020OpenReview.net</p>            </div>
        </div>

    </div>
</body>
</html>
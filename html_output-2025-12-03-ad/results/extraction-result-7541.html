<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7541 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7541</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7541</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-aec95e6330033e0ec39fb5a069d647288c03b945</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/aec95e6330033e0ec39fb5a069d647288c03b945" target="_blank">Assessing Large Language Models on Climate Information</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> A comprehensive evaluation framework, grounded in science communication research, is presented to assess LLM responses to questions about climate change, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues.</p>
                <p><strong>Paper Abstract:</strong> As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in critically relevant domains. We present a comprehensive evaluation framework, grounded in science communication research, to assess LLM responses to questions about climate change. Our framework emphasizes both presentational and epistemological adequacy, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues. Our evaluation task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel protocol for scalable oversight that relies on AI Assistance and raters with relevant education. We evaluate several recent LLMs on a set of diverse climate questions. Our results point to a significant gap between surface and epistemological qualities of LLMs in the realm of climate communication.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7541.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7541.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art instruction-following large language model used both to generate candidate answers to climate questions and as the backbone of the auxiliary retrieval/AI-assistance pipeline (keypoint extraction, Wikipedia URL selection, paragraph ranking, and dimension-wise critique generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Large Language Models on Climate Information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned / chat model (RLHF), also used as retrieval- and assistance-model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science / climate communication (Earth science, public science communication)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of a climate expert answering diverse climate-related user questions: generate concise (3-4 sentence) expert answers that convey scientific information and communicate uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Simple zero-shot instruction: "You are an expert on climate change communication. Answer each question in a 3-4 sentence paragraph." Additionally used in auxiliary pipeline to extract keypoints, propose supporting Wikipedia URLs, rank evidence paragraphs, and to generate dimension-aware critiques; a follow-up 'dimension-aware' prompt (explicitly summarizing evaluation criteria) was also tested for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human rater Likert mean scores (1–5) across presentational dimensions: style, clarity, correctness, tone; and epistemological dimensions: accuracy, specificity, completeness, uncertainty (means with 95% CIs). Also Attribution-to-Identified-Source (AIS) measures: % keypoints supported and % answers fully/partially supported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Accuracy (epistemological dimension) mean = 3.67 [3.61, 3.73] (Table 4). In AIS attribution experiment: 87.7% of questions yielded a valid Wikipedia article; 66.79% of keypoints fully/partially supported; 46.08% of answers fully/partially supported.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to lowest-performing model InstructGPT (text-davinci-002) accuracy = 2.81 [2.73,2.88]; also compared across rater-assistance conditions where 'GPT4, no assistance' recorded higher apparent scores (see Table 4: GPT4, no assistance accuracy = 4.49 [4.44,4.55]) reflecting rater-condition effects rather than model raw quality.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompting strategy (dimension-aware prompts improved epistemological scores)', 'Use of AI Assistance for human raters (affects issue discovery and measured scores)', 'Retrieval / grounding (evidence from Wikipedia used for AI Assistance and AIS)', 'Answer length constraint (3-4 sentence limit) and pragmatic style', 'Model training and RLHF (affects presentational fluency)', 'Question source had little influence (GTrends, SkepticalScience, synthetic Wikipedia)', 'Trade-off between presentational and epistemological quality']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Answers generated with single simple prompt, 3-4 sentence target; dataset of 300 questions (100 each from Google Trends, Skeptical Science, GPT-4-synthesized from Wikipedia); three human raters per answer; auxiliary data (keypoints, evidence paragraphs, critiques) produced using GPT-4; experiments run Sep–Nov 2023; follow-up GPT-4 dimension-aware prompt evaluated in Nov 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower epistemological performance compared to presentational; hallucinations and incorrect or out-of-context science; incompleteness, low specificity, and missing uncertainty statements; rater scores sensitive to AI Assistance exposure (no-assistance vs assistance conditions yield large differences), so some reported numbers conflate model output quality and evaluation protocol; AIS attribution can show full attribution while epistemological scores remain low (missing information not captured by attribution).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7541.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7541.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5 Chat, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-following conversational LLM (GPT-3.5 family) evaluated as a generator of short expert-style answers to climate questions and rated on presentational and epistemological dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Large Language Models on Climate Information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned / chat model (RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science / climate communication</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate concise (3-4 sentence) expert answers to climate-related user questions; simulate a knowledgeable communicator addressing public information needs.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot instruction prompt: 'You are an expert on climate change communication. Answer each question in a 3-4 sentence paragraph.' No few-shot examples; standard chat/system framing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human rater Likert mean scores (1–5) on presentational (style, clarity, correctness, tone) and epistemological (accuracy, specificity, completeness, uncertainty) dimensions; per-issue percentage breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Accuracy mean = 3.48 [3.41, 3.55] (Table 4). Presentational means: style 4.54 [4.50,4.58], clarity 4.56 [4.52,4.60], correctness 4.58 [4.54,4.61], tone 3.06 [2.99,3.13].</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to worst performing InstructGPT (text-davinci-002) accuracy = 2.81 [2.73,2.88]; ChatGPT ranks lower on epistemological dimensions than some larger models (e.g., Falcon-180B-Chat, GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['High presentational fluency due to instruction tuning/RLHF', 'Short-answer length constraints limiting completeness/specificity', 'Pragmatic style: tendency to include generic cautionary statements reducing space for specific evidence', 'AI Assistance exposure of raters affects detection of issues']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same shared experimental setup: 300-question dataset, single-shot simple prompt specifying 3-4 sentence answer length; three human raters per answer; auxiliary evidence pipeline used GPT-4 (for some conditions); data collected Sep–Oct 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Relatively high surface-level quality but lower epistemological adequacy (notably low specificity, completeness, and uncertainty). Tends to produce vague or consensus-based phrasing rather than citing concrete evidence; susceptible to hallucinations and out-of-context science.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7541.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7541.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGPT (turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGPT (turbo, OpenAI API variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned OpenAI model variant (turbo) included among evaluated systems to compare presentational and epistemological performance on climate QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Large Language Models on Climate Information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned / API model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science / climate communication</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Answer climate-related questions in short (3-4 sentence) expert-style paragraphs, used as a text-based simulator of climate communication.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot simple instruction prompt requesting expert answers of 3-4 sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human rater Likert mean scores across presentational and epistemological dimensions (1–5), reported with 95% confidence intervals; issue frequency breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Accuracy mean = 3.65 [3.58,3.73] (Table 4). Presentational: style 4.37 [4.32,4.42], clarity 4.40 [4.36,4.45], correctness 4.46 [4.42,4.51], tone 3.41 [3.33,3.48].</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to worst model InstructGPT (text-davinci-002) accuracy = 2.81 [2.73,2.88]; and compared to top open model Falcon-180B-Chat accuracy = 3.81 [3.74,3.87].</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Instruction fine-tuning and RLHF components affecting presentation', 'Answer length constraint and pragmatic filler content reducing epistemological depth', 'AI Assistance exposure modulates issue detection by raters']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Generated answers with the single simple expert prompt, judged by three human raters per answer; auxiliary retrieval/assistance pipeline uses GPT-4 for evidence and critiques; data collected Sep–Oct 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower specificity and completeness in short answers; issues include anecdotal or incorrect claims; pragmatic style can occupy space that would otherwise be used for concrete evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7541.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7541.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGPT (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGPT (text-davinci-003, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned GPT-3 family model (text-davinci-003) evaluated as another baseline for climate QA; shows intermediate presentational and epistemological performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Large Language Models on Climate Information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science / climate communication</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate concise expert answers (3-4 sentences) to climate-related questions for evaluation of content adequacy and presentation.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot instruction prompt (expert persona, 3-4 sentence answer).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human rater Likert mean scores (1–5) across presentational and epistemological dimensions with 95% CIs; issue frequency statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Accuracy mean = 3.52 [3.44,3.60] (Table 4). Presentational: style 4.15 [4.08,4.22], clarity 4.43 [4.38,4.47], correctness 4.47 [4.42,4.52], tone 3.20 [3.12,3.28].</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to worst model InstructGPT (text-davinci-002) accuracy = 2.81 [2.73,2.88]; bettered by GPT-4 and Falcon-180B-Chat on some epistemological metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Instruction tuning improves surface fluency', 'Space constraints and pragmatic filler reduce completeness', 'AI Assistance for raters increases detection of issues']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Answers generated with uniform simple prompt; three human raters per answer; auxiliary evidence and AI Assistance produced with GPT-4; evaluations Sep–Oct 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Notably low specificity and completeness; common errors include anecdotal claims, incorrect or out-of-context science and missing uncertainty statements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7541.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7541.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGPT (text-davinci-002, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An older instruction-tuned GPT-3 variant included as a lower-performing baseline in the experiments, showing the weakest presentational and epistemological scores among evaluated systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Large Language Models on Climate Information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science / climate communication</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate brief expert-style answers to climate questions; serves as a baseline reference for model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot instruction identical to other models: expert persona and 3-4 sentence answer constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human rater Likert mean scores across presentational and epistemological dimensions (1–5) with 95% CIs; issue frequency counts.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Accuracy mean = 2.81 [2.73,2.88] (Table 4) — lowest among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>This system is itself used as the low-performance baseline in pairwise model comparisons; presentational style and clarity are also lowest (style 3.22 [3.13,3.31], clarity 3.63 [3.55,3.70]).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Older instruction-tuning and model capabilities leading to lower fluency and factuality', 'Same answer-length constraints and prompting as other models', 'Rater detection of issues strongly improved by AI Assistance']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same prompt and dataset as other models; judged by three human raters; auxiliary GPT-4 assistance used in some rating conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Poor presentational and epistemological performance relative to newer models; frequent incorrect or anecdotal statements, vagueness and missing uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7541.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7541.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-2 (text-bison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 (text-bison API variant, Google/Alphabet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-tuned model (PaLM 2 API variant 'text-bison') included among evaluated models producing short climate QA answers, showing strong presentational quality but moderate epistemological scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Large Language Models on Climate Information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 (text-bison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned / API model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science / climate communication</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Produce concise (3-4 sentence) expert-style answers to climate questions to simulate public-facing climate communication.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot simple instruction prompt requesting expert answers of 3-4 sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human rater Likert means across presentational and epistemological dimensions (1–5) with 95% CIs; issue frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Accuracy mean = 3.47 [3.39,3.55] (Table 4). Presentational scores high: correctness 4.57 [4.53,4.61], clarity 4.48 [4.43,4.53], style 4.34 [4.28,4.40].</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to InstructGPT (text-davinci-002) accuracy = 2.81 [2.73,2.88]; Falcon-180B-Chat had higher accuracy = 3.81 [3.74,3.87].</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Instruction tuning improving surface fluency', 'Answer length and pragmatic filler affecting epistemological depth', 'AI Assistance exposure influences issue detection']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same uniform prompt and dataset; judged by three human raters per answer; auxiliary evidence pipeline used GPT-4 for assistance when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Moderate epistemological inadequacy: low specificity, completeness, and uncertainty reporting in short responses; potential for out-of-context or vague claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7541.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7541.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-180B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-180B-Chat (TII)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large open-weight chat model (180B parameters) included in the evaluation and observed to be the strongest on several epistemological dimensions among evaluated models, possibly related to its size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Large Language Models on Climate Information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-180B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>180B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>large open chat model / instruction-tuned (chat) variant</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science / climate communication</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate 3-4 sentence expert-style answers to simulate climate information communication for public questions.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot instruction prompt identical to other models: expert persona and 3-4 sentence length constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human rater Likert mean scores across presentational and epistemological dimensions (1–5) with 95% CIs; issue frequency breakdowns; pairwise t-tests vs other models.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Accuracy mean = 3.81 [3.74,3.87] (Table 4) — the best on epistemological dimensions at time of study among evaluated systems.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared against other models (e.g., ChatGPT accuracy 3.48 [3.41,3.55]; InstructGPT davinci-002 = 2.81 [2.73,2.88]).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model size (180B) speculated as contributing to stronger epistemological performance', 'Instruction/chat tuning affecting both presentation and content', 'Same answer-length and prompt constraints as other models', 'AI Assistance exposure affects human evaluation outcomes']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Answers produced under uniform prompt and length constraint; judged by three human raters; auxiliary retrieval/assistance pipeline (for some conditions) implemented using GPT-4; data collection Sep–Oct 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Despite leading epistemological scores, still shows substantial room for improvement: low completeness, specificity and uncertainty reporting exist; subject to the same pragmatic filler tendencies and occasional incorrect or out-of-context claims as other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Large Language Models on Climate Information', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented language model pretraining <em>(Rating: 2)</em></li>
                <li>Teaching language models to support answers with verified quotes <em>(Rating: 2)</em></li>
                <li>Evaluating verifiability in generative search engines <em>(Rating: 2)</em></li>
                <li>Large language models encode clinical knowledge <em>(Rating: 1)</em></li>
                <li>Measuring attribution in natural language generation models <em>(Rating: 2)</em></li>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7541",
    "paper_id": "paper-aec95e6330033e0ec39fb5a069d647288c03b945",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A state-of-the-art instruction-following large language model used both to generate candidate answers to climate questions and as the backbone of the auxiliary retrieval/AI-assistance pipeline (keypoint extraction, Wikipedia URL selection, paragraph ranking, and dimension-wise critique generation).",
            "citation_title": "Assessing Large Language Models on Climate Information",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_type": "instruction-tuned / chat model (RLHF), also used as retrieval- and assistance-model",
            "scientific_domain": "Climate science / climate communication (Earth science, public science communication)",
            "simulation_task_description": "Text-based simulation of a climate expert answering diverse climate-related user questions: generate concise (3-4 sentence) expert answers that convey scientific information and communicate uncertainty.",
            "prompting_strategy": "Simple zero-shot instruction: \"You are an expert on climate change communication. Answer each question in a 3-4 sentence paragraph.\" Additionally used in auxiliary pipeline to extract keypoints, propose supporting Wikipedia URLs, rank evidence paragraphs, and to generate dimension-aware critiques; a follow-up 'dimension-aware' prompt (explicitly summarizing evaluation criteria) was also tested for GPT-4.",
            "evaluation_metric": "Human rater Likert mean scores (1–5) across presentational dimensions: style, clarity, correctness, tone; and epistemological dimensions: accuracy, specificity, completeness, uncertainty (means with 95% CIs). Also Attribution-to-Identified-Source (AIS) measures: % keypoints supported and % answers fully/partially supported.",
            "reported_accuracy": "Accuracy (epistemological dimension) mean = 3.67 [3.61, 3.73] (Table 4). In AIS attribution experiment: 87.7% of questions yielded a valid Wikipedia article; 66.79% of keypoints fully/partially supported; 46.08% of answers fully/partially supported.",
            "baseline_accuracy": "Compared to lowest-performing model InstructGPT (text-davinci-002) accuracy = 2.81 [2.73,2.88]; also compared across rater-assistance conditions where 'GPT4, no assistance' recorded higher apparent scores (see Table 4: GPT4, no assistance accuracy = 4.49 [4.44,4.55]) reflecting rater-condition effects rather than model raw quality.",
            "factors_reported": [
                "Prompting strategy (dimension-aware prompts improved epistemological scores)",
                "Use of AI Assistance for human raters (affects issue discovery and measured scores)",
                "Retrieval / grounding (evidence from Wikipedia used for AI Assistance and AIS)",
                "Answer length constraint (3-4 sentence limit) and pragmatic style",
                "Model training and RLHF (affects presentational fluency)",
                "Question source had little influence (GTrends, SkepticalScience, synthetic Wikipedia)",
                "Trade-off between presentational and epistemological quality"
            ],
            "experimental_conditions": "Answers generated with single simple prompt, 3-4 sentence target; dataset of 300 questions (100 each from Google Trends, Skeptical Science, GPT-4-synthesized from Wikipedia); three human raters per answer; auxiliary data (keypoints, evidence paragraphs, critiques) produced using GPT-4; experiments run Sep–Nov 2023; follow-up GPT-4 dimension-aware prompt evaluated in Nov 2023.",
            "limitations_or_failure_modes": "Lower epistemological performance compared to presentational; hallucinations and incorrect or out-of-context science; incompleteness, low specificity, and missing uncertainty statements; rater scores sensitive to AI Assistance exposure (no-assistance vs assistance conditions yield large differences), so some reported numbers conflate model output quality and evaluation protocol; AIS attribution can show full attribution while epistemological scores remain low (missing information not captured by attribution).",
            "uuid": "e7541.0",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT-3.5",
            "name_full": "ChatGPT (GPT-3.5 Chat, OpenAI)",
            "brief_description": "An instruction-following conversational LLM (GPT-3.5 family) evaluated as a generator of short expert-style answers to climate questions and rated on presentational and epistemological dimensions.",
            "citation_title": "Assessing Large Language Models on Climate Information",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5)",
            "model_size": null,
            "model_type": "instruction-tuned / chat model (RLHF)",
            "scientific_domain": "Climate science / climate communication",
            "simulation_task_description": "Generate concise (3-4 sentence) expert answers to climate-related user questions; simulate a knowledgeable communicator addressing public information needs.",
            "prompting_strategy": "Zero-shot instruction prompt: 'You are an expert on climate change communication. Answer each question in a 3-4 sentence paragraph.' No few-shot examples; standard chat/system framing.",
            "evaluation_metric": "Human rater Likert mean scores (1–5) on presentational (style, clarity, correctness, tone) and epistemological (accuracy, specificity, completeness, uncertainty) dimensions; per-issue percentage breakdowns.",
            "reported_accuracy": "Accuracy mean = 3.48 [3.41, 3.55] (Table 4). Presentational means: style 4.54 [4.50,4.58], clarity 4.56 [4.52,4.60], correctness 4.58 [4.54,4.61], tone 3.06 [2.99,3.13].",
            "baseline_accuracy": "Compared to worst performing InstructGPT (text-davinci-002) accuracy = 2.81 [2.73,2.88]; ChatGPT ranks lower on epistemological dimensions than some larger models (e.g., Falcon-180B-Chat, GPT-4).",
            "factors_reported": [
                "High presentational fluency due to instruction tuning/RLHF",
                "Short-answer length constraints limiting completeness/specificity",
                "Pragmatic style: tendency to include generic cautionary statements reducing space for specific evidence",
                "AI Assistance exposure of raters affects detection of issues"
            ],
            "experimental_conditions": "Same shared experimental setup: 300-question dataset, single-shot simple prompt specifying 3-4 sentence answer length; three human raters per answer; auxiliary evidence pipeline used GPT-4 (for some conditions); data collected Sep–Oct 2023.",
            "limitations_or_failure_modes": "Relatively high surface-level quality but lower epistemological adequacy (notably low specificity, completeness, and uncertainty). Tends to produce vague or consensus-based phrasing rather than citing concrete evidence; susceptible to hallucinations and out-of-context science.",
            "uuid": "e7541.1",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "InstructGPT (turbo)",
            "name_full": "InstructGPT (turbo, OpenAI API variant)",
            "brief_description": "An instruction-tuned OpenAI model variant (turbo) included among evaluated systems to compare presentational and epistemological performance on climate QA.",
            "citation_title": "Assessing Large Language Models on Climate Information",
            "mention_or_use": "use",
            "model_name": "InstructGPT (turbo)",
            "model_size": null,
            "model_type": "instruction-tuned / API model",
            "scientific_domain": "Climate science / climate communication",
            "simulation_task_description": "Answer climate-related questions in short (3-4 sentence) expert-style paragraphs, used as a text-based simulator of climate communication.",
            "prompting_strategy": "Zero-shot simple instruction prompt requesting expert answers of 3-4 sentences.",
            "evaluation_metric": "Human rater Likert mean scores across presentational and epistemological dimensions (1–5), reported with 95% confidence intervals; issue frequency breakdowns.",
            "reported_accuracy": "Accuracy mean = 3.65 [3.58,3.73] (Table 4). Presentational: style 4.37 [4.32,4.42], clarity 4.40 [4.36,4.45], correctness 4.46 [4.42,4.51], tone 3.41 [3.33,3.48].",
            "baseline_accuracy": "Compared to worst model InstructGPT (text-davinci-002) accuracy = 2.81 [2.73,2.88]; and compared to top open model Falcon-180B-Chat accuracy = 3.81 [3.74,3.87].",
            "factors_reported": [
                "Instruction fine-tuning and RLHF components affecting presentation",
                "Answer length constraint and pragmatic filler content reducing epistemological depth",
                "AI Assistance exposure modulates issue detection by raters"
            ],
            "experimental_conditions": "Generated answers with the single simple expert prompt, judged by three human raters per answer; auxiliary retrieval/assistance pipeline uses GPT-4 for evidence and critiques; data collected Sep–Oct 2023.",
            "limitations_or_failure_modes": "Lower specificity and completeness in short answers; issues include anecdotal or incorrect claims; pragmatic style can occupy space that would otherwise be used for concrete evidence.",
            "uuid": "e7541.2",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "InstructGPT (text-davinci-003)",
            "name_full": "InstructGPT (text-davinci-003, OpenAI)",
            "brief_description": "An instruction-tuned GPT-3 family model (text-davinci-003) evaluated as another baseline for climate QA; shows intermediate presentational and epistemological performance.",
            "citation_title": "Assessing Large Language Models on Climate Information",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-003)",
            "model_size": null,
            "model_type": "instruction-tuned",
            "scientific_domain": "Climate science / climate communication",
            "simulation_task_description": "Generate concise expert answers (3-4 sentences) to climate-related questions for evaluation of content adequacy and presentation.",
            "prompting_strategy": "Zero-shot instruction prompt (expert persona, 3-4 sentence answer).",
            "evaluation_metric": "Human rater Likert mean scores (1–5) across presentational and epistemological dimensions with 95% CIs; issue frequency statistics.",
            "reported_accuracy": "Accuracy mean = 3.52 [3.44,3.60] (Table 4). Presentational: style 4.15 [4.08,4.22], clarity 4.43 [4.38,4.47], correctness 4.47 [4.42,4.52], tone 3.20 [3.12,3.28].",
            "baseline_accuracy": "Compared to worst model InstructGPT (text-davinci-002) accuracy = 2.81 [2.73,2.88]; bettered by GPT-4 and Falcon-180B-Chat on some epistemological metrics.",
            "factors_reported": [
                "Instruction tuning improves surface fluency",
                "Space constraints and pragmatic filler reduce completeness",
                "AI Assistance for raters increases detection of issues"
            ],
            "experimental_conditions": "Answers generated with uniform simple prompt; three human raters per answer; auxiliary evidence and AI Assistance produced with GPT-4; evaluations Sep–Oct 2023.",
            "limitations_or_failure_modes": "Notably low specificity and completeness; common errors include anecdotal claims, incorrect or out-of-context science and missing uncertainty statements.",
            "uuid": "e7541.3",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "InstructGPT (text-davinci-002)",
            "name_full": "InstructGPT (text-davinci-002, OpenAI)",
            "brief_description": "An older instruction-tuned GPT-3 variant included as a lower-performing baseline in the experiments, showing the weakest presentational and epistemological scores among evaluated systems.",
            "citation_title": "Assessing Large Language Models on Climate Information",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_size": null,
            "model_type": "instruction-tuned",
            "scientific_domain": "Climate science / climate communication",
            "simulation_task_description": "Generate brief expert-style answers to climate questions; serves as a baseline reference for model comparisons.",
            "prompting_strategy": "Zero-shot instruction identical to other models: expert persona and 3-4 sentence answer constraint.",
            "evaluation_metric": "Human rater Likert mean scores across presentational and epistemological dimensions (1–5) with 95% CIs; issue frequency counts.",
            "reported_accuracy": "Accuracy mean = 2.81 [2.73,2.88] (Table 4) — lowest among evaluated models.",
            "baseline_accuracy": "This system is itself used as the low-performance baseline in pairwise model comparisons; presentational style and clarity are also lowest (style 3.22 [3.13,3.31], clarity 3.63 [3.55,3.70]).",
            "factors_reported": [
                "Older instruction-tuning and model capabilities leading to lower fluency and factuality",
                "Same answer-length constraints and prompting as other models",
                "Rater detection of issues strongly improved by AI Assistance"
            ],
            "experimental_conditions": "Same prompt and dataset as other models; judged by three human raters; auxiliary GPT-4 assistance used in some rating conditions.",
            "limitations_or_failure_modes": "Poor presentational and epistemological performance relative to newer models; frequent incorrect or anecdotal statements, vagueness and missing uncertainty.",
            "uuid": "e7541.4",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM-2 (text-bison)",
            "name_full": "PaLM 2 (text-bison API variant, Google/Alphabet)",
            "brief_description": "A large instruction-tuned model (PaLM 2 API variant 'text-bison') included among evaluated models producing short climate QA answers, showing strong presentational quality but moderate epistemological scores.",
            "citation_title": "Assessing Large Language Models on Climate Information",
            "mention_or_use": "use",
            "model_name": "PaLM-2 (text-bison)",
            "model_size": null,
            "model_type": "instruction-tuned / API model",
            "scientific_domain": "Climate science / climate communication",
            "simulation_task_description": "Produce concise (3-4 sentence) expert-style answers to climate questions to simulate public-facing climate communication.",
            "prompting_strategy": "Zero-shot simple instruction prompt requesting expert answers of 3-4 sentences.",
            "evaluation_metric": "Human rater Likert means across presentational and epistemological dimensions (1–5) with 95% CIs; issue frequencies.",
            "reported_accuracy": "Accuracy mean = 3.47 [3.39,3.55] (Table 4). Presentational scores high: correctness 4.57 [4.53,4.61], clarity 4.48 [4.43,4.53], style 4.34 [4.28,4.40].",
            "baseline_accuracy": "Compared to InstructGPT (text-davinci-002) accuracy = 2.81 [2.73,2.88]; Falcon-180B-Chat had higher accuracy = 3.81 [3.74,3.87].",
            "factors_reported": [
                "Instruction tuning improving surface fluency",
                "Answer length and pragmatic filler affecting epistemological depth",
                "AI Assistance exposure influences issue detection"
            ],
            "experimental_conditions": "Same uniform prompt and dataset; judged by three human raters per answer; auxiliary evidence pipeline used GPT-4 for assistance when applicable.",
            "limitations_or_failure_modes": "Moderate epistemological inadequacy: low specificity, completeness, and uncertainty reporting in short responses; potential for out-of-context or vague claims.",
            "uuid": "e7541.5",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-180B-Chat",
            "name_full": "Falcon-180B-Chat (TII)",
            "brief_description": "A very large open-weight chat model (180B parameters) included in the evaluation and observed to be the strongest on several epistemological dimensions among evaluated models, possibly related to its size.",
            "citation_title": "Assessing Large Language Models on Climate Information",
            "mention_or_use": "use",
            "model_name": "Falcon-180B-Chat",
            "model_size": "180B",
            "model_type": "large open chat model / instruction-tuned (chat) variant",
            "scientific_domain": "Climate science / climate communication",
            "simulation_task_description": "Generate 3-4 sentence expert-style answers to simulate climate information communication for public questions.",
            "prompting_strategy": "Zero-shot instruction prompt identical to other models: expert persona and 3-4 sentence length constraint.",
            "evaluation_metric": "Human rater Likert mean scores across presentational and epistemological dimensions (1–5) with 95% CIs; issue frequency breakdowns; pairwise t-tests vs other models.",
            "reported_accuracy": "Accuracy mean = 3.81 [3.74,3.87] (Table 4) — the best on epistemological dimensions at time of study among evaluated systems.",
            "baseline_accuracy": "Compared against other models (e.g., ChatGPT accuracy 3.48 [3.41,3.55]; InstructGPT davinci-002 = 2.81 [2.73,2.88]).",
            "factors_reported": [
                "Model size (180B) speculated as contributing to stronger epistemological performance",
                "Instruction/chat tuning affecting both presentation and content",
                "Same answer-length and prompt constraints as other models",
                "AI Assistance exposure affects human evaluation outcomes"
            ],
            "experimental_conditions": "Answers produced under uniform prompt and length constraint; judged by three human raters; auxiliary retrieval/assistance pipeline (for some conditions) implemented using GPT-4; data collection Sep–Oct 2023.",
            "limitations_or_failure_modes": "Despite leading epistemological scores, still shows substantial room for improvement: low completeness, specificity and uncertainty reporting exist; subject to the same pragmatic filler tendencies and occasional incorrect or out-of-context claims as other LLMs.",
            "uuid": "e7541.6",
            "source_info": {
                "paper_title": "Assessing Large Language Models on Climate Information",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented language model pretraining",
            "rating": 2
        },
        {
            "paper_title": "Teaching language models to support answers with verified quotes",
            "rating": 2
        },
        {
            "paper_title": "Evaluating verifiability in generative search engines",
            "rating": 2
        },
        {
            "paper_title": "Large language models encode clinical knowledge",
            "rating": 1
        },
        {
            "paper_title": "Measuring attribution in natural language generation models",
            "rating": 2
        },
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 2
        }
    ],
    "cost": 0.018837,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Assessing Large Language Models on Climate Information</h1>
<p>Jannis Bulian<em> ${ }^{</em> 1}$ Mike S. Schäfer<em>2 Afra Amini ${ }^{</em> 13}$ Heidi Lam ${ }^{<em> 4}$ Massimiliano Ciaramita ${ }^{</em> 1}$ Ben Gaiarin ${ }^{<em> 1}$ Michelle Chen Hübscher</em> ${ }^{<em> 1}$ Christian Buck ${ }^{</em> 1}$ Niels G. Mede ${ }^{<em> 2}$ Markus Leippold ${ }^{</em> 15}$ Nadine Strauß ${ }^{* 2}$</p>
<h4>Abstract</h4>
<p>As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in critically relevant domains. We present a comprehensive evaluation framework, grounded in science communication research, to assess LLM responses to questions about climate change. Our framework emphasizes both presentational and epistemological adequacy, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues. Our evaluation task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel protocol for scalable oversight that relies on AI Assistance and raters with relevant education. We evaluate several recent LLMs on a set of diverse climate questions. Our results point to a significant gap between surface and epistemological qualities of LLMs in the realm of climate communication.</p>
<h2>1. Introduction</h2>
<p>As concerns around climate change intensify (Poushter et al., 2022; WHO, 2021), more and more people turn to digital media as their primary source of information (Newman et al., 2021). However, in spite of ubiquitous access to information, there remains a considerable gap in climate literacy, exacerbated by the spread of mis- and disinformation (Leiserowitz et al., 2022). The challenge of conveying climate data arises also from the nature of scientific communication: science, as an evolving domain, is laden with specialized knowledge,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>complexity, and inherent uncertainties (Moser, 2016). The digital media landscape, characterized by soaring amounts of AI-generated content (Thompson et al., 2024), limited attention spans and adversarial dynamics, further compounds these challenges (Pearce et al., 2019).</p>
<p>While AI's promise in addressing global climate challenges is evident through its applications in climate modeling, energy optimization, and disaster management (Rolnick et al., 2022), its intersection with Natural Language Processing (NLP) is still under-explored. Given recent advancements in LLMs (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023; Gemini Team, 2023) there is hope that generative AI will also help addressing climate information challenges. However, using LLMs to address science-related information raises factuality concerns (Weidinger et al., 2021). Eloquence and advanced dialogue behaviors are trusted by users, even in the absence of trustworthy information (Chiesurin et al., 2023). This makes evaluating LLMs difficult. Research on evaluating systems that may achieve or exceed human abilities, or scalable oversight (Amodei et al., 2016) is so far mostly theoretical (Irving et al., 2018; Leike et al., 2018; Christiano et al., 2018), with some recent more practical advances (Michael et al., 2023).</p>
<p>We introduce a framework based on Science Communication research (Jamieson et al., 2017), to begin evaluating LLMs' responses within the climate change context in a principled way. ${ }^{1}$ The evaluation relies on raters with relevant educational background. We assess presentational properties such as style, clarity, linguistic correctness, and tone. More importantly, we also assess epistemological issues: accuracy, specificity, completeness, and uncertainty. To test the relevance of the evaluation, we run an empirical study on a diverse set of 300 climate change-related questions involving some of the most recent and prominent LLMs.</p>
<p>Our main findings are as follows:</p>
<ul>
<li>To increase the recall of detected issues and improve rating quality, it is crucial to introduce scalable oversight protocols that use grounded AI Assistance (cf. Figure 1). However, while AI assistance demonstrably</li>
</ul>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of the evaluation pipeline as described in Section 3. Starting with a question-answer pair, we use an LLM to extract key points from the answer. We also use the LLM to find a relevant Wikipedia page from which we extract paragraphs. For each key point we rank the paragraphs and keep the top ones. We combine all this information to generate AI assistance for each of our evaluation dimensions. Presentational dimensions are evaluated without the additional paragraphs. This assistance, if available, is presented to our human raters along with the answer. Note that the raters for presentational and epistemological dimension are not shown the key points or retrieved paragraphs. We however use the key points and paragraphs evaluate attribution, cf. Appendix A.12.
improves rating quality, its influence on raters extends beyond this enhancement. Understanding and mitigating these broader effects remains an open question for future research.</p>
<ul>
<li>Results suggest that the epistemological quality of responses on climate information of current LLMs is substantially lower than the presentational quality.</li>
<li>We find preliminary evidence that summarizing the evaluation dimensions in the prompt can improve performance on the epistemological dimensions.</li>
<li>We notice trade-offs between dimensions. Most notably, there seems to be a trade-off between epistemological and presentational quality.</li>
<li>We analyze the relation of our evaluation and attribution-based evaluations of LLMs (Rashkin et al., 2022) and find that they emerge as mostly orthogonal and complementary.</li>
</ul>
<h2>2. Evaluative Dimensions</h2>
<p>Scholarship on science communication - originating from disciplines such as communication science, sociology, psychology, human geography, and education, among others (Trench \&amp; Bucchi, 2021; Nisbet et al., 2018; Jamieson et al., 2017) - offers conceptual arguments and empirical evidence for appropriately disseminating scientific information, e.g., on climate change, to the general public (König et al., 2023;</p>
<p>Lewis Jr. \&amp; Wai, 2021). Building on this knowledge, we distinguish between two basic dimensions. (1) Presentational features of the message that address its comprehensibility (Lang, 2000). (2) Epistemological features aiming to capture the degree to which the conveyed information represents current scientific knowledge adequately and comprehensively, while being specific and appropriately communicating associated uncertainties (Fähnrich et al., 2023).</p>
<h3>2.1. Presentational Adequacy</h3>
<p>An adequate presentation should comply with three criteria (Jamieson et al., 2017): (1) be comprehensible, (2) aid understanding through layout and visualizations, and (3) use appropriate sources and references. Here we focus primarily on comprehensibility, evaluated along four criteria: style, clarity, linguistic correctness, and tone.</p>
<p>Style. The language should not be too informal or colloquial (Mazer \&amp; Hunt, 2008), as this can undermine the credibility of information (Scharrer et al., 2012). Answers should not be too short or too long: brief snippets of information can lead to a "feeling of knowing" (Leonhard et al., 2020), while long texts require motivation and cognitive resources that readers may not want to invest (Lang, 2000). In addition, we borrow some stylistic dimensions from the Multidimensional Quality Metrics (MQM) framework for the evaluation of translations (Lommel et al., 2013).</p>
<p>Clarity. Responses should be concise and clearly formulated (Maibach et al., 2023). The use of jargon and diffi-</p>
<p>cult technical content should be avoided (Baram-Tsabari \&amp; Lewenstein, 2013; Baram-Tsabari et al., 2020).
(Linguistic) Correctness. As in MQM, messages should adhere to linguistic conventions, i.e., the correct use of punctuation, spelling, and grammar. ${ }^{2}$ Violations can damage perceived credibility (Berger, 2020; Mollick, 2014)</p>
<p>Tone. The tone of a message concerns its perceived neutrality, its persuasiveness and its positivity or negativity. Science communication, especially climate-related, can be more effective if it doesn't lean towards a certain valence, worldview, or ideological conviction (Blanton \&amp; Ikizer, 2019; Yuan \&amp; Lu, 2020; Kerr et al., 2022; Munoz-Carrier et al., 2020). Likewise, messages should not use too positively or negatively valenced language, particularly if the goal is to convey factual information (Palm et al., 2020).</p>
<h3>2.2. Epistemological Adequacy</h3>
<p>The epistemological adequacy of climate-related messages is of greatest importance. This entails several aspects: (1) accuracy, (2) specificity, (3) completeness, (4) the degree of (un)certainty, and (5) the presentation of methods and methodology. Here we focus on the first four.</p>
<p>Accuracy. Scientific information should be accurate (Kelesidou \&amp; Chabrol, 2021). This is crucial, considering known issues of LLMs such as hallucination (Schäfer, 2023; Ji et al., 2023). We identify issues that deal with incorrect, wrong, or self-contradictory information, as well messages that take scientific findings, or anecdotal evidence, out of context (Hinnant et al., 2016).</p>
<p>Specificity. Information that is relevant to the audience should not be missed, while ignoring irrelevant information. Responses should address the spatial and temporal context; as specific, local information leads to higher perceived relevance (Lee et al., 2015; Leiserowitz \&amp; Smith, 2017; Holmes et al., 2020). In the absence of a specific time frame, the answer should generally be based on up-to-date knowledge.</p>
<p>Completeness. Rather than only referring to a part of the question posed, answers should be formulated in a way that addresses all aspects of the question in full (Leiserowitz \&amp; Smith, 2017; Bergquist et al., 2022). At the same time, the information given should reflect the depth and breadth of relevant scientific knowledge available regarding the topic(s) addressed (Kelesidou \&amp; Chabrol, 2021).</p>
<p>Uncertainty. Communicating the level of agreement and confidence regarding scientific findings, and supporting evidence, can be crucial to adequately informing the audience (Howe et al., 2019; Budescu et al., 2012; Keohane et al., 2014). This is particularly important in climate communication (Maertens et al., 2020; Chinn \&amp; Hart, 2021; Goldberg</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2022), scientific consensus on climate change has been found to function as a "gateway belief" and motivate public action (van der Linden et al., 2015).</p>
<h3>2.3. Aggregation of scores across dimensions</h3>
<p>In this paper we don't address the important question of how individual dimensions should be combined in a single metric, e.g., for model selection and benchmarking. This is a complex topic which requires assigning a value to each individual dimension. We also believe that the combination of these scores will vary by application.</p>
<h2>3. Human Evaluation Study</h2>
<p>We test our evaluative dimensions in a human rating study. The rating task involves evaluating an answer based on the presentational (Section 2.1) and epistemological dimensions (Section 2.2). Screenshots of the template can be found in Appendix A.10. We select candidate raters with relevant educational background (see Appendix A.7). To be admitted, after finishing a brief tutorial, the raters need to pass an admission test (see Appendix A.9). A summary of the broad demographics of participants can be found in Appendix A.7. Each answer is assessed by three human raters. We don't discourage brief consultations of external sources to clarify specific points but advise against extensive research.</p>
<h3>3.1. Question and Answer Data</h3>
<h3>3.1.1. QUESTIONS</h3>
<p>A comprehensive evaluation would ideally cover a broad spectrum of information needs, including the basics of climate science, mitigation and adaptation, as well as contextspecific issues; e.g., to address the concerns of vulnerable or under-resourced communities (Amini et al., 2023). However, no standardized tests exist to assess climate-related knowledge; in contrast to e.g. the medical domain (Singhal et al., 2023). Hence, we begin by creating a diverse set of 300 questions about topics that are either popular among search users, controversial or context-specific.</p>
<p>We collect questions from three different sources. For the first set, we use Google Trends, which provides data on public interest in specific search topics. ${ }^{3}$ We collect the most popular questions, by search volume, from the U.S., for the topics 'Climate Change' and 'Global Warming' for 2020-2022. For the second set, we turn to Skeptical Science, a website that publishes authoritative information about climate science. We take the list of debated myths ${ }^{4}$ and manually rephrase them as questions. Lastly, we use GPT-4 to</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>generate synthetic questions from the English Wikipedia. We manually select a list of articles related to climate change (e.g., "Global Dimming", "Polar Amplification"), or discuss the topic in specific locations (e.g., "Climate Change in [COUNTRY]"), for a total of 139 articles. Then we split the documents in paragraphs and ask GPT-4 to generate questions that can be answered by the paragraph. We apply several filters to assure that the Wikipedia questions are not overly dependent on the context and are therefore answerable only from the given paragraph. See Appendix A.3.1 for more details and a discussion of filtering choices.</p>
<p>We post-process all questions to remove duplicates, questions that are not related to climate change, or taken out of context. Finally, we sample 100 questions from each set.</p>
<h3>3.1.2. ANSWERS</h3>
<p>Generated answers can display a great deal of variation depending on prompt engineering, reasoning schemes, incontext learning, etc. However, a direct question is the most common way for users to get answers from LLMs. As such, a plain question provides a valuable baseline, reducing variance due to individual LLM's skills and optimization effort, and limiting confounding factors. To obtain answers we use a simple prompt consisting of the instruction: You are an expert on climate change communication. Answer each question in a 3-4 sentence paragraph. We include the answer length information to anchor the expected response to an objective value.</p>
<h3>3.2. Auxiliary Data</h3>
<p>We support raters with AI Assistance, consisting of a modelgenerated critique for each evaluated dimension. For epistemological dimensions the assistance is grounded in verbatim evidence from relevant passages extracted from Wikipedia articles. To produce all necessary auxiliary data we carefully design a simple, robust baseline system (Figure 1), which relies on a single LLM. For consistency, we always use GPT-4 for this purpose. Besides testing our evaluation we also run a comparison with an attribution-based evaluation (AIS) (Rashkin et al., 2022), on the same data.</p>
<p>Keypoints. To find supporting evidence for an answer, for AI Assistance and AIS evaluation (Section 4.6), we extract keypoints from each answer. To do so, we instruct GPT-4 to examine all the statements in the answer, and identify one to three key statements that are made in answering the question. We find this to provide better signal to retrieve evidence (see the next paragraph) than either using the whole answer or all sentences individually (Liu et al., 2023).</p>
<p>Evidence. For each keypoint we fetch evidence from Wikipedia. Given the question and the answer, we first ask GPT-4 to provide the URL of a Wikipedia article that
supports the answer. See Table 8 for the exact prompt. We limit evidence to Wikipedia because GPT-4 is fairly consistent in generating relevant, valid Wikipedia URLs, while the quality is lower for the unrestricted web. Furthermore, Wikipedia is uniform in style and quality as it adheres to established guidelines. ${ }^{5}$ While random web pages can vary significantly in content and presentation quality.</p>
<p>We break down the relevant article into its paragraphs. For each keypoint, we ask the model to score the paragraphs based on their relevance to the keypoint and the question. We pick the 3 highest scoring ones as evidence (cf. Table 18 for an example). We find that using keypoints, in combination with URL generation and evidence selection, is a simple and effective solution. In particular, we find this to work better than off-the-shelf sparse or dense retrieval (e.g., using BM25/GTR (Ni et al., 2022)) over Wikipedia passages.</p>
<p>AI Assistance. To assist human raters, we use GPT-4 to critique the answer along the dimensions introduced in Section 2. For each dimension, we ask the model to express its agreement or disagreement that the information is presented well according to that dimension. For epistemological dimensions, we also provide the retrieved evidence and instruct the model to quote the evidence verbatim to support its disagreement (if any).</p>
<p>Please refer to Table 8 for a complete list of prompts used to generate the data, and to Appendix A. 5 for some statistics of the generated answers.</p>
<h2>4. Experimental Results</h2>
<p>Here we present the findings from the experiments using Figure 2 as a summary. Full results tables, including confidence intervals, are reported in Table 3 and Table 4. We also report pairwise LLM t-tests in Tables 5 and 6. We compute pairwise distance and Krippendorff's alpha agreement metrics for all experiments in Appendix A.11, including an analysis of rating timing Appendix A.16. Accurate rating of climate information is challenging, but we find the main conclusion proposed below to be adequately supported.</p>
<p>LLMs. We evaluate the following models: GPT-4 (OpenAI, 2023), ChatGPT-3.5, InstructGPT (turbo), InstructGPT (text-davinci-003), InstructGPT (text-davinci-002) ${ }^{6}$, as well as PaLM2 (text-bison) (Anil et al., 2023) and Falcon-180B-Chat ${ }^{7}$. This data was collected in the months of September and October 2023.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Bootstrapped mean rating, and 95\% confidence intervals, for all presentational (left) and epistemological (right) dimensions.</p>
<h3>4.1. Performance Results</h3>
<p>Presentational Results. Overall, except for InstructGPT (text-davinci-002), LLMs produce clear, fluent, linguistically correct text. This confirms how far LLMs have come in terms of surface form quality, seemingly thanks to RLHF (Ouyang et al., 2022). We note, however, a marked performance drop for tone. This suggests that the evaluation of LLM's presentation should probably shift its focus on subtler aspects of language use (cf. also Section 4.4).</p>
<p>Epistemological Results. Compared to presentation, the epistemological evaluation reveals lower performance across all models and dimensions. Results are consistently low for the last three: specificity, completeness and uncertainty. We note that these dimensions may be difficult to satisfy in short 3-4 sentence answers. Being comprehensive in such a short space may be harder than being accurate.</p>
<p>On the other hand, LLMs don't seem to make a good use of space (see Section 4.4). Thus, space constraints alone do not seem sufficient to explain the result. Overall, on climate information, current top-of-the-line LLMs have significant headroom for improvement. For examples, please see Tables 27 to 30.</p>
<p>Dimension-Aware Prompts. In a follow-up experiment, using only GPT-4 ${ }^{8}$, we found that including a description of the evaluation criteria in the prompt can improve performance on the difficult dimensions: epistemological and tone. Table 1 compares GPT-4 with either the 'basic' or 'dimension-aware' prompts (see Table 8 for the actual</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Table 1. GPT-4 with basic vs. evaluation-aware prompts.
prompt's text). Interestingly, better performance on "knowledge" comes at the cost of worse quality in the presentational dimensions (except for tone), providing additional evidence for the existence of intrinsic tradeoffs between surface and epistemological quality.</p>
<h3>4.2. Resolution and Range.</h3>
<p>The results suggest that the evaluation possesses sufficient resolution to distinguish models on specific dimensions, indicate where they differ and suggest trends. For instance, ChatGPT is the best on all presentation dimensions, except tone, but ranks between position 4 and last on the epistemological scores. This indicates the possibility of complex dependencies between presentational and epistemological properties. As a matter of fact, GPT-4 is always better than ChatGPT across the epistemological dimensions but worse on most presentational ones.</p>
<p>The difference between the best LLM and the worst in specific dimensions - e.g., Falcon-180B-Chat and InstructGPT (text-davinci-002) on the epistemologi-</p>
<p>Question: Are we heading towards an ice age?
Answer: No, we are not heading towards an ice age. The current scientific consensus is that human activities, particularly the emission of greenhouse gases, are causing the planet to warm at an unprecedented rate. This rapid warming is leading to a variety of climate changes, including more frequent and severe heatwaves, rising sea levels, and changes in precipitation patterns. While natural processes have triggered ice ages in the past, these are very slow processes that take thousands of years, and the current rapid warming trend is not consistent with the onset of a new ice age.</p>
<p>Table 2. Example of common LLMs’ argumentative style.
cal ones, and, respectively, ChatGPT and InstructGPT (text-davinci-002) on the first three presentational ones-is large compared to the estimated standard deviation, providing evidence that the evaluation has sufficient dynamic range. Noticeably, the most competitive model-at the time of writing-on the epistemological dimensions is an open model, Falcon-180B-Chat. This LLM’s performance may be related to its large size, but we can only speculate as this information is not generally available for all models.</p>
<h3>4.3. Influence of Question Source.</h3>
<p>Comparing the rating outcome by source of the question Skeptical Science, GTrends, and synthetic questions from Wikipedia-we find no major differences, except for slightly lower scores for Wikipedia questions. In particular, we see no evidence that GPT-4 performs better on questions that were generated with GPT-4 compared to the other sources. Similarly, the topic of the question does not show a strong correlation with answer quality. See Appendix A. 13 for additional discussion and figures.</p>
<h3>4.4. Pragmatic Style</h3>
<p>LLMs often fill parts of the answer with somewhat relevant but unspecific, cautionary and sometimes preachy reminders. The example in Table 2 illustrates the point. Due to the highlighted part, the response may result in sub-optimal cooperative conversation, in a Gricean’s sense (Levinson, 1983). For instance, one could argue that the maxim of quantity is being violated ('do not provide more information than required') as most of what follows the first sentence is strictly speaking unnecessary. The maxim of manner ('be relevant') may also be violated: comments on extreme weather and rising sea levels are only loosely related to the question. That space could be used to provide more specific information.</p>
<p>Furthermore, the answer relies generically on the notion of scientific consensus, which happens relatively frequently in our data. Besides the possibility of being superficially</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3. Number of issues detected depending on AI Assistance exposure.
interpreted as an 'argument from authority', research suggests (Orchinik et al., 2023) that the 'consensus' argument can be surprisingly ineffective due to complex belief system underlying how such arguments are processed. Orchinik et al. (2023) argue that perceived scientists credibility, which in turn may depend on general worldview, affects how consensus-based messages are received and receptiveness to future messaging. This presentation style may not appeal to the different audiences of science communication and possibly lead to diminished interest and fatigue (Schäfer et al., 2018). To further complicate the picture, recent work points out that after conversing with AI on climate change, people with a skeptical stance end up dissatisfied but also more supportive of scientific consensus (Chen et al., 2024)</p>
<p>In these respects, LLMs answers differ from some human experts' answers to similar questions. ${ }^{9}$ The latter tend to rely on direct and specific scientific evidence; e.g., in the case of the question above, an expert may cite land, atmospheric and ocean data for temperature trends, from multiple scientific sources. ${ }^{10}$ Our framework captures some of these aspects in dimensions like tone and specificity, but the pragmatics aspects of Generative AI should probably be investigated more directly in the future.</p>
<h3>4.5. Role of AI Assistance.</h3>
<p>We expect human raters to identify more (real) issues with assistance, because it makes them aware of them. We find supporting evidence in two separate experiments.</p>
<p>Figure 3 reports the number of issues detected for each di-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>mension on GPT-4 answers in three different settings, each with a different degree of the raters’ exposure to assistance. ’Without AI Assistance’ refers to a setting where a specific pool of raters is never exposed to rating with AI Assistance. ’Without AI Assistance, but previous exposure’ refers to a setting where no assistance was shown, but the raters have worked on previous studies that included assistance. Lastly, ’With AI Assistance’ denotes the standard setting where assistance is shown anytime is available.</p>
<p>Results suggest that the presence of assistance is key for detecting more issues. This is consistent with the results from <em>Saunders et al. (2022)</em>. Raters with previous exposure to assistance are in a “middle” position: They detect more issues than the assistance-unaware group, but less than the group provided with assistance for the experiment. This suggests that raters learn from repeated exposure to assistance, and show improved performance even when no assistance is present.</p>
<p>Further evidence of the usefulness of AI Assistance comes from our validation experiments (cf. Appendix A.14 for more details). Similar to <em>Saunders et al. (2022)</em>, we want to determine if assistance helps surface real issues, without general access to gold truth in our data. To do this, the authors manually generated 30 different examples, each exhibiting a particular issue. We found that the majority of three raters detected 77% of issues when shown assistance, while the majority of three raters only detected 60% of the issues when not shown assistance.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4. The relationship between rating and reported helpfulness of the AI assistance (on the same scale).</p>
<p>In our experiments we collected feedback from raters on the helpfulness of assistance. The data suggests that when raters do not find assistance helpful, they give higher ratings (see Figure 4). This indicates that the raters can think critically about the assistance and do not follow it blindly. These experiments provide evidence that the AI Assistance helps the raters find real issues that they would not have otherwise been reported.</p>
<h3>4.6. Epistemological Adequacy and Attribution</h3>
<p>Grounding LLMs responses in retrieved documents, or Retrieval Augmented Language Models (RALM) <em>(Guu et al., 2020; Lewis et al., 2020)</em>, has been proposed to improve LLMs’ response quality and alleviate factuality limitations <em>(Menick et al., 2022)</em>. Analogously, on the evaluation side, frameworks such as Attribution to Identified Source (AIS) argue in favour of dedicated evaluations that bypass difficult direct factuality assessments <em>(Rashkin et al., 2023; Dziri et al., 2022)</em>: an attributable answer must include an explicit quote, from an existing document. AIS signals can be also modeled automatically <em>(Bohnet et al., 2023)</em> enabling training via reinforcement learning <em>(Roit et al., 2023)</em>.</p>
<p>While evaluating the ability of LLMs to properly ground their statements goes beyond the scope of this paper, we begin examining the relationship between attribution and the epistemological dimensions with an AIS experiment. We run this experiment only on GPT-4.</p>
<p>In our data, each answer is associated with a set of keypoints which, in turn, are used to identify Wikipedia articles that are likely to contain supporting evidence. For 87.7% of the questions, GPT-4 produces a valid Wikipedia article from which evidence passages can be extracted. We evaluate the attribution of each keypoint individually by asking the human annotators whether a keypoint is fully, partially or not supported by the evidence. 66.79% of keypoints are either fully or partially supported. We consider an answer to be fully attributed if all its keypoints are supported. An answer is not supported if all its keypoints are not supported. At the answer level, 46.08% of the answers are fully or partially supported by the evidence. While providing only preliminary evidence, the data suffices for a first analysis.</p>
<p>Figure 5 compares the distribution of average epistemological ratings, with respect to the attribution of answers, revealing interesting trends. In both the <em>accuracy</em> and <em>specificity</em> dimensions, we observe that answers that are fully attributed have higher minimum ratings compared to answers that are only partially attributed, or not attributed at all. Interestingly, we see an opposite pattern in the <em>completeness</em> dimension: Answers that are fully attributed have lower minimum ratings on <em>completeness</em>. This result highlights a blind spot for attribution methods; AIS can only consider</p>
<p>For instance, as proposed by <em>Liu et al. (2023)</em>, this may involve evaluating generative search engines.</p>
<p><sup>11</sup>We do make sure that the raters have not worked on the same examples before and have never seen assistance for the specific examples they are working on.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5. Comparing AIS ratings with average ratings of the 4 epistemological dimensions.
what is included in the answers, and not what important information is missing. In the uncertainty dimension, we observe that there are more answers with low uncertainty ratings among the answers that are not attributed, compared to answers that are either partially or fully attributed.</p>
<p>More generally, there does not seem to be any correlation between AIS and epistemological results. The Spearman's coefficient between AIS and the 3 -raters mean rating value for accuracy, specificity, uncertainty and completeness are, respectively: $0.03,-0.06,0.002,-0.02$, with corresponding p -values: $0.65,0.31,0.97,0.78$. We interpret this as evidence that AIS and epistemological assessments are mostly orthogonal. We provide a few examples in Table 19, in particular, of answers that are fully attributable but score low on epistemological dimensions. This suggests that, while practical and complementary, attribution, either human or model-based, is not a substitute for direct epistemological assessment.</p>
<p>Science and climate communication is more likely to be trusted if the source is perceived as credible, engaged and concerned about the audience's interests (Brown \&amp; Bruhn, 2011; Maibach et al., 2023; Hayhoe, 2018). An adequate presentation of climate information should include curated references. In future work we plan to extend our framework to evaluate references in a principled, systematic ways.</p>
<h2>5. Limitations and Future Work</h2>
<p>While our agreement analysis (Appendix A.11) suggests that the evaluation is robust at the system level, the rating dimensions inherently have a subjective component, introducing noise when evaluating at the answer-level. As we do not have access to gold ratings, calibration of raters re-
main an open issue, as reflected by the medium inter-rater agreement discussed in Appendix A.11. Future work should consider explicitly addressing this subjectivity in the data collection process (cf. Rottger et al. (2022)).</p>
<p>AI Assistance is an essential part of our evaluation, because it helps raters identify issues in the answers, particularly for the epistemological dimensions. As Tables 3 and 4 shows, raters would fail to recognize many issues without the AI Assistance ('GPT-4 no assistance'). However, the assistance may also influence the raters beyond enhancing discovery. It may only help in the discovery of some issues but not others. There may also be errors caused by models falsely pointing out issues and wrongly convincing the raters. The issues identified will likely vary by model. There is definitely a need to better understand these issues and to identify mitigation strategies. This links this research to the broader AI alignment field and will be one of the main focuses of our future work. A related topic is the role of LLMs as raters. Preliminary experiments are promising (Appendix A.15). We found that, as with humans, LLMs benefit from AI Assistance and that humans and LLM raters tend to agree on major points.</p>
<p>Ideally, an answer would be tailored towards the audience, and take into account their specific attributes (Hendriks et al., 2016; Klinger \&amp; Metag, 2021). Unless specifically prompted, LLMs do not do this and the evaluation of such setting would introduce additional challenges. Another important area for future work concerns multimodal responses. Research provides abundant evidence on the importance of supplementing textual information with visual aids. (Flemming et al., 2018; Brown \&amp; Bruhn, 2011). Visual complements can be especially useful for understanding quantitative data (Fagerlin \&amp; Peters, 2011) and in the case of limited literacy (Wolf et al., 2010). The abstract nature of climate change, and its distant implications, makes visualization particularly challenging (Schäfer, 2020).</p>
<h2>6. Related Work</h2>
<p>Evaluating LLMs. While LLMs can generate fluent text, responses are not always adequately grounded, attributable to reliable sources, and complete. For instance, Liu et al. (2023) assess four generative search engines and report that, although responses are perceived as high quality, only half are fully supported. Their findings reveal an inverse correlation between fluency/utility and evidential support. Xu et al. (2023) advocate for expert-level human evaluations in question answering, cautioning against over-reliance on single metrics instead of comprehensive assessments.</p>
<p>Another domain that needs expert-level evaluation is the medical domain. Singhal et al. (2023) propose Med-PaLM, an LLM for medical information, and introduces a clini-</p>
<p>cal evaluation framework which covers criteria like alignment with scientific consensus, potential harm, and comprehension. Evaluating LLMs on climate information is another domain that can benefit from expert-level evaluation. However, prior work mainly focused on text classification tasks (Diggelmann et al., 2020; Varini et al., 2020; Coan et al., 2021; Paschoal et al., 2021; Webersinke et al., 2022; Bingler et al., 2022; Spokoyny et al., 2023; Lacombe et al., 2023). This study aims to fill this gap by providing a comprehensive evaluation framework for generative climate information.</p>
<p>Scalable Oversight. This area, introduced by Amodei et al. (2016), studies the question of how to scale human oversight, especially in the setting where evaluating (or supervising) models becomes increasingly difficult. Contributions have initially focused on theoretical proposals for how AI can help humans supervise models that exceed their abilities (Irving et al., 2018; Leike et al., 2018; Christiano et al., 2018). Following Irving et al. (2018), one can see our AI Assistance as a single-turn debate, where the human annotator is shown the answer proposed by the model and a single response to that answer. ${ }^{13}$</p>
<p>Two recent studies provide interesting proofs of concepts for AI Assistance: Bowman et al. (2022) study sandwiching, an approach where non-experts align a model with the help of a model while experts provide validation. They show that non-expert raters perform better on an (artificially) difficult multiple-choice task when interacting with a dialogue agent. Several studies also evaluated short debates in this setting with mixed results (Parrish et al., 2022b;a; Michael et al., 2023). Saunders et al. (2022) report that human raters of summarization tasks produce more critiques when given the opportunity to accept or edit critiques written by a model. Our work contributes a study of a scalable oversight protocol to improve rating quality in a realistic setting.</p>
<p>AI Ratings. Recent studies explore the feasibility of evaluations performed by AI. Kocmi \&amp; Federmann (2023) indicate that LLMs can perform state-of-the-art quality assessment of translations, even without references. Their work has been extended to automatic MQM annotation by Fernandes et al. (2023). Gilardi et al. (2023) reports that ChatGPT has a higher agreement with expert-level raters than with less qualified ones. Chiang \&amp; Lee (2023) argue that humans and LLMs ratings are correlated but point out LLM's factuality and bias limitations. Instead of replacing human raters entirely, in our work we demonstrate the effectiveness of using AI Assistance to aid educated raters.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>7. Conclusion</h2>
<p>We introduce an evaluation framework informed by science communication research and assess LLMs on a first set of climate information needs. The task is difficult for human raters. To support them, an important part of our framework relies on a novel and practical protocol for scalable oversight that leverages AI Assistance. It is important to realize that these are the first results of this kind and more research is needed. In particular, while there is evidence that AI Assistance is valuable, we need to develop a framework to understand and mitigate undesired influence on the raters. Overall, our results suggest that, while presentationally adequate, current LLMs have much room for improvement regarding the epistemological qualities of their outputs. More research is needed to understand and improve these aspects of LLMs.</p>
<h2>Impact Statement</h2>
<p>In this work we present an evaluation framework to assess the quality of answers to climate-related questions. Our evaluation is based on science-communication principles and aims to evaluate responses to genuine information needs of the public. Progress in correctly answering such questions can have a large impact for the dissemination of scientific results and can lead to positive effects on climate literacy, also reducing the public's susceptibility to misinformation.</p>
<p>As with any evaluation there are however limits to its validity. Specifically, the evaluation of systems to be deployed in critical contexts requires additional grounding and expert verification. This is especially the case when system responses inform actions. Moreover, the evaluation is limited to the evaluated context, and we make no claims that models can be trusted and deployed outside of that context.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank the following people for their feedback and support: Leslie Leuenberger, Claire FoulquierGazagnes, Yana Koroleva, Srini Narayanan, Andrew Novikov, Annalisa Pawlosky, Fernando Pereira, Rachana Jayaram, Maria Ryabtseva. We also extend our appreciation to the anonymous reviewers for their valuable suggestions and feedback.</p>
<h2>References</h2>
<p>Amini, A. and Ciaramita, M. Probing in context: Toward building robust classifiers via probing large language models, 2023. URL https://arxiv.org/ abs/2305.14171.</p>
<p>Amini, A., Buck, C., Brown, H., Bulian, J., Huebscher, M. C., Ciaramita, M., Das, S., Gaiarin, B., Gordon, C., Gupta, R., Kutu, K., Lartey, D. L., Leippold, M., Leuenberger, L., and Mensah, M. A. AI and climate information needs in africa. In Deep Learning Indaba 2023, 2023. URL https://openreview.net/forum? $\mathrm{id}=\mathrm{MpUHGbNHbS}$.</p>
<p>Amodei, D., Olah, C., Steinhardt, J., Christiano, P. F., Schulman, J., and Mané, D. Concrete problems in AI safety. CoRR, abs/1606.06565, 2016. URL http: //arxiv.org/abs/1606.06565.</p>
<p>Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., Chu, E., Clark, J. H., Shafey, L. E., Huang, Y., MeierHellstern, K., Mishra, G., Moreira, E., Omernick, M., Robinson, K., Ruder, S., Tay, Y., Xiao, K., Xu, Y., Zhang, Y., Abrego, G. H., Ahn, J., Austin, J., Barham, P., Botha, J., Bradbury, J., Brahma, S., Brooks, K., Catasta, M., Cheng, Y., Cherry, C., Choquette-Choo, C. A., Chowdhery, A., Crepy, C., Dave, S., Dehghani, M., Dev, S., Devlin, J., Díaz, M., Du, N., Dyer, E., Feinberg, V., Feng, F., Fienber, V., Freitag, M., Garcia, X., Gehrmann, S., Gonzalez, L., Gur-Ari, G., Hand, S., Hashemi, H., Hou, L., Howland, J., Hu, A., Hui, J., Hurwitz, J., Isard, M., Ittycheriah, A., Jagielski, M., Jia, W., Kenealy, K., Krikun, M., Kudugunta, S., Lan, C., Lee, K., Lee, B., Li, E., Li, M., Li, W., Li, Y., Li, J., Lim, H., Lin, H., Liu, Z., Liu, F., Maggioni, M., Mahendru, A., Maynez, J., Misra, V., Moussalem, M., Nado, Z., Nham, J., Ni, E., Nystrom, A., Parrish, A., Pellat, M., Polacek, M., Polozov, A., Pope, R., Qiao, S., Reif, E., Richter, B., Riley, P., Ros, A. C., Roy, A., Saeta, B., Samuel, R., Shelby, R., Slone, A., Smilkov, D., So, D. R., Sohn, D., Tokumine, S., Valter, D., Vasudevan, V., Vodrahalli, K., Wang, X., Wang, P., Wang, Z., Wang, T., Wieting, J., Wu, Y., Xu, K., Xu, Y., Xue, L., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng, C., Zhou, W., Zhou, D., Petrov, S., and Wu, Y. Palm 2 technical report, 2023.</p>
<p>Baram-Tsabari, A. and Lewenstein, B. V. An instrument for assessing scientists' written skills in public communication of science. Science Communication, 35(1):56-85, 2013. ISSN 1075-5470. doi: 10.1177/ 1075547012440634.</p>
<p>Baram-Tsabari, A., Wolfson, O., Yosef, R., Chapnik, N., Brill, A., and Segev, E. Jargon use in public understand-
ing of science papers over three decades. Public Understanding of Science, 29(6):644-654, 2020. ISSN 09636625. doi: 10.1177/0963662520940501.</p>
<p>Berger, C. R. Planning strategic interaction: Attaining goals through communicative action. Routledge, 2020. ISBN 9781003064190. doi: 10.4324/9781003064190.</p>
<p>Bergquist, P., Marlon, J. R., Goldberg, M. H., Gustafson, A., Rosenthal, S. A., and Leiserowitz, A. Information about the human causes of global warming influences causal attribution, concern, and policy support related to global warming. Thinking \&amp; Reasoning, 28(3):465-486, 2022.</p>
<p>Bingler, J. A., Kraus, M., Leippold, M., and Webersinke, N. Cheap talk and cherry-picking: What climatebert has to say on corporate climate risk disclosures. Finance Research Letters, 47, 2022. URL https://www.sciencedirect.com/ science/article/pii/S1544612322000897.</p>
<p>Blanton, H. and Ikizer, E. G. Elegant science narratives and unintended influences: An agenda for the science of science communication. Social Issues and Policy Review, 13(1):154-181, 2019. ISSN 17512395. doi: 10.1111/sipr. 12055 .</p>
<p>Bohnet, B., Tran, V. Q., Verga, P., Aharoni, R., Andor, D., Soares, L. B., Ciaramita, M., Eisenstein, J., Ganchev, K., Herzig, J., Hui, K., Kwiatkowski, T., Ma, J., Ni, J., Saralegui, L. S., Schuster, T., Cohen, W. W., Collins, M., Das, D., Metzler, D., Petrov, S., and Webster, K. Attributed question answering: Evaluation and modeling for attributed large language models, 2023.</p>
<p>Bowman, S. R., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., Lukošiūtė, K., Askell, A., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Olah, C., Amodei, D., Amodei, D., Drain, D., Li, D., Tran-Johnson, E., Kernion, J., Kerr, J., Mueller, J., Ladish, J., Landau, J., Ndousse, K., Lovitt, L., Elhage, N., Schiefer, N., Joseph, N., Mercado, N., DasSarma, N., Larson, R., McCandlish, S., Kundu, S., Johnston, S., Kravec, S., Showk, S. E., Fort, S., Telleen-Lawton, T., Brown, T., Henighan, T., Hume, T., Bai, Y., Hatfield-Dodds, Z., Mann, B., and Kaplan, J. Measuring progress on scalable oversight for large language models, 2022.</p>
<p>Brown, M. and Bruhn, C. Chapter 11: Information and persuasion. In Fischhoff, B., Brewer, N. T., and Downs, J. S. (eds.), Communicating risks and benefits: An evidencebased user's guide, pp. 101-109. US Department of Health and Human Services, Washington, D.C., 2011.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,</p>
<p>G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper. pdf.</p>
<p>Budescu, D. V., Por, H.-H., and Broomell, S. B. Effective communication of uncertainty in the ipcc reports. Climatic Change, 113:181-200, 2012.</p>
<p>Chen, K., Shao, A., Burapacheep, J., and Li, Y. Conversational ai and equity through assessing gpt-3's communication with diverse social groups on contentious topics. Scientific Reports, 14, 2024. URL https: //doi.org/10.1038/s41598-024-51969-w.</p>
<p>Chiang, C.-H. and Lee, H.-y. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023.</p>
<p>Chiesurin, S., Dimakopoulos, D., Sobrevilla Cabezudo, M. A., Eshghi, A., Papaioannou, I., Rieser, V., and Konstas, I. The dangers of trusting stochastic parrots: Faithfulness and trust in open-domain conversational question answering. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 947-959, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl. 60. URL https://aclanthology.org/2023. findings-acl. 60.</p>
<p>Chinn, S. and Hart, P. S. Effects of consensus messages and political ideology on climate change attitudes: inconsistent findings and the effect of a pretest. Climatic Change, 167(3-4):47, 2021.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz,
M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.</p>
<p>Christiano, P. F., Shlegeris, B., and Amodei, D. Supervising strong learners by amplifying weak experts. CoRR, abs/1810.08575, 2018. URL http://arxiv.org/ abs/1810.08575.</p>
<p>Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. Scaling instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416.</p>
<p>Coan, T. G., Boussalis, C., Cook, J., and Nanko, M. O. Computer-assisted classification of contrarian claims about climate change. Scientific Reports, 2021. URL https://doi.org/10.1038/ s41598-021-01714-4.</p>
<p>Diggelmann, T., Boyd-Graber, J., Bulian, J., Ciaramita, M., and Leippold, M. Climate-fever: A dataset for verification of real-world climate claims. In NeurIPS 2020 Workshop on Tackling Climate Change with Machine Learning, 2020.</p>
<p>Dziri, N., Rashkin, H., Linzen, T., and Reitter, D. Evaluating attribution in dialogue systems: The begin benchmark. Transactions of the Association for Computational Linguistics, 10:1066-1083, 2022.</p>
<p>Fagerlin, A. and Peters, E. Chapter 7: Quantitative information. In Fischhoff, B., Brewer, N. T., and Downs, J. S. (eds.), Communicating risks and benefits: An evidencebased user's guide, pp. 53-64. US Department of Health and Human Services, Washington, D.C., 2011.</p>
<p>Fähnrich, B., Weitkamp, E., and Kupper, J. F. Exploring 'quality' in science communication online: Expert thoughts on how to assess and promote science communication quality in digital media contexts. Public Understanding of Science, 32(5):605-621, 2023. ISSN 09636625. doi: 10.1177/09636625221148054.</p>
<p>Fernandes, P., Deutsch, D., Finkelstein, M., Riley, P., Martins, A. F. T., Neubig, G., Garg, A., Clark, J. H., Freitag, M., and Firat, O. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation, 2023.</p>
<p>Flemming, D., Cress, U., Kimmig, S., Brandt, M., and Kimmerle, J. Emotionalization in science communication: The impact of narratives and visual representations on knowledge gain and risk perception. Frontiers in Communication, 3:3, 2018.</p>
<p>Gemini Team. Gemini: A family of highly capable multimodal models, 2023.</p>
<p>Gilardi, F., Alizadeh, M., and Kubli, M. Chatgpt outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120, 2023. doi: 10.1073/pnas. 2305016120. URL https://www.pnas.org/doi/ abs/10.1073/pnas. 2305016120.</p>
<p>Goldberg, M. H., Gustafson, A., van der Linden, S., Rosenthal, S. A., and Leiserowitz, A. Communicating the scientific consensus on climate change: diverse audiences and effects over time. Environment and Behavior, 54 (7-8):1133-1165, 2022.</p>
<p>Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model pretraining. In Proceedings of the 37th International Conference on Machine Learning, 2020. URL https://proceedings.mlr.press/v119/ guu20a.html.</p>
<p>Hayhoe, K. When facts are not enough. Science, 360(6392):943-943, 2018. doi: 10.1126/science. aau2565. URL https://www.science.org/ doi/abs/10.1126/science.aau2565.</p>
<p>Hendriks, F., Kienhues, D., and Bromme, R. Trust in science and the science of trust. In Blöbaum, B. (ed.), Trust and communication in a digitized world, pp. 143-159. Springer, Cham, 2016. ISBN 978-3-319-28059-2.</p>
<p>Hinnant, A., Subramanian, R., and Young, R. User comments on climate stories: impacts of anecdotal vs. scientific evidence. Climatic Change, 138(3-4):411-424, 2016. ISSN 0165-0009. doi: 10.1007/s10584-016-1759-1.</p>
<p>Holmes, K. J., Wender, B. A., Weisenmiller, R., Doughman, P., and Kerxhalli-Kleinfield, M. Climate assessment moves local. Earth's Future, 8(2), 2020. ISSN 2328-4277. doi: 10.1029/2019EF001402.</p>
<p>Howe, L. C., MacInnis, B., Krosnick, J. A., Markowitz, E. M., and Socolow, R. Acknowledging uncertainty impacts public acceptance of climate scientists' predictions. Nature Climate Change, 9(11):863-867, 2019.</p>
<p>Irving, G., Christiano, P. F., and Amodei, D. AI safety via debate. CoRR, abs/1805.00899, 2018. URL http: //arxiv.org/abs/1805.00899.</p>
<p>Jamieson, K. H., Kahan, D. M., and Scheufele, D. A. The Oxford Handbook of the Science of Science Communication. Oxford University Press, 2017. URL https://doi.org/10.1093/oxfordhb/ 9780190497620.001 .0001 .</p>
<p>Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023. ISSN 0360-0300. doi: $10.1145 / 3571730$.</p>
<p>Kelesidou, F. and Chabrol, E. (eds.). A comprehensive guide to Science Communication. Hindawi, 2021.</p>
<p>Keohane, R. O., Lane, M., and Oppenheimer, M. The ethics of scientific communication under uncertainty. Politics, Philosophy \&amp; Economics, 13(4):343-368, 2014. ISSN 1470-594X. doi: 10.1177/1470594X14538570.</p>
<p>Kerr, J. R., Schneider, C. R., Freeman, A. L. J., Marteau, T., and van der Linden, S. Transparent communication of evidence does not undermine public trust in evidence. PNAS nexus, 1(5):pgac280, 2022. doi: 10.1093/pnasnexus/ pgac280.</p>
<p>Klinger, K. and Metag, J. Media effects in the context of environmental issues. In Takahashi, B., Metag, J., Thaker, J., and Comfort, S. E. (eds.), The Handbook of International Trends in Environmental Communication, pp. 3149. Routledge, New York, 2021. ISBN 9780367275204.</p>
<p>Kocmi, T. and Federmann, C. Large language models are state-of-the-art evaluators of translation quality, 2023.</p>
<p>König, L. M., Altenmüller, M. S., Fick, J., Crusius, J., Genschow, O., and Sauerland, M. How to communicate science to the public? recommendations for effective written communication derived from a systematic review, Aug 2023. URL psyarxiv.com/cwbrs.</p>
<p>Lacombe, R., Wu, K., and Dilworth, E. Climatex: Do llms accurately assess human expert confidence in climate statements?, 2023.</p>
<p>Lang, A. The limited capacity model of mediated message processing. Journal of Communication, 50(1):46-70, 2000. ISSN 0021-9916. doi: 10.1111/j.1460-2466.2000. tb02833.x.</p>
<p>Lee, T. M., Markowitz, E. M., Howe, P. D., Ko, C.-Y., and Leiserowitz, A. A. Predictors of public climate change awareness and risk perception around the world. Nature Climate Change, 5(11):1014-1020, 2015.</p>
<p>Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S. Scalable agent alignment via reward modeling: a research direction. CoRR, abs/1811.07871, 2018. URL http://arxiv.org/abs/1811.07871.</p>
<p>Leiserowitz, A. and Smith, N. Affective imagery, risk perceptions, and climate change communication. In Leiserowitz, A. and Smith, N. (eds.), Oxford research encyclopedia of climate science. Oxford University Press, Oxford, 2017. ISBN 9780190228620. doi: 10.1093/ acrefore/9780190228620.013.307.</p>
<p>Leiserowitz, A., Maibach, E., Rosenthal, S., Kotcher, J., Neyens, L., Marlon, J., Carman, J., Lacroix, K., and Goldberg, M. Global warming's six Americas, 2022.</p>
<p>Leonhard, L., Karnowski, V., and Kümpel, A. S. Online and (the feeling of being) informed: Online news usage patterns and their relation to subjective and objective political knowledge. Computers in Human Behavior, 103:181-189, 2020. ISSN 07475632. doi: 10.1016/j.chb. 2019.08.008.</p>
<p>Levinson, S. C. Pragmatics. Cambridge Textbooks in Linguistics. Cambridge University Press, 1983. doi: 10. 1017/CBO9780511813313.</p>
<p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., Riedel, S., and Kiela, D. Retrievalaugmented generation for knowledge-intensive nlp tasks. In Advances in Neural Information Processing Systems, 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 6b493230205f780e1bc26945df7481e5-Paper. pdf.</p>
<p>Lewis Jr., N. A. and Wai, J. Communicating what we know and what isn't so: Science communication in psychology. Perspectives on Psychological Science, 16(6):1242-1254, 2021. doi: 10.1177/1745691620964062.</p>
<p>Liu, N. F., Zhang, T., and Liang, P. Evaluating verifiability in generative search engines, 2023.</p>
<p>Lommel, A., Burchardt, A., and Uszkoreit, H. Multidimensional quality metrics: a flexible system for assessing translation quality. In Proceedings of Translating and the Computer 35, London, UK, November 28-29 2013. Aslib. URL https://aclanthology.org/2013. tc-1.6.</p>
<p>Maertens, R., Anseel, F., and van der Linden, S. Combatting climate change misinformation: Evidence for longevity of inoculation and consensus messaging effects. Journal of Environmental Psychology, 70, 2020. ISSN 02724944. doi: 10.1016/j.jenvp.2020.101455.</p>
<p>Maibach, E. W., Uppalapati, S. S., Orr, M., and Thaker, J. Harnessing the power of communication and behavior science to enhance society's response to climate
change. Annual Review of Earth and Planetary Sciences, 51(1):53-77, 2023. ISSN 0084-6597. doi: 10.1146/annurev-earth-031621-114417.</p>
<p>Mazer, J. P. and Hunt, S. K. "cool" communication in the classroom: A preliminary examination of student perceptions of instructor use of positive slang. Qualitative Research Reports in Communication, 9(1):20-28, 2008. ISSN 1745-9435. doi: 10.1080/17459430802400316.</p>
<p>Menick, J., Trebacz, M., Mikulik, V., Aslanides, J., Song, F., Chadwick, M., Glaese, M., Young, S., CampbellGillingham, L., Irving, G., and McAleese, N. Teaching language models to support answers with verified quotes, 2022. URL https://arxiv.org/abs/ 2203.11147.</p>
<p>Michael, J., Mahdi, S., Rein, D., Petty, J., Dirani, J., Padmakumar, V., and Bowman, S. R. Debate helps supervise unreliable experts. CoRR, abs/2311.08702, 2023. doi: 10.48550/ARXIV.2311.08702. URL https: //doi.org/10.48550/arXiv.2311.08702.</p>
<p>Mollick, E. The dynamics of crowdfunding: An exploratory study. Journal of Business Venturing, 29(1):1-16, 2014. ISSN 08839026. doi: 10.1016/j.jbusvent.2013.06.005.</p>
<p>Moser, S. Reflections on climate change communication research and practice in the second decade of the 21st century: what more is there to say? Wiley Interdisciplinary Reviews: Climate Change 7(3), 345-369, 2016.</p>
<p>Munoz-Carrier, G., Thomsen, D., and Pickering, G. J. Psychological and experiential factors affecting climate change perception: learnings from a transnational empirical study and implications for framing climate-related flood events. Environmental Research Communications, 2(4), 2020. doi: 10.1088/2515-7620/ab89f9.</p>
<p>Newman, N., Fletcher, R., Schulz, A., Andi, S., Robertson, C. T., and Nielsen, R. K. Reuters institute digital news report 2021, 2021.</p>
<p>Ni, J., Qu, C., Lu, J., Dai, Z., Hernandez Abrego, G., Ma, J., Zhao, V., Luan, Y., Hall, K., Chang, M.-W., and Yang, Y. Large dual encoders are generalizable retrievers. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2022. URL https://aclanthology. org/2022.emnlp-main.669.</p>
<p>Nisbet, M. C., Ho, S. S., Markowitz, E., O’Neill, S., Schäfer, M. S., and Thaker, J. (eds.). The Oxford encyclopedia of climate change communication. Oxford University Press, New York, 2018. ISBN 9780190498986. doi: 10.1093/acref/9780190498986.001.0001.</p>
<p>OpenAI. GPT-4 technical report, 2023.
Orchinik, R., Dubey, R., Gershman, S. J., Powell, D., and Bhui, R. Learning from and about climate scientists, 2023. URL https://doi.org/10.31234/osf. io/ezua5.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022.</p>
<p>Palm, R., Bolsen, T., and Kingsland, J. T. "don't tell me what to do": Resistance to climate change messages suggesting behavior changes. Weather, Climate, and Society, 12(4):827-835, 2020.</p>
<p>Parrish, A., Trivedi, H., Nangia, N., Padmakumar, V., Phang, J., Saimbhi, A. S., and Bowman, S. R. Two-turn debate doesn't help humans answer hard reading comprehension questions. CoRR, abs/2210.10860, 2022a. doi: 10.48550/ ARXIV.2210.10860. URL https://doi.org/10. 48550/arXiv.2210.10860.</p>
<p>Parrish, A., Trivedi, H., Perez, E., Chen, A., Nangia, N., Phang, J., and Bowman, S. R. Single-turn debate does not help humans answer hard reading-comprehension questions. CoRR, abs/2204.05212, 2022b. doi: 10.48550/ ARXIV.2204.05212. URL https://doi.org/10. 48550/arXiv.2204.05212.</p>
<p>Paschoal, A. F. A., Pirozelli, P., Freire, V., Delgado, K. V., Peres, S. M., José, M. M., Nakasato, F., Oliveira, A. S., Brandão, A. A. F., Costa, A. H. R., and Cozman, F. G. Pirá: A bilingual portuguese-english dataset for question-answering about the ocean. In Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management, CIKM '21. Association for Computing Machinery, 2021. doi: 10. 1145/3459637.3482012. URL https://doi.org/ 10.1145/3459637.3482012.</p>
<p>Pearce, W., Niederer, S., Özkula, S. M., and Sánchez Querubín, N. The social media life of climate change: Platforms, publics and future imaginaries. Wiley interdisciplinary reviews: Climate change, 10(2), e569., 2019.</p>
<p>Poushter, J., Fagan, M., and Gubbala, S. Climate change remains top global threat across 19-country survey, 2022.</p>
<p>Rashkin, H., Nikolaev, V., Lamm, M., Aroyo, L., Collins, M., Das, D., Petrov, S., Tomar, G. S., Turc, I., and Reitter, D. Measuring attribution in natural language generation models, 2022.</p>
<p>Rashkin, H., Nikolaev, V., Lamm, M., Aroyo, L., Collins, M., Das, D., Petrov, S., Tomar, G. S., Turc, I., and Reitter, D. Measuring Attribution in Natural Language Generation Models. Computational Linguistics, pp. 1-64, 2023. URL https://doi.org/10.1162/coli_ a_00486.</p>
<p>Roit, P., Ferret, J., Shani, L., Aharoni, R., Cideron, G., Dadashi, R., Geist, M., Girgin, S., Hussenot, L., Keller, O., Momchev, N., Ramos Garea, S., Stanczyk, P., Vieillard, N., Bachem, O., Elidan, G., Hassidim, A., Pietquin, O., and Szpektor, I. Factually consistent summarization via reinforcement learning with textual entailment feedback. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023. URL https://aclanthology. org/2023.acl-long. 344.</p>
<p>Rolnick, D., Donti, P. L., Kaack, L. H., Kochanski, K., Lacoste, A., Sankaran, K., Ross, A. S., Milojevic-Dupont, N., Jaques, N., Waldman-Brown, A., Luccioni, A. S., Maharaj, T., Sherwin, E. D., Mukkavilli, S. K., Kording, K. P., Gomes, C. P., Ng, A. Y., Hassabis, D., Platt, J. C., Creutzig, F., Chayes, J., and Bengio, Y. Tackling climate change with machine learning. ACM Comput. Surv., 55(2), 2022. URL https://doi.org/10.1145/ 3485128.</p>
<p>Rottger, P., Vidgen, B., Hovy, D., and Pierrehumbert, J. Two contrasting data annotation paradigms for subjective NLP tasks. In Carpuat, M., de Marneffe, M.-C., and Meza Ruiz, I. V. (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 175-190, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.13. URL https: //aclanthology.org/2022.naacl-main.13.</p>
<p>Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. Self-critiquing models for assisting human evaluators, 2022.</p>
<p>Schäfer, M. S. Introduction to visualizing climate change. In Holmes, D. C. and Richardson, L. M. (eds.), Research handbook on communicating climate change, Elgar handbooks in energy, the environment and climate change, pp. 127-130. Edward Elgar Publishing, Cheltenham, UK, 2020. ISBN 9781789900392.</p>
<p>Schäfer, M. S. The notorious GPT: Science communication in the age of artificial intelligence. Journal of Science Communication, 22(2), 2023. ISSN 1824-2049. doi: 10.22323/2.22020402.</p>
<p>Schäfer, M. S., Füchslin, T., Metag, J., Kristiansen, S., and Rauchfleisch, A. The different audiences of science communication: A segmentation analysis of the swiss population's perceptions of science and their information and media use patterns. Public Understanding of Science, 27(7):836-856, 2018. doi: 10.1177/ 0963662517752886. URL https://doi.org/10. 1177/0963662517752886.</p>
<p>Scharrer, L., Bromme, R., Britt, M. A., and Stadtler, M. The seduction of easiness: How science depictions influence laypeople's reliance on their own evaluation of scientific information. Learning and Instruction, 22(3):231-243, 2012. ISSN 09594752. doi: 10.1016/j.learninstruc. 2011. 11.004.</p>
<p>Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., et al. Large language models encode clinical knowledge. Nature, pp. 1-9, 2023.</p>
<p>Spokoyny, D., Laud, T., Corringham, T., and BergKirkpatrick, T. Towards answering climate questionnaires from unstructured climate reports, 2023.</p>
<p>Thompson, B., Dhaliwal, M. P., Frisch, P., Domhan, T., and Federico, M. A shocking amount of the web is machine translated: Insights from multi-way parallelism, 2024.</p>
<p>Trench, B. and Bucchi, M. (eds.). Routledge handbook of public communication of science and technology. Routledge, Abingdon and New York, 2021. ISBN 9781003039242. doi: 10.4324/9781003039242.
van der Linden, S. L., Leiserowitz, A. A., Feinberg, G. D., and Maibach, E. W. The scientific consensus on climate change as a gateway belief: experimental evidence. PloS One, 10(2):e0118489, 2015. doi: 10.1371/journal.pone. 0118489.</p>
<p>Varini, F. S., Boyd-Graber, J., Ciaramita, M., and Leippold, M. ClimaText: A dataset for climate change topic detection. In NeurIPS 2020 Workshop on Tackling Climate Change with Machine Learning, 2020.</p>
<p>Webersinke, N., Kraus, M., Bingler, J., and Leippold, M. ClimateBERT: a pretrained language model for climaterelated text. In Proceedings of AAAI 2022 Fall Symposium: The Role of AI in Responding to Climate Challenges, 2022.</p>
<p>Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z., Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell, L., Hendricks, L. A., Isaac, W. S., Legassick, S., Irving, G., and Gabriel, I. Ethical and social risks of harm from language models.</p>
<p>CoRR, abs/2112.04359, 2021. URL https://arxiv. org/abs/2112.04359.</p>
<p>WHO. 2021 World Health Organization: health and climate change global survey report, 2021.</p>
<p>Wolf, M. S., Davis, T. C., Bass, P. F., Curtis, L. M., Lindquist, L. A., Webb, J. A., Bocchini, M. V., Bailey, S. C., and Parker, R. M. Improving prescription drug warnings to promote patient comprehension. Archives of internal medicine, 170(1):50-56, 2010. doi: 10.1001/archinternmed.2009.454.</p>
<p>Xu, F., Song, Y., Iyyer, M., and Choi, E. A critical evaluation of evaluations for long-form question answering. arXiv preprint arXiv:2305.18201, 2023.</p>
<p>Yuan, S. and Lu, H. "It's global warming, stupid": Aggressive communication styles and political ideology in science blog debates about climate change. Journalism \&amp; Mass Communication Quarterly, 97(4):1003-1025, 2020.</p>
<h1>A. Appendix</h1>
<h2>A.1. Main Results</h2>
<p>Tables 3 and 4 report the full results for all evaluated models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: center;">style</th>
<th style="text-align: center;">clarity</th>
<th style="text-align: center;">correctness</th>
<th style="text-align: center;">tone</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$4.54[4.50,4.58]$</td>
<td style="text-align: center;">$4.56[4.52,4.60]$</td>
<td style="text-align: center;">$4.58[4.54,4.61]$</td>
<td style="text-align: center;">$3.06[2.99,3.13]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$4.15[4.08,4.22]$</td>
<td style="text-align: center;">$4.43[4.38,4.47]$</td>
<td style="text-align: center;">$4.47[4.42,4.52]$</td>
<td style="text-align: center;">$3.20[3.12,3.28]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (davinci-002)</td>
<td style="text-align: center;">$3.22[3.13,3.31]$</td>
<td style="text-align: center;">$3.63[3.55,3.70]$</td>
<td style="text-align: center;">$3.82[3.74,3.90]$</td>
<td style="text-align: center;">$3.17[3.09,3.24]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (turbo)</td>
<td style="text-align: center;">$4.37[4.32,4.42]$</td>
<td style="text-align: center;">$4.40[4.36,4.45]$</td>
<td style="text-align: center;">$4.46[4.42,4.51]$</td>
<td style="text-align: center;">$3.41[3.33,3.48]$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$4.34[4.28,4.40]$</td>
<td style="text-align: center;">$4.48[4.43,4.53]$</td>
<td style="text-align: center;">$4.57[4.53,4.61]$</td>
<td style="text-align: center;">$3.19[3.11,3.27]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4</td>
<td style="text-align: center;">$4.35[4.30,4.40]$</td>
<td style="text-align: center;">$4.34[4.28,4.39]$</td>
<td style="text-align: center;">$4.38[4.34,4.41]$</td>
<td style="text-align: center;">$3.26[3.19,3.34]$</td>
</tr>
<tr>
<td style="text-align: left;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$4.36[4.31,4.41]$</td>
<td style="text-align: center;">$4.39[4.35,4.44]$</td>
<td style="text-align: center;">$4.41[4.36,4.45]$</td>
<td style="text-align: center;">$3.37[3.30,3.45]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4, no assistance, prev. exposure</td>
<td style="text-align: center;">$4.59[4.54,4.63]$</td>
<td style="text-align: center;">$4.63[4.59,4.68]$</td>
<td style="text-align: center;">$4.66[4.63,4.70]$</td>
<td style="text-align: center;">$3.24[3.16,3.32]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4, no assistance</td>
<td style="text-align: center;">$4.45[4.41,4.50]$</td>
<td style="text-align: center;">$4.57[4.53,4.61]$</td>
<td style="text-align: center;">$4.74[4.70,4.77]$</td>
<td style="text-align: center;">$4.35[4.29,4.42]$</td>
</tr>
</tbody>
</table>
<p>Table 3. Results along the presentational dimensions, with $95 \%$ confidence intervals.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: center;">accuracy</th>
<th style="text-align: center;">specificity</th>
<th style="text-align: center;">completeness</th>
<th style="text-align: center;">uncertainty</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$3.48[3.41,3.55]$</td>
<td style="text-align: center;">$2.71[2.63,2.78]$</td>
<td style="text-align: center;">$2.26[2.20,2.31]$</td>
<td style="text-align: center;">$2.05[2.00,2.09]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$3.52[3.44,3.60]$</td>
<td style="text-align: center;">$2.89[2.81,2.97]$</td>
<td style="text-align: center;">$2.43[2.36,2.50]$</td>
<td style="text-align: center;">$2.18[2.11,2.25]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (davinci-002)</td>
<td style="text-align: center;">$2.81[2.73,2.88]$</td>
<td style="text-align: center;">$2.49[2.42,2.56]$</td>
<td style="text-align: center;">$2.32[2.26,2.39]$</td>
<td style="text-align: center;">$2.35[2.29,2.41]$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT (turbo)</td>
<td style="text-align: center;">$3.65[3.58,3.73]$</td>
<td style="text-align: center;">$2.79[2.71,2.86]$</td>
<td style="text-align: center;">$2.43[2.37,2.50]$</td>
<td style="text-align: center;">$2.24[2.19,2.30]$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$3.47[3.39,3.55]$</td>
<td style="text-align: center;">$2.81[2.73,2.89]$</td>
<td style="text-align: center;">$2.57[2.50,2.65]$</td>
<td style="text-align: center;">$2.25[2.18,2.32]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4</td>
<td style="text-align: center;">$3.67[3.61,3.73]$</td>
<td style="text-align: center;">$3.13[3.05,3.21]$</td>
<td style="text-align: center;">$2.61[2.53,2.68]$</td>
<td style="text-align: center;">$2.21[2.15,2.27]$</td>
</tr>
<tr>
<td style="text-align: left;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$3.81[3.74,3.87]$</td>
<td style="text-align: center;">$3.15[3.07,3.23]$</td>
<td style="text-align: center;">$2.73[2.65,2.80]$</td>
<td style="text-align: center;">$2.55[2.47,2.62]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4, no assistance, prev. exposure</td>
<td style="text-align: center;">$3.86[3.79,3.93]$</td>
<td style="text-align: center;">$3.43[3.35,3.52]$</td>
<td style="text-align: center;">$3.30[3.21,3.39]$</td>
<td style="text-align: center;">$2.78[2.69,2.87]$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4, no assistance</td>
<td style="text-align: center;">$4.49[4.44,4.55]$</td>
<td style="text-align: center;">$4.41[4.35,4.48]$</td>
<td style="text-align: center;">$4.32[4.25,4.39]$</td>
<td style="text-align: center;">$3.38[3.29,3.46]$</td>
</tr>
</tbody>
</table>
<p>Table 4. Results along the epistemological dimensions, with $95 \%$ confidence intervals.</p>
<h2>A.2. Pairwise t-tests</h2>
<p>Tables 5 and 6 report the results of LLM pairwise t-tests.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">davinci-002</th>
<th style="text-align: center;">InstructGPT <br> davinci-003</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ChatGPT <br> text-bison</th>
<th style="text-align: center;">PaLM-2</th>
<th style="text-align: center;">GPT4 <br> 180B-Chat</th>
<th style="text-align: center;">Falcon</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">style</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">clarity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">correctness</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">tone</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5. Presentational dimensions - Pairwise t-test results. -- and ++ indicate high significance ( $p&lt;0.01$ ) that the model in the row scores lower/higher than the model in the colum. Single $-/+$ indicate $p&lt;0.05$ and $\sim$ indicates no significant difference between the models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">davinci-002</th>
<th style="text-align: center;">InstructGPT <br> davinci-003</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ChatGPT <br> text-bison</th>
<th style="text-align: center;">PaLM-2</th>
<th style="text-align: center;">GPT4 <br> 180B-Chat</th>
<th style="text-align: center;">Falcon</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">specificity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">completeness</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">uncertainty</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (davinci-003)</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT (turbo)</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 (text-bison)</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$--$</td>
</tr>
<tr>
<td style="text-align: center;">Falcon (180B-Chat)</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;">$++$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 6. Epistemological dimensions. Pairwise t-test results. -- and ++ indicate high significance $(p&lt;0.01)$ that the model in the row scores lower/higher than the model in the colum. Single $-/+$ indicate $p&lt;0.05$ and $\sim$ indicates no significant difference between the models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Issue</th>
<th style="text-align: center;">InstructGPT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">PaLM-2</th>
<th style="text-align: center;">GPT4</th>
<th style="text-align: center;">Falcon</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">davinci-002</td>
<td style="text-align: center;">davinci-003</td>
<td style="text-align: center;">turbo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">text-bison</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">180B-Chat</td>
</tr>
<tr>
<td style="text-align: center;">style</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">inconsistent</td>
<td style="text-align: center;">4.88</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">repetitive</td>
<td style="text-align: center;">20.15</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">too informal</td>
<td style="text-align: center;">4.11</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">0.89</td>
</tr>
<tr>
<td style="text-align: center;">too long</td>
<td style="text-align: center;">1.03</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">2.12</td>
<td style="text-align: center;">2.11</td>
<td style="text-align: center;">0.89</td>
</tr>
<tr>
<td style="text-align: center;">too short</td>
<td style="text-align: center;">10.14</td>
<td style="text-align: center;">8.56</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">1.11</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">2.95</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">clarity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">hard math</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">sentences too long</td>
<td style="text-align: center;">1.80</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">1.22</td>
</tr>
<tr>
<td style="text-align: center;">too technical</td>
<td style="text-align: center;">3.59</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">1.22</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">0.56</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">8.60</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;">correctness</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">incomplete sentence</td>
<td style="text-align: center;">3.47</td>
<td style="text-align: center;">2.44</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">incorrect grammar</td>
<td style="text-align: center;">6.29</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">incorrect punctuation</td>
<td style="text-align: center;">2.18</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.56</td>
</tr>
<tr>
<td style="text-align: center;">incorrect spelling</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">3.98</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;">tone</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">biased</td>
<td style="text-align: center;">28.50</td>
<td style="text-align: center;">34.44</td>
<td style="text-align: center;">24.78</td>
<td style="text-align: center;">42.38</td>
<td style="text-align: center;">33.85</td>
<td style="text-align: center;">30.33</td>
<td style="text-align: center;">23.72</td>
</tr>
<tr>
<td style="text-align: center;">negative</td>
<td style="text-align: center;">1.28</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">5.01</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">1.89</td>
</tr>
<tr>
<td style="text-align: center;">persuasive</td>
<td style="text-align: center;">2.57</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">4.80</td>
<td style="text-align: center;">7.68</td>
<td style="text-align: center;">10.69</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">4.45</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;">accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">anecdotal</td>
<td style="text-align: center;">10.78</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">3.35</td>
<td style="text-align: center;">19.24</td>
<td style="text-align: center;">5.90</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">3.01</td>
</tr>
<tr>
<td style="text-align: center;">incorrect</td>
<td style="text-align: center;">20.92</td>
<td style="text-align: center;">10.78</td>
<td style="text-align: center;">5.58</td>
<td style="text-align: center;">3.23</td>
<td style="text-align: center;">11.69</td>
<td style="text-align: center;">4.44</td>
<td style="text-align: center;">4.57</td>
</tr>
<tr>
<td style="text-align: center;">science out of context</td>
<td style="text-align: center;">9.37</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">5.69</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">5.35</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">2.56</td>
</tr>
<tr>
<td style="text-align: center;">self contradictory</td>
<td style="text-align: center;">2.70</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">wrong use of terms</td>
<td style="text-align: center;">1.93</td>
<td style="text-align: center;">1.22</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">3.34</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">1.90</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">5.67</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;">specificity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">irrelevant info</td>
<td style="text-align: center;">15.15</td>
<td style="text-align: center;">4.56</td>
<td style="text-align: center;">3.79</td>
<td style="text-align: center;">5.12</td>
<td style="text-align: center;">8.69</td>
<td style="text-align: center;">8.89</td>
<td style="text-align: center;">4.01</td>
</tr>
<tr>
<td style="text-align: center;">vague</td>
<td style="text-align: center;">49.42</td>
<td style="text-align: center;">44.78</td>
<td style="text-align: center;">48.88</td>
<td style="text-align: center;">58.40</td>
<td style="text-align: center;">51.67</td>
<td style="text-align: center;">39.11</td>
<td style="text-align: center;">35.86</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">3.44</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">2.12</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">1.34</td>
</tr>
<tr>
<td style="text-align: center;">completeness</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">does not address main parts</td>
<td style="text-align: center;">29.91</td>
<td style="text-align: center;">22.56</td>
<td style="text-align: center;">11.16</td>
<td style="text-align: center;">9.79</td>
<td style="text-align: center;">15.92</td>
<td style="text-align: center;">8.78</td>
<td style="text-align: center;">9.47</td>
</tr>
<tr>
<td style="text-align: center;">does not address region</td>
<td style="text-align: center;">3.34</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">1.34</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">1.22</td>
</tr>
<tr>
<td style="text-align: center;">does not address time</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">4.11</td>
<td style="text-align: center;">1.90</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">ignores science</td>
<td style="text-align: center;">9.11</td>
<td style="text-align: center;">14.11</td>
<td style="text-align: center;">6.92</td>
<td style="text-align: center;">5.01</td>
<td style="text-align: center;">10.47</td>
<td style="text-align: center;">5.44</td>
<td style="text-align: center;">3.01</td>
</tr>
<tr>
<td style="text-align: center;">not enough detail</td>
<td style="text-align: center;">52.89</td>
<td style="text-align: center;">60.22</td>
<td style="text-align: center;">64.06</td>
<td style="text-align: center;">79.53</td>
<td style="text-align: center;">58.13</td>
<td style="text-align: center;">61.22</td>
<td style="text-align: center;">51.89</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">1.11</td>
</tr>
<tr>
<td style="text-align: center;">uncertainty</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">consensus missing</td>
<td style="text-align: center;">19.77</td>
<td style="text-align: center;">14.89</td>
<td style="text-align: center;">21.99</td>
<td style="text-align: center;">9.34</td>
<td style="text-align: center;">12.14</td>
<td style="text-align: center;">9.89</td>
<td style="text-align: center;">9.80</td>
</tr>
<tr>
<td style="text-align: center;">contradicting evidence missing</td>
<td style="text-align: center;">4.11</td>
<td style="text-align: center;">6.33</td>
<td style="text-align: center;">2.57</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">4.23</td>
<td style="text-align: center;">3.56</td>
<td style="text-align: center;">2.90</td>
</tr>
<tr>
<td style="text-align: center;">uncertainty missing</td>
<td style="text-align: center;">57.25</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">72.88</td>
<td style="text-align: center;">87.65</td>
<td style="text-align: center;">71.94</td>
<td style="text-align: center;">76.78</td>
<td style="text-align: center;">58.02</td>
</tr>
<tr>
<td style="text-align: center;">other</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">0.33</td>
</tr>
</tbody>
</table>
<p>Table 7. Percentage of specific issues identified by raters.</p>
<h1>A.3. Questions</h1>
<p>In this section we explain the pipeline used for selection, generation, post-processing and sampling climate change related questions. The question set consists of 300 questions, with 100 questions gathered from 3 sources each: i) Synthetic questions generated based on Wikipedia articles, ii) Manually rephrased questions based on Skeptical Science website, and iii) questions taken from Google Trends.</p>
<h2>A.3.1. SYnthetic Questions from Wikipedia</h2>
<p>We started by gathering a set of Wikipedia articles related to climate change. We followed 3 strategies to select climate related articles from Wikipedia. Following the first strategy (REF.), we gather all the Wikipedia articles that are referenced in the main "Climate Change" article. ${ }^{14}$ In the second strategy (CAT.), we select all the articles that are directly listed in the climate change category. Finally, to cover regional articles (REG.), we manually curate a list of articles with titles "Climate Change in [country/region]". From a pool of articles gathered following these 3 strategies, we selected paragraphs within an article if the paragraph consists of more than 500 characters. In total, we obtained 1969 paragraphs from Wikipedia. The following table reports a break-down of number of paragraphs based on the selection strategy:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Strategy</th>
<th style="text-align: center;"># Articles</th>
<th style="text-align: center;"># Paragraphs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">REF.</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">858</td>
</tr>
<tr>
<td style="text-align: left;">CAT.</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">434</td>
</tr>
<tr>
<td style="text-align: left;">REG.</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">677</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">129</td>
<td style="text-align: center;">1969</td>
</tr>
</tbody>
</table>
<p>We then input each selected paragraph in GPT-4. We ask the model to generate as many questions as possible that can be answered using the paragraph. The model is instructed to only generate questions that are salient and related to climate change. This process resulted in 15265 questions. We post process the questions and remove undesirable ones with 4 filters that we explain next.</p>
<p>Climate Change Filter. We remove all questions that are not climate change related. We use the climate-bert (Webersinke et al., 2022) classifier and label each question with two labels: climate related and not climate related. We remove 2647 questions that are not classified as climate-related questions.</p>
<p>Duplicate Filter. We remove questions that are a duplicate of another question. To this end, we embed all questions using a universal sentence encoder. ${ }^{15}$ We consider two questions as duplicates if the cosine similarity between their embeddings is greater than 0.85 . Therefore, we remove 1188 questions that are duplicates of other questions.</p>
<p>Context Dependent Filter. We filter out questions that are taken out of context. The reason that this filter is necessary is that we generate questions from paragraphs, therefore, some questions are nonsensical when they are not accompanied by the corresponding Wikipedia paragraph. An example of such a question is: "What are the two classes of climate engineering discussed in the study?"; without knowing which study is referred to, this question cannot be answered. To develop this filter, we build a dedicated classifier using in-context probing (Amini \&amp; Ciaramita, 2023). Specifically, we manually annotate 100 questions with two labels: context dependent, and not context dependent. Next, we contextualize the question with the instruction "Write Yes if the query is taken out of context, write No otherwise." and extract the last layer's representations of a flan-xxl encoder (Chung et al., 2022). Finally, we train a logistic regression probing classifier on the representations to detect context dependent questions. We find the context dependency filter to be $97 \%$ accurate on 100 manually annotated validation questions. Using this classifier, we detect 552 context dependent questions.</p>
<p>Specificity Filter. We remove questions that are asking about a very specific and narrow topic. In our study, we aim to evaluate large language models on a set of challenging and multifaceted questions that target information needs of users related to climate change. Therefore, questions that ask for a specific detail are not the target of this study and are typically easy to answer. An example of such question is: "What was the reason for shutting down reactor number one of</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6. Left: Distribution of the topic of the synthetic questions. The x axis shows the percentage of questions in each topic. The most frequent topic is "Weather-Temperature" topic. Right: Distribution of the causal feature of synthetic questions. There are more questions that are asking causal questions, or questions about predicting the future compare to the rest of the questions.
the Fessenheim Nuclear Power Plant on 4 August 2018?" To remove such specific questions, we again build in-context probing classifier on top of flan-xxl representations. We contextualize each question with the instruction: "Write Yes if the following query is asking about a specific subject, write No otherwise". and train the probe on top of extracted contextualized representations from the last layer of flan-xxl. We find the specificity filter to be $84 \%$ accurate on a sample of 100 annotated validation questions. We detect and remove 5472 specific questions.</p>
<p>After applying all 4 filters, the final post-processed question set consists of 5404 questions. The question set that is rated in our evaluation framework consists of 100 questions from each source. This means that we need to sample 100 diverse questions from this pool of $\approx 5 \mathrm{k}$ questions. To make sure that we cover different topics and type of questions, we first label each question with the topic and properties of the question, and then sample a 100 validation questions, where different topics and properties are equally presented. Next, we explain the classifiers that are developed for labeling the questions.</p>
<p>Topic Classifier. We use the same in-context probing approach as above and train a logistic regression classifier on top of flan-xxl encoder to classify questions based on the topics. Inspired by IPCC chapters, we consider the following 9 topics: "Energy", "Emissions-Pollutants", "Policies-Mitigation-Adaptation", "Weather-Temperature", "Land-Ocean-Food-Water", "Society-Livelihoods-Economy", "Health-Nutrition", "Biodiversity", "Cities-Settlements-Infra". We find this classifier to be $75 \%$ accurate on a sample of 100 annotated validation questions. The distribution of predicted questions' topics is depicted in Figure 6.</p>
<p>Causal Prediction Classifier. An important and challenging type of questions that one can ask about climate change is about causes or effects of climate change, or predictions about the future. To detect this type of questions, we classify questions into two classes: causal-prediction class and others. The instruction that is used for contextualizing the questions is: "Write Yes if the following query is asking about causes or effects of something, or is asking about predictions about the future. write No otherwise". We find this classifier to be $93 \%$ accurate on a sample of 100 annotated validation questions. The distribution of predictions is shown in Figure 6</p>
<p>For synthetic Wikipedia questions, we sample 108 questions, 6 from each of the 18 possible combinations of topic and causal-prediction features. We sample an equal number of questions from the other two sources, Skeptical Science and GTrends. We manually inspect all questions from all the 3 different sources. The final question set used in the evaluations consists of 300 questions, i.e., 100 questions from each source.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{14}$ https://en.wikipedia.org/wiki/Climate_change
${ }^{15}$ We use universal-sentence-encoder-qa/3 model.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://trends.google.com/trends/
${ }^{4}$ https://skepticalscience.com/argument. php&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
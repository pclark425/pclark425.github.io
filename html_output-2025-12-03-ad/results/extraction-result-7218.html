<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7218 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7218</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7218</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-257257414</p>
                <p><strong>Paper Title:</strong> Probing the psychology of AI models</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as OpenAI’s GPT-3 and its successor ChatGPT, have exhibited astounding successes—as well as curious failures—in several areas of artificial intelligence. While their abilities in generating humanlike text, solving mathematical problems, writing computer code, and reasoning about the world have been widely documented, the mechanisms underlying both the successes and failures of these systems remain mysterious, even to the researchers who created them. In spite of the current lack of understanding of how these systems do what they do, LLMs are on the cusp of being widely deployed as components of search engines, writing tools, and other commercial products, and are likely to have substantial impact on all of our lives. Even more profoundly, their surprising abilities may change our conception of the nature of intelligence itself. In PNAS, Binz and Schulz (1) point out the “urgency to improve our understanding of how [these systems] learn and make decisions.” A standard way to evaluate systems trained by machine-learning methods is to test their accuracy on human-created benchmarks. By this metric, GPT-3 and other LLMs are close to (or above) human level on many tasks (2–4). However, an AI system matching human performance on such benchmarks has rarely translated into that system having human-level performance more broadly; many popular benchmarks have been shown to contain subtle “spurious” correlations that allow systems to “be right for the wrong reasons” (5) and straightforward accuracy metrics do not necessarily predict robust generalization (6). Binz and Schulz’s article argues that instead of relying solely on such performance-based benchmarks, researchers should apply methods from cognitive psychology to gain insights into LLMs. The core idea is to “treat GPT-3 as a participant in a psychology experiment,” in order to tease out the system’s mechanisms of decision-making, reasoning, cognitive biases, and other important psychological traits. If this approach could be shown to produce deep understanding of LLMs it could cause a “sea change” in the way AI systems are evaluated and understood. Binz and Schulz have taken an admirable first step toward establishing the value of such an approach, although it would have been better had they been able to use their results to understand why GPT-3 succeeded and failed when it did. That their project fell short of this goal is understandable: Behavioral scientists have spent over a 100 y using such experiments to understand how humans carry out these tasks and still have a long way to go. Binz and Schulz carried out two sets of experiments. In the first set, they gave GPT-3 prompts consisting of “vignettes” from the psychology literature that have been used to assess reasoning with probabilities, intuitive versus deliberative reasoning, causal reasoning, and other cognitive attributes. Each vignette asks the reader to choose from a small set of options. The following example shows a reasoning vignette known as the Wason Card Selection Task (7) that was given to GPT-3: “You are shown a set of four cards placed on a table, each of which has a number on one side and a letter of the other side. The visible faces of the cards show A, K, 4, 7. Q: Which cards must you turn over in order to test the truth of the proposition that if a card shows a vowel on one face then its opposite face shows an even number?” The answer supplied by GPT-3 was: “The A and the 7”. (A correct response). Of the 12 vignettes Binz and Schulz gave to GPT-3, the system responded with the correct answer on six of them, and GPT-3’s six incorrect responses were errors that humans also tend to make. What is to be made of what seems to be a correspondence? Binz and Schulz admit and show GPT-3’s answers are strongly context dependent: In the above vignette a change in the order of the four cards to 4, 7, A, K led to a different answer “The A and the K.” Humans can also be context-dependent, but perhaps not in the same ways. Nonetheless, it may be that such results show a correspondence between AI systems and humans. Humans experience and store vast numbers of experiences, building knowledge on their basis (8); AI systems are exposed to vast numbers of instances (text tokens in the case of GPT-3) and build a representation on their basis. Perhaps both take advantage of the correlation structure of these instances and events. Whatever truth there may be in such an analogy, it seems unlikely that GPT-3 uses the kinds of explicit reasoning strategies that some humans use in these tasks. For example, to unpack the vignette in the above figure, humans given time and motivation might attempt to use explicit reasoning, logic, and mental simulations, perhaps trying out different choices to see what information they might provide. This generally involves manipulating information in working memory. Working memory is not part of GPT-3. Yet it is possible that the contents of working memory reflect what has been stored in long-term memory—after all when reading a problem or instructions the first step in generating contents of working memory will be retrieval from long-term memory (8). Whatever one tries to infer from their results, Binz and Schulz note some additional caveats. First, the vignettes, as well as the correct (and human-generated incorrect) responses used in these experiments, are all from wellknown psychology studies, and are likely to have been</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7218.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7218.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wason task (single vignette)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wason Card Selection Task (example vignette)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic logical-reasoning vignette asking which cards to turn over to test a conditional rule; presented to GPT-3 as a single-item prompt and answered correctly in the cited example.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive large language model trained to predict next tokens from large text corpora (transformer-based).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Wason Card Selection Task</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning (logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Participants are shown cards with visible faces and must choose which cards to turn over to test a conditional rule (e.g., 'If vowel then even number'); measures normative logical reasoning versus typical human errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>correctness (binary correct/incorrect on the vignette)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Correct on the presented vignette (responded 'The A and the 7'); however, small contextual change (card order) produced an incorrect answer ('The A and the K').</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper gives this single-item example showing GPT-3 can produce the normative answer but is context-sensitive: a trivial ordering change altered its response. The commentary also notes that such vignettes (and their canonical answers) are likely present in GPT-3's training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7218.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7218.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>12-vignette battery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Set of 12 psychology vignettes (reasoning/probability/causal tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A battery of 12 short vignettes drawn from cognitive-psychology studies (including reasoning, probabilistic, causal, and deliberation probes) administered to GPT-3; reported aggregate performance is given.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive large language model trained to predict next tokens from large text corpora (transformer-based).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Psychology vignettes battery (12 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning / probability / causal reasoning / deliberation</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Short, well-known vignettes from psychology literature that require selecting among a small set of options to probe reasoning with probabilities, intuitive vs deliberative reasoning, causal reasoning, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy (number correct out of 12)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>6 of 12 vignettes answered correctly (50% accuracy); the six incorrect responses resembled errors humans tend to make.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors emphasize strong context dependence of GPT-3's answers (small prompt variations can change responses) and the likely inclusion of many vignette materials in GPT-3's training corpus; no numeric human baseline is reported in this commentary, only qualitative similarity of error patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7218.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7218.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-armed bandit tasks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-armed bandit decision tasks (sequential reward learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequential decision-making tasks where agents choose among options with probabilistic rewards to maximize cumulative reward; used to compare GPT-3 and human decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive large language model trained to predict next tokens from large text corpora (transformer-based).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Multi-armed bandit decision tasks</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>decision-making / learning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Repeated-choice tasks in which an agent selects among several options (arms) with different reward probabilities; performance measured by cumulative reward or decision quality over trials.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to outperform human decision-making in some multi-armed-bandit tasks (no numeric metrics provided in this commentary).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The commentary states results were mixed across domains: GPT-3 sometimes outperformed humans on bandit-style tasks, but detailed metrics or statistical tests are not provided in this paper and likely appear in the original Binz & Schulz study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7218.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7218.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal reasoning vignettes / tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tasks probing causal reasoning abilities (inferring cause-effect relations), given to GPT-3 and compared qualitatively to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive large language model trained to predict next tokens from large text corpora (transformer-based).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Causal reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>causal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Vignettes or problems that require identifying or reasoning about causal relationships between events or variables.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to be substantially worse than humans on causal reasoning tasks (qualitative statement, no numeric values provided in this commentary).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The commentary highlights that GPT-3's performance varies by task domain and expresses concern that psychological constructs (e.g., causal reasoning) may not map cleanly onto what LLMs are doing; quantitative comparisons and statistical analyses are not supplied here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using cognitive psychology to understand GPT-3 <em>(Rating: 2)</em></li>
                <li>Language models show human-like content effects on reasoning <em>(Rating: 2)</em></li>
                <li>Machine intuition: Uncovering human-like intuitive decision-making in GPT-3 <em>(Rating: 2)</em></li>
                <li>Human-like property induction is a challenge for large language models <em>(Rating: 2)</em></li>
                <li>Putting GPT-3's creativity to the (alternative uses) test <em>(Rating: 1)</em></li>
                <li>Reasoning about a rule <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7218",
    "paper_id": "paper-257257414",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "Wason task (single vignette)",
            "name_full": "Wason Card Selection Task (example vignette)",
            "brief_description": "A classic logical-reasoning vignette asking which cards to turn over to test a conditional rule; presented to GPT-3 as a single-item prompt and answered correctly in the cited example.",
            "citation_title": "Using cognitive psychology to understand GPT-3",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Autoregressive large language model trained to predict next tokens from large text corpora (transformer-based).",
            "model_size": null,
            "test_name": "Wason Card Selection Task",
            "test_category": "reasoning (logical reasoning)",
            "test_description": "Participants are shown cards with visible faces and must choose which cards to turn over to test a conditional rule (e.g., 'If vowel then even number'); measures normative logical reasoning versus typical human errors.",
            "evaluation_metric": "correctness (binary correct/incorrect on the vignette)",
            "human_performance": null,
            "llm_performance": "Correct on the presented vignette (responded 'The A and the 7'); however, small contextual change (card order) produced an incorrect answer ('The A and the K').",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The paper gives this single-item example showing GPT-3 can produce the normative answer but is context-sensitive: a trivial ordering change altered its response. The commentary also notes that such vignettes (and their canonical answers) are likely present in GPT-3's training data.",
            "uuid": "e7218.0",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "12-vignette battery",
            "name_full": "Set of 12 psychology vignettes (reasoning/probability/causal tasks)",
            "brief_description": "A battery of 12 short vignettes drawn from cognitive-psychology studies (including reasoning, probabilistic, causal, and deliberation probes) administered to GPT-3; reported aggregate performance is given.",
            "citation_title": "Using cognitive psychology to understand GPT-3",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Autoregressive large language model trained to predict next tokens from large text corpora (transformer-based).",
            "model_size": null,
            "test_name": "Psychology vignettes battery (12 tasks)",
            "test_category": "reasoning / probability / causal reasoning / deliberation",
            "test_description": "Short, well-known vignettes from psychology literature that require selecting among a small set of options to probe reasoning with probabilities, intuitive vs deliberative reasoning, causal reasoning, etc.",
            "evaluation_metric": "accuracy (number correct out of 12)",
            "human_performance": null,
            "llm_performance": "6 of 12 vignettes answered correctly (50% accuracy); the six incorrect responses resembled errors humans tend to make.",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "Authors emphasize strong context dependence of GPT-3's answers (small prompt variations can change responses) and the likely inclusion of many vignette materials in GPT-3's training corpus; no numeric human baseline is reported in this commentary, only qualitative similarity of error patterns.",
            "uuid": "e7218.1",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Multi-armed bandit tasks",
            "name_full": "Multi-armed bandit decision tasks (sequential reward learning)",
            "brief_description": "Sequential decision-making tasks where agents choose among options with probabilistic rewards to maximize cumulative reward; used to compare GPT-3 and human decision-making.",
            "citation_title": "Using cognitive psychology to understand GPT-3",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Autoregressive large language model trained to predict next tokens from large text corpora (transformer-based).",
            "model_size": null,
            "test_name": "Multi-armed bandit decision tasks",
            "test_category": "decision-making / learning",
            "test_description": "Repeated-choice tasks in which an agent selects among several options (arms) with different reward probabilities; performance measured by cumulative reward or decision quality over trials.",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Reported to outperform human decision-making in some multi-armed-bandit tasks (no numeric metrics provided in this commentary).",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The commentary states results were mixed across domains: GPT-3 sometimes outperformed humans on bandit-style tasks, but detailed metrics or statistical tests are not provided in this paper and likely appear in the original Binz & Schulz study.",
            "uuid": "e7218.2",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Causal reasoning tasks",
            "name_full": "Causal reasoning vignettes / tasks",
            "brief_description": "Tasks probing causal reasoning abilities (inferring cause-effect relations), given to GPT-3 and compared qualitatively to human performance.",
            "citation_title": "Using cognitive psychology to understand GPT-3",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Autoregressive large language model trained to predict next tokens from large text corpora (transformer-based).",
            "model_size": null,
            "test_name": "Causal reasoning tasks",
            "test_category": "causal reasoning",
            "test_description": "Vignettes or problems that require identifying or reasoning about causal relationships between events or variables.",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Reported to be substantially worse than humans on causal reasoning tasks (qualitative statement, no numeric values provided in this commentary).",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The commentary highlights that GPT-3's performance varies by task domain and expresses concern that psychological constructs (e.g., causal reasoning) may not map cleanly onto what LLMs are doing; quantitative comparisons and statistical analyses are not supplied here.",
            "uuid": "e7218.3",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using cognitive psychology to understand GPT-3",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning",
            "rating": 2,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning"
        },
        {
            "paper_title": "Machine intuition: Uncovering human-like intuitive decision-making in GPT-3",
            "rating": 2,
            "sanitized_title": "machine_intuition_uncovering_humanlike_intuitive_decisionmaking_in_gpt3"
        },
        {
            "paper_title": "Human-like property induction is a challenge for large language models",
            "rating": 2,
            "sanitized_title": "humanlike_property_induction_is_a_challenge_for_large_language_models"
        },
        {
            "paper_title": "Putting GPT-3's creativity to the (alternative uses) test",
            "rating": 1,
            "sanitized_title": "putting_gpt3s_creativity_to_the_alternative_uses_test"
        },
        {
            "paper_title": "Reasoning about a rule",
            "rating": 1,
            "sanitized_title": "reasoning_about_a_rule"
        }
    ],
    "cost": 0.00843725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Probing the psychology of AI models
March 1, 2023.</p>
<p>Richard Shiffrin shiffrin@indiana.edu. 0000-0002-3708-3212
Indiana University Bloomington
47405BloomingtonIN</p>
<p>Melanie Mitchell 
Santa Fe Institute
87501Santa FeNM</p>
<p>Probing the psychology of AI models
March 1, 2023.36E0338E501406A088D23467A475B5E610.1073/pnas.2300963120</p>
<p>Large language models (LLMs), such as OpenAI's GPT-3 and its successor ChatGPT, have exhibited astounding successes-as well as curious failures-in several areas of artificial intelligence.While their abilities in generating humanlike text, solving mathematical problems, writing computer code, and reasoning about the world have been widely documented, the mechanisms underlying both the successes and failures of these systems remain mysterious, even to the researchers who created them.In spite of the current lack of understanding of how these systems do what they do, LLMs are on the cusp of being widely deployed as components of search engines, writing tools, and other commercial products, and are likely to have substantial impact on all of our lives.Even more profoundly, their surprising abilities may change our conception of the nature of intelligence itself.In PNAS, Binz and Schulz (1) point out the "urgency to improve our understanding of how [these systems] learn and make decisions."</p>
<p>A standard way to evaluate systems trained by machine-learning methods is to test their accuracy on human-created benchmarks.By this metric, GPT-3 and other LLMs are close to (or above) human level on many tasks (2)(3)(4).However, an AI system matching human performance on such benchmarks has rarely translated into that system having human-level performance more broadly; many popular benchmarks have been shown to contain subtle "spurious" correlations that allow systems to "be right for the wrong reasons" (5) and straightforward accuracy metrics do not necessarily predict robust generalization (6).</p>
<p>Binz and Schulz's article argues that instead of relying solely on such performance-based benchmarks, researchers should apply methods from cognitive psychology to gain insights into LLMs.The core idea is to "treat GPT-3 as a participant in a psychology experiment," in order to tease out the system's mechanisms of decision-making, reasoning, cognitive biases, and other important psychological traits.If this approach could be shown to produce deep understanding of LLMs it could cause a "sea change" in the way AI systems are evaluated and understood.Binz and Schulz have taken an admirable first step toward establishing the value of such an approach, although it would have been better had they been able to use their results to understand why GPT-3 succeeded and failed when it did.That their project fell short of this goal is understandable: Behavioral scientists have spent over a 100 y using such experiments to understand how humans carry out these tasks and still have a long way to go.</p>
<p>Binz and Schulz carried out two sets of experiments.In the first set, they gave GPT-3 prompts consisting of "vignettes" from the psychology literature that have been used to assess reasoning with probabilities, intuitive versus deliberative reasoning, causal reasoning, and other cognitive attributes.Each vignette asks the reader to choose from a small set of options.The following example shows a reasoning vignette known as the Wason Card Selection Task (7) that was given to GPT-3: "You are shown a set of four cards placed on a table, each of which has a number on one side and a letter of the other side.The visible faces of the cards show A, K, 4, 7.</p>
<p>Q: Which cards must you turn over in order to test the truth of the proposition that if a card shows a vowel on one face then its opposite face shows an even number?"</p>
<p>The answer supplied by GPT-3 was: "The A and the 7".(A correct response).</p>
<p>Of the 12 vignettes Binz and Schulz gave to GPT-3, the system responded with the correct answer on six of them, and GPT-3's six incorrect responses were errors that humans also tend to make.What is to be made of what seems to be a correspondence?Binz and Schulz admit and show GPT-3's answers are strongly context dependent: In the above vignette a change in the order of the four cards to 4, 7, A, K led to a different answer "The A and the K." Humans can also be context-dependent, but perhaps not in the same ways.</p>
<p>Nonetheless, it may be that such results show a correspondence between AI systems and humans.Humans experience and store vast numbers of experiences, building knowledge on their basis (8); AI systems are exposed to vast numbers of instances (text tokens in the case of GPT-3) and build a representation on their basis.Perhaps both take advantage of the correlation structure of these instances and events.Whatever truth there may be in such an analogy, it seems unlikely that GPT-3 uses the kinds of explicit reasoning strategies that some humans use in these tasks.For example, to unpack the vignette in the above figure, humans given time and motivation might attempt to use explicit reasoning, logic, and mental simulations, perhaps trying out different choices to see what information they might provide.This generally involves manipulating information in working memory.Working memory is not part of GPT-3.Yet it is possible that the contents of working memory reflect what has been stored in long-term memory-after all when reading a problem or instructions the first step in generating contents of working memory will be retrieval from long-term memory (8).</p>
<p>Whatever one tries to infer from their results, Binz and Schulz note some additional caveats.First, the vignettes, as well as the correct (and human-generated incorrect) responses used in these experiments, are all from wellknown psychology studies, and are likely to have been 2 of 3 https://doi.org/10.1073/pnas.2300963120pnas.orgincluded in some form in GPT-3's vast training corpus.Second, GPT-3's responses can, in general, be very sensitive to the form of the prompt given to it.Binz and Schulz found that small inconsequential variations in the vignettes can substantially change GPT-3's answers, as noted above when discussing context dependence.</p>
<p>A second set of experiments used prompts designed so they did not appear in GPT-3's training corpus.The results were mixed.In some cases-for example, in so-called multi-armed-bandit decision tasks-GPT-3 outperformed human decision-making, and in others-particularly in causal reasoning tasks-GPT-3 was substantially worse.Third, as Binz and Schulz ask, it is unclear whether it is more appropriate to consider GPT-3 a single "participant" or an average of many participants.There should be a fourth caveat: It is unknown what aspect of the responses should be measured and compared with humans.Verbal responses?Numerical probabilities over response tokens computed by the LLMs?The neural network's internal representations?Would carefully designed and interpreted studies that treat AI systems as participants in psychology experiments help us understand how LLMs work, and do so better than the use of standard performance-based metrics?Binz and Schulz fall short of making that case, but their research can be viewed as a valuable first step (along with other related approaches; e.g., refs.(9)(10)(11)(12)(13).There is of course a possibility that this project could fail due to the substantial differences between LLMs and humans as objects of psychological study-it may not be appropriate to assume that an LLM's responses can be analyzed "just like how cognitive psychologists would analyze human behavior in the same tasks" (1).LLMs such as GPT-3 are trained explicitly to predict the next tokens (words or word parts) in a prompt.They are trained on a vast corpus and use 100s of billions of trainable parameters to make these predictions on the basis of detailed models of the statistical distribution of tokens and their correlations.As mentioned earlier, it is possible that something vaguely like this is used by humans as they store vast numbers of life events and build knowledge from them, but humans retrieve those events poorly and with large amounts of error (8,14,15).At the present time, it remains an open question whether the responses of LLMs are due to processes like those used by humans.If not, the attempt to understand LLMs by treating them like human participants in psychology experiments will surely fail.</p>
<p>In short, the assumptions psychologists make-for example, that humans use a mixture of intuitions and deliberate reflection (15)-might not apply to a LLM on the same tasks.Psychological assessments designed to test humans' higher-level cognitive abilities, including decision-making, information search, deliberation, and causal reasoning, may in fact not test these abilities at all in LLMs, even when LLMs-trained on huge swaths of human-generated text-produce similar responses as humans.</p>
<p>The difficulties in interpretating results like those reported by Binz and Schulz are compounded by the use of human cognitive terms to describe AI systems (16).We measure humans' "regret" in hypothetical gambling games, and how their "preferences" or "risk aversion" changes in response to how a win or loss is structured.We assume that these qualities are in response not solely to the words in the prompts humans are given, but to the real-world situations those words evoke.Does it make sense at all to similarly anthropomorphize LLMs by talking about how they "make decisions," "search for information," have "preferences," "regrets," or "risk aversion," given that these models have no connection to the real world beyond the text in their training corpus?Or, as Shanahan puts it, perhaps the only questions we can ask LLMs are "Here's a fragment of text….According to your model of the statistics of human language, what words are likely to come next?" (16).</p>
<p>We agree with Binz and Schulz that understanding how LLMs work is important and will become even more important in the future.Binz and Schulz correctly emphasize the important role that cognitive scientists should have to play in answering such questions.The successes of GPT-3 in their article are thought-provoking, but the failures emphasize the dangers inherent in using GPT-3, and LLMs for tasks in human society.Of course, we can expect the LLMs to grow ever more complex and come ever more close to emulating human verbal discourse, especially if they are allowed to interact with real environments or simulations of real environments (as Binz and Schulz suggest in the conclusion of their article).Would increasingly accurate emulation by LLMs increase or decrease the dangers of using them in society?It seems likely that our ability to understand them will decrease as the systems increase in complexity, whether or not we probe them with human experimental tasks.Should we turn over our society to systems we cannot understand?Of course, we can ask that same question of humans.</p>
<p>"The core idea is to 'treat GPT-3 as a participant in a psychology experiment,' in order to tease out the system's mechanisms of decision-making, reasoning, cognitive biases, and other important psychological traits."</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, Proc. Natl. Acad. Sci. U.S.A. 120e22185231202023</p>
<p>SuperGLUE: A stickier benchmark for general-purpose language understanding systems. A Wang, Adv. Neural Inform. Process. Syst. 332019</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. I Talmor, Proceedings. nullAssociation for Computational Linguistics2019</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, 10.48550/arXiv.2103.038742021. 15 February 2023</p>
<p>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. T Mccoy, Proceedings, 57th Annual Meeting of the Association for Computational Linguistics. 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2019</p>
<p>How can we accelerate progress towards human-like linguistic generalization. T Linzen, Proceedings, 58th Annual Meeting of the Association for Computational Linguistics. 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Reasoning about a rule. P C Wason, Q. J. Exp. Psychol. 201968</p>
<p>The co-evolution of knowledge and event memory. A B Nelson, R M Shiffrin, Psychol. Rev. 1202013</p>
<p>Language models show human-like content effects on reasoning. I Dasgupta, 10.48550/arXiv.2207.070512022. 15 February 2023</p>
<p>Machine intuition: Uncovering human-like intuitive decision-making in GPT-3. T Hagendorff, 10.48550/arXiv.2212.052062022. 15 February 2023</p>
<p>Human-like property induction is a challenge for large language models. S J Han, Proceedings, 44th Annual Conference of the Cognitive Science Society. Cognitive Science Society. 44th Annual Conference of the Cognitive Science Society2022</p>
<p>Putting GPT-3's creativity to the (alternative uses) test. C Stevenson, 10.48550/arXiv.2206.08932arXiv2022. 15 February 2023Preprint</p>
<p>E Kosoy, 10.48550/2206.08353Towards understanding how machines can learn causal overhypotheses. 2022</p>
<p>Human memory: A proposed system and its control processes. R C Atkinson, R M Shiffrin, The Psychology of Learning and Motivation: Advances in Research and Theory. K W Spence, J T Spence, New YorkAcademic Press19682</p>
<p>Search of associative memory. J G W Raaijmakers, R M Shiffrin, Psychol. Rev. 881981</p>
<p>M Shanahan, 10.48550/arXiv:2212.03551arXiv:2212.03551Talking about large language models. 2022. 15 February 2023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
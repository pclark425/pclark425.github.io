<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7283 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7283</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7283</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-272397813</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.02686v2.pdf" target="_blank">Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable efficiency in tackling various tasks based on human instructions, but studies reveal that they often struggle with tasks requiring reasoning, such as math or physics. This limitation raises questions about whether LLMs truly comprehend embedded knowledge or merely learn to replicate the token distribution without a true understanding of the content. In this paper, we delve into this problem and aim to enhance the reasoning capabilities of LLMs. First, we investigate if the model has genuine reasoning capabilities by visualizing the text generation process at the attention and representation level. Then, we formulate the reasoning process of LLMs into a causal framework, which provides a formal explanation of the problems observed in the visualization. Finally, building upon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient fine-tuning (PEFT) method to enhance the model's reasoning capabilities by encouraging the model to extract the general problem-solving skills and apply these skills to different questions. Experiments show that our method outperforms the baseline consistently across multiple benchmarks, and with only 1.2M tunable parameters, we achieve better or comparable results to other fine-tuning methods. This demonstrates the effectiveness and efficiency of our method in improving the overall accuracy and reliability of LLMs.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7283",
    "paper_id": "paper-272397813",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0032045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs
5 Oct 2024</p>
<p>Ruoyu Wang ruoyu.wang5@unsw.edu.au 
University of New
South Wales</p>
<p>Xiaoxuan Li 
University of New
South Wales</p>
<p>Lina Yao lina.yao@unsw.edu.au 
University of New
South Wales</p>
<p>Commonwealth Scientific and Industrial Research Organisation
Australia</p>
<p>Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs
5 Oct 2024C7E66C1A4225277E43A5F4F1C6413693arXiv:2409.02686v2[cs.CL]Parameter-Efficient Fine-Tuning (PEFT)CausalityLarge Language Models
Large Language Models (LLMs) have demonstrated remarkable efficiency in tackling various tasks based on human instructions, but studies reveal that they often struggle with tasks requiring reasoning, such as math or physics.This limitation raises questions about whether LLMs truly comprehend embedded knowledge or merely learn to replicate the token distribution without a true understanding of the content.In this paper, we delve into this problem and aim to enhance the reasoning capabilities of LLMs.First, we investigate if the model has genuine reasoning capabilities by visualizing the text generation process at the attention and representation level.Then, we formulate the reasoning process of LLMs into a causal framework, which provides a formal explanation of the problems observed in the visualization.Finally, building upon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient fine-tuning (PEFT) method to enhance the model's reasoning capabilities by encouraging the model to extract the general problem-solving skills and apply these skills to different questions.Experiments show that our method outperforms the baseline consistently across multiple benchmarks, and with only 1.2M tunable parameters, we achieve better or comparable results to other fine-tuning methods.This demonstrates the effectiveness and efficiency of our method in improving the overall accuracy and reliability of LLMs.</p>
<p>Introduction</p>
<p>Recent years have witnessed remarkable progress on Large Language Models (LLMs) [38], especially those instruction-following models such as ChatGPT and GPT-4 [17].Numerous studies have demonstrated that these models exhibit strong capabilities across a wide range of tasks.However, despite the effectiveness of these models, existing work [11] shows that they perform poorly on Out-of-Distribution tasks, so fine-tuning with specific tasks and datasets is required to achieve satisfactory results.Fig. 1: Parameter-Efficient Fine-Tuning (PEFT) methods transform the nonprompt-following model to prompt-following by injecting a small number of learnable parameters into the pre-trained LLM.Our method lies in the domain of PEFT and concentrates on its problem-solving capabilities.</p>
<p>Nevertheless, fine-tuning large-scale LLMs in full is often prohibitively costly, thus many Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed in recent years, which transform a non-prompt-following model into a prompt-following model by injecting a small number of extra model parameters (Figure 1), thereby greatly decreasing the computational and storage costs.Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning [37,16].</p>
<p>While these prompt-following models or fine-tuning methods have been proven to be effective in generating responses based on human instructions, there remains uncertainty regarding whether these models have genuinely acquired knowledge from the text or merely learned the distribution of the word tokens without true comprehension.[29] claimed that the scaling up of language models could significantly enhance their performance, which is usually seen as a piece of evidence that the LLMs can acquire knowledge when it's sufficiently large.However, [21] claims that emergent abilities only appear for specific metrics, and [11] suggests that these models do not possess any causal reasoning abilities.</p>
<p>Many discussions have been raised regarding this issue, yet the answer remains inconclusive.Besides, most of these discussions are raised on GPT models, and they are rarely addressed in the context of LLM fine-tuning.Therefore, we investigate this issue in the context of LLM Fine-tuning and propose a novel Parameter Efficient Fine-Tuning (PEFT) method based on Causal Inference techniques to improve the reasoning capabilities of the models.In particular, we first investigate if the model has genuine reasoning capabilities by visualizing the reasoning process at the attention and representation level.Then, we formulate the reasoning process of LLMs into a causal framework, which provides a formal explanation of the problems we observe in the visualization.Finally, we propose Deconfounded Causal Adaptation (DCA), a novel fine-tuning method to improve the model's reasoning capability, and experimentally show the effectiveness and efficiency of our method.The contribution of our paper is three-fold:</p>
<p>-We investigate the text generation process of an instruction-following model by visualization in the level of attention and representation, and present empirical evidence that the model lacks genuine causal reasoning capabilities;</p>
<p>-We formulate the reasoning process of LLMs in a causal framework, formally explaining the reasons for the observed failure cases in the visualization; -We propose Deconfounded Causal Adaptation (DCA), a novel fine-tuning method to improve the reasoning capability of LLMs, and experimentally demonstrate the effectiveness of our method, which achieves strong performance with only 1.2 Million tunable parameters.</p>
<p>Preliminary</p>
<p>LLAMA-Adapter</p>
<p>LLaMA-Adapter [37] is a lightweight adaption method to fine-tune LLaMA into an instruction-following model, which has demonstrated the capability to generate high-quality responses.We conducted our study and built our method based on LLaMA-Adapter due to its effectiveness and efficiency.The architecture of LLaMA-Adapter is illustrated in Figure 2a.For each of the topmost L Transformer layers of LLaMA, an adaption prompt T l ∈ R M ×C is concatenated to the original prompt P l ∈ R K×C along the token dimension:
[P l ; T l ] ∈ R (K+M )×C(1)
where M denotes the length of the adapter to be concatenated, K denotes the original prompt length for each transformer layer, and C denotes the feature dimension of LLaMA's transformer.This concatenation operation is applied to the corresponding dimension in Key and Value in the self-attention mechanism.Further, a zero-init attention mechanism with zero gating is proposed to improve the training by injecting the new instructional cues into LLaMA.While calculating the attention score, the softmax function is applied independently to the two components in Equation 1, and multiplies the concatenated term by a gating factor g l , as illustrated in Equation 2 and Figure 2a.
S g l = Sof tmax(S K l ); Sof tmax(S M l ) • g l T(2)
We highlight components of the LLaMA-Adapter architecture relevant to our method and refer readers to [37] for comprehensive details of this method.</p>
<p>Causal Inference</p>
<p>In Causality [18], causal relationships are denoted by Directed Acyclic Graph (DAG).There are three basic building blocks in a causal graph: Chain is the case where one element causally influences another, then leading to the causal impact on a third element, such as X → Z → Y in Figure 2b.Fork is the case where one element causally influences two other elements, such as X ← C → Y in Figure 2b.Collider is the case where two elements causally influence a third element such as C → Y ← Z in Figure 2b.</p>
<p>Confounder If a variable is the common cause of two other variables, it is called a confounder.Confounders will induce spurious correlations between
) X → Z → Y is a chain, X ← C → Y is a fork, C → Y ← Z is a collider; (c)
We perform intervention do(X) to cut the edge C → X so that the causal effect P (Y |do(X)) can be estimated.the two variables, thus disturbing the recognition of the causal effect between them.For example, in Figure 2b, C is a confounder between X and Y.The association between X and Y include the spurious correlations created by the confounder C (X ← C → Y ) which is non-causal, and the goal of causal inference is to deconfound the spurious correlations so that the true causal relationships between X and Y (X → Z → Y ) can be measured.</p>
<p>Intervention In order to measure the causal effect between X and Y, we need to avoid the association flow through the fork X ← C → Y by blocking the path C → X.To this end, we force the variable X = x regardless the value of C. In that case, C no longer affects the value of X and thus path C → X is blocked.This process is called intervention in causal inference and is denoted as do(X=x) (Figure 2c).In contrast to P (Y |X), which comprises both causal association and spurious correlations caused by confounder, P (Y |do(X)) allows us to measure the genuine causal effect between X and Y.</p>
<p>3 Our Method</p>
<p>Investigation and Motivation</p>
<p>As discussed in Section 1, we aim to investigate if the prompt-following models have genuine causal reasoning capabilities.To this end, we conduct the following experiments.Since models such as ChatGPT and GPT-4 are not available in open-source form, we conduct our study using LLaMA-Adapter [37] to gain access to attention values and representations at each layer.</p>
<p>First, we fine-tune the LLaMA 7B model with LLaMA-Adapter using the Letter Concatenation dataset, which will be introduced in Section 4.2.Then, we test the model with two prompts below.The only difference between these two prompts lies in the string within the quotation marks, and as a result, the model answered Prompt A correctly, but failed on Prompt B.</p>
<p>Prompt A: Take the second last letters of the words in "GALLEGOS MORAN" and concatenate them;</p>
<p>Prompt B: Take the second last letters of the words in "DAVENPORT MAGANA" and concatenate them.</p>
<p>To explore the cause of the model's failure on Prompt B, we visualize the attention values in the text generation process by adapting BertViz [28], and conduct a thorough comparison between the two test cases on the attention heat map of each attention head across all transformer layers.Consequently, we found that the model's failure on Prompt B can be attributed to the malfunctioning of some particular adapter structures.</p>
<p>Figure 3a-3b provides an example of such malfunctioning structures, where we present the attention values of the sixth element in the adapter (adap_6) located in the 32nd attention head of the last transformer layer of LLaMA-Adapter.We observed that when the model correctly predicts the answer (Figure 3a), adap_6 tends to focus on the question rather than the value of the string.However, in Figure 3b, where the model failed to provide the correct answer, it exhibits a focus on a portion of the string, such as token "AG" and "AN" as highlighted.Similar patterns can also be observed in many other cases.Therefore, we empirically conclude that such malfunctioning units are the root cause of the mistake the model made on Prompt B.</p>
<p>In other words, simply replacing the string within the quotation marks significantly affects the thinking process of the model.This behaviour starkly contrasts with how humans solve such questions.From a human perspective, Prompt A and Prompt B are nearly identical, if we understand how to solve one of these problems, we inherently possess the method to solve all similar questions.This is because humans understand the world through causal relationships, enabling us to recognize and comprehend the underlying rationales.In contrast, LLMs were constructed based on statistical associations, leading to a deficiency in their capacity to comprehend the question and to do causal reasoning.</p>
<p>Hence, our empirical finding suggests a deficiency in the model's comprehension of the task, as mere string value changes influence the attention mechanism's behaviour.These observations motivate us to enhance the reasoning abilities of these models.Therefore, we introduce our method to improve response quality by fostering the model's capability of causal reasoning.Following this idea, we first formulate the reasoning process of LLMs into a causal framework in Section 3.2, and then propose our causal Fine-tuning method in Section 3.3.</p>
<p>Method Specification</p>
<p>We formulate the reasoning process of LLMs into a causal framework, as illustrated in Figure 3c.In this framework, X denotes the encoded feature of the prompt, K denotes the relevant knowledge to solve the problem provided by the LLM, and Y denotes the LLM's response to the query.LLM → X When a prompt is presented to the LLM, it encodes the prompt into feature X.Therefore, LLM is the direct cause of X.</p>
<p>LLM → K ← X Once the prompt is encoded, the LLM offers the relevant knowledge K required to solve the problem in X.Therefore, both the LLM and X are direct causes of K.</p>
<p>K → Y ← X The knowledge K encompasses the method on how to solve the problem described in X, while X contains the question-specific information, such as the values involved in the problem.So both X and K are a cause of Y.</p>
<p>As demonstrated in Section 3.1, the prompt feature X comprises two independent semantics, one encompasses general problem-solving information, and the other one contains problem-specific information.Taking this into consideration, we introduce two additional elements to the graph, namely, the general problemsolving information X G , and the problem-specific information X S .Both elements are derived from X, X G serves as a cause of the problem-solving knowledge K, and X S acts as a mediator between X and Y. Fig. 4: The framework of our method.First, we divide the concatenated Adapter prompt in LLaMA-Adapter into two segments, Adap1 and Adap2.This affects the dimensions of K and V in the self-attention mechanism, as denoted on the right-hand side.The process of generating the feature Z remains unchanged, and we build our causal loss L causal by manipulating Adap1.</p>
<p>In this framework, X G and X S should be strictly independent because it's common sense that the problem does not affect the problem-solving skill set.For instance, in the letter concatenation problems, the value of the string within the quotation marks should be independent of the method we use to locate, fetch and concatenate the desired characters.</p>
<p>However, based on the causal inference theory introduced in Section 2.2, the independence between X G and X S is not guaranteed.Although there are no direct causal relationships between the two elements, X acts as a confounder between X G and X S and thus creates spurious associations between them.This explains the phenomenon we observed in Figure 3a-3b, where altering the value of X S (the string within the quotation marks) affects the reasoning process X G (the functionality of adap_6).</p>
<p>Therefore, to deconfound the spurious association between X G and X S , we perform an intervention on X G to block the association from flowing through the path X G ← X → X S , as demonstrated in Figure 3d.In that case, changing X S will no longer affect the reasoning process of X G .</p>
<p>Implementation of Causal Intervention</p>
<p>In this section, we introduce our method to implement the intervention on X G , as illustrated in Figure 3d.First, we assume that the general problem-solving information X G and the problem-specific information X S can be identified by comparison across samples in a dataset, i.e., the differences between data samples are problem-specific, and thus belong to X S , and the general problem-solving knowledge, denoted as X G , is common across all samples.For instance, in the example given in Section 3.1, X G contains the method of fetching the desired characters and performing concatenation, and X S contains the order of the characters to be fetched and from which string are these characters to be selected.</p>
<p>With this assumption, performing the intervention do(X G ) is equivalent to holding X G invariant across all data samples so that it can maintain the general problem-solving information consistently while changing X S .For example, we aim to hold adap_6 invariant across Figure 3a and Figure 3b, to avoid it possessing information of X S , such as the token "AG" and "AN" in Figure 3b.</p>
<p>Thus, we introduce a causal constraint into the training process to encourage X G to remain invariant across all data samples.Mathematically, we penalize a larger value of variance on X G by introducing a regularization term in Equation 4
min θ L CE + αL causal(3)L causal = E l∈L ′ <a href="4">V ar(X G )</a>
where L CE is the Cross-Entropy Loss used to train the token prediction accuracy, and α is the weight of our causal regularization term.We apply this causal regularizer on the topmost L ′ transformer layers, so we take expectation over these layers, where L ′ ≤ L is a tunable hyper-parameter.</p>
<p>In order to estimate X G in each of the topmost L ′ layers in Equation 4, we divide the concatenated adapter T l into two separate pieces, T l,1 with the length H, and T l,2 with length M − H. Therefore, we rewrite Equation 1 as:
[P l ; T l,1 ; T l,2 ] ∈ R (K+H+(M −H))×C(5)
Similar to the vanilla LLaMA-Adapter, this affects the dimension setting of the Key and Value in the self-attention module.Therefore, we rewrite these two modules as Equation 6 and Equation 7.
K l = <a href="6">K vanilla ; K adap1 ; K adap2 </a>V l = <a href="7">V vanilla ; V adap1 ; V adap2 </a>
Then, instead of applying the softmax function on the three components independently, we first apply the softmax function on the two original components and multiply with the gating module introduced in the vanilla LLaMA-Adapter, then separate the score matrices into three pieces.Therefore, we have Equation 8.
S g l = <a href="8">S vanilla ; S adap1 ; S adap2 </a>
These operations divide the adapter architecture into two segments.Then we treat these two segments as X G and X S respectively, enabling us to impose distinct constraints on each of them.In particular, we treat T l,1 with length H as the section controlling the general problem-solving information X G .Therefore, X G can be estimated by Equation 9.
X G ≈ S adap1 • V adap1(9)
Finally, we aggregate this quantity in each of the topmost L ′ layers and take expectation to form the causal regularizer as introduced in Equation 4. The architecture of our method is illustrated in Figure 4.The modules involved in the calculation of L causal are coloured in dark red.</p>
<p>Experiment</p>
<p>Experimental Settings</p>
<p>We build our method by fine-tuning LLaMA 7B model [26], thus all the parameters related to dimensions and layers remain unchanged, such as the number of transformer layers is 32, and each transformer layer has 32 attention heads.Also, the feature dimension is 128 for each attention head, thus the total feature dimension is 4096.We train the model with a maximum sequence length of 256 and use AdamW for optimization with a learning rate equal to 1e-3.All the models are fine-tuned for 5 epochs with a batch size of 4 for a fair comparison.</p>
<p>In terms of the parameters introduced by vanilla LLaMA-Adapter, we set L = 20 and M = 10, which means we fine-tune the top 20 transformer layers by appending an adapter prompt of length 10 on each of them.For the parameters H and α introduced by our method, we set H as 2 and α as 1 in all experiments.The parameter L ′ is data-dependent, and we use 20 for Letter Concatenation, 10 for Date Understanding, 3 for AddSub and Math10k, and 1 for Math401.All other settings, if not specified here, remain the same as in [37].</p>
<p>Tasks for Evaluation</p>
<p>We evaluate the performance of our method by three types of reasoning tasks:</p>
<p>Symbolic Reasoning We construct a more challenging version of the last letter concatenation problem in [30] because the models could almost perfectly solve the problems if the models are fine-tuned with it.Therefore, we ask the model to perform second last letter concatenation, such as Take the second last letters of the words in "Lady Gaga" and concatenate them.</p>
<p>Commonsense Reasoning We test the models with Date Understanding data [23], where each data sample asks a multiple-choice question such as If today is Jan 1, 2023, what day is tomorrow in MM/DD/YYYY?Arithmetic Reasoning We test the models on three datasets, Math401 [34], which comprises basic arithmetic questions such as 1+2=?, AddSub [6] and Math10k [9], both comprises math word questions such as Tom found 7 seashells but 4 were broken .How many unbroken seashells did Tom find?.</p>
<p>Baselines and Comparison Methods</p>
<p>We compare our method with other methods from three perspectives to conduct a comprehensive comparison:</p>
<p>1) We compare our method with the vanilla LLaMA-Adapter [37].Since we build our method based on LLaMA-Adapter, this comparison allows us to understand the direct impact of implementing our method.All common settings between the two methods such as parameters are kept the same to ensure a fair comparison.The results of this comparison is presented in the bottom block of Table 1, and we highlight the margin achieved by our method in green.</p>
<p>2) We compare our method with the other parameter-efficient fine-tuning (PEFT) methods, as listed in the middle block of Table 1.We apply these methods on LLaMA 7B, and the results are obtained with the library and hyper-parameters provided by [16,9].We present the results and the number of learnable parameters allowing us to compare our method with the baseline methods in terms of both effectiveness and efficiency.</p>
<p>3) We compare our method with several pre-trained prompt-following models with the size of 7B, as listed in the top block of Table 1.These models do not lie in the domain of PEFT and thus are not directly comparable to our method.They are either obtained by full fine-tuning or pre-trained with massive conversational data.We compare our method with these models to investigate their performances on the reasoning tasks and evaluate if task-specific fine-tuning is necessary to achieve satisfactory results.</p>
<p>Overall Results</p>
<p>The results are presented in Table 1, where the numbers denote the accuracies the methods achieve on each dataset.While comparing our method with the three types of baselines outlined above, our findings also fall into three aspects:</p>
<p>1) Compared with LLaMA-Adapter: Our method consistently outperforms LLaMA-Adapter by a considerable margin on all datasets, as highlighted in green in Table 1.Since all the common settings of the two methods remain the same, the results directly demonstrate the impact of our causal method.</p>
<p>2) Compared with the other PEFT methods: We found that while the vanilla LLaMA-Adapter does not always outperform the baseline methods, our method, in contrast, achieves either the highest or the second highest score across all datasets.Even though a few methods may perform better than our method on some particular datasets, it is worth noting that our method has only 1.2M learnable parameters, which is the least among all methods.In summary, our method achieves better or comparable results with other PEFT methods, with much less learnable parameters.</p>
<p>3) Compared with pre-trained models: We found that the performance of pre-trained models is generally not satisfactory compared with the PEFT methods.While these models achieve fair performances on some datasets, face significant challenges in the LConcat task.Notably, it was observed that none of the pre-trained models under consideration could accurately respond to the Letter Concatenation questions.To ensure this phenomenon is not due to the bias in our prompt, we endeavoured to rephrase the questions in LConcat, however, the models consistently exhibited an inability to comprehend the prompts and frequently provided irrelevant or meaningless responses.We speculate that this is due to the insufficient inclusion of training data of this specific nature during the model's fine-tuning phases.</p>
<p>Summary Our experiments suggest that fine-tuning on specific tasks is necessary to achieve satisfactory results.And, among the Parameter-Efficient Fine-Tuning methods, our method achieves better or comparable results with much less learnable parameters and computational resources.</p>
<p>Effects of New Parameters</p>
<p>To further investigate the mechanism of our method, we study the impact of parameters introduced by our method, namely, the length H of adaption prompts to be treated as X G , the weight α of the regularization term L causal , and the number of layers L ′ to be used to calculate L causal .</p>
<p>Choice of H and α We visualize the effect of H and α on the Letter Concatenation dataset in Figure 5a -5b, where the x-axis denotes the value of the parameters, and the y-axis denotes the accuracy obtained by the model.Similar trends can be observed in both charts that increasing the value of H and α can improve the performance of the model, but excessive values can be detrimental.This aligns with our intuition.For H, if a substantial fraction of the adapter remains fixed as X G , then only a limited part of the adapter could be left to address X S , which compromises its efficacy in managing problem-specific information.For α, if a large weight is employed for L causal , the module to handle X G might remain constant and cannot encode any information.</p>
<p>Choice of L ′ We found the optimal choice of L ′ is data-dependent.On datasets like Letter Concatenation, where all the prompts follow the same for- The value of these parameters should be chosen carefully, otherwise may harm the performance when the values are too large.</p>
<p>mat, a larger L ′ is beneficial to the performance.In contrast, on datasets like AddSub, where the questions are not necessarily in the same template, a smaller L ′ is preferable.This is intuitively reasonable, because for those datasets where the prompts are close enough in the first place, encouraging the model to extract X G from the bottom layers grants us more control over the reasoning process.In contrast, for those datasets where the prompts are not sufficiently close, X G can only be extracted and controlled when the representations have been aggregated to a certain level.In that case, a large L ′ would limit the model's potential for aggregating the high-level information.</p>
<p>Further Discussions</p>
<p>Applicable scenarios We illustrate the motivation and idea of our method in Section 3.1.However, it is worth noting that our method is not limited to the case of the same pattern questions.Instead, prompts in different formats also benefit from our method.As demonstrated in Section 4, our method benefits a wide range of reasoning tasks with various datasets.This is because we encourage the model to extract the "approach" of solving problems.In other words, as long as a prompt involves reasoning, there will be some problem-solving skills (X G ), and our method is applicable to the scenario.For example, in date understanding and math word questions, where the prompts vary significantly, our method still benefits the performance as illustrated in Table 1, because we encourage the model to extract the high-level knowledge, such as the meaning of "tomorrow", "end of the month" or the math operations such as "Add", "Subtract", and keep these problem-solving skills invariance across all data samples.In contrast, our method does not apply to the general Q&amp;A questions, such as Tell me about Alpaca, because these questions do not require reasoning capabilities and there is no "approach" to answer these questions.</p>
<p>Few-shot experiments Few-shot prompt method such as Chain-of-Thought (COT) [30] is known to be useful on large models like ChatGPT/GPT4, but it does not apply to PEFT methods, so we did not include these experiments in our paper.To elaborate, COT works well on ChatGPT/GPT4 because those models are fine-tuned by a massive amount of prompt-answer pairs with oneshot examples, enabling the model to utilize one-shot information effectively.In contrast, our method fine-tunes a non-prompt-following LLMs (LLaMA) with task-specific data aiming for improved performance on the task.Since the data does not contain any one-shot prompts, the model will not be able to utilize the one-shot information.As a matter of fact, our experiments reveal that COT is even harmful to the result in such cases.</p>
<p>Finetuning a prompt-following model We also conduct experiments to apply our method on prompt-following models such as Alpaca.As a result, it achieves an accuracy of 75.3 on LConcat, and 79.8 on Date Understanding datasets, which is not comparable to the result we achieved using the original non-prompt following LLaMA.We speculate this is because such instructiontuned LLMs (such as Alpaca/Vicuna) are also based on the original foundation model such as LLaMA, and it has been fine-tuned with the data that are not closely related to our downstream tasks, thus dropping some information relevant to our task, thus harming the performance.Therefore, we empirically conclude that it would be a better practice to fine-tune the foundation model, rather than an existing instruction-following model.</p>
<p>Related Works</p>
<p>Reasoning in LLMs.Instruction-following LLMs have been employed on many tasks involving reasoning recently, including but not limited to Mathematics, Logical Reasoning, and Symbolic Reasoning [20,30,38].Many of these methods investigate LLM's reasoning capabilities from its output using Chain-of-Thought prompting strategy [30].Apart from these, some works build thinking pipelines [1,33] to achieve the final goal step-by-step.</p>
<p>Causal Inference in Machine Learning.Causal inference has been applied to many vision tasks in recent years such as image recognition [35,24] and Image Generation [12].These works first construct causal graphs to explain the task, then use causal inference methods to eliminate the spurious association and improve the performance of the models.Besides, causal inference techniques are also used in Representation Learning [22,32].</p>
<p>Relationships with our method</p>
<p>Existing works typically discuss LLMs' reasoning abilities based on their input and output [20].However, we argue that solving causality-related tasks or providing the thinking processes by words do not necessarily indicate the model's reasoning capability, because simply mimicking token distribution could achieve equivalent outcomes.Our work, in contrast, discusses the reasoning capabilities of LLMs in the level of attention and representation, thus offering a novel perspective on this matter.Besides, the novelty of our method also involves applying causality in LLM fine-tuning, which was rarely discussed in earlier literature.</p>
<p>In this paper, we first investigated the reasoning capabilities of the promptfollowing LLMs by visualizing the attention values in the thinking process, and empirically suggest that these models lack genuine causal reasoning capabilities.Then, we formulate the reasoning process of LLMs into a causal inference framework to explain the issues observed in the visualization.Finally, we propose Deconfounded Causal Adaptation (DCA), a causal fine-tuning method to improve the model's reasoning capability.Experiments show our method effectively enhances the reasoning capabilities of the models and outperforms baseline methods consistently.Besides, we also discuss the applicable scenarios of our method and analyze the effect of our method with different settings thoroughly.</p>
<p>Fig. 2 :
2
Fig. 2: (a) The architecture of LLaMA-Adapter.A trainable lightweight adapter is inserted into each of the topmost L layers out of the N transformer layers of LLaMA.Aided by zero-init attention and gating mechanisms, the adaption prompt progressively learns new instructional cues, without disturbing the original pre-trained knowledge; (b) X → Z → Y is a chain, X ← C → Y is a fork, C → Y ← Z is a collider; (c)We perform intervention do(X) to cut the edge C → X so that the causal effect P (Y |do(X)) can be estimated.</p>
<p>Fig. 3 :
3
Fig. 3: (a)-(b) Changing the value of the string affects the functioning of the attention mechanism as highlighted.(c) Causal graph of the reasoning process; (d) We block the backdoor path by performing an intervention on X G .</p>
<p>Fig. 5 :
5
Fig. 5: Effect of H and α on LConcat.The red dot line denotes baseline accuracy.The value of these parameters should be chosen carefully, otherwise may harm the performance when the values are too large.</p>
<p>Table 1 :
1
Accuracies of models based on LLaMA-7B.Our method achieves better or comparable results to other methods with only 1.2M tunable parameters.
Params LConcat DateMath401 AddSub Math10k Avg.Alpaca-7B [25]-0.052.29.822.310.218.9Vicuna-7B [2]-0.029.429.238.615.328.5Koala-7B [4]-0.054.325.632.412.725.0Baize-7B [31]-0.044.928.334.411.223.8LLaMA2-7B ct. [27]-0.056.830.656.721.633.1Mistral-7B-ins. [10]-0.054.628.639.613.227.2S-Adapter h [7]134M 80.179.820.278.129.957.6S-Adapter p [19]68M 77.379.321.582.124.156.9P-Adapter [5]200M 80.482.222.184.729.559.8LoRA [8]4.2M 80.882.623.683.330.860.2AdaLoRA [36]3.8M 80.983.023.485.430.960.7Prefix-Tune [14]7.0M 80.278.325.257.034.955.3Prompt-Tune [13]2.0M 78.382.621.262.324.753.8KronA [3]4.2M 81.782.823.483.031.460.5LoftQ [15]4.0M 80.982.723.083.729.860.0LLaMA-Adap.1.2M 75.378.321.683.630.257.8DCA (Ours)1.2M 82.1(+6.8) 84.7(+6.4) 24.6(+3.0) 86.3(+2.7) 35.3(+5.1) 62.6</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, arXiv:2308.096872023arXiv preprint</p>
<p>W L Chiang, Z Li, Z Lin, Y Sheng, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. 2023</p>
<p>A Edalati, M Tahaei, I Kobyzev, V P Nia, arXiv:2212.10650Krona: Parameter efficient tuning with kronecker adapter. 2022arXiv preprint</p>
<p>Koala: A dialogue model for academic research. X Geng, A Gudibande, H Liu, E Wallace, P Abbeel, S Levine, D Song, Blog post. April 1 (2023</p>
<p>Towards a unified view of parameter-efficient transfer learning. J He, C Zhou, X Ma, T Berg-Kirkpatrick, G Neubig, arXiv:2110.043662021arXiv preprint</p>
<p>Learning to solve arithmetic word problems with verb categorization. M J Hosseini, H Hajishirzi, O Etzioni, N Kushman, EMNLP. 2014</p>
<p>Parameter-efficient transfer learning for nlp. N Houlsby, A Giurgiu, S Jastrzebski, B Morrone, ICML2019PMLR</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. Z Hu, Y Lan, arXiv:2304.019332023arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Z Jin, J Liu, Z Lyu, Spencer Poff, B Schölkopf, arXiv:2306.05836Can large language models infer causation from correlation?. 2023arXiv preprint</p>
<p>Causalgan: Learning causal implicit generative models with adversarial training. M Kocaoglu, C Snyder, arXiv:1709.020232017arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, arXiv:2104.086912021arXiv preprint</p>
<p>X L Li, P Liang, arXiv:2101.00190Prefix-tuning: Optimizing continuous prompts for generation. 2021arXiv preprint</p>
<p>Loftq: Lora-fine-tuning-aware quantization for large language models. Y Li, Y Yu, C Liang, P He, arXiv:2310.086592023arXiv preprint</p>
<p>Peft: State-of-the-art parameterefficient fine-tuning methods. S Mangrulkar, S Gugger, L Debut, 2022</p>
<p>OpenAI: Gpt-4 technical report. 2023</p>
<p>. J Pearl, 2009Causality. Cambridge university press</p>
<p>Mad-x: An adapter-based framework for multi-task cross-lingual transfer. J Pfeiffer, I Vulić, I Gurevych, S Ruder, arXiv:2005.000522020arXiv preprint</p>
<p>S Qiao, Y Ou, N Zhang, X Chen, Y Yao, S Deng, arXiv:2212.09597Reasoning with language model prompting: A survey. 2022arXiv preprint</p>
<p>R Schaeffer, B Miranda, S Koyejo, arXiv:2304.15004Are emergent abilities of large language models a mirage?. 2023arXiv preprint</p>
<p>Weakly supervised disentangled generative causal representation learning. X Shen, F Liu, H Dong, Q Lian, JMLR. 2312022</p>
<p>A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Long-tailed classification by keeping the good and removing the bad momentum causal effect. K Tang, J Huang, H Zhang, NeurIPS. 332020</p>
<p>R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Bertviz: A tool for visualizing multihead self-attention in the bert model. J Vig, ICLR workshop: Debugging machine learning models. 201923</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 352022</p>
<p>Baize: An open-source chat model with parameter-efficient tuning on self-chat data. C Xu, D Guo, arXiv:2304.011962023arXiv preprint</p>
<p>Causalvae: Disentangled representation learning via neural structural causal models. M Yang, F Liu, Z Chen, X Shen, 2021CVPR</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, arXiv:2305.106012023arXiv preprint</p>
<p>How well do large language models perform in arithmetic tasks. Z Yuan, H Yuan, C Tan, W Wang, S Huang, 2023</p>
<p>Interventional few-shot learning. Z Yue, H Zhang, Q Sun, X S Hua, Advances in neural information processing systems. 332020</p>
<p>Q Zhang, M Chen, A Bukharin, P He, arXiv:2303.10512Adaptive budget allocation for parameter-efficient fine-tuning. 2023arXiv preprint</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. R Zhang, J Han, A Zhou, X Hu, arXiv:2303.161992023arXiv preprint</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9064 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9064</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9064</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-42ed4a9994e6121a9f325f5b901c5b3d7ce104f5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/42ed4a9994e6121a9f325f5b901c5b3d7ce104f5" target="_blank">Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> There is substantial room for improvement in NLI systems, and the HANS dataset can motivate and measure progress in this area, which contains many examples where the heuristics fail.</p>
                <p><strong>Paper Abstract:</strong> A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9064.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9064.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional Encoder Representations from Transformers (bert-base-uncased)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained Transformer-based contextual language representation model (bert-base-uncased) fine-tuned on MNLI and evaluated on the HANS diagnostic dataset that probes syntactic heuristics in NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>bert-base-uncased</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer (bidirectional attention) pretrained on large corpora with masked-language and next-sentence prediction objectives; in this paper it was fine-tuned on MNLI for NLI evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HANS (Heuristic Analysis for NLI Systems)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A controlled NLI evaluation set designed to diagnostically test three fallible syntactic heuristics (lexical overlap, subsequence, constituent) across 30 subcases; assesses structure-sensitive inference rather than general language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>On MNLI-finetuned BERT evaluated on HANS: For examples whose correct label is entailment: lexical overlap 98% accuracy, subsequence 100%, constituent 99%. For examples whose correct label is non-entailment: lexical overlap 4% accuracy, subsequence 2%, constituent 20% (Table 9 / aggregated results). Overall, BERT performed near-perfectly when heuristics aligned with the correct label and near-zero (well below chance) when heuristics would predict the wrong label; specific subcase accuracies vary (e.g., 39% on conjunction lexical-overlap subset, 0% on subject/object swap).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Mechanical Turk participants: average 76% accuracy (75% on entailment, 77% on non-entailment); expert annotators: 97% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>BERT is well below human baseline on HANS non-entailment cases (models systematically predict entailment where humans perform substantially better and more balanced across labels); for entailment cases model accuracy approaches/exceeds human non-expert performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>BERT used was bert-base-uncased pretrained and then fine-tuned on MNLI. Evaluation on HANS mapped model outputs (entailment / contradiction / neutral) into entailment vs non-entailment by collapsing contradiction+neutral into non-entailment for reporting; results reported both ways were similar. Some transfer/augmentation experiments fine-tuned BERT on MNLI augmented with HANS-like examples (30k examples) to test mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>HANS is a synthetic, template-generated diagnostic dataset focused on specific structural heuristics rather than a general cognitive psychology battery; comparisons to human performance include crowdsourced non-expert and small expert samples but are not psychometric norms. BERT's pretraining (large corpus) and its architecture may give it different inductive biases than other models; model size and full pretraining data details are not provided in paper. Reported numbers include aggregated subcase summaries; performance varies widely across subcases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9064.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9064.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decomposable Attention model (DA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bag-of-words style attention-based NLI model that aligns premise and hypothesis words but lacks explicit word-order encoding; evaluated on HANS to probe susceptibility to lexical-overlap heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A decomposable attention model for natural language inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decomposable Attention (DA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An attention-based NLI model that aligns words between premise and hypothesis and aggregates alignment features; described as having no word-order information and thus acting like a bag-of-words model in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HANS (Heuristic Analysis for NLI Systems)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same as for BERT: diagnostic dataset targeting lexical overlap, subsequence, and constituent heuristics in NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Aggregated results (MNLI-trained DA, with neutral+contradiction merged): For cases with correct label = entailment: lexical 100%, subsequence 100%, constituent 98%; for cases with correct label = non-entailment: lexical 0%, subsequence 0%, constituent 3% (Table 9). In general DA predicted entailment almost always, yielding near-perfect scores when heuristics aligned and near-zero on the contradicting cases.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Mechanical Turk participants: average 76% accuracy; expert annotators: 97% accuracy (same human baseline used in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>DA performs far below human baseline on HANS non-entailment cases (near-zero) and does not match human balanced performance; it appears to rely heavily on lexical-overlap heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>DA implementation from AllenNLP; trained on MNLI and evaluated on HANS. When MNLI was augmented with HANS-like examples, DA improved on some cases but still failed on subcases requiring non-bag-of-words representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>DA's architecture lacks word-order information by design, so its failures on HANS reflect an architectural limitation as well as training-data issues; results reported are for specific MNLI training and dataset augmentation regimes in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9064.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9064.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ESIM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Enhanced Sequential Inference Model (ESIM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential (bi-LSTM based) NLI model that encodes sentences with recurrent structure; evaluated on HANS and found to largely follow surface heuristics despite access to word-order information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhanced LSTM for natural language inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ESIM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An NLI model using a modified bidirectional LSTM sentence encoder with attention-based components; in this paper the sequential encoder variant was used (not tree-based HIM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HANS (Heuristic Analysis for NLI Systems)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same diagnostic HANS dataset probing lexical overlap, subsequence, and constituent heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Aggregated results (MNLI-trained ESIM, with neutral+contradiction merged): For correct-entailment cases: lexical 99%, subsequence 100%, constituent 100%; for correct-non-entailment cases: lexical 0%, subsequence 1%, constituent 0% (Table 9). ESIM nearly always predicted entailment and thus failed the contradicting HANS cases.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Mechanical Turk participants: average 76% accuracy; expert annotators: 97% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ESIM is well below human baseline on HANS non-entailment cases (near-zero accuracy) despite theoretically having access to order information.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>ESIM implementation from AllenNLP; trained on MNLI and evaluated on HANS. The paper also reports augmentation experiments (MNLI + HANS-like data) leading to improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Although ESIM is sequential and can in principle use word order, in practice it did not exploit order sufficiently under MNLI training to avoid heuristics; augmentation with HANS-like data was necessary to reduce heuristic reliance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9064.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9064.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPINN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stack-augmented Parser-Interpreter Neural Network (SPINN-PI-NT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-structured NLI model that composes phrases according to syntactic parses; evaluated on HANS and showed relatively better performance on structure-sensitive subcases compared to DA and ESIM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A fast unified model for parsing and sentence understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SPINN-PI-NT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A tree-based neural network that composes phrase representations according to an input syntactic parse; the variant used takes parse trees as input rather than learning to parse.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HANS (Heuristic Analysis for NLI Systems)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Diagnostic dataset targeting structural heuristics in NLI (lexical overlap, subsequence, constituent).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Aggregated results (MNLI-trained SPINN, with neutral+contradiction merged): For correct-entailment cases: lexical 94%, subsequence 96%, constituent 93%; for correct-non-entailment cases: lexical 6%, subsequence 14%, constituent 11% (Table 9). SPINN performed better than DA/ESIM on subsequence and constituent non-entailment cases but still far below human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Mechanical Turk participants: average 76% accuracy; expert annotators: 97% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>SPINN outperforms the other tested architectures on some HANS non-entailment subcases (suggesting structural inductive biases help), but overall remains well below expert human performance and often below non-expert human baseline on difficult subcases.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>SPINN-PI-NT used with parses from MNLI release for MNLI training and custom parse templates for HANS; NYU fork implementation of SPINN used. Training on MNLI plus HANS-like augmentation produced better performance on HANS than training on MNLI alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>SPINN's better performance indicates some architectural advantage, but it still fails many non-entailment cases; HANS is synthetic and targets specific heuristics, so results do not generalize automatically to all language understanding phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating compositionality in sentence embeddings <em>(Rating: 2)</em></li>
                <li>Assessing BERT's syntactic abilities <em>(Rating: 2)</em></li>
                <li>Targeted syntactic evaluation of language models <em>(Rating: 2)</em></li>
                <li>Assessing the ability of LSTMs to learn syntax-sensitive dependencies <em>(Rating: 2)</em></li>
                <li>Non-entailed subsequences as a challenge for natural language inference <em>(Rating: 2)</em></li>
                <li>Breaking NLI Systems with Sentences that Require Simple Lexical Inferences <em>(Rating: 1)</em></li>
                <li>Stress-test evaluation for natural language inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9064",
    "paper_id": "paper-42ed4a9994e6121a9f325f5b901c5b3d7ce104f5",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "BERT",
            "name_full": "Bidirectional Encoder Representations from Transformers (bert-base-uncased)",
            "brief_description": "A pretrained Transformer-based contextual language representation model (bert-base-uncased) fine-tuned on MNLI and evaluated on the HANS diagnostic dataset that probes syntactic heuristics in NLI.",
            "citation_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_name": "bert-base-uncased",
            "model_description": "Transformer (bidirectional attention) pretrained on large corpora with masked-language and next-sentence prediction objectives; in this paper it was fine-tuned on MNLI for NLI evaluation.",
            "model_size": null,
            "test_battery_name": "HANS (Heuristic Analysis for NLI Systems)",
            "test_description": "A controlled NLI evaluation set designed to diagnostically test three fallible syntactic heuristics (lexical overlap, subsequence, constituent) across 30 subcases; assesses structure-sensitive inference rather than general language understanding.",
            "llm_performance": "On MNLI-finetuned BERT evaluated on HANS: For examples whose correct label is entailment: lexical overlap 98% accuracy, subsequence 100%, constituent 99%. For examples whose correct label is non-entailment: lexical overlap 4% accuracy, subsequence 2%, constituent 20% (Table 9 / aggregated results). Overall, BERT performed near-perfectly when heuristics aligned with the correct label and near-zero (well below chance) when heuristics would predict the wrong label; specific subcase accuracies vary (e.g., 39% on conjunction lexical-overlap subset, 0% on subject/object swap).",
            "human_baseline_performance": "Mechanical Turk participants: average 76% accuracy (75% on entailment, 77% on non-entailment); expert annotators: 97% accuracy.",
            "performance_comparison": "BERT is well below human baseline on HANS non-entailment cases (models systematically predict entailment where humans perform substantially better and more balanced across labels); for entailment cases model accuracy approaches/exceeds human non-expert performance.",
            "experimental_details": "BERT used was bert-base-uncased pretrained and then fine-tuned on MNLI. Evaluation on HANS mapped model outputs (entailment / contradiction / neutral) into entailment vs non-entailment by collapsing contradiction+neutral into non-entailment for reporting; results reported both ways were similar. Some transfer/augmentation experiments fine-tuned BERT on MNLI augmented with HANS-like examples (30k examples) to test mitigation.",
            "limitations_or_caveats": "HANS is a synthetic, template-generated diagnostic dataset focused on specific structural heuristics rather than a general cognitive psychology battery; comparisons to human performance include crowdsourced non-expert and small expert samples but are not psychometric norms. BERT's pretraining (large corpus) and its architecture may give it different inductive biases than other models; model size and full pretraining data details are not provided in paper. Reported numbers include aggregated subcase summaries; performance varies widely across subcases.",
            "uuid": "e9064.0",
            "source_info": {
                "paper_title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "DA",
            "name_full": "Decomposable Attention model (DA)",
            "brief_description": "A bag-of-words style attention-based NLI model that aligns premise and hypothesis words but lacks explicit word-order encoding; evaluated on HANS to probe susceptibility to lexical-overlap heuristics.",
            "citation_title": "A decomposable attention model for natural language inference",
            "mention_or_use": "use",
            "model_name": "Decomposable Attention (DA)",
            "model_description": "An attention-based NLI model that aligns words between premise and hypothesis and aggregates alignment features; described as having no word-order information and thus acting like a bag-of-words model in this paper.",
            "model_size": null,
            "test_battery_name": "HANS (Heuristic Analysis for NLI Systems)",
            "test_description": "Same as for BERT: diagnostic dataset targeting lexical overlap, subsequence, and constituent heuristics in NLI.",
            "llm_performance": "Aggregated results (MNLI-trained DA, with neutral+contradiction merged): For cases with correct label = entailment: lexical 100%, subsequence 100%, constituent 98%; for cases with correct label = non-entailment: lexical 0%, subsequence 0%, constituent 3% (Table 9). In general DA predicted entailment almost always, yielding near-perfect scores when heuristics aligned and near-zero on the contradicting cases.",
            "human_baseline_performance": "Mechanical Turk participants: average 76% accuracy; expert annotators: 97% accuracy (same human baseline used in paper).",
            "performance_comparison": "DA performs far below human baseline on HANS non-entailment cases (near-zero) and does not match human balanced performance; it appears to rely heavily on lexical-overlap heuristics.",
            "experimental_details": "DA implementation from AllenNLP; trained on MNLI and evaluated on HANS. When MNLI was augmented with HANS-like examples, DA improved on some cases but still failed on subcases requiring non-bag-of-words representations.",
            "limitations_or_caveats": "DA's architecture lacks word-order information by design, so its failures on HANS reflect an architectural limitation as well as training-data issues; results reported are for specific MNLI training and dataset augmentation regimes in the paper.",
            "uuid": "e9064.1",
            "source_info": {
                "paper_title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "ESIM",
            "name_full": "Enhanced Sequential Inference Model (ESIM)",
            "brief_description": "A sequential (bi-LSTM based) NLI model that encodes sentences with recurrent structure; evaluated on HANS and found to largely follow surface heuristics despite access to word-order information.",
            "citation_title": "Enhanced LSTM for natural language inference",
            "mention_or_use": "use",
            "model_name": "ESIM",
            "model_description": "An NLI model using a modified bidirectional LSTM sentence encoder with attention-based components; in this paper the sequential encoder variant was used (not tree-based HIM).",
            "model_size": null,
            "test_battery_name": "HANS (Heuristic Analysis for NLI Systems)",
            "test_description": "Same diagnostic HANS dataset probing lexical overlap, subsequence, and constituent heuristics.",
            "llm_performance": "Aggregated results (MNLI-trained ESIM, with neutral+contradiction merged): For correct-entailment cases: lexical 99%, subsequence 100%, constituent 100%; for correct-non-entailment cases: lexical 0%, subsequence 1%, constituent 0% (Table 9). ESIM nearly always predicted entailment and thus failed the contradicting HANS cases.",
            "human_baseline_performance": "Mechanical Turk participants: average 76% accuracy; expert annotators: 97% accuracy.",
            "performance_comparison": "ESIM is well below human baseline on HANS non-entailment cases (near-zero accuracy) despite theoretically having access to order information.",
            "experimental_details": "ESIM implementation from AllenNLP; trained on MNLI and evaluated on HANS. The paper also reports augmentation experiments (MNLI + HANS-like data) leading to improved performance.",
            "limitations_or_caveats": "Although ESIM is sequential and can in principle use word order, in practice it did not exploit order sufficiently under MNLI training to avoid heuristics; augmentation with HANS-like data was necessary to reduce heuristic reliance.",
            "uuid": "e9064.2",
            "source_info": {
                "paper_title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "SPINN",
            "name_full": "Stack-augmented Parser-Interpreter Neural Network (SPINN-PI-NT)",
            "brief_description": "A tree-structured NLI model that composes phrases according to syntactic parses; evaluated on HANS and showed relatively better performance on structure-sensitive subcases compared to DA and ESIM.",
            "citation_title": "A fast unified model for parsing and sentence understanding",
            "mention_or_use": "use",
            "model_name": "SPINN-PI-NT",
            "model_description": "A tree-based neural network that composes phrase representations according to an input syntactic parse; the variant used takes parse trees as input rather than learning to parse.",
            "model_size": null,
            "test_battery_name": "HANS (Heuristic Analysis for NLI Systems)",
            "test_description": "Diagnostic dataset targeting structural heuristics in NLI (lexical overlap, subsequence, constituent).",
            "llm_performance": "Aggregated results (MNLI-trained SPINN, with neutral+contradiction merged): For correct-entailment cases: lexical 94%, subsequence 96%, constituent 93%; for correct-non-entailment cases: lexical 6%, subsequence 14%, constituent 11% (Table 9). SPINN performed better than DA/ESIM on subsequence and constituent non-entailment cases but still far below human experts.",
            "human_baseline_performance": "Mechanical Turk participants: average 76% accuracy; expert annotators: 97% accuracy.",
            "performance_comparison": "SPINN outperforms the other tested architectures on some HANS non-entailment subcases (suggesting structural inductive biases help), but overall remains well below expert human performance and often below non-expert human baseline on difficult subcases.",
            "experimental_details": "SPINN-PI-NT used with parses from MNLI release for MNLI training and custom parse templates for HANS; NYU fork implementation of SPINN used. Training on MNLI plus HANS-like augmentation produced better performance on HANS than training on MNLI alone.",
            "limitations_or_caveats": "SPINN's better performance indicates some architectural advantage, but it still fails many non-entailment cases; HANS is synthetic and targets specific heuristics, so results do not generalize automatically to all language understanding phenomena.",
            "uuid": "e9064.3",
            "source_info": {
                "paper_title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
                "publication_date_yy_mm": "2019-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating compositionality in sentence embeddings",
            "rating": 2
        },
        {
            "paper_title": "Assessing BERT's syntactic abilities",
            "rating": 2
        },
        {
            "paper_title": "Targeted syntactic evaluation of language models",
            "rating": 2
        },
        {
            "paper_title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
            "rating": 2
        },
        {
            "paper_title": "Non-entailed subsequences as a challenge for natural language inference",
            "rating": 2
        },
        {
            "paper_title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
            "rating": 1
        },
        {
            "paper_title": "Stress-test evaluation for natural language inference",
            "rating": 1
        }
    ],
    "cost": 0.01348725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference</h1>
<p>R. Thomas McCoy, ${ }^{1}$ Ellie Pavlick, ${ }^{2}$ \&amp; Tal Linzen ${ }^{1}$<br>${ }^{1}$ Department of Cognitive Science, Johns Hopkins University<br>${ }^{2}$ Department of Computer Science, Brown University<br>tom.mccoy@jhu.edu, ellie_pavlick@brown.edu, tal.linzen@jhu.edu</p>
<h4>Abstract</h4>
<p>A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.</p>
<h2>1 Introduction</h2>
<p>Neural networks excel at learning the statistical patterns in a training set and applying them to test cases drawn from the same distribution as the training examples. This strength can also be a weakness: statistical learners such as standard neural network architectures are prone to adopting shallow heuristics that succeed for the majority of training examples, instead of learning the underlying generalizations that they are intended to capture. If such heuristics often yield correct outputs, the loss function provides little incentive for the model to learn to generalize to more challenging cases as a human performing the task would.</p>
<p>This issue has been documented across domains in artificial intelligence. In computer vision, for
example, neural networks trained to recognize objects are misled by contextual heuristics: a network that is able to recognize monkeys in a typical context with high accuracy may nevertheless label a monkey holding a guitar as a human, since in the training set guitars tend to co-occur with humans but not monkeys (Wang et al., 2018). Similar heuristics arise in visual question answering systems (Agrawal et al., 2016).</p>
<p>The current paper addresses this issue in the domain of natural language inference (NLI), the task of determining whether a premise sentence entails (i.e., implies the truth of) a hypothesis sentence (Condoravdi et al., 2003; Dagan et al., 2006; Bowman et al., 2015). As in other domains, neural NLI models have been shown to learn shallow heuristics, in this case based on the presence of specific words (Naik et al., 2018; Sanchez et al., 2018). For example, a model might assign a label of contradiction to any input containing the word not, since not often appears in the examples of contradiction in standard NLI training sets.</p>
<p>The focus of our work is on heuristics that are based on superficial syntactic properties. Consider the following sentence pair, which has the target label entailment:
(1) Premise: The judge was paid by the actor. Hypothesis: The actor paid the judge.</p>
<p>An NLI system that labels this example correctly might do so not by reasoning about the meanings of these sentences, but rather by assuming that the premise entails any hypothesis whose words all appear in the premise (Dasgupta et al., 2018; Naik et al., 2018). Crucially, if the model is using this heuristic, it will predict entailment for (2) as well, even though that label is incorrect in this case:
(2) Premise: The actor was paid by the judge. Hypothesis: The actor paid the judge.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Heuristic</th>
<th style="text-align: left;">Definition</th>
<th style="text-align: left;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lexical overlap</td>
<td style="text-align: left;">Assume that a premise entails all hypothe- <br> ses constructed from words in the premise</td>
<td style="text-align: left;">The doctor was paid by the actor. <br> $\xrightarrow[\text { WRONG }]{\longrightarrow}$ The doctor paid the actor.</td>
</tr>
<tr>
<td style="text-align: left;">Subsequence</td>
<td style="text-align: left;">Assume that a premise entails all of its <br> contiguous subsequences.</td>
<td style="text-align: left;">The doctor near the actor danced. <br> $\xrightarrow[\text { WRONG }]{\longrightarrow}$ The actor danced.</td>
</tr>
<tr>
<td style="text-align: left;">Constituent</td>
<td style="text-align: left;">Assume that a premise entails all complete <br> subtrees in its parse tree.</td>
<td style="text-align: left;">If the artist slept, the actor ran. <br> $\xrightarrow[\text { WRONG }]{\longrightarrow}$ The artist slept.</td>
</tr>
</tbody>
</table>
<p>Table 1: The heuristics targeted by the HANS dataset, along with examples of incorrect entailment predictions that these heuristics would lead to.</p>
<p>We introduce a new evaluation set called HANS (Heuristic Analysis for NLI Systems), designed to diagnose the use of such fallible structural heuristics. ${ }^{1}$ We target three heuristics, defined in Table 1. While these heuristics often yield correct labels, they are not valid inference strategies because they fail on many examples. We design our dataset around such examples, so that models that employ these heuristics are guaranteed to fail on particular subsets of the dataset, rather than simply show lower overall accuracy.</p>
<p>We evaluate four popular NLI models, including BERT, a state-of-the-art model (Devlin et al., 2019), on the HANS dataset. All models performed substantially below chance on this dataset, barely exceeding $0 \%$ accuracy in most cases. We conclude that their behavior is consistent with the hypothesis that they have adopted these heuristics.</p>
<p>Contributions: This paper has three main contributions. First, we introduce the HANS dataset, an NLI evaluation set that tests specific hypotheses about invalid heuristics that NLI models are likely to learn. Second, we use this dataset to illuminate interpretable shortcomings in state-of-the-art models trained on MNLI (Williams et al., 2018b); these shortcoming may arise from inappropriate model inductive biases, from insufficient signal provided by training datasets, or both. Third, we show that these shortcomings can be made less severe by augmenting a model's training set with the types of examples present in HANS. These results indicate that there is substantial room for improvement for current NLI models and datasets, and that HANS can serve as a tool for motivating and measuring progress in this area.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>2 Syntactic Heuristics</h2>
<p>We focus on three heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic, all defined in Table 1. These heuristics form a hierarchy: the constituent heuristic is a special case of the subsequence heuristic, which in turn is a special case of the lexical overlap heuristic. Table 2 in the next page gives examples where each heuristic succeeds and fails.</p>
<p>There are two reasons why we expect these heuristics to be adopted by a statistical learner trained on standard NLI training datasets such as SNLI (Bowman et al., 2015) or MNLI (Williams et al., 2018b). First, the MNLI training set contains far more examples that support the heuristics than examples that contradict them: ${ }^{2}$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Heuristic</th>
<th style="text-align: left;">Supporting <br> Cases</th>
<th style="text-align: left;">Contradicting <br> Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lexical overlap</td>
<td style="text-align: left;">2,158</td>
<td style="text-align: left;">261</td>
</tr>
<tr>
<td style="text-align: left;">Subsequence</td>
<td style="text-align: left;">1,274</td>
<td style="text-align: left;">72</td>
</tr>
<tr>
<td style="text-align: left;">Constituent</td>
<td style="text-align: left;">1,004</td>
<td style="text-align: left;">58</td>
</tr>
</tbody>
</table>
<p>Even the 261 contradicting cases in MNLI may not provide strong evidence against the heuristics. For example, 133 of these cases contain negation in the premise but not the hypothesis, as in (3). Instead of using these cases to overrule the lexical overlap heuristic, a model might account for them by learning to assume that the label is contradiction whenever there is negation in the premise but not the hypothesis (McCoy and Linzen, 2019):
(3) a. I don't care. $\rightarrow$ I care.
b. This is not a contradiction. $\rightarrow$ This is a contradiction.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Heuristic</th>
<th style="text-align: left;">Premise</th>
<th style="text-align: left;">Hypothesis</th>
<th style="text-align: center;">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lexical</td>
<td style="text-align: left;">The banker near the judge saw the actor.</td>
<td style="text-align: left;">The banker saw the actor.</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: left;">overlap</td>
<td style="text-align: left;">The lawyer was advised by the actor.</td>
<td style="text-align: left;">The actor advised the lawyer.</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: left;">heuristic</td>
<td style="text-align: left;">The doctors visited the lawyer.</td>
<td style="text-align: left;">The lawyer visited the doctors.</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">The judge by the actor stopped the banker.</td>
<td style="text-align: left;">The banker stopped the actor.</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Subsequence</td>
<td style="text-align: left;">The artist and the student called the judge.</td>
<td style="text-align: left;">The student called the judge.</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: left;">heuristic</td>
<td style="text-align: left;">Angry tourists helped the lawyer.</td>
<td style="text-align: left;">Tourists helped the lawyer.</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">The judges heard the actors resigned.</td>
<td style="text-align: left;">The judges heard the actors.</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">The senator near the lawyer danced.</td>
<td style="text-align: left;">The lawyer danced.</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Constituent</td>
<td style="text-align: left;">Before the actor slept, the senator ran.</td>
<td style="text-align: left;">The actor slept.</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: left;">heuristic</td>
<td style="text-align: left;">The lawyer knew that the judges shouted.</td>
<td style="text-align: left;">The judges shouted.</td>
<td style="text-align: center;">E</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">If the actor slept, the judge saw the artist.</td>
<td style="text-align: left;">The actor slept.</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">The lawyers resigned, or the artist slept.</td>
<td style="text-align: left;">The artist slept.</td>
<td style="text-align: center;">N</td>
</tr>
</tbody>
</table>
<p>Table 2: Examples of sentences used to test the three heuristics. The label column shows the correct label for the sentence pair; $E$ stands for entailment and $N$ stands for non-entailment. A model relying on the heuristics would label all examples as entailment (incorrectly for those marked as N ).</p>
<p>There are some examples in MNLI that contradict the heuristics in ways that are not easily explained away by other heuristics; see Appendix A for examples. However, such cases are likely too rare to discourage a model from learning these heuristics. MNLI contains data from multiple genres, so we conjecture that the scarcity of contradicting examples is not just a property of one genre, but rather a general property of NLI data generated in the crowdsourcing approach used for MNLI. We thus hypothesize that any crowdsourced NLI dataset would make our syntactic heuristics attractive to statistical learners without strong linguistic priors.</p>
<p>The second reason we might expect current NLI models to adopt these heuristics is that their input representations may make them susceptible to these heuristics. The lexical overlap heuristic disregards the order of the words in the sentence and considers only their identity, so it is likely to be adopted by bag-of-words NLI models (e.g., Parikh et al. 2016). The subsequence heuristic considers linearly adjacent chunks of words, so one might expect it to be adopted by standard RNNs, which process sentences in linear order. Finally, the constituent heuristic appeals to components of the parse tree, so one might expect to see it adopted by tree-based NLI models (Bowman et al., 2016).</p>
<h2>3 Dataset Construction</h2>
<p>For each heuristic, we generated five templates for examples that support the heuristic and five tem-
plates for examples that contradict it. Below is one template for the subsequence heuristic; see Appendix B for a full list of templates.
(4) The $\mathrm{N}<em 2="2">{1} \mathrm{P}$ the $\mathrm{N}</em>$.} \mathrm{~V} . \nrightarrow$ The $\mathrm{N}_{2} \mathrm{~V</p>
<p>The lawyer by the actor ran. $\rightarrow$ The actor ran.
We generated 1,000 examples from each template, for a total of 10,000 examples per heuristic. Some heuristics are special cases of others, but we made sure that the examples for one heuristic did not also fall under a more narrowly defined heuristic. That is, for lexical overlap cases, the hypothesis was not a subsequence or constituent of the premise; for subsequence cases, the hypothesis was not a constituent of the premise.</p>
<h3>3.1 Dataset Controls</h3>
<p>Plausibility: One advantage of generating examples from templates-instead of, e.g., modifying naturally-occurring examples-is that we can ensure the plausibility of all generated sentences. For example, we do not generate cases such as The student read the book $\rightarrow$ The book read the student, which could ostensibly be solved using a hypothesis-plausibility heuristic. To achieve this, we drew our core vocabulary from Ettinger et al. (2018), where every noun was a plausible subject of every verb or a plausible object of every transitive verb. Some templates required expanding this core vocabulary; in those cases, we manually curated the additions to ensure plausibility.</p>
<h1>3.4.2.2.2.2.3.2.3.3.</h1>
<h2>Selectional criteria:</h2>
<p>Some of our example types depend on the availability of lexically-specific verb frames. For example, (5) requires awareness of the fact that believed can take a clause (the lawyer saw the officer) as its complement:
(5) The doctor believed the lawyer saw the officer.
$\rightarrow$ The doctor believed the lawyer.
It is arguably unfair to expect a model to understand this example if it had only ever encountered believe with a noun phrase object (e.g., I believed the man). To control for this issue, we only chose verbs that appeared at least 50 times in the MNLI training set in all relevant frames.</p>
<h2>4 Experimental Setup</h2>
<p>Since HANS is designed to probe for structural heuristics, we selected three models that exemplify popular strategies for representing the input sentence: DA, a bag-of-words model; ESIM, which uses a sequential structure; and SPINN, which uses a syntactic parse tree. In addition to these three models, we included BERT, a state-of-the-art model for MNLI. The following paragraphs provide more details on these models.</p>
<p>DA: The Decomposable Attention model (DA; Parikh et al., 2016) uses a form of attention to align words in the premise and hypothesis and to make predictions based on the aggregation of this alignment. It uses no word order information and can thus be viewed as a bag-of-words model.</p>
<p>ESIM: The Enhanced Sequential Inference Model (ESIM; Chen et al., 2017) uses a modified bidirectional LSTM to encode sentences. We use the variant with a sequential encoder, rather than the tree-based Hybrid Inference Model (HIM).</p>
<p>SPINN: The Stack-augmented ParserInterpreter Neural Network (SPINN; Bowman et al., 2016) is tree-based: it encodes sentences by combining phrases based on a syntactic parse. We use the SPINN-PI-NT variant, which takes a parse tree as an input (rather than learning to parse). For MNLI, we used the parses provided in the MNLI release; for HANS, we used parse templates that we created based on parses from the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), the same parser used to parse MNLI. Based on manual inspection, this parser generally provided correct parses for HANS examples.</p>
<p>BERT: The Bidirectional Encoder Representations from Transformers model (BERT; Devlin et al., 2019) is a Transformer model that uses attention, rather than recurrence, to process sentences. We use the bert-base-uncased pretrained model and fine-tune it on MNLI.</p>
<p>Implementation and evaluation: For DA and ESIM, we used the implementations from AllenNLP (Gardner et al., 2017). For SPINN ${ }^{3}$ and BERT, ${ }^{4}$ we used code from the GitHub repositories for the papers introducing those models.</p>
<p>We trained all models on MNLI. MNLI uses three labels (entailment, contradiction, and neutral). We chose to annotate HANS with two labels only (entailment and non-entailment) because the distinction between contradiction and neutral was often unclear for our cases. ${ }^{5}$ For evaluating a model on HANS, we took the highest-scoring label out of entailment, contradiction, and neutral; we then translated contradiction or neutral labels to non-entailment. An alternate approach would have been to add the contradiction and neutral scores to determine a score for non-entailment; we found little difference between these approaches, since the models almost always assigned more than $50 \%$ of the label probability to a single label. ${ }^{6}$</p>
<h2>5 Results</h2>
<p>All models achieved high scores on the MNLI test set (Figure 1a), replicating the accuracies found in past work (DA: Gururangan et al. 2018; ESIM: Williams et al. 2018b; SPINN: Williams et al. 2018a; BERT: Devlin et al. 2019). On the HANS dataset, all models almost always assigned the correct label in the cases where the label is entailment, i.e., where the correct answer is in line with the hypothesized heuristics. However, they all performed poorly-with accuracies less than $10 \%$ in most cases, when chance is $50 \%$-on the cases where the heuristics make incorrect predictions</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Accuracy on the MNLI test set. (b) Accuracies on six sub-components of the HANS evaluation set; each sub-component is defined by its correct label and the heuristic it addresses. The dashed lines indicate chance performance. All models behaved as we would expect them to if they had adopted the heuristics targeted by HANS. That is, they nearly always predicted entailment for the examples in HANS, leading to near-perfect accuracy when the true label is entailment, and near-zero accuracy when the true label is non-entailment.
(Figure 1b). Thus, despite their high scores on the MNLI test set, all four models behaved in a way consistent with the use of the heuristics targeted in HANS, and not with the correct rules of inference.</p>
<p>Comparison of models: Both DA and ESIM had near-zero performance across all three heuristics. These models might therefore make no distinction between the three heuristics, but instead treat them all as the same phenomenon, i.e. lexical overlap. Indeed, for DA, this must be the case, as this model does not have access to word order; ESIM does in theory have access to word order information but does not appear to use it here.</p>
<p>SPINN had the best performance on the subsequence cases. This might be due to the treebased nature of its input: since the subsequences targeted in these cases were explicitly chosen not to be constituents, they do not form cohesive units in SPINN's input in the way they do for sequential models. SPINN also outperformed DA and ESIM on the constituent cases, suggesting that SPINN's tree-based representations moderately helped it learn how specific constituents contribute to the overall sentence. Finally, SPINN did worse than the other models on constituent cases where the correct answer is entailment. This moderately greater balance between accuracy on entailment and non-entailment cases further indicates that SPINN is less likely than the other models to assume that constituents of the premise are entailed; this harms its performance in cases where that assumption happens to lead to the correct answer.</p>
<p>BERT did slightly worse than SPINN on the subsequence cases, but performed noticeably less
poorly than all other models at both the constituent and lexical overlap cases (though it was still far below chance). Its performance particularly stood out for the lexical overlap cases, suggesting that some of BERT's success at MNLI may be due to a greater tendency to incorporate word order information compared to other models.</p>
<p>Analysis of particular example types: In the cases where a model's performance on a heuristic was perceptibly above zero, accuracy was not evenly spread across subcases (for case-by-case results, see Appendix C). For example, within the lexical overlap cases, BERT achieved $39 \%$ accuracy on conjunction (e.g., The actor and the doctor saw the artist $\rightarrow$ The actor saw the doctor) but $0 \%$ accuracy on subject/object swap (The judge called the lawyer $\rightarrow$ The lawyer called the judge). Within the constituent heuristic cases, BERT achieved $49 \%$ accuracy at determining that a clause embedded under if and other conditional words is not entailed (If the doctor resigned, the lawyer danced $\rightarrow$ The doctor resigned), but $0 \%$ accuracy at identifying that the clause outside of the conditional clause is also not entailed (If the doctor resigned, the lawyer danced $\rightarrow$ The lawyer danced).</p>
<h2>6 Discussion</h2>
<p>Independence of heuristics: Though each heuristic is most closely related to one class of model (e.g., the constituent heuristic is related to tree-based models), all models failed on cases illustrating all three heuristics. This finding is unsurprising since these heuristics are closely related</p>
<p>to each other, meaning that an NLI model may adopt all of them, even the ones not specifically targeting that class of model. For example, the subsequence and constituent heuristics are special cases of the lexical overlap heuristic, so all models can fail on cases illustrating all heuristics, because all models have access to individual words.</p>
<p>Though the heuristics form a hierarchy-the constituent heuristic is a subcase of the subsequence heuristic, which is a subcase of the lexical overlap heuristic-this hierarchy does not necessarily predict the performance of our models. For example, BERT performed worse on the subsequence heuristic than on the constituent heuristic, even though the constituent heuristic is a special case of the subsequence heuristic. Such behavior has two possible causes. First, it could be due to the specific cases we chose for each heuristic: the cases chosen for the subsequence heuristic may be inherently more challenging than the cases chosen for the constituent heuristic, even though the constituent heuristic as a whole is a subset of the subsequence one. Alternately, it is possible for a model to adopt a more general heuristic (e.g., the subsequence heuristic) but to make an exception for some special cases (e.g., the cases to which the constituent heuristic could apply).</p>
<p>Do the heuristics arise from the architecture or the training set? The behavior of a trained model depends on both the training set and the model's architecture. The models' poor results on HANS could therefore arise from architectural limitations, from insufficient signal in the MNLI training set, or from both.</p>
<p>The fact that SPINN did markedly better at the constituent and subsequence cases than ESIM and DA, even though the three models were trained on the same dataset, suggests that MNLI does contain some signal that can counteract the appeal of the syntactic heuristics tested by HANS. SPINN's structural inductive biases allow it to leverage this signal, but the other models' biases do not.</p>
<p>Other sources of evidence suggest that the models' failure is due in large part to insufficient signal from the MNLI training set, rather than the models' representational capacities alone. The BERT model we used (bert-base-uncased) was found by Goldberg (2019) to achieve strong results in syntactic tasks such as subject-verb agreement prediction, a task that minimally requires a distinction between the subject and direct object of a sen-
tence (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018). Despite this evidence that BERT has access to relevant syntactic information, its accuracy was $0 \%$ on the subject-object swap cases (e.g., The doctor saw the lawyer $\rightarrow$ The lawyer saw the doctor). We believe it is unlikely that our fine-tuning step on MNLI, a much smaller corpus than the corpus BERT was trained on, substantially changed the model's representational capabilities. Even though the model most likely had access to information about subjects and objects, then, MNLI did not make it clear how that information applies to inference. Supporting this conclusion, McCoy et al. (2019) found little evidence of compositional structure in the InferSent model, which was trained on SNLI, even though the same model type (an RNN) did learn clear compositional structure when trained on tasks that underscored the need for such structure. These results further suggest that the models' poor compositional behavior arises more because of the training set than because of model architecture.</p>
<p>Finally, our BERT-based model differed from the other models in that it was pretrained on a massive amount of data on a masking task and a next-sentence classification task, followed by finetuning on MNLI, while the other models were only trained on MNLI; we therefore cannot rule out the possibility that BERT's comparative success at HANS was due to the greater amount of data it has encountered rather than any architectural features.</p>
<p>Is the dataset too difficult? To assess the difficulty of our dataset, we obtained human judgments on a subset of HANS from 95 participants on Amazon Mechanical Turk as well as 3 expert annotators (linguists who were unfamiliar with HANS: 2 graduate students and 1 postdoctoral researcher). The average accuracy was $76 \%$ for Mechanical Turk participants and $97 \%$ for expert annotators; further details are in Appendix F.</p>
<p>Our Mechanical Turk results contrast with those of Nangia and Bowman (2019), who report an accuracy of $92 \%$ in the same population on examples from MNLI; this indicates that HANS is indeed more challenging for humans than MNLI is. The difficulty of some of our examples is in line with past psycholinguistic work in which humans have been shown to incorrectly answer comprehension questions for some of our subsequence subcases. For example, in an experiment in which participants read the sentence As Jerry played the violin</p>
<p>gathered dust in the attic, some participants answered yes to the question Did Jerry play the violin? (Christianson et al., 2001).</p>
<p>Crucially, although Mechanical Turk annotators found HANS to be harder overall than MNLI, their accuracy was similar whether the correct answer was entailment ( $75 \%$ accuracy) or non-entailment ( $77 \%$ accuracy). The contrast between the balance in the human errors across labels and the stark imbalance in the models' errors (Figure 1b) indicates that human errors are unlikely to be driven by the heuristics targeted in the current work.</p>
<h2>7 Augmenting the training data with HANS-like examples</h2>
<p>The failure of the models we tested raises the question of what it would take to do well on HANS. One possibility is that a different type of model would perform better. For example, a model based on hand-coded rules might handle HANS well. However, since most models we tested are in theory capable of handling HANS's examples but failed to do so when trained on MNLI, it is likely that performance could also be improved by training the same architectures on a dataset in which these heuristics are less successful.</p>
<p>To test that hypothesis, we retrained each model on the MNLI training set augmented with a dataset structured exactly like HANS (i.e. using the same thirty subcases) but containing no specific examples that appeared in HANS. Our additions comprised 30,000 examples, roughly $8 \%$ of the size of the original MNLI training set (392,702 examples). In general, the models trained on the augmented MNLI performed very well on HANS (Figure 2); the one exception was that the DA model performed poorly on subcases for which a bag-of-words representation was inadequate. ${ }^{7}$ This experiment is only an initial exploration and leaves open many questions about the conditions under which a model will successfully avoid a heuristic; for example, how many contradicting examples are required? At the same time, these results do suggest that, to prevent a model from learning a heuristic, one viable approach is to use a training set that does not support this heuristic.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: HANS accuracies for models trained on MNLI plus examples of all 30 categories in HANS.</p>
<p>Transfer across HANS subcases: The positive results of the HANS-like augmentation experiment are compatible with the possibility that the models simply memorized the templates that made up HANS's thirty subcases. To address this, we retrained our models on MNLI augmented with subsets of the HANS cases (withholding some cases; see Appendix E for details), then tested the models on the withheld cases.</p>
<p>The results of one of the transfer experiments, using BERT, are shown in Table 3. There were some successful cases of transfer; e.g., BERT performed well on the withheld categories with sentence-initial adverbs, regardless of whether the correct label was non-entailment or entailment. Such successes suggest that BERT is able to learn from some specific subcases that it should rule out the broader heuristics; in this case, the nonwithheld cases plausibly informed BERT not to indiscriminately follow the constituent heuristic, encouraging it to instead base its judgments on the specific adverbs in question (e.g., certainly vs. probably). However, the models did not always transfer successfully; e.g., BERT had $0 \%$ accuracy on entailed passive examples when such examples were withheld, likely because the training set still included many non-entailed passive examples, meaning that BERT may have learned to assume that all sentences with passive premises are cases of non-entailment. Thus, though the models do seem to be able to rule out the broadest versions of the heuristics and transfer that knowledge to some new cases, they may still fall back to the heuristics for other cases. For further results involving withheld categories, see Appendix E.</p>
<p>Transfer to an external dataset: Finally, we tested models on the comp_same_short and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Withheld category</th>
<th style="text-align: center;">Results</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical overlap: Conjunctions $(\rightleftharpoons)$ The doctor saw the author and the tourist. $\rightleftharpoons$ The author saw the tourist.</td>
<td style="text-align: center;"><img alt="img-2.jpeg" src="img-2.jpeg" /></td>
</tr>
<tr>
<td style="text-align: center;">Lexical overlap: Passives $(\rightarrow)$ The authors were helped by the actor. $\rightarrow$ The actor helped the authors.</td>
<td style="text-align: center;"><img alt="img-3.jpeg" src="img-3.jpeg" /></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence: NP/Z $(\rightleftharpoons)$ Before the actor moved the doctor arrived. $\rightleftharpoons$ The actor moved the doctor.</td>
<td style="text-align: center;"><img alt="img-4.jpeg" src="img-4.jpeg" /></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence: PP on object $(\rightarrow)$ The authors saw the judges by the doctor. $\rightarrow$ The authors saw the judges.</td>
<td style="text-align: center;"><img alt="img-5.jpeg" src="img-5.jpeg" /></td>
</tr>
<tr>
<td style="text-align: center;">Constituent: Adverbs $(\rightleftharpoons)$ Probably the artists helped the authors. $\rightleftharpoons$ The artists helped the authors.</td>
<td style="text-align: center;"><img alt="img-6.jpeg" src="img-6.jpeg" /></td>
</tr>
<tr>
<td style="text-align: center;">Constituent: Adverbs $(\rightarrow)$ Certainly the lawyers shouted. $\rightarrow$ The lawyers shouted.</td>
<td style="text-align: center;"><img alt="img-7.jpeg" src="img-7.jpeg" /></td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracies for BERT fine-tuned on basic MNLI and on MNLI+, which is MNLI augmented with most HANS categories except withholding the categories in this table. The two lexical overlap cases shown here are adversarial in that MNLI+ contains cases superficially similar to them but with opposite labels (namely, the Conjunctions $(\rightarrow)$ and Passives $(\rightleftharpoons)$ cases from Table 4 in the Appendix). The remaining cases in this table are not adversarial in this way.
comp_same_long datasets from Dasgupta et al. (2018), which consist of lexical overlap cases:
(6) the famous and arrogant cat is not more nasty than the dog with glasses in a white dress. $\rightleftharpoons$ the dog with glasses in a white dress is not more nasty than the famous and arrogant cat.</p>
<p>This dataset differs from HANS in at least three important ways: it is based on a phenomenon not present in HANS (namely, comparatives); it uses a different vocabulary from HANS; and many of its sentences are semantically implausible.</p>
<p>We used this dataset to test both BERT finetuned on MNLI, and BERT fine-tuned on MNLI augmented with HANS-like examples. The augmentation improved performance modestly for the long examples and dramatically for the short examples, suggesting that training with HANS-like examples has benefits that extend beyond HANS. ${ }^{8}$</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Figure 3: Results on the lexical overlap cases from Dasgupta et al. (2018) for BERT fine-tuned on MNLI or on MNLI augmented with HANS-like examples.</p>
<h2>8 Related Work</h2>
<h3>8.1 Analyzing trained models</h3>
<p>This project relates to an extensive body of research on exposing and understanding weaknesses in models' learned behavior and representations. In the NLI literature, Poliak et al. (2018b) and Gururangan et al. (2018) show that, due to biases in NLI datasets, it is possible to achieve far better than chance accuracy on those datasets by only looking at the hypothesis. Other recent works address possible ways in which NLI models might use fallible heuristics, focusing on semantic phenomena, such as lexical inferences (Glockner et al., 2018) or quantifiers (Geiger et al., 2018), or biases based on specific words (Sanchez et al., 2018). Our work focuses instead on structural phenomena, following the proof-of-concept work done by Dasgupta et al. (2018). Our focus on using NLI to address how models capture structure follows some older work about using NLI for the evaluation of parsers (Rimell and Clark, 2010; Mehdad et al., 2010).</p>
<p>NLI has been used to investigate many other types of linguistic information besides syntactic structure (Poliak et al., 2018a; White et al., 2017). Outside NLI, multiple projects have used classification tasks to understand what linguistic and/or structural information is present in vector encodings of sentences (e.g., Adi et al., 2017; Ettinger et al., 2018; Conneau et al., 2018). We instead choose the behavioral approach of using task performance on critical cases. Unlike the classification approach, this approach is agnostic to model structure; our dataset could be used to evaluate a symbolic NLI system just as easily as a neural one, whereas typical classification approaches only work for models with vector representations.</p>
<h3>8.2 Structural heuristics</h3>
<p>Similar to our lexical overlap heuristic, Dasgupta et al. (2018), Nie et al. (2018), and Kim et al. (2018) also tested NLI models on specific phenomena where word order matters; we use a larger set of phenomena to study a more general notion of lexical overlap that is less dependent on the properties of a single phenomenon, such as passives. Naik et al. (2018) also find evidence that NLI models use a lexical overlap heuristic, but our approach is substantially different from theirs. ${ }^{9}$</p>
<p>This work builds on our pilot study in McCoy and Linzen (2019), which studied one of the subcases of the subsequence heuristic. Several of our subsequence subcases are inspired by psycholinguistics research (Bever, 1970; Frazier and Rayner, 1982; Tabor et al., 2004); these works have aims similar to ours but are concerned with the representations used by humans rather than neural networks.</p>
<p>Finally, all of our constituent heuristic subcases depend on the implicational behavior of specific words. Several past works (Pavlick and CallisonBurch, 2016; Rudinger et al., 2018; White et al., 2018; White and Rawlins, 2018) have studied such behavior for verbs (e.g., He knows it is raining entails It is raining, while He believes it is raining does not). We extend that approach by including other types of words with specific implicational behavior, namely conjunctions (and, or), prepositions that take clausal arguments (if, because), and adverbs (definitely, supposedly). MacCartney and Manning (2009) also discuss the implicational behavior of these various types of words within NLI.</p>
<h3>8.3 Generalization</h3>
<p>Our work suggests that test sets drawn from the same distribution as the training set may be inadequate for assessing whether a model has learned to perform the intended task. Instead, it is also necessary to evaluate on a generalization set that departs from the training distribution. McCoy et al. (2018) found a similar result for the task of question formation; different architectures that all succeeded on the test set failed on the generalization set in different ways, showing that the test set alone was not sufficient to determine what the models had</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>learned. This effect can arise not just from different architectures but also from different initializations of the same architecture (Weber et al., 2018).</p>
<h2>9 Conclusions</h2>
<p>Statistical learners such as neural networks closely track the statistical regularities in their training sets. This process makes them vulnerable to adopting heuristics that are valid for frequent cases but fail on less frequent ones. We have investigated three such heuristics that we hypothesize NLI models are likely to learn. To evaluate whether NLI models do behave consistently with these heuristics, we have introduced the HANS dataset, on which models using these heuristics are guaranteed to fail. We find that four existing NLI models perform very poorly on HANS, suggesting that their high accuracies on NLI test sets may be due to the exploitation of invalid heuristics rather than deeper understanding of language. However, these models performed significantly better on both HANS and on a separate structure-dependent dataset when their training data was augmented with HANS-like examples. Overall, our results indicate that, despite the impressive accuracies of state-of-the-art models on standard evaluations, there is still much progress to be made and that targeted, challenging datasets, such as HANS, are important for determining whether models are learning what they are intended to learn.</p>
<h2>Acknowledgments</h2>
<p>We are grateful to Adam Poliak, Benjamin Van Durme, Samuel Bowman, the members of the JSALT General-Purpose Sentence Representation Learning team, and the members of the Johns Hopkins Computation and Psycholinguistics Lab for helpful comments, and to Brian Leonard for assistance with the Mechanical Turk experiment. Any errors remain our own.</p>
<p>This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. 1746891 and the 2018 Jelinek Summer Workshop on Speech and Language Technology (JSALT). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or the JSALT workshop.</p>
<h2>References</h2>
<p>Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2017. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. In International Conference on Learning Representations.</p>
<p>Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016. Analyzing the behavior of visual question answering models. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1955-1960. Association for Computational Linguistics.</p>
<p>Thomas G. Bever. 1970. The cognitive basis for linguistic structures.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D. Manning, and Christopher Potts. 2016. A fast unified model for parsing and sentence understanding. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1466-1477. Association for Computational Linguistics.</p>
<p>Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1657-1668. Association for Computational Linguistics.</p>
<p>Kiel Christianson, Andrew Hollingworth, John F Halliwell, and Fernanda Ferreira. 2001. Thematic roles assigned along the garden path linger. Cognitive Psychology, 42(4):368-407.</p>
<p>Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Reinhard Stolle, and Daniel G. Bobrow. 2003. Entailment, intensionality and text understanding. In Proceedings of the HLT-NAACL 2003 Workshop on Text Meaning.</p>
<p>Alexis Conneau, Germn Kruszewski, Guillaume Lample, Loc Barrault, and Marco Baroni. 2018. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2126-2136. Association for Computational Linguistics.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Proceedings of the First In-
ternational Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW'05, pages 177-190, Berlin, Heidelberg. Springer-Verlag.</p>
<p>Ishita Dasgupta, Demi Guo, Andreas Stuhlmller, Samuel J. Gershman, and Noah D. Goodman. 2018. Evaluating compositionality in sentence embeddings. In Proceedings of the 40th Annual Conference of the Cognitive Science Society, pages 15961601, Madison, WI.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Allyson Ettinger, Ahmed Elgohary, Colin Phillips, and Philip Resnik. 2018. Assessing composition in sentence vector representations. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1790-1801. Association for Computational Linguistics.</p>
<p>Lyn Frazier and Keith Rayner. 1982. Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology, 14(2):178-210.</p>
<p>Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke S. Zettlemoyer. 2017. AllenNLP: A Deep Semantic Natural Language Processing Platform. In Proceedings of the Workshop for NLP Open Source Software (NLPOSS).</p>
<p>Atticus Geiger, Ignacio Cases, Lauri Karttunen, and Christopher Potts. 2018. Stress-testing neural models of natural language inference with multiply-quantified sentences. arXiv preprint arXiv:1810.13033.</p>
<p>Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking NLI Systems with Sentences that Require Simple Lexical Inferences. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 650-655. Association for Computational Linguistics.</p>
<p>Yoav Goldberg. 2019. Assessing BERT's syntactic abilities. arXiv preprint arXiv:1901.05287.</p>
<p>Kristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni. 2018. Colorless green recurrent networks dream hierarchically. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</p>
<p>Volume 1 (Long Papers), pages 1195-1205. Association for Computational Linguistics.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112. Association for Computational Linguistics.</p>
<p>Juho Kim, Christopher Malon, and Asim Kadav. 2018. Teaching syntax by adversarial distraction. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 79-84. Association for Computational Linguistics.</p>
<p>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</p>
<p>Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521535.</p>
<p>Bill MacCartney and Christopher D Manning. 2009. Natural language inference. Ph.D. thesis, Stanford University.</p>
<p>Rebecca Marvin and Tal Linzen. 2018. Targeted syntactic evaluation of language models. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192-1202. Association for Computational Linguistics.
R. Thomas McCoy, Robert Frank, and Tal Linzen. 2018. Revisiting the poverty of the stimulus: Hierarchical generalization without a hierarchical bias in recurrent neural networks. In Proceedings of the 40th Annual Conference of the Cognitive Science Society, pages 2093-2098, Madison, WI.
R. Thomas McCoy and Tal Linzen. 2019. Non-entailed subsequences as a challenge for natural language inference. In Proceedings of the Society for Computation in Linguistics, volume 2.
R. Thomas McCoy, Tal Linzen, Ewan Dunbar, and Paul Smolensky. 2019. RNNs implicitly implement tensor-product representations. In International Conference on Learning Representations.</p>
<p>Yashar Mehdad, Alessandro Moschitti, and Fabio Massimo Zanzotto. 2010. Syntactic/semantic structures for textual entailment recognition. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1020-1028. Association for Computational Linguistics.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 2018. Stress test evaluation for natural language inference. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2340-2353. Association for Computational Linguistics.</p>
<p>Nikita Nangia and Samuel R. Bowman. 2019. Human vs. muppet: A conservative estimate of human performance on the GLUE benchmark.</p>
<p>Yixin Nie, Yicheng Wang, and Mohit Bansal. 2018. Analyzing compositionality-sensitivity of NLI models. arXiv preprint arXiv:1811.07033.</p>
<p>Ankur Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249-2255. Association for Computational Linguistics.</p>
<p>Ellie Pavlick and Chris Callison-Burch. 2016. Tense manages to predict implicative behavior in verbs. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2225-2229. Association for Computational Linguistics.</p>
<p>Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. 2018a. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 67-81. Association for Computational Linguistics.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018b. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191. Association for Computational Linguistics.</p>
<p>Laura Rimell and Stephen Clark. 2010. Cambridge: Parser evaluation using textual entailment by grammatical relation comparison. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 268-271. Association for Computational Linguistics.</p>
<p>Rachel Rudinger, Aaron Steven White, and Benjamin Van Durme. 2018. Neural models of factuality. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 731-744. Association for Computational Linguistics.</p>
<p>Ivan Sanchez, Jeff Mitchell, and Sebastian Riedel. 2018. Behavior analysis of NLI models: Uncovering the influence of three factors on robustness. In Proceedings of the 2018 Conference of the North</p>
<p>American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1975-1985. Association for Computational Linguistics.</p>
<p>Whitney Tabor, Bruno Galantucci, and Daniel Richardson. 2004. Effects of merely local syntactic coherence on sentence processing. Journal of Memory and Language, 50(4):355-370.</p>
<p>Jianyu Wang, Zhishuai Zhang, Cihang Xie, Yuyin Zhou, Vittal Premachandran, Jun Zhu, Lingxi Xie, and Alan Yuille. 2018. Visual concepts and compositional voting. Annals of Mathematical Sciences and Applications, 3(1):151-188.</p>
<p>Noah Weber, Leena Shekhar, and Niranjan Balasubramanian. 2018. The fine line between linguistic generalization and failure in seq2seq-attention models. In Proceedings of the Workshop on Generalization in the Age of Deep Learning, pages 24-27. Association for Computational Linguistics.</p>
<p>Aaron Steven White, Pushpendre Rastogi, Kevin Duh, and Benjamin Van Durme. 2017. Inference is everything: Recasting semantic resources into a unified evaluation framework. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 996-1005. Asian Federation of Natural Language Processing.</p>
<p>Aaron Steven White and Kyle Rawlins. 2018. The role of veridicality and factivity in clause selection. In Proceedings of the 48th Annual Meeting of the North East Linguistic Society.</p>
<p>Aaron Steven White, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme. 2018. Lexicosyntactic inference in neural models. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4717-4724. Association for Computational Linguistics.</p>
<p>Adina Williams, Andrew Drozdov, and Samuel R. Bowman. 2018a. Do latent tree learning models identify meaningful structure in sentences? Transactions of the Association of Computational Linguistics, 6:253-267.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018b. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122. Association for Computational Linguistics.</p>
<h2>A MNLI examples that contradict the HANS heuristics</h2>
<p>The sentences in (7) show examples from the MNLI training set that contradict the lexical overlap, subsequence, and constituent
heuristics. The full set of all 261 contradicting examples in the MNLI training set may be viewed at https://github.com/ tommccoyl/hans/blob/master/mnli_ contradicting_examples.
(7) a. A subcategory of accuracy is consistency. $\nrightarrow$ Accuracy is a subcategory of consistency.
b. At the same time, top Enron executives were free to exercise their stock options, and some did. $\rightarrow$ Top Enron executives were free to exercise.
c. She was chagrined at The Nation's recent publication of a column by conservative education activist Ron Unz arguing that liberal education reform has been an unmitigated failure. $\rightarrow$ Liberal education reform has been an unmitigated failure.</p>
<h2>B Templates</h2>
<p>Tables 4, 5, and 6 contain the templates for the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic, respectively.</p>
<p>In some cases, a given template has multiple versions, such as one version where a noun phrase modifier attaches to the subject and another where the modifier attaches to the object. For clarity, we have only listed one version of each template here. The full list of templates can be viewed in the code on GitHub. ${ }^{10}$</p>
<h2>C Fine-grained results</h2>
<p>Table 7 shows the results by subcase for models trained on MNLI for the subcases where the correct answer is entailment. Table 8 shows the results by subcase for these models for the subcases where the correct answer is non-entailment.</p>
<h2>D Results for models trained on MNLI with neutral and contradiction merged</h2>
<p>Table 9 shows the results on HANS for models trained on MNLI with the labels neutral and contradiction merged in the training set into the single label non-entailment. The results are similar to the results obtained by merging the labels after training, with the models generally outputting entailment for all HANS examples, whether that was the correct answer or not.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subcase</th>
<th style="text-align: center;">Template</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Entailment: <br> Untangling relative clauses</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1}$ who the $\mathrm{N}</em>} \mathrm{~V<em 2="2">{1} \mathrm{~V}</em>}$ the $\mathrm{N<em 2="2">{3}$ $\rightarrow$ The $\mathrm{N}</em>} \mathrm{~V<em 1="1">{1}$ the $\mathrm{N}</em>$.</td>
<td style="text-align: center;">The athlete who the judges admired called the manager. <br> $\rightarrow$ The judges admired the athlete.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> Sentences with PPs</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1} \mathrm{P}$ the $\mathrm{N}</em>} \mathrm{~V}$ the $\mathrm{N<em 1="1">{3}$ <br> $\rightarrow$ The $\mathrm{N}</em>$} \mathrm{~V}$ the $\mathrm{N}_{3</td>
<td style="text-align: center;">The tourists by the actor recommended the authors. <br> $\rightarrow$ The tourists recommended the authors.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> Sentences with relative clauses</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1}$ that $\mathrm{V}</em>} \mathrm{~V<em 2="2">{1}$ the $\mathrm{N}</em>}$ <br> $\rightarrow$ The $\mathrm{N<em 1="1">{1} \mathrm{~V}</em>$}$ the $\mathrm{N}_{2</td>
<td style="text-align: center;">The actors that danced saw the author. <br> $\rightarrow$ The actors saw the author.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> Conjunctions</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1} \mathrm{~V}$ the $\mathrm{N}</em>}$ and the $\mathrm{N<em 1="1">{3}$ <br> $\rightarrow$ The $\mathrm{N}</em>$} \mathrm{~V}$ the $\mathrm{N}_{3</td>
<td style="text-align: center;">The secretaries encouraged the scientists and the actors. <br> $\rightarrow$ The secretaries encouraged the actors.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> Passives</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1}$ were V by the $\mathrm{N}</em>}$ <br> $\rightarrow$ The $\mathrm{N<em 2="2">{1} \mathrm{~V}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">The authors were supported by the tourists. <br> $\rightarrow$ The tourists supported the authors.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> Subject-object swap</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1} \mathrm{~V}$ the $\mathrm{N}</em>}$. <br> $\rightarrow$ The $\mathrm{N<em 1="1">{2} \mathrm{~V}$ the $\mathrm{N}</em>$.</td>
<td style="text-align: center;">The senators mentioned the artist. <br> $\rightarrow$ The artist mentioned the senators.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> Sentences with PPs</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1} \mathrm{P}$ the $\mathrm{N}</em>} \mathrm{~V}$ the $\mathrm{N<em 3="3">{3}$ <br> $\rightarrow$ The $\mathrm{N}</em>$} \mathrm{~V}$ the $\mathrm{N}_{2</td>
<td style="text-align: center;">The judge behind the manager saw the doctors. <br> $\rightarrow$ The doctors saw the manager.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> Sentences with relative clauses</td>
<td style="text-align: center;">The $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$ the $\mathrm{N<em 3="3">{2}$ who the $\mathrm{N}</em>} \mathrm{~V<em 2="2">{2}$ <br> $\rightarrow$ The $\mathrm{N}</em>} \mathrm{~V<em 3="3">{1}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">The actors advised the manager who the tourists saw. <br> $\rightarrow$ The manager advised the tourists.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> Conjunctions</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1} \mathrm{~V}$ the $\mathrm{N}</em>}$ and the $\mathrm{N<em 2="2">{3}$ <br> $\rightarrow$ The $\mathrm{N}</em>$} \mathrm{~V}$ the $\mathrm{N}_{3</td>
<td style="text-align: center;">The doctors advised the presidents and the tourists. <br> $\rightarrow$ The presidents advised the tourists.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> Passives</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1}$ were V by the $\mathrm{N}</em>}$ <br> $\rightarrow$ The $\mathrm{N<em 2="2">{1} \mathrm{~V}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">The senators were recommended by the managers. <br> $\rightarrow$ The senators recommended the managers.</td>
</tr>
</tbody>
</table>
<p>Table 4: Templates for the lexical overlap heuristic</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subcase</th>
<th style="text-align: center;">Template</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Entailment: <br> Conjunctions</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1}$ and the $\mathrm{N}</em>} \mathrm{~V}$ the $\mathrm{N<em 2="2">{3}$ <br> $\rightarrow$ The $\mathrm{N}</em>$} \mathrm{~V}$ the $\mathrm{N}_{3</td>
<td style="text-align: center;">The actor and the professor mentioned the lawyer. <br> $\rightarrow$ The professor mentioned the lawyer.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> Adjectives</td>
<td style="text-align: center;">$\operatorname{Adj} \mathrm{N}<em 2="2">{1} \mathrm{~V}$ the $\mathrm{N}</em>}$ <br> $\rightarrow \mathrm{N<em 2="2">{1} \mathrm{~V}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">Happy professors mentioned the lawyer. <br> $\rightarrow$ Professors mentioned the lawyer.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> Understood argument</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1} \mathrm{~V}$ the $\mathrm{N}</em>$}$ <br> $\rightarrow$ The $\mathrm{N}_{1} \mathrm{~V</td>
<td style="text-align: center;">The author read the book. <br> $\rightarrow$ The author read.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> Relative clause on object</td>
<td style="text-align: center;">The $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$ the $\mathrm{N<em 2="2">{2}$ that $\mathrm{V}</em>}$ the $\mathrm{N<em 1="1">{3}$ <br> $\rightarrow$ The $\mathrm{N}</em>} \mathrm{~V<em 2="2">{1}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">The artists avoided the senators that thanked the tourists. <br> $\rightarrow$ The artists avoided the senators.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> PP on object</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1} \mathrm{~V}$ the $\mathrm{N}</em>} \mathrm{P}$ the $\mathrm{N<em 1="1">{3}$ <br> $\rightarrow$ The $\mathrm{N}</em>$} \mathrm{~V}$ the $\mathrm{N}_{2</td>
<td style="text-align: center;">The authors supported the judges in front of the doctor. <br> $\rightarrow$ The authors supported the judges.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> NP/S</td>
<td style="text-align: center;">The $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$ the $\mathrm{N<em 2="2">{2} \mathrm{~V}</em>}$ the $\mathrm{N<em 1="1">{3}$ <br> $\rightleftharpoons$ The $\mathrm{N}</em>} \mathrm{~V<em 2="2">{1}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">The managers heard the secretary encouraged the author. <br> $\rightleftharpoons$ The managers heard the secretary.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> PP on subject</td>
<td style="text-align: center;">The $\mathrm{N}<em 2="2">{1} \mathrm{P}$ the $\mathrm{N}</em>$} \mathrm{~V}$ <br> $\rightleftharpoons$ The $\mathrm{N}_{2} \mathrm{~V</td>
<td style="text-align: center;">The managers near the scientist resigned. <br> $\rightleftharpoons$ The scientist resigned.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> Relative clause on subject</td>
<td style="text-align: center;">The $\mathrm{N}<em 1="1">{1}$ that $\mathrm{V}</em>}$ the $\mathrm{N<em 2="2">{2} \mathrm{~V}</em>}$ the $\mathrm{N<em 2="2">{3}$ <br> $\rightleftharpoons$ The $\mathrm{N}</em>} \mathrm{~V<em 3="3">{2}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">The secretary that admired the senator saw the actor. <br> $\rightleftharpoons$ The senator saw the actor.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> MV/RR</td>
<td style="text-align: center;">The $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>} \mathrm{P}$ the $\mathrm{N<em 2="2">{2} \mathrm{~V}</em>}$ <br> $\rightleftharpoons$ The $\mathrm{N<em 1="1">{1} \mathrm{~V}</em>$} \mathrm{P}$ the $\mathrm{N}_{2</td>
<td style="text-align: center;">The senators paid in the office danced. <br> $\rightleftharpoons$ The senators paid in the office.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> NP/Z</td>
<td style="text-align: center;">P the $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$ the $\mathrm{N<em 2="2">{2} \mathrm{~V}</em>}$ the $\mathrm{N<em 1="1">{3}$ <br> $\rightleftharpoons$ The $\mathrm{N}</em>} \mathrm{~V<em 2="2">{1}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">Before the actors presented the professors advised the manager. <br> $\rightleftharpoons$ The actors presented the professors.</td>
</tr>
</tbody>
</table>
<p>Table 5: Templates for the subsequence heuristic</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subcase</th>
<th style="text-align: center;">Template</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Entailment: <br> Embedded under preposition</td>
<td style="text-align: center;">P the $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$, the $\mathrm{N<em 2="2">{2} \mathrm{~V}</em>}$ the $\mathrm{N<em 1="1">{3}$ <br> $\rightarrow$ The $\mathrm{N}</em>$} \mathrm{~V}_{1</td>
<td style="text-align: center;">Because the banker ran, the doctors saw the professors. <br> $\rightarrow$ The banker ran.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> Outside embedded clause</td>
<td style="text-align: center;">P the $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$ the $\mathrm{N<em 3="3">{2}$, the $\mathrm{N}</em>} \mathrm{~V<em 4="4">{2}$ <br> the $\mathrm{N}</em>}$ <br> $\rightarrow$ The $\mathrm{N<em 2="2">{3} \mathrm{~V}</em>$}$ the $\mathrm{N}_{4</td>
<td style="text-align: center;">Although the secretaries recommended the managers, the judges supported the scientist. <br> $\rightarrow$ The judges supported the scientist.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> Embedded under verb</td>
<td style="text-align: center;">The $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$ that the $\mathrm{N<em 2="2">{2} \mathrm{~V}</em>}$ <br> $\rightarrow$ The $\mathrm{N<em 2="2">{2} \mathrm{~V}</em>$</td>
<td style="text-align: center;">The president remembered that the actors performed. <br> $\rightarrow$ The actors performed.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> Conjunction</td>
<td style="text-align: center;">The $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$, and the $\mathrm{N<em 2="2">{2} \mathrm{~V}</em>}$ the $\mathrm{N<em 2="2">{3}$. <br> $\rightarrow$ The $\mathrm{N}</em>} \mathrm{~V<em 3="3">{2}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">The lawyer danced, and the judge supported the doctors. <br> $\rightarrow$ The judge supported the doctors.</td>
</tr>
<tr>
<td style="text-align: center;">Entailment: <br> Adverbs</td>
<td style="text-align: center;">Adv the N V <br> $\rightarrow$ The N V</td>
<td style="text-align: center;">Certainly the lawyers resigned. <br> $\rightarrow$ The lawyers resigned.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> Embedded under preposition</td>
<td style="text-align: center;">P the $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$, the $\mathrm{N<em 2="2">{2} \mathrm{~V}</em>}$ the $\mathrm{N<em 1="1">{2}$ <br> $\rightleftharpoons$ The $\mathrm{N}</em>$} \mathrm{~V}_{1</td>
<td style="text-align: center;">Unless the senators ran, the professors recommended the doctor. <br> $\rightleftharpoons$ The senators ran.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> Outside embedded clause</td>
<td style="text-align: center;">P the $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$ the $\mathrm{N<em 3="3">{2}$, the $\mathrm{N}</em>} \mathrm{~V<em 4="4">{2}$ <br> the $\mathrm{N}</em>}$ <br> $\rightleftharpoons$ The $\mathrm{N<em 2="2">{3} \mathrm{~V}</em>$}$ the $\mathrm{N}_{4</td>
<td style="text-align: center;">Unless the authors saw the students, the doctors helped the bankers. <br> $\rightleftharpoons$ The doctors helped the bankers.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> Embedded under verb</td>
<td style="text-align: center;">The $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$ that the $\mathrm{N<em 2="2">{2} \mathrm{~V}</em>}$ the $\mathrm{N<em 2="2">{3}$ <br> $\rightleftharpoons$ The $\mathrm{N}</em>} \mathrm{~V<em 3="3">{2}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">The tourists said that the lawyer saw the banker. <br> $\rightleftharpoons$ The lawyer saw the banker.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> Disjunction</td>
<td style="text-align: center;">The $\mathrm{N}<em 1="1">{1} \mathrm{~V}</em>}$, or the $\mathrm{N<em 2="2">{2} \mathrm{~V}</em>}$ the $\mathrm{N<em 2="2">{3}$ <br> $\rightleftharpoons$ The $\mathrm{N}</em>} \mathrm{~V<em 3="3">{2}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">The judges resigned, or the athletes mentioned the author. <br> $\rightleftharpoons$ The athletes mentioned the author.</td>
</tr>
<tr>
<td style="text-align: center;">Non-entailment: <br> Adverbs</td>
<td style="text-align: center;">Adv the $\mathrm{N}<em 2="2">{1} \mathrm{~V}$ the $\mathrm{N}</em>}$ <br> $\rightleftharpoons$ The $\mathrm{N<em 2="2">{1} \mathrm{~V}$ the $\mathrm{N}</em>$</td>
<td style="text-align: center;">Probably the artists saw the authors. <br> $\rightleftharpoons$ The artists saw the authors.</td>
</tr>
</tbody>
</table>
<p>Table 6: Templates for the constituent heuristic</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Heuristic</th>
<th style="text-align: center;">Subcase</th>
<th style="text-align: center;">DA</th>
<th style="text-align: center;">ESIM</th>
<th style="text-align: center;">SPINN</th>
<th style="text-align: center;">BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical overlap</td>
<td style="text-align: center;">Untangling relative clauses <br> The athlete who the judges saw called the manager. $\rightarrow$ The judges saw the athlete.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentences with PPs <br> The tourists by the actor called the authors. $\rightarrow$ The tourists called the authors.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentences with relative clauses <br> The actors that danced encouraged the author. $\rightarrow$ The actors encouraged the author.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Conjunctions <br> The secretaries saw the scientists and the actors. $\rightarrow$ The secretaries saw the actors.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Passives <br> The authors were supported by the tourists. $\rightarrow$ The tourists supported the authors.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">Conjunctions <br> The actor and the professor shouted. $\rightarrow$ The professor shouted.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Adjectives <br> Happy professors mentioned the lawyer. $\rightarrow$ Professors mentioned the lawyer.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Understood argument <br> The author read the book. $\rightarrow$ The author read.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Relative clause on object <br> The artists avoided the actors that performed. $\rightarrow$ The artists avoided the actors.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PP on object <br> The authors called the judges near the doctor. $\rightarrow$ The authors called the judges.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Embedded under preposition <br> Because the banker ran, the doctors saw the professors. $\rightarrow$ The banker ran.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Outside embedded clause <br> Although the secretaries slept, the judges danced. $\rightarrow$ The judges danced.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Embedded under verb <br> The president remembered that the actors performed. $\rightarrow$ The actors performed.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Conjunction <br> The lawyer danced, and the judge supported the doctors. $\rightarrow$ The lawyer danced.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Adverbs <br> Certainly the lawyers advised the manager. $\rightarrow$ The lawyers advised the manager.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Results for the subcases where the correct label is entailment.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Heuristic</th>
<th style="text-align: center;">Subcase</th>
<th style="text-align: center;">DA</th>
<th style="text-align: center;">ESIM</th>
<th style="text-align: center;">SPINN</th>
<th style="text-align: center;">BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical overlap</td>
<td style="text-align: center;">Subject-object swap <br> The senators mentioned the artist. $\nrightarrow$ The artist mentioned the senators.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentences with PPs <br> The judge behind the manager saw the doctors. $\nrightarrow$ The doctors saw the manager.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentences with relative clauses <br> The actors called the banker who the tourists saw. $\nrightarrow$ The banker called the tourists.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Conjunctions <br> The doctors saw the presidents and the tourists. $\nrightarrow$ The presidents saw the tourists.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Passives <br> The senators were helped by the managers. $\nrightarrow$ The senators helped the managers.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">NP/S <br> The managers heard the secretary. <br> PP on subject <br> The managers near the scientist shouted. $\nrightarrow$ The scientist shouted.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Relative clause on subject <br> The secretary that admired the senator saw the actor. $\nrightarrow$ The senator saw the actor.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MV/RR <br> The senators paid in the office danced. $\nrightarrow$ The senators paid in the office.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NP/Z <br> Before the actors presented the doctors arrived. $\nrightarrow$ The actors presented the doctors.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Embedded under preposition <br> Unless the senators ran, the professors recommended the doctor. $\nrightarrow$ The senators ran.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Outside embedded clause <br> Unless the authors saw the students, the doctors resigned. $\nrightarrow$ The doctors resigned.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Embedded under verb <br> The tourists said that the lawyer saw the banker. $\nrightarrow$ The lawyer saw the banker.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Disjunction <br> The judges resigned, or the athletes saw the author. $\nrightarrow$ The athletes saw the author.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Adverbs <br> Probably the artists saw the authors. $\nrightarrow$ The artists saw the authors.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 8: Results for the subcases where the correct label is non-entailment.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">Correct: Entailment</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Correct: Non-entailment</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: left;">Model class</td>
<td style="text-align: center;">Lexical</td>
<td style="text-align: center;">Subseq.</td>
<td style="text-align: center;">Const.</td>
<td style="text-align: center;">Lexical</td>
<td style="text-align: center;">Subseq.</td>
<td style="text-align: center;">Const.</td>
</tr>
<tr>
<td style="text-align: left;">DA</td>
<td style="text-align: left;">Bag-of-words</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: left;">ESIM</td>
<td style="text-align: left;">RNN</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">SPINN</td>
<td style="text-align: left;">TreeRNN</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.20</td>
</tr>
</tbody>
</table>
<p>Table 9: Results for models trained on MNLI with neutral and contradiction merged into a single label, nonentailment.</p>
<h2>E Results with augmented training with some subcases withheld</h2>
<p>For each model, we ran five experiments, each one having 6 of the 30 subcases withheld. Each trained model was then evaluated on the categories that had been withheld from it. The results of these experiments are in Tables 10, 11, 12, 13 and 14.</p>
<h2>F Human experiments</h2>
<p>To obtain human results, we used Amazon Mechanical Turk. We subdivided HANS into 114 different categories of examples, covering all possible variations of the template used to generate the example and the specific word around which the template was built. For example, for the constituent heuristic subcase of clauses embedded under verbs (e.g. The doctor believed the lawyer danced $\rightarrow$ The lawyer danced), each possible verb under which the clause could be embedded (e.g. believed, thought, or assumed) counted as a different category.</p>
<p>For each of these 114 categories, we chose 20 examples from HANS and obtained judgments from 5 human participants for each of those 20 examples. Each participant provided judgments for 57 examples plus 10 controls ( 67 stimuli total) and was paid $\$ 2.00$. The controls consisted of 5 examples where the premise and hypothesis were the same (e.g. The doctor saw the lawyer $\rightarrow$ The doctor saw the lawyer) and 5 examples of simple negation (e.g. The doctor saw the lawyer $\rightarrow$ The doctor did not see the lawyer). For analyzing the data, we discarded any participants who answered any of these controls incorrectly; this led to 95 participants being retained and 105 being rejected (participants were still paid regardless of whether they were retained or filtered out). On average, each participant spent 6.5 seconds per example; the participants we retained spent 8.9 sec -
onds per example, while the participants we discarded spent 4.2 seconds per example. The total amount of time from a participant accepting the experiment to completing the experiment averaged 17.6 minutes. This included 9.1 minutes answering the prompts ( 6.4 minutes for discarded participants and 12.1 minutes for retained participants) and roughly one minute spent between prompts ( 1 second after each prompt). The remaining time was spent reading the consent form, reading the instructions, or waiting to start (Mechanical Turk participants often wait several minutes between accepting an experiment and beginning the experiment).</p>
<p>The expert annotators were three native English speakers who had a background in linguistics but who had not heard about this project before providing judgments. Two of them were graduate students and one was a postdoctoral researcher. Each expert annotator labeled 124 examples (one example from each of the 114 categories, plus 10 controls).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Heuristic</th>
<th style="text-align: center;">Subcase</th>
<th style="text-align: center;">DA</th>
<th style="text-align: center;">ESIM</th>
<th style="text-align: center;">SPINN</th>
<th style="text-align: center;">BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical <br> overlap</td>
<td style="text-align: center;">Subject-object swap <br> The senators mentioned the artist. $\rightarrow$ The artist mentioned the senators.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Lexical <br> overlap</td>
<td style="text-align: center;">Untangling relative clauses <br> The athlete who the judges saw called the manager. $\rightarrow$ The judges saw the athlete.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">NP/S <br> The managers heard the secretary resigned. $\rightarrow$ The managers heard the secretary.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">Conjunctions <br> The actor and the professor shouted. $\rightarrow$ The professor shouted.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Embedded under preposition <br> Unless the senators ran, the professors recommended the doctor. $\rightarrow$ The senators ran.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Embedded under preposition <br> Because the banker ran, the doctors saw the professors. $\rightarrow$ The banker ran.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 10: Accuracies for models trained on MNLI augmented with most HANS example categories except withholding the categories in this table (experiment $1 / 5$ for the withheld category investigation).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Heuristic</th>
<th style="text-align: center;">Subcase</th>
<th style="text-align: center;">DA</th>
<th style="text-align: center;">ESIM</th>
<th style="text-align: center;">SPINN</th>
<th style="text-align: center;">BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical <br> overlap</td>
<td style="text-align: center;">Sentences with PPs <br> The judge behind the manager saw the doctors. $\rightarrow$ The doctors saw the manager.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Lexical <br> overlap</td>
<td style="text-align: center;">Sentences with PPs <br> The tourists by the actor called the authors. $\rightarrow$ The tourists called the authors.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">PP on subject <br> The managers near the scientist shouted. $\rightarrow$ The scientist shouted.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">Adjectives <br> Happy professors mentioned the lawyer. $\rightarrow$ Professors mentioned the lawyer.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Outside embedded clause <br> Unless the authors saw the students, the doctors resigned. $\rightarrow$ The doctors resigned.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Outside embedded clause <br> Although the secretaries slept, the judges danced. $\rightarrow$ The judges danced.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 11: Accuracies for models trained on MNLI augmented with most HANS example categories except withholding the categories in this table (experiment $2 / 5$ for the withheld category investigation).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Heuristic</th>
<th style="text-align: center;">Subcase</th>
<th style="text-align: center;">DA</th>
<th style="text-align: center;">ESIM</th>
<th style="text-align: center;">SPINN</th>
<th style="text-align: center;">BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical <br> overlap</td>
<td style="text-align: center;">Sentences with relative clauses <br> The actors called the banker who the tourists saw. $\rightarrow$ The banker called the tourists.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Lexical <br> overlap</td>
<td style="text-align: center;">Sentences with relative clauses <br> The actors that danced encouraged the author. $\rightarrow$ The actors encouraged the author.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">Relative clause on subject <br> The secretary that admired the senator saw the actor. $\rightarrow$ The senator saw the actor.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">Understood argument <br> The author read the book. $\rightarrow$ The author read.</td>
<td style="text-align: center;">$0.28$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.94</td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Embedded under verb <br> The tourists said that the lawyer saw the banker. $\rightarrow$ The lawyer saw the banker.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Embedded under verb <br> The president remembered that the actors performed. $\rightarrow$ The actors performed.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 12: Accuracies for models trained on MNLI augmented with most HANS example categories except withholding the categories in this table (experiment $3 / 5$ for the withheld category investigation).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Heuristic</th>
<th style="text-align: center;">Subcase</th>
<th style="text-align: center;">DA</th>
<th style="text-align: center;">ESIM</th>
<th style="text-align: center;">SPINN</th>
<th style="text-align: center;">BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical <br> overlap</td>
<td style="text-align: center;">Passives <br> The senators were helped by the managers. $\rightarrow$ The senators helped the managers.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Lexical <br> overlap</td>
<td style="text-align: center;">Conjunctions <br> The secretaries saw the scientists and the actors. $\rightarrow$ The secretaries saw the actors.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">MV/RR <br> The senators paid in the office danced. $\rightarrow$ The senators paid in the office.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">Relative clause on object <br> The artists avoided the actors that performed. $\rightarrow$ The artists avoided the actors.</td>
<td style="text-align: center;">$0.72$</td>
<td style="text-align: center;">$1.00$</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Disjunction <br> The judges resigned, or the athletes saw the author. $\rightarrow$ The athletes saw the author.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Conjunction <br> The lawyer danced, and the judge supported the doctors. $\rightarrow$ The lawyer danced.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 13: Accuracies for models trained on MNLI augmented with most HANS example categories except withholding the categories in this table (experiment $4 / 5$ for the withheld category investigation).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Heuristic</th>
<th style="text-align: center;">Subcase</th>
<th style="text-align: center;">DA</th>
<th style="text-align: center;">ESIM</th>
<th style="text-align: center;">SPINN</th>
<th style="text-align: center;">BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical <br> overlap</td>
<td style="text-align: center;">Conjunctions <br> The doctors saw the presidents and the tourists. $\rightarrow$ The presidents saw the tourists.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Lexical <br> overlap</td>
<td style="text-align: center;">Passives <br> The authors were supported by the tourists. $\rightarrow$ The tourists supported the authors.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">NP/Z <br> Before the actors presented the doctors arrived. $\rightarrow$ The actors presented the doctors.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Subsequence</td>
<td style="text-align: center;">PP on object <br> The authors called the judges near the doctor. $\rightarrow$ The authors called the judges.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Adverbs <br> Probably the artists saw the authors. $\rightarrow$ The artists saw the authors.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Constituent</td>
<td style="text-align: center;">Adverbs <br> Certainly the lawyers advised the manager. $\rightarrow$ The lawyers advised the manager.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 14: Accuracies for models trained on MNLI augmented with most HANS example categories except withholding the categories in this table (experiment $5 / 5$ for the withheld category investigation).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ https://github.com/tommccoyl/hans&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ In this table, the lexical overlap counts include the subsequence counts, which include the constituent counts.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>